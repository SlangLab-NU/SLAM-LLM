/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
encoder_config: wavlm-mono
num_epochs: 10
batch_size_training: 4
train_data_folder: librispeech-100_phoneme
test_data_folder: librispeech-100_phoneme
use_peft: true
seed: 
llm_name: llama32_1b
debug: 
test_small: 
freeze_encoder: true
eval_ckpt: best
encoder_projector: linear
encoder_projector_ds_rate: 5
save_embedding: false
projector_transfer_learning: true
transfer_data_folder: psst_phoneme
Final identifier: librispeech-100_phoneme_wavlm_llama32_1b_linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.050975121557712555



----- Transfer Learning Information -----
Resume Epoch: 1
Resume Step: 0
Train Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl
Validation Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl
Test Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl
Identifier: librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
Output Directory: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
----------------------------------------
Resume epoch: 1
Resume step: 0
[2025-01-06 00:59:04][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-01-06 00:59:04][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-06 00:59:04][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-06 00:59:04][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-01-06_00-59-03.txt', 'log_interval': 5}
[2025-01-06 00:59:32][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-06 00:59:37][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 00:59:37][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-06 00:59:37][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 00:59:37][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-06 00:59:47][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-06 00:59:47][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.050975121557712555/model.pt
[2025-01-06 00:59:48][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-06 00:59:48][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-06 00:59:50][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-06 00:59:53][root][INFO] - --> Training Set Length = 2298
[2025-01-06 00:59:53][root][INFO] - --> Validation Set Length = 341
[2025-01-06 00:59:53][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-06 00:59:53][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-06 00:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:56][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 4.565918922424316, acc: 0.2222222238779068)
[2025-01-06 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:57][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 3.6701340675354004, acc: 0.2800000011920929)
[2025-01-06 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:57][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 3.0726566314697266, acc: 0.4054054021835327)
[2025-01-06 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 4.179104328155518, acc: 0.2368421107530594)
[2025-01-06 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 4.1991047859191895, acc: 0.21621622145175934)
[2025-01-06 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 3.3390657901763916, acc: 0.3928571343421936)
[2025-01-06 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 4.611793518066406, acc: 0.30612245202064514)
[2025-01-06 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 3.025235414505005, acc: 0.46666666865348816)
[2025-01-06 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 3.5062613487243652, acc: 0.5)
[2025-01-06 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 1.9377830028533936, acc: 0.692307710647583)
[2025-01-06 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 1.4856634140014648, acc: 0.6666666865348816)
[2025-01-06 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:01][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 3.778326988220215, acc: 0.3076923191547394)
[2025-01-06 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:01][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 3.122279405593872, acc: 0.4545454680919647)
[2025-01-06 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 3.3837459087371826, acc: 0.3695652186870575)
[2025-01-06 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 3.736240863800049, acc: 0.47058823704719543)
[2025-01-06 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 2.7348246574401855, acc: 0.5102040767669678)
[2025-01-06 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 3.640578031539917, acc: 0.42105263471603394)
[2025-01-06 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 2.8711702823638916, acc: 0.4583333432674408)
[2025-01-06 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 4.022885322570801, acc: 0.3611111044883728)
[2025-01-06 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 3.6987786293029785, acc: 0.5263158082962036)
[2025-01-06 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 2.517129421234131, acc: 0.5769230723381042)
[2025-01-06 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 2.9080355167388916, acc: 0.48275861144065857)
[2025-01-06 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 4.26425313949585, acc: 0.3199999928474426)
[2025-01-06 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 2.4392616748809814, acc: 0.6666666865348816)
[2025-01-06 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 2.8660073280334473, acc: 0.5625)
[2025-01-06 01:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 3.410203456878662, acc: 0.4150943458080292)
[2025-01-06 01:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 3.3793797492980957, acc: 0.27397260069847107)
[2025-01-06 01:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08][root][INFO] - Training Epoch: 1/10, step 27/574 completed (loss: 3.235335111618042, acc: 0.3596837818622589)
[2025-01-06 01:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08][root][INFO] - Training Epoch: 1/10, step 28/574 completed (loss: 3.8047492504119873, acc: 0.3488371968269348)
[2025-01-06 01:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08][root][INFO] - Training Epoch: 1/10, step 29/574 completed (loss: 3.430837631225586, acc: 0.40963855385780334)
[2025-01-06 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:09][root][INFO] - Training Epoch: 1/10, step 30/574 completed (loss: 3.1183393001556396, acc: 0.43209877610206604)
[2025-01-06 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:09][root][INFO] - Training Epoch: 1/10, step 31/574 completed (loss: 3.7057902812957764, acc: 0.3928571343421936)
[2025-01-06 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10][root][INFO] - Training Epoch: 1/10, step 32/574 completed (loss: 2.630953073501587, acc: 0.5185185074806213)
[2025-01-06 01:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10][root][INFO] - Training Epoch: 1/10, step 33/574 completed (loss: 2.9639978408813477, acc: 0.6086956262588501)
[2025-01-06 01:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11][root][INFO] - Training Epoch: 1/10, step 34/574 completed (loss: 2.693814754486084, acc: 0.45378151535987854)
[2025-01-06 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11][root][INFO] - Training Epoch: 1/10, step 35/574 completed (loss: 2.742159366607666, acc: 0.5409836173057556)
[2025-01-06 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11][root][INFO] - Training Epoch: 1/10, step 36/574 completed (loss: 2.9530844688415527, acc: 0.4285714328289032)
[2025-01-06 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12][root][INFO] - Training Epoch: 1/10, step 37/574 completed (loss: 2.8723032474517822, acc: 0.5423728823661804)
[2025-01-06 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12][root][INFO] - Training Epoch: 1/10, step 38/574 completed (loss: 2.8279476165771484, acc: 0.5632184147834778)
[2025-01-06 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13][root][INFO] - Training Epoch: 1/10, step 39/574 completed (loss: 4.378084659576416, acc: 0.2380952388048172)
[2025-01-06 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13][root][INFO] - Training Epoch: 1/10, step 40/574 completed (loss: 2.7955381870269775, acc: 0.6538461446762085)
[2025-01-06 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13][root][INFO] - Training Epoch: 1/10, step 41/574 completed (loss: 2.3172032833099365, acc: 0.5945945978164673)
[2025-01-06 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:14][root][INFO] - Training Epoch: 1/10, step 42/574 completed (loss: 3.5862834453582764, acc: 0.3692307770252228)
[2025-01-06 01:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:14][root][INFO] - Training Epoch: 1/10, step 43/574 completed (loss: 3.4920356273651123, acc: 0.3232323229312897)
[2025-01-06 01:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15][root][INFO] - Training Epoch: 1/10, step 44/574 completed (loss: 2.637434244155884, acc: 0.5670102834701538)
[2025-01-06 01:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15][root][INFO] - Training Epoch: 1/10, step 45/574 completed (loss: 3.285001039505005, acc: 0.40441176295280457)
[2025-01-06 01:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16][root][INFO] - Training Epoch: 1/10, step 46/574 completed (loss: 3.3272156715393066, acc: 0.38461539149284363)
[2025-01-06 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16][root][INFO] - Training Epoch: 1/10, step 47/574 completed (loss: 1.9891725778579712, acc: 0.5925925970077515)
[2025-01-06 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16][root][INFO] - Training Epoch: 1/10, step 48/574 completed (loss: 2.2156660556793213, acc: 0.5357142686843872)
[2025-01-06 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17][root][INFO] - Training Epoch: 1/10, step 49/574 completed (loss: 2.1350274085998535, acc: 0.5833333134651184)
[2025-01-06 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17][root][INFO] - Training Epoch: 1/10, step 50/574 completed (loss: 3.502253293991089, acc: 0.4736842215061188)
[2025-01-06 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17][root][INFO] - Training Epoch: 1/10, step 51/574 completed (loss: 3.330608606338501, acc: 0.460317462682724)
[2025-01-06 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18][root][INFO] - Training Epoch: 1/10, step 52/574 completed (loss: 4.11309814453125, acc: 0.35211268067359924)
[2025-01-06 01:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18][root][INFO] - Training Epoch: 1/10, step 53/574 completed (loss: 3.9685912132263184, acc: 0.2866666615009308)
[2025-01-06 01:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:19][root][INFO] - Training Epoch: 1/10, step 54/574 completed (loss: 4.304090976715088, acc: 0.21621622145175934)
[2025-01-06 01:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:19][root][INFO] - Training Epoch: 1/10, step 55/574 completed (loss: 1.8609943389892578, acc: 0.5384615659713745)
[2025-01-06 01:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:22][root][INFO] - Training Epoch: 1/10, step 56/574 completed (loss: 3.0619962215423584, acc: 0.3344709873199463)
[2025-01-06 01:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:23][root][INFO] - Training Epoch: 1/10, step 57/574 completed (loss: 3.080946207046509, acc: 0.35729846358299255)
[2025-01-06 01:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:24][root][INFO] - Training Epoch: 1/10, step 58/574 completed (loss: 3.339125156402588, acc: 0.3693181872367859)
[2025-01-06 01:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:25][root][INFO] - Training Epoch: 1/10, step 59/574 completed (loss: 2.5193119049072266, acc: 0.4485294222831726)
[2025-01-06 01:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:25][root][INFO] - Training Epoch: 1/10, step 60/574 completed (loss: 2.8575000762939453, acc: 0.4057970941066742)
[2025-01-06 01:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26][root][INFO] - Training Epoch: 1/10, step 61/574 completed (loss: 2.9075052738189697, acc: 0.38749998807907104)
[2025-01-06 01:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26][root][INFO] - Training Epoch: 1/10, step 62/574 completed (loss: 1.6552412509918213, acc: 0.529411792755127)
[2025-01-06 01:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26][root][INFO] - Training Epoch: 1/10, step 63/574 completed (loss: 2.6073663234710693, acc: 0.5277777910232544)
[2025-01-06 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:27][root][INFO] - Training Epoch: 1/10, step 64/574 completed (loss: 1.9236969947814941, acc: 0.640625)
[2025-01-06 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:27][root][INFO] - Training Epoch: 1/10, step 65/574 completed (loss: 1.4708648920059204, acc: 0.6206896305084229)
[2025-01-06 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28][root][INFO] - Training Epoch: 1/10, step 66/574 completed (loss: 3.460084915161133, acc: 0.3392857015132904)
[2025-01-06 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28][root][INFO] - Training Epoch: 1/10, step 67/574 completed (loss: 2.7413833141326904, acc: 0.4000000059604645)
[2025-01-06 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28][root][INFO] - Training Epoch: 1/10, step 68/574 completed (loss: 1.189255714416504, acc: 0.7200000286102295)
[2025-01-06 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29][root][INFO] - Training Epoch: 1/10, step 69/574 completed (loss: 2.2416422367095947, acc: 0.4166666567325592)
[2025-01-06 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29][root][INFO] - Training Epoch: 1/10, step 70/574 completed (loss: 3.6767418384552, acc: 0.3636363744735718)
[2025-01-06 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:30][root][INFO] - Training Epoch: 1/10, step 71/574 completed (loss: 2.6239001750946045, acc: 0.40441176295280457)
[2025-01-06 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:30][root][INFO] - Training Epoch: 1/10, step 72/574 completed (loss: 1.813328742980957, acc: 0.5555555820465088)
[2025-01-06 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31][root][INFO] - Training Epoch: 1/10, step 73/574 completed (loss: 2.6633832454681396, acc: 0.3692307770252228)
[2025-01-06 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31][root][INFO] - Training Epoch: 1/10, step 74/574 completed (loss: 3.560786008834839, acc: 0.30612245202064514)
[2025-01-06 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31][root][INFO] - Training Epoch: 1/10, step 75/574 completed (loss: 2.566763162612915, acc: 0.3805970251560211)
[2025-01-06 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32][root][INFO] - Training Epoch: 1/10, step 76/574 completed (loss: 3.073969602584839, acc: 0.3540146052837372)
[2025-01-06 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32][root][INFO] - Training Epoch: 1/10, step 77/574 completed (loss: 1.0587918758392334, acc: 0.7142857313156128)
[2025-01-06 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32][root][INFO] - Training Epoch: 1/10, step 78/574 completed (loss: 1.4693115949630737, acc: 0.625)
[2025-01-06 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33][root][INFO] - Training Epoch: 1/10, step 79/574 completed (loss: 1.207265019416809, acc: 0.6666666865348816)
[2025-01-06 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33][root][INFO] - Training Epoch: 1/10, step 80/574 completed (loss: 1.974440097808838, acc: 0.692307710647583)
[2025-01-06 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33][root][INFO] - Training Epoch: 1/10, step 81/574 completed (loss: 2.6414620876312256, acc: 0.557692289352417)
[2025-01-06 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34][root][INFO] - Training Epoch: 1/10, step 82/574 completed (loss: 2.7963244915008545, acc: 0.4423076808452606)
[2025-01-06 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34][root][INFO] - Training Epoch: 1/10, step 83/574 completed (loss: 0.808995246887207, acc: 0.84375)
[2025-01-06 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35][root][INFO] - Training Epoch: 1/10, step 84/574 completed (loss: 1.9839420318603516, acc: 0.5942028760910034)
[2025-01-06 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35][root][INFO] - Training Epoch: 1/10, step 85/574 completed (loss: 2.5024800300598145, acc: 0.5600000023841858)
[2025-01-06 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35][root][INFO] - Training Epoch: 1/10, step 86/574 completed (loss: 0.9810752272605896, acc: 0.695652186870575)
[2025-01-06 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:36][root][INFO] - Training Epoch: 1/10, step 87/574 completed (loss: 2.8885109424591064, acc: 0.4000000059604645)
[2025-01-06 01:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:36][root][INFO] - Training Epoch: 1/10, step 88/574 completed (loss: 3.065351963043213, acc: 0.3883495032787323)
[2025-01-06 01:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:37][root][INFO] - Training Epoch: 1/10, step 89/574 completed (loss: 2.4020309448242188, acc: 0.49514561891555786)
[2025-01-06 01:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:38][root][INFO] - Training Epoch: 1/10, step 90/574 completed (loss: 2.8965706825256348, acc: 0.42473119497299194)
[2025-01-06 01:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:39][root][INFO] - Training Epoch: 1/10, step 91/574 completed (loss: 2.404137372970581, acc: 0.5)
[2025-01-06 01:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:40][root][INFO] - Training Epoch: 1/10, step 92/574 completed (loss: 2.4086596965789795, acc: 0.5263158082962036)
[2025-01-06 01:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41][root][INFO] - Training Epoch: 1/10, step 93/574 completed (loss: 3.314141035079956, acc: 0.2673267424106598)
[2025-01-06 01:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41][root][INFO] - Training Epoch: 1/10, step 94/574 completed (loss: 2.3490023612976074, acc: 0.4838709533214569)
[2025-01-06 01:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41][root][INFO] - Training Epoch: 1/10, step 95/574 completed (loss: 2.4161081314086914, acc: 0.3478260934352875)
[2025-01-06 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42][root][INFO] - Training Epoch: 1/10, step 96/574 completed (loss: 3.2233736515045166, acc: 0.32773110270500183)
[2025-01-06 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42][root][INFO] - Training Epoch: 1/10, step 97/574 completed (loss: 3.274331569671631, acc: 0.32692307233810425)
[2025-01-06 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43][root][INFO] - Training Epoch: 1/10, step 98/574 completed (loss: 3.3142571449279785, acc: 0.30656933784484863)
[2025-01-06 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43][root][INFO] - Training Epoch: 1/10, step 99/574 completed (loss: 3.4230663776397705, acc: 0.31343284249305725)
[2025-01-06 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43][root][INFO] - Training Epoch: 1/10, step 100/574 completed (loss: 2.4190423488616943, acc: 0.550000011920929)
[2025-01-06 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44][root][INFO] - Training Epoch: 1/10, step 101/574 completed (loss: 1.362923502922058, acc: 0.7272727489471436)
[2025-01-06 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44][root][INFO] - Training Epoch: 1/10, step 102/574 completed (loss: 0.7006093859672546, acc: 0.739130437374115)
[2025-01-06 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44][root][INFO] - Training Epoch: 1/10, step 103/574 completed (loss: 0.9565266966819763, acc: 0.7727272510528564)
[2025-01-06 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45][root][INFO] - Training Epoch: 1/10, step 104/574 completed (loss: 1.5804924964904785, acc: 0.6206896305084229)
[2025-01-06 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45][root][INFO] - Training Epoch: 1/10, step 105/574 completed (loss: 1.3597323894500732, acc: 0.6976743936538696)
[2025-01-06 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46][root][INFO] - Training Epoch: 1/10, step 106/574 completed (loss: 0.986910343170166, acc: 0.7599999904632568)
[2025-01-06 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46][root][INFO] - Training Epoch: 1/10, step 107/574 completed (loss: 0.9224901795387268, acc: 0.8235294222831726)
[2025-01-06 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46][root][INFO] - Training Epoch: 1/10, step 108/574 completed (loss: 0.8043091297149658, acc: 0.8461538553237915)
[2025-01-06 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:47][root][INFO] - Training Epoch: 1/10, step 109/574 completed (loss: 0.6700358390808105, acc: 0.8809523582458496)
[2025-01-06 01:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:47][root][INFO] - Training Epoch: 1/10, step 110/574 completed (loss: 1.8345379829406738, acc: 0.6615384817123413)
[2025-01-06 01:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48][root][INFO] - Training Epoch: 1/10, step 111/574 completed (loss: 1.5172370672225952, acc: 0.6315789222717285)
[2025-01-06 01:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48][root][INFO] - Training Epoch: 1/10, step 112/574 completed (loss: 2.771487236022949, acc: 0.4736842215061188)
[2025-01-06 01:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48][root][INFO] - Training Epoch: 1/10, step 113/574 completed (loss: 1.5295499563217163, acc: 0.5897436141967773)
[2025-01-06 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49][root][INFO] - Training Epoch: 1/10, step 114/574 completed (loss: 1.512942910194397, acc: 0.6530612111091614)
[2025-01-06 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49][root][INFO] - Training Epoch: 1/10, step 115/574 completed (loss: 1.3965150117874146, acc: 0.7272727489471436)
[2025-01-06 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50][root][INFO] - Training Epoch: 1/10, step 116/574 completed (loss: 1.3159472942352295, acc: 0.6190476417541504)
[2025-01-06 01:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50][root][INFO] - Training Epoch: 1/10, step 117/574 completed (loss: 1.4864555597305298, acc: 0.6341463327407837)
[2025-01-06 01:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50][root][INFO] - Training Epoch: 1/10, step 118/574 completed (loss: 1.5972137451171875, acc: 0.7096773982048035)
[2025-01-06 01:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:51][root][INFO] - Training Epoch: 1/10, step 119/574 completed (loss: 1.9935545921325684, acc: 0.5399239659309387)
[2025-01-06 01:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52][root][INFO] - Training Epoch: 1/10, step 120/574 completed (loss: 1.3953064680099487, acc: 0.6933333277702332)
[2025-01-06 01:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52][root][INFO] - Training Epoch: 1/10, step 121/574 completed (loss: 2.0686235427856445, acc: 0.5961538553237915)
[2025-01-06 01:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:53][root][INFO] - Training Epoch: 1/10, step 122/574 completed (loss: 1.1900187730789185, acc: 0.6666666865348816)
[2025-01-06 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:53][root][INFO] - Training Epoch: 1/10, step 123/574 completed (loss: 1.0193647146224976, acc: 0.6842105388641357)
[2025-01-06 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54][root][INFO] - Training Epoch: 1/10, step 124/574 completed (loss: 2.0188281536102295, acc: 0.5521472096443176)
[2025-01-06 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54][root][INFO] - Training Epoch: 1/10, step 125/574 completed (loss: 2.0531625747680664, acc: 0.5)
[2025-01-06 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54][root][INFO] - Training Epoch: 1/10, step 126/574 completed (loss: 1.9220811128616333, acc: 0.49166667461395264)
[2025-01-06 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55][root][INFO] - Training Epoch: 1/10, step 127/574 completed (loss: 1.6308506727218628, acc: 0.5773809552192688)
[2025-01-06 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55][root][INFO] - Training Epoch: 1/10, step 128/574 completed (loss: 1.7957671880722046, acc: 0.5897436141967773)
[2025-01-06 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56][root][INFO] - Training Epoch: 1/10, step 129/574 completed (loss: 1.7102088928222656, acc: 0.5441176295280457)
[2025-01-06 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56][root][INFO] - Training Epoch: 1/10, step 130/574 completed (loss: 2.6644608974456787, acc: 0.3461538553237915)
[2025-01-06 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56][root][INFO] - Training Epoch: 1/10, step 131/574 completed (loss: 2.473231554031372, acc: 0.43478259444236755)
[2025-01-06 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57][root][INFO] - Training Epoch: 1/10, step 132/574 completed (loss: 2.0688424110412598, acc: 0.5)
[2025-01-06 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57][root][INFO] - Training Epoch: 1/10, step 133/574 completed (loss: 2.3398048877716064, acc: 0.3913043439388275)
[2025-01-06 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57][root][INFO] - Training Epoch: 1/10, step 134/574 completed (loss: 1.6979541778564453, acc: 0.5714285969734192)
[2025-01-06 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58][root][INFO] - Training Epoch: 1/10, step 135/574 completed (loss: 1.8395874500274658, acc: 0.5)
[2025-01-06 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58][root][INFO] - Training Epoch: 1/10, step 136/574 completed (loss: 1.822038173675537, acc: 0.5952380895614624)
[2025-01-06 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59][root][INFO] - Training Epoch: 1/10, step 137/574 completed (loss: 2.21230149269104, acc: 0.36666667461395264)
[2025-01-06 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59][root][INFO] - Training Epoch: 1/10, step 138/574 completed (loss: 1.7173728942871094, acc: 0.6521739363670349)
[2025-01-06 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59][root][INFO] - Training Epoch: 1/10, step 139/574 completed (loss: 0.6513708233833313, acc: 0.8571428656578064)
[2025-01-06 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00][root][INFO] - Training Epoch: 1/10, step 140/574 completed (loss: 0.7563849687576294, acc: 0.7692307829856873)
[2025-01-06 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00][root][INFO] - Training Epoch: 1/10, step 141/574 completed (loss: 1.36531662940979, acc: 0.6451612710952759)
[2025-01-06 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00][root][INFO] - Training Epoch: 1/10, step 142/574 completed (loss: 1.5777860879898071, acc: 0.5945945978164673)
[2025-01-06 01:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.4351, device='cuda:0') eval_epoch_loss=tensor(1.2341, device='cuda:0') eval_epoch_acc=tensor(0.7115, device='cuda:0')
[2025-01-06 01:01:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:01:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:01:34][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_143_loss_1.2340573072433472/model.pt
[2025-01-06 01:01:34][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:01:34][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.2340573072433472
[2025-01-06 01:01:34][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.711460292339325
[2025-01-06 01:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:34][root][INFO] - Training Epoch: 1/10, step 143/574 completed (loss: 1.9549590349197388, acc: 0.5526315569877625)
[2025-01-06 01:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35][root][INFO] - Training Epoch: 1/10, step 144/574 completed (loss: 1.4470947980880737, acc: 0.6865671873092651)
[2025-01-06 01:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35][root][INFO] - Training Epoch: 1/10, step 145/574 completed (loss: 1.6241623163223267, acc: 0.5510203838348389)
[2025-01-06 01:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36][root][INFO] - Training Epoch: 1/10, step 146/574 completed (loss: 1.932931900024414, acc: 0.478723406791687)
[2025-01-06 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36][root][INFO] - Training Epoch: 1/10, step 147/574 completed (loss: 2.225083827972412, acc: 0.4714285731315613)
[2025-01-06 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36][root][INFO] - Training Epoch: 1/10, step 148/574 completed (loss: 1.9084231853485107, acc: 0.4642857015132904)
[2025-01-06 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:37][root][INFO] - Training Epoch: 1/10, step 149/574 completed (loss: 1.9028998613357544, acc: 0.52173912525177)
[2025-01-06 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:37][root][INFO] - Training Epoch: 1/10, step 150/574 completed (loss: 1.4511011838912964, acc: 0.5862069129943848)
[2025-01-06 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38][root][INFO] - Training Epoch: 1/10, step 151/574 completed (loss: 2.0051543712615967, acc: 0.5652173757553101)
[2025-01-06 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38][root][INFO] - Training Epoch: 1/10, step 152/574 completed (loss: 1.2925492525100708, acc: 0.6610169410705566)
[2025-01-06 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38][root][INFO] - Training Epoch: 1/10, step 153/574 completed (loss: 1.4737142324447632, acc: 0.6140350699424744)
[2025-01-06 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39][root][INFO] - Training Epoch: 1/10, step 154/574 completed (loss: 1.682312250137329, acc: 0.6486486196517944)
[2025-01-06 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39][root][INFO] - Training Epoch: 1/10, step 155/574 completed (loss: 1.2318115234375, acc: 0.7857142686843872)
[2025-01-06 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39][root][INFO] - Training Epoch: 1/10, step 156/574 completed (loss: 1.203286051750183, acc: 0.6521739363670349)
[2025-01-06 01:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:40][root][INFO] - Training Epoch: 1/10, step 157/574 completed (loss: 2.9605486392974854, acc: 0.3684210479259491)
[2025-01-06 01:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:41][root][INFO] - Training Epoch: 1/10, step 158/574 completed (loss: 3.461270809173584, acc: 0.3918918967247009)
[2025-01-06 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42][root][INFO] - Training Epoch: 1/10, step 159/574 completed (loss: 2.571671724319458, acc: 0.40740740299224854)
[2025-01-06 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42][root][INFO] - Training Epoch: 1/10, step 160/574 completed (loss: 2.955364227294922, acc: 0.3720930218696594)
[2025-01-06 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:43][root][INFO] - Training Epoch: 1/10, step 161/574 completed (loss: 3.0573654174804688, acc: 0.3529411852359772)
[2025-01-06 01:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:43][root][INFO] - Training Epoch: 1/10, step 162/574 completed (loss: 3.1949915885925293, acc: 0.33707866072654724)
[2025-01-06 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44][root][INFO] - Training Epoch: 1/10, step 163/574 completed (loss: 1.6060246229171753, acc: 0.7045454382896423)
[2025-01-06 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44][root][INFO] - Training Epoch: 1/10, step 164/574 completed (loss: 0.9491782188415527, acc: 0.7142857313156128)
[2025-01-06 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45][root][INFO] - Training Epoch: 1/10, step 165/574 completed (loss: 1.607667326927185, acc: 0.5517241358757019)
[2025-01-06 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45][root][INFO] - Training Epoch: 1/10, step 166/574 completed (loss: 0.8239145278930664, acc: 0.8367347121238708)
[2025-01-06 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45][root][INFO] - Training Epoch: 1/10, step 167/574 completed (loss: 0.7499496340751648, acc: 0.8199999928474426)
[2025-01-06 01:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46][root][INFO] - Training Epoch: 1/10, step 168/574 completed (loss: 1.4847655296325684, acc: 0.7222222089767456)
[2025-01-06 01:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46][root][INFO] - Training Epoch: 1/10, step 169/574 completed (loss: 1.375377893447876, acc: 0.6470588445663452)
[2025-01-06 01:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:47][root][INFO] - Training Epoch: 1/10, step 170/574 completed (loss: 2.257110595703125, acc: 0.5068492889404297)
[2025-01-06 01:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48][root][INFO] - Training Epoch: 1/10, step 171/574 completed (loss: 1.0497509241104126, acc: 0.6666666865348816)
[2025-01-06 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48][root][INFO] - Training Epoch: 1/10, step 172/574 completed (loss: 1.6921805143356323, acc: 0.5925925970077515)
[2025-01-06 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48][root][INFO] - Training Epoch: 1/10, step 173/574 completed (loss: 2.112257719039917, acc: 0.4642857015132904)
[2025-01-06 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:49][root][INFO] - Training Epoch: 1/10, step 174/574 completed (loss: 1.498665452003479, acc: 0.6371681690216064)
[2025-01-06 01:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:49][root][INFO] - Training Epoch: 1/10, step 175/574 completed (loss: 1.578950047492981, acc: 0.6376811861991882)
[2025-01-06 01:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:50][root][INFO] - Training Epoch: 1/10, step 176/574 completed (loss: 1.294198989868164, acc: 0.6477272510528564)
[2025-01-06 01:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:50][root][INFO] - Training Epoch: 1/10, step 177/574 completed (loss: 2.227262496948242, acc: 0.49618321657180786)
[2025-01-06 01:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:51][root][INFO] - Training Epoch: 1/10, step 178/574 completed (loss: 2.326470136642456, acc: 0.4888888895511627)
[2025-01-06 01:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52][root][INFO] - Training Epoch: 1/10, step 179/574 completed (loss: 1.264778971672058, acc: 0.6721311211585999)
[2025-01-06 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52][root][INFO] - Training Epoch: 1/10, step 180/574 completed (loss: 0.6173291802406311, acc: 0.8333333134651184)
[2025-01-06 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52][root][INFO] - Training Epoch: 1/10, step 181/574 completed (loss: 0.1056605651974678, acc: 1.0)
[2025-01-06 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:53][root][INFO] - Training Epoch: 1/10, step 182/574 completed (loss: 0.72105872631073, acc: 0.7857142686843872)
[2025-01-06 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:53][root][INFO] - Training Epoch: 1/10, step 183/574 completed (loss: 0.8877244591712952, acc: 0.792682945728302)
[2025-01-06 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54][root][INFO] - Training Epoch: 1/10, step 184/574 completed (loss: 1.178899884223938, acc: 0.7643504738807678)
[2025-01-06 01:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54][root][INFO] - Training Epoch: 1/10, step 185/574 completed (loss: 0.9880927205085754, acc: 0.7694524526596069)
[2025-01-06 01:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54][root][INFO] - Training Epoch: 1/10, step 186/574 completed (loss: 1.0301824808120728, acc: 0.746874988079071)
[2025-01-06 01:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:55][root][INFO] - Training Epoch: 1/10, step 187/574 completed (loss: 0.9308052062988281, acc: 0.7786116600036621)
[2025-01-06 01:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:55][root][INFO] - Training Epoch: 1/10, step 188/574 completed (loss: 1.1477410793304443, acc: 0.6975088715553284)
[2025-01-06 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:56][root][INFO] - Training Epoch: 1/10, step 189/574 completed (loss: 1.1203824281692505, acc: 0.6800000071525574)
[2025-01-06 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:56][root][INFO] - Training Epoch: 1/10, step 190/574 completed (loss: 1.9090445041656494, acc: 0.5348837375640869)
[2025-01-06 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:57][root][INFO] - Training Epoch: 1/10, step 191/574 completed (loss: 2.757408618927002, acc: 0.4126984179019928)
[2025-01-06 01:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:58][root][INFO] - Training Epoch: 1/10, step 192/574 completed (loss: 2.4030003547668457, acc: 0.4545454680919647)
[2025-01-06 01:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:59][root][INFO] - Training Epoch: 1/10, step 193/574 completed (loss: 2.01914644241333, acc: 0.5176470875740051)
[2025-01-06 01:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:00][root][INFO] - Training Epoch: 1/10, step 194/574 completed (loss: 2.286694049835205, acc: 0.42592594027519226)
[2025-01-06 01:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01][root][INFO] - Training Epoch: 1/10, step 195/574 completed (loss: 2.034234046936035, acc: 0.5161290168762207)
[2025-01-06 01:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01][root][INFO] - Training Epoch: 1/10, step 196/574 completed (loss: 0.8813538551330566, acc: 0.6785714030265808)
[2025-01-06 01:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01][root][INFO] - Training Epoch: 1/10, step 197/574 completed (loss: 2.1258342266082764, acc: 0.550000011920929)
[2025-01-06 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:02][root][INFO] - Training Epoch: 1/10, step 198/574 completed (loss: 1.7744529247283936, acc: 0.6470588445663452)
[2025-01-06 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:02][root][INFO] - Training Epoch: 1/10, step 199/574 completed (loss: 1.7422068119049072, acc: 0.654411792755127)
[2025-01-06 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03][root][INFO] - Training Epoch: 1/10, step 200/574 completed (loss: 1.3642441034317017, acc: 0.6440678238868713)
[2025-01-06 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03][root][INFO] - Training Epoch: 1/10, step 201/574 completed (loss: 1.7914937734603882, acc: 0.5820895433425903)
[2025-01-06 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03][root][INFO] - Training Epoch: 1/10, step 202/574 completed (loss: 2.061892509460449, acc: 0.5436893105506897)
[2025-01-06 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04][root][INFO] - Training Epoch: 1/10, step 203/574 completed (loss: 1.547580599784851, acc: 0.60317462682724)
[2025-01-06 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04][root][INFO] - Training Epoch: 1/10, step 204/574 completed (loss: 0.40879595279693604, acc: 0.901098906993866)
[2025-01-06 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04][root][INFO] - Training Epoch: 1/10, step 205/574 completed (loss: 0.896608829498291, acc: 0.7982062697410583)
[2025-01-06 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05][root][INFO] - Training Epoch: 1/10, step 206/574 completed (loss: 0.8917878270149231, acc: 0.7795275449752808)
[2025-01-06 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05][root][INFO] - Training Epoch: 1/10, step 207/574 completed (loss: 1.0290192365646362, acc: 0.7887930870056152)
[2025-01-06 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06][root][INFO] - Training Epoch: 1/10, step 208/574 completed (loss: 0.829142689704895, acc: 0.8007246255874634)
[2025-01-06 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06][root][INFO] - Training Epoch: 1/10, step 209/574 completed (loss: 1.019532322883606, acc: 0.774319052696228)
[2025-01-06 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06][root][INFO] - Training Epoch: 1/10, step 210/574 completed (loss: 0.783573567867279, acc: 0.804347813129425)
[2025-01-06 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07][root][INFO] - Training Epoch: 1/10, step 211/574 completed (loss: 0.9122943878173828, acc: 0.782608687877655)
[2025-01-06 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07][root][INFO] - Training Epoch: 1/10, step 212/574 completed (loss: 0.45959654450416565, acc: 0.8571428656578064)
[2025-01-06 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07][root][INFO] - Training Epoch: 1/10, step 213/574 completed (loss: 0.9614325761795044, acc: 0.8510638475418091)
[2025-01-06 01:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:08][root][INFO] - Training Epoch: 1/10, step 214/574 completed (loss: 1.046085000038147, acc: 0.807692289352417)
[2025-01-06 01:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09][root][INFO] - Training Epoch: 1/10, step 215/574 completed (loss: 0.721667468547821, acc: 0.8108108043670654)
[2025-01-06 01:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09][root][INFO] - Training Epoch: 1/10, step 216/574 completed (loss: 0.8763503432273865, acc: 0.8372092843055725)
[2025-01-06 01:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09][root][INFO] - Training Epoch: 1/10, step 217/574 completed (loss: 0.774082362651825, acc: 0.837837815284729)
[2025-01-06 01:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:10][root][INFO] - Training Epoch: 1/10, step 218/574 completed (loss: 0.6503811478614807, acc: 0.8666666746139526)
[2025-01-06 01:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:10][root][INFO] - Training Epoch: 1/10, step 219/574 completed (loss: 0.6247416138648987, acc: 0.8484848737716675)
[2025-01-06 01:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11][root][INFO] - Training Epoch: 1/10, step 220/574 completed (loss: 0.841682493686676, acc: 0.7407407164573669)
[2025-01-06 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11][root][INFO] - Training Epoch: 1/10, step 221/574 completed (loss: 0.41468486189842224, acc: 0.8799999952316284)
[2025-01-06 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11][root][INFO] - Training Epoch: 1/10, step 222/574 completed (loss: 1.4962539672851562, acc: 0.6538461446762085)
[2025-01-06 01:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:12][root][INFO] - Training Epoch: 1/10, step 223/574 completed (loss: 1.2454911470413208, acc: 0.7336956262588501)
[2025-01-06 01:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:13][root][INFO] - Training Epoch: 1/10, step 224/574 completed (loss: 1.2627167701721191, acc: 0.7102272510528564)
[2025-01-06 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:13][root][INFO] - Training Epoch: 1/10, step 225/574 completed (loss: 1.3118818998336792, acc: 0.6489361524581909)
[2025-01-06 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14][root][INFO] - Training Epoch: 1/10, step 226/574 completed (loss: 1.9364824295043945, acc: 0.6037735939025879)
[2025-01-06 01:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14][root][INFO] - Training Epoch: 1/10, step 227/574 completed (loss: 1.1364891529083252, acc: 0.6000000238418579)
[2025-01-06 01:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14][root][INFO] - Training Epoch: 1/10, step 228/574 completed (loss: 1.4006057977676392, acc: 0.6279069781303406)
[2025-01-06 01:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15][root][INFO] - Training Epoch: 1/10, step 229/574 completed (loss: 2.627882957458496, acc: 0.4333333373069763)
[2025-01-06 01:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15][root][INFO] - Training Epoch: 1/10, step 230/574 completed (loss: 2.986215591430664, acc: 0.3052631616592407)
[2025-01-06 01:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15][root][INFO] - Training Epoch: 1/10, step 231/574 completed (loss: 2.362199544906616, acc: 0.41111111640930176)
[2025-01-06 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:16][root][INFO] - Training Epoch: 1/10, step 232/574 completed (loss: 2.284888982772827, acc: 0.44999998807907104)
[2025-01-06 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:16][root][INFO] - Training Epoch: 1/10, step 233/574 completed (loss: 2.469341278076172, acc: 0.4082568883895874)
[2025-01-06 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17][root][INFO] - Training Epoch: 1/10, step 234/574 completed (loss: 2.4537644386291504, acc: 0.4384615421295166)
[2025-01-06 01:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17][root][INFO] - Training Epoch: 1/10, step 235/574 completed (loss: 1.0212740898132324, acc: 0.7368420958518982)
[2025-01-06 01:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17][root][INFO] - Training Epoch: 1/10, step 236/574 completed (loss: 1.0644100904464722, acc: 0.7083333134651184)
[2025-01-06 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18][root][INFO] - Training Epoch: 1/10, step 237/574 completed (loss: 1.4477680921554565, acc: 0.5909090638160706)
[2025-01-06 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18][root][INFO] - Training Epoch: 1/10, step 238/574 completed (loss: 1.5191031694412231, acc: 0.5925925970077515)
[2025-01-06 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19][root][INFO] - Training Epoch: 1/10, step 239/574 completed (loss: 1.396470546722412, acc: 0.6285714507102966)
[2025-01-06 01:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19][root][INFO] - Training Epoch: 1/10, step 240/574 completed (loss: 1.687599539756775, acc: 0.5909090638160706)
[2025-01-06 01:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19][root][INFO] - Training Epoch: 1/10, step 241/574 completed (loss: 1.2372450828552246, acc: 0.7045454382896423)
[2025-01-06 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:20][root][INFO] - Training Epoch: 1/10, step 242/574 completed (loss: 2.298060655593872, acc: 0.4354838728904724)
[2025-01-06 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21][root][INFO] - Training Epoch: 1/10, step 243/574 completed (loss: 1.98967707157135, acc: 0.47727271914482117)
[2025-01-06 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21][root][INFO] - Training Epoch: 1/10, step 244/574 completed (loss: 0.6574301719665527, acc: 0.8571428656578064)
[2025-01-06 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21][root][INFO] - Training Epoch: 1/10, step 245/574 completed (loss: 0.9231048822402954, acc: 0.7307692170143127)
[2025-01-06 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22][root][INFO] - Training Epoch: 1/10, step 246/574 completed (loss: 0.6213657259941101, acc: 0.8064516186714172)
[2025-01-06 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22][root][INFO] - Training Epoch: 1/10, step 247/574 completed (loss: 0.9209882616996765, acc: 0.6000000238418579)
[2025-01-06 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23][root][INFO] - Training Epoch: 1/10, step 248/574 completed (loss: 1.426820993423462, acc: 0.7027027010917664)
[2025-01-06 01:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23][root][INFO] - Training Epoch: 1/10, step 249/574 completed (loss: 0.8498873114585876, acc: 0.7837837934494019)
[2025-01-06 01:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23][root][INFO] - Training Epoch: 1/10, step 250/574 completed (loss: 0.9119333028793335, acc: 0.837837815284729)
[2025-01-06 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:24][root][INFO] - Training Epoch: 1/10, step 251/574 completed (loss: 0.7752217054367065, acc: 0.8088235259056091)
[2025-01-06 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:24][root][INFO] - Training Epoch: 1/10, step 252/574 completed (loss: 0.756057858467102, acc: 0.8048780560493469)
[2025-01-06 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25][root][INFO] - Training Epoch: 1/10, step 253/574 completed (loss: 0.7279499769210815, acc: 0.7599999904632568)
[2025-01-06 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25][root][INFO] - Training Epoch: 1/10, step 254/574 completed (loss: 0.24241739511489868, acc: 0.9599999785423279)
[2025-01-06 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25][root][INFO] - Training Epoch: 1/10, step 255/574 completed (loss: 0.746372640132904, acc: 0.774193525314331)
[2025-01-06 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26][root][INFO] - Training Epoch: 1/10, step 256/574 completed (loss: 0.8064952492713928, acc: 0.859649121761322)
[2025-01-06 01:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26][root][INFO] - Training Epoch: 1/10, step 257/574 completed (loss: 0.5405676960945129, acc: 0.8571428656578064)
[2025-01-06 01:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26][root][INFO] - Training Epoch: 1/10, step 258/574 completed (loss: 0.45191457867622375, acc: 0.8947368264198303)
[2025-01-06 01:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:27][root][INFO] - Training Epoch: 1/10, step 259/574 completed (loss: 0.8321369290351868, acc: 0.7735849022865295)
[2025-01-06 01:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28][root][INFO] - Training Epoch: 1/10, step 260/574 completed (loss: 0.6877263784408569, acc: 0.7916666865348816)
[2025-01-06 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28][root][INFO] - Training Epoch: 1/10, step 261/574 completed (loss: 0.8236425518989563, acc: 0.7777777910232544)
[2025-01-06 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28][root][INFO] - Training Epoch: 1/10, step 262/574 completed (loss: 1.3900659084320068, acc: 0.6451612710952759)
[2025-01-06 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:29][root][INFO] - Training Epoch: 1/10, step 263/574 completed (loss: 1.7632949352264404, acc: 0.6133333444595337)
[2025-01-06 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:29][root][INFO] - Training Epoch: 1/10, step 264/574 completed (loss: 1.2382087707519531, acc: 0.625)
[2025-01-06 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:30][root][INFO] - Training Epoch: 1/10, step 265/574 completed (loss: 2.0578370094299316, acc: 0.4399999976158142)
[2025-01-06 01:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:30][root][INFO] - Training Epoch: 1/10, step 266/574 completed (loss: 1.9848097562789917, acc: 0.550561785697937)
[2025-01-06 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:31][root][INFO] - Training Epoch: 1/10, step 267/574 completed (loss: 1.7886708974838257, acc: 0.4864864945411682)
[2025-01-06 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:31][root][INFO] - Training Epoch: 1/10, step 268/574 completed (loss: 1.342010259628296, acc: 0.568965494632721)
[2025-01-06 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32][root][INFO] - Training Epoch: 1/10, step 269/574 completed (loss: 0.4594060182571411, acc: 0.8636363744735718)
[2025-01-06 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32][root][INFO] - Training Epoch: 1/10, step 270/574 completed (loss: 0.38095492124557495, acc: 0.8636363744735718)
[2025-01-06 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32][root][INFO] - Training Epoch: 1/10, step 271/574 completed (loss: 0.5395060181617737, acc: 0.875)
[2025-01-06 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:33][root][INFO] - Training Epoch: 1/10, step 272/574 completed (loss: 0.2678928077220917, acc: 0.9333333373069763)
[2025-01-06 01:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:33][root][INFO] - Training Epoch: 1/10, step 273/574 completed (loss: 0.6038561463356018, acc: 0.9166666865348816)
[2025-01-06 01:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34][root][INFO] - Training Epoch: 1/10, step 274/574 completed (loss: 0.7103723287582397, acc: 0.78125)
[2025-01-06 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34][root][INFO] - Training Epoch: 1/10, step 275/574 completed (loss: 0.8151403665542603, acc: 0.8666666746139526)
[2025-01-06 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34][root][INFO] - Training Epoch: 1/10, step 276/574 completed (loss: 0.8275508880615234, acc: 0.7931034564971924)
[2025-01-06 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35][root][INFO] - Training Epoch: 1/10, step 277/574 completed (loss: 0.5819392204284668, acc: 0.8399999737739563)
[2025-01-06 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35][root][INFO] - Training Epoch: 1/10, step 278/574 completed (loss: 1.18849778175354, acc: 0.7446808218955994)
[2025-01-06 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35][root][INFO] - Training Epoch: 1/10, step 279/574 completed (loss: 0.8837224841117859, acc: 0.7708333134651184)
[2025-01-06 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36][root][INFO] - Training Epoch: 1/10, step 280/574 completed (loss: 0.5034644603729248, acc: 0.8636363744735718)
[2025-01-06 01:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36][root][INFO] - Training Epoch: 1/10, step 281/574 completed (loss: 1.4269325733184814, acc: 0.6265060305595398)
[2025-01-06 01:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36][root][INFO] - Training Epoch: 1/10, step 282/574 completed (loss: 1.5381371974945068, acc: 0.6018518805503845)
[2025-01-06 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37][root][INFO] - Training Epoch: 1/10, step 283/574 completed (loss: 0.4800514280796051, acc: 0.8684210777282715)
[2025-01-06 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37][root][INFO] - Training Epoch: 1/10, step 284/574 completed (loss: 0.829131543636322, acc: 0.6764705777168274)
[2025-01-06 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:38][root][INFO] - Training Epoch: 1/10, step 285/574 completed (loss: 0.524396538734436, acc: 0.8999999761581421)
[2025-01-06 01:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:08][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4780, device='cuda:0') eval_epoch_loss=tensor(0.9075, device='cuda:0') eval_epoch_acc=tensor(0.7637, device='cuda:0')
[2025-01-06 01:03:08][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:03:08][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:03:08][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_286_loss_0.9074683785438538/model.pt
[2025-01-06 01:03:08][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:03:08][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.9074683785438538
[2025-01-06 01:03:08][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7636511325836182
[2025-01-06 01:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:09][root][INFO] - Training Epoch: 1/10, step 286/574 completed (loss: 0.800575852394104, acc: 0.8046875)
[2025-01-06 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:09][root][INFO] - Training Epoch: 1/10, step 287/574 completed (loss: 1.2071130275726318, acc: 0.6880000233650208)
[2025-01-06 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:09][root][INFO] - Training Epoch: 1/10, step 288/574 completed (loss: 1.182611346244812, acc: 0.7472527623176575)
[2025-01-06 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10][root][INFO] - Training Epoch: 1/10, step 289/574 completed (loss: 1.1689751148223877, acc: 0.7453415989875793)
[2025-01-06 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10][root][INFO] - Training Epoch: 1/10, step 290/574 completed (loss: 1.1326242685317993, acc: 0.7474226951599121)
[2025-01-06 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10][root][INFO] - Training Epoch: 1/10, step 291/574 completed (loss: 0.6020389199256897, acc: 0.7727272510528564)
[2025-01-06 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11][root][INFO] - Training Epoch: 1/10, step 292/574 completed (loss: 1.2851964235305786, acc: 0.6190476417541504)
[2025-01-06 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11][root][INFO] - Training Epoch: 1/10, step 293/574 completed (loss: 0.980122983455658, acc: 0.7758620977401733)
[2025-01-06 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:12][root][INFO] - Training Epoch: 1/10, step 294/574 completed (loss: 0.9006593823432922, acc: 0.6909090876579285)
[2025-01-06 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:12][root][INFO] - Training Epoch: 1/10, step 295/574 completed (loss: 0.9171807169914246, acc: 0.7525773048400879)
[2025-01-06 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13][root][INFO] - Training Epoch: 1/10, step 296/574 completed (loss: 1.0033504962921143, acc: 0.7413793206214905)
[2025-01-06 01:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13][root][INFO] - Training Epoch: 1/10, step 297/574 completed (loss: 0.617792010307312, acc: 0.8148148059844971)
[2025-01-06 01:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13][root][INFO] - Training Epoch: 1/10, step 298/574 completed (loss: 1.1562755107879639, acc: 0.6842105388641357)
[2025-01-06 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14][root][INFO] - Training Epoch: 1/10, step 299/574 completed (loss: 0.4605979025363922, acc: 0.875)
[2025-01-06 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14][root][INFO] - Training Epoch: 1/10, step 300/574 completed (loss: 0.32380232214927673, acc: 0.875)
[2025-01-06 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15][root][INFO] - Training Epoch: 1/10, step 301/574 completed (loss: 0.8154354691505432, acc: 0.7735849022865295)
[2025-01-06 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15][root][INFO] - Training Epoch: 1/10, step 302/574 completed (loss: 0.4464772641658783, acc: 0.9056603908538818)
[2025-01-06 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15][root][INFO] - Training Epoch: 1/10, step 303/574 completed (loss: 0.25509384274482727, acc: 0.9117646813392639)
[2025-01-06 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16][root][INFO] - Training Epoch: 1/10, step 304/574 completed (loss: 0.6133726835250854, acc: 0.75)
[2025-01-06 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16][root][INFO] - Training Epoch: 1/10, step 305/574 completed (loss: 0.9109714031219482, acc: 0.7540983557701111)
[2025-01-06 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16][root][INFO] - Training Epoch: 1/10, step 306/574 completed (loss: 0.9649522304534912, acc: 0.800000011920929)
[2025-01-06 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17][root][INFO] - Training Epoch: 1/10, step 307/574 completed (loss: 0.49676308035850525, acc: 0.9473684430122375)
[2025-01-06 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17][root][INFO] - Training Epoch: 1/10, step 308/574 completed (loss: 0.8945132493972778, acc: 0.7246376872062683)
[2025-01-06 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18][root][INFO] - Training Epoch: 1/10, step 309/574 completed (loss: 0.9551562070846558, acc: 0.7638888955116272)
[2025-01-06 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18][root][INFO] - Training Epoch: 1/10, step 310/574 completed (loss: 0.9806303381919861, acc: 0.7228915691375732)
[2025-01-06 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18][root][INFO] - Training Epoch: 1/10, step 311/574 completed (loss: 0.851824939250946, acc: 0.7564102411270142)
[2025-01-06 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19][root][INFO] - Training Epoch: 1/10, step 312/574 completed (loss: 0.4736308157444, acc: 0.8877550959587097)
[2025-01-06 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19][root][INFO] - Training Epoch: 1/10, step 313/574 completed (loss: 0.2748422920703888, acc: 0.8333333134651184)
[2025-01-06 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20][root][INFO] - Training Epoch: 1/10, step 314/574 completed (loss: 0.2949470579624176, acc: 0.8333333134651184)
[2025-01-06 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20][root][INFO] - Training Epoch: 1/10, step 315/574 completed (loss: 0.41140976548194885, acc: 0.9032257795333862)
[2025-01-06 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20][root][INFO] - Training Epoch: 1/10, step 316/574 completed (loss: 1.5117298364639282, acc: 0.7096773982048035)
[2025-01-06 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21][root][INFO] - Training Epoch: 1/10, step 317/574 completed (loss: 0.7789700627326965, acc: 0.8358209133148193)
[2025-01-06 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21][root][INFO] - Training Epoch: 1/10, step 318/574 completed (loss: 0.2856273353099823, acc: 0.932692289352417)
[2025-01-06 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21][root][INFO] - Training Epoch: 1/10, step 319/574 completed (loss: 0.4508999288082123, acc: 0.8666666746139526)
[2025-01-06 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22][root][INFO] - Training Epoch: 1/10, step 320/574 completed (loss: 0.4137743413448334, acc: 0.9032257795333862)
[2025-01-06 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22][root][INFO] - Training Epoch: 1/10, step 321/574 completed (loss: 0.3533754348754883, acc: 0.9399999976158142)
[2025-01-06 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23][root][INFO] - Training Epoch: 1/10, step 322/574 completed (loss: 1.9104934930801392, acc: 0.48148149251937866)
[2025-01-06 01:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23][root][INFO] - Training Epoch: 1/10, step 323/574 completed (loss: 2.4777448177337646, acc: 0.37142857909202576)
[2025-01-06 01:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23][root][INFO] - Training Epoch: 1/10, step 324/574 completed (loss: 2.5568830966949463, acc: 0.4615384638309479)
[2025-01-06 01:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24][root][INFO] - Training Epoch: 1/10, step 325/574 completed (loss: 2.0951898097991943, acc: 0.46341463923454285)
[2025-01-06 01:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24][root][INFO] - Training Epoch: 1/10, step 326/574 completed (loss: 2.2164864540100098, acc: 0.3947368562221527)
[2025-01-06 01:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24][root][INFO] - Training Epoch: 1/10, step 327/574 completed (loss: 0.992021381855011, acc: 0.7368420958518982)
[2025-01-06 01:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25][root][INFO] - Training Epoch: 1/10, step 328/574 completed (loss: 0.4220721423625946, acc: 0.8928571343421936)
[2025-01-06 01:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25][root][INFO] - Training Epoch: 1/10, step 329/574 completed (loss: 0.4945085644721985, acc: 0.8888888955116272)
[2025-01-06 01:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25][root][INFO] - Training Epoch: 1/10, step 330/574 completed (loss: 0.27728354930877686, acc: 0.90625)
[2025-01-06 01:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26][root][INFO] - Training Epoch: 1/10, step 331/574 completed (loss: 0.5294846296310425, acc: 0.8709677457809448)
[2025-01-06 01:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26][root][INFO] - Training Epoch: 1/10, step 332/574 completed (loss: 0.5554423928260803, acc: 0.8771929740905762)
[2025-01-06 01:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26][root][INFO] - Training Epoch: 1/10, step 333/574 completed (loss: 0.7267000079154968, acc: 0.78125)
[2025-01-06 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27][root][INFO] - Training Epoch: 1/10, step 334/574 completed (loss: 0.4007596969604492, acc: 0.8999999761581421)
[2025-01-06 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27][root][INFO] - Training Epoch: 1/10, step 335/574 completed (loss: 0.9927200675010681, acc: 0.7368420958518982)
[2025-01-06 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:28][root][INFO] - Training Epoch: 1/10, step 336/574 completed (loss: 1.3236831426620483, acc: 0.6399999856948853)
[2025-01-06 01:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:28][root][INFO] - Training Epoch: 1/10, step 337/574 completed (loss: 1.7609409093856812, acc: 0.5747126340866089)
[2025-01-06 01:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29][root][INFO] - Training Epoch: 1/10, step 338/574 completed (loss: 1.782442569732666, acc: 0.521276593208313)
[2025-01-06 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29][root][INFO] - Training Epoch: 1/10, step 339/574 completed (loss: 1.7319250106811523, acc: 0.5542168617248535)
[2025-01-06 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29][root][INFO] - Training Epoch: 1/10, step 340/574 completed (loss: 0.4994446635246277, acc: 0.8260869383811951)
[2025-01-06 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:30][root][INFO] - Training Epoch: 1/10, step 341/574 completed (loss: 1.0789324045181274, acc: 0.7948718070983887)
[2025-01-06 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:30][root][INFO] - Training Epoch: 1/10, step 342/574 completed (loss: 0.9572851061820984, acc: 0.7228915691375732)
[2025-01-06 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31][root][INFO] - Training Epoch: 1/10, step 343/574 completed (loss: 1.1073215007781982, acc: 0.698113203048706)
[2025-01-06 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31][root][INFO] - Training Epoch: 1/10, step 344/574 completed (loss: 0.40788283944129944, acc: 0.8860759735107422)
[2025-01-06 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31][root][INFO] - Training Epoch: 1/10, step 345/574 completed (loss: 0.2984617352485657, acc: 0.9215686321258545)
[2025-01-06 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32][root][INFO] - Training Epoch: 1/10, step 346/574 completed (loss: 1.2375860214233398, acc: 0.6865671873092651)
[2025-01-06 01:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32][root][INFO] - Training Epoch: 1/10, step 347/574 completed (loss: 0.43967288732528687, acc: 0.8999999761581421)
[2025-01-06 01:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32][root][INFO] - Training Epoch: 1/10, step 348/574 completed (loss: 1.075378656387329, acc: 0.7200000286102295)
[2025-01-06 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33][root][INFO] - Training Epoch: 1/10, step 349/574 completed (loss: 1.3324178457260132, acc: 0.7222222089767456)
[2025-01-06 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33][root][INFO] - Training Epoch: 1/10, step 350/574 completed (loss: 1.5017699003219604, acc: 0.5116279125213623)
[2025-01-06 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33][root][INFO] - Training Epoch: 1/10, step 351/574 completed (loss: 0.9801742434501648, acc: 0.7179487347602844)
[2025-01-06 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34][root][INFO] - Training Epoch: 1/10, step 352/574 completed (loss: 2.320504665374756, acc: 0.3777777850627899)
[2025-01-06 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34][root][INFO] - Training Epoch: 1/10, step 353/574 completed (loss: 0.33119118213653564, acc: 0.95652174949646)
[2025-01-06 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:35][root][INFO] - Training Epoch: 1/10, step 354/574 completed (loss: 1.048629641532898, acc: 0.7307692170143127)
[2025-01-06 01:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:35][root][INFO] - Training Epoch: 1/10, step 355/574 completed (loss: 1.4434646368026733, acc: 0.5824176073074341)
[2025-01-06 01:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36][root][INFO] - Training Epoch: 1/10, step 356/574 completed (loss: 1.304717779159546, acc: 0.6173912882804871)
[2025-01-06 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36][root][INFO] - Training Epoch: 1/10, step 357/574 completed (loss: 1.1995216608047485, acc: 0.6521739363670349)
[2025-01-06 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36][root][INFO] - Training Epoch: 1/10, step 358/574 completed (loss: 1.0373660326004028, acc: 0.6938775777816772)
[2025-01-06 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37][root][INFO] - Training Epoch: 1/10, step 359/574 completed (loss: 0.1597065031528473, acc: 0.9583333134651184)
[2025-01-06 01:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37][root][INFO] - Training Epoch: 1/10, step 360/574 completed (loss: 0.6347118616104126, acc: 0.7692307829856873)
[2025-01-06 01:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38][root][INFO] - Training Epoch: 1/10, step 361/574 completed (loss: 1.1913514137268066, acc: 0.6585366129875183)
[2025-01-06 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38][root][INFO] - Training Epoch: 1/10, step 362/574 completed (loss: 0.8908540606498718, acc: 0.8444444537162781)
[2025-01-06 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38][root][INFO] - Training Epoch: 1/10, step 363/574 completed (loss: 0.6556486487388611, acc: 0.8421052694320679)
[2025-01-06 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39][root][INFO] - Training Epoch: 1/10, step 364/574 completed (loss: 0.4499427080154419, acc: 0.9024389982223511)
[2025-01-06 01:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39][root][INFO] - Training Epoch: 1/10, step 365/574 completed (loss: 0.6365296244621277, acc: 0.7575757503509521)
[2025-01-06 01:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39][root][INFO] - Training Epoch: 1/10, step 366/574 completed (loss: 0.1341606229543686, acc: 0.9583333134651184)
[2025-01-06 01:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:40][root][INFO] - Training Epoch: 1/10, step 367/574 completed (loss: 0.7416619062423706, acc: 0.739130437374115)
[2025-01-06 01:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:40][root][INFO] - Training Epoch: 1/10, step 368/574 completed (loss: 0.42271074652671814, acc: 0.9285714030265808)
[2025-01-06 01:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:41][root][INFO] - Training Epoch: 1/10, step 369/574 completed (loss: 1.1527148485183716, acc: 0.78125)
[2025-01-06 01:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:41][root][INFO] - Training Epoch: 1/10, step 370/574 completed (loss: 1.2338299751281738, acc: 0.6424242258071899)
[2025-01-06 01:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:42][root][INFO] - Training Epoch: 1/10, step 371/574 completed (loss: 0.9760611057281494, acc: 0.7547169923782349)
[2025-01-06 01:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43][root][INFO] - Training Epoch: 1/10, step 372/574 completed (loss: 0.4556219279766083, acc: 0.8999999761581421)
[2025-01-06 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43][root][INFO] - Training Epoch: 1/10, step 373/574 completed (loss: 0.6744551658630371, acc: 0.875)
[2025-01-06 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43][root][INFO] - Training Epoch: 1/10, step 374/574 completed (loss: 0.7303482890129089, acc: 0.8571428656578064)
[2025-01-06 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:44][root][INFO] - Training Epoch: 1/10, step 375/574 completed (loss: 0.06871733069419861, acc: 1.0)
[2025-01-06 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:44][root][INFO] - Training Epoch: 1/10, step 376/574 completed (loss: 0.30556172132492065, acc: 0.8695651888847351)
[2025-01-06 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:45][root][INFO] - Training Epoch: 1/10, step 377/574 completed (loss: 0.520036518573761, acc: 0.8958333134651184)
[2025-01-06 01:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:45][root][INFO] - Training Epoch: 1/10, step 378/574 completed (loss: 0.3549460172653198, acc: 0.9052631855010986)
[2025-01-06 01:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:46][root][INFO] - Training Epoch: 1/10, step 379/574 completed (loss: 0.617901623249054, acc: 0.8502994179725647)
[2025-01-06 01:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:46][root][INFO] - Training Epoch: 1/10, step 380/574 completed (loss: 0.7491542100906372, acc: 0.8045112490653992)
[2025-01-06 01:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:47][root][INFO] - Training Epoch: 1/10, step 381/574 completed (loss: 1.1627987623214722, acc: 0.6951871514320374)
[2025-01-06 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48][root][INFO] - Training Epoch: 1/10, step 382/574 completed (loss: 0.5621185898780823, acc: 0.8738738894462585)
[2025-01-06 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48][root][INFO] - Training Epoch: 1/10, step 383/574 completed (loss: 0.9198907017707825, acc: 0.7857142686843872)
[2025-01-06 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49][root][INFO] - Training Epoch: 1/10, step 384/574 completed (loss: 0.130144864320755, acc: 1.0)
[2025-01-06 01:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49][root][INFO] - Training Epoch: 1/10, step 385/574 completed (loss: 0.392193078994751, acc: 0.90625)
[2025-01-06 01:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49][root][INFO] - Training Epoch: 1/10, step 386/574 completed (loss: 0.3417164087295532, acc: 0.9444444179534912)
[2025-01-06 01:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50][root][INFO] - Training Epoch: 1/10, step 387/574 completed (loss: 0.13900156319141388, acc: 0.9736841917037964)
[2025-01-06 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50][root][INFO] - Training Epoch: 1/10, step 388/574 completed (loss: 0.22418004274368286, acc: 0.9090909361839294)
[2025-01-06 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50][root][INFO] - Training Epoch: 1/10, step 389/574 completed (loss: 0.16893219947814941, acc: 0.949999988079071)
[2025-01-06 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51][root][INFO] - Training Epoch: 1/10, step 390/574 completed (loss: 0.9741240739822388, acc: 0.8095238208770752)
[2025-01-06 01:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51][root][INFO] - Training Epoch: 1/10, step 391/574 completed (loss: 1.4040613174438477, acc: 0.5555555820465088)
[2025-01-06 01:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51][root][INFO] - Training Epoch: 1/10, step 392/574 completed (loss: 1.3457921743392944, acc: 0.6796116232872009)
[2025-01-06 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52][root][INFO] - Training Epoch: 1/10, step 393/574 completed (loss: 1.3180418014526367, acc: 0.7132353186607361)
[2025-01-06 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52][root][INFO] - Training Epoch: 1/10, step 394/574 completed (loss: 1.458412766456604, acc: 0.6200000047683716)
[2025-01-06 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53][root][INFO] - Training Epoch: 1/10, step 395/574 completed (loss: 1.2426193952560425, acc: 0.6666666865348816)
[2025-01-06 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53][root][INFO] - Training Epoch: 1/10, step 396/574 completed (loss: 0.8792814016342163, acc: 0.7674418687820435)
[2025-01-06 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53][root][INFO] - Training Epoch: 1/10, step 397/574 completed (loss: 0.5168969035148621, acc: 0.875)
[2025-01-06 01:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54][root][INFO] - Training Epoch: 1/10, step 398/574 completed (loss: 0.6819217801094055, acc: 0.7674418687820435)
[2025-01-06 01:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54][root][INFO] - Training Epoch: 1/10, step 399/574 completed (loss: 0.47838741540908813, acc: 0.8799999952316284)
[2025-01-06 01:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:55][root][INFO] - Training Epoch: 1/10, step 400/574 completed (loss: 0.6825177669525146, acc: 0.7941176295280457)
[2025-01-06 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:55][root][INFO] - Training Epoch: 1/10, step 401/574 completed (loss: 0.6369131207466125, acc: 0.800000011920929)
[2025-01-06 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56][root][INFO] - Training Epoch: 1/10, step 402/574 completed (loss: 1.2276859283447266, acc: 0.6969696879386902)
[2025-01-06 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56][root][INFO] - Training Epoch: 1/10, step 403/574 completed (loss: 0.593711793422699, acc: 0.7575757503509521)
[2025-01-06 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56][root][INFO] - Training Epoch: 1/10, step 404/574 completed (loss: 1.0676870346069336, acc: 0.7419354915618896)
[2025-01-06 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:57][root][INFO] - Training Epoch: 1/10, step 405/574 completed (loss: 0.27686911821365356, acc: 0.8518518805503845)
[2025-01-06 01:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:57][root][INFO] - Training Epoch: 1/10, step 406/574 completed (loss: 0.5479732751846313, acc: 0.8399999737739563)
[2025-01-06 01:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58][root][INFO] - Training Epoch: 1/10, step 407/574 completed (loss: 0.32269275188446045, acc: 0.8888888955116272)
[2025-01-06 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58][root][INFO] - Training Epoch: 1/10, step 408/574 completed (loss: 0.4737546741962433, acc: 0.8888888955116272)
[2025-01-06 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58][root][INFO] - Training Epoch: 1/10, step 409/574 completed (loss: 0.4564135670661926, acc: 0.8846153616905212)
[2025-01-06 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59][root][INFO] - Training Epoch: 1/10, step 410/574 completed (loss: 0.648516058921814, acc: 0.8620689511299133)
[2025-01-06 01:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59][root][INFO] - Training Epoch: 1/10, step 411/574 completed (loss: 0.15739594399929047, acc: 1.0)
[2025-01-06 01:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59][root][INFO] - Training Epoch: 1/10, step 412/574 completed (loss: 0.4289228320121765, acc: 0.8666666746139526)
[2025-01-06 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:00][root][INFO] - Training Epoch: 1/10, step 413/574 completed (loss: 0.6232861280441284, acc: 0.8484848737716675)
[2025-01-06 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:00][root][INFO] - Training Epoch: 1/10, step 414/574 completed (loss: 0.6507843136787415, acc: 0.8636363744735718)
[2025-01-06 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01][root][INFO] - Training Epoch: 1/10, step 415/574 completed (loss: 0.777901291847229, acc: 0.7647058963775635)
[2025-01-06 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01][root][INFO] - Training Epoch: 1/10, step 416/574 completed (loss: 0.6846603155136108, acc: 0.807692289352417)
[2025-01-06 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01][root][INFO] - Training Epoch: 1/10, step 417/574 completed (loss: 0.7815579771995544, acc: 0.7777777910232544)
[2025-01-06 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01][root][INFO] - Training Epoch: 1/10, step 418/574 completed (loss: 0.7046415209770203, acc: 0.800000011920929)
[2025-01-06 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02][root][INFO] - Training Epoch: 1/10, step 419/574 completed (loss: 0.9392013549804688, acc: 0.8500000238418579)
[2025-01-06 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02][root][INFO] - Training Epoch: 1/10, step 420/574 completed (loss: 0.4147797226905823, acc: 0.8571428656578064)
[2025-01-06 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03][root][INFO] - Training Epoch: 1/10, step 421/574 completed (loss: 0.6348479986190796, acc: 0.8333333134651184)
[2025-01-06 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03][root][INFO] - Training Epoch: 1/10, step 422/574 completed (loss: 0.7560218572616577, acc: 0.75)
[2025-01-06 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03][root][INFO] - Training Epoch: 1/10, step 423/574 completed (loss: 1.1663399934768677, acc: 0.7222222089767456)
[2025-01-06 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04][root][INFO] - Training Epoch: 1/10, step 424/574 completed (loss: 0.9293185472488403, acc: 0.8888888955116272)
[2025-01-06 01:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04][root][INFO] - Training Epoch: 1/10, step 425/574 completed (loss: 0.5985555052757263, acc: 0.9090909361839294)
[2025-01-06 01:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04][root][INFO] - Training Epoch: 1/10, step 426/574 completed (loss: 0.3075922429561615, acc: 0.9130434989929199)
[2025-01-06 01:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:05][root][INFO] - Training Epoch: 1/10, step 427/574 completed (loss: 0.45873352885246277, acc: 0.8918918967247009)
[2025-01-06 01:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:05][root][INFO] - Training Epoch: 1/10, step 428/574 completed (loss: 0.5386975407600403, acc: 0.8888888955116272)
[2025-01-06 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2722, device='cuda:0') eval_epoch_loss=tensor(0.8207, device='cuda:0') eval_epoch_acc=tensor(0.7822, device='cuda:0')
[2025-01-06 01:04:37][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:04:37][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:04:37][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_429_loss_0.8207386136054993/model.pt
[2025-01-06 01:04:37][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:04:37][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.8207386136054993
[2025-01-06 01:04:37][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7821862697601318
[2025-01-06 01:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37][root][INFO] - Training Epoch: 1/10, step 429/574 completed (loss: 0.6107479333877563, acc: 0.8695651888847351)
[2025-01-06 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38][root][INFO] - Training Epoch: 1/10, step 430/574 completed (loss: 0.0583699494600296, acc: 1.0)
[2025-01-06 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38][root][INFO] - Training Epoch: 1/10, step 431/574 completed (loss: 0.3080192506313324, acc: 0.9259259104728699)
[2025-01-06 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39][root][INFO] - Training Epoch: 1/10, step 432/574 completed (loss: 0.9462891221046448, acc: 0.782608687877655)
[2025-01-06 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39][root][INFO] - Training Epoch: 1/10, step 433/574 completed (loss: 0.4937931299209595, acc: 0.8888888955116272)
[2025-01-06 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39][root][INFO] - Training Epoch: 1/10, step 434/574 completed (loss: 0.0135536203160882, acc: 1.0)
[2025-01-06 01:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:40][root][INFO] - Training Epoch: 1/10, step 435/574 completed (loss: 0.19751256704330444, acc: 0.939393937587738)
[2025-01-06 01:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:40][root][INFO] - Training Epoch: 1/10, step 436/574 completed (loss: 0.5690749287605286, acc: 0.8611111044883728)
[2025-01-06 01:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41][root][INFO] - Training Epoch: 1/10, step 437/574 completed (loss: 0.43743622303009033, acc: 0.8863636255264282)
[2025-01-06 01:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41][root][INFO] - Training Epoch: 1/10, step 438/574 completed (loss: 0.20373959839344025, acc: 0.9523809552192688)
[2025-01-06 01:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41][root][INFO] - Training Epoch: 1/10, step 439/574 completed (loss: 0.8317444920539856, acc: 0.8205128312110901)
[2025-01-06 01:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:42][root][INFO] - Training Epoch: 1/10, step 440/574 completed (loss: 0.9778807759284973, acc: 0.6969696879386902)
[2025-01-06 01:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43][root][INFO] - Training Epoch: 1/10, step 441/574 completed (loss: 1.385555624961853, acc: 0.6159999966621399)
[2025-01-06 01:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43][root][INFO] - Training Epoch: 1/10, step 442/574 completed (loss: 1.2924500703811646, acc: 0.6451612710952759)
[2025-01-06 01:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44][root][INFO] - Training Epoch: 1/10, step 443/574 completed (loss: 0.9760524034500122, acc: 0.7910447716712952)
[2025-01-06 01:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44][root][INFO] - Training Epoch: 1/10, step 444/574 completed (loss: 0.6218789219856262, acc: 0.7924528121948242)
[2025-01-06 01:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45][root][INFO] - Training Epoch: 1/10, step 445/574 completed (loss: 0.5878549814224243, acc: 0.8636363744735718)
[2025-01-06 01:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45][root][INFO] - Training Epoch: 1/10, step 446/574 completed (loss: 0.7213225364685059, acc: 0.8260869383811951)
[2025-01-06 01:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45][root][INFO] - Training Epoch: 1/10, step 447/574 completed (loss: 1.1609572172164917, acc: 0.7692307829856873)
[2025-01-06 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46][root][INFO] - Training Epoch: 1/10, step 448/574 completed (loss: 0.3881829082965851, acc: 0.9285714030265808)
[2025-01-06 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46][root][INFO] - Training Epoch: 1/10, step 449/574 completed (loss: 0.5998897552490234, acc: 0.8656716346740723)
[2025-01-06 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46][root][INFO] - Training Epoch: 1/10, step 450/574 completed (loss: 0.2890923023223877, acc: 0.9305555820465088)
[2025-01-06 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47][root][INFO] - Training Epoch: 1/10, step 451/574 completed (loss: 0.2511082589626312, acc: 0.9347826242446899)
[2025-01-06 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47][root][INFO] - Training Epoch: 1/10, step 452/574 completed (loss: 0.5718399882316589, acc: 0.8333333134651184)
[2025-01-06 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48][root][INFO] - Training Epoch: 1/10, step 453/574 completed (loss: 0.9038090705871582, acc: 0.8421052694320679)
[2025-01-06 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48][root][INFO] - Training Epoch: 1/10, step 454/574 completed (loss: 0.5114114284515381, acc: 0.8571428656578064)
[2025-01-06 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48][root][INFO] - Training Epoch: 1/10, step 455/574 completed (loss: 0.5477638840675354, acc: 0.8787878751754761)
[2025-01-06 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49][root][INFO] - Training Epoch: 1/10, step 456/574 completed (loss: 0.7635353207588196, acc: 0.8247422575950623)
[2025-01-06 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49][root][INFO] - Training Epoch: 1/10, step 457/574 completed (loss: 0.278919517993927, acc: 0.9428571462631226)
[2025-01-06 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49][root][INFO] - Training Epoch: 1/10, step 458/574 completed (loss: 0.9675696492195129, acc: 0.7558139562606812)
[2025-01-06 01:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50][root][INFO] - Training Epoch: 1/10, step 459/574 completed (loss: 0.2173621654510498, acc: 0.9464285969734192)
[2025-01-06 01:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50][root][INFO] - Training Epoch: 1/10, step 460/574 completed (loss: 0.5489699244499207, acc: 0.8395061492919922)
[2025-01-06 01:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51][root][INFO] - Training Epoch: 1/10, step 461/574 completed (loss: 0.9013494849205017, acc: 0.7222222089767456)
[2025-01-06 01:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51][root][INFO] - Training Epoch: 1/10, step 462/574 completed (loss: 0.4354873597621918, acc: 0.875)
[2025-01-06 01:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51][root][INFO] - Training Epoch: 1/10, step 463/574 completed (loss: 0.5493838787078857, acc: 0.8846153616905212)
[2025-01-06 01:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52][root][INFO] - Training Epoch: 1/10, step 464/574 completed (loss: 0.5402861833572388, acc: 0.782608687877655)
[2025-01-06 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52][root][INFO] - Training Epoch: 1/10, step 465/574 completed (loss: 0.7293288111686707, acc: 0.773809552192688)
[2025-01-06 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52][root][INFO] - Training Epoch: 1/10, step 466/574 completed (loss: 0.8611147999763489, acc: 0.7831325531005859)
[2025-01-06 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53][root][INFO] - Training Epoch: 1/10, step 467/574 completed (loss: 0.5321391820907593, acc: 0.8648648858070374)
[2025-01-06 01:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53][root][INFO] - Training Epoch: 1/10, step 468/574 completed (loss: 1.2532474994659424, acc: 0.708737850189209)
[2025-01-06 01:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53][root][INFO] - Training Epoch: 1/10, step 469/574 completed (loss: 0.8582654595375061, acc: 0.772357702255249)
[2025-01-06 01:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54][root][INFO] - Training Epoch: 1/10, step 470/574 completed (loss: 0.5404624938964844, acc: 0.8333333134651184)
[2025-01-06 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54][root][INFO] - Training Epoch: 1/10, step 471/574 completed (loss: 0.8309752345085144, acc: 0.75)
[2025-01-06 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55][root][INFO] - Training Epoch: 1/10, step 472/574 completed (loss: 1.300202488899231, acc: 0.6078431606292725)
[2025-01-06 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55][root][INFO] - Training Epoch: 1/10, step 473/574 completed (loss: 1.024533748626709, acc: 0.7379912734031677)
[2025-01-06 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55][root][INFO] - Training Epoch: 1/10, step 474/574 completed (loss: 0.9437771439552307, acc: 0.7395833134651184)
[2025-01-06 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56][root][INFO] - Training Epoch: 1/10, step 475/574 completed (loss: 0.6173370480537415, acc: 0.8098159432411194)
[2025-01-06 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56][root][INFO] - Training Epoch: 1/10, step 476/574 completed (loss: 0.6626073718070984, acc: 0.8201438784599304)
[2025-01-06 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57][root][INFO] - Training Epoch: 1/10, step 477/574 completed (loss: 1.1828337907791138, acc: 0.6733668446540833)
[2025-01-06 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57][root][INFO] - Training Epoch: 1/10, step 478/574 completed (loss: 0.9856246113777161, acc: 0.75)
[2025-01-06 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57][root][INFO] - Training Epoch: 1/10, step 479/574 completed (loss: 1.298697590827942, acc: 0.6363636255264282)
[2025-01-06 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58][root][INFO] - Training Epoch: 1/10, step 480/574 completed (loss: 1.0767120122909546, acc: 0.7037037014961243)
[2025-01-06 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58][root][INFO] - Training Epoch: 1/10, step 481/574 completed (loss: 1.096494436264038, acc: 0.699999988079071)
[2025-01-06 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59][root][INFO] - Training Epoch: 1/10, step 482/574 completed (loss: 1.3855215311050415, acc: 0.6499999761581421)
[2025-01-06 01:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59][root][INFO] - Training Epoch: 1/10, step 483/574 completed (loss: 1.3720108270645142, acc: 0.517241358757019)
[2025-01-06 01:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59][root][INFO] - Training Epoch: 1/10, step 484/574 completed (loss: 0.34149572253227234, acc: 0.9032257795333862)
[2025-01-06 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00][root][INFO] - Training Epoch: 1/10, step 485/574 completed (loss: 1.070867657661438, acc: 0.7894737124443054)
[2025-01-06 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00][root][INFO] - Training Epoch: 1/10, step 486/574 completed (loss: 1.9896806478500366, acc: 0.37037035822868347)
[2025-01-06 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00][root][INFO] - Training Epoch: 1/10, step 487/574 completed (loss: 0.9430075287818909, acc: 0.5714285969734192)
[2025-01-06 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01][root][INFO] - Training Epoch: 1/10, step 488/574 completed (loss: 1.364682912826538, acc: 0.7272727489471436)
[2025-01-06 01:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01][root][INFO] - Training Epoch: 1/10, step 489/574 completed (loss: 1.380945086479187, acc: 0.6307692527770996)
[2025-01-06 01:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02][root][INFO] - Training Epoch: 1/10, step 490/574 completed (loss: 0.748026430606842, acc: 0.8333333134651184)
[2025-01-06 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02][root][INFO] - Training Epoch: 1/10, step 491/574 completed (loss: 0.8464487195014954, acc: 0.8275862336158752)
[2025-01-06 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02][root][INFO] - Training Epoch: 1/10, step 492/574 completed (loss: 0.8348668217658997, acc: 0.7058823704719543)
[2025-01-06 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03][root][INFO] - Training Epoch: 1/10, step 493/574 completed (loss: 0.7297345399856567, acc: 0.7931034564971924)
[2025-01-06 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03][root][INFO] - Training Epoch: 1/10, step 494/574 completed (loss: 0.8753478527069092, acc: 0.7894737124443054)
[2025-01-06 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03][root][INFO] - Training Epoch: 1/10, step 495/574 completed (loss: 1.0834988355636597, acc: 0.7368420958518982)
[2025-01-06 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04][root][INFO] - Training Epoch: 1/10, step 496/574 completed (loss: 1.034338355064392, acc: 0.7232142686843872)
[2025-01-06 01:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04][root][INFO] - Training Epoch: 1/10, step 497/574 completed (loss: 0.7710062265396118, acc: 0.7977527976036072)
[2025-01-06 01:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04][root][INFO] - Training Epoch: 1/10, step 498/574 completed (loss: 1.0848405361175537, acc: 0.6853932738304138)
[2025-01-06 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:05][root][INFO] - Training Epoch: 1/10, step 499/574 completed (loss: 1.717499017715454, acc: 0.5390070676803589)
[2025-01-06 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:05][root][INFO] - Training Epoch: 1/10, step 500/574 completed (loss: 1.2562012672424316, acc: 0.739130437374115)
[2025-01-06 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06][root][INFO] - Training Epoch: 1/10, step 501/574 completed (loss: 0.14643943309783936, acc: 1.0)
[2025-01-06 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06][root][INFO] - Training Epoch: 1/10, step 502/574 completed (loss: 0.34026798605918884, acc: 0.8846153616905212)
[2025-01-06 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06][root][INFO] - Training Epoch: 1/10, step 503/574 completed (loss: 0.6445216536521912, acc: 0.7777777910232544)
[2025-01-06 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07][root][INFO] - Training Epoch: 1/10, step 504/574 completed (loss: 0.49842390418052673, acc: 0.8518518805503845)
[2025-01-06 01:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07][root][INFO] - Training Epoch: 1/10, step 505/574 completed (loss: 0.8640716671943665, acc: 0.8301886916160583)
[2025-01-06 01:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07][root][INFO] - Training Epoch: 1/10, step 506/574 completed (loss: 0.6573166251182556, acc: 0.8620689511299133)
[2025-01-06 01:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:08][root][INFO] - Training Epoch: 1/10, step 507/574 completed (loss: 1.6458792686462402, acc: 0.5495495200157166)
[2025-01-06 01:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:08][root][INFO] - Training Epoch: 1/10, step 508/574 completed (loss: 1.0776070356369019, acc: 0.7605633735656738)
[2025-01-06 01:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:09][root][INFO] - Training Epoch: 1/10, step 509/574 completed (loss: 0.42100539803504944, acc: 0.8500000238418579)
[2025-01-06 01:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:09][root][INFO] - Training Epoch: 1/10, step 510/574 completed (loss: 0.7084566950798035, acc: 0.7333333492279053)
[2025-01-06 01:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:09][root][INFO] - Training Epoch: 1/10, step 511/574 completed (loss: 0.909852147102356, acc: 0.7307692170143127)
[2025-01-06 01:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:12][root][INFO] - Training Epoch: 1/10, step 512/574 completed (loss: 1.7807687520980835, acc: 0.5714285969734192)
[2025-01-06 01:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13][root][INFO] - Training Epoch: 1/10, step 513/574 completed (loss: 0.7448337078094482, acc: 0.8333333134651184)
[2025-01-06 01:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13][root][INFO] - Training Epoch: 1/10, step 514/574 completed (loss: 0.982801616191864, acc: 0.7857142686843872)
[2025-01-06 01:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:14][root][INFO] - Training Epoch: 1/10, step 515/574 completed (loss: 0.32773035764694214, acc: 0.8999999761581421)
[2025-01-06 01:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:14][root][INFO] - Training Epoch: 1/10, step 516/574 completed (loss: 0.8477798700332642, acc: 0.7916666865348816)
[2025-01-06 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15][root][INFO] - Training Epoch: 1/10, step 517/574 completed (loss: 0.08881428837776184, acc: 0.9615384340286255)
[2025-01-06 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15][root][INFO] - Training Epoch: 1/10, step 518/574 completed (loss: 0.4121459424495697, acc: 0.8387096524238586)
[2025-01-06 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15][root][INFO] - Training Epoch: 1/10, step 519/574 completed (loss: 0.5745356678962708, acc: 0.800000011920929)
[2025-01-06 01:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:16][root][INFO] - Training Epoch: 1/10, step 520/574 completed (loss: 0.6283704042434692, acc: 0.8148148059844971)
[2025-01-06 01:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17][root][INFO] - Training Epoch: 1/10, step 521/574 completed (loss: 0.9966349005699158, acc: 0.7372881174087524)
[2025-01-06 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17][root][INFO] - Training Epoch: 1/10, step 522/574 completed (loss: 0.5043137669563293, acc: 0.8507462739944458)
[2025-01-06 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:18][root][INFO] - Training Epoch: 1/10, step 523/574 completed (loss: 0.5579720735549927, acc: 0.8029196858406067)
[2025-01-06 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:18][root][INFO] - Training Epoch: 1/10, step 524/574 completed (loss: 1.038009524345398, acc: 0.6899999976158142)
[2025-01-06 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19][root][INFO] - Training Epoch: 1/10, step 525/574 completed (loss: 0.08338426798582077, acc: 1.0)
[2025-01-06 01:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19][root][INFO] - Training Epoch: 1/10, step 526/574 completed (loss: 0.3480377793312073, acc: 0.8846153616905212)
[2025-01-06 01:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19][root][INFO] - Training Epoch: 1/10, step 527/574 completed (loss: 0.6482868194580078, acc: 0.8571428656578064)
[2025-01-06 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20][root][INFO] - Training Epoch: 1/10, step 528/574 completed (loss: 2.339184284210205, acc: 0.4262295067310333)
[2025-01-06 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20][root][INFO] - Training Epoch: 1/10, step 529/574 completed (loss: 0.5050150752067566, acc: 0.7966101765632629)
[2025-01-06 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20][root][INFO] - Training Epoch: 1/10, step 530/574 completed (loss: 1.8235059976577759, acc: 0.5581395626068115)
[2025-01-06 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:21][root][INFO] - Training Epoch: 1/10, step 531/574 completed (loss: 1.4658379554748535, acc: 0.6590909361839294)
[2025-01-06 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:21][root][INFO] - Training Epoch: 1/10, step 532/574 completed (loss: 1.544353723526001, acc: 0.6037735939025879)
[2025-01-06 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22][root][INFO] - Training Epoch: 1/10, step 533/574 completed (loss: 1.3673487901687622, acc: 0.6818181872367859)
[2025-01-06 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22][root][INFO] - Training Epoch: 1/10, step 534/574 completed (loss: 1.0020240545272827, acc: 0.7599999904632568)
[2025-01-06 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22][root][INFO] - Training Epoch: 1/10, step 535/574 completed (loss: 0.8914302587509155, acc: 0.800000011920929)
[2025-01-06 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23][root][INFO] - Training Epoch: 1/10, step 536/574 completed (loss: 0.5468420386314392, acc: 0.8181818127632141)
[2025-01-06 01:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23][root][INFO] - Training Epoch: 1/10, step 537/574 completed (loss: 0.9660962820053101, acc: 0.7692307829856873)
[2025-01-06 01:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24][root][INFO] - Training Epoch: 1/10, step 538/574 completed (loss: 0.9848487377166748, acc: 0.75)
[2025-01-06 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24][root][INFO] - Training Epoch: 1/10, step 539/574 completed (loss: 0.8472231030464172, acc: 0.75)
[2025-01-06 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24][root][INFO] - Training Epoch: 1/10, step 540/574 completed (loss: 1.2104729413986206, acc: 0.6666666865348816)
[2025-01-06 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25][root][INFO] - Training Epoch: 1/10, step 541/574 completed (loss: 0.7870060801506042, acc: 0.6875)
[2025-01-06 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25][root][INFO] - Training Epoch: 1/10, step 542/574 completed (loss: 0.18602575361728668, acc: 0.9354838728904724)
[2025-01-06 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25][root][INFO] - Training Epoch: 1/10, step 543/574 completed (loss: 0.32974886894226074, acc: 0.95652174949646)
[2025-01-06 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26][root][INFO] - Training Epoch: 1/10, step 544/574 completed (loss: 0.38916221261024475, acc: 0.8999999761581421)
[2025-01-06 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26][root][INFO] - Training Epoch: 1/10, step 545/574 completed (loss: 0.30903303623199463, acc: 0.8780487775802612)
[2025-01-06 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26][root][INFO] - Training Epoch: 1/10, step 546/574 completed (loss: 0.04746584966778755, acc: 0.9714285731315613)
[2025-01-06 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27][root][INFO] - Training Epoch: 1/10, step 547/574 completed (loss: 0.10361748933792114, acc: 0.9736841917037964)
[2025-01-06 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27][root][INFO] - Training Epoch: 1/10, step 548/574 completed (loss: 0.7043358683586121, acc: 0.8064516186714172)
[2025-01-06 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28][root][INFO] - Training Epoch: 1/10, step 549/574 completed (loss: 0.07201486825942993, acc: 1.0)
[2025-01-06 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28][root][INFO] - Training Epoch: 1/10, step 550/574 completed (loss: 0.5123323798179626, acc: 0.9090909361839294)
[2025-01-06 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28][root][INFO] - Training Epoch: 1/10, step 551/574 completed (loss: 0.3498573899269104, acc: 0.8999999761581421)
[2025-01-06 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29][root][INFO] - Training Epoch: 1/10, step 552/574 completed (loss: 0.40772366523742676, acc: 0.8714285492897034)
[2025-01-06 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29][root][INFO] - Training Epoch: 1/10, step 553/574 completed (loss: 0.6664871573448181, acc: 0.8321167826652527)
[2025-01-06 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29][root][INFO] - Training Epoch: 1/10, step 554/574 completed (loss: 0.55913907289505, acc: 0.8413792848587036)
[2025-01-06 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:30][root][INFO] - Training Epoch: 1/10, step 555/574 completed (loss: 0.6280606985092163, acc: 0.8428571224212646)
[2025-01-06 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:30][root][INFO] - Training Epoch: 1/10, step 556/574 completed (loss: 0.6382216215133667, acc: 0.8344370722770691)
[2025-01-06 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31][root][INFO] - Training Epoch: 1/10, step 557/574 completed (loss: 0.5511949062347412, acc: 0.8803418874740601)
[2025-01-06 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31][root][INFO] - Training Epoch: 1/10, step 558/574 completed (loss: 0.32688361406326294, acc: 0.8799999952316284)
[2025-01-06 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31][root][INFO] - Training Epoch: 1/10, step 559/574 completed (loss: 0.6946844458580017, acc: 0.8461538553237915)
[2025-01-06 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32][root][INFO] - Training Epoch: 1/10, step 560/574 completed (loss: 0.22296521067619324, acc: 0.9230769276618958)
[2025-01-06 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32][root][INFO] - Training Epoch: 1/10, step 561/574 completed (loss: 0.2832656800746918, acc: 0.9487179517745972)
[2025-01-06 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32][root][INFO] - Training Epoch: 1/10, step 562/574 completed (loss: 0.791174054145813, acc: 0.8333333134651184)
[2025-01-06 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33][root][INFO] - Training Epoch: 1/10, step 563/574 completed (loss: 0.497627854347229, acc: 0.8831169009208679)
[2025-01-06 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33][root][INFO] - Training Epoch: 1/10, step 564/574 completed (loss: 0.6823926568031311, acc: 0.8125)
[2025-01-06 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33][root][INFO] - Training Epoch: 1/10, step 565/574 completed (loss: 0.42319729924201965, acc: 0.8448275923728943)
[2025-01-06 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:34][root][INFO] - Training Epoch: 1/10, step 566/574 completed (loss: 0.6059696078300476, acc: 0.8809523582458496)
[2025-01-06 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:34][root][INFO] - Training Epoch: 1/10, step 567/574 completed (loss: 0.08468832075595856, acc: 0.9736841917037964)
[2025-01-06 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35][root][INFO] - Training Epoch: 1/10, step 568/574 completed (loss: 0.23136001825332642, acc: 0.8888888955116272)
[2025-01-06 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35][root][INFO] - Training Epoch: 1/10, step 569/574 completed (loss: 0.4034912884235382, acc: 0.8823529481887817)
[2025-01-06 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35][root][INFO] - Training Epoch: 1/10, step 570/574 completed (loss: 0.07803592085838318, acc: 0.9677419066429138)
[2025-01-06 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:36][root][INFO] - Training Epoch: 1/10, step 571/574 completed (loss: 0.6019605994224548, acc: 0.8547008633613586)
[2025-01-06 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9956, device='cuda:0') eval_epoch_loss=tensor(0.6909, device='cuda:0') eval_epoch_acc=tensor(0.8103, device='cuda:0')
[2025-01-06 01:06:06][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:06:06][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:06:07][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_572_loss_0.6909285187721252/model.pt
[2025-01-06 01:06:07][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:06:07][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6909285187721252
[2025-01-06 01:06:07][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8103269934654236
[2025-01-06 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07][root][INFO] - Training Epoch: 1/10, step 572/574 completed (loss: 0.5614253282546997, acc: 0.8367347121238708)
[2025-01-06 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07][root][INFO] - Training Epoch: 1/10, step 573/574 completed (loss: 0.5300466418266296, acc: 0.8427672982215881)
[2025-01-06 01:06:08][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=3.8630, train_epoch_loss=1.3514, epoch time 374.95060996711254s
[2025-01-06 01:06:08][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-06 01:06:08][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:06:08][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-06 01:06:08][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2025-01-06 01:06:08][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09][root][INFO] - Training Epoch: 2/10, step 0/574 completed (loss: 0.9133339524269104, acc: 0.7037037014961243)
[2025-01-06 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09][root][INFO] - Training Epoch: 2/10, step 1/574 completed (loss: 0.6777719855308533, acc: 0.800000011920929)
[2025-01-06 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09][root][INFO] - Training Epoch: 2/10, step 2/574 completed (loss: 0.9455122947692871, acc: 0.7297297120094299)
[2025-01-06 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10][root][INFO] - Training Epoch: 2/10, step 3/574 completed (loss: 0.8657743334770203, acc: 0.7894737124443054)
[2025-01-06 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10][root][INFO] - Training Epoch: 2/10, step 4/574 completed (loss: 1.0268062353134155, acc: 0.7837837934494019)
[2025-01-06 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10][root][INFO] - Training Epoch: 2/10, step 5/574 completed (loss: 0.9393935799598694, acc: 0.75)
[2025-01-06 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11][root][INFO] - Training Epoch: 2/10, step 6/574 completed (loss: 1.426051139831543, acc: 0.5714285969734192)
[2025-01-06 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11][root][INFO] - Training Epoch: 2/10, step 7/574 completed (loss: 0.9144074320793152, acc: 0.800000011920929)
[2025-01-06 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11][root][INFO] - Training Epoch: 2/10, step 8/574 completed (loss: 0.32294389605522156, acc: 0.8636363744735718)
[2025-01-06 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12][root][INFO] - Training Epoch: 2/10, step 9/574 completed (loss: 0.1564411073923111, acc: 0.9615384340286255)
[2025-01-06 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12][root][INFO] - Training Epoch: 2/10, step 10/574 completed (loss: 0.28796252608299255, acc: 0.9259259104728699)
[2025-01-06 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13][root][INFO] - Training Epoch: 2/10, step 11/574 completed (loss: 0.6557880640029907, acc: 0.8461538553237915)
[2025-01-06 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13][root][INFO] - Training Epoch: 2/10, step 12/574 completed (loss: 0.20382533967494965, acc: 0.939393937587738)
[2025-01-06 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13][root][INFO] - Training Epoch: 2/10, step 13/574 completed (loss: 0.4493919909000397, acc: 0.804347813129425)
[2025-01-06 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14][root][INFO] - Training Epoch: 2/10, step 14/574 completed (loss: 0.4013718366622925, acc: 0.8823529481887817)
[2025-01-06 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14][root][INFO] - Training Epoch: 2/10, step 15/574 completed (loss: 0.6352143883705139, acc: 0.8571428656578064)
[2025-01-06 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14][root][INFO] - Training Epoch: 2/10, step 16/574 completed (loss: 0.3341027498245239, acc: 0.8947368264198303)
[2025-01-06 01:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:15][root][INFO] - Training Epoch: 2/10, step 17/574 completed (loss: 0.884286642074585, acc: 0.75)
[2025-01-06 01:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:15][root][INFO] - Training Epoch: 2/10, step 18/574 completed (loss: 1.2267910242080688, acc: 0.6944444179534912)
[2025-01-06 01:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16][root][INFO] - Training Epoch: 2/10, step 19/574 completed (loss: 0.7219353318214417, acc: 0.8421052694320679)
[2025-01-06 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16][root][INFO] - Training Epoch: 2/10, step 20/574 completed (loss: 0.5476475358009338, acc: 0.8461538553237915)
[2025-01-06 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16][root][INFO] - Training Epoch: 2/10, step 21/574 completed (loss: 0.8605398535728455, acc: 0.7931034564971924)
[2025-01-06 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17][root][INFO] - Training Epoch: 2/10, step 22/574 completed (loss: 1.3240591287612915, acc: 0.5199999809265137)
[2025-01-06 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17][root][INFO] - Training Epoch: 2/10, step 23/574 completed (loss: 1.0482147932052612, acc: 0.761904776096344)
[2025-01-06 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17][root][INFO] - Training Epoch: 2/10, step 24/574 completed (loss: 0.243309885263443, acc: 0.9375)
[2025-01-06 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18][root][INFO] - Training Epoch: 2/10, step 25/574 completed (loss: 1.0673270225524902, acc: 0.7547169923782349)
[2025-01-06 01:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18][root][INFO] - Training Epoch: 2/10, step 26/574 completed (loss: 1.1473195552825928, acc: 0.6712328791618347)
[2025-01-06 01:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:19][root][INFO] - Training Epoch: 2/10, step 27/574 completed (loss: 1.2405128479003906, acc: 0.6640316247940063)
[2025-01-06 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20][root][INFO] - Training Epoch: 2/10, step 28/574 completed (loss: 0.7247819900512695, acc: 0.7209302186965942)
[2025-01-06 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20][root][INFO] - Training Epoch: 2/10, step 29/574 completed (loss: 0.8935021758079529, acc: 0.6867470145225525)
[2025-01-06 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20][root][INFO] - Training Epoch: 2/10, step 30/574 completed (loss: 1.1130157709121704, acc: 0.7037037014961243)
[2025-01-06 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20][root][INFO] - Training Epoch: 2/10, step 31/574 completed (loss: 1.200917363166809, acc: 0.6785714030265808)
[2025-01-06 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21][root][INFO] - Training Epoch: 2/10, step 32/574 completed (loss: 0.7356235384941101, acc: 0.7777777910232544)
[2025-01-06 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21][root][INFO] - Training Epoch: 2/10, step 33/574 completed (loss: 0.22570620477199554, acc: 0.95652174949646)
[2025-01-06 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22][root][INFO] - Training Epoch: 2/10, step 34/574 completed (loss: 0.6020208597183228, acc: 0.7983193397521973)
[2025-01-06 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22][root][INFO] - Training Epoch: 2/10, step 35/574 completed (loss: 0.5814775824546814, acc: 0.8524590134620667)
[2025-01-06 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22][root][INFO] - Training Epoch: 2/10, step 36/574 completed (loss: 0.8273266553878784, acc: 0.8253968358039856)
[2025-01-06 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23][root][INFO] - Training Epoch: 2/10, step 37/574 completed (loss: 0.7772249579429626, acc: 0.8474576473236084)
[2025-01-06 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23][root][INFO] - Training Epoch: 2/10, step 38/574 completed (loss: 0.5141057968139648, acc: 0.8850574493408203)
[2025-01-06 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23][root][INFO] - Training Epoch: 2/10, step 39/574 completed (loss: 0.6401789784431458, acc: 0.8095238208770752)
[2025-01-06 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24][root][INFO] - Training Epoch: 2/10, step 40/574 completed (loss: 0.8183995485305786, acc: 0.7692307829856873)
[2025-01-06 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24][root][INFO] - Training Epoch: 2/10, step 41/574 completed (loss: 0.5470304489135742, acc: 0.837837815284729)
[2025-01-06 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24][root][INFO] - Training Epoch: 2/10, step 42/574 completed (loss: 1.0828540325164795, acc: 0.692307710647583)
[2025-01-06 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25][root][INFO] - Training Epoch: 2/10, step 43/574 completed (loss: 0.9191879034042358, acc: 0.7777777910232544)
[2025-01-06 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25][root][INFO] - Training Epoch: 2/10, step 44/574 completed (loss: 0.6063882112503052, acc: 0.8350515365600586)
[2025-01-06 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26][root][INFO] - Training Epoch: 2/10, step 45/574 completed (loss: 0.6248728036880493, acc: 0.8308823704719543)
[2025-01-06 01:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26][root][INFO] - Training Epoch: 2/10, step 46/574 completed (loss: 0.7312148809432983, acc: 0.7692307829856873)
[2025-01-06 01:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26][root][INFO] - Training Epoch: 2/10, step 47/574 completed (loss: 0.35456177592277527, acc: 0.9629629850387573)
[2025-01-06 01:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:27][root][INFO] - Training Epoch: 2/10, step 48/574 completed (loss: 0.8277263641357422, acc: 0.7857142686843872)
[2025-01-06 01:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:27][root][INFO] - Training Epoch: 2/10, step 49/574 completed (loss: 0.44759538769721985, acc: 0.8333333134651184)
[2025-01-06 01:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:27][root][INFO] - Training Epoch: 2/10, step 50/574 completed (loss: 1.0201468467712402, acc: 0.7543859481811523)
[2025-01-06 01:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:28][root][INFO] - Training Epoch: 2/10, step 51/574 completed (loss: 1.0099365711212158, acc: 0.6984127163887024)
[2025-01-06 01:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:28][root][INFO] - Training Epoch: 2/10, step 52/574 completed (loss: 1.2446990013122559, acc: 0.6901408433914185)
[2025-01-06 01:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:28][root][INFO] - Training Epoch: 2/10, step 53/574 completed (loss: 1.7022802829742432, acc: 0.5266666412353516)
[2025-01-06 01:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:29][root][INFO] - Training Epoch: 2/10, step 54/574 completed (loss: 1.6674350500106812, acc: 0.6216216087341309)
[2025-01-06 01:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:29][root][INFO] - Training Epoch: 2/10, step 55/574 completed (loss: 0.2721173167228699, acc: 0.8846153616905212)
[2025-01-06 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:32][root][INFO] - Training Epoch: 2/10, step 56/574 completed (loss: 1.4432233572006226, acc: 0.6109215021133423)
[2025-01-06 01:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:33][root][INFO] - Training Epoch: 2/10, step 57/574 completed (loss: 1.4148821830749512, acc: 0.6143791079521179)
[2025-01-06 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:34][root][INFO] - Training Epoch: 2/10, step 58/574 completed (loss: 1.1297752857208252, acc: 0.6761363744735718)
[2025-01-06 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:35][root][INFO] - Training Epoch: 2/10, step 59/574 completed (loss: 0.589104413986206, acc: 0.8308823704719543)
[2025-01-06 01:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:35][root][INFO] - Training Epoch: 2/10, step 60/574 completed (loss: 1.1552704572677612, acc: 0.695652186870575)
[2025-01-06 01:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36][root][INFO] - Training Epoch: 2/10, step 61/574 completed (loss: 1.2747642993927002, acc: 0.625)
[2025-01-06 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36][root][INFO] - Training Epoch: 2/10, step 62/574 completed (loss: 0.5923870205879211, acc: 0.8529411554336548)
[2025-01-06 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36][root][INFO] - Training Epoch: 2/10, step 63/574 completed (loss: 0.586694598197937, acc: 0.8055555820465088)
[2025-01-06 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37][root][INFO] - Training Epoch: 2/10, step 64/574 completed (loss: 0.37577974796295166, acc: 0.890625)
[2025-01-06 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37][root][INFO] - Training Epoch: 2/10, step 65/574 completed (loss: 0.3316398859024048, acc: 0.8965517282485962)
[2025-01-06 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37][root][INFO] - Training Epoch: 2/10, step 66/574 completed (loss: 1.2506530284881592, acc: 0.6785714030265808)
[2025-01-06 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38][root][INFO] - Training Epoch: 2/10, step 67/574 completed (loss: 0.5357372164726257, acc: 0.8500000238418579)
[2025-01-06 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38][root][INFO] - Training Epoch: 2/10, step 68/574 completed (loss: 0.12450381368398666, acc: 0.9599999785423279)
[2025-01-06 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38][root][INFO] - Training Epoch: 2/10, step 69/574 completed (loss: 1.1978963613510132, acc: 0.6388888955116272)
[2025-01-06 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38][root][INFO] - Training Epoch: 2/10, step 70/574 completed (loss: 1.3448141813278198, acc: 0.5757575631141663)
[2025-01-06 01:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39][root][INFO] - Training Epoch: 2/10, step 71/574 completed (loss: 1.2154016494750977, acc: 0.6397058963775635)
[2025-01-06 01:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39][root][INFO] - Training Epoch: 2/10, step 72/574 completed (loss: 0.9209513068199158, acc: 0.7539682388305664)
[2025-01-06 01:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40][root][INFO] - Training Epoch: 2/10, step 73/574 completed (loss: 1.6394895315170288, acc: 0.5692307949066162)
[2025-01-06 01:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40][root][INFO] - Training Epoch: 2/10, step 74/574 completed (loss: 1.41606867313385, acc: 0.6428571343421936)
[2025-01-06 01:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40][root][INFO] - Training Epoch: 2/10, step 75/574 completed (loss: 1.4697654247283936, acc: 0.5970149040222168)
[2025-01-06 01:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41][root][INFO] - Training Epoch: 2/10, step 76/574 completed (loss: 1.60781729221344, acc: 0.5729926824569702)
[2025-01-06 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41][root][INFO] - Training Epoch: 2/10, step 77/574 completed (loss: 0.08179499208927155, acc: 1.0)
[2025-01-06 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41][root][INFO] - Training Epoch: 2/10, step 78/574 completed (loss: 0.39916980266571045, acc: 0.9166666865348816)
[2025-01-06 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:42][root][INFO] - Training Epoch: 2/10, step 79/574 completed (loss: 0.2954017221927643, acc: 0.8787878751754761)
[2025-01-06 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:42][root][INFO] - Training Epoch: 2/10, step 80/574 completed (loss: 0.5731922388076782, acc: 0.8461538553237915)
[2025-01-06 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:43][root][INFO] - Training Epoch: 2/10, step 81/574 completed (loss: 0.9046562910079956, acc: 0.7692307829856873)
[2025-01-06 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:43][root][INFO] - Training Epoch: 2/10, step 82/574 completed (loss: 0.991618812084198, acc: 0.7692307829856873)
[2025-01-06 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:43][root][INFO] - Training Epoch: 2/10, step 83/574 completed (loss: 0.46963316202163696, acc: 0.875)
[2025-01-06 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44][root][INFO] - Training Epoch: 2/10, step 84/574 completed (loss: 0.651268720626831, acc: 0.8260869383811951)
[2025-01-06 01:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44][root][INFO] - Training Epoch: 2/10, step 85/574 completed (loss: 0.9607124328613281, acc: 0.7200000286102295)
[2025-01-06 01:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44][root][INFO] - Training Epoch: 2/10, step 86/574 completed (loss: 0.5214285850524902, acc: 0.8695651888847351)
[2025-01-06 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:45][root][INFO] - Training Epoch: 2/10, step 87/574 completed (loss: 1.2623591423034668, acc: 0.6000000238418579)
[2025-01-06 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:45][root][INFO] - Training Epoch: 2/10, step 88/574 completed (loss: 1.2196738719940186, acc: 0.6796116232872009)
[2025-01-06 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:46][root][INFO] - Training Epoch: 2/10, step 89/574 completed (loss: 1.0991523265838623, acc: 0.708737850189209)
[2025-01-06 01:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:47][root][INFO] - Training Epoch: 2/10, step 90/574 completed (loss: 1.4791160821914673, acc: 0.6129032373428345)
[2025-01-06 01:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:48][root][INFO] - Training Epoch: 2/10, step 91/574 completed (loss: 1.2229055166244507, acc: 0.6594827771186829)
[2025-01-06 01:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:49][root][INFO] - Training Epoch: 2/10, step 92/574 completed (loss: 0.9012254476547241, acc: 0.7263157963752747)
[2025-01-06 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50][root][INFO] - Training Epoch: 2/10, step 93/574 completed (loss: 1.8095333576202393, acc: 0.4752475321292877)
[2025-01-06 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50][root][INFO] - Training Epoch: 2/10, step 94/574 completed (loss: 1.4738682508468628, acc: 0.5483871102333069)
[2025-01-06 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50][root][INFO] - Training Epoch: 2/10, step 95/574 completed (loss: 1.225109338760376, acc: 0.6376811861991882)
[2025-01-06 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51][root][INFO] - Training Epoch: 2/10, step 96/574 completed (loss: 1.4899866580963135, acc: 0.5546218752861023)
[2025-01-06 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51][root][INFO] - Training Epoch: 2/10, step 97/574 completed (loss: 1.703063726425171, acc: 0.5384615659713745)
[2025-01-06 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51][root][INFO] - Training Epoch: 2/10, step 98/574 completed (loss: 1.6566827297210693, acc: 0.525547444820404)
[2025-01-06 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52][root][INFO] - Training Epoch: 2/10, step 99/574 completed (loss: 1.789255976676941, acc: 0.5223880410194397)
[2025-01-06 01:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52][root][INFO] - Training Epoch: 2/10, step 100/574 completed (loss: 0.616720974445343, acc: 0.8500000238418579)
[2025-01-06 01:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52][root][INFO] - Training Epoch: 2/10, step 101/574 completed (loss: 0.11157049983739853, acc: 1.0)
[2025-01-06 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:53][root][INFO] - Training Epoch: 2/10, step 102/574 completed (loss: 0.15495948493480682, acc: 1.0)
[2025-01-06 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:53][root][INFO] - Training Epoch: 2/10, step 103/574 completed (loss: 0.2740823030471802, acc: 0.8863636255264282)
[2025-01-06 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54][root][INFO] - Training Epoch: 2/10, step 104/574 completed (loss: 0.7611956000328064, acc: 0.7931034564971924)
[2025-01-06 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54][root][INFO] - Training Epoch: 2/10, step 105/574 completed (loss: 0.4637688100337982, acc: 0.8604651093482971)
[2025-01-06 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54][root][INFO] - Training Epoch: 2/10, step 106/574 completed (loss: 0.4685255289077759, acc: 0.8799999952316284)
[2025-01-06 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:55][root][INFO] - Training Epoch: 2/10, step 107/574 completed (loss: 0.07409940659999847, acc: 1.0)
[2025-01-06 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:55][root][INFO] - Training Epoch: 2/10, step 108/574 completed (loss: 0.14384879171848297, acc: 0.9615384340286255)
[2025-01-06 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56][root][INFO] - Training Epoch: 2/10, step 109/574 completed (loss: 0.18903996050357819, acc: 0.9047619104385376)
[2025-01-06 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56][root][INFO] - Training Epoch: 2/10, step 110/574 completed (loss: 0.20350012183189392, acc: 0.9538461565971375)
[2025-01-06 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56][root][INFO] - Training Epoch: 2/10, step 111/574 completed (loss: 0.7454786896705627, acc: 0.7894737124443054)
[2025-01-06 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:57][root][INFO] - Training Epoch: 2/10, step 112/574 completed (loss: 1.2991013526916504, acc: 0.6491228342056274)
[2025-01-06 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:57][root][INFO] - Training Epoch: 2/10, step 113/574 completed (loss: 0.8160881996154785, acc: 0.7948718070983887)
[2025-01-06 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58][root][INFO] - Training Epoch: 2/10, step 114/574 completed (loss: 0.5404468774795532, acc: 0.8367347121238708)
[2025-01-06 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58][root][INFO] - Training Epoch: 2/10, step 115/574 completed (loss: 0.22033271193504333, acc: 0.9545454382896423)
[2025-01-06 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58][root][INFO] - Training Epoch: 2/10, step 116/574 completed (loss: 0.6746638417243958, acc: 0.8095238208770752)
[2025-01-06 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59][root][INFO] - Training Epoch: 2/10, step 117/574 completed (loss: 0.6661014556884766, acc: 0.8292682766914368)
[2025-01-06 01:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59][root][INFO] - Training Epoch: 2/10, step 118/574 completed (loss: 0.4066845178604126, acc: 0.9032257795333862)
[2025-01-06 01:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:00][root][INFO] - Training Epoch: 2/10, step 119/574 completed (loss: 0.9176133275032043, acc: 0.7642585635185242)
[2025-01-06 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:00][root][INFO] - Training Epoch: 2/10, step 120/574 completed (loss: 0.5087900161743164, acc: 0.8399999737739563)
[2025-01-06 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01][root][INFO] - Training Epoch: 2/10, step 121/574 completed (loss: 0.6349326372146606, acc: 0.8653846383094788)
[2025-01-06 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01][root][INFO] - Training Epoch: 2/10, step 122/574 completed (loss: 0.43168124556541443, acc: 0.7916666865348816)
[2025-01-06 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01][root][INFO] - Training Epoch: 2/10, step 123/574 completed (loss: 0.43594273924827576, acc: 0.8947368264198303)
[2025-01-06 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02][root][INFO] - Training Epoch: 2/10, step 124/574 completed (loss: 1.314558506011963, acc: 0.6380367875099182)
[2025-01-06 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02][root][INFO] - Training Epoch: 2/10, step 125/574 completed (loss: 1.3862396478652954, acc: 0.6319444179534912)
[2025-01-06 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02][root][INFO] - Training Epoch: 2/10, step 126/574 completed (loss: 1.4471691846847534, acc: 0.6083333492279053)
[2025-01-06 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03][root][INFO] - Training Epoch: 2/10, step 127/574 completed (loss: 1.0160081386566162, acc: 0.6964285969734192)
[2025-01-06 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03][root][INFO] - Training Epoch: 2/10, step 128/574 completed (loss: 1.0685341358184814, acc: 0.7128205299377441)
[2025-01-06 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04][root][INFO] - Training Epoch: 2/10, step 129/574 completed (loss: 1.1767491102218628, acc: 0.6617646813392639)
[2025-01-06 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04][root][INFO] - Training Epoch: 2/10, step 130/574 completed (loss: 1.1313488483428955, acc: 0.692307710647583)
[2025-01-06 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04][root][INFO] - Training Epoch: 2/10, step 131/574 completed (loss: 0.8780121803283691, acc: 0.6086956262588501)
[2025-01-06 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:05][root][INFO] - Training Epoch: 2/10, step 132/574 completed (loss: 1.3835855722427368, acc: 0.59375)
[2025-01-06 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:05][root][INFO] - Training Epoch: 2/10, step 133/574 completed (loss: 1.6373695135116577, acc: 0.5652173757553101)
[2025-01-06 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:05][root][INFO] - Training Epoch: 2/10, step 134/574 completed (loss: 1.027084469795227, acc: 0.6571428775787354)
[2025-01-06 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06][root][INFO] - Training Epoch: 2/10, step 135/574 completed (loss: 1.1612765789031982, acc: 0.7307692170143127)
[2025-01-06 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06][root][INFO] - Training Epoch: 2/10, step 136/574 completed (loss: 0.9953247308731079, acc: 0.7142857313156128)
[2025-01-06 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06][root][INFO] - Training Epoch: 2/10, step 137/574 completed (loss: 1.426124930381775, acc: 0.5)
[2025-01-06 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07][root][INFO] - Training Epoch: 2/10, step 138/574 completed (loss: 1.1886519193649292, acc: 0.695652186870575)
[2025-01-06 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07][root][INFO] - Training Epoch: 2/10, step 139/574 completed (loss: 0.4343222975730896, acc: 0.8571428656578064)
[2025-01-06 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07][root][INFO] - Training Epoch: 2/10, step 140/574 completed (loss: 0.5515655279159546, acc: 0.7692307829856873)
[2025-01-06 01:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0212, device='cuda:0') eval_epoch_loss=tensor(0.7037, device='cuda:0') eval_epoch_acc=tensor(0.8032, device='cuda:0')
[2025-01-06 01:07:38][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:07:38][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:07:39][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_141_loss_0.7036666870117188/model.pt
[2025-01-06 01:07:39][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:39][root][INFO] - Training Epoch: 2/10, step 141/574 completed (loss: 1.1107159852981567, acc: 0.7096773982048035)
[2025-01-06 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:39][root][INFO] - Training Epoch: 2/10, step 142/574 completed (loss: 1.338393211364746, acc: 0.6486486196517944)
[2025-01-06 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40][root][INFO] - Training Epoch: 2/10, step 143/574 completed (loss: 1.1484960317611694, acc: 0.6666666865348816)
[2025-01-06 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40][root][INFO] - Training Epoch: 2/10, step 144/574 completed (loss: 0.9128457903862, acc: 0.7835820913314819)
[2025-01-06 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41][root][INFO] - Training Epoch: 2/10, step 145/574 completed (loss: 0.9253627061843872, acc: 0.6938775777816772)
[2025-01-06 01:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41][root][INFO] - Training Epoch: 2/10, step 146/574 completed (loss: 1.3438760042190552, acc: 0.585106372833252)
[2025-01-06 01:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41][root][INFO] - Training Epoch: 2/10, step 147/574 completed (loss: 1.138297200202942, acc: 0.6714285612106323)
[2025-01-06 01:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:42][root][INFO] - Training Epoch: 2/10, step 148/574 completed (loss: 1.2592055797576904, acc: 0.6428571343421936)
[2025-01-06 01:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:42][root][INFO] - Training Epoch: 2/10, step 149/574 completed (loss: 1.0741157531738281, acc: 0.695652186870575)
[2025-01-06 01:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:42][root][INFO] - Training Epoch: 2/10, step 150/574 completed (loss: 0.9285979270935059, acc: 0.7241379022598267)
[2025-01-06 01:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:43][root][INFO] - Training Epoch: 2/10, step 151/574 completed (loss: 1.4082645177841187, acc: 0.6521739363670349)
[2025-01-06 01:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:43][root][INFO] - Training Epoch: 2/10, step 152/574 completed (loss: 0.8926615715026855, acc: 0.7627118825912476)
[2025-01-06 01:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:43][root][INFO] - Training Epoch: 2/10, step 153/574 completed (loss: 1.1508110761642456, acc: 0.6491228342056274)
[2025-01-06 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:44][root][INFO] - Training Epoch: 2/10, step 154/574 completed (loss: 0.9452139139175415, acc: 0.7702702879905701)
[2025-01-06 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:44][root][INFO] - Training Epoch: 2/10, step 155/574 completed (loss: 0.4679718017578125, acc: 0.8214285969734192)
[2025-01-06 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:45][root][INFO] - Training Epoch: 2/10, step 156/574 completed (loss: 0.6239530444145203, acc: 0.8260869383811951)
[2025-01-06 01:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:45][root][INFO] - Training Epoch: 2/10, step 157/574 completed (loss: 2.2824904918670654, acc: 0.3684210479259491)
[2025-01-06 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47][root][INFO] - Training Epoch: 2/10, step 158/574 completed (loss: 1.391862154006958, acc: 0.5675675868988037)
[2025-01-06 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47][root][INFO] - Training Epoch: 2/10, step 159/574 completed (loss: 1.5636720657348633, acc: 0.5)
[2025-01-06 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47][root][INFO] - Training Epoch: 2/10, step 160/574 completed (loss: 1.559796690940857, acc: 0.569767415523529)
[2025-01-06 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:48][root][INFO] - Training Epoch: 2/10, step 161/574 completed (loss: 1.8415956497192383, acc: 0.4588235318660736)
[2025-01-06 01:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:48][root][INFO] - Training Epoch: 2/10, step 162/574 completed (loss: 1.8640868663787842, acc: 0.550561785697937)
[2025-01-06 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49][root][INFO] - Training Epoch: 2/10, step 163/574 completed (loss: 0.6927237510681152, acc: 0.8863636255264282)
[2025-01-06 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49][root][INFO] - Training Epoch: 2/10, step 164/574 completed (loss: 0.6448192000389099, acc: 0.9047619104385376)
[2025-01-06 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50][root][INFO] - Training Epoch: 2/10, step 165/574 completed (loss: 1.2257397174835205, acc: 0.6551724076271057)
[2025-01-06 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50][root][INFO] - Training Epoch: 2/10, step 166/574 completed (loss: 0.2816638648509979, acc: 0.8775510191917419)
[2025-01-06 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50][root][INFO] - Training Epoch: 2/10, step 167/574 completed (loss: 0.3582890033721924, acc: 0.8799999952316284)
[2025-01-06 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:51][root][INFO] - Training Epoch: 2/10, step 168/574 completed (loss: 0.7416444420814514, acc: 0.8055555820465088)
[2025-01-06 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:51][root][INFO] - Training Epoch: 2/10, step 169/574 completed (loss: 1.150766134262085, acc: 0.7352941036224365)
[2025-01-06 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:52][root][INFO] - Training Epoch: 2/10, step 170/574 completed (loss: 1.1843867301940918, acc: 0.6780821681022644)
[2025-01-06 01:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:52][root][INFO] - Training Epoch: 2/10, step 171/574 completed (loss: 0.20448057353496552, acc: 0.9583333134651184)
[2025-01-06 01:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53][root][INFO] - Training Epoch: 2/10, step 172/574 completed (loss: 0.6186341643333435, acc: 0.7777777910232544)
[2025-01-06 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53][root][INFO] - Training Epoch: 2/10, step 173/574 completed (loss: 1.1657450199127197, acc: 0.6785714030265808)
[2025-01-06 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54][root][INFO] - Training Epoch: 2/10, step 174/574 completed (loss: 1.19468355178833, acc: 0.6991150379180908)
[2025-01-06 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54][root][INFO] - Training Epoch: 2/10, step 175/574 completed (loss: 0.9587777256965637, acc: 0.7681159377098083)
[2025-01-06 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54][root][INFO] - Training Epoch: 2/10, step 176/574 completed (loss: 0.7448518872261047, acc: 0.7840909361839294)
[2025-01-06 01:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:55][root][INFO] - Training Epoch: 2/10, step 177/574 completed (loss: 1.4390206336975098, acc: 0.580152690410614)
[2025-01-06 01:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:56][root][INFO] - Training Epoch: 2/10, step 178/574 completed (loss: 1.351707100868225, acc: 0.614814817905426)
[2025-01-06 01:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:56][root][INFO] - Training Epoch: 2/10, step 179/574 completed (loss: 0.7173662185668945, acc: 0.8196721076965332)
[2025-01-06 01:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57][root][INFO] - Training Epoch: 2/10, step 180/574 completed (loss: 0.09252341836690903, acc: 0.9583333134651184)
[2025-01-06 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57][root][INFO] - Training Epoch: 2/10, step 181/574 completed (loss: 0.15208859741687775, acc: 0.8799999952316284)
[2025-01-06 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57][root][INFO] - Training Epoch: 2/10, step 182/574 completed (loss: 0.3389569818973541, acc: 0.8928571343421936)
[2025-01-06 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:58][root][INFO] - Training Epoch: 2/10, step 183/574 completed (loss: 0.44498756527900696, acc: 0.8780487775802612)
[2025-01-06 01:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:58][root][INFO] - Training Epoch: 2/10, step 184/574 completed (loss: 0.5518134832382202, acc: 0.861027181148529)
[2025-01-06 01:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:58][root][INFO] - Training Epoch: 2/10, step 185/574 completed (loss: 0.6129282116889954, acc: 0.8472622632980347)
[2025-01-06 01:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:59][root][INFO] - Training Epoch: 2/10, step 186/574 completed (loss: 0.5316880345344543, acc: 0.8031250238418579)
[2025-01-06 01:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:59][root][INFO] - Training Epoch: 2/10, step 187/574 completed (loss: 0.5804099440574646, acc: 0.8405253291130066)
[2025-01-06 01:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:00][root][INFO] - Training Epoch: 2/10, step 188/574 completed (loss: 0.6789749264717102, acc: 0.7971529960632324)
[2025-01-06 01:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:00][root][INFO] - Training Epoch: 2/10, step 189/574 completed (loss: 0.6927076578140259, acc: 0.7599999904632568)
[2025-01-06 01:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:01][root][INFO] - Training Epoch: 2/10, step 190/574 completed (loss: 1.290390968322754, acc: 0.6162790656089783)
[2025-01-06 01:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02][root][INFO] - Training Epoch: 2/10, step 191/574 completed (loss: 1.8432713747024536, acc: 0.523809552192688)
[2025-01-06 01:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02][root][INFO] - Training Epoch: 2/10, step 192/574 completed (loss: 1.471564531326294, acc: 0.5757575631141663)
[2025-01-06 01:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:03][root][INFO] - Training Epoch: 2/10, step 193/574 completed (loss: 1.1743431091308594, acc: 0.6470588445663452)
[2025-01-06 01:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:04][root][INFO] - Training Epoch: 2/10, step 194/574 completed (loss: 1.2994639873504639, acc: 0.6111111044883728)
[2025-01-06 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:05][root][INFO] - Training Epoch: 2/10, step 195/574 completed (loss: 0.8253999948501587, acc: 0.725806474685669)
[2025-01-06 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06][root][INFO] - Training Epoch: 2/10, step 196/574 completed (loss: 0.38803812861442566, acc: 0.8928571343421936)
[2025-01-06 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06][root][INFO] - Training Epoch: 2/10, step 197/574 completed (loss: 1.2606747150421143, acc: 0.625)
[2025-01-06 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06][root][INFO] - Training Epoch: 2/10, step 198/574 completed (loss: 1.1138734817504883, acc: 0.6911764740943909)
[2025-01-06 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07][root][INFO] - Training Epoch: 2/10, step 199/574 completed (loss: 1.2111351490020752, acc: 0.6985294222831726)
[2025-01-06 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07][root][INFO] - Training Epoch: 2/10, step 200/574 completed (loss: 1.0008152723312378, acc: 0.6864407062530518)
[2025-01-06 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07][root][INFO] - Training Epoch: 2/10, step 201/574 completed (loss: 1.1293528079986572, acc: 0.7164179086685181)
[2025-01-06 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08][root][INFO] - Training Epoch: 2/10, step 202/574 completed (loss: 1.116487741470337, acc: 0.7281553149223328)
[2025-01-06 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08][root][INFO] - Training Epoch: 2/10, step 203/574 completed (loss: 0.9825674295425415, acc: 0.6984127163887024)
[2025-01-06 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08][root][INFO] - Training Epoch: 2/10, step 204/574 completed (loss: 0.20517607033252716, acc: 0.9560439586639404)
[2025-01-06 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:09][root][INFO] - Training Epoch: 2/10, step 205/574 completed (loss: 0.44004425406455994, acc: 0.878923773765564)
[2025-01-06 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:09][root][INFO] - Training Epoch: 2/10, step 206/574 completed (loss: 0.5178534388542175, acc: 0.8503937125205994)
[2025-01-06 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10][root][INFO] - Training Epoch: 2/10, step 207/574 completed (loss: 0.5225468873977661, acc: 0.8663793206214905)
[2025-01-06 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10][root][INFO] - Training Epoch: 2/10, step 208/574 completed (loss: 0.5759710073471069, acc: 0.8731883764266968)
[2025-01-06 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10][root][INFO] - Training Epoch: 2/10, step 209/574 completed (loss: 0.5134397149085999, acc: 0.8677042722702026)
[2025-01-06 01:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11][root][INFO] - Training Epoch: 2/10, step 210/574 completed (loss: 0.44975611567497253, acc: 0.9130434989929199)
[2025-01-06 01:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11][root][INFO] - Training Epoch: 2/10, step 211/574 completed (loss: 0.5391284227371216, acc: 0.8260869383811951)
[2025-01-06 01:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11][root][INFO] - Training Epoch: 2/10, step 212/574 completed (loss: 0.24943843483924866, acc: 0.9285714030265808)
[2025-01-06 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:12][root][INFO] - Training Epoch: 2/10, step 213/574 completed (loss: 0.3177429437637329, acc: 0.914893627166748)
[2025-01-06 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:12][root][INFO] - Training Epoch: 2/10, step 214/574 completed (loss: 0.3348345160484314, acc: 0.9153845906257629)
[2025-01-06 01:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:13][root][INFO] - Training Epoch: 2/10, step 215/574 completed (loss: 0.36478880047798157, acc: 0.8918918967247009)
[2025-01-06 01:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:13][root][INFO] - Training Epoch: 2/10, step 216/574 completed (loss: 0.3427331745624542, acc: 0.895348846912384)
[2025-01-06 01:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14][root][INFO] - Training Epoch: 2/10, step 217/574 completed (loss: 0.4472092390060425, acc: 0.9009009003639221)
[2025-01-06 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14][root][INFO] - Training Epoch: 2/10, step 218/574 completed (loss: 0.247029110789299, acc: 0.9333333373069763)
[2025-01-06 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14][root][INFO] - Training Epoch: 2/10, step 219/574 completed (loss: 0.30232441425323486, acc: 0.9090909361839294)
[2025-01-06 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15][root][INFO] - Training Epoch: 2/10, step 220/574 completed (loss: 0.2709232568740845, acc: 0.8518518805503845)
[2025-01-06 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15][root][INFO] - Training Epoch: 2/10, step 221/574 completed (loss: 0.3248930275440216, acc: 0.8399999737739563)
[2025-01-06 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:16][root][INFO] - Training Epoch: 2/10, step 222/574 completed (loss: 1.024468183517456, acc: 0.7115384340286255)
[2025-01-06 01:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:16][root][INFO] - Training Epoch: 2/10, step 223/574 completed (loss: 0.5085110068321228, acc: 0.8804348111152649)
[2025-01-06 01:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:17][root][INFO] - Training Epoch: 2/10, step 224/574 completed (loss: 0.7215670943260193, acc: 0.7613636255264282)
[2025-01-06 01:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:17][root][INFO] - Training Epoch: 2/10, step 225/574 completed (loss: 1.0328419208526611, acc: 0.7765957713127136)
[2025-01-06 01:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18][root][INFO] - Training Epoch: 2/10, step 226/574 completed (loss: 0.8248197436332703, acc: 0.7547169923782349)
[2025-01-06 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18][root][INFO] - Training Epoch: 2/10, step 227/574 completed (loss: 0.5166445374488831, acc: 0.800000011920929)
[2025-01-06 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18][root][INFO] - Training Epoch: 2/10, step 228/574 completed (loss: 0.41050755977630615, acc: 0.8604651093482971)
[2025-01-06 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19][root][INFO] - Training Epoch: 2/10, step 229/574 completed (loss: 1.7885030508041382, acc: 0.5)
[2025-01-06 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19][root][INFO] - Training Epoch: 2/10, step 230/574 completed (loss: 2.156675100326538, acc: 0.4842105209827423)
[2025-01-06 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19][root][INFO] - Training Epoch: 2/10, step 231/574 completed (loss: 1.648942232131958, acc: 0.5777778029441833)
[2025-01-06 01:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:20][root][INFO] - Training Epoch: 2/10, step 232/574 completed (loss: 1.679942011833191, acc: 0.5166666507720947)
[2025-01-06 01:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:20][root][INFO] - Training Epoch: 2/10, step 233/574 completed (loss: 1.9365042448043823, acc: 0.47706422209739685)
[2025-01-06 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21][root][INFO] - Training Epoch: 2/10, step 234/574 completed (loss: 1.6456435918807983, acc: 0.5384615659713745)
[2025-01-06 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21][root][INFO] - Training Epoch: 2/10, step 235/574 completed (loss: 0.5908181071281433, acc: 0.8421052694320679)
[2025-01-06 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21][root][INFO] - Training Epoch: 2/10, step 236/574 completed (loss: 0.7237928509712219, acc: 0.75)
[2025-01-06 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22][root][INFO] - Training Epoch: 2/10, step 237/574 completed (loss: 1.3903725147247314, acc: 0.6363636255264282)
[2025-01-06 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22][root][INFO] - Training Epoch: 2/10, step 238/574 completed (loss: 0.8137289881706238, acc: 0.7407407164573669)
[2025-01-06 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23][root][INFO] - Training Epoch: 2/10, step 239/574 completed (loss: 1.0234472751617432, acc: 0.7142857313156128)
[2025-01-06 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23][root][INFO] - Training Epoch: 2/10, step 240/574 completed (loss: 1.2512983083724976, acc: 0.6818181872367859)
[2025-01-06 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23][root][INFO] - Training Epoch: 2/10, step 241/574 completed (loss: 0.8742824196815491, acc: 0.7045454382896423)
[2025-01-06 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:24][root][INFO] - Training Epoch: 2/10, step 242/574 completed (loss: 1.4291614294052124, acc: 0.5645161271095276)
[2025-01-06 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25][root][INFO] - Training Epoch: 2/10, step 243/574 completed (loss: 1.4617270231246948, acc: 0.5909090638160706)
[2025-01-06 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25][root][INFO] - Training Epoch: 2/10, step 244/574 completed (loss: 0.1645388901233673, acc: 0.9523809552192688)
[2025-01-06 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25][root][INFO] - Training Epoch: 2/10, step 245/574 completed (loss: 0.6848965883255005, acc: 0.7692307829856873)
[2025-01-06 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26][root][INFO] - Training Epoch: 2/10, step 246/574 completed (loss: 0.47292816638946533, acc: 0.8709677457809448)
[2025-01-06 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26][root][INFO] - Training Epoch: 2/10, step 247/574 completed (loss: 0.4774598479270935, acc: 0.800000011920929)
[2025-01-06 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26][root][INFO] - Training Epoch: 2/10, step 248/574 completed (loss: 0.39929139614105225, acc: 0.8918918967247009)
[2025-01-06 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27][root][INFO] - Training Epoch: 2/10, step 249/574 completed (loss: 0.5828083157539368, acc: 0.8648648858070374)
[2025-01-06 01:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27][root][INFO] - Training Epoch: 2/10, step 250/574 completed (loss: 0.15384477376937866, acc: 1.0)
[2025-01-06 01:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27][root][INFO] - Training Epoch: 2/10, step 251/574 completed (loss: 0.4606676995754242, acc: 0.8382353186607361)
[2025-01-06 01:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28][root][INFO] - Training Epoch: 2/10, step 252/574 completed (loss: 0.22685541212558746, acc: 0.9268292784690857)
[2025-01-06 01:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28][root][INFO] - Training Epoch: 2/10, step 253/574 completed (loss: 0.16112935543060303, acc: 0.9200000166893005)
[2025-01-06 01:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28][root][INFO] - Training Epoch: 2/10, step 254/574 completed (loss: 0.07000767439603806, acc: 0.9599999785423279)
[2025-01-06 01:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:29][root][INFO] - Training Epoch: 2/10, step 255/574 completed (loss: 0.30965256690979004, acc: 0.9032257795333862)
[2025-01-06 01:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:29][root][INFO] - Training Epoch: 2/10, step 256/574 completed (loss: 0.3608449399471283, acc: 0.9122806787490845)
[2025-01-06 01:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30][root][INFO] - Training Epoch: 2/10, step 257/574 completed (loss: 0.2807997167110443, acc: 0.8999999761581421)
[2025-01-06 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30][root][INFO] - Training Epoch: 2/10, step 258/574 completed (loss: 0.23863835632801056, acc: 0.9210526347160339)
[2025-01-06 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30][root][INFO] - Training Epoch: 2/10, step 259/574 completed (loss: 0.5140376091003418, acc: 0.8867924809455872)
[2025-01-06 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:31][root][INFO] - Training Epoch: 2/10, step 260/574 completed (loss: 0.4816912114620209, acc: 0.875)
[2025-01-06 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:31][root][INFO] - Training Epoch: 2/10, step 261/574 completed (loss: 0.25921133160591125, acc: 0.9166666865348816)
[2025-01-06 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32][root][INFO] - Training Epoch: 2/10, step 262/574 completed (loss: 0.8781804442405701, acc: 0.7419354915618896)
[2025-01-06 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32][root][INFO] - Training Epoch: 2/10, step 263/574 completed (loss: 1.6142441034317017, acc: 0.653333306312561)
[2025-01-06 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32][root][INFO] - Training Epoch: 2/10, step 264/574 completed (loss: 0.9257746338844299, acc: 0.6458333134651184)
[2025-01-06 01:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33][root][INFO] - Training Epoch: 2/10, step 265/574 completed (loss: 1.5328377485275269, acc: 0.5920000076293945)
[2025-01-06 01:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33][root][INFO] - Training Epoch: 2/10, step 266/574 completed (loss: 1.7311326265335083, acc: 0.584269642829895)
[2025-01-06 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:34][root][INFO] - Training Epoch: 2/10, step 267/574 completed (loss: 1.3272011280059814, acc: 0.5810810923576355)
[2025-01-06 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:34][root][INFO] - Training Epoch: 2/10, step 268/574 completed (loss: 0.9866839647293091, acc: 0.6724137663841248)
[2025-01-06 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35][root][INFO] - Training Epoch: 2/10, step 269/574 completed (loss: 0.27042487263679504, acc: 0.9090909361839294)
[2025-01-06 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35][root][INFO] - Training Epoch: 2/10, step 270/574 completed (loss: 0.27540504932403564, acc: 0.8636363744735718)
[2025-01-06 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35][root][INFO] - Training Epoch: 2/10, step 271/574 completed (loss: 0.32662853598594666, acc: 0.90625)
[2025-01-06 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36][root][INFO] - Training Epoch: 2/10, step 272/574 completed (loss: 0.23037365078926086, acc: 0.9666666388511658)
[2025-01-06 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36][root][INFO] - Training Epoch: 2/10, step 273/574 completed (loss: 0.4436975419521332, acc: 0.9166666865348816)
[2025-01-06 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36][root][INFO] - Training Epoch: 2/10, step 274/574 completed (loss: 0.4884307384490967, acc: 0.8125)
[2025-01-06 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:37][root][INFO] - Training Epoch: 2/10, step 275/574 completed (loss: 0.3587588667869568, acc: 0.9333333373069763)
[2025-01-06 01:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:37][root][INFO] - Training Epoch: 2/10, step 276/574 completed (loss: 0.548528254032135, acc: 0.8965517282485962)
[2025-01-06 01:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38][root][INFO] - Training Epoch: 2/10, step 277/574 completed (loss: 0.2840701937675476, acc: 0.9599999785423279)
[2025-01-06 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38][root][INFO] - Training Epoch: 2/10, step 278/574 completed (loss: 0.7451688647270203, acc: 0.8085106611251831)
[2025-01-06 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38][root][INFO] - Training Epoch: 2/10, step 279/574 completed (loss: 0.6262233853340149, acc: 0.8541666865348816)
[2025-01-06 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39][root][INFO] - Training Epoch: 2/10, step 280/574 completed (loss: 0.2707430422306061, acc: 0.9318181872367859)
[2025-01-06 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39][root][INFO] - Training Epoch: 2/10, step 281/574 completed (loss: 0.9986971020698547, acc: 0.7228915691375732)
[2025-01-06 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39][root][INFO] - Training Epoch: 2/10, step 282/574 completed (loss: 1.1496199369430542, acc: 0.6944444179534912)
[2025-01-06 01:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:40][root][INFO] - Training Epoch: 2/10, step 283/574 completed (loss: 0.27838239073753357, acc: 0.9736841917037964)
[2025-01-06 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:11][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9448, device='cuda:0') eval_epoch_loss=tensor(0.6652, device='cuda:0') eval_epoch_acc=tensor(0.8124, device='cuda:0')
[2025-01-06 01:09:11][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:09:11][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:09:11][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.6651830673217773/model.pt
[2025-01-06 01:09:11][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:09:11][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6651830673217773
[2025-01-06 01:09:11][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.812403678894043
[2025-01-06 01:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:11][root][INFO] - Training Epoch: 2/10, step 284/574 completed (loss: 0.6168701648712158, acc: 0.7941176295280457)
[2025-01-06 01:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12][root][INFO] - Training Epoch: 2/10, step 285/574 completed (loss: 0.5492855310440063, acc: 0.8999999761581421)
[2025-01-06 01:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12][root][INFO] - Training Epoch: 2/10, step 286/574 completed (loss: 0.6606484651565552, acc: 0.828125)
[2025-01-06 01:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12][root][INFO] - Training Epoch: 2/10, step 287/574 completed (loss: 0.6651232242584229, acc: 0.8159999847412109)
[2025-01-06 01:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13][root][INFO] - Training Epoch: 2/10, step 288/574 completed (loss: 0.6465622782707214, acc: 0.8131868243217468)
[2025-01-06 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13][root][INFO] - Training Epoch: 2/10, step 289/574 completed (loss: 0.6241762042045593, acc: 0.8260869383811951)
[2025-01-06 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13][root][INFO] - Training Epoch: 2/10, step 290/574 completed (loss: 0.6455180048942566, acc: 0.8247422575950623)
[2025-01-06 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14][root][INFO] - Training Epoch: 2/10, step 291/574 completed (loss: 0.12009942531585693, acc: 0.9545454382896423)
[2025-01-06 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14][root][INFO] - Training Epoch: 2/10, step 292/574 completed (loss: 0.8011101484298706, acc: 0.8095238208770752)
[2025-01-06 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:15][root][INFO] - Training Epoch: 2/10, step 293/574 completed (loss: 0.2818209230899811, acc: 0.931034505367279)
[2025-01-06 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:15][root][INFO] - Training Epoch: 2/10, step 294/574 completed (loss: 0.7031814455986023, acc: 0.800000011920929)
[2025-01-06 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16][root][INFO] - Training Epoch: 2/10, step 295/574 completed (loss: 0.7168219685554504, acc: 0.8144329786300659)
[2025-01-06 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16][root][INFO] - Training Epoch: 2/10, step 296/574 completed (loss: 0.5965160727500916, acc: 0.8103448152542114)
[2025-01-06 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16][root][INFO] - Training Epoch: 2/10, step 297/574 completed (loss: 0.19028055667877197, acc: 0.9629629850387573)
[2025-01-06 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17][root][INFO] - Training Epoch: 2/10, step 298/574 completed (loss: 0.6389436721801758, acc: 0.7894737124443054)
[2025-01-06 01:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17][root][INFO] - Training Epoch: 2/10, step 299/574 completed (loss: 0.2737196087837219, acc: 0.9285714030265808)
[2025-01-06 01:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17][root][INFO] - Training Epoch: 2/10, step 300/574 completed (loss: 0.21276552975177765, acc: 0.875)
[2025-01-06 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:18][root][INFO] - Training Epoch: 2/10, step 301/574 completed (loss: 0.5832223892211914, acc: 0.849056601524353)
[2025-01-06 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:18][root][INFO] - Training Epoch: 2/10, step 302/574 completed (loss: 0.07429399341344833, acc: 0.9811320900917053)
[2025-01-06 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19][root][INFO] - Training Epoch: 2/10, step 303/574 completed (loss: 0.14134302735328674, acc: 0.9411764740943909)
[2025-01-06 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19][root][INFO] - Training Epoch: 2/10, step 304/574 completed (loss: 0.31531980633735657, acc: 0.875)
[2025-01-06 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19][root][INFO] - Training Epoch: 2/10, step 305/574 completed (loss: 0.5841683149337769, acc: 0.8196721076965332)
[2025-01-06 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20][root][INFO] - Training Epoch: 2/10, step 306/574 completed (loss: 0.15854591131210327, acc: 0.9666666388511658)
[2025-01-06 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20][root][INFO] - Training Epoch: 2/10, step 307/574 completed (loss: 0.03738139942288399, acc: 1.0)
[2025-01-06 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20][root][INFO] - Training Epoch: 2/10, step 308/574 completed (loss: 0.47030794620513916, acc: 0.8695651888847351)
[2025-01-06 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21][root][INFO] - Training Epoch: 2/10, step 309/574 completed (loss: 0.5245707631111145, acc: 0.8888888955116272)
[2025-01-06 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21][root][INFO] - Training Epoch: 2/10, step 310/574 completed (loss: 0.39003437757492065, acc: 0.8674699068069458)
[2025-01-06 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22][root][INFO] - Training Epoch: 2/10, step 311/574 completed (loss: 0.5097469687461853, acc: 0.807692289352417)
[2025-01-06 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22][root][INFO] - Training Epoch: 2/10, step 312/574 completed (loss: 0.16562628746032715, acc: 0.9591836929321289)
[2025-01-06 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22][root][INFO] - Training Epoch: 2/10, step 313/574 completed (loss: 0.0333632193505764, acc: 1.0)
[2025-01-06 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23][root][INFO] - Training Epoch: 2/10, step 314/574 completed (loss: 0.224751815199852, acc: 0.9166666865348816)
[2025-01-06 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23][root][INFO] - Training Epoch: 2/10, step 315/574 completed (loss: 0.3069223463535309, acc: 0.9677419066429138)
[2025-01-06 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23][root][INFO] - Training Epoch: 2/10, step 316/574 completed (loss: 0.7794902920722961, acc: 0.774193525314331)
[2025-01-06 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24][root][INFO] - Training Epoch: 2/10, step 317/574 completed (loss: 0.3509006202220917, acc: 0.8805969953536987)
[2025-01-06 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24][root][INFO] - Training Epoch: 2/10, step 318/574 completed (loss: 0.1283833235502243, acc: 0.9615384340286255)
[2025-01-06 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24][root][INFO] - Training Epoch: 2/10, step 319/574 completed (loss: 0.3025926351547241, acc: 0.9333333373069763)
[2025-01-06 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25][root][INFO] - Training Epoch: 2/10, step 320/574 completed (loss: 0.29721564054489136, acc: 0.9193548560142517)
[2025-01-06 01:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25][root][INFO] - Training Epoch: 2/10, step 321/574 completed (loss: 0.02706492692232132, acc: 1.0)
[2025-01-06 01:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25][root][INFO] - Training Epoch: 2/10, step 322/574 completed (loss: 1.6276440620422363, acc: 0.5925925970077515)
[2025-01-06 01:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26][root][INFO] - Training Epoch: 2/10, step 323/574 completed (loss: 2.1261789798736572, acc: 0.4285714328289032)
[2025-01-06 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26][root][INFO] - Training Epoch: 2/10, step 324/574 completed (loss: 1.5780341625213623, acc: 0.6410256624221802)
[2025-01-06 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26][root][INFO] - Training Epoch: 2/10, step 325/574 completed (loss: 1.6940350532531738, acc: 0.6341463327407837)
[2025-01-06 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27][root][INFO] - Training Epoch: 2/10, step 326/574 completed (loss: 1.481992483139038, acc: 0.6052631735801697)
[2025-01-06 01:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27][root][INFO] - Training Epoch: 2/10, step 327/574 completed (loss: 0.5474200248718262, acc: 0.8421052694320679)
[2025-01-06 01:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27][root][INFO] - Training Epoch: 2/10, step 328/574 completed (loss: 0.12916651368141174, acc: 0.9642857313156128)
[2025-01-06 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:28][root][INFO] - Training Epoch: 2/10, step 329/574 completed (loss: 0.2567994296550751, acc: 0.8888888955116272)
[2025-01-06 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:28][root][INFO] - Training Epoch: 2/10, step 330/574 completed (loss: 0.09706132858991623, acc: 0.96875)
[2025-01-06 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29][root][INFO] - Training Epoch: 2/10, step 331/574 completed (loss: 0.36778780817985535, acc: 0.8870967626571655)
[2025-01-06 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29][root][INFO] - Training Epoch: 2/10, step 332/574 completed (loss: 0.2047010362148285, acc: 0.9473684430122375)
[2025-01-06 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29][root][INFO] - Training Epoch: 2/10, step 333/574 completed (loss: 0.6009753942489624, acc: 0.84375)
[2025-01-06 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30][root][INFO] - Training Epoch: 2/10, step 334/574 completed (loss: 0.2603321373462677, acc: 0.9333333373069763)
[2025-01-06 01:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30][root][INFO] - Training Epoch: 2/10, step 335/574 completed (loss: 0.5891343951225281, acc: 0.8947368264198303)
[2025-01-06 01:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30][root][INFO] - Training Epoch: 2/10, step 336/574 completed (loss: 1.0494027137756348, acc: 0.6800000071525574)
[2025-01-06 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:31][root][INFO] - Training Epoch: 2/10, step 337/574 completed (loss: 1.4862301349639893, acc: 0.5862069129943848)
[2025-01-06 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:31][root][INFO] - Training Epoch: 2/10, step 338/574 completed (loss: 1.3762805461883545, acc: 0.5957446694374084)
[2025-01-06 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32][root][INFO] - Training Epoch: 2/10, step 339/574 completed (loss: 1.5745128393173218, acc: 0.6144578456878662)
[2025-01-06 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32][root][INFO] - Training Epoch: 2/10, step 340/574 completed (loss: 0.08595943450927734, acc: 1.0)
[2025-01-06 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32][root][INFO] - Training Epoch: 2/10, step 341/574 completed (loss: 0.7647905349731445, acc: 0.8205128312110901)
[2025-01-06 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33][root][INFO] - Training Epoch: 2/10, step 342/574 completed (loss: 0.4887314736843109, acc: 0.8554216623306274)
[2025-01-06 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33][root][INFO] - Training Epoch: 2/10, step 343/574 completed (loss: 0.8174465894699097, acc: 0.7735849022865295)
[2025-01-06 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33][root][INFO] - Training Epoch: 2/10, step 344/574 completed (loss: 0.19233040511608124, acc: 0.949367105960846)
[2025-01-06 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34][root][INFO] - Training Epoch: 2/10, step 345/574 completed (loss: 0.11620313674211502, acc: 0.9607843160629272)
[2025-01-06 01:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34][root][INFO] - Training Epoch: 2/10, step 346/574 completed (loss: 0.527898907661438, acc: 0.8805969953536987)
[2025-01-06 01:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34][root][INFO] - Training Epoch: 2/10, step 347/574 completed (loss: 0.15993864834308624, acc: 0.949999988079071)
[2025-01-06 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35][root][INFO] - Training Epoch: 2/10, step 348/574 completed (loss: 0.22145487368106842, acc: 0.9599999785423279)
[2025-01-06 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35][root][INFO] - Training Epoch: 2/10, step 349/574 completed (loss: 1.0039154291152954, acc: 0.7222222089767456)
[2025-01-06 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36][root][INFO] - Training Epoch: 2/10, step 350/574 completed (loss: 0.9951119422912598, acc: 0.7209302186965942)
[2025-01-06 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36][root][INFO] - Training Epoch: 2/10, step 351/574 completed (loss: 0.46675628423690796, acc: 0.8205128312110901)
[2025-01-06 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36][root][INFO] - Training Epoch: 2/10, step 352/574 completed (loss: 1.5320285558700562, acc: 0.5555555820465088)
[2025-01-06 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37][root][INFO] - Training Epoch: 2/10, step 353/574 completed (loss: 0.21157999336719513, acc: 0.95652174949646)
[2025-01-06 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37][root][INFO] - Training Epoch: 2/10, step 354/574 completed (loss: 0.47858163714408875, acc: 0.7692307829856873)
[2025-01-06 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37][root][INFO] - Training Epoch: 2/10, step 355/574 completed (loss: 1.0630406141281128, acc: 0.6813187003135681)
[2025-01-06 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38][root][INFO] - Training Epoch: 2/10, step 356/574 completed (loss: 0.8129240870475769, acc: 0.739130437374115)
[2025-01-06 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38][root][INFO] - Training Epoch: 2/10, step 357/574 completed (loss: 0.61314457654953, acc: 0.8260869383811951)
[2025-01-06 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39][root][INFO] - Training Epoch: 2/10, step 358/574 completed (loss: 0.8070095777511597, acc: 0.7551020383834839)
[2025-01-06 01:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39][root][INFO] - Training Epoch: 2/10, step 359/574 completed (loss: 0.011247295886278152, acc: 1.0)
[2025-01-06 01:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39][root][INFO] - Training Epoch: 2/10, step 360/574 completed (loss: 0.31717145442962646, acc: 0.9230769276618958)
[2025-01-06 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40][root][INFO] - Training Epoch: 2/10, step 361/574 completed (loss: 0.6410989761352539, acc: 0.7560975551605225)
[2025-01-06 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40][root][INFO] - Training Epoch: 2/10, step 362/574 completed (loss: 0.5756107568740845, acc: 0.8666666746139526)
[2025-01-06 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40][root][INFO] - Training Epoch: 2/10, step 363/574 completed (loss: 0.2690730690956116, acc: 0.9078947305679321)
[2025-01-06 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41][root][INFO] - Training Epoch: 2/10, step 364/574 completed (loss: 0.16566899418830872, acc: 0.9268292784690857)
[2025-01-06 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41][root][INFO] - Training Epoch: 2/10, step 365/574 completed (loss: 0.26001209020614624, acc: 0.9090909361839294)
[2025-01-06 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42][root][INFO] - Training Epoch: 2/10, step 366/574 completed (loss: 0.010090782307088375, acc: 1.0)
[2025-01-06 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42][root][INFO] - Training Epoch: 2/10, step 367/574 completed (loss: 0.09916984289884567, acc: 0.95652174949646)
[2025-01-06 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42][root][INFO] - Training Epoch: 2/10, step 368/574 completed (loss: 0.15661272406578064, acc: 0.9642857313156128)
[2025-01-06 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42][root][INFO] - Training Epoch: 2/10, step 369/574 completed (loss: 0.4494965076446533, acc: 0.84375)
[2025-01-06 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:43][root][INFO] - Training Epoch: 2/10, step 370/574 completed (loss: 0.6670113801956177, acc: 0.800000011920929)
[2025-01-06 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:44][root][INFO] - Training Epoch: 2/10, step 371/574 completed (loss: 0.49591344594955444, acc: 0.8679245114326477)
[2025-01-06 01:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:44][root][INFO] - Training Epoch: 2/10, step 372/574 completed (loss: 0.2848043739795685, acc: 0.9444444179534912)
[2025-01-06 01:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45][root][INFO] - Training Epoch: 2/10, step 373/574 completed (loss: 0.3176209032535553, acc: 0.9642857313156128)
[2025-01-06 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45][root][INFO] - Training Epoch: 2/10, step 374/574 completed (loss: 0.2478903979063034, acc: 0.9142857193946838)
[2025-01-06 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45][root][INFO] - Training Epoch: 2/10, step 375/574 completed (loss: 0.010029543191194534, acc: 1.0)
[2025-01-06 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46][root][INFO] - Training Epoch: 2/10, step 376/574 completed (loss: 0.09454314410686493, acc: 0.9130434989929199)
[2025-01-06 01:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46][root][INFO] - Training Epoch: 2/10, step 377/574 completed (loss: 0.17936229705810547, acc: 0.875)
[2025-01-06 01:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46][root][INFO] - Training Epoch: 2/10, step 378/574 completed (loss: 0.10418328642845154, acc: 0.9578947424888611)
[2025-01-06 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47][root][INFO] - Training Epoch: 2/10, step 379/574 completed (loss: 0.36025166511535645, acc: 0.910179615020752)
[2025-01-06 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47][root][INFO] - Training Epoch: 2/10, step 380/574 completed (loss: 0.4501027762889862, acc: 0.9097744226455688)
[2025-01-06 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49][root][INFO] - Training Epoch: 2/10, step 381/574 completed (loss: 0.6939969658851624, acc: 0.8235294222831726)
[2025-01-06 01:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49][root][INFO] - Training Epoch: 2/10, step 382/574 completed (loss: 0.2274595946073532, acc: 0.9459459185600281)
[2025-01-06 01:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49][root][INFO] - Training Epoch: 2/10, step 383/574 completed (loss: 0.528541624546051, acc: 0.8928571343421936)
[2025-01-06 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:50][root][INFO] - Training Epoch: 2/10, step 384/574 completed (loss: 0.06688641756772995, acc: 1.0)
[2025-01-06 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:50][root][INFO] - Training Epoch: 2/10, step 385/574 completed (loss: 0.0771106705069542, acc: 0.96875)
[2025-01-06 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51][root][INFO] - Training Epoch: 2/10, step 386/574 completed (loss: 0.024916164577007294, acc: 1.0)
[2025-01-06 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51][root][INFO] - Training Epoch: 2/10, step 387/574 completed (loss: 0.04450995475053787, acc: 0.9736841917037964)
[2025-01-06 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51][root][INFO] - Training Epoch: 2/10, step 388/574 completed (loss: 0.10319656878709793, acc: 0.9545454382896423)
[2025-01-06 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52][root][INFO] - Training Epoch: 2/10, step 389/574 completed (loss: 0.019262904301285744, acc: 1.0)
[2025-01-06 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52][root][INFO] - Training Epoch: 2/10, step 390/574 completed (loss: 0.31717580556869507, acc: 0.8571428656578064)
[2025-01-06 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52][root][INFO] - Training Epoch: 2/10, step 391/574 completed (loss: 1.328967571258545, acc: 0.6296296119689941)
[2025-01-06 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53][root][INFO] - Training Epoch: 2/10, step 392/574 completed (loss: 1.2751805782318115, acc: 0.6893203854560852)
[2025-01-06 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53][root][INFO] - Training Epoch: 2/10, step 393/574 completed (loss: 1.1472257375717163, acc: 0.7352941036224365)
[2025-01-06 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54][root][INFO] - Training Epoch: 2/10, step 394/574 completed (loss: 1.107508897781372, acc: 0.6399999856948853)
[2025-01-06 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54][root][INFO] - Training Epoch: 2/10, step 395/574 completed (loss: 1.0389156341552734, acc: 0.75)
[2025-01-06 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54][root][INFO] - Training Epoch: 2/10, step 396/574 completed (loss: 0.6651244759559631, acc: 0.8139534592628479)
[2025-01-06 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55][root][INFO] - Training Epoch: 2/10, step 397/574 completed (loss: 0.31915155053138733, acc: 0.875)
[2025-01-06 01:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55][root][INFO] - Training Epoch: 2/10, step 398/574 completed (loss: 0.31795263290405273, acc: 0.930232584476471)
[2025-01-06 01:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55][root][INFO] - Training Epoch: 2/10, step 399/574 completed (loss: 0.2779463529586792, acc: 0.9599999785423279)
[2025-01-06 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56][root][INFO] - Training Epoch: 2/10, step 400/574 completed (loss: 0.363051176071167, acc: 0.9117646813392639)
[2025-01-06 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56][root][INFO] - Training Epoch: 2/10, step 401/574 completed (loss: 0.6040917038917542, acc: 0.800000011920929)
[2025-01-06 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57][root][INFO] - Training Epoch: 2/10, step 402/574 completed (loss: 0.8297485113143921, acc: 0.8181818127632141)
[2025-01-06 01:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57][root][INFO] - Training Epoch: 2/10, step 403/574 completed (loss: 0.3363214433193207, acc: 0.9090909361839294)
[2025-01-06 01:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57][root][INFO] - Training Epoch: 2/10, step 404/574 completed (loss: 0.4623267948627472, acc: 0.8709677457809448)
[2025-01-06 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58][root][INFO] - Training Epoch: 2/10, step 405/574 completed (loss: 0.12635573744773865, acc: 0.9629629850387573)
[2025-01-06 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58][root][INFO] - Training Epoch: 2/10, step 406/574 completed (loss: 0.2146584689617157, acc: 0.9599999785423279)
[2025-01-06 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58][root][INFO] - Training Epoch: 2/10, step 407/574 completed (loss: 0.10952962934970856, acc: 0.9444444179534912)
[2025-01-06 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59][root][INFO] - Training Epoch: 2/10, step 408/574 completed (loss: 0.23821204900741577, acc: 0.9259259104728699)
[2025-01-06 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59][root][INFO] - Training Epoch: 2/10, step 409/574 completed (loss: 0.18722344934940338, acc: 1.0)
[2025-01-06 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59][root][INFO] - Training Epoch: 2/10, step 410/574 completed (loss: 0.21929322183132172, acc: 0.9655172228813171)
[2025-01-06 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00][root][INFO] - Training Epoch: 2/10, step 411/574 completed (loss: 0.052036020904779434, acc: 1.0)
[2025-01-06 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00][root][INFO] - Training Epoch: 2/10, step 412/574 completed (loss: 0.19314555823802948, acc: 0.9666666388511658)
[2025-01-06 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01][root][INFO] - Training Epoch: 2/10, step 413/574 completed (loss: 0.2812613844871521, acc: 0.9696969985961914)
[2025-01-06 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01][root][INFO] - Training Epoch: 2/10, step 414/574 completed (loss: 0.10105549544095993, acc: 1.0)
[2025-01-06 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01][root][INFO] - Training Epoch: 2/10, step 415/574 completed (loss: 0.6512507200241089, acc: 0.8039215803146362)
[2025-01-06 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01][root][INFO] - Training Epoch: 2/10, step 416/574 completed (loss: 0.4154156446456909, acc: 0.8846153616905212)
[2025-01-06 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:02][root][INFO] - Training Epoch: 2/10, step 417/574 completed (loss: 0.32837915420532227, acc: 0.9444444179534912)
[2025-01-06 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:02][root][INFO] - Training Epoch: 2/10, step 418/574 completed (loss: 0.4806744158267975, acc: 0.875)
[2025-01-06 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03][root][INFO] - Training Epoch: 2/10, step 419/574 completed (loss: 0.4938703179359436, acc: 0.8999999761581421)
[2025-01-06 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03][root][INFO] - Training Epoch: 2/10, step 420/574 completed (loss: 0.0491812564432621, acc: 1.0)
[2025-01-06 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03][root][INFO] - Training Epoch: 2/10, step 421/574 completed (loss: 0.3204670250415802, acc: 0.9333333373069763)
[2025-01-06 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04][root][INFO] - Training Epoch: 2/10, step 422/574 completed (loss: 0.32321760058403015, acc: 0.90625)
[2025-01-06 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04][root][INFO] - Training Epoch: 2/10, step 423/574 completed (loss: 0.531399667263031, acc: 0.8888888955116272)
[2025-01-06 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04][root][INFO] - Training Epoch: 2/10, step 424/574 completed (loss: 0.5174068808555603, acc: 0.9259259104728699)
[2025-01-06 01:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:05][root][INFO] - Training Epoch: 2/10, step 425/574 completed (loss: 0.15421438217163086, acc: 0.9696969985961914)
[2025-01-06 01:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:05][root][INFO] - Training Epoch: 2/10, step 426/574 completed (loss: 0.0247255377471447, acc: 1.0)
[2025-01-06 01:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8975, device='cuda:0') eval_epoch_loss=tensor(0.6405, device='cuda:0') eval_epoch_acc=tensor(0.8231, device='cuda:0')
[2025-01-06 01:10:36][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:10:36][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:10:36][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.6405490040779114/model.pt
[2025-01-06 01:10:36][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:10:36][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6405490040779114
[2025-01-06 01:10:36][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8230716586112976
[2025-01-06 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36][root][INFO] - Training Epoch: 2/10, step 427/574 completed (loss: 0.2849588990211487, acc: 0.9189189076423645)
[2025-01-06 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37][root][INFO] - Training Epoch: 2/10, step 428/574 completed (loss: 0.04519073665142059, acc: 1.0)
[2025-01-06 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37][root][INFO] - Training Epoch: 2/10, step 429/574 completed (loss: 0.3486056625843048, acc: 0.9130434989929199)
[2025-01-06 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37][root][INFO] - Training Epoch: 2/10, step 430/574 completed (loss: 0.006068844348192215, acc: 1.0)
[2025-01-06 01:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:38][root][INFO] - Training Epoch: 2/10, step 431/574 completed (loss: 0.015352661721408367, acc: 1.0)
[2025-01-06 01:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:38][root][INFO] - Training Epoch: 2/10, step 432/574 completed (loss: 0.29622504115104675, acc: 0.95652174949646)
[2025-01-06 01:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39][root][INFO] - Training Epoch: 2/10, step 433/574 completed (loss: 0.28522443771362305, acc: 0.9166666865348816)
[2025-01-06 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39][root][INFO] - Training Epoch: 2/10, step 434/574 completed (loss: 0.0019678231328725815, acc: 1.0)
[2025-01-06 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39][root][INFO] - Training Epoch: 2/10, step 435/574 completed (loss: 0.04568886384367943, acc: 1.0)
[2025-01-06 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40][root][INFO] - Training Epoch: 2/10, step 436/574 completed (loss: 0.5636825561523438, acc: 0.8333333134651184)
[2025-01-06 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40][root][INFO] - Training Epoch: 2/10, step 437/574 completed (loss: 0.09642784297466278, acc: 0.9772727489471436)
[2025-01-06 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40][root][INFO] - Training Epoch: 2/10, step 438/574 completed (loss: 0.016112664714455605, acc: 1.0)
[2025-01-06 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41][root][INFO] - Training Epoch: 2/10, step 439/574 completed (loss: 0.4888823926448822, acc: 0.8717948794364929)
[2025-01-06 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41][root][INFO] - Training Epoch: 2/10, step 440/574 completed (loss: 0.5602733492851257, acc: 0.8484848737716675)
[2025-01-06 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42][root][INFO] - Training Epoch: 2/10, step 441/574 completed (loss: 1.036765694618225, acc: 0.7039999961853027)
[2025-01-06 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42][root][INFO] - Training Epoch: 2/10, step 442/574 completed (loss: 1.0389773845672607, acc: 0.7338709831237793)
[2025-01-06 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:43][root][INFO] - Training Epoch: 2/10, step 443/574 completed (loss: 0.681837260723114, acc: 0.8407959938049316)
[2025-01-06 01:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:43][root][INFO] - Training Epoch: 2/10, step 444/574 completed (loss: 0.40219399333000183, acc: 0.849056601524353)
[2025-01-06 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44][root][INFO] - Training Epoch: 2/10, step 445/574 completed (loss: 0.2313428521156311, acc: 0.9090909361839294)
[2025-01-06 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44][root][INFO] - Training Epoch: 2/10, step 446/574 completed (loss: 0.38690119981765747, acc: 0.8695651888847351)
[2025-01-06 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44][root][INFO] - Training Epoch: 2/10, step 447/574 completed (loss: 0.6503881216049194, acc: 0.8461538553237915)
[2025-01-06 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45][root][INFO] - Training Epoch: 2/10, step 448/574 completed (loss: 0.24027416110038757, acc: 0.9642857313156128)
[2025-01-06 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45][root][INFO] - Training Epoch: 2/10, step 449/574 completed (loss: 0.2125880867242813, acc: 0.9253731369972229)
[2025-01-06 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45][root][INFO] - Training Epoch: 2/10, step 450/574 completed (loss: 0.07370167970657349, acc: 1.0)
[2025-01-06 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46][root][INFO] - Training Epoch: 2/10, step 451/574 completed (loss: 0.12399082630872726, acc: 0.945652186870575)
[2025-01-06 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46][root][INFO] - Training Epoch: 2/10, step 452/574 completed (loss: 0.3591907322406769, acc: 0.8974359035491943)
[2025-01-06 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47][root][INFO] - Training Epoch: 2/10, step 453/574 completed (loss: 0.40343764424324036, acc: 0.8815789222717285)
[2025-01-06 01:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47][root][INFO] - Training Epoch: 2/10, step 454/574 completed (loss: 0.22122415900230408, acc: 0.9387755393981934)
[2025-01-06 01:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47][root][INFO] - Training Epoch: 2/10, step 455/574 completed (loss: 0.34365034103393555, acc: 0.939393937587738)
[2025-01-06 01:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48][root][INFO] - Training Epoch: 2/10, step 456/574 completed (loss: 0.6678654551506042, acc: 0.8350515365600586)
[2025-01-06 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48][root][INFO] - Training Epoch: 2/10, step 457/574 completed (loss: 0.07520558685064316, acc: 0.9714285731315613)
[2025-01-06 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48][root][INFO] - Training Epoch: 2/10, step 458/574 completed (loss: 0.5238818526268005, acc: 0.854651153087616)
[2025-01-06 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49][root][INFO] - Training Epoch: 2/10, step 459/574 completed (loss: 0.11464543640613556, acc: 0.9464285969734192)
[2025-01-06 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49][root][INFO] - Training Epoch: 2/10, step 460/574 completed (loss: 0.3232545554637909, acc: 0.9382715821266174)
[2025-01-06 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49][root][INFO] - Training Epoch: 2/10, step 461/574 completed (loss: 0.35763970017433167, acc: 0.8888888955116272)
[2025-01-06 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50][root][INFO] - Training Epoch: 2/10, step 462/574 completed (loss: 0.12366125732660294, acc: 0.96875)
[2025-01-06 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50][root][INFO] - Training Epoch: 2/10, step 463/574 completed (loss: 0.5497692823410034, acc: 0.8461538553237915)
[2025-01-06 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51][root][INFO] - Training Epoch: 2/10, step 464/574 completed (loss: 0.3248831629753113, acc: 0.9130434989929199)
[2025-01-06 01:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51][root][INFO] - Training Epoch: 2/10, step 465/574 completed (loss: 0.438041627407074, acc: 0.8690476417541504)
[2025-01-06 01:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51][root][INFO] - Training Epoch: 2/10, step 466/574 completed (loss: 0.6879991888999939, acc: 0.8554216623306274)
[2025-01-06 01:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52][root][INFO] - Training Epoch: 2/10, step 467/574 completed (loss: 0.2989368736743927, acc: 0.8738738894462585)
[2025-01-06 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52][root][INFO] - Training Epoch: 2/10, step 468/574 completed (loss: 0.8723134994506836, acc: 0.7961165308952332)
[2025-01-06 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52][root][INFO] - Training Epoch: 2/10, step 469/574 completed (loss: 0.6739899516105652, acc: 0.8211382031440735)
[2025-01-06 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53][root][INFO] - Training Epoch: 2/10, step 470/574 completed (loss: 0.194166898727417, acc: 0.9166666865348816)
[2025-01-06 01:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53][root][INFO] - Training Epoch: 2/10, step 471/574 completed (loss: 0.49355369806289673, acc: 0.8571428656578064)
[2025-01-06 01:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53][root][INFO] - Training Epoch: 2/10, step 472/574 completed (loss: 0.7360285520553589, acc: 0.7941176295280457)
[2025-01-06 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54][root][INFO] - Training Epoch: 2/10, step 473/574 completed (loss: 0.9375327229499817, acc: 0.7423580884933472)
[2025-01-06 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54][root][INFO] - Training Epoch: 2/10, step 474/574 completed (loss: 0.6587620377540588, acc: 0.7708333134651184)
[2025-01-06 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55][root][INFO] - Training Epoch: 2/10, step 475/574 completed (loss: 0.4416588544845581, acc: 0.8527607321739197)
[2025-01-06 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55][root][INFO] - Training Epoch: 2/10, step 476/574 completed (loss: 0.5558565855026245, acc: 0.8345323801040649)
[2025-01-06 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55][root][INFO] - Training Epoch: 2/10, step 477/574 completed (loss: 0.9334056377410889, acc: 0.7537688612937927)
[2025-01-06 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56][root][INFO] - Training Epoch: 2/10, step 478/574 completed (loss: 0.5687565803527832, acc: 0.8055555820465088)
[2025-01-06 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56][root][INFO] - Training Epoch: 2/10, step 479/574 completed (loss: 0.7035488486289978, acc: 0.7878788113594055)
[2025-01-06 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56][root][INFO] - Training Epoch: 2/10, step 480/574 completed (loss: 0.379312664270401, acc: 0.8518518805503845)
[2025-01-06 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57][root][INFO] - Training Epoch: 2/10, step 481/574 completed (loss: 0.5532132387161255, acc: 0.800000011920929)
[2025-01-06 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57][root][INFO] - Training Epoch: 2/10, step 482/574 completed (loss: 0.6576956510543823, acc: 0.75)
[2025-01-06 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57][root][INFO] - Training Epoch: 2/10, step 483/574 completed (loss: 0.874003529548645, acc: 0.6551724076271057)
[2025-01-06 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58][root][INFO] - Training Epoch: 2/10, step 484/574 completed (loss: 0.14652971923351288, acc: 0.9032257795333862)
[2025-01-06 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58][root][INFO] - Training Epoch: 2/10, step 485/574 completed (loss: 0.4842515289783478, acc: 0.8421052694320679)
[2025-01-06 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58][root][INFO] - Training Epoch: 2/10, step 486/574 completed (loss: 1.03984534740448, acc: 0.6666666865348816)
[2025-01-06 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59][root][INFO] - Training Epoch: 2/10, step 487/574 completed (loss: 0.6370152831077576, acc: 0.8571428656578064)
[2025-01-06 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59][root][INFO] - Training Epoch: 2/10, step 488/574 completed (loss: 0.6591618061065674, acc: 0.7272727489471436)
[2025-01-06 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00][root][INFO] - Training Epoch: 2/10, step 489/574 completed (loss: 1.1156779527664185, acc: 0.7076923251152039)
[2025-01-06 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00][root][INFO] - Training Epoch: 2/10, step 490/574 completed (loss: 0.29634249210357666, acc: 0.8999999761581421)
[2025-01-06 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00][root][INFO] - Training Epoch: 2/10, step 491/574 completed (loss: 0.6492756009101868, acc: 0.8275862336158752)
[2025-01-06 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01][root][INFO] - Training Epoch: 2/10, step 492/574 completed (loss: 0.5760522484779358, acc: 0.7647058963775635)
[2025-01-06 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01][root][INFO] - Training Epoch: 2/10, step 493/574 completed (loss: 0.6068443655967712, acc: 0.8275862336158752)
[2025-01-06 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01][root][INFO] - Training Epoch: 2/10, step 494/574 completed (loss: 0.6140942573547363, acc: 0.8947368264198303)
[2025-01-06 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01][root][INFO] - Training Epoch: 2/10, step 495/574 completed (loss: 0.8346094489097595, acc: 0.7368420958518982)
[2025-01-06 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02][root][INFO] - Training Epoch: 2/10, step 496/574 completed (loss: 0.8245750069618225, acc: 0.7946428656578064)
[2025-01-06 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02][root][INFO] - Training Epoch: 2/10, step 497/574 completed (loss: 0.5816889405250549, acc: 0.8089887499809265)
[2025-01-06 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03][root][INFO] - Training Epoch: 2/10, step 498/574 completed (loss: 0.8450591564178467, acc: 0.7415730357170105)
[2025-01-06 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03][root][INFO] - Training Epoch: 2/10, step 499/574 completed (loss: 1.4758129119873047, acc: 0.6028369069099426)
[2025-01-06 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03][root][INFO] - Training Epoch: 2/10, step 500/574 completed (loss: 0.9911988973617554, acc: 0.75)
[2025-01-06 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:04][root][INFO] - Training Epoch: 2/10, step 501/574 completed (loss: 0.14787045121192932, acc: 0.9599999785423279)
[2025-01-06 01:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:04][root][INFO] - Training Epoch: 2/10, step 502/574 completed (loss: 0.07813665270805359, acc: 0.9615384340286255)
[2025-01-06 01:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:04][root][INFO] - Training Epoch: 2/10, step 503/574 completed (loss: 0.21526287496089935, acc: 0.9629629850387573)
[2025-01-06 01:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:05][root][INFO] - Training Epoch: 2/10, step 504/574 completed (loss: 0.14470598101615906, acc: 0.9259259104728699)
[2025-01-06 01:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:05][root][INFO] - Training Epoch: 2/10, step 505/574 completed (loss: 0.7612183690071106, acc: 0.849056601524353)
[2025-01-06 01:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:05][root][INFO] - Training Epoch: 2/10, step 506/574 completed (loss: 0.75859534740448, acc: 0.7931034564971924)
[2025-01-06 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:06][root][INFO] - Training Epoch: 2/10, step 507/574 completed (loss: 1.278651475906372, acc: 0.6486486196517944)
[2025-01-06 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:06][root][INFO] - Training Epoch: 2/10, step 508/574 completed (loss: 0.9559063911437988, acc: 0.7183098793029785)
[2025-01-06 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07][root][INFO] - Training Epoch: 2/10, step 509/574 completed (loss: 0.17709192633628845, acc: 0.949999988079071)
[2025-01-06 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07][root][INFO] - Training Epoch: 2/10, step 510/574 completed (loss: 0.3577384352684021, acc: 0.8333333134651184)
[2025-01-06 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07][root][INFO] - Training Epoch: 2/10, step 511/574 completed (loss: 0.47905606031417847, acc: 0.807692289352417)
[2025-01-06 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:10][root][INFO] - Training Epoch: 2/10, step 512/574 completed (loss: 1.3694146871566772, acc: 0.6071428656578064)
[2025-01-06 01:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11][root][INFO] - Training Epoch: 2/10, step 513/574 completed (loss: 0.2563062608242035, acc: 0.9126983880996704)
[2025-01-06 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11][root][INFO] - Training Epoch: 2/10, step 514/574 completed (loss: 0.7277806997299194, acc: 0.75)
[2025-01-06 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11][root][INFO] - Training Epoch: 2/10, step 515/574 completed (loss: 0.16635937988758087, acc: 0.9666666388511658)
[2025-01-06 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:12][root][INFO] - Training Epoch: 2/10, step 516/574 completed (loss: 0.6812318563461304, acc: 0.8333333134651184)
[2025-01-06 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:12][root][INFO] - Training Epoch: 2/10, step 517/574 completed (loss: 0.008811053819954395, acc: 1.0)
[2025-01-06 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13][root][INFO] - Training Epoch: 2/10, step 518/574 completed (loss: 0.18448109924793243, acc: 0.9032257795333862)
[2025-01-06 01:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13][root][INFO] - Training Epoch: 2/10, step 519/574 completed (loss: 0.3471771478652954, acc: 0.949999988079071)
[2025-01-06 01:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13][root][INFO] - Training Epoch: 2/10, step 520/574 completed (loss: 0.5741863250732422, acc: 0.8148148059844971)
[2025-01-06 01:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:14][root][INFO] - Training Epoch: 2/10, step 521/574 completed (loss: 0.8659738302230835, acc: 0.7542372941970825)
[2025-01-06 01:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:15][root][INFO] - Training Epoch: 2/10, step 522/574 completed (loss: 0.355327844619751, acc: 0.888059675693512)
[2025-01-06 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:15][root][INFO] - Training Epoch: 2/10, step 523/574 completed (loss: 0.4796633720397949, acc: 0.8394160866737366)
[2025-01-06 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16][root][INFO] - Training Epoch: 2/10, step 524/574 completed (loss: 0.8562946319580078, acc: 0.800000011920929)
[2025-01-06 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16][root][INFO] - Training Epoch: 2/10, step 525/574 completed (loss: 0.08796323090791702, acc: 0.9814814925193787)
[2025-01-06 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16][root][INFO] - Training Epoch: 2/10, step 526/574 completed (loss: 0.25693896412849426, acc: 0.9230769276618958)
[2025-01-06 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17][root][INFO] - Training Epoch: 2/10, step 527/574 completed (loss: 0.4258284866809845, acc: 0.8571428656578064)
[2025-01-06 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17][root][INFO] - Training Epoch: 2/10, step 528/574 completed (loss: 1.880699634552002, acc: 0.5737704634666443)
[2025-01-06 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17][root][INFO] - Training Epoch: 2/10, step 529/574 completed (loss: 0.34461790323257446, acc: 0.9322034120559692)
[2025-01-06 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18][root][INFO] - Training Epoch: 2/10, step 530/574 completed (loss: 1.5330902338027954, acc: 0.5581395626068115)
[2025-01-06 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18][root][INFO] - Training Epoch: 2/10, step 531/574 completed (loss: 0.9447256326675415, acc: 0.7727272510528564)
[2025-01-06 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18][root][INFO] - Training Epoch: 2/10, step 532/574 completed (loss: 1.2534576654434204, acc: 0.698113203048706)
[2025-01-06 01:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19][root][INFO] - Training Epoch: 2/10, step 533/574 completed (loss: 0.9677390456199646, acc: 0.7727272510528564)
[2025-01-06 01:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19][root][INFO] - Training Epoch: 2/10, step 534/574 completed (loss: 0.7308734059333801, acc: 0.800000011920929)
[2025-01-06 01:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19][root][INFO] - Training Epoch: 2/10, step 535/574 completed (loss: 0.5254901647567749, acc: 0.8999999761581421)
[2025-01-06 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20][root][INFO] - Training Epoch: 2/10, step 536/574 completed (loss: 0.2941884696483612, acc: 0.9090909361839294)
[2025-01-06 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20][root][INFO] - Training Epoch: 2/10, step 537/574 completed (loss: 0.7827628254890442, acc: 0.7692307829856873)
[2025-01-06 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21][root][INFO] - Training Epoch: 2/10, step 538/574 completed (loss: 0.5839024782180786, acc: 0.84375)
[2025-01-06 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21][root][INFO] - Training Epoch: 2/10, step 539/574 completed (loss: 0.5395469069480896, acc: 0.875)
[2025-01-06 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21][root][INFO] - Training Epoch: 2/10, step 540/574 completed (loss: 0.7892234921455383, acc: 0.7878788113594055)
[2025-01-06 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22][root][INFO] - Training Epoch: 2/10, step 541/574 completed (loss: 0.2698283791542053, acc: 0.875)
[2025-01-06 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22][root][INFO] - Training Epoch: 2/10, step 542/574 completed (loss: 0.07680760324001312, acc: 1.0)
[2025-01-06 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22][root][INFO] - Training Epoch: 2/10, step 543/574 completed (loss: 0.09416507929563522, acc: 0.95652174949646)
[2025-01-06 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23][root][INFO] - Training Epoch: 2/10, step 544/574 completed (loss: 0.15187953412532806, acc: 0.9666666388511658)
[2025-01-06 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23][root][INFO] - Training Epoch: 2/10, step 545/574 completed (loss: 0.145022451877594, acc: 0.9512194991111755)
[2025-01-06 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23][root][INFO] - Training Epoch: 2/10, step 546/574 completed (loss: 0.031694330275058746, acc: 1.0)
[2025-01-06 01:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24][root][INFO] - Training Epoch: 2/10, step 547/574 completed (loss: 0.026211680844426155, acc: 1.0)
[2025-01-06 01:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24][root][INFO] - Training Epoch: 2/10, step 548/574 completed (loss: 0.3324419856071472, acc: 0.9032257795333862)
[2025-01-06 01:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25][root][INFO] - Training Epoch: 2/10, step 549/574 completed (loss: 0.01596648432314396, acc: 1.0)
[2025-01-06 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25][root][INFO] - Training Epoch: 2/10, step 550/574 completed (loss: 0.34736567735671997, acc: 0.9090909361839294)
[2025-01-06 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25][root][INFO] - Training Epoch: 2/10, step 551/574 completed (loss: 0.14163705706596375, acc: 0.9750000238418579)
[2025-01-06 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26][root][INFO] - Training Epoch: 2/10, step 552/574 completed (loss: 0.19319598376750946, acc: 0.8999999761581421)
[2025-01-06 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26][root][INFO] - Training Epoch: 2/10, step 553/574 completed (loss: 0.546653687953949, acc: 0.8540145754814148)
[2025-01-06 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26][root][INFO] - Training Epoch: 2/10, step 554/574 completed (loss: 0.24139487743377686, acc: 0.9241379499435425)
[2025-01-06 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27][root][INFO] - Training Epoch: 2/10, step 555/574 completed (loss: 0.4271969199180603, acc: 0.8714285492897034)
[2025-01-06 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27][root][INFO] - Training Epoch: 2/10, step 556/574 completed (loss: 0.4633369743824005, acc: 0.887417197227478)
[2025-01-06 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27][root][INFO] - Training Epoch: 2/10, step 557/574 completed (loss: 0.24788255989551544, acc: 0.9059829115867615)
[2025-01-06 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:28][root][INFO] - Training Epoch: 2/10, step 558/574 completed (loss: 0.142043337225914, acc: 0.9200000166893005)
[2025-01-06 01:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:28][root][INFO] - Training Epoch: 2/10, step 559/574 completed (loss: 0.44133901596069336, acc: 0.9230769276618958)
[2025-01-06 01:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:28][root][INFO] - Training Epoch: 2/10, step 560/574 completed (loss: 0.019989589229226112, acc: 1.0)
[2025-01-06 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29][root][INFO] - Training Epoch: 2/10, step 561/574 completed (loss: 0.05176506191492081, acc: 1.0)
[2025-01-06 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29][root][INFO] - Training Epoch: 2/10, step 562/574 completed (loss: 0.5967127680778503, acc: 0.855555534362793)
[2025-01-06 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29][root][INFO] - Training Epoch: 2/10, step 563/574 completed (loss: 0.47364896535873413, acc: 0.8701298832893372)
[2025-01-06 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30][root][INFO] - Training Epoch: 2/10, step 564/574 completed (loss: 0.2598082423210144, acc: 0.8958333134651184)
[2025-01-06 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30][root][INFO] - Training Epoch: 2/10, step 565/574 completed (loss: 0.2278451770544052, acc: 0.8965517282485962)
[2025-01-06 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30][root][INFO] - Training Epoch: 2/10, step 566/574 completed (loss: 0.3654184937477112, acc: 0.9166666865348816)
[2025-01-06 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31][root][INFO] - Training Epoch: 2/10, step 567/574 completed (loss: 0.05148665979504585, acc: 1.0)
[2025-01-06 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31][root][INFO] - Training Epoch: 2/10, step 568/574 completed (loss: 0.07089527696371078, acc: 0.9629629850387573)
[2025-01-06 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:32][root][INFO] - Training Epoch: 2/10, step 569/574 completed (loss: 0.21853910386562347, acc: 0.9304812550544739)
[2025-01-06 01:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:02][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9307, device='cuda:0') eval_epoch_loss=tensor(0.6579, device='cuda:0') eval_epoch_acc=tensor(0.8247, device='cuda:0')
[2025-01-06 01:12:02][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:12:02][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:12:02][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_570_loss_0.657892107963562/model.pt
[2025-01-06 01:12:02][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:12:02][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8246538043022156
[2025-01-06 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03][root][INFO] - Training Epoch: 2/10, step 570/574 completed (loss: 0.058918263763189316, acc: 0.9677419066429138)
[2025-01-06 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03][root][INFO] - Training Epoch: 2/10, step 571/574 completed (loss: 0.35964691638946533, acc: 0.9230769276618958)
[2025-01-06 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03][root][INFO] - Training Epoch: 2/10, step 572/574 completed (loss: 0.42932558059692383, acc: 0.8673469424247742)
[2025-01-06 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:04][root][INFO] - Training Epoch: 2/10, step 573/574 completed (loss: 0.36012065410614014, acc: 0.8805031180381775)
[2025-01-06 01:12:04][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.9230, train_epoch_loss=0.6539, epoch time 356.3926611430943s
[2025-01-06 01:12:04][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-06 01:12:04][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:12:04][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-06 01:12:04][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-01-06 01:12:04][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05][root][INFO] - Training Epoch: 3/10, step 0/574 completed (loss: 0.2969715893268585, acc: 0.8888888955116272)
[2025-01-06 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05][root][INFO] - Training Epoch: 3/10, step 1/574 completed (loss: 0.46744704246520996, acc: 0.8799999952316284)
[2025-01-06 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06][root][INFO] - Training Epoch: 3/10, step 2/574 completed (loss: 0.8835070729255676, acc: 0.7567567825317383)
[2025-01-06 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06][root][INFO] - Training Epoch: 3/10, step 3/574 completed (loss: 0.48732316493988037, acc: 0.8947368264198303)
[2025-01-06 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06][root][INFO] - Training Epoch: 3/10, step 4/574 completed (loss: 0.6756794452667236, acc: 0.7837837934494019)
[2025-01-06 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07][root][INFO] - Training Epoch: 3/10, step 5/574 completed (loss: 0.27157095074653625, acc: 0.8928571343421936)
[2025-01-06 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07][root][INFO] - Training Epoch: 3/10, step 6/574 completed (loss: 1.055375099182129, acc: 0.6938775777816772)
[2025-01-06 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08][root][INFO] - Training Epoch: 3/10, step 7/574 completed (loss: 0.7153661847114563, acc: 0.8666666746139526)
[2025-01-06 01:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08][root][INFO] - Training Epoch: 3/10, step 8/574 completed (loss: 0.15766330063343048, acc: 0.9545454382896423)
[2025-01-06 01:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08][root][INFO] - Training Epoch: 3/10, step 9/574 completed (loss: 0.06890681385993958, acc: 0.9615384340286255)
[2025-01-06 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09][root][INFO] - Training Epoch: 3/10, step 10/574 completed (loss: 0.3150053024291992, acc: 0.9259259104728699)
[2025-01-06 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09][root][INFO] - Training Epoch: 3/10, step 11/574 completed (loss: 0.33812853693962097, acc: 0.8974359035491943)
[2025-01-06 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09][root][INFO] - Training Epoch: 3/10, step 12/574 completed (loss: 0.05262838676571846, acc: 1.0)
[2025-01-06 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10][root][INFO] - Training Epoch: 3/10, step 13/574 completed (loss: 0.18848365545272827, acc: 0.95652174949646)
[2025-01-06 01:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10][root][INFO] - Training Epoch: 3/10, step 14/574 completed (loss: 0.20208357274532318, acc: 0.9803921580314636)
[2025-01-06 01:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10][root][INFO] - Training Epoch: 3/10, step 15/574 completed (loss: 0.5153516530990601, acc: 0.8979591727256775)
[2025-01-06 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:11][root][INFO] - Training Epoch: 3/10, step 16/574 completed (loss: 0.16581639647483826, acc: 0.9473684430122375)
[2025-01-06 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:11][root][INFO] - Training Epoch: 3/10, step 17/574 completed (loss: 0.4369361102581024, acc: 0.875)
[2025-01-06 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12][root][INFO] - Training Epoch: 3/10, step 18/574 completed (loss: 0.725810170173645, acc: 0.7777777910232544)
[2025-01-06 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12][root][INFO] - Training Epoch: 3/10, step 19/574 completed (loss: 0.30993136763572693, acc: 0.8421052694320679)
[2025-01-06 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12][root][INFO] - Training Epoch: 3/10, step 20/574 completed (loss: 0.3086751699447632, acc: 0.9615384340286255)
[2025-01-06 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13][root][INFO] - Training Epoch: 3/10, step 21/574 completed (loss: 0.7116150856018066, acc: 0.8620689511299133)
[2025-01-06 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13][root][INFO] - Training Epoch: 3/10, step 22/574 completed (loss: 0.7018419504165649, acc: 0.8399999737739563)
[2025-01-06 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13][root][INFO] - Training Epoch: 3/10, step 23/574 completed (loss: 0.7727564573287964, acc: 0.8571428656578064)
[2025-01-06 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14][root][INFO] - Training Epoch: 3/10, step 24/574 completed (loss: 0.5306544899940491, acc: 0.875)
[2025-01-06 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14][root][INFO] - Training Epoch: 3/10, step 25/574 completed (loss: 0.7066053748130798, acc: 0.8301886916160583)
[2025-01-06 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14][root][INFO] - Training Epoch: 3/10, step 26/574 completed (loss: 0.8184294700622559, acc: 0.767123281955719)
[2025-01-06 01:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16][root][INFO] - Training Epoch: 3/10, step 27/574 completed (loss: 0.9461022615432739, acc: 0.747035562992096)
[2025-01-06 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16][root][INFO] - Training Epoch: 3/10, step 28/574 completed (loss: 0.4574422538280487, acc: 0.8139534592628479)
[2025-01-06 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16][root][INFO] - Training Epoch: 3/10, step 29/574 completed (loss: 0.6188654899597168, acc: 0.759036123752594)
[2025-01-06 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17][root][INFO] - Training Epoch: 3/10, step 30/574 completed (loss: 0.648086667060852, acc: 0.8148148059844971)
[2025-01-06 01:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17][root][INFO] - Training Epoch: 3/10, step 31/574 completed (loss: 0.6971855759620667, acc: 0.7857142686843872)
[2025-01-06 01:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17][root][INFO] - Training Epoch: 3/10, step 32/574 completed (loss: 0.3343809247016907, acc: 0.8888888955116272)
[2025-01-06 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18][root][INFO] - Training Epoch: 3/10, step 33/574 completed (loss: 0.07848891615867615, acc: 1.0)
[2025-01-06 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18][root][INFO] - Training Epoch: 3/10, step 34/574 completed (loss: 0.4710620641708374, acc: 0.8571428656578064)
[2025-01-06 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19][root][INFO] - Training Epoch: 3/10, step 35/574 completed (loss: 0.3867657482624054, acc: 0.9016393423080444)
[2025-01-06 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19][root][INFO] - Training Epoch: 3/10, step 36/574 completed (loss: 0.5857899188995361, acc: 0.8571428656578064)
[2025-01-06 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19][root][INFO] - Training Epoch: 3/10, step 37/574 completed (loss: 0.659912109375, acc: 0.8644067645072937)
[2025-01-06 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20][root][INFO] - Training Epoch: 3/10, step 38/574 completed (loss: 0.3222159147262573, acc: 0.8965517282485962)
[2025-01-06 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20][root][INFO] - Training Epoch: 3/10, step 39/574 completed (loss: 0.23712237179279327, acc: 0.9523809552192688)
[2025-01-06 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21][root][INFO] - Training Epoch: 3/10, step 40/574 completed (loss: 0.44764238595962524, acc: 0.8846153616905212)
[2025-01-06 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21][root][INFO] - Training Epoch: 3/10, step 41/574 completed (loss: 0.2723695635795593, acc: 0.9189189076423645)
[2025-01-06 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21][root][INFO] - Training Epoch: 3/10, step 42/574 completed (loss: 0.5640169978141785, acc: 0.8153846263885498)
[2025-01-06 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:22][root][INFO] - Training Epoch: 3/10, step 43/574 completed (loss: 0.5983984470367432, acc: 0.808080792427063)
[2025-01-06 01:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:22][root][INFO] - Training Epoch: 3/10, step 44/574 completed (loss: 0.3955545723438263, acc: 0.8865979313850403)
[2025-01-06 01:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:23][root][INFO] - Training Epoch: 3/10, step 45/574 completed (loss: 0.4528030455112457, acc: 0.875)
[2025-01-06 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:23][root][INFO] - Training Epoch: 3/10, step 46/574 completed (loss: 0.677653431892395, acc: 0.807692289352417)
[2025-01-06 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:23][root][INFO] - Training Epoch: 3/10, step 47/574 completed (loss: 0.25315889716148376, acc: 0.9629629850387573)
[2025-01-06 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:24][root][INFO] - Training Epoch: 3/10, step 48/574 completed (loss: 0.2792011797428131, acc: 0.9642857313156128)
[2025-01-06 01:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:24][root][INFO] - Training Epoch: 3/10, step 49/574 completed (loss: 0.07347944378852844, acc: 1.0)
[2025-01-06 01:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:24][root][INFO] - Training Epoch: 3/10, step 50/574 completed (loss: 0.7061207890510559, acc: 0.7894737124443054)
[2025-01-06 01:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:25][root][INFO] - Training Epoch: 3/10, step 51/574 completed (loss: 0.5107907652854919, acc: 0.8571428656578064)
[2025-01-06 01:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:25][root][INFO] - Training Epoch: 3/10, step 52/574 completed (loss: 0.912458062171936, acc: 0.7746478915214539)
[2025-01-06 01:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26][root][INFO] - Training Epoch: 3/10, step 53/574 completed (loss: 1.4076746702194214, acc: 0.5933333039283752)
[2025-01-06 01:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26][root][INFO] - Training Epoch: 3/10, step 54/574 completed (loss: 0.81767737865448, acc: 0.7837837934494019)
[2025-01-06 01:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26][root][INFO] - Training Epoch: 3/10, step 55/574 completed (loss: 0.10880298167467117, acc: 0.9615384340286255)
[2025-01-06 01:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:29][root][INFO] - Training Epoch: 3/10, step 56/574 completed (loss: 1.021549105644226, acc: 0.7098976373672485)
[2025-01-06 01:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:30][root][INFO] - Training Epoch: 3/10, step 57/574 completed (loss: 1.1707760095596313, acc: 0.673202633857727)
[2025-01-06 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:31][root][INFO] - Training Epoch: 3/10, step 58/574 completed (loss: 0.8751172423362732, acc: 0.7386363744735718)
[2025-01-06 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32][root][INFO] - Training Epoch: 3/10, step 59/574 completed (loss: 0.40065714716911316, acc: 0.8897058963775635)
[2025-01-06 01:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32][root][INFO] - Training Epoch: 3/10, step 60/574 completed (loss: 0.9432113170623779, acc: 0.7028985619544983)
[2025-01-06 01:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33][root][INFO] - Training Epoch: 3/10, step 61/574 completed (loss: 0.786537230014801, acc: 0.75)
[2025-01-06 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33][root][INFO] - Training Epoch: 3/10, step 62/574 completed (loss: 0.4225085973739624, acc: 0.9117646813392639)
[2025-01-06 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33][root][INFO] - Training Epoch: 3/10, step 63/574 completed (loss: 0.4055541455745697, acc: 0.8055555820465088)
[2025-01-06 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34][root][INFO] - Training Epoch: 3/10, step 64/574 completed (loss: 0.2126307338476181, acc: 0.9375)
[2025-01-06 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34][root][INFO] - Training Epoch: 3/10, step 65/574 completed (loss: 0.10122071951627731, acc: 1.0)
[2025-01-06 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34][root][INFO] - Training Epoch: 3/10, step 66/574 completed (loss: 0.7945946455001831, acc: 0.7678571343421936)
[2025-01-06 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35][root][INFO] - Training Epoch: 3/10, step 67/574 completed (loss: 0.4720187187194824, acc: 0.8333333134651184)
[2025-01-06 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35][root][INFO] - Training Epoch: 3/10, step 68/574 completed (loss: 0.022817080840468407, acc: 1.0)
[2025-01-06 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35][root][INFO] - Training Epoch: 3/10, step 69/574 completed (loss: 0.6695833206176758, acc: 0.7777777910232544)
[2025-01-06 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36][root][INFO] - Training Epoch: 3/10, step 70/574 completed (loss: 0.7335181832313538, acc: 0.7878788113594055)
[2025-01-06 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36][root][INFO] - Training Epoch: 3/10, step 71/574 completed (loss: 1.051248550415039, acc: 0.6985294222831726)
[2025-01-06 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37][root][INFO] - Training Epoch: 3/10, step 72/574 completed (loss: 0.7595371603965759, acc: 0.7857142686843872)
[2025-01-06 01:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37][root][INFO] - Training Epoch: 3/10, step 73/574 completed (loss: 1.3735201358795166, acc: 0.6307692527770996)
[2025-01-06 01:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37][root][INFO] - Training Epoch: 3/10, step 74/574 completed (loss: 1.1263517141342163, acc: 0.7142857313156128)
[2025-01-06 01:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:38][root][INFO] - Training Epoch: 3/10, step 75/574 completed (loss: 1.1627302169799805, acc: 0.7164179086685181)
[2025-01-06 01:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:38][root][INFO] - Training Epoch: 3/10, step 76/574 completed (loss: 1.440047264099121, acc: 0.6277372241020203)
[2025-01-06 01:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:39][root][INFO] - Training Epoch: 3/10, step 77/574 completed (loss: 0.03694751858711243, acc: 1.0)
[2025-01-06 01:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:39][root][INFO] - Training Epoch: 3/10, step 78/574 completed (loss: 0.17323656380176544, acc: 0.9583333134651184)
[2025-01-06 01:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:39][root][INFO] - Training Epoch: 3/10, step 79/574 completed (loss: 0.13632000982761383, acc: 0.9696969985961914)
[2025-01-06 01:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:40][root][INFO] - Training Epoch: 3/10, step 80/574 completed (loss: 0.3069800138473511, acc: 0.9230769276618958)
[2025-01-06 01:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:40][root][INFO] - Training Epoch: 3/10, step 81/574 completed (loss: 0.5213814377784729, acc: 0.807692289352417)
[2025-01-06 01:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:40][root][INFO] - Training Epoch: 3/10, step 82/574 completed (loss: 0.6798635721206665, acc: 0.807692289352417)
[2025-01-06 01:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41][root][INFO] - Training Epoch: 3/10, step 83/574 completed (loss: 0.35583215951919556, acc: 0.90625)
[2025-01-06 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41][root][INFO] - Training Epoch: 3/10, step 84/574 completed (loss: 0.37025511264801025, acc: 0.8985507488250732)
[2025-01-06 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41][root][INFO] - Training Epoch: 3/10, step 85/574 completed (loss: 0.4180171489715576, acc: 0.8600000143051147)
[2025-01-06 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:42][root][INFO] - Training Epoch: 3/10, step 86/574 completed (loss: 0.3969917297363281, acc: 0.8695651888847351)
[2025-01-06 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:42][root][INFO] - Training Epoch: 3/10, step 87/574 completed (loss: 0.8406510353088379, acc: 0.7599999904632568)
[2025-01-06 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:43][root][INFO] - Training Epoch: 3/10, step 88/574 completed (loss: 0.818023681640625, acc: 0.8155339956283569)
[2025-01-06 01:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:44][root][INFO] - Training Epoch: 3/10, step 89/574 completed (loss: 0.9349761605262756, acc: 0.7669903039932251)
[2025-01-06 01:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45][root][INFO] - Training Epoch: 3/10, step 90/574 completed (loss: 1.0387440919876099, acc: 0.7311828136444092)
[2025-01-06 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45][root][INFO] - Training Epoch: 3/10, step 91/574 completed (loss: 0.9956043362617493, acc: 0.7284482717514038)
[2025-01-06 01:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:46][root][INFO] - Training Epoch: 3/10, step 92/574 completed (loss: 0.6736940145492554, acc: 0.821052610874176)
[2025-01-06 01:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47][root][INFO] - Training Epoch: 3/10, step 93/574 completed (loss: 1.2249929904937744, acc: 0.603960394859314)
[2025-01-06 01:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47][root][INFO] - Training Epoch: 3/10, step 94/574 completed (loss: 1.118378758430481, acc: 0.6129032373428345)
[2025-01-06 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:48][root][INFO] - Training Epoch: 3/10, step 95/574 completed (loss: 0.8677001595497131, acc: 0.7101449370384216)
[2025-01-06 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:48][root][INFO] - Training Epoch: 3/10, step 96/574 completed (loss: 1.019187569618225, acc: 0.6386554837226868)
[2025-01-06 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49][root][INFO] - Training Epoch: 3/10, step 97/574 completed (loss: 1.1715366840362549, acc: 0.7019230723381042)
[2025-01-06 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49][root][INFO] - Training Epoch: 3/10, step 98/574 completed (loss: 1.2410436868667603, acc: 0.6350364685058594)
[2025-01-06 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49][root][INFO] - Training Epoch: 3/10, step 99/574 completed (loss: 1.31521737575531, acc: 0.5820895433425903)
[2025-01-06 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50][root][INFO] - Training Epoch: 3/10, step 100/574 completed (loss: 0.41302689909935, acc: 0.8999999761581421)
[2025-01-06 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50][root][INFO] - Training Epoch: 3/10, step 101/574 completed (loss: 0.014900913462042809, acc: 1.0)
[2025-01-06 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50][root][INFO] - Training Epoch: 3/10, step 102/574 completed (loss: 0.048842333257198334, acc: 1.0)
[2025-01-06 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51][root][INFO] - Training Epoch: 3/10, step 103/574 completed (loss: 0.03846347704529762, acc: 1.0)
[2025-01-06 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51][root][INFO] - Training Epoch: 3/10, step 104/574 completed (loss: 0.4750395119190216, acc: 0.8793103694915771)
[2025-01-06 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51][root][INFO] - Training Epoch: 3/10, step 105/574 completed (loss: 0.20411905646324158, acc: 0.9534883499145508)
[2025-01-06 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:52][root][INFO] - Training Epoch: 3/10, step 106/574 completed (loss: 0.3474701941013336, acc: 0.8799999952316284)
[2025-01-06 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:52][root][INFO] - Training Epoch: 3/10, step 107/574 completed (loss: 0.015886373817920685, acc: 1.0)
[2025-01-06 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:52][root][INFO] - Training Epoch: 3/10, step 108/574 completed (loss: 0.027669651433825493, acc: 1.0)
[2025-01-06 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53][root][INFO] - Training Epoch: 3/10, step 109/574 completed (loss: 0.04575137421488762, acc: 0.976190447807312)
[2025-01-06 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53][root][INFO] - Training Epoch: 3/10, step 110/574 completed (loss: 0.149763822555542, acc: 0.9384615421295166)
[2025-01-06 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54][root][INFO] - Training Epoch: 3/10, step 111/574 completed (loss: 0.4639698565006256, acc: 0.8771929740905762)
[2025-01-06 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54][root][INFO] - Training Epoch: 3/10, step 112/574 completed (loss: 0.7883179783821106, acc: 0.7543859481811523)
[2025-01-06 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54][root][INFO] - Training Epoch: 3/10, step 113/574 completed (loss: 0.5692649483680725, acc: 0.9230769276618958)
[2025-01-06 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55][root][INFO] - Training Epoch: 3/10, step 114/574 completed (loss: 0.3139324486255646, acc: 0.8979591727256775)
[2025-01-06 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55][root][INFO] - Training Epoch: 3/10, step 115/574 completed (loss: 0.03258367255330086, acc: 1.0)
[2025-01-06 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55][root][INFO] - Training Epoch: 3/10, step 116/574 completed (loss: 0.4621047377586365, acc: 0.8730158805847168)
[2025-01-06 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56][root][INFO] - Training Epoch: 3/10, step 117/574 completed (loss: 0.5142388343811035, acc: 0.8617886304855347)
[2025-01-06 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56][root][INFO] - Training Epoch: 3/10, step 118/574 completed (loss: 0.2767086625099182, acc: 0.9354838728904724)
[2025-01-06 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57][root][INFO] - Training Epoch: 3/10, step 119/574 completed (loss: 0.6306795477867126, acc: 0.8212927579879761)
[2025-01-06 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57][root][INFO] - Training Epoch: 3/10, step 120/574 completed (loss: 0.2916421890258789, acc: 0.9200000166893005)
[2025-01-06 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58][root][INFO] - Training Epoch: 3/10, step 121/574 completed (loss: 0.41610047221183777, acc: 0.942307710647583)
[2025-01-06 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58][root][INFO] - Training Epoch: 3/10, step 122/574 completed (loss: 0.21084065735340118, acc: 0.9166666865348816)
[2025-01-06 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58][root][INFO] - Training Epoch: 3/10, step 123/574 completed (loss: 0.19386200606822968, acc: 0.9473684430122375)
[2025-01-06 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:59][root][INFO] - Training Epoch: 3/10, step 124/574 completed (loss: 1.0558736324310303, acc: 0.7055214643478394)
[2025-01-06 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:59][root][INFO] - Training Epoch: 3/10, step 125/574 completed (loss: 1.0804702043533325, acc: 0.7152777910232544)
[2025-01-06 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:00][root][INFO] - Training Epoch: 3/10, step 126/574 completed (loss: 1.1419291496276855, acc: 0.699999988079071)
[2025-01-06 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:00][root][INFO] - Training Epoch: 3/10, step 127/574 completed (loss: 0.6826201677322388, acc: 0.7976190447807312)
[2025-01-06 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:00][root][INFO] - Training Epoch: 3/10, step 128/574 completed (loss: 0.7889321446418762, acc: 0.7794871926307678)
[2025-01-06 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01][root][INFO] - Training Epoch: 3/10, step 129/574 completed (loss: 0.9723509550094604, acc: 0.7426470518112183)
[2025-01-06 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01][root][INFO] - Training Epoch: 3/10, step 130/574 completed (loss: 0.5785620212554932, acc: 0.807692289352417)
[2025-01-06 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02][root][INFO] - Training Epoch: 3/10, step 131/574 completed (loss: 0.30657002329826355, acc: 0.8695651888847351)
[2025-01-06 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02][root][INFO] - Training Epoch: 3/10, step 132/574 completed (loss: 0.404623806476593, acc: 0.875)
[2025-01-06 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02][root][INFO] - Training Epoch: 3/10, step 133/574 completed (loss: 0.6955602765083313, acc: 0.8260869383811951)
[2025-01-06 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:03][root][INFO] - Training Epoch: 3/10, step 134/574 completed (loss: 0.3498804271221161, acc: 0.8857142925262451)
[2025-01-06 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:03][root][INFO] - Training Epoch: 3/10, step 135/574 completed (loss: 0.40919816493988037, acc: 0.8846153616905212)
[2025-01-06 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04][root][INFO] - Training Epoch: 3/10, step 136/574 completed (loss: 0.6360428929328918, acc: 0.8571428656578064)
[2025-01-06 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04][root][INFO] - Training Epoch: 3/10, step 137/574 completed (loss: 0.7994871139526367, acc: 0.7666666507720947)
[2025-01-06 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04][root][INFO] - Training Epoch: 3/10, step 138/574 completed (loss: 0.5126736164093018, acc: 0.8260869383811951)
[2025-01-06 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8659, device='cuda:0') eval_epoch_loss=tensor(0.6238, device='cuda:0') eval_epoch_acc=tensor(0.8318, device='cuda:0')
[2025-01-06 01:13:35][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:13:35][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:13:35][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_139_loss_0.6237507462501526/model.pt
[2025-01-06 01:13:35][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:13:35][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.6237507462501526
[2025-01-06 01:13:35][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.8318161964416504
[2025-01-06 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35][root][INFO] - Training Epoch: 3/10, step 139/574 completed (loss: 0.06416343152523041, acc: 1.0)
[2025-01-06 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36][root][INFO] - Training Epoch: 3/10, step 140/574 completed (loss: 0.16638115048408508, acc: 0.9615384340286255)
[2025-01-06 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36][root][INFO] - Training Epoch: 3/10, step 141/574 completed (loss: 0.5335115194320679, acc: 0.8387096524238586)
[2025-01-06 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:37][root][INFO] - Training Epoch: 3/10, step 142/574 completed (loss: 0.8274957537651062, acc: 0.7837837934494019)
[2025-01-06 01:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:37][root][INFO] - Training Epoch: 3/10, step 143/574 completed (loss: 0.8206576108932495, acc: 0.7280701994895935)
[2025-01-06 01:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:37][root][INFO] - Training Epoch: 3/10, step 144/574 completed (loss: 0.8491632342338562, acc: 0.7313432693481445)
[2025-01-06 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:38][root][INFO] - Training Epoch: 3/10, step 145/574 completed (loss: 0.6551363468170166, acc: 0.7653061151504517)
[2025-01-06 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:38][root][INFO] - Training Epoch: 3/10, step 146/574 completed (loss: 1.0641086101531982, acc: 0.6170212626457214)
[2025-01-06 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39][root][INFO] - Training Epoch: 3/10, step 147/574 completed (loss: 0.7201356887817383, acc: 0.7571428418159485)
[2025-01-06 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39][root][INFO] - Training Epoch: 3/10, step 148/574 completed (loss: 0.6557187438011169, acc: 0.6785714030265808)
[2025-01-06 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39][root][INFO] - Training Epoch: 3/10, step 149/574 completed (loss: 1.0667182207107544, acc: 0.739130437374115)
[2025-01-06 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40][root][INFO] - Training Epoch: 3/10, step 150/574 completed (loss: 0.43496695160865784, acc: 0.8620689511299133)
[2025-01-06 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40][root][INFO] - Training Epoch: 3/10, step 151/574 completed (loss: 0.8687807321548462, acc: 0.782608687877655)
[2025-01-06 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40][root][INFO] - Training Epoch: 3/10, step 152/574 completed (loss: 0.6702168583869934, acc: 0.8305084705352783)
[2025-01-06 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41][root][INFO] - Training Epoch: 3/10, step 153/574 completed (loss: 0.6868039965629578, acc: 0.8070175647735596)
[2025-01-06 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41][root][INFO] - Training Epoch: 3/10, step 154/574 completed (loss: 0.8243822455406189, acc: 0.7702702879905701)
[2025-01-06 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41][root][INFO] - Training Epoch: 3/10, step 155/574 completed (loss: 0.17373108863830566, acc: 0.8928571343421936)
[2025-01-06 01:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42][root][INFO] - Training Epoch: 3/10, step 156/574 completed (loss: 0.24244916439056396, acc: 0.9130434989929199)
[2025-01-06 01:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42][root][INFO] - Training Epoch: 3/10, step 157/574 completed (loss: 1.5861811637878418, acc: 0.5263158082962036)
[2025-01-06 01:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44][root][INFO] - Training Epoch: 3/10, step 158/574 completed (loss: 0.8428838849067688, acc: 0.7297297120094299)
[2025-01-06 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44][root][INFO] - Training Epoch: 3/10, step 159/574 completed (loss: 1.4368919134140015, acc: 0.5555555820465088)
[2025-01-06 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:45][root][INFO] - Training Epoch: 3/10, step 160/574 completed (loss: 1.1934137344360352, acc: 0.6976743936538696)
[2025-01-06 01:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:45][root][INFO] - Training Epoch: 3/10, step 161/574 completed (loss: 1.3912482261657715, acc: 0.6000000238418579)
[2025-01-06 01:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46][root][INFO] - Training Epoch: 3/10, step 162/574 completed (loss: 1.4005590677261353, acc: 0.6404494643211365)
[2025-01-06 01:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46][root][INFO] - Training Epoch: 3/10, step 163/574 completed (loss: 0.3488502502441406, acc: 0.9318181872367859)
[2025-01-06 01:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46][root][INFO] - Training Epoch: 3/10, step 164/574 completed (loss: 0.48737049102783203, acc: 0.9047619104385376)
[2025-01-06 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:47][root][INFO] - Training Epoch: 3/10, step 165/574 completed (loss: 0.730234682559967, acc: 0.7241379022598267)
[2025-01-06 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:47][root][INFO] - Training Epoch: 3/10, step 166/574 completed (loss: 0.20801587402820587, acc: 0.918367326259613)
[2025-01-06 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48][root][INFO] - Training Epoch: 3/10, step 167/574 completed (loss: 0.26969507336616516, acc: 0.9200000166893005)
[2025-01-06 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48][root][INFO] - Training Epoch: 3/10, step 168/574 completed (loss: 0.4657936096191406, acc: 0.875)
[2025-01-06 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48][root][INFO] - Training Epoch: 3/10, step 169/574 completed (loss: 0.9687838554382324, acc: 0.7156862616539001)
[2025-01-06 01:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:49][root][INFO] - Training Epoch: 3/10, step 170/574 completed (loss: 1.019410490989685, acc: 0.6849315166473389)
[2025-01-06 01:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50][root][INFO] - Training Epoch: 3/10, step 171/574 completed (loss: 0.07042814046144485, acc: 1.0)
[2025-01-06 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50][root][INFO] - Training Epoch: 3/10, step 172/574 completed (loss: 0.6944243907928467, acc: 0.7407407164573669)
[2025-01-06 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50][root][INFO] - Training Epoch: 3/10, step 173/574 completed (loss: 0.325202614068985, acc: 0.9285714030265808)
[2025-01-06 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:51][root][INFO] - Training Epoch: 3/10, step 174/574 completed (loss: 1.0099748373031616, acc: 0.7433628439903259)
[2025-01-06 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:51][root][INFO] - Training Epoch: 3/10, step 175/574 completed (loss: 0.6870454549789429, acc: 0.8115941882133484)
[2025-01-06 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:52][root][INFO] - Training Epoch: 3/10, step 176/574 completed (loss: 0.5225124955177307, acc: 0.8295454382896423)
[2025-01-06 01:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:53][root][INFO] - Training Epoch: 3/10, step 177/574 completed (loss: 1.1238247156143188, acc: 0.6793892979621887)
[2025-01-06 01:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:53][root][INFO] - Training Epoch: 3/10, step 178/574 completed (loss: 0.9420706033706665, acc: 0.7259259223937988)
[2025-01-06 01:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:54][root][INFO] - Training Epoch: 3/10, step 179/574 completed (loss: 0.5099349021911621, acc: 0.8360655903816223)
[2025-01-06 01:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:54][root][INFO] - Training Epoch: 3/10, step 180/574 completed (loss: 0.06360957771539688, acc: 0.9583333134651184)
[2025-01-06 01:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:54][root][INFO] - Training Epoch: 3/10, step 181/574 completed (loss: 0.05271025002002716, acc: 1.0)
[2025-01-06 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:55][root][INFO] - Training Epoch: 3/10, step 182/574 completed (loss: 0.23901258409023285, acc: 0.9285714030265808)
[2025-01-06 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:55][root][INFO] - Training Epoch: 3/10, step 183/574 completed (loss: 0.25371047854423523, acc: 0.9268292784690857)
[2025-01-06 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:55][root][INFO] - Training Epoch: 3/10, step 184/574 completed (loss: 0.4548562169075012, acc: 0.8942598104476929)
[2025-01-06 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:56][root][INFO] - Training Epoch: 3/10, step 185/574 completed (loss: 0.4758659899234772, acc: 0.8760806918144226)
[2025-01-06 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:56][root][INFO] - Training Epoch: 3/10, step 186/574 completed (loss: 0.43346577882766724, acc: 0.8687499761581421)
[2025-01-06 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57][root][INFO] - Training Epoch: 3/10, step 187/574 completed (loss: 0.47528621554374695, acc: 0.8742964267730713)
[2025-01-06 01:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57][root][INFO] - Training Epoch: 3/10, step 188/574 completed (loss: 0.5461955666542053, acc: 0.8612099885940552)
[2025-01-06 01:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58][root][INFO] - Training Epoch: 3/10, step 189/574 completed (loss: 0.18250232934951782, acc: 0.9200000166893005)
[2025-01-06 01:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58][root][INFO] - Training Epoch: 3/10, step 190/574 completed (loss: 0.9652294516563416, acc: 0.7325581312179565)
[2025-01-06 01:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:59][root][INFO] - Training Epoch: 3/10, step 191/574 completed (loss: 1.3182814121246338, acc: 0.6507936716079712)
[2025-01-06 01:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:00][root][INFO] - Training Epoch: 3/10, step 192/574 completed (loss: 1.0311870574951172, acc: 0.6439393758773804)
[2025-01-06 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:01][root][INFO] - Training Epoch: 3/10, step 193/574 completed (loss: 0.7787827253341675, acc: 0.7529411911964417)
[2025-01-06 01:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:02][root][INFO] - Training Epoch: 3/10, step 194/574 completed (loss: 1.0607200860977173, acc: 0.7222222089767456)
[2025-01-06 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03][root][INFO] - Training Epoch: 3/10, step 195/574 completed (loss: 0.44860169291496277, acc: 0.8225806355476379)
[2025-01-06 01:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03][root][INFO] - Training Epoch: 3/10, step 196/574 completed (loss: 0.14288724958896637, acc: 0.9642857313156128)
[2025-01-06 01:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03][root][INFO] - Training Epoch: 3/10, step 197/574 completed (loss: 0.8216242790222168, acc: 0.800000011920929)
[2025-01-06 01:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04][root][INFO] - Training Epoch: 3/10, step 198/574 completed (loss: 0.945226788520813, acc: 0.720588207244873)
[2025-01-06 01:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04][root][INFO] - Training Epoch: 3/10, step 199/574 completed (loss: 1.0020174980163574, acc: 0.7647058963775635)
[2025-01-06 01:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04][root][INFO] - Training Epoch: 3/10, step 200/574 completed (loss: 0.81917804479599, acc: 0.7372881174087524)
[2025-01-06 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:05][root][INFO] - Training Epoch: 3/10, step 201/574 completed (loss: 0.876707911491394, acc: 0.7910447716712952)
[2025-01-06 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:05][root][INFO] - Training Epoch: 3/10, step 202/574 completed (loss: 0.909838855266571, acc: 0.737864077091217)
[2025-01-06 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06][root][INFO] - Training Epoch: 3/10, step 203/574 completed (loss: 0.6117732524871826, acc: 0.8253968358039856)
[2025-01-06 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06][root][INFO] - Training Epoch: 3/10, step 204/574 completed (loss: 0.15180537104606628, acc: 0.9450549483299255)
[2025-01-06 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06][root][INFO] - Training Epoch: 3/10, step 205/574 completed (loss: 0.367413192987442, acc: 0.9058296084403992)
[2025-01-06 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07][root][INFO] - Training Epoch: 3/10, step 206/574 completed (loss: 0.451217919588089, acc: 0.8661417365074158)
[2025-01-06 01:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07][root][INFO] - Training Epoch: 3/10, step 207/574 completed (loss: 0.3234395980834961, acc: 0.9137930870056152)
[2025-01-06 01:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07][root][INFO] - Training Epoch: 3/10, step 208/574 completed (loss: 0.43919065594673157, acc: 0.8768116235733032)
[2025-01-06 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:08][root][INFO] - Training Epoch: 3/10, step 209/574 completed (loss: 0.3763013184070587, acc: 0.8949416279792786)
[2025-01-06 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:08][root][INFO] - Training Epoch: 3/10, step 210/574 completed (loss: 0.2100914716720581, acc: 0.945652186870575)
[2025-01-06 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09][root][INFO] - Training Epoch: 3/10, step 211/574 completed (loss: 0.1728481650352478, acc: 0.95652174949646)
[2025-01-06 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09][root][INFO] - Training Epoch: 3/10, step 212/574 completed (loss: 0.10350947827100754, acc: 0.9642857313156128)
[2025-01-06 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09][root][INFO] - Training Epoch: 3/10, step 213/574 completed (loss: 0.17529359459877014, acc: 0.957446813583374)
[2025-01-06 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:10][root][INFO] - Training Epoch: 3/10, step 214/574 completed (loss: 0.18679893016815186, acc: 0.9384615421295166)
[2025-01-06 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:10][root][INFO] - Training Epoch: 3/10, step 215/574 completed (loss: 0.21481485664844513, acc: 0.9459459185600281)
[2025-01-06 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11][root][INFO] - Training Epoch: 3/10, step 216/574 completed (loss: 0.19511905312538147, acc: 0.930232584476471)
[2025-01-06 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11][root][INFO] - Training Epoch: 3/10, step 217/574 completed (loss: 0.39241722226142883, acc: 0.9099099040031433)
[2025-01-06 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11][root][INFO] - Training Epoch: 3/10, step 218/574 completed (loss: 0.1735450029373169, acc: 0.9333333373069763)
[2025-01-06 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:12][root][INFO] - Training Epoch: 3/10, step 219/574 completed (loss: 0.16147339344024658, acc: 0.939393937587738)
[2025-01-06 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:12][root][INFO] - Training Epoch: 3/10, step 220/574 completed (loss: 0.045502670109272, acc: 1.0)
[2025-01-06 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13][root][INFO] - Training Epoch: 3/10, step 221/574 completed (loss: 0.09455527365207672, acc: 0.9599999785423279)
[2025-01-06 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13][root][INFO] - Training Epoch: 3/10, step 222/574 completed (loss: 0.6807976961135864, acc: 0.8269230723381042)
[2025-01-06 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14][root][INFO] - Training Epoch: 3/10, step 223/574 completed (loss: 0.3999723494052887, acc: 0.89673912525177)
[2025-01-06 01:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14][root][INFO] - Training Epoch: 3/10, step 224/574 completed (loss: 0.6766343712806702, acc: 0.8068181872367859)
[2025-01-06 01:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:15][root][INFO] - Training Epoch: 3/10, step 225/574 completed (loss: 0.9125653505325317, acc: 0.7553191781044006)
[2025-01-06 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:15][root][INFO] - Training Epoch: 3/10, step 226/574 completed (loss: 0.5562268495559692, acc: 0.849056601524353)
[2025-01-06 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:15][root][INFO] - Training Epoch: 3/10, step 227/574 completed (loss: 0.3264996409416199, acc: 0.8833333253860474)
[2025-01-06 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16][root][INFO] - Training Epoch: 3/10, step 228/574 completed (loss: 0.26599445939064026, acc: 0.930232584476471)
[2025-01-06 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16][root][INFO] - Training Epoch: 3/10, step 229/574 completed (loss: 0.8729885220527649, acc: 0.7666666507720947)
[2025-01-06 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16][root][INFO] - Training Epoch: 3/10, step 230/574 completed (loss: 1.8695404529571533, acc: 0.5473684072494507)
[2025-01-06 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17][root][INFO] - Training Epoch: 3/10, step 231/574 completed (loss: 1.4446157217025757, acc: 0.644444465637207)
[2025-01-06 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17][root][INFO] - Training Epoch: 3/10, step 232/574 completed (loss: 1.384101390838623, acc: 0.605555534362793)
[2025-01-06 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18][root][INFO] - Training Epoch: 3/10, step 233/574 completed (loss: 1.8265550136566162, acc: 0.5)
[2025-01-06 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18][root][INFO] - Training Epoch: 3/10, step 234/574 completed (loss: 1.348418951034546, acc: 0.6230769157409668)
[2025-01-06 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19][root][INFO] - Training Epoch: 3/10, step 235/574 completed (loss: 0.21431797742843628, acc: 0.8947368264198303)
[2025-01-06 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19][root][INFO] - Training Epoch: 3/10, step 236/574 completed (loss: 0.17529423534870148, acc: 0.9583333134651184)
[2025-01-06 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19][root][INFO] - Training Epoch: 3/10, step 237/574 completed (loss: 0.5626278519630432, acc: 0.8636363744735718)
[2025-01-06 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20][root][INFO] - Training Epoch: 3/10, step 238/574 completed (loss: 0.5852564573287964, acc: 0.8518518805503845)
[2025-01-06 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20][root][INFO] - Training Epoch: 3/10, step 239/574 completed (loss: 0.6190545558929443, acc: 0.800000011920929)
[2025-01-06 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20][root][INFO] - Training Epoch: 3/10, step 240/574 completed (loss: 0.9346797466278076, acc: 0.7954545617103577)
[2025-01-06 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:21][root][INFO] - Training Epoch: 3/10, step 241/574 completed (loss: 0.45315447449684143, acc: 0.8409090638160706)
[2025-01-06 01:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:21][root][INFO] - Training Epoch: 3/10, step 242/574 completed (loss: 1.0750685930252075, acc: 0.6774193644523621)
[2025-01-06 01:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22][root][INFO] - Training Epoch: 3/10, step 243/574 completed (loss: 0.8437646627426147, acc: 0.75)
[2025-01-06 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22][root][INFO] - Training Epoch: 3/10, step 244/574 completed (loss: 0.0362459160387516, acc: 1.0)
[2025-01-06 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22][root][INFO] - Training Epoch: 3/10, step 245/574 completed (loss: 0.21349546313285828, acc: 0.8846153616905212)
[2025-01-06 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23][root][INFO] - Training Epoch: 3/10, step 246/574 completed (loss: 0.36595457792282104, acc: 0.9032257795333862)
[2025-01-06 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23][root][INFO] - Training Epoch: 3/10, step 247/574 completed (loss: 0.11488036066293716, acc: 0.949999988079071)
[2025-01-06 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23][root][INFO] - Training Epoch: 3/10, step 248/574 completed (loss: 0.15973901748657227, acc: 0.9729729890823364)
[2025-01-06 01:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24][root][INFO] - Training Epoch: 3/10, step 249/574 completed (loss: 0.3648070991039276, acc: 0.9189189076423645)
[2025-01-06 01:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24][root][INFO] - Training Epoch: 3/10, step 250/574 completed (loss: 0.017631281167268753, acc: 1.0)
[2025-01-06 01:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24][root][INFO] - Training Epoch: 3/10, step 251/574 completed (loss: 0.18662510812282562, acc: 0.9264705777168274)
[2025-01-06 01:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25][root][INFO] - Training Epoch: 3/10, step 252/574 completed (loss: 0.0930001512169838, acc: 0.9268292784690857)
[2025-01-06 01:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25][root][INFO] - Training Epoch: 3/10, step 253/574 completed (loss: 0.024219265207648277, acc: 1.0)
[2025-01-06 01:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25][root][INFO] - Training Epoch: 3/10, step 254/574 completed (loss: 0.04437189921736717, acc: 0.9599999785423279)
[2025-01-06 01:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:26][root][INFO] - Training Epoch: 3/10, step 255/574 completed (loss: 0.08513307571411133, acc: 1.0)
[2025-01-06 01:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:26][root][INFO] - Training Epoch: 3/10, step 256/574 completed (loss: 0.2846541404724121, acc: 0.9122806787490845)
[2025-01-06 01:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27][root][INFO] - Training Epoch: 3/10, step 257/574 completed (loss: 0.11639082431793213, acc: 0.9571428298950195)
[2025-01-06 01:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27][root][INFO] - Training Epoch: 3/10, step 258/574 completed (loss: 0.15864861011505127, acc: 0.9473684430122375)
[2025-01-06 01:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27][root][INFO] - Training Epoch: 3/10, step 259/574 completed (loss: 0.33952006697654724, acc: 0.8867924809455872)
[2025-01-06 01:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28][root][INFO] - Training Epoch: 3/10, step 260/574 completed (loss: 0.42473042011260986, acc: 0.8999999761581421)
[2025-01-06 01:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28][root][INFO] - Training Epoch: 3/10, step 261/574 completed (loss: 0.14450155198574066, acc: 0.9444444179534912)
[2025-01-06 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29][root][INFO] - Training Epoch: 3/10, step 262/574 completed (loss: 0.3882101774215698, acc: 0.9032257795333862)
[2025-01-06 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29][root][INFO] - Training Epoch: 3/10, step 263/574 completed (loss: 1.0891592502593994, acc: 0.746666669845581)
[2025-01-06 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:30][root][INFO] - Training Epoch: 3/10, step 264/574 completed (loss: 0.6773910522460938, acc: 0.7916666865348816)
[2025-01-06 01:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:30][root][INFO] - Training Epoch: 3/10, step 265/574 completed (loss: 1.2856931686401367, acc: 0.6800000071525574)
[2025-01-06 01:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:31][root][INFO] - Training Epoch: 3/10, step 266/574 completed (loss: 1.3556040525436401, acc: 0.6853932738304138)
[2025-01-06 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:31][root][INFO] - Training Epoch: 3/10, step 267/574 completed (loss: 0.7885806560516357, acc: 0.7567567825317383)
[2025-01-06 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32][root][INFO] - Training Epoch: 3/10, step 268/574 completed (loss: 0.6928821206092834, acc: 0.7931034564971924)
[2025-01-06 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32][root][INFO] - Training Epoch: 3/10, step 269/574 completed (loss: 0.039059121161699295, acc: 1.0)
[2025-01-06 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32][root][INFO] - Training Epoch: 3/10, step 270/574 completed (loss: 0.10388296097517014, acc: 1.0)
[2025-01-06 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32][root][INFO] - Training Epoch: 3/10, step 271/574 completed (loss: 0.13035467267036438, acc: 0.96875)
[2025-01-06 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33][root][INFO] - Training Epoch: 3/10, step 272/574 completed (loss: 0.18902209401130676, acc: 0.9666666388511658)
[2025-01-06 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33][root][INFO] - Training Epoch: 3/10, step 273/574 completed (loss: 0.21485260128974915, acc: 0.9666666388511658)
[2025-01-06 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34][root][INFO] - Training Epoch: 3/10, step 274/574 completed (loss: 0.22690050303936005, acc: 0.96875)
[2025-01-06 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34][root][INFO] - Training Epoch: 3/10, step 275/574 completed (loss: 0.14762796461582184, acc: 0.9333333373069763)
[2025-01-06 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34][root][INFO] - Training Epoch: 3/10, step 276/574 completed (loss: 0.43226808309555054, acc: 0.8965517282485962)
[2025-01-06 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35][root][INFO] - Training Epoch: 3/10, step 277/574 completed (loss: 0.18403583765029907, acc: 0.9200000166893005)
[2025-01-06 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35][root][INFO] - Training Epoch: 3/10, step 278/574 completed (loss: 0.35200682282447815, acc: 0.8723404407501221)
[2025-01-06 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35][root][INFO] - Training Epoch: 3/10, step 279/574 completed (loss: 0.37936079502105713, acc: 0.8958333134651184)
[2025-01-06 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36][root][INFO] - Training Epoch: 3/10, step 280/574 completed (loss: 0.10405722260475159, acc: 0.9545454382896423)
[2025-01-06 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36][root][INFO] - Training Epoch: 3/10, step 281/574 completed (loss: 0.6354795098304749, acc: 0.8072289228439331)
[2025-01-06 01:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9138, device='cuda:0') eval_epoch_loss=tensor(0.6491, device='cuda:0') eval_epoch_acc=tensor(0.8248, device='cuda:0')
[2025-01-06 01:15:08][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:15:08][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:15:08][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_282_loss_0.6490678787231445/model.pt
[2025-01-06 01:15:08][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08][root][INFO] - Training Epoch: 3/10, step 282/574 completed (loss: 0.7938093543052673, acc: 0.7685185074806213)
[2025-01-06 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09][root][INFO] - Training Epoch: 3/10, step 283/574 completed (loss: 0.05642525851726532, acc: 1.0)
[2025-01-06 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09][root][INFO] - Training Epoch: 3/10, step 284/574 completed (loss: 0.20272275805473328, acc: 0.970588207244873)
[2025-01-06 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09][root][INFO] - Training Epoch: 3/10, step 285/574 completed (loss: 0.3243986666202545, acc: 0.949999988079071)
[2025-01-06 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10][root][INFO] - Training Epoch: 3/10, step 286/574 completed (loss: 0.5116139650344849, acc: 0.8828125)
[2025-01-06 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10][root][INFO] - Training Epoch: 3/10, step 287/574 completed (loss: 0.4497109353542328, acc: 0.8880000114440918)
[2025-01-06 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10][root][INFO] - Training Epoch: 3/10, step 288/574 completed (loss: 0.4709470868110657, acc: 0.8901098966598511)
[2025-01-06 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11][root][INFO] - Training Epoch: 3/10, step 289/574 completed (loss: 0.46812698245048523, acc: 0.8447204828262329)
[2025-01-06 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11][root][INFO] - Training Epoch: 3/10, step 290/574 completed (loss: 0.5253749489784241, acc: 0.8505154848098755)
[2025-01-06 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11][root][INFO] - Training Epoch: 3/10, step 291/574 completed (loss: 0.030947471037507057, acc: 1.0)
[2025-01-06 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12][root][INFO] - Training Epoch: 3/10, step 292/574 completed (loss: 0.2414332777261734, acc: 0.9523809552192688)
[2025-01-06 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12][root][INFO] - Training Epoch: 3/10, step 293/574 completed (loss: 0.1089262142777443, acc: 0.982758641242981)
[2025-01-06 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13][root][INFO] - Training Epoch: 3/10, step 294/574 completed (loss: 0.33350706100463867, acc: 0.9090909361839294)
[2025-01-06 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13][root][INFO] - Training Epoch: 3/10, step 295/574 completed (loss: 0.6376342177391052, acc: 0.8247422575950623)
[2025-01-06 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13][root][INFO] - Training Epoch: 3/10, step 296/574 completed (loss: 0.25727730989456177, acc: 0.931034505367279)
[2025-01-06 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:14][root][INFO] - Training Epoch: 3/10, step 297/574 completed (loss: 0.12968558073043823, acc: 0.9629629850387573)
[2025-01-06 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:14][root][INFO] - Training Epoch: 3/10, step 298/574 completed (loss: 0.31571024656295776, acc: 0.8684210777282715)
[2025-01-06 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15][root][INFO] - Training Epoch: 3/10, step 299/574 completed (loss: 0.09524931013584137, acc: 0.9821428656578064)
[2025-01-06 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15][root][INFO] - Training Epoch: 3/10, step 300/574 completed (loss: 0.07614228874444962, acc: 0.96875)
[2025-01-06 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15][root][INFO] - Training Epoch: 3/10, step 301/574 completed (loss: 0.2492920160293579, acc: 0.9622641801834106)
[2025-01-06 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16][root][INFO] - Training Epoch: 3/10, step 302/574 completed (loss: 0.011992232874035835, acc: 1.0)
[2025-01-06 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16][root][INFO] - Training Epoch: 3/10, step 303/574 completed (loss: 0.03665999695658684, acc: 1.0)
[2025-01-06 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16][root][INFO] - Training Epoch: 3/10, step 304/574 completed (loss: 0.07891428470611572, acc: 0.96875)
[2025-01-06 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17][root][INFO] - Training Epoch: 3/10, step 305/574 completed (loss: 0.2924841046333313, acc: 0.9344262480735779)
[2025-01-06 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17][root][INFO] - Training Epoch: 3/10, step 306/574 completed (loss: 0.044469866901636124, acc: 1.0)
[2025-01-06 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17][root][INFO] - Training Epoch: 3/10, step 307/574 completed (loss: 0.007169403601437807, acc: 1.0)
[2025-01-06 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18][root][INFO] - Training Epoch: 3/10, step 308/574 completed (loss: 0.2517654001712799, acc: 0.9275362491607666)
[2025-01-06 01:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18][root][INFO] - Training Epoch: 3/10, step 309/574 completed (loss: 0.2484862506389618, acc: 0.9305555820465088)
[2025-01-06 01:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19][root][INFO] - Training Epoch: 3/10, step 310/574 completed (loss: 0.1948668509721756, acc: 0.9397590160369873)
[2025-01-06 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19][root][INFO] - Training Epoch: 3/10, step 311/574 completed (loss: 0.289458692073822, acc: 0.8974359035491943)
[2025-01-06 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19][root][INFO] - Training Epoch: 3/10, step 312/574 completed (loss: 0.09563061594963074, acc: 0.9693877696990967)
[2025-01-06 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20][root][INFO] - Training Epoch: 3/10, step 313/574 completed (loss: 0.014484360814094543, acc: 1.0)
[2025-01-06 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20][root][INFO] - Training Epoch: 3/10, step 314/574 completed (loss: 0.03843170776963234, acc: 1.0)
[2025-01-06 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20][root][INFO] - Training Epoch: 3/10, step 315/574 completed (loss: 0.1864684671163559, acc: 0.9354838728904724)
[2025-01-06 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21][root][INFO] - Training Epoch: 3/10, step 316/574 completed (loss: 0.48675310611724854, acc: 0.8709677457809448)
[2025-01-06 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21][root][INFO] - Training Epoch: 3/10, step 317/574 completed (loss: 0.19571155309677124, acc: 0.9402984976768494)
[2025-01-06 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21][root][INFO] - Training Epoch: 3/10, step 318/574 completed (loss: 0.10043726861476898, acc: 0.9711538553237915)
[2025-01-06 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22][root][INFO] - Training Epoch: 3/10, step 319/574 completed (loss: 0.2206631302833557, acc: 0.9555555582046509)
[2025-01-06 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22][root][INFO] - Training Epoch: 3/10, step 320/574 completed (loss: 0.07792617380619049, acc: 0.9838709831237793)
[2025-01-06 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22][root][INFO] - Training Epoch: 3/10, step 321/574 completed (loss: 0.016495557501912117, acc: 1.0)
[2025-01-06 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:23][root][INFO] - Training Epoch: 3/10, step 322/574 completed (loss: 0.9523965120315552, acc: 0.7407407164573669)
[2025-01-06 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:23][root][INFO] - Training Epoch: 3/10, step 323/574 completed (loss: 1.0232012271881104, acc: 0.6857143044471741)
[2025-01-06 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24][root][INFO] - Training Epoch: 3/10, step 324/574 completed (loss: 1.1991486549377441, acc: 0.7179487347602844)
[2025-01-06 01:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24][root][INFO] - Training Epoch: 3/10, step 325/574 completed (loss: 1.1301724910736084, acc: 0.6341463327407837)
[2025-01-06 01:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24][root][INFO] - Training Epoch: 3/10, step 326/574 completed (loss: 0.7316701412200928, acc: 0.8421052694320679)
[2025-01-06 01:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25][root][INFO] - Training Epoch: 3/10, step 327/574 completed (loss: 0.2481403648853302, acc: 0.8947368264198303)
[2025-01-06 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25][root][INFO] - Training Epoch: 3/10, step 328/574 completed (loss: 0.058537568897008896, acc: 1.0)
[2025-01-06 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25][root][INFO] - Training Epoch: 3/10, step 329/574 completed (loss: 0.04279946908354759, acc: 1.0)
[2025-01-06 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26][root][INFO] - Training Epoch: 3/10, step 330/574 completed (loss: 0.02718840353190899, acc: 1.0)
[2025-01-06 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26][root][INFO] - Training Epoch: 3/10, step 331/574 completed (loss: 0.2427111566066742, acc: 0.9354838728904724)
[2025-01-06 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27][root][INFO] - Training Epoch: 3/10, step 332/574 completed (loss: 0.14751775562763214, acc: 0.9473684430122375)
[2025-01-06 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27][root][INFO] - Training Epoch: 3/10, step 333/574 completed (loss: 0.35041335225105286, acc: 0.9375)
[2025-01-06 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27][root][INFO] - Training Epoch: 3/10, step 334/574 completed (loss: 0.029620399698615074, acc: 1.0)
[2025-01-06 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28][root][INFO] - Training Epoch: 3/10, step 335/574 completed (loss: 0.043285269290208817, acc: 1.0)
[2025-01-06 01:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28][root][INFO] - Training Epoch: 3/10, step 336/574 completed (loss: 0.7131581902503967, acc: 0.7400000095367432)
[2025-01-06 01:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28][root][INFO] - Training Epoch: 3/10, step 337/574 completed (loss: 1.1159641742706299, acc: 0.6666666865348816)
[2025-01-06 01:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29][root][INFO] - Training Epoch: 3/10, step 338/574 completed (loss: 1.1144872903823853, acc: 0.6382978558540344)
[2025-01-06 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29][root][INFO] - Training Epoch: 3/10, step 339/574 completed (loss: 1.2136934995651245, acc: 0.6746987700462341)
[2025-01-06 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29][root][INFO] - Training Epoch: 3/10, step 340/574 completed (loss: 0.030933957546949387, acc: 1.0)
[2025-01-06 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30][root][INFO] - Training Epoch: 3/10, step 341/574 completed (loss: 0.3550207018852234, acc: 0.9230769276618958)
[2025-01-06 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30][root][INFO] - Training Epoch: 3/10, step 342/574 completed (loss: 0.3018559515476227, acc: 0.9156626462936401)
[2025-01-06 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30][root][INFO] - Training Epoch: 3/10, step 343/574 completed (loss: 0.3971070647239685, acc: 0.8867924809455872)
[2025-01-06 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31][root][INFO] - Training Epoch: 3/10, step 344/574 completed (loss: 0.13258399069309235, acc: 0.949367105960846)
[2025-01-06 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31][root][INFO] - Training Epoch: 3/10, step 345/574 completed (loss: 0.08560283482074738, acc: 0.9607843160629272)
[2025-01-06 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31][root][INFO] - Training Epoch: 3/10, step 346/574 completed (loss: 0.4651908576488495, acc: 0.8805969953536987)
[2025-01-06 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32][root][INFO] - Training Epoch: 3/10, step 347/574 completed (loss: 0.09901846200227737, acc: 0.949999988079071)
[2025-01-06 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32][root][INFO] - Training Epoch: 3/10, step 348/574 completed (loss: 0.1113874539732933, acc: 0.9599999785423279)
[2025-01-06 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33][root][INFO] - Training Epoch: 3/10, step 349/574 completed (loss: 0.7341336607933044, acc: 0.8333333134651184)
[2025-01-06 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33][root][INFO] - Training Epoch: 3/10, step 350/574 completed (loss: 0.5264135599136353, acc: 0.8372092843055725)
[2025-01-06 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33][root][INFO] - Training Epoch: 3/10, step 351/574 completed (loss: 0.2478056699037552, acc: 0.9230769276618958)
[2025-01-06 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34][root][INFO] - Training Epoch: 3/10, step 352/574 completed (loss: 0.8173856139183044, acc: 0.7111111283302307)
[2025-01-06 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34][root][INFO] - Training Epoch: 3/10, step 353/574 completed (loss: 0.04147307202219963, acc: 1.0)
[2025-01-06 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34][root][INFO] - Training Epoch: 3/10, step 354/574 completed (loss: 0.24682481586933136, acc: 0.9615384340286255)
[2025-01-06 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35][root][INFO] - Training Epoch: 3/10, step 355/574 completed (loss: 0.5860764384269714, acc: 0.8241758346557617)
[2025-01-06 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35][root][INFO] - Training Epoch: 3/10, step 356/574 completed (loss: 0.4918621778488159, acc: 0.8782608509063721)
[2025-01-06 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36][root][INFO] - Training Epoch: 3/10, step 357/574 completed (loss: 0.4580104649066925, acc: 0.8695651888847351)
[2025-01-06 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36][root][INFO] - Training Epoch: 3/10, step 358/574 completed (loss: 0.4927566945552826, acc: 0.8163265585899353)
[2025-01-06 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36][root][INFO] - Training Epoch: 3/10, step 359/574 completed (loss: 0.0029643706511706114, acc: 1.0)
[2025-01-06 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37][root][INFO] - Training Epoch: 3/10, step 360/574 completed (loss: 0.18181052803993225, acc: 0.9615384340286255)
[2025-01-06 01:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37][root][INFO] - Training Epoch: 3/10, step 361/574 completed (loss: 0.2690717875957489, acc: 0.9268292784690857)
[2025-01-06 01:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37][root][INFO] - Training Epoch: 3/10, step 362/574 completed (loss: 0.43291211128234863, acc: 0.9111111164093018)
[2025-01-06 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:38][root][INFO] - Training Epoch: 3/10, step 363/574 completed (loss: 0.12655746936798096, acc: 0.9473684430122375)
[2025-01-06 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:38][root][INFO] - Training Epoch: 3/10, step 364/574 completed (loss: 0.0991176962852478, acc: 0.9512194991111755)
[2025-01-06 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:39][root][INFO] - Training Epoch: 3/10, step 365/574 completed (loss: 0.0507643036544323, acc: 1.0)
[2025-01-06 01:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:39][root][INFO] - Training Epoch: 3/10, step 366/574 completed (loss: 0.01074989140033722, acc: 1.0)
[2025-01-06 01:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:39][root][INFO] - Training Epoch: 3/10, step 367/574 completed (loss: 0.10245286673307419, acc: 0.95652174949646)
[2025-01-06 01:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40][root][INFO] - Training Epoch: 3/10, step 368/574 completed (loss: 0.026752609759569168, acc: 1.0)
[2025-01-06 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40][root][INFO] - Training Epoch: 3/10, step 369/574 completed (loss: 0.17189617455005646, acc: 0.96875)
[2025-01-06 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:41][root][INFO] - Training Epoch: 3/10, step 370/574 completed (loss: 0.5395378470420837, acc: 0.842424213886261)
[2025-01-06 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42][root][INFO] - Training Epoch: 3/10, step 371/574 completed (loss: 0.36140546202659607, acc: 0.8679245114326477)
[2025-01-06 01:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42][root][INFO] - Training Epoch: 3/10, step 372/574 completed (loss: 0.20778796076774597, acc: 0.9222221970558167)
[2025-01-06 01:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42][root][INFO] - Training Epoch: 3/10, step 373/574 completed (loss: 0.16468189656734467, acc: 0.9642857313156128)
[2025-01-06 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:43][root][INFO] - Training Epoch: 3/10, step 374/574 completed (loss: 0.08020570129156113, acc: 0.9428571462631226)
[2025-01-06 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:43][root][INFO] - Training Epoch: 3/10, step 375/574 completed (loss: 0.0014177280245348811, acc: 1.0)
[2025-01-06 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44][root][INFO] - Training Epoch: 3/10, step 376/574 completed (loss: 0.017732633277773857, acc: 1.0)
[2025-01-06 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44][root][INFO] - Training Epoch: 3/10, step 377/574 completed (loss: 0.03412945196032524, acc: 1.0)
[2025-01-06 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44][root][INFO] - Training Epoch: 3/10, step 378/574 completed (loss: 0.027988217771053314, acc: 0.9894737005233765)
[2025-01-06 01:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45][root][INFO] - Training Epoch: 3/10, step 379/574 completed (loss: 0.30499300360679626, acc: 0.9281437397003174)
[2025-01-06 01:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45][root][INFO] - Training Epoch: 3/10, step 380/574 completed (loss: 0.3885491192340851, acc: 0.8947368264198303)
[2025-01-06 01:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47][root][INFO] - Training Epoch: 3/10, step 381/574 completed (loss: 0.510583221912384, acc: 0.8449198007583618)
[2025-01-06 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47][root][INFO] - Training Epoch: 3/10, step 382/574 completed (loss: 0.0879257544875145, acc: 0.9729729890823364)
[2025-01-06 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47][root][INFO] - Training Epoch: 3/10, step 383/574 completed (loss: 0.14248155057430267, acc: 0.9642857313156128)
[2025-01-06 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48][root][INFO] - Training Epoch: 3/10, step 384/574 completed (loss: 0.016508718952536583, acc: 1.0)
[2025-01-06 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48][root][INFO] - Training Epoch: 3/10, step 385/574 completed (loss: 0.04986963048577309, acc: 1.0)
[2025-01-06 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49][root][INFO] - Training Epoch: 3/10, step 386/574 completed (loss: 0.003072814317420125, acc: 1.0)
[2025-01-06 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49][root][INFO] - Training Epoch: 3/10, step 387/574 completed (loss: 0.005350311286747456, acc: 1.0)
[2025-01-06 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49][root][INFO] - Training Epoch: 3/10, step 388/574 completed (loss: 0.0029854984022676945, acc: 1.0)
[2025-01-06 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49][root][INFO] - Training Epoch: 3/10, step 389/574 completed (loss: 0.007839367724955082, acc: 1.0)
[2025-01-06 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50][root][INFO] - Training Epoch: 3/10, step 390/574 completed (loss: 0.293498158454895, acc: 0.9523809552192688)
[2025-01-06 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50][root][INFO] - Training Epoch: 3/10, step 391/574 completed (loss: 0.7035606503486633, acc: 0.7222222089767456)
[2025-01-06 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51][root][INFO] - Training Epoch: 3/10, step 392/574 completed (loss: 0.9253897666931152, acc: 0.7669903039932251)
[2025-01-06 01:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51][root][INFO] - Training Epoch: 3/10, step 393/574 completed (loss: 0.7591496706008911, acc: 0.8161764740943909)
[2025-01-06 01:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51][root][INFO] - Training Epoch: 3/10, step 394/574 completed (loss: 0.7577818036079407, acc: 0.7733333110809326)
[2025-01-06 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52][root][INFO] - Training Epoch: 3/10, step 395/574 completed (loss: 0.6880870461463928, acc: 0.8194444179534912)
[2025-01-06 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52][root][INFO] - Training Epoch: 3/10, step 396/574 completed (loss: 0.2494271695613861, acc: 0.930232584476471)
[2025-01-06 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53][root][INFO] - Training Epoch: 3/10, step 397/574 completed (loss: 0.08450823277235031, acc: 0.9583333134651184)
[2025-01-06 01:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53][root][INFO] - Training Epoch: 3/10, step 398/574 completed (loss: 0.2836136519908905, acc: 0.930232584476471)
[2025-01-06 01:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53][root][INFO] - Training Epoch: 3/10, step 399/574 completed (loss: 0.04839092120528221, acc: 1.0)
[2025-01-06 01:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54][root][INFO] - Training Epoch: 3/10, step 400/574 completed (loss: 0.22722794115543365, acc: 0.9264705777168274)
[2025-01-06 01:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54][root][INFO] - Training Epoch: 3/10, step 401/574 completed (loss: 0.4111926555633545, acc: 0.9066666960716248)
[2025-01-06 01:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:55][root][INFO] - Training Epoch: 3/10, step 402/574 completed (loss: 0.2489480972290039, acc: 0.9090909361839294)
[2025-01-06 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:55][root][INFO] - Training Epoch: 3/10, step 403/574 completed (loss: 0.08816812187433243, acc: 1.0)
[2025-01-06 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:55][root][INFO] - Training Epoch: 3/10, step 404/574 completed (loss: 0.08068209886550903, acc: 0.9677419066429138)
[2025-01-06 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56][root][INFO] - Training Epoch: 3/10, step 405/574 completed (loss: 0.007555874064564705, acc: 1.0)
[2025-01-06 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56][root][INFO] - Training Epoch: 3/10, step 406/574 completed (loss: 0.01107053179293871, acc: 1.0)
[2025-01-06 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56][root][INFO] - Training Epoch: 3/10, step 407/574 completed (loss: 0.0280768945813179, acc: 1.0)
[2025-01-06 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57][root][INFO] - Training Epoch: 3/10, step 408/574 completed (loss: 0.07464251667261124, acc: 1.0)
[2025-01-06 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57][root][INFO] - Training Epoch: 3/10, step 409/574 completed (loss: 0.05082641541957855, acc: 1.0)
[2025-01-06 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57][root][INFO] - Training Epoch: 3/10, step 410/574 completed (loss: 0.08860082924365997, acc: 0.9655172228813171)
[2025-01-06 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58][root][INFO] - Training Epoch: 3/10, step 411/574 completed (loss: 0.007608836982399225, acc: 1.0)
[2025-01-06 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58][root][INFO] - Training Epoch: 3/10, step 412/574 completed (loss: 0.022236274555325508, acc: 1.0)
[2025-01-06 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58][root][INFO] - Training Epoch: 3/10, step 413/574 completed (loss: 0.11889122426509857, acc: 0.939393937587738)
[2025-01-06 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59][root][INFO] - Training Epoch: 3/10, step 414/574 completed (loss: 0.06965141743421555, acc: 0.9545454382896423)
[2025-01-06 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59][root][INFO] - Training Epoch: 3/10, step 415/574 completed (loss: 0.19108349084854126, acc: 0.9607843160629272)
[2025-01-06 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59][root][INFO] - Training Epoch: 3/10, step 416/574 completed (loss: 0.09822952002286911, acc: 1.0)
[2025-01-06 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00][root][INFO] - Training Epoch: 3/10, step 417/574 completed (loss: 0.06902056932449341, acc: 1.0)
[2025-01-06 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00][root][INFO] - Training Epoch: 3/10, step 418/574 completed (loss: 0.1699514389038086, acc: 0.925000011920929)
[2025-01-06 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00][root][INFO] - Training Epoch: 3/10, step 419/574 completed (loss: 0.0772695317864418, acc: 0.949999988079071)
[2025-01-06 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01][root][INFO] - Training Epoch: 3/10, step 420/574 completed (loss: 0.025835206732153893, acc: 1.0)
[2025-01-06 01:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01][root][INFO] - Training Epoch: 3/10, step 421/574 completed (loss: 0.10125043243169785, acc: 0.9666666388511658)
[2025-01-06 01:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01][root][INFO] - Training Epoch: 3/10, step 422/574 completed (loss: 0.054414913058280945, acc: 0.96875)
[2025-01-06 01:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02][root][INFO] - Training Epoch: 3/10, step 423/574 completed (loss: 0.12420007586479187, acc: 0.9444444179534912)
[2025-01-06 01:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02][root][INFO] - Training Epoch: 3/10, step 424/574 completed (loss: 0.06926039606332779, acc: 0.9629629850387573)
[2025-01-06 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0050, device='cuda:0') eval_epoch_loss=tensor(0.6957, device='cuda:0') eval_epoch_acc=tensor(0.8313, device='cuda:0')
[2025-01-06 01:16:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:16:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:16:33][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_425_loss_0.6956643462181091/model.pt
[2025-01-06 01:16:33][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34][root][INFO] - Training Epoch: 3/10, step 425/574 completed (loss: 0.32046154141426086, acc: 0.9090909361839294)
[2025-01-06 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34][root][INFO] - Training Epoch: 3/10, step 426/574 completed (loss: 0.006335684563964605, acc: 1.0)
[2025-01-06 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34][root][INFO] - Training Epoch: 3/10, step 427/574 completed (loss: 0.08747099339962006, acc: 0.9729729890823364)
[2025-01-06 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35][root][INFO] - Training Epoch: 3/10, step 428/574 completed (loss: 0.00467432476580143, acc: 1.0)
[2025-01-06 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35][root][INFO] - Training Epoch: 3/10, step 429/574 completed (loss: 0.010933632031083107, acc: 1.0)
[2025-01-06 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35][root][INFO] - Training Epoch: 3/10, step 430/574 completed (loss: 0.0025649338494986296, acc: 1.0)
[2025-01-06 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36][root][INFO] - Training Epoch: 3/10, step 431/574 completed (loss: 0.00869173463433981, acc: 1.0)
[2025-01-06 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36][root][INFO] - Training Epoch: 3/10, step 432/574 completed (loss: 0.011874904856085777, acc: 1.0)
[2025-01-06 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36][root][INFO] - Training Epoch: 3/10, step 433/574 completed (loss: 0.18298856914043427, acc: 0.9444444179534912)
[2025-01-06 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37][root][INFO] - Training Epoch: 3/10, step 434/574 completed (loss: 0.07338668406009674, acc: 0.9599999785423279)
[2025-01-06 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37][root][INFO] - Training Epoch: 3/10, step 435/574 completed (loss: 0.0010105199180543423, acc: 1.0)
[2025-01-06 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37][root][INFO] - Training Epoch: 3/10, step 436/574 completed (loss: 0.2600998282432556, acc: 0.8888888955116272)
[2025-01-06 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38][root][INFO] - Training Epoch: 3/10, step 437/574 completed (loss: 0.01447305642068386, acc: 1.0)
[2025-01-06 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38][root][INFO] - Training Epoch: 3/10, step 438/574 completed (loss: 0.05322973057627678, acc: 0.9523809552192688)
[2025-01-06 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38][root][INFO] - Training Epoch: 3/10, step 439/574 completed (loss: 0.11676646769046783, acc: 0.9487179517745972)
[2025-01-06 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:39][root][INFO] - Training Epoch: 3/10, step 440/574 completed (loss: 0.33996570110321045, acc: 0.8787878751754761)
[2025-01-06 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40][root][INFO] - Training Epoch: 3/10, step 441/574 completed (loss: 0.8082929253578186, acc: 0.7680000066757202)
[2025-01-06 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40][root][INFO] - Training Epoch: 3/10, step 442/574 completed (loss: 0.8577013611793518, acc: 0.7903226017951965)
[2025-01-06 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41][root][INFO] - Training Epoch: 3/10, step 443/574 completed (loss: 0.5054462552070618, acc: 0.9104477763175964)
[2025-01-06 01:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41][root][INFO] - Training Epoch: 3/10, step 444/574 completed (loss: 0.14162255823612213, acc: 0.9622641801834106)
[2025-01-06 01:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41][root][INFO] - Training Epoch: 3/10, step 445/574 completed (loss: 0.15050701797008514, acc: 0.9772727489471436)
[2025-01-06 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42][root][INFO] - Training Epoch: 3/10, step 446/574 completed (loss: 0.2686474621295929, acc: 0.9130434989929199)
[2025-01-06 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42][root][INFO] - Training Epoch: 3/10, step 447/574 completed (loss: 0.1453745812177658, acc: 0.9230769276618958)
[2025-01-06 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43][root][INFO] - Training Epoch: 3/10, step 448/574 completed (loss: 0.14054502546787262, acc: 0.9285714030265808)
[2025-01-06 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43][root][INFO] - Training Epoch: 3/10, step 449/574 completed (loss: 0.25170406699180603, acc: 0.9552238583564758)
[2025-01-06 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43][root][INFO] - Training Epoch: 3/10, step 450/574 completed (loss: 0.036410775035619736, acc: 1.0)
[2025-01-06 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44][root][INFO] - Training Epoch: 3/10, step 451/574 completed (loss: 0.055278170853853226, acc: 0.989130437374115)
[2025-01-06 01:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44][root][INFO] - Training Epoch: 3/10, step 452/574 completed (loss: 0.19824615120887756, acc: 0.9358974099159241)
[2025-01-06 01:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44][root][INFO] - Training Epoch: 3/10, step 453/574 completed (loss: 0.23202595114707947, acc: 0.9078947305679321)
[2025-01-06 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45][root][INFO] - Training Epoch: 3/10, step 454/574 completed (loss: 0.07718682289123535, acc: 0.9795918464660645)
[2025-01-06 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45][root][INFO] - Training Epoch: 3/10, step 455/574 completed (loss: 0.3387884497642517, acc: 0.8787878751754761)
[2025-01-06 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46][root][INFO] - Training Epoch: 3/10, step 456/574 completed (loss: 0.5351601243019104, acc: 0.876288652420044)
[2025-01-06 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46][root][INFO] - Training Epoch: 3/10, step 457/574 completed (loss: 0.04056369140744209, acc: 0.9857142567634583)
[2025-01-06 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46][root][INFO] - Training Epoch: 3/10, step 458/574 completed (loss: 0.2746192216873169, acc: 0.930232584476471)
[2025-01-06 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47][root][INFO] - Training Epoch: 3/10, step 459/574 completed (loss: 0.04570302367210388, acc: 0.9821428656578064)
[2025-01-06 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47][root][INFO] - Training Epoch: 3/10, step 460/574 completed (loss: 0.19184643030166626, acc: 0.9506173133850098)
[2025-01-06 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47][root][INFO] - Training Epoch: 3/10, step 461/574 completed (loss: 0.21794456243515015, acc: 0.9166666865348816)
[2025-01-06 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48][root][INFO] - Training Epoch: 3/10, step 462/574 completed (loss: 0.07138444483280182, acc: 0.96875)
[2025-01-06 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48][root][INFO] - Training Epoch: 3/10, step 463/574 completed (loss: 0.32570409774780273, acc: 0.9615384340286255)
[2025-01-06 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49][root][INFO] - Training Epoch: 3/10, step 464/574 completed (loss: 0.16699087619781494, acc: 0.9347826242446899)
[2025-01-06 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49][root][INFO] - Training Epoch: 3/10, step 465/574 completed (loss: 0.36776769161224365, acc: 0.8928571343421936)
[2025-01-06 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49][root][INFO] - Training Epoch: 3/10, step 466/574 completed (loss: 0.47041982412338257, acc: 0.8674699068069458)
[2025-01-06 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50][root][INFO] - Training Epoch: 3/10, step 467/574 completed (loss: 0.21125635504722595, acc: 0.9369369149208069)
[2025-01-06 01:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50][root][INFO] - Training Epoch: 3/10, step 468/574 completed (loss: 0.5620225071907043, acc: 0.8543689250946045)
[2025-01-06 01:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50][root][INFO] - Training Epoch: 3/10, step 469/574 completed (loss: 0.39464399218559265, acc: 0.8861788511276245)
[2025-01-06 01:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51][root][INFO] - Training Epoch: 3/10, step 470/574 completed (loss: 0.16004343330860138, acc: 0.9166666865348816)
[2025-01-06 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51][root][INFO] - Training Epoch: 3/10, step 471/574 completed (loss: 0.3914039134979248, acc: 0.8928571343421936)
[2025-01-06 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51][root][INFO] - Training Epoch: 3/10, step 472/574 completed (loss: 0.5580617189407349, acc: 0.843137264251709)
[2025-01-06 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52][root][INFO] - Training Epoch: 3/10, step 473/574 completed (loss: 0.7277350425720215, acc: 0.807860255241394)
[2025-01-06 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52][root][INFO] - Training Epoch: 3/10, step 474/574 completed (loss: 0.44096890091896057, acc: 0.84375)
[2025-01-06 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52][root][INFO] - Training Epoch: 3/10, step 475/574 completed (loss: 0.37670037150382996, acc: 0.9018405079841614)
[2025-01-06 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53][root][INFO] - Training Epoch: 3/10, step 476/574 completed (loss: 0.3539148271083832, acc: 0.9136690497398376)
[2025-01-06 01:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53][root][INFO] - Training Epoch: 3/10, step 477/574 completed (loss: 0.7339615821838379, acc: 0.7839195728302002)
[2025-01-06 01:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53][root][INFO] - Training Epoch: 3/10, step 478/574 completed (loss: 0.23272565007209778, acc: 0.9444444179534912)
[2025-01-06 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54][root][INFO] - Training Epoch: 3/10, step 479/574 completed (loss: 0.23271703720092773, acc: 0.939393937587738)
[2025-01-06 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54][root][INFO] - Training Epoch: 3/10, step 480/574 completed (loss: 0.5013447403907776, acc: 0.8888888955116272)
[2025-01-06 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55][root][INFO] - Training Epoch: 3/10, step 481/574 completed (loss: 0.6254019141197205, acc: 0.8500000238418579)
[2025-01-06 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55][root][INFO] - Training Epoch: 3/10, step 482/574 completed (loss: 0.4783399701118469, acc: 0.949999988079071)
[2025-01-06 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55][root][INFO] - Training Epoch: 3/10, step 483/574 completed (loss: 0.5032283663749695, acc: 0.7931034564971924)
[2025-01-06 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56][root][INFO] - Training Epoch: 3/10, step 484/574 completed (loss: 0.09189873188734055, acc: 0.9677419066429138)
[2025-01-06 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56][root][INFO] - Training Epoch: 3/10, step 485/574 completed (loss: 0.21131104230880737, acc: 0.8947368264198303)
[2025-01-06 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56][root][INFO] - Training Epoch: 3/10, step 486/574 completed (loss: 0.4743732810020447, acc: 0.8518518805503845)
[2025-01-06 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57][root][INFO] - Training Epoch: 3/10, step 487/574 completed (loss: 0.32513347268104553, acc: 0.9047619104385376)
[2025-01-06 01:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57][root][INFO] - Training Epoch: 3/10, step 488/574 completed (loss: 0.1922198235988617, acc: 0.9545454382896423)
[2025-01-06 01:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57][root][INFO] - Training Epoch: 3/10, step 489/574 completed (loss: 0.9553903341293335, acc: 0.7230769395828247)
[2025-01-06 01:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:58][root][INFO] - Training Epoch: 3/10, step 490/574 completed (loss: 0.18137803673744202, acc: 0.9666666388511658)
[2025-01-06 01:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:58][root][INFO] - Training Epoch: 3/10, step 491/574 completed (loss: 0.3227439820766449, acc: 0.8965517282485962)
[2025-01-06 01:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:58][root][INFO] - Training Epoch: 3/10, step 492/574 completed (loss: 0.4201149046421051, acc: 0.9019607901573181)
[2025-01-06 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:59][root][INFO] - Training Epoch: 3/10, step 493/574 completed (loss: 0.25507956743240356, acc: 0.8965517282485962)
[2025-01-06 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:59][root][INFO] - Training Epoch: 3/10, step 494/574 completed (loss: 0.2823775112628937, acc: 0.8947368264198303)
[2025-01-06 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:59][root][INFO] - Training Epoch: 3/10, step 495/574 completed (loss: 0.219626784324646, acc: 0.9473684430122375)
[2025-01-06 01:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:00][root][INFO] - Training Epoch: 3/10, step 496/574 completed (loss: 0.6188015937805176, acc: 0.8392857313156128)
[2025-01-06 01:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:00][root][INFO] - Training Epoch: 3/10, step 497/574 completed (loss: 0.3899924159049988, acc: 0.8539325594902039)
[2025-01-06 01:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01][root][INFO] - Training Epoch: 3/10, step 498/574 completed (loss: 0.6506513953208923, acc: 0.8089887499809265)
[2025-01-06 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01][root][INFO] - Training Epoch: 3/10, step 499/574 completed (loss: 1.1668671369552612, acc: 0.6241135001182556)
[2025-01-06 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01][root][INFO] - Training Epoch: 3/10, step 500/574 completed (loss: 0.7028369307518005, acc: 0.8369565010070801)
[2025-01-06 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:02][root][INFO] - Training Epoch: 3/10, step 501/574 completed (loss: 0.010846695862710476, acc: 1.0)
[2025-01-06 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:02][root][INFO] - Training Epoch: 3/10, step 502/574 completed (loss: 0.005208149552345276, acc: 1.0)
[2025-01-06 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03][root][INFO] - Training Epoch: 3/10, step 503/574 completed (loss: 0.20358887314796448, acc: 0.8888888955116272)
[2025-01-06 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03][root][INFO] - Training Epoch: 3/10, step 504/574 completed (loss: 0.06371233612298965, acc: 1.0)
[2025-01-06 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03][root][INFO] - Training Epoch: 3/10, step 505/574 completed (loss: 0.5511409044265747, acc: 0.8301886916160583)
[2025-01-06 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:04][root][INFO] - Training Epoch: 3/10, step 506/574 completed (loss: 0.5272637009620667, acc: 0.8620689511299133)
[2025-01-06 01:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:04][root][INFO] - Training Epoch: 3/10, step 507/574 completed (loss: 0.8694260716438293, acc: 0.7837837934494019)
[2025-01-06 01:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05][root][INFO] - Training Epoch: 3/10, step 508/574 completed (loss: 0.624876081943512, acc: 0.8309859037399292)
[2025-01-06 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05][root][INFO] - Training Epoch: 3/10, step 509/574 completed (loss: 0.5761305689811707, acc: 0.8999999761581421)
[2025-01-06 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05][root][INFO] - Training Epoch: 3/10, step 510/574 completed (loss: 0.09969920665025711, acc: 0.9666666388511658)
[2025-01-06 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:06][root][INFO] - Training Epoch: 3/10, step 511/574 completed (loss: 0.14282503724098206, acc: 0.9230769276618958)
[2025-01-06 01:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:08][root][INFO] - Training Epoch: 3/10, step 512/574 completed (loss: 0.9666503071784973, acc: 0.7214285731315613)
[2025-01-06 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09][root][INFO] - Training Epoch: 3/10, step 513/574 completed (loss: 0.2693909704685211, acc: 0.9126983880996704)
[2025-01-06 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09][root][INFO] - Training Epoch: 3/10, step 514/574 completed (loss: 0.5381748676300049, acc: 0.8571428656578064)
[2025-01-06 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09][root][INFO] - Training Epoch: 3/10, step 515/574 completed (loss: 0.07251861691474915, acc: 0.9666666388511658)
[2025-01-06 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10][root][INFO] - Training Epoch: 3/10, step 516/574 completed (loss: 0.44115859270095825, acc: 0.875)
[2025-01-06 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10][root][INFO] - Training Epoch: 3/10, step 517/574 completed (loss: 0.0022854176349937916, acc: 1.0)
[2025-01-06 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11][root][INFO] - Training Epoch: 3/10, step 518/574 completed (loss: 0.048522572964429855, acc: 1.0)
[2025-01-06 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11][root][INFO] - Training Epoch: 3/10, step 519/574 completed (loss: 0.15483129024505615, acc: 0.949999988079071)
[2025-01-06 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11][root][INFO] - Training Epoch: 3/10, step 520/574 completed (loss: 0.2175174355506897, acc: 0.8888888955116272)
[2025-01-06 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:12][root][INFO] - Training Epoch: 3/10, step 521/574 completed (loss: 0.5801116824150085, acc: 0.8220338821411133)
[2025-01-06 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13][root][INFO] - Training Epoch: 3/10, step 522/574 completed (loss: 0.30690231919288635, acc: 0.9029850959777832)
[2025-01-06 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13][root][INFO] - Training Epoch: 3/10, step 523/574 completed (loss: 0.35000112652778625, acc: 0.8905109763145447)
[2025-01-06 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14][root][INFO] - Training Epoch: 3/10, step 524/574 completed (loss: 0.6275310516357422, acc: 0.8399999737739563)
[2025-01-06 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14][root][INFO] - Training Epoch: 3/10, step 525/574 completed (loss: 0.13394750654697418, acc: 0.9629629850387573)
[2025-01-06 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15][root][INFO] - Training Epoch: 3/10, step 526/574 completed (loss: 0.17820808291435242, acc: 0.942307710647583)
[2025-01-06 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15][root][INFO] - Training Epoch: 3/10, step 527/574 completed (loss: 0.10425693541765213, acc: 1.0)
[2025-01-06 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15][root][INFO] - Training Epoch: 3/10, step 528/574 completed (loss: 1.2861210107803345, acc: 0.6557376980781555)
[2025-01-06 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16][root][INFO] - Training Epoch: 3/10, step 529/574 completed (loss: 0.27841904759407043, acc: 0.9322034120559692)
[2025-01-06 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16][root][INFO] - Training Epoch: 3/10, step 530/574 completed (loss: 0.921375036239624, acc: 0.7209302186965942)
[2025-01-06 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16][root][INFO] - Training Epoch: 3/10, step 531/574 completed (loss: 0.5653144121170044, acc: 0.8181818127632141)
[2025-01-06 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17][root][INFO] - Training Epoch: 3/10, step 532/574 completed (loss: 0.4901794195175171, acc: 0.849056601524353)
[2025-01-06 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17][root][INFO] - Training Epoch: 3/10, step 533/574 completed (loss: 0.6266969442367554, acc: 0.8636363744735718)
[2025-01-06 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17][root][INFO] - Training Epoch: 3/10, step 534/574 completed (loss: 0.2718747556209564, acc: 0.9200000166893005)
[2025-01-06 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18][root][INFO] - Training Epoch: 3/10, step 535/574 completed (loss: 0.28422898054122925, acc: 0.8999999761581421)
[2025-01-06 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18][root][INFO] - Training Epoch: 3/10, step 536/574 completed (loss: 0.0394948273897171, acc: 1.0)
[2025-01-06 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19][root][INFO] - Training Epoch: 3/10, step 537/574 completed (loss: 0.5271860361099243, acc: 0.8307692408561707)
[2025-01-06 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19][root][INFO] - Training Epoch: 3/10, step 538/574 completed (loss: 0.31669193506240845, acc: 0.921875)
[2025-01-06 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19][root][INFO] - Training Epoch: 3/10, step 539/574 completed (loss: 0.5114613175392151, acc: 0.875)
[2025-01-06 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20][root][INFO] - Training Epoch: 3/10, step 540/574 completed (loss: 0.4606063663959503, acc: 0.8787878751754761)
[2025-01-06 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20][root][INFO] - Training Epoch: 3/10, step 541/574 completed (loss: 0.06641175597906113, acc: 0.9375)
[2025-01-06 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20][root][INFO] - Training Epoch: 3/10, step 542/574 completed (loss: 0.01657508872449398, acc: 1.0)
[2025-01-06 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21][root][INFO] - Training Epoch: 3/10, step 543/574 completed (loss: 0.07741523534059525, acc: 0.95652174949646)
[2025-01-06 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21][root][INFO] - Training Epoch: 3/10, step 544/574 completed (loss: 0.008575190789997578, acc: 1.0)
[2025-01-06 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21][root][INFO] - Training Epoch: 3/10, step 545/574 completed (loss: 0.0369839072227478, acc: 1.0)
[2025-01-06 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22][root][INFO] - Training Epoch: 3/10, step 546/574 completed (loss: 0.027471642941236496, acc: 1.0)
[2025-01-06 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22][root][INFO] - Training Epoch: 3/10, step 547/574 completed (loss: 0.03397027775645256, acc: 0.9736841917037964)
[2025-01-06 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22][root][INFO] - Training Epoch: 3/10, step 548/574 completed (loss: 0.041830871254205704, acc: 0.9677419066429138)
[2025-01-06 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23][root][INFO] - Training Epoch: 3/10, step 549/574 completed (loss: 0.0024004606530070305, acc: 1.0)
[2025-01-06 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23][root][INFO] - Training Epoch: 3/10, step 550/574 completed (loss: 0.25217005610466003, acc: 0.939393937587738)
[2025-01-06 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23][root][INFO] - Training Epoch: 3/10, step 551/574 completed (loss: 0.016424963250756264, acc: 1.0)
[2025-01-06 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24][root][INFO] - Training Epoch: 3/10, step 552/574 completed (loss: 0.13590902090072632, acc: 0.9571428298950195)
[2025-01-06 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24][root][INFO] - Training Epoch: 3/10, step 553/574 completed (loss: 0.531755805015564, acc: 0.8759124279022217)
[2025-01-06 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24][root][INFO] - Training Epoch: 3/10, step 554/574 completed (loss: 0.2682551145553589, acc: 0.9034482836723328)
[2025-01-06 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25][root][INFO] - Training Epoch: 3/10, step 555/574 completed (loss: 0.3215143084526062, acc: 0.9071428775787354)
[2025-01-06 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25][root][INFO] - Training Epoch: 3/10, step 556/574 completed (loss: 0.34960368275642395, acc: 0.9337748289108276)
[2025-01-06 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26][root][INFO] - Training Epoch: 3/10, step 557/574 completed (loss: 0.09240107238292694, acc: 0.9572649598121643)
[2025-01-06 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26][root][INFO] - Training Epoch: 3/10, step 558/574 completed (loss: 0.028470315039157867, acc: 1.0)
[2025-01-06 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26][root][INFO] - Training Epoch: 3/10, step 559/574 completed (loss: 0.08460167795419693, acc: 0.9615384340286255)
[2025-01-06 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27][root][INFO] - Training Epoch: 3/10, step 560/574 completed (loss: 0.145677387714386, acc: 0.9230769276618958)
[2025-01-06 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27][root][INFO] - Training Epoch: 3/10, step 561/574 completed (loss: 0.014552495442330837, acc: 1.0)
[2025-01-06 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27][root][INFO] - Training Epoch: 3/10, step 562/574 completed (loss: 0.3486213982105255, acc: 0.8888888955116272)
[2025-01-06 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28][root][INFO] - Training Epoch: 3/10, step 563/574 completed (loss: 0.33394235372543335, acc: 0.8961039185523987)
[2025-01-06 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28][root][INFO] - Training Epoch: 3/10, step 564/574 completed (loss: 0.06830577552318573, acc: 0.9791666865348816)
[2025-01-06 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28][root][INFO] - Training Epoch: 3/10, step 565/574 completed (loss: 0.12731888890266418, acc: 0.9482758641242981)
[2025-01-06 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29][root][INFO] - Training Epoch: 3/10, step 566/574 completed (loss: 0.26082202792167664, acc: 0.9285714030265808)
[2025-01-06 01:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29][root][INFO] - Training Epoch: 3/10, step 567/574 completed (loss: 0.005393632221966982, acc: 1.0)
[2025-01-06 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:00][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1218, device='cuda:0') eval_epoch_loss=tensor(0.7523, device='cuda:0') eval_epoch_acc=tensor(0.8247, device='cuda:0')
[2025-01-06 01:18:00][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:18:00][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:18:00][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_568_loss_0.7522604465484619/model.pt
[2025-01-06 01:18:00][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01][root][INFO] - Training Epoch: 3/10, step 568/574 completed (loss: 0.017584344372153282, acc: 1.0)
[2025-01-06 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01][root][INFO] - Training Epoch: 3/10, step 569/574 completed (loss: 0.13858453929424286, acc: 0.9572192430496216)
[2025-01-06 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01][root][INFO] - Training Epoch: 3/10, step 570/574 completed (loss: 0.019572226330637932, acc: 0.9838709831237793)
[2025-01-06 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02][root][INFO] - Training Epoch: 3/10, step 571/574 completed (loss: 0.22209486365318298, acc: 0.9572649598121643)
[2025-01-06 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02][root][INFO] - Training Epoch: 3/10, step 572/574 completed (loss: 0.3008228540420532, acc: 0.9132652878761292)
[2025-01-06 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02][root][INFO] - Training Epoch: 3/10, step 573/574 completed (loss: 0.2623974680900574, acc: 0.8993710875511169)
[2025-01-06 01:18:03][slam_llm.utils.train_utils][INFO] - Epoch 3: train_perplexity=1.5145, train_epoch_loss=0.4151, epoch time 358.7459137327969s
[2025-01-06 01:18:03][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:18:03][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2025-01-06 01:18:03][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:18:03][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 9
[2025-01-06 01:18:03][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04][root][INFO] - Training Epoch: 4/10, step 0/574 completed (loss: 0.1399715542793274, acc: 0.9629629850387573)
[2025-01-06 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04][root][INFO] - Training Epoch: 4/10, step 1/574 completed (loss: 0.05825558304786682, acc: 0.9599999785423279)
[2025-01-06 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04][root][INFO] - Training Epoch: 4/10, step 2/574 completed (loss: 0.5325559973716736, acc: 0.8648648858070374)
[2025-01-06 01:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:05][root][INFO] - Training Epoch: 4/10, step 3/574 completed (loss: 0.24084630608558655, acc: 0.9210526347160339)
[2025-01-06 01:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:05][root][INFO] - Training Epoch: 4/10, step 4/574 completed (loss: 0.46459662914276123, acc: 0.8918918967247009)
[2025-01-06 01:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:05][root][INFO] - Training Epoch: 4/10, step 5/574 completed (loss: 0.16418816149234772, acc: 0.9642857313156128)
[2025-01-06 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:06][root][INFO] - Training Epoch: 4/10, step 6/574 completed (loss: 0.7419252991676331, acc: 0.795918345451355)
[2025-01-06 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:06][root][INFO] - Training Epoch: 4/10, step 7/574 completed (loss: 0.16459152102470398, acc: 0.9333333373069763)
[2025-01-06 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07][root][INFO] - Training Epoch: 4/10, step 8/574 completed (loss: 0.01635882817208767, acc: 1.0)
[2025-01-06 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07][root][INFO] - Training Epoch: 4/10, step 9/574 completed (loss: 0.006790520623326302, acc: 1.0)
[2025-01-06 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07][root][INFO] - Training Epoch: 4/10, step 10/574 completed (loss: 0.019588414579629898, acc: 1.0)
[2025-01-06 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08][root][INFO] - Training Epoch: 4/10, step 11/574 completed (loss: 0.18087705969810486, acc: 0.9487179517745972)
[2025-01-06 01:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08][root][INFO] - Training Epoch: 4/10, step 12/574 completed (loss: 0.04808439686894417, acc: 1.0)
[2025-01-06 01:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08][root][INFO] - Training Epoch: 4/10, step 13/574 completed (loss: 0.1346047818660736, acc: 0.9130434989929199)
[2025-01-06 01:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09][root][INFO] - Training Epoch: 4/10, step 14/574 completed (loss: 0.05733349174261093, acc: 1.0)
[2025-01-06 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09][root][INFO] - Training Epoch: 4/10, step 15/574 completed (loss: 0.32167115807533264, acc: 0.8979591727256775)
[2025-01-06 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10][root][INFO] - Training Epoch: 4/10, step 16/574 completed (loss: 0.028416050598025322, acc: 1.0)
[2025-01-06 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10][root][INFO] - Training Epoch: 4/10, step 17/574 completed (loss: 0.041381511837244034, acc: 1.0)
[2025-01-06 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10][root][INFO] - Training Epoch: 4/10, step 18/574 completed (loss: 0.303844153881073, acc: 0.9166666865348816)
[2025-01-06 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11][root][INFO] - Training Epoch: 4/10, step 19/574 completed (loss: 0.06409524381160736, acc: 0.9473684430122375)
[2025-01-06 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11][root][INFO] - Training Epoch: 4/10, step 20/574 completed (loss: 0.05396197363734245, acc: 1.0)
[2025-01-06 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11][root][INFO] - Training Epoch: 4/10, step 21/574 completed (loss: 0.21182921528816223, acc: 0.8965517282485962)
[2025-01-06 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12][root][INFO] - Training Epoch: 4/10, step 22/574 completed (loss: 0.27289631962776184, acc: 0.8799999952316284)
[2025-01-06 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12][root][INFO] - Training Epoch: 4/10, step 23/574 completed (loss: 0.37770622968673706, acc: 0.9047619104385376)
[2025-01-06 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12][root][INFO] - Training Epoch: 4/10, step 24/574 completed (loss: 0.013538476079702377, acc: 1.0)
[2025-01-06 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13][root][INFO] - Training Epoch: 4/10, step 25/574 completed (loss: 0.6293315291404724, acc: 0.9056603908538818)
[2025-01-06 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13][root][INFO] - Training Epoch: 4/10, step 26/574 completed (loss: 0.7996149063110352, acc: 0.7808219194412231)
[2025-01-06 01:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:14][root][INFO] - Training Epoch: 4/10, step 27/574 completed (loss: 0.8297079205513, acc: 0.7628458738327026)
[2025-01-06 01:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15][root][INFO] - Training Epoch: 4/10, step 28/574 completed (loss: 0.3325270414352417, acc: 0.9069767594337463)
[2025-01-06 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15][root][INFO] - Training Epoch: 4/10, step 29/574 completed (loss: 0.3887271285057068, acc: 0.8433734774589539)
[2025-01-06 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15][root][INFO] - Training Epoch: 4/10, step 30/574 completed (loss: 0.37670671939849854, acc: 0.8765432238578796)
[2025-01-06 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16][root][INFO] - Training Epoch: 4/10, step 31/574 completed (loss: 0.22929278016090393, acc: 0.8928571343421936)
[2025-01-06 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16][root][INFO] - Training Epoch: 4/10, step 32/574 completed (loss: 0.07861226052045822, acc: 0.9629629850387573)
[2025-01-06 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16][root][INFO] - Training Epoch: 4/10, step 33/574 completed (loss: 0.03207525610923767, acc: 1.0)
[2025-01-06 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:17][root][INFO] - Training Epoch: 4/10, step 34/574 completed (loss: 0.48182228207588196, acc: 0.8571428656578064)
[2025-01-06 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:17][root][INFO] - Training Epoch: 4/10, step 35/574 completed (loss: 0.22871357202529907, acc: 0.9344262480735779)
[2025-01-06 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:18][root][INFO] - Training Epoch: 4/10, step 36/574 completed (loss: 0.49908241629600525, acc: 0.8730158805847168)
[2025-01-06 01:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:18][root][INFO] - Training Epoch: 4/10, step 37/574 completed (loss: 0.47910037636756897, acc: 0.8813559412956238)
[2025-01-06 01:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:18][root][INFO] - Training Epoch: 4/10, step 38/574 completed (loss: 0.25196677446365356, acc: 0.9195402264595032)
[2025-01-06 01:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:19][root][INFO] - Training Epoch: 4/10, step 39/574 completed (loss: 0.15299689769744873, acc: 0.9523809552192688)
[2025-01-06 01:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:19][root][INFO] - Training Epoch: 4/10, step 40/574 completed (loss: 0.2041599005460739, acc: 0.9230769276618958)
[2025-01-06 01:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:19][root][INFO] - Training Epoch: 4/10, step 41/574 completed (loss: 0.21846918761730194, acc: 0.9324324131011963)
[2025-01-06 01:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:20][root][INFO] - Training Epoch: 4/10, step 42/574 completed (loss: 0.3926714062690735, acc: 0.892307698726654)
[2025-01-06 01:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:20][root][INFO] - Training Epoch: 4/10, step 43/574 completed (loss: 0.4204394817352295, acc: 0.8989899158477783)
[2025-01-06 01:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:21][root][INFO] - Training Epoch: 4/10, step 44/574 completed (loss: 0.2286277860403061, acc: 0.9587628841400146)
[2025-01-06 01:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:21][root][INFO] - Training Epoch: 4/10, step 45/574 completed (loss: 0.3164675533771515, acc: 0.875)
[2025-01-06 01:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22][root][INFO] - Training Epoch: 4/10, step 46/574 completed (loss: 0.123244509100914, acc: 0.9615384340286255)
[2025-01-06 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22][root][INFO] - Training Epoch: 4/10, step 47/574 completed (loss: 0.0705571323633194, acc: 0.9629629850387573)
[2025-01-06 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22][root][INFO] - Training Epoch: 4/10, step 48/574 completed (loss: 0.13534867763519287, acc: 0.9642857313156128)
[2025-01-06 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23][root][INFO] - Training Epoch: 4/10, step 49/574 completed (loss: 0.016676349565386772, acc: 1.0)
[2025-01-06 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23][root][INFO] - Training Epoch: 4/10, step 50/574 completed (loss: 0.4462243318557739, acc: 0.8421052694320679)
[2025-01-06 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23][root][INFO] - Training Epoch: 4/10, step 51/574 completed (loss: 0.32661640644073486, acc: 0.8888888955116272)
[2025-01-06 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24][root][INFO] - Training Epoch: 4/10, step 52/574 completed (loss: 0.5445103049278259, acc: 0.8309859037399292)
[2025-01-06 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24][root][INFO] - Training Epoch: 4/10, step 53/574 completed (loss: 1.2897841930389404, acc: 0.6466666460037231)
[2025-01-06 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:25][root][INFO] - Training Epoch: 4/10, step 54/574 completed (loss: 0.5317159295082092, acc: 0.8108108043670654)
[2025-01-06 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:25][root][INFO] - Training Epoch: 4/10, step 55/574 completed (loss: 0.15143798291683197, acc: 0.9615384340286255)
[2025-01-06 01:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:28][root][INFO] - Training Epoch: 4/10, step 56/574 completed (loss: 0.8373098373413086, acc: 0.7542662024497986)
[2025-01-06 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30][root][INFO] - Training Epoch: 4/10, step 57/574 completed (loss: 1.103640079498291, acc: 0.6884531378746033)
[2025-01-06 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30][root][INFO] - Training Epoch: 4/10, step 58/574 completed (loss: 0.6631272435188293, acc: 0.7897727489471436)
[2025-01-06 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31][root][INFO] - Training Epoch: 4/10, step 59/574 completed (loss: 0.29491353034973145, acc: 0.9264705777168274)
[2025-01-06 01:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31][root][INFO] - Training Epoch: 4/10, step 60/574 completed (loss: 0.7412809729576111, acc: 0.782608687877655)
[2025-01-06 01:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32][root][INFO] - Training Epoch: 4/10, step 61/574 completed (loss: 0.537589967250824, acc: 0.8500000238418579)
[2025-01-06 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32][root][INFO] - Training Epoch: 4/10, step 62/574 completed (loss: 0.24426676332950592, acc: 0.9117646813392639)
[2025-01-06 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32][root][INFO] - Training Epoch: 4/10, step 63/574 completed (loss: 0.25607484579086304, acc: 0.9166666865348816)
[2025-01-06 01:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:33][root][INFO] - Training Epoch: 4/10, step 64/574 completed (loss: 0.08647508919239044, acc: 0.984375)
[2025-01-06 01:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:33][root][INFO] - Training Epoch: 4/10, step 65/574 completed (loss: 0.07557505369186401, acc: 1.0)
[2025-01-06 01:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:34][root][INFO] - Training Epoch: 4/10, step 66/574 completed (loss: 0.43134790658950806, acc: 0.8571428656578064)
[2025-01-06 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:34][root][INFO] - Training Epoch: 4/10, step 67/574 completed (loss: 0.16013269126415253, acc: 0.9666666388511658)
[2025-01-06 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:34][root][INFO] - Training Epoch: 4/10, step 68/574 completed (loss: 0.061402760446071625, acc: 0.9599999785423279)
[2025-01-06 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35][root][INFO] - Training Epoch: 4/10, step 69/574 completed (loss: 0.3118288516998291, acc: 0.9166666865348816)
[2025-01-06 01:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35][root][INFO] - Training Epoch: 4/10, step 70/574 completed (loss: 0.38438838720321655, acc: 0.8787878751754761)
[2025-01-06 01:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35][root][INFO] - Training Epoch: 4/10, step 71/574 completed (loss: 0.7470759153366089, acc: 0.779411792755127)
[2025-01-06 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:36][root][INFO] - Training Epoch: 4/10, step 72/574 completed (loss: 0.6198471188545227, acc: 0.8333333134651184)
[2025-01-06 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:36][root][INFO] - Training Epoch: 4/10, step 73/574 completed (loss: 1.2564903497695923, acc: 0.6512820720672607)
[2025-01-06 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37][root][INFO] - Training Epoch: 4/10, step 74/574 completed (loss: 0.8872793316841125, acc: 0.7551020383834839)
[2025-01-06 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37][root][INFO] - Training Epoch: 4/10, step 75/574 completed (loss: 0.9910928606987, acc: 0.7238805890083313)
[2025-01-06 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37][root][INFO] - Training Epoch: 4/10, step 76/574 completed (loss: 1.3383458852767944, acc: 0.6386861205101013)
[2025-01-06 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38][root][INFO] - Training Epoch: 4/10, step 77/574 completed (loss: 0.023512806743383408, acc: 1.0)
[2025-01-06 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38][root][INFO] - Training Epoch: 4/10, step 78/574 completed (loss: 0.1403171420097351, acc: 0.9583333134651184)
[2025-01-06 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38][root][INFO] - Training Epoch: 4/10, step 79/574 completed (loss: 0.24905502796173096, acc: 0.9696969985961914)
[2025-01-06 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39][root][INFO] - Training Epoch: 4/10, step 80/574 completed (loss: 0.1313323825597763, acc: 0.9615384340286255)
[2025-01-06 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39][root][INFO] - Training Epoch: 4/10, step 81/574 completed (loss: 0.33864662051200867, acc: 0.9038461446762085)
[2025-01-06 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40][root][INFO] - Training Epoch: 4/10, step 82/574 completed (loss: 0.41329261660575867, acc: 0.8846153616905212)
[2025-01-06 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40][root][INFO] - Training Epoch: 4/10, step 83/574 completed (loss: 0.26981890201568604, acc: 0.9375)
[2025-01-06 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40][root][INFO] - Training Epoch: 4/10, step 84/574 completed (loss: 0.28440284729003906, acc: 0.9130434989929199)
[2025-01-06 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41][root][INFO] - Training Epoch: 4/10, step 85/574 completed (loss: 0.20283707976341248, acc: 0.9200000166893005)
[2025-01-06 01:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41][root][INFO] - Training Epoch: 4/10, step 86/574 completed (loss: 0.11380242556333542, acc: 0.95652174949646)
[2025-01-06 01:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:42][root][INFO] - Training Epoch: 4/10, step 87/574 completed (loss: 0.5786399245262146, acc: 0.7799999713897705)
[2025-01-06 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:42][root][INFO] - Training Epoch: 4/10, step 88/574 completed (loss: 0.7061253786087036, acc: 0.7864077687263489)
[2025-01-06 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:43][root][INFO] - Training Epoch: 4/10, step 89/574 completed (loss: 0.8472293615341187, acc: 0.7961165308952332)
[2025-01-06 01:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:44][root][INFO] - Training Epoch: 4/10, step 90/574 completed (loss: 0.9007250070571899, acc: 0.7580645084381104)
[2025-01-06 01:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:45][root][INFO] - Training Epoch: 4/10, step 91/574 completed (loss: 0.85622638463974, acc: 0.7715517282485962)
[2025-01-06 01:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:45][root][INFO] - Training Epoch: 4/10, step 92/574 completed (loss: 0.6246361136436462, acc: 0.8105263113975525)
[2025-01-06 01:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:46][root][INFO] - Training Epoch: 4/10, step 93/574 completed (loss: 0.8289371132850647, acc: 0.7524752616882324)
[2025-01-06 01:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47][root][INFO] - Training Epoch: 4/10, step 94/574 completed (loss: 0.6176038980484009, acc: 0.8548387289047241)
[2025-01-06 01:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47][root][INFO] - Training Epoch: 4/10, step 95/574 completed (loss: 0.6225376129150391, acc: 0.8115941882133484)
[2025-01-06 01:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47][root][INFO] - Training Epoch: 4/10, step 96/574 completed (loss: 0.8775286674499512, acc: 0.7142857313156128)
[2025-01-06 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:48][root][INFO] - Training Epoch: 4/10, step 97/574 completed (loss: 0.931708037853241, acc: 0.7403846383094788)
[2025-01-06 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:48][root][INFO] - Training Epoch: 4/10, step 98/574 completed (loss: 1.109279751777649, acc: 0.6642335653305054)
[2025-01-06 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49][root][INFO] - Training Epoch: 4/10, step 99/574 completed (loss: 0.7883751392364502, acc: 0.746268630027771)
[2025-01-06 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49][root][INFO] - Training Epoch: 4/10, step 100/574 completed (loss: 0.17495808005332947, acc: 0.949999988079071)
[2025-01-06 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49][root][INFO] - Training Epoch: 4/10, step 101/574 completed (loss: 0.010944616049528122, acc: 1.0)
[2025-01-06 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50][root][INFO] - Training Epoch: 4/10, step 102/574 completed (loss: 0.017396483570337296, acc: 1.0)
[2025-01-06 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50][root][INFO] - Training Epoch: 4/10, step 103/574 completed (loss: 0.03828638792037964, acc: 0.9772727489471436)
[2025-01-06 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50][root][INFO] - Training Epoch: 4/10, step 104/574 completed (loss: 0.32829633355140686, acc: 0.8965517282485962)
[2025-01-06 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51][root][INFO] - Training Epoch: 4/10, step 105/574 completed (loss: 0.09164189547300339, acc: 0.9534883499145508)
[2025-01-06 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51][root][INFO] - Training Epoch: 4/10, step 106/574 completed (loss: 0.2383081614971161, acc: 0.9200000166893005)
[2025-01-06 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52][root][INFO] - Training Epoch: 4/10, step 107/574 completed (loss: 0.012627365998923779, acc: 1.0)
[2025-01-06 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52][root][INFO] - Training Epoch: 4/10, step 108/574 completed (loss: 0.006031022407114506, acc: 1.0)
[2025-01-06 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52][root][INFO] - Training Epoch: 4/10, step 109/574 completed (loss: 0.038848891854286194, acc: 0.976190447807312)
[2025-01-06 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53][root][INFO] - Training Epoch: 4/10, step 110/574 completed (loss: 0.15161225199699402, acc: 0.9384615421295166)
[2025-01-06 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53][root][INFO] - Training Epoch: 4/10, step 111/574 completed (loss: 0.3272704780101776, acc: 0.8947368264198303)
[2025-01-06 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53][root][INFO] - Training Epoch: 4/10, step 112/574 completed (loss: 0.5505033135414124, acc: 0.8245614171028137)
[2025-01-06 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:54][root][INFO] - Training Epoch: 4/10, step 113/574 completed (loss: 0.2598865330219269, acc: 0.9487179517745972)
[2025-01-06 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:54][root][INFO] - Training Epoch: 4/10, step 114/574 completed (loss: 0.22864671051502228, acc: 0.918367326259613)
[2025-01-06 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:54][root][INFO] - Training Epoch: 4/10, step 115/574 completed (loss: 0.005392166785895824, acc: 1.0)
[2025-01-06 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55][root][INFO] - Training Epoch: 4/10, step 116/574 completed (loss: 0.25657200813293457, acc: 0.9365079402923584)
[2025-01-06 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55][root][INFO] - Training Epoch: 4/10, step 117/574 completed (loss: 0.3613496720790863, acc: 0.8943089246749878)
[2025-01-06 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:56][root][INFO] - Training Epoch: 4/10, step 118/574 completed (loss: 0.18282561004161835, acc: 0.9516128897666931)
[2025-01-06 01:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:56][root][INFO] - Training Epoch: 4/10, step 119/574 completed (loss: 0.6281323432922363, acc: 0.8365018963813782)
[2025-01-06 01:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57][root][INFO] - Training Epoch: 4/10, step 120/574 completed (loss: 0.16933922469615936, acc: 0.9599999785423279)
[2025-01-06 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57][root][INFO] - Training Epoch: 4/10, step 121/574 completed (loss: 0.34238573908805847, acc: 0.9230769276618958)
[2025-01-06 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58][root][INFO] - Training Epoch: 4/10, step 122/574 completed (loss: 0.021643327549099922, acc: 1.0)
[2025-01-06 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58][root][INFO] - Training Epoch: 4/10, step 123/574 completed (loss: 0.10135834664106369, acc: 0.9473684430122375)
[2025-01-06 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58][root][INFO] - Training Epoch: 4/10, step 124/574 completed (loss: 0.7494078874588013, acc: 0.7484662532806396)
[2025-01-06 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59][root][INFO] - Training Epoch: 4/10, step 125/574 completed (loss: 0.8461563587188721, acc: 0.7777777910232544)
[2025-01-06 01:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59][root][INFO] - Training Epoch: 4/10, step 126/574 completed (loss: 0.8733332753181458, acc: 0.7333333492279053)
[2025-01-06 01:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59][root][INFO] - Training Epoch: 4/10, step 127/574 completed (loss: 0.5047833323478699, acc: 0.8214285969734192)
[2025-01-06 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00][root][INFO] - Training Epoch: 4/10, step 128/574 completed (loss: 0.6494486331939697, acc: 0.800000011920929)
[2025-01-06 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00][root][INFO] - Training Epoch: 4/10, step 129/574 completed (loss: 0.63129723072052, acc: 0.8161764740943909)
[2025-01-06 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01][root][INFO] - Training Epoch: 4/10, step 130/574 completed (loss: 0.13190732896327972, acc: 0.9615384340286255)
[2025-01-06 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01][root][INFO] - Training Epoch: 4/10, step 131/574 completed (loss: 0.3995800018310547, acc: 0.9130434989929199)
[2025-01-06 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01][root][INFO] - Training Epoch: 4/10, step 132/574 completed (loss: 0.12644295394420624, acc: 1.0)
[2025-01-06 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02][root][INFO] - Training Epoch: 4/10, step 133/574 completed (loss: 0.42923256754875183, acc: 0.8260869383811951)
[2025-01-06 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02][root][INFO] - Training Epoch: 4/10, step 134/574 completed (loss: 0.10291770845651627, acc: 0.9714285731315613)
[2025-01-06 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02][root][INFO] - Training Epoch: 4/10, step 135/574 completed (loss: 0.20792429149150848, acc: 0.9615384340286255)
[2025-01-06 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03][root][INFO] - Training Epoch: 4/10, step 136/574 completed (loss: 0.2692171633243561, acc: 0.9047619104385376)
[2025-01-06 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9606, device='cuda:0') eval_epoch_loss=tensor(0.6733, device='cuda:0') eval_epoch_acc=tensor(0.8349, device='cuda:0')
[2025-01-06 01:19:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:19:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:19:33][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_137_loss_0.6732606291770935/model.pt
[2025-01-06 01:19:33][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:19:33][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.8348817825317383
[2025-01-06 01:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34][root][INFO] - Training Epoch: 4/10, step 137/574 completed (loss: 0.3507217764854431, acc: 0.8999999761581421)
[2025-01-06 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34][root][INFO] - Training Epoch: 4/10, step 138/574 completed (loss: 0.03613696247339249, acc: 1.0)
[2025-01-06 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34][root][INFO] - Training Epoch: 4/10, step 139/574 completed (loss: 0.024240439757704735, acc: 1.0)
[2025-01-06 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:35][root][INFO] - Training Epoch: 4/10, step 140/574 completed (loss: 0.12973825633525848, acc: 0.9615384340286255)
[2025-01-06 01:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:35][root][INFO] - Training Epoch: 4/10, step 141/574 completed (loss: 0.1588064283132553, acc: 0.9354838728904724)
[2025-01-06 01:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:36][root][INFO] - Training Epoch: 4/10, step 142/574 completed (loss: 0.24500060081481934, acc: 0.9459459185600281)
[2025-01-06 01:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:36][root][INFO] - Training Epoch: 4/10, step 143/574 completed (loss: 0.5391263365745544, acc: 0.7982456088066101)
[2025-01-06 01:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37][root][INFO] - Training Epoch: 4/10, step 144/574 completed (loss: 0.6243345737457275, acc: 0.8059701323509216)
[2025-01-06 01:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37][root][INFO] - Training Epoch: 4/10, step 145/574 completed (loss: 0.3923567831516266, acc: 0.8775510191917419)
[2025-01-06 01:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37][root][INFO] - Training Epoch: 4/10, step 146/574 completed (loss: 0.8226107954978943, acc: 0.7340425252914429)
[2025-01-06 01:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38][root][INFO] - Training Epoch: 4/10, step 147/574 completed (loss: 0.3526741564273834, acc: 0.8714285492897034)
[2025-01-06 01:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38][root][INFO] - Training Epoch: 4/10, step 148/574 completed (loss: 0.21439680457115173, acc: 0.9285714030265808)
[2025-01-06 01:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38][root][INFO] - Training Epoch: 4/10, step 149/574 completed (loss: 0.08658505976200104, acc: 0.95652174949646)
[2025-01-06 01:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:39][root][INFO] - Training Epoch: 4/10, step 150/574 completed (loss: 0.045463163405656815, acc: 1.0)
[2025-01-06 01:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:39][root][INFO] - Training Epoch: 4/10, step 151/574 completed (loss: 0.5652123093605042, acc: 0.804347813129425)
[2025-01-06 01:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:39][root][INFO] - Training Epoch: 4/10, step 152/574 completed (loss: 0.3312835991382599, acc: 0.8983050584793091)
[2025-01-06 01:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:40][root][INFO] - Training Epoch: 4/10, step 153/574 completed (loss: 0.38315775990486145, acc: 0.8947368264198303)
[2025-01-06 01:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:40][root][INFO] - Training Epoch: 4/10, step 154/574 completed (loss: 0.5013526678085327, acc: 0.837837815284729)
[2025-01-06 01:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41][root][INFO] - Training Epoch: 4/10, step 155/574 completed (loss: 0.12984026968479156, acc: 0.8928571343421936)
[2025-01-06 01:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41][root][INFO] - Training Epoch: 4/10, step 156/574 completed (loss: 0.028890011832118034, acc: 1.0)
[2025-01-06 01:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41][root][INFO] - Training Epoch: 4/10, step 157/574 completed (loss: 0.6187095046043396, acc: 0.8421052694320679)
[2025-01-06 01:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:43][root][INFO] - Training Epoch: 4/10, step 158/574 completed (loss: 0.5810673832893372, acc: 0.7702702879905701)
[2025-01-06 01:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:43][root][INFO] - Training Epoch: 4/10, step 159/574 completed (loss: 0.7853304743766785, acc: 0.7222222089767456)
[2025-01-06 01:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44][root][INFO] - Training Epoch: 4/10, step 160/574 completed (loss: 0.7728598117828369, acc: 0.7674418687820435)
[2025-01-06 01:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44][root][INFO] - Training Epoch: 4/10, step 161/574 completed (loss: 1.0559359788894653, acc: 0.7411764860153198)
[2025-01-06 01:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:45][root][INFO] - Training Epoch: 4/10, step 162/574 completed (loss: 1.0237449407577515, acc: 0.7640449404716492)
[2025-01-06 01:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:45][root][INFO] - Training Epoch: 4/10, step 163/574 completed (loss: 0.3383086919784546, acc: 0.9090909361839294)
[2025-01-06 01:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46][root][INFO] - Training Epoch: 4/10, step 164/574 completed (loss: 0.18647174537181854, acc: 0.9523809552192688)
[2025-01-06 01:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46][root][INFO] - Training Epoch: 4/10, step 165/574 completed (loss: 0.569567859172821, acc: 0.8275862336158752)
[2025-01-06 01:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46][root][INFO] - Training Epoch: 4/10, step 166/574 completed (loss: 0.2379361242055893, acc: 0.9387755393981934)
[2025-01-06 01:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47][root][INFO] - Training Epoch: 4/10, step 167/574 completed (loss: 0.08440667390823364, acc: 0.9800000190734863)
[2025-01-06 01:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47][root][INFO] - Training Epoch: 4/10, step 168/574 completed (loss: 0.4037245512008667, acc: 0.8888888955116272)
[2025-01-06 01:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47][root][INFO] - Training Epoch: 4/10, step 169/574 completed (loss: 0.9330845475196838, acc: 0.7647058963775635)
[2025-01-06 01:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:48][root][INFO] - Training Epoch: 4/10, step 170/574 completed (loss: 0.673824667930603, acc: 0.8082191944122314)
[2025-01-06 01:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:49][root][INFO] - Training Epoch: 4/10, step 171/574 completed (loss: 0.17332255840301514, acc: 0.9583333134651184)
[2025-01-06 01:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:49][root][INFO] - Training Epoch: 4/10, step 172/574 completed (loss: 0.5129712820053101, acc: 0.8518518805503845)
[2025-01-06 01:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:49][root][INFO] - Training Epoch: 4/10, step 173/574 completed (loss: 0.2075737565755844, acc: 0.9642857313156128)
[2025-01-06 01:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:50][root][INFO] - Training Epoch: 4/10, step 174/574 completed (loss: 0.9648066759109497, acc: 0.7345132827758789)
[2025-01-06 01:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:50][root][INFO] - Training Epoch: 4/10, step 175/574 completed (loss: 0.6829808354377747, acc: 0.7971014380455017)
[2025-01-06 01:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:51][root][INFO] - Training Epoch: 4/10, step 176/574 completed (loss: 0.22799935936927795, acc: 0.9204545617103577)
[2025-01-06 01:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:52][root][INFO] - Training Epoch: 4/10, step 177/574 completed (loss: 1.1668695211410522, acc: 0.6717557311058044)
[2025-01-06 01:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:52][root][INFO] - Training Epoch: 4/10, step 178/574 completed (loss: 0.8339667916297913, acc: 0.7555555701255798)
[2025-01-06 01:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53][root][INFO] - Training Epoch: 4/10, step 179/574 completed (loss: 0.36522310972213745, acc: 0.868852436542511)
[2025-01-06 01:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53][root][INFO] - Training Epoch: 4/10, step 180/574 completed (loss: 0.0035420414060354233, acc: 1.0)
[2025-01-06 01:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53][root][INFO] - Training Epoch: 4/10, step 181/574 completed (loss: 0.012440201826393604, acc: 1.0)
[2025-01-06 01:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54][root][INFO] - Training Epoch: 4/10, step 182/574 completed (loss: 0.08723090589046478, acc: 0.9642857313156128)
[2025-01-06 01:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54][root][INFO] - Training Epoch: 4/10, step 183/574 completed (loss: 0.2256375551223755, acc: 0.9268292784690857)
[2025-01-06 01:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54][root][INFO] - Training Epoch: 4/10, step 184/574 completed (loss: 0.37289419770240784, acc: 0.9093655347824097)
[2025-01-06 01:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:55][root][INFO] - Training Epoch: 4/10, step 185/574 completed (loss: 0.43154898285865784, acc: 0.8962535858154297)
[2025-01-06 01:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:55][root][INFO] - Training Epoch: 4/10, step 186/574 completed (loss: 0.3742423951625824, acc: 0.8999999761581421)
[2025-01-06 01:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56][root][INFO] - Training Epoch: 4/10, step 187/574 completed (loss: 0.5095587968826294, acc: 0.8611631989479065)
[2025-01-06 01:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56][root][INFO] - Training Epoch: 4/10, step 188/574 completed (loss: 0.47390052676200867, acc: 0.854092538356781)
[2025-01-06 01:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56][root][INFO] - Training Epoch: 4/10, step 189/574 completed (loss: 0.10047892481088638, acc: 1.0)
[2025-01-06 01:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:57][root][INFO] - Training Epoch: 4/10, step 190/574 completed (loss: 0.6478592753410339, acc: 0.7906976938247681)
[2025-01-06 01:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:58][root][INFO] - Training Epoch: 4/10, step 191/574 completed (loss: 1.0336121320724487, acc: 0.6984127163887024)
[2025-01-06 01:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:59][root][INFO] - Training Epoch: 4/10, step 192/574 completed (loss: 0.9025354385375977, acc: 0.7272727489471436)
[2025-01-06 01:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:59][root][INFO] - Training Epoch: 4/10, step 193/574 completed (loss: 0.6506028771400452, acc: 0.8117647171020508)
[2025-01-06 01:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:01][root][INFO] - Training Epoch: 4/10, step 194/574 completed (loss: 0.8836892247200012, acc: 0.7592592835426331)
[2025-01-06 01:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:01][root][INFO] - Training Epoch: 4/10, step 195/574 completed (loss: 0.30984076857566833, acc: 0.9193548560142517)
[2025-01-06 01:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02][root][INFO] - Training Epoch: 4/10, step 196/574 completed (loss: 0.035883910953998566, acc: 1.0)
[2025-01-06 01:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02][root][INFO] - Training Epoch: 4/10, step 197/574 completed (loss: 0.3199087083339691, acc: 0.8500000238418579)
[2025-01-06 01:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02][root][INFO] - Training Epoch: 4/10, step 198/574 completed (loss: 0.6028430461883545, acc: 0.8529411554336548)
[2025-01-06 01:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:03][root][INFO] - Training Epoch: 4/10, step 199/574 completed (loss: 0.9757100343704224, acc: 0.7426470518112183)
[2025-01-06 01:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:03][root][INFO] - Training Epoch: 4/10, step 200/574 completed (loss: 0.6432700157165527, acc: 0.8220338821411133)
[2025-01-06 01:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04][root][INFO] - Training Epoch: 4/10, step 201/574 completed (loss: 0.7736186385154724, acc: 0.7761194109916687)
[2025-01-06 01:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04][root][INFO] - Training Epoch: 4/10, step 202/574 completed (loss: 0.7095327377319336, acc: 0.7961165308952332)
[2025-01-06 01:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04][root][INFO] - Training Epoch: 4/10, step 203/574 completed (loss: 0.48252439498901367, acc: 0.8730158805847168)
[2025-01-06 01:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05][root][INFO] - Training Epoch: 4/10, step 204/574 completed (loss: 0.07934793084859848, acc: 0.9560439586639404)
[2025-01-06 01:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05][root][INFO] - Training Epoch: 4/10, step 205/574 completed (loss: 0.20436272025108337, acc: 0.9506726264953613)
[2025-01-06 01:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05][root][INFO] - Training Epoch: 4/10, step 206/574 completed (loss: 0.3621352016925812, acc: 0.8858267664909363)
[2025-01-06 01:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06][root][INFO] - Training Epoch: 4/10, step 207/574 completed (loss: 0.259611040353775, acc: 0.9353448152542114)
[2025-01-06 01:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06][root][INFO] - Training Epoch: 4/10, step 208/574 completed (loss: 0.3682381808757782, acc: 0.9094203114509583)
[2025-01-06 01:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07][root][INFO] - Training Epoch: 4/10, step 209/574 completed (loss: 0.2861364483833313, acc: 0.9027237296104431)
[2025-01-06 01:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07][root][INFO] - Training Epoch: 4/10, step 210/574 completed (loss: 0.1417105346918106, acc: 0.95652174949646)
[2025-01-06 01:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07][root][INFO] - Training Epoch: 4/10, step 211/574 completed (loss: 0.04632413759827614, acc: 1.0)
[2025-01-06 01:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08][root][INFO] - Training Epoch: 4/10, step 212/574 completed (loss: 0.49452584981918335, acc: 0.8571428656578064)
[2025-01-06 01:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08][root][INFO] - Training Epoch: 4/10, step 213/574 completed (loss: 0.10086455941200256, acc: 0.957446813583374)
[2025-01-06 01:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09][root][INFO] - Training Epoch: 4/10, step 214/574 completed (loss: 0.15171784162521362, acc: 0.9692307710647583)
[2025-01-06 01:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09][root][INFO] - Training Epoch: 4/10, step 215/574 completed (loss: 0.14613063633441925, acc: 0.9729729890823364)
[2025-01-06 01:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09][root][INFO] - Training Epoch: 4/10, step 216/574 completed (loss: 0.09446313977241516, acc: 0.9767441749572754)
[2025-01-06 01:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10][root][INFO] - Training Epoch: 4/10, step 217/574 completed (loss: 0.20821548998355865, acc: 0.9459459185600281)
[2025-01-06 01:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10][root][INFO] - Training Epoch: 4/10, step 218/574 completed (loss: 0.13689938187599182, acc: 0.9444444179534912)
[2025-01-06 01:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:11][root][INFO] - Training Epoch: 4/10, step 219/574 completed (loss: 0.0359843447804451, acc: 1.0)
[2025-01-06 01:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:11][root][INFO] - Training Epoch: 4/10, step 220/574 completed (loss: 0.05595815181732178, acc: 0.9629629850387573)
[2025-01-06 01:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12][root][INFO] - Training Epoch: 4/10, step 221/574 completed (loss: 0.18912586569786072, acc: 0.9599999785423279)
[2025-01-06 01:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12][root][INFO] - Training Epoch: 4/10, step 222/574 completed (loss: 0.43739035725593567, acc: 0.9038461446762085)
[2025-01-06 01:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:13][root][INFO] - Training Epoch: 4/10, step 223/574 completed (loss: 0.3307238817214966, acc: 0.91847825050354)
[2025-01-06 01:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:13][root][INFO] - Training Epoch: 4/10, step 224/574 completed (loss: 0.519719660282135, acc: 0.8181818127632141)
[2025-01-06 01:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14][root][INFO] - Training Epoch: 4/10, step 225/574 completed (loss: 0.5965326428413391, acc: 0.8085106611251831)
[2025-01-06 01:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14][root][INFO] - Training Epoch: 4/10, step 226/574 completed (loss: 0.37341710925102234, acc: 0.9056603908538818)
[2025-01-06 01:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14][root][INFO] - Training Epoch: 4/10, step 227/574 completed (loss: 0.3014659285545349, acc: 0.9166666865348816)
[2025-01-06 01:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15][root][INFO] - Training Epoch: 4/10, step 228/574 completed (loss: 0.17822366952896118, acc: 0.9069767594337463)
[2025-01-06 01:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15][root][INFO] - Training Epoch: 4/10, step 229/574 completed (loss: 0.6171354651451111, acc: 0.800000011920929)
[2025-01-06 01:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15][root][INFO] - Training Epoch: 4/10, step 230/574 completed (loss: 1.6458402872085571, acc: 0.5684210658073425)
[2025-01-06 01:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16][root][INFO] - Training Epoch: 4/10, step 231/574 completed (loss: 1.2535566091537476, acc: 0.644444465637207)
[2025-01-06 01:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16][root][INFO] - Training Epoch: 4/10, step 232/574 completed (loss: 1.0760852098464966, acc: 0.699999988079071)
[2025-01-06 01:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:17][root][INFO] - Training Epoch: 4/10, step 233/574 completed (loss: 1.6066793203353882, acc: 0.536697268486023)
[2025-01-06 01:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:17][root][INFO] - Training Epoch: 4/10, step 234/574 completed (loss: 1.1541016101837158, acc: 0.6692307591438293)
[2025-01-06 01:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18][root][INFO] - Training Epoch: 4/10, step 235/574 completed (loss: 0.2997646927833557, acc: 0.8947368264198303)
[2025-01-06 01:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18][root][INFO] - Training Epoch: 4/10, step 236/574 completed (loss: 0.08273845165967941, acc: 1.0)
[2025-01-06 01:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18][root][INFO] - Training Epoch: 4/10, step 237/574 completed (loss: 0.41701382398605347, acc: 0.8636363744735718)
[2025-01-06 01:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19][root][INFO] - Training Epoch: 4/10, step 238/574 completed (loss: 0.4047975540161133, acc: 0.8888888955116272)
[2025-01-06 01:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19][root][INFO] - Training Epoch: 4/10, step 239/574 completed (loss: 0.19042949378490448, acc: 0.9714285731315613)
[2025-01-06 01:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19][root][INFO] - Training Epoch: 4/10, step 240/574 completed (loss: 0.6569918394088745, acc: 0.8863636255264282)
[2025-01-06 01:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:20][root][INFO] - Training Epoch: 4/10, step 241/574 completed (loss: 0.19773341715335846, acc: 0.9318181872367859)
[2025-01-06 01:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:20][root][INFO] - Training Epoch: 4/10, step 242/574 completed (loss: 0.6774840950965881, acc: 0.7903226017951965)
[2025-01-06 01:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21][root][INFO] - Training Epoch: 4/10, step 243/574 completed (loss: 0.6391875743865967, acc: 0.8181818127632141)
[2025-01-06 01:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21][root][INFO] - Training Epoch: 4/10, step 244/574 completed (loss: 0.028497235849499702, acc: 1.0)
[2025-01-06 01:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21][root][INFO] - Training Epoch: 4/10, step 245/574 completed (loss: 0.1549498438835144, acc: 0.9615384340286255)
[2025-01-06 01:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22][root][INFO] - Training Epoch: 4/10, step 246/574 completed (loss: 0.040901318192481995, acc: 0.9677419066429138)
[2025-01-06 01:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22][root][INFO] - Training Epoch: 4/10, step 247/574 completed (loss: 0.12861087918281555, acc: 0.949999988079071)
[2025-01-06 01:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23][root][INFO] - Training Epoch: 4/10, step 248/574 completed (loss: 0.12859965860843658, acc: 0.9459459185600281)
[2025-01-06 01:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23][root][INFO] - Training Epoch: 4/10, step 249/574 completed (loss: 0.2745617628097534, acc: 0.9189189076423645)
[2025-01-06 01:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23][root][INFO] - Training Epoch: 4/10, step 250/574 completed (loss: 0.029276402667164803, acc: 1.0)
[2025-01-06 01:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24][root][INFO] - Training Epoch: 4/10, step 251/574 completed (loss: 0.13995306193828583, acc: 0.9411764740943909)
[2025-01-06 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24][root][INFO] - Training Epoch: 4/10, step 252/574 completed (loss: 0.11648611724376678, acc: 0.9512194991111755)
[2025-01-06 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24][root][INFO] - Training Epoch: 4/10, step 253/574 completed (loss: 0.003987269476056099, acc: 1.0)
[2025-01-06 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25][root][INFO] - Training Epoch: 4/10, step 254/574 completed (loss: 0.01645890437066555, acc: 1.0)
[2025-01-06 01:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25][root][INFO] - Training Epoch: 4/10, step 255/574 completed (loss: 0.010393165983259678, acc: 1.0)
[2025-01-06 01:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25][root][INFO] - Training Epoch: 4/10, step 256/574 completed (loss: 0.20810075104236603, acc: 0.9473684430122375)
[2025-01-06 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:26][root][INFO] - Training Epoch: 4/10, step 257/574 completed (loss: 0.27890288829803467, acc: 0.9285714030265808)
[2025-01-06 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:26][root][INFO] - Training Epoch: 4/10, step 258/574 completed (loss: 0.18651674687862396, acc: 0.9473684430122375)
[2025-01-06 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27][root][INFO] - Training Epoch: 4/10, step 259/574 completed (loss: 0.4420483112335205, acc: 0.8679245114326477)
[2025-01-06 01:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27][root][INFO] - Training Epoch: 4/10, step 260/574 completed (loss: 0.24279151856899261, acc: 0.9166666865348816)
[2025-01-06 01:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28][root][INFO] - Training Epoch: 4/10, step 261/574 completed (loss: 0.03913861885666847, acc: 1.0)
[2025-01-06 01:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28][root][INFO] - Training Epoch: 4/10, step 262/574 completed (loss: 0.25006598234176636, acc: 0.9677419066429138)
[2025-01-06 01:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28][root][INFO] - Training Epoch: 4/10, step 263/574 completed (loss: 0.8515480160713196, acc: 0.800000011920929)
[2025-01-06 01:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:29][root][INFO] - Training Epoch: 4/10, step 264/574 completed (loss: 0.6735844016075134, acc: 0.7291666865348816)
[2025-01-06 01:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30][root][INFO] - Training Epoch: 4/10, step 265/574 completed (loss: 1.194926381111145, acc: 0.6959999799728394)
[2025-01-06 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30][root][INFO] - Training Epoch: 4/10, step 266/574 completed (loss: 1.0903522968292236, acc: 0.7078651785850525)
[2025-01-06 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30][root][INFO] - Training Epoch: 4/10, step 267/574 completed (loss: 0.5487020611763, acc: 0.8108108043670654)
[2025-01-06 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31][root][INFO] - Training Epoch: 4/10, step 268/574 completed (loss: 0.5169104933738708, acc: 0.8103448152542114)
[2025-01-06 01:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31][root][INFO] - Training Epoch: 4/10, step 269/574 completed (loss: 0.016308866441249847, acc: 1.0)
[2025-01-06 01:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31][root][INFO] - Training Epoch: 4/10, step 270/574 completed (loss: 0.07240009307861328, acc: 1.0)
[2025-01-06 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32][root][INFO] - Training Epoch: 4/10, step 271/574 completed (loss: 0.009807689115405083, acc: 1.0)
[2025-01-06 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32][root][INFO] - Training Epoch: 4/10, step 272/574 completed (loss: 0.09545411914587021, acc: 0.9666666388511658)
[2025-01-06 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33][root][INFO] - Training Epoch: 4/10, step 273/574 completed (loss: 0.1424160599708557, acc: 0.9666666388511658)
[2025-01-06 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33][root][INFO] - Training Epoch: 4/10, step 274/574 completed (loss: 0.21571630239486694, acc: 0.9375)
[2025-01-06 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33][root][INFO] - Training Epoch: 4/10, step 275/574 completed (loss: 0.09034531563520432, acc: 1.0)
[2025-01-06 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34][root][INFO] - Training Epoch: 4/10, step 276/574 completed (loss: 0.2652711272239685, acc: 0.931034505367279)
[2025-01-06 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34][root][INFO] - Training Epoch: 4/10, step 277/574 completed (loss: 0.21901682019233704, acc: 0.9200000166893005)
[2025-01-06 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34][root][INFO] - Training Epoch: 4/10, step 278/574 completed (loss: 0.5585406422615051, acc: 0.8510638475418091)
[2025-01-06 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:35][root][INFO] - Training Epoch: 4/10, step 279/574 completed (loss: 0.27524349093437195, acc: 0.8958333134651184)
[2025-01-06 01:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0713, device='cuda:0') eval_epoch_loss=tensor(0.7282, device='cuda:0') eval_epoch_acc=tensor(0.8149, device='cuda:0')
[2025-01-06 01:21:05][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:21:05][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:21:06][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_280_loss_0.7281871438026428/model.pt
[2025-01-06 01:21:06][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06][root][INFO] - Training Epoch: 4/10, step 280/574 completed (loss: 0.15222173929214478, acc: 0.9318181872367859)
[2025-01-06 01:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06][root][INFO] - Training Epoch: 4/10, step 281/574 completed (loss: 0.4450002908706665, acc: 0.8313252925872803)
[2025-01-06 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07][root][INFO] - Training Epoch: 4/10, step 282/574 completed (loss: 0.7683567404747009, acc: 0.8055555820465088)
[2025-01-06 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07][root][INFO] - Training Epoch: 4/10, step 283/574 completed (loss: 0.03555142506957054, acc: 1.0)
[2025-01-06 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07][root][INFO] - Training Epoch: 4/10, step 284/574 completed (loss: 0.3964216411113739, acc: 0.9411764740943909)
[2025-01-06 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08][root][INFO] - Training Epoch: 4/10, step 285/574 completed (loss: 0.07220914959907532, acc: 0.9750000238418579)
[2025-01-06 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08][root][INFO] - Training Epoch: 4/10, step 286/574 completed (loss: 0.3607608675956726, acc: 0.90625)
[2025-01-06 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09][root][INFO] - Training Epoch: 4/10, step 287/574 completed (loss: 0.4251275062561035, acc: 0.8799999952316284)
[2025-01-06 01:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09][root][INFO] - Training Epoch: 4/10, step 288/574 completed (loss: 0.3022698760032654, acc: 0.8901098966598511)
[2025-01-06 01:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09][root][INFO] - Training Epoch: 4/10, step 289/574 completed (loss: 0.35760948061943054, acc: 0.888198733329773)
[2025-01-06 01:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10][root][INFO] - Training Epoch: 4/10, step 290/574 completed (loss: 0.4320169687271118, acc: 0.8917526006698608)
[2025-01-06 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10][root][INFO] - Training Epoch: 4/10, step 291/574 completed (loss: 0.03621724620461464, acc: 1.0)
[2025-01-06 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10][root][INFO] - Training Epoch: 4/10, step 292/574 completed (loss: 0.17612753808498383, acc: 0.9523809552192688)
[2025-01-06 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:11][root][INFO] - Training Epoch: 4/10, step 293/574 completed (loss: 0.12567827105522156, acc: 0.9655172228813171)
[2025-01-06 01:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:13][root][INFO] - Training Epoch: 4/10, step 294/574 completed (loss: 0.23976507782936096, acc: 0.9272727370262146)
[2025-01-06 01:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14][root][INFO] - Training Epoch: 4/10, step 295/574 completed (loss: 0.6130373477935791, acc: 0.8453608155250549)
[2025-01-06 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14][root][INFO] - Training Epoch: 4/10, step 296/574 completed (loss: 0.2190001755952835, acc: 0.8620689511299133)
[2025-01-06 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14][root][INFO] - Training Epoch: 4/10, step 297/574 completed (loss: 0.016672981902956963, acc: 1.0)
[2025-01-06 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15][root][INFO] - Training Epoch: 4/10, step 298/574 completed (loss: 0.16415934264659882, acc: 0.9473684430122375)
[2025-01-06 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15][root][INFO] - Training Epoch: 4/10, step 299/574 completed (loss: 0.06936071068048477, acc: 0.9821428656578064)
[2025-01-06 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15][root][INFO] - Training Epoch: 4/10, step 300/574 completed (loss: 0.2828173041343689, acc: 0.90625)
[2025-01-06 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16][root][INFO] - Training Epoch: 4/10, step 301/574 completed (loss: 0.17218761146068573, acc: 0.9245283007621765)
[2025-01-06 01:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16][root][INFO] - Training Epoch: 4/10, step 302/574 completed (loss: 0.015957552939653397, acc: 1.0)
[2025-01-06 01:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16][root][INFO] - Training Epoch: 4/10, step 303/574 completed (loss: 0.05598403513431549, acc: 0.970588207244873)
[2025-01-06 01:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17][root][INFO] - Training Epoch: 4/10, step 304/574 completed (loss: 0.0423211008310318, acc: 1.0)
[2025-01-06 01:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17][root][INFO] - Training Epoch: 4/10, step 305/574 completed (loss: 0.2123013287782669, acc: 0.9180327653884888)
[2025-01-06 01:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17][root][INFO] - Training Epoch: 4/10, step 306/574 completed (loss: 0.060674652457237244, acc: 1.0)
[2025-01-06 01:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18][root][INFO] - Training Epoch: 4/10, step 307/574 completed (loss: 0.004761653486639261, acc: 1.0)
[2025-01-06 01:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18][root][INFO] - Training Epoch: 4/10, step 308/574 completed (loss: 0.20445966720581055, acc: 0.9275362491607666)
[2025-01-06 01:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19][root][INFO] - Training Epoch: 4/10, step 309/574 completed (loss: 0.09994117170572281, acc: 0.9722222089767456)
[2025-01-06 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19][root][INFO] - Training Epoch: 4/10, step 310/574 completed (loss: 0.0937582328915596, acc: 0.9759036302566528)
[2025-01-06 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19][root][INFO] - Training Epoch: 4/10, step 311/574 completed (loss: 0.15515699982643127, acc: 0.9615384340286255)
[2025-01-06 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20][root][INFO] - Training Epoch: 4/10, step 312/574 completed (loss: 0.1024276614189148, acc: 0.9795918464660645)
[2025-01-06 01:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20][root][INFO] - Training Epoch: 4/10, step 313/574 completed (loss: 0.004831863567233086, acc: 1.0)
[2025-01-06 01:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20][root][INFO] - Training Epoch: 4/10, step 314/574 completed (loss: 0.005703693721443415, acc: 1.0)
[2025-01-06 01:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21][root][INFO] - Training Epoch: 4/10, step 315/574 completed (loss: 0.16618698835372925, acc: 0.9677419066429138)
[2025-01-06 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21][root][INFO] - Training Epoch: 4/10, step 316/574 completed (loss: 0.33269813656806946, acc: 0.8709677457809448)
[2025-01-06 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21][root][INFO] - Training Epoch: 4/10, step 317/574 completed (loss: 0.180448979139328, acc: 0.9253731369972229)
[2025-01-06 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22][root][INFO] - Training Epoch: 4/10, step 318/574 completed (loss: 0.10186808556318283, acc: 0.9615384340286255)
[2025-01-06 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22][root][INFO] - Training Epoch: 4/10, step 319/574 completed (loss: 0.16408155858516693, acc: 0.9333333373069763)
[2025-01-06 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22][root][INFO] - Training Epoch: 4/10, step 320/574 completed (loss: 0.05968225374817848, acc: 0.9677419066429138)
[2025-01-06 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23][root][INFO] - Training Epoch: 4/10, step 321/574 completed (loss: 0.006041589193046093, acc: 1.0)
[2025-01-06 01:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23][root][INFO] - Training Epoch: 4/10, step 322/574 completed (loss: 0.7607033252716064, acc: 0.7777777910232544)
[2025-01-06 01:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23][root][INFO] - Training Epoch: 4/10, step 323/574 completed (loss: 0.641832709312439, acc: 0.8285714387893677)
[2025-01-06 01:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24][root][INFO] - Training Epoch: 4/10, step 324/574 completed (loss: 0.5186849236488342, acc: 0.8717948794364929)
[2025-01-06 01:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24][root][INFO] - Training Epoch: 4/10, step 325/574 completed (loss: 0.7182384133338928, acc: 0.7317073345184326)
[2025-01-06 01:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24][root][INFO] - Training Epoch: 4/10, step 326/574 completed (loss: 0.3662148714065552, acc: 0.8157894611358643)
[2025-01-06 01:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25][root][INFO] - Training Epoch: 4/10, step 327/574 completed (loss: 0.038249943405389786, acc: 1.0)
[2025-01-06 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25][root][INFO] - Training Epoch: 4/10, step 328/574 completed (loss: 0.010504183359444141, acc: 1.0)
[2025-01-06 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25][root][INFO] - Training Epoch: 4/10, step 329/574 completed (loss: 0.12264353781938553, acc: 0.9629629850387573)
[2025-01-06 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26][root][INFO] - Training Epoch: 4/10, step 330/574 completed (loss: 0.003111542435362935, acc: 1.0)
[2025-01-06 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26][root][INFO] - Training Epoch: 4/10, step 331/574 completed (loss: 0.28246772289276123, acc: 0.9516128897666931)
[2025-01-06 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26][root][INFO] - Training Epoch: 4/10, step 332/574 completed (loss: 0.017503833398222923, acc: 1.0)
[2025-01-06 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27][root][INFO] - Training Epoch: 4/10, step 333/574 completed (loss: 0.14382131397724152, acc: 0.96875)
[2025-01-06 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27][root][INFO] - Training Epoch: 4/10, step 334/574 completed (loss: 0.014750657603144646, acc: 1.0)
[2025-01-06 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27][root][INFO] - Training Epoch: 4/10, step 335/574 completed (loss: 0.17064766585826874, acc: 0.8947368264198303)
[2025-01-06 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28][root][INFO] - Training Epoch: 4/10, step 336/574 completed (loss: 0.30788227915763855, acc: 0.8999999761581421)
[2025-01-06 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28][root][INFO] - Training Epoch: 4/10, step 337/574 completed (loss: 0.6801286339759827, acc: 0.7471264600753784)
[2025-01-06 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28][root][INFO] - Training Epoch: 4/10, step 338/574 completed (loss: 1.007913589477539, acc: 0.6914893388748169)
[2025-01-06 01:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:29][root][INFO] - Training Epoch: 4/10, step 339/574 completed (loss: 0.8326373100280762, acc: 0.759036123752594)
[2025-01-06 01:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:29][root][INFO] - Training Epoch: 4/10, step 340/574 completed (loss: 0.00480172224342823, acc: 1.0)
[2025-01-06 01:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:29][root][INFO] - Training Epoch: 4/10, step 341/574 completed (loss: 0.1382802277803421, acc: 0.9230769276618958)
[2025-01-06 01:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30][root][INFO] - Training Epoch: 4/10, step 342/574 completed (loss: 0.12498508393764496, acc: 0.9638554453849792)
[2025-01-06 01:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30][root][INFO] - Training Epoch: 4/10, step 343/574 completed (loss: 0.3145267963409424, acc: 0.9056603908538818)
[2025-01-06 01:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31][root][INFO] - Training Epoch: 4/10, step 344/574 completed (loss: 0.1282031089067459, acc: 0.9620253443717957)
[2025-01-06 01:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31][root][INFO] - Training Epoch: 4/10, step 345/574 completed (loss: 0.05416213721036911, acc: 0.9803921580314636)
[2025-01-06 01:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31][root][INFO] - Training Epoch: 4/10, step 346/574 completed (loss: 0.21543055772781372, acc: 0.9552238583564758)
[2025-01-06 01:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32][root][INFO] - Training Epoch: 4/10, step 347/574 completed (loss: 0.0463738739490509, acc: 0.949999988079071)
[2025-01-06 01:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32][root][INFO] - Training Epoch: 4/10, step 348/574 completed (loss: 0.0901418998837471, acc: 0.9599999785423279)
[2025-01-06 01:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32][root][INFO] - Training Epoch: 4/10, step 349/574 completed (loss: 0.4509819746017456, acc: 0.8611111044883728)
[2025-01-06 01:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33][root][INFO] - Training Epoch: 4/10, step 350/574 completed (loss: 0.6013359427452087, acc: 0.7906976938247681)
[2025-01-06 01:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33][root][INFO] - Training Epoch: 4/10, step 351/574 completed (loss: 0.0982942208647728, acc: 0.9743589758872986)
[2025-01-06 01:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33][root][INFO] - Training Epoch: 4/10, step 352/574 completed (loss: 0.6148399710655212, acc: 0.7555555701255798)
[2025-01-06 01:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:34][root][INFO] - Training Epoch: 4/10, step 353/574 completed (loss: 0.014246582984924316, acc: 1.0)
[2025-01-06 01:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:34][root][INFO] - Training Epoch: 4/10, step 354/574 completed (loss: 0.40411561727523804, acc: 0.8846153616905212)
[2025-01-06 01:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:34][root][INFO] - Training Epoch: 4/10, step 355/574 completed (loss: 0.7333722114562988, acc: 0.8021978139877319)
[2025-01-06 01:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:35][root][INFO] - Training Epoch: 4/10, step 356/574 completed (loss: 0.43997544050216675, acc: 0.843478262424469)
[2025-01-06 01:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:35][root][INFO] - Training Epoch: 4/10, step 357/574 completed (loss: 0.31581711769104004, acc: 0.9021739363670349)
[2025-01-06 01:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36][root][INFO] - Training Epoch: 4/10, step 358/574 completed (loss: 0.26962482929229736, acc: 0.9591836929321289)
[2025-01-06 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36][root][INFO] - Training Epoch: 4/10, step 359/574 completed (loss: 0.0011892591137439013, acc: 1.0)
[2025-01-06 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36][root][INFO] - Training Epoch: 4/10, step 360/574 completed (loss: 0.11493802070617676, acc: 0.9230769276618958)
[2025-01-06 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37][root][INFO] - Training Epoch: 4/10, step 361/574 completed (loss: 0.27374041080474854, acc: 0.8780487775802612)
[2025-01-06 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37][root][INFO] - Training Epoch: 4/10, step 362/574 completed (loss: 0.5707768201828003, acc: 0.8666666746139526)
[2025-01-06 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37][root][INFO] - Training Epoch: 4/10, step 363/574 completed (loss: 0.07340347766876221, acc: 0.9868420958518982)
[2025-01-06 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37][root][INFO] - Training Epoch: 4/10, step 364/574 completed (loss: 0.04781655594706535, acc: 1.0)
[2025-01-06 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38][root][INFO] - Training Epoch: 4/10, step 365/574 completed (loss: 0.041060108691453934, acc: 1.0)
[2025-01-06 01:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38][root][INFO] - Training Epoch: 4/10, step 366/574 completed (loss: 0.185704305768013, acc: 0.9583333134651184)
[2025-01-06 01:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38][root][INFO] - Training Epoch: 4/10, step 367/574 completed (loss: 0.00156989018432796, acc: 1.0)
[2025-01-06 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:39][root][INFO] - Training Epoch: 4/10, step 368/574 completed (loss: 0.03659499064087868, acc: 1.0)
[2025-01-06 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:39][root][INFO] - Training Epoch: 4/10, step 369/574 completed (loss: 0.27371665835380554, acc: 0.90625)
[2025-01-06 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:40][root][INFO] - Training Epoch: 4/10, step 370/574 completed (loss: 0.36944523453712463, acc: 0.903030276298523)
[2025-01-06 01:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41][root][INFO] - Training Epoch: 4/10, step 371/574 completed (loss: 0.2494678497314453, acc: 0.9150943160057068)
[2025-01-06 01:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41][root][INFO] - Training Epoch: 4/10, step 372/574 completed (loss: 0.14909440279006958, acc: 0.9555555582046509)
[2025-01-06 01:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41][root][INFO] - Training Epoch: 4/10, step 373/574 completed (loss: 0.09719843417406082, acc: 0.9821428656578064)
[2025-01-06 01:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42][root][INFO] - Training Epoch: 4/10, step 374/574 completed (loss: 0.048467688262462616, acc: 1.0)
[2025-01-06 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42][root][INFO] - Training Epoch: 4/10, step 375/574 completed (loss: 0.0038903376553207636, acc: 1.0)
[2025-01-06 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42][root][INFO] - Training Epoch: 4/10, step 376/574 completed (loss: 0.04414154216647148, acc: 0.95652174949646)
[2025-01-06 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43][root][INFO] - Training Epoch: 4/10, step 377/574 completed (loss: 0.037184346467256546, acc: 1.0)
[2025-01-06 01:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43][root][INFO] - Training Epoch: 4/10, step 378/574 completed (loss: 0.012583471834659576, acc: 1.0)
[2025-01-06 01:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44][root][INFO] - Training Epoch: 4/10, step 379/574 completed (loss: 0.2492944449186325, acc: 0.916167676448822)
[2025-01-06 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44][root][INFO] - Training Epoch: 4/10, step 380/574 completed (loss: 0.3528273105621338, acc: 0.9097744226455688)
[2025-01-06 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:45][root][INFO] - Training Epoch: 4/10, step 381/574 completed (loss: 0.4913289248943329, acc: 0.8449198007583618)
[2025-01-06 01:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46][root][INFO] - Training Epoch: 4/10, step 382/574 completed (loss: 0.1046077162027359, acc: 0.954954981803894)
[2025-01-06 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46][root][INFO] - Training Epoch: 4/10, step 383/574 completed (loss: 0.014304899610579014, acc: 1.0)
[2025-01-06 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47][root][INFO] - Training Epoch: 4/10, step 384/574 completed (loss: 0.003953320439904928, acc: 1.0)
[2025-01-06 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47][root][INFO] - Training Epoch: 4/10, step 385/574 completed (loss: 0.010528836399316788, acc: 1.0)
[2025-01-06 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47][root][INFO] - Training Epoch: 4/10, step 386/574 completed (loss: 0.0016149583971127868, acc: 1.0)
[2025-01-06 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48][root][INFO] - Training Epoch: 4/10, step 387/574 completed (loss: 0.04081099107861519, acc: 0.9736841917037964)
[2025-01-06 01:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48][root][INFO] - Training Epoch: 4/10, step 388/574 completed (loss: 0.1268429458141327, acc: 0.9545454382896423)
[2025-01-06 01:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48][root][INFO] - Training Epoch: 4/10, step 389/574 completed (loss: 0.002009653951972723, acc: 1.0)
[2025-01-06 01:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49][root][INFO] - Training Epoch: 4/10, step 390/574 completed (loss: 0.3262891173362732, acc: 0.9047619104385376)
[2025-01-06 01:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49][root][INFO] - Training Epoch: 4/10, step 391/574 completed (loss: 0.4982603192329407, acc: 0.8148148059844971)
[2025-01-06 01:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49][root][INFO] - Training Epoch: 4/10, step 392/574 completed (loss: 0.6785101294517517, acc: 0.8155339956283569)
[2025-01-06 01:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:50][root][INFO] - Training Epoch: 4/10, step 393/574 completed (loss: 0.7651153802871704, acc: 0.8161764740943909)
[2025-01-06 01:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:50][root][INFO] - Training Epoch: 4/10, step 394/574 completed (loss: 0.5320213437080383, acc: 0.8199999928474426)
[2025-01-06 01:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51][root][INFO] - Training Epoch: 4/10, step 395/574 completed (loss: 0.5320644378662109, acc: 0.8402777910232544)
[2025-01-06 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51][root][INFO] - Training Epoch: 4/10, step 396/574 completed (loss: 0.08636265993118286, acc: 1.0)
[2025-01-06 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51][root][INFO] - Training Epoch: 4/10, step 397/574 completed (loss: 0.049033407121896744, acc: 0.9583333134651184)
[2025-01-06 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52][root][INFO] - Training Epoch: 4/10, step 398/574 completed (loss: 0.16126365959644318, acc: 0.9534883499145508)
[2025-01-06 01:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52][root][INFO] - Training Epoch: 4/10, step 399/574 completed (loss: 0.032127827405929565, acc: 1.0)
[2025-01-06 01:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53][root][INFO] - Training Epoch: 4/10, step 400/574 completed (loss: 0.22477920353412628, acc: 0.9264705777168274)
[2025-01-06 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53][root][INFO] - Training Epoch: 4/10, step 401/574 completed (loss: 0.2285977452993393, acc: 0.9200000166893005)
[2025-01-06 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53][root][INFO] - Training Epoch: 4/10, step 402/574 completed (loss: 0.2076028436422348, acc: 0.9090909361839294)
[2025-01-06 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54][root][INFO] - Training Epoch: 4/10, step 403/574 completed (loss: 0.021198315545916557, acc: 1.0)
[2025-01-06 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54][root][INFO] - Training Epoch: 4/10, step 404/574 completed (loss: 0.06027401238679886, acc: 1.0)
[2025-01-06 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54][root][INFO] - Training Epoch: 4/10, step 405/574 completed (loss: 0.05890626087784767, acc: 0.9629629850387573)
[2025-01-06 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55][root][INFO] - Training Epoch: 4/10, step 406/574 completed (loss: 0.09812594205141068, acc: 0.9599999785423279)
[2025-01-06 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55][root][INFO] - Training Epoch: 4/10, step 407/574 completed (loss: 0.01433519646525383, acc: 1.0)
[2025-01-06 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55][root][INFO] - Training Epoch: 4/10, step 408/574 completed (loss: 0.034168973565101624, acc: 1.0)
[2025-01-06 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56][root][INFO] - Training Epoch: 4/10, step 409/574 completed (loss: 0.054039184004068375, acc: 0.9615384340286255)
[2025-01-06 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56][root][INFO] - Training Epoch: 4/10, step 410/574 completed (loss: 0.058832354843616486, acc: 0.982758641242981)
[2025-01-06 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56][root][INFO] - Training Epoch: 4/10, step 411/574 completed (loss: 0.008215324021875858, acc: 1.0)
[2025-01-06 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56][root][INFO] - Training Epoch: 4/10, step 412/574 completed (loss: 0.06851939857006073, acc: 1.0)
[2025-01-06 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57][root][INFO] - Training Epoch: 4/10, step 413/574 completed (loss: 0.024616921320557594, acc: 1.0)
[2025-01-06 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57][root][INFO] - Training Epoch: 4/10, step 414/574 completed (loss: 0.22863611578941345, acc: 0.9545454382896423)
[2025-01-06 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57][root][INFO] - Training Epoch: 4/10, step 415/574 completed (loss: 0.29507648944854736, acc: 0.8823529481887817)
[2025-01-06 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58][root][INFO] - Training Epoch: 4/10, step 416/574 completed (loss: 0.03826937451958656, acc: 1.0)
[2025-01-06 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58][root][INFO] - Training Epoch: 4/10, step 417/574 completed (loss: 0.0387914702296257, acc: 1.0)
[2025-01-06 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58][root][INFO] - Training Epoch: 4/10, step 418/574 completed (loss: 0.06389330327510834, acc: 0.9750000238418579)
[2025-01-06 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59][root][INFO] - Training Epoch: 4/10, step 419/574 completed (loss: 0.039621688425540924, acc: 1.0)
[2025-01-06 01:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59][root][INFO] - Training Epoch: 4/10, step 420/574 completed (loss: 0.021754834800958633, acc: 1.0)
[2025-01-06 01:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59][root][INFO] - Training Epoch: 4/10, step 421/574 completed (loss: 0.03892768546938896, acc: 1.0)
[2025-01-06 01:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:00][root][INFO] - Training Epoch: 4/10, step 422/574 completed (loss: 0.11125194281339645, acc: 0.96875)
[2025-01-06 01:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2197, device='cuda:0') eval_epoch_loss=tensor(0.7974, device='cuda:0') eval_epoch_acc=tensor(0.8197, device='cuda:0')
[2025-01-06 01:22:28][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:22:28][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:22:28][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_423_loss_0.7973592281341553/model.pt
[2025-01-06 01:22:28][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28][root][INFO] - Training Epoch: 4/10, step 423/574 completed (loss: 0.06323947012424469, acc: 1.0)
[2025-01-06 01:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29][root][INFO] - Training Epoch: 4/10, step 424/574 completed (loss: 0.12017863243818283, acc: 0.9629629850387573)
[2025-01-06 01:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29][root][INFO] - Training Epoch: 4/10, step 425/574 completed (loss: 0.35735613107681274, acc: 0.9696969985961914)
[2025-01-06 01:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29][root][INFO] - Training Epoch: 4/10, step 426/574 completed (loss: 0.0023238311987370253, acc: 1.0)
[2025-01-06 01:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30][root][INFO] - Training Epoch: 4/10, step 427/574 completed (loss: 0.07662884891033173, acc: 0.9729729890823364)
[2025-01-06 01:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30][root][INFO] - Training Epoch: 4/10, step 428/574 completed (loss: 0.024895118549466133, acc: 1.0)
[2025-01-06 01:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30][root][INFO] - Training Epoch: 4/10, step 429/574 completed (loss: 0.012890007346868515, acc: 1.0)
[2025-01-06 01:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31][root][INFO] - Training Epoch: 4/10, step 430/574 completed (loss: 0.0008905597496777773, acc: 1.0)
[2025-01-06 01:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31][root][INFO] - Training Epoch: 4/10, step 431/574 completed (loss: 0.004207609221339226, acc: 1.0)
[2025-01-06 01:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31][root][INFO] - Training Epoch: 4/10, step 432/574 completed (loss: 0.21018363535404205, acc: 0.9130434989929199)
[2025-01-06 01:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32][root][INFO] - Training Epoch: 4/10, step 433/574 completed (loss: 0.06134774163365364, acc: 0.9722222089767456)
[2025-01-06 01:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32][root][INFO] - Training Epoch: 4/10, step 434/574 completed (loss: 0.006480048876255751, acc: 1.0)
[2025-01-06 01:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32][root][INFO] - Training Epoch: 4/10, step 435/574 completed (loss: 0.015198580920696259, acc: 1.0)
[2025-01-06 01:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33][root][INFO] - Training Epoch: 4/10, step 436/574 completed (loss: 0.04134620353579521, acc: 1.0)
[2025-01-06 01:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33][root][INFO] - Training Epoch: 4/10, step 437/574 completed (loss: 0.10889771580696106, acc: 0.9772727489471436)
[2025-01-06 01:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33][root][INFO] - Training Epoch: 4/10, step 438/574 completed (loss: 0.000863320310600102, acc: 1.0)
[2025-01-06 01:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33][root][INFO] - Training Epoch: 4/10, step 439/574 completed (loss: 0.03861968219280243, acc: 1.0)
[2025-01-06 01:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:34][root][INFO] - Training Epoch: 4/10, step 440/574 completed (loss: 0.2170870304107666, acc: 0.9242424368858337)
[2025-01-06 01:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35][root][INFO] - Training Epoch: 4/10, step 441/574 completed (loss: 0.6236439347267151, acc: 0.8320000171661377)
[2025-01-06 01:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35][root][INFO] - Training Epoch: 4/10, step 442/574 completed (loss: 0.6185793876647949, acc: 0.8145161271095276)
[2025-01-06 01:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36][root][INFO] - Training Epoch: 4/10, step 443/574 completed (loss: 0.36474883556365967, acc: 0.9004974961280823)
[2025-01-06 01:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36][root][INFO] - Training Epoch: 4/10, step 444/574 completed (loss: 0.11292330920696259, acc: 0.9433962106704712)
[2025-01-06 01:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36][root][INFO] - Training Epoch: 4/10, step 445/574 completed (loss: 0.05550414323806763, acc: 1.0)
[2025-01-06 01:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37][root][INFO] - Training Epoch: 4/10, step 446/574 completed (loss: 0.20715077221393585, acc: 0.8695651888847351)
[2025-01-06 01:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37][root][INFO] - Training Epoch: 4/10, step 447/574 completed (loss: 0.017618926241993904, acc: 1.0)
[2025-01-06 01:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37][root][INFO] - Training Epoch: 4/10, step 448/574 completed (loss: 0.024911612272262573, acc: 1.0)
[2025-01-06 01:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38][root][INFO] - Training Epoch: 4/10, step 449/574 completed (loss: 0.07261493057012558, acc: 0.9850746393203735)
[2025-01-06 01:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38][root][INFO] - Training Epoch: 4/10, step 450/574 completed (loss: 0.05327631160616875, acc: 0.9722222089767456)
[2025-01-06 01:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38][root][INFO] - Training Epoch: 4/10, step 451/574 completed (loss: 0.018603742122650146, acc: 1.0)
[2025-01-06 01:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39][root][INFO] - Training Epoch: 4/10, step 452/574 completed (loss: 0.11896369606256485, acc: 0.9743589758872986)
[2025-01-06 01:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39][root][INFO] - Training Epoch: 4/10, step 453/574 completed (loss: 0.19282881915569305, acc: 0.9210526347160339)
[2025-01-06 01:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39][root][INFO] - Training Epoch: 4/10, step 454/574 completed (loss: 0.03639229014515877, acc: 0.9795918464660645)
[2025-01-06 01:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40][root][INFO] - Training Epoch: 4/10, step 455/574 completed (loss: 0.09566587209701538, acc: 0.939393937587738)
[2025-01-06 01:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40][root][INFO] - Training Epoch: 4/10, step 456/574 completed (loss: 0.37826940417289734, acc: 0.907216489315033)
[2025-01-06 01:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40][root][INFO] - Training Epoch: 4/10, step 457/574 completed (loss: 0.03848157823085785, acc: 0.9857142567634583)
[2025-01-06 01:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41][root][INFO] - Training Epoch: 4/10, step 458/574 completed (loss: 0.32179611921310425, acc: 0.9069767594337463)
[2025-01-06 01:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41][root][INFO] - Training Epoch: 4/10, step 459/574 completed (loss: 0.0842776969075203, acc: 0.9821428656578064)
[2025-01-06 01:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41][root][INFO] - Training Epoch: 4/10, step 460/574 completed (loss: 0.0931936651468277, acc: 0.9629629850387573)
[2025-01-06 01:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42][root][INFO] - Training Epoch: 4/10, step 461/574 completed (loss: 0.0910695344209671, acc: 0.9444444179534912)
[2025-01-06 01:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42][root][INFO] - Training Epoch: 4/10, step 462/574 completed (loss: 0.20738749206066132, acc: 0.9375)
[2025-01-06 01:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42][root][INFO] - Training Epoch: 4/10, step 463/574 completed (loss: 0.10574112832546234, acc: 0.9615384340286255)
[2025-01-06 01:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43][root][INFO] - Training Epoch: 4/10, step 464/574 completed (loss: 0.2892673909664154, acc: 0.9347826242446899)
[2025-01-06 01:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43][root][INFO] - Training Epoch: 4/10, step 465/574 completed (loss: 0.1950216293334961, acc: 0.9404761791229248)
[2025-01-06 01:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43][root][INFO] - Training Epoch: 4/10, step 466/574 completed (loss: 0.4034315049648285, acc: 0.8795180916786194)
[2025-01-06 01:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44][root][INFO] - Training Epoch: 4/10, step 467/574 completed (loss: 0.11726292967796326, acc: 0.9729729890823364)
[2025-01-06 01:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44][root][INFO] - Training Epoch: 4/10, step 468/574 completed (loss: 0.39935368299484253, acc: 0.8737863898277283)
[2025-01-06 01:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44][root][INFO] - Training Epoch: 4/10, step 469/574 completed (loss: 0.29284870624542236, acc: 0.9512194991111755)
[2025-01-06 01:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45][root][INFO] - Training Epoch: 4/10, step 470/574 completed (loss: 0.03823625296354294, acc: 1.0)
[2025-01-06 01:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45][root][INFO] - Training Epoch: 4/10, step 471/574 completed (loss: 0.1314634084701538, acc: 0.9642857313156128)
[2025-01-06 01:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45][root][INFO] - Training Epoch: 4/10, step 472/574 completed (loss: 0.20980766415596008, acc: 0.9313725233078003)
[2025-01-06 01:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46][root][INFO] - Training Epoch: 4/10, step 473/574 completed (loss: 0.5958543419837952, acc: 0.8253275156021118)
[2025-01-06 01:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46][root][INFO] - Training Epoch: 4/10, step 474/574 completed (loss: 0.22833800315856934, acc: 0.8958333134651184)
[2025-01-06 01:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46][root][INFO] - Training Epoch: 4/10, step 475/574 completed (loss: 0.36633461713790894, acc: 0.89570552110672)
[2025-01-06 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47][root][INFO] - Training Epoch: 4/10, step 476/574 completed (loss: 0.40893521904945374, acc: 0.8776978254318237)
[2025-01-06 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47][root][INFO] - Training Epoch: 4/10, step 477/574 completed (loss: 0.6551988124847412, acc: 0.7839195728302002)
[2025-01-06 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47][root][INFO] - Training Epoch: 4/10, step 478/574 completed (loss: 0.2099432349205017, acc: 0.8888888955116272)
[2025-01-06 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48][root][INFO] - Training Epoch: 4/10, step 479/574 completed (loss: 0.13191451132297516, acc: 0.939393937587738)
[2025-01-06 01:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48][root][INFO] - Training Epoch: 4/10, step 480/574 completed (loss: 0.1959751695394516, acc: 0.9259259104728699)
[2025-01-06 01:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48][root][INFO] - Training Epoch: 4/10, step 481/574 completed (loss: 0.032229986041784286, acc: 1.0)
[2025-01-06 01:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49][root][INFO] - Training Epoch: 4/10, step 482/574 completed (loss: 0.38581138849258423, acc: 0.8500000238418579)
[2025-01-06 01:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49][root][INFO] - Training Epoch: 4/10, step 483/574 completed (loss: 0.4260374903678894, acc: 0.8620689511299133)
[2025-01-06 01:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49][root][INFO] - Training Epoch: 4/10, step 484/574 completed (loss: 0.07956818491220474, acc: 0.9677419066429138)
[2025-01-06 01:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50][root][INFO] - Training Epoch: 4/10, step 485/574 completed (loss: 0.012035146355628967, acc: 1.0)
[2025-01-06 01:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50][root][INFO] - Training Epoch: 4/10, step 486/574 completed (loss: 0.10299853980541229, acc: 1.0)
[2025-01-06 01:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50][root][INFO] - Training Epoch: 4/10, step 487/574 completed (loss: 0.30103492736816406, acc: 0.8095238208770752)
[2025-01-06 01:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51][root][INFO] - Training Epoch: 4/10, step 488/574 completed (loss: 0.07112368941307068, acc: 0.9545454382896423)
[2025-01-06 01:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51][root][INFO] - Training Epoch: 4/10, step 489/574 completed (loss: 0.6194183230400085, acc: 0.800000011920929)
[2025-01-06 01:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51][root][INFO] - Training Epoch: 4/10, step 490/574 completed (loss: 0.05165845900774002, acc: 1.0)
[2025-01-06 01:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51][root][INFO] - Training Epoch: 4/10, step 491/574 completed (loss: 0.07308417558670044, acc: 0.9655172228813171)
[2025-01-06 01:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52][root][INFO] - Training Epoch: 4/10, step 492/574 completed (loss: 0.15894654393196106, acc: 0.9019607901573181)
[2025-01-06 01:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52][root][INFO] - Training Epoch: 4/10, step 493/574 completed (loss: 0.1777518391609192, acc: 0.931034505367279)
[2025-01-06 01:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52][root][INFO] - Training Epoch: 4/10, step 494/574 completed (loss: 0.3206091523170471, acc: 0.9473684430122375)
[2025-01-06 01:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:53][root][INFO] - Training Epoch: 4/10, step 495/574 completed (loss: 0.24434785544872284, acc: 0.9473684430122375)
[2025-01-06 01:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:53][root][INFO] - Training Epoch: 4/10, step 496/574 completed (loss: 0.41561177372932434, acc: 0.9107142686843872)
[2025-01-06 01:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:53][root][INFO] - Training Epoch: 4/10, step 497/574 completed (loss: 0.2872997522354126, acc: 0.898876428604126)
[2025-01-06 01:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:54][root][INFO] - Training Epoch: 4/10, step 498/574 completed (loss: 0.4714609980583191, acc: 0.8764045238494873)
[2025-01-06 01:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:54][root][INFO] - Training Epoch: 4/10, step 499/574 completed (loss: 0.8859103918075562, acc: 0.7234042286872864)
[2025-01-06 01:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:54][root][INFO] - Training Epoch: 4/10, step 500/574 completed (loss: 0.46045029163360596, acc: 0.8804348111152649)
[2025-01-06 01:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55][root][INFO] - Training Epoch: 4/10, step 501/574 completed (loss: 0.007947621867060661, acc: 1.0)
[2025-01-06 01:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55][root][INFO] - Training Epoch: 4/10, step 502/574 completed (loss: 0.003387964330613613, acc: 1.0)
[2025-01-06 01:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55][root][INFO] - Training Epoch: 4/10, step 503/574 completed (loss: 0.1803082823753357, acc: 0.8888888955116272)
[2025-01-06 01:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55][root][INFO] - Training Epoch: 4/10, step 504/574 completed (loss: 0.20364944636821747, acc: 0.9629629850387573)
[2025-01-06 01:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56][root][INFO] - Training Epoch: 4/10, step 505/574 completed (loss: 0.33179405331611633, acc: 0.9245283007621765)
[2025-01-06 01:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56][root][INFO] - Training Epoch: 4/10, step 506/574 completed (loss: 0.6674069762229919, acc: 0.8620689511299133)
[2025-01-06 01:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57][root][INFO] - Training Epoch: 4/10, step 507/574 completed (loss: 0.7982131242752075, acc: 0.7837837934494019)
[2025-01-06 01:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57][root][INFO] - Training Epoch: 4/10, step 508/574 completed (loss: 0.5264246463775635, acc: 0.8450704216957092)
[2025-01-06 01:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57][root][INFO] - Training Epoch: 4/10, step 509/574 completed (loss: 0.09685276448726654, acc: 0.949999988079071)
[2025-01-06 01:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:58][root][INFO] - Training Epoch: 4/10, step 510/574 completed (loss: 0.10434329509735107, acc: 0.9666666388511658)
[2025-01-06 01:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:58][root][INFO] - Training Epoch: 4/10, step 511/574 completed (loss: 0.23686066269874573, acc: 0.9615384340286255)
[2025-01-06 01:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:00][root][INFO] - Training Epoch: 4/10, step 512/574 completed (loss: 0.8645724654197693, acc: 0.7357142567634583)
[2025-01-06 01:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:01][root][INFO] - Training Epoch: 4/10, step 513/574 completed (loss: 0.19247761368751526, acc: 0.9285714030265808)
[2025-01-06 01:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02][root][INFO] - Training Epoch: 4/10, step 514/574 completed (loss: 0.3862728178501129, acc: 0.8571428656578064)
[2025-01-06 01:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02][root][INFO] - Training Epoch: 4/10, step 515/574 completed (loss: 0.04077287018299103, acc: 1.0)
[2025-01-06 01:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02][root][INFO] - Training Epoch: 4/10, step 516/574 completed (loss: 0.3522711396217346, acc: 0.8888888955116272)
[2025-01-06 01:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03][root][INFO] - Training Epoch: 4/10, step 517/574 completed (loss: 0.003098860615864396, acc: 1.0)
[2025-01-06 01:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03][root][INFO] - Training Epoch: 4/10, step 518/574 completed (loss: 0.09185205399990082, acc: 0.9677419066429138)
[2025-01-06 01:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04][root][INFO] - Training Epoch: 4/10, step 519/574 completed (loss: 0.0803263857960701, acc: 1.0)
[2025-01-06 01:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04][root][INFO] - Training Epoch: 4/10, step 520/574 completed (loss: 0.24206963181495667, acc: 0.9259259104728699)
[2025-01-06 01:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05][root][INFO] - Training Epoch: 4/10, step 521/574 completed (loss: 0.535933792591095, acc: 0.8644067645072937)
[2025-01-06 01:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05][root][INFO] - Training Epoch: 4/10, step 522/574 completed (loss: 0.24363374710083008, acc: 0.9402984976768494)
[2025-01-06 01:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05][root][INFO] - Training Epoch: 4/10, step 523/574 completed (loss: 0.2519153654575348, acc: 0.9124087691307068)
[2025-01-06 01:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:06][root][INFO] - Training Epoch: 4/10, step 524/574 completed (loss: 0.5593525171279907, acc: 0.8550000190734863)
[2025-01-06 01:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:06][root][INFO] - Training Epoch: 4/10, step 525/574 completed (loss: 0.05102510377764702, acc: 0.9814814925193787)
[2025-01-06 01:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07][root][INFO] - Training Epoch: 4/10, step 526/574 completed (loss: 0.12516695261001587, acc: 0.9615384340286255)
[2025-01-06 01:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07][root][INFO] - Training Epoch: 4/10, step 527/574 completed (loss: 0.17288754880428314, acc: 0.9047619104385376)
[2025-01-06 01:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07][root][INFO] - Training Epoch: 4/10, step 528/574 completed (loss: 1.085253357887268, acc: 0.7213114500045776)
[2025-01-06 01:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08][root][INFO] - Training Epoch: 4/10, step 529/574 completed (loss: 0.22630099952220917, acc: 0.9491525292396545)
[2025-01-06 01:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08][root][INFO] - Training Epoch: 4/10, step 530/574 completed (loss: 0.6846886873245239, acc: 0.7441860437393188)
[2025-01-06 01:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08][root][INFO] - Training Epoch: 4/10, step 531/574 completed (loss: 0.2614665925502777, acc: 0.9090909361839294)
[2025-01-06 01:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09][root][INFO] - Training Epoch: 4/10, step 532/574 completed (loss: 0.2438683956861496, acc: 0.9433962106704712)
[2025-01-06 01:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09][root][INFO] - Training Epoch: 4/10, step 533/574 completed (loss: 0.3821519613265991, acc: 0.9090909361839294)
[2025-01-06 01:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09][root][INFO] - Training Epoch: 4/10, step 534/574 completed (loss: 0.12868434190750122, acc: 0.9599999785423279)
[2025-01-06 01:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09][root][INFO] - Training Epoch: 4/10, step 535/574 completed (loss: 0.1761985719203949, acc: 0.949999988079071)
[2025-01-06 01:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10][root][INFO] - Training Epoch: 4/10, step 536/574 completed (loss: 0.030517591163516045, acc: 1.0)
[2025-01-06 01:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10][root][INFO] - Training Epoch: 4/10, step 537/574 completed (loss: 0.24300187826156616, acc: 0.9538461565971375)
[2025-01-06 01:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11][root][INFO] - Training Epoch: 4/10, step 538/574 completed (loss: 0.20113636553287506, acc: 0.90625)
[2025-01-06 01:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11][root][INFO] - Training Epoch: 4/10, step 539/574 completed (loss: 0.3999653160572052, acc: 0.875)
[2025-01-06 01:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11][root][INFO] - Training Epoch: 4/10, step 540/574 completed (loss: 0.2411472350358963, acc: 0.939393937587738)
[2025-01-06 01:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12][root][INFO] - Training Epoch: 4/10, step 541/574 completed (loss: 0.08707943558692932, acc: 1.0)
[2025-01-06 01:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12][root][INFO] - Training Epoch: 4/10, step 542/574 completed (loss: 0.02462976612150669, acc: 1.0)
[2025-01-06 01:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12][root][INFO] - Training Epoch: 4/10, step 543/574 completed (loss: 0.05360109731554985, acc: 0.95652174949646)
[2025-01-06 01:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13][root][INFO] - Training Epoch: 4/10, step 544/574 completed (loss: 0.08394782990217209, acc: 0.9666666388511658)
[2025-01-06 01:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13][root][INFO] - Training Epoch: 4/10, step 545/574 completed (loss: 0.023089570924639702, acc: 1.0)
[2025-01-06 01:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13][root][INFO] - Training Epoch: 4/10, step 546/574 completed (loss: 0.0022501181811094284, acc: 1.0)
[2025-01-06 01:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14][root][INFO] - Training Epoch: 4/10, step 547/574 completed (loss: 0.0033594374544918537, acc: 1.0)
[2025-01-06 01:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14][root][INFO] - Training Epoch: 4/10, step 548/574 completed (loss: 0.06876944750547409, acc: 0.9677419066429138)
[2025-01-06 01:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14][root][INFO] - Training Epoch: 4/10, step 549/574 completed (loss: 0.002886863425374031, acc: 1.0)
[2025-01-06 01:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15][root][INFO] - Training Epoch: 4/10, step 550/574 completed (loss: 0.14141038060188293, acc: 0.9696969985961914)
[2025-01-06 01:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15][root][INFO] - Training Epoch: 4/10, step 551/574 completed (loss: 0.10419692099094391, acc: 0.9750000238418579)
[2025-01-06 01:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15][root][INFO] - Training Epoch: 4/10, step 552/574 completed (loss: 0.0927051231265068, acc: 0.9857142567634583)
[2025-01-06 01:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15][root][INFO] - Training Epoch: 4/10, step 553/574 completed (loss: 0.3398391604423523, acc: 0.9051094651222229)
[2025-01-06 01:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:16][root][INFO] - Training Epoch: 4/10, step 554/574 completed (loss: 0.15180739760398865, acc: 0.9379310607910156)
[2025-01-06 01:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:16][root][INFO] - Training Epoch: 4/10, step 555/574 completed (loss: 0.16645757853984833, acc: 0.949999988079071)
[2025-01-06 01:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:16][root][INFO] - Training Epoch: 4/10, step 556/574 completed (loss: 0.28905603289604187, acc: 0.9205297827720642)
[2025-01-06 01:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17][root][INFO] - Training Epoch: 4/10, step 557/574 completed (loss: 0.09116626530885696, acc: 0.9658119678497314)
[2025-01-06 01:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17][root][INFO] - Training Epoch: 4/10, step 558/574 completed (loss: 0.01964888535439968, acc: 1.0)
[2025-01-06 01:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17][root][INFO] - Training Epoch: 4/10, step 559/574 completed (loss: 0.4145185053348541, acc: 0.9230769276618958)
[2025-01-06 01:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18][root][INFO] - Training Epoch: 4/10, step 560/574 completed (loss: 0.11728302389383316, acc: 0.9615384340286255)
[2025-01-06 01:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18][root][INFO] - Training Epoch: 4/10, step 561/574 completed (loss: 0.03066842444241047, acc: 1.0)
[2025-01-06 01:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18][root][INFO] - Training Epoch: 4/10, step 562/574 completed (loss: 0.28765514492988586, acc: 0.9444444179534912)
[2025-01-06 01:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19][root][INFO] - Training Epoch: 4/10, step 563/574 completed (loss: 0.3114374577999115, acc: 0.9220778942108154)
[2025-01-06 01:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19][root][INFO] - Training Epoch: 4/10, step 564/574 completed (loss: 0.12258636951446533, acc: 0.9583333134651184)
[2025-01-06 01:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19][root][INFO] - Training Epoch: 4/10, step 565/574 completed (loss: 0.083772674202919, acc: 0.9482758641242981)
[2025-01-06 01:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2180, device='cuda:0') eval_epoch_loss=tensor(0.7966, device='cuda:0') eval_epoch_acc=tensor(0.8216, device='cuda:0')
[2025-01-06 01:23:47][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:23:47][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:23:47][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_566_loss_0.7965880632400513/model.pt
[2025-01-06 01:23:47][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47][root][INFO] - Training Epoch: 4/10, step 566/574 completed (loss: 0.12054792046546936, acc: 0.988095223903656)
[2025-01-06 01:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48][root][INFO] - Training Epoch: 4/10, step 567/574 completed (loss: 0.010292035527527332, acc: 1.0)
[2025-01-06 01:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48][root][INFO] - Training Epoch: 4/10, step 568/574 completed (loss: 0.024124758318066597, acc: 1.0)
[2025-01-06 01:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48][root][INFO] - Training Epoch: 4/10, step 569/574 completed (loss: 0.13754720985889435, acc: 0.9679144620895386)
[2025-01-06 01:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:49][root][INFO] - Training Epoch: 4/10, step 570/574 completed (loss: 0.009369137696921825, acc: 1.0)
[2025-01-06 01:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:49][root][INFO] - Training Epoch: 4/10, step 571/574 completed (loss: 0.13227543234825134, acc: 0.9743589758872986)
[2025-01-06 01:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:49][root][INFO] - Training Epoch: 4/10, step 572/574 completed (loss: 0.28141146898269653, acc: 0.918367326259613)
[2025-01-06 01:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:50][root][INFO] - Training Epoch: 4/10, step 573/574 completed (loss: 0.18590644001960754, acc: 0.9182389974594116)
[2025-01-06 01:23:50][slam_llm.utils.train_utils][INFO] - Epoch 4: train_perplexity=1.3426, train_epoch_loss=0.2946, epoch time 347.26975252851844s
[2025-01-06 01:23:50][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:23:50][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 14 GB
[2025-01-06 01:23:50][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:23:50][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2025-01-06 01:23:50][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51][root][INFO] - Training Epoch: 5/10, step 0/574 completed (loss: 0.042511891573667526, acc: 0.9629629850387573)
[2025-01-06 01:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51][root][INFO] - Training Epoch: 5/10, step 1/574 completed (loss: 0.14490270614624023, acc: 0.9599999785423279)
[2025-01-06 01:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52][root][INFO] - Training Epoch: 5/10, step 2/574 completed (loss: 0.5421382188796997, acc: 0.8918918967247009)
[2025-01-06 01:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52][root][INFO] - Training Epoch: 5/10, step 3/574 completed (loss: 0.21070344746112823, acc: 0.9210526347160339)
[2025-01-06 01:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52][root][INFO] - Training Epoch: 5/10, step 4/574 completed (loss: 0.29521986842155457, acc: 0.9459459185600281)
[2025-01-06 01:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52][root][INFO] - Training Epoch: 5/10, step 5/574 completed (loss: 0.03585276007652283, acc: 1.0)
[2025-01-06 01:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53][root][INFO] - Training Epoch: 5/10, step 6/574 completed (loss: 0.3836023807525635, acc: 0.8571428656578064)
[2025-01-06 01:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53][root][INFO] - Training Epoch: 5/10, step 7/574 completed (loss: 0.061826594173908234, acc: 1.0)
[2025-01-06 01:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54][root][INFO] - Training Epoch: 5/10, step 8/574 completed (loss: 0.022074537351727486, acc: 1.0)
[2025-01-06 01:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54][root][INFO] - Training Epoch: 5/10, step 9/574 completed (loss: 0.36869674921035767, acc: 0.9230769276618958)
[2025-01-06 01:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54][root][INFO] - Training Epoch: 5/10, step 10/574 completed (loss: 0.004682532045990229, acc: 1.0)
[2025-01-06 01:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55][root][INFO] - Training Epoch: 5/10, step 11/574 completed (loss: 0.12962153553962708, acc: 0.9487179517745972)
[2025-01-06 01:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55][root][INFO] - Training Epoch: 5/10, step 12/574 completed (loss: 0.026287630200386047, acc: 1.0)
[2025-01-06 01:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55][root][INFO] - Training Epoch: 5/10, step 13/574 completed (loss: 0.10970594733953476, acc: 0.95652174949646)
[2025-01-06 01:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56][root][INFO] - Training Epoch: 5/10, step 14/574 completed (loss: 0.08736486732959747, acc: 0.9607843160629272)
[2025-01-06 01:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56][root][INFO] - Training Epoch: 5/10, step 15/574 completed (loss: 0.11781901121139526, acc: 0.9591836929321289)
[2025-01-06 01:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56][root][INFO] - Training Epoch: 5/10, step 16/574 completed (loss: 0.0051681362092494965, acc: 1.0)
[2025-01-06 01:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57][root][INFO] - Training Epoch: 5/10, step 17/574 completed (loss: 0.07266836613416672, acc: 0.9583333134651184)
[2025-01-06 01:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57][root][INFO] - Training Epoch: 5/10, step 18/574 completed (loss: 0.4189874231815338, acc: 0.9166666865348816)
[2025-01-06 01:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57][root][INFO] - Training Epoch: 5/10, step 19/574 completed (loss: 0.2877032160758972, acc: 0.8947368264198303)
[2025-01-06 01:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58][root][INFO] - Training Epoch: 5/10, step 20/574 completed (loss: 0.29439297318458557, acc: 0.9230769276618958)
[2025-01-06 01:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58][root][INFO] - Training Epoch: 5/10, step 21/574 completed (loss: 0.05571673810482025, acc: 0.9655172228813171)
[2025-01-06 01:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58][root][INFO] - Training Epoch: 5/10, step 22/574 completed (loss: 0.04743504151701927, acc: 1.0)
[2025-01-06 01:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:59][root][INFO] - Training Epoch: 5/10, step 23/574 completed (loss: 0.11622361838817596, acc: 0.9523809552192688)
[2025-01-06 01:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:59][root][INFO] - Training Epoch: 5/10, step 24/574 completed (loss: 0.012843431904911995, acc: 1.0)
[2025-01-06 01:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:59][root][INFO] - Training Epoch: 5/10, step 25/574 completed (loss: 0.543913722038269, acc: 0.849056601524353)
[2025-01-06 01:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:00][root][INFO] - Training Epoch: 5/10, step 26/574 completed (loss: 0.4928233027458191, acc: 0.8493150472640991)
[2025-01-06 01:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01][root][INFO] - Training Epoch: 5/10, step 27/574 completed (loss: 0.6757522225379944, acc: 0.8063241243362427)
[2025-01-06 01:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01][root][INFO] - Training Epoch: 5/10, step 28/574 completed (loss: 0.19546332955360413, acc: 0.8837209343910217)
[2025-01-06 01:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01][root][INFO] - Training Epoch: 5/10, step 29/574 completed (loss: 0.23184983432292938, acc: 0.9277108311653137)
[2025-01-06 01:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02][root][INFO] - Training Epoch: 5/10, step 30/574 completed (loss: 0.3591136932373047, acc: 0.8888888955116272)
[2025-01-06 01:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02][root][INFO] - Training Epoch: 5/10, step 31/574 completed (loss: 0.2698059380054474, acc: 0.9642857313156128)
[2025-01-06 01:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02][root][INFO] - Training Epoch: 5/10, step 32/574 completed (loss: 0.1534896194934845, acc: 0.9259259104728699)
[2025-01-06 01:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03][root][INFO] - Training Epoch: 5/10, step 33/574 completed (loss: 0.0024783979170024395, acc: 1.0)
[2025-01-06 01:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03][root][INFO] - Training Epoch: 5/10, step 34/574 completed (loss: 0.2858404815196991, acc: 0.8823529481887817)
[2025-01-06 01:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04][root][INFO] - Training Epoch: 5/10, step 35/574 completed (loss: 0.24241939187049866, acc: 0.9344262480735779)
[2025-01-06 01:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04][root][INFO] - Training Epoch: 5/10, step 36/574 completed (loss: 0.29493415355682373, acc: 0.920634925365448)
[2025-01-06 01:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04][root][INFO] - Training Epoch: 5/10, step 37/574 completed (loss: 0.16715174913406372, acc: 0.9491525292396545)
[2025-01-06 01:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05][root][INFO] - Training Epoch: 5/10, step 38/574 completed (loss: 0.19993124902248383, acc: 0.9655172228813171)
[2025-01-06 01:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05][root][INFO] - Training Epoch: 5/10, step 39/574 completed (loss: 0.03342073783278465, acc: 1.0)
[2025-01-06 01:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05][root][INFO] - Training Epoch: 5/10, step 40/574 completed (loss: 0.10674410313367844, acc: 0.9615384340286255)
[2025-01-06 01:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06][root][INFO] - Training Epoch: 5/10, step 41/574 completed (loss: 0.1004740297794342, acc: 0.9594594836235046)
[2025-01-06 01:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06][root][INFO] - Training Epoch: 5/10, step 42/574 completed (loss: 0.29416489601135254, acc: 0.9076923131942749)
[2025-01-06 01:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06][root][INFO] - Training Epoch: 5/10, step 43/574 completed (loss: 0.4947601854801178, acc: 0.8585858345031738)
[2025-01-06 01:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07][root][INFO] - Training Epoch: 5/10, step 44/574 completed (loss: 0.20577576756477356, acc: 0.938144326210022)
[2025-01-06 01:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07][root][INFO] - Training Epoch: 5/10, step 45/574 completed (loss: 0.18365295231342316, acc: 0.9264705777168274)
[2025-01-06 01:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07][root][INFO] - Training Epoch: 5/10, step 46/574 completed (loss: 0.1138027086853981, acc: 0.9230769276618958)
[2025-01-06 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08][root][INFO] - Training Epoch: 5/10, step 47/574 completed (loss: 0.18041470646858215, acc: 0.9629629850387573)
[2025-01-06 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08][root][INFO] - Training Epoch: 5/10, step 48/574 completed (loss: 0.08622419834136963, acc: 0.9642857313156128)
[2025-01-06 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08][root][INFO] - Training Epoch: 5/10, step 49/574 completed (loss: 0.0744021087884903, acc: 0.9722222089767456)
[2025-01-06 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09][root][INFO] - Training Epoch: 5/10, step 50/574 completed (loss: 0.459373414516449, acc: 0.8421052694320679)
[2025-01-06 01:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09][root][INFO] - Training Epoch: 5/10, step 51/574 completed (loss: 0.11781123280525208, acc: 0.9841269850730896)
[2025-01-06 01:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09][root][INFO] - Training Epoch: 5/10, step 52/574 completed (loss: 0.31634488701820374, acc: 0.8591549396514893)
[2025-01-06 01:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10][root][INFO] - Training Epoch: 5/10, step 53/574 completed (loss: 1.0545570850372314, acc: 0.6666666865348816)
[2025-01-06 01:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10][root][INFO] - Training Epoch: 5/10, step 54/574 completed (loss: 0.37017127871513367, acc: 0.8648648858070374)
[2025-01-06 01:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10][root][INFO] - Training Epoch: 5/10, step 55/574 completed (loss: 0.1018521636724472, acc: 0.9230769276618958)
[2025-01-06 01:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:14][root][INFO] - Training Epoch: 5/10, step 56/574 completed (loss: 0.6769174933433533, acc: 0.80887371301651)
[2025-01-06 01:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:15][root][INFO] - Training Epoch: 5/10, step 57/574 completed (loss: 1.0139861106872559, acc: 0.7320261597633362)
[2025-01-06 01:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:15][root][INFO] - Training Epoch: 5/10, step 58/574 completed (loss: 0.5706998109817505, acc: 0.8011363744735718)
[2025-01-06 01:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:16][root][INFO] - Training Epoch: 5/10, step 59/574 completed (loss: 0.19377189874649048, acc: 0.9485294222831726)
[2025-01-06 01:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:16][root][INFO] - Training Epoch: 5/10, step 60/574 completed (loss: 0.6361778378486633, acc: 0.8188405632972717)
[2025-01-06 01:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17][root][INFO] - Training Epoch: 5/10, step 61/574 completed (loss: 0.40464216470718384, acc: 0.875)
[2025-01-06 01:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17][root][INFO] - Training Epoch: 5/10, step 62/574 completed (loss: 0.08862245082855225, acc: 0.970588207244873)
[2025-01-06 01:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18][root][INFO] - Training Epoch: 5/10, step 63/574 completed (loss: 0.11122985929250717, acc: 0.9722222089767456)
[2025-01-06 01:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18][root][INFO] - Training Epoch: 5/10, step 64/574 completed (loss: 0.08155645430088043, acc: 0.984375)
[2025-01-06 01:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18][root][INFO] - Training Epoch: 5/10, step 65/574 completed (loss: 0.016835596412420273, acc: 1.0)
[2025-01-06 01:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19][root][INFO] - Training Epoch: 5/10, step 66/574 completed (loss: 0.36944350600242615, acc: 0.9285714030265808)
[2025-01-06 01:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19][root][INFO] - Training Epoch: 5/10, step 67/574 completed (loss: 0.1169133186340332, acc: 0.9666666388511658)
[2025-01-06 01:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19][root][INFO] - Training Epoch: 5/10, step 68/574 completed (loss: 0.006466264836490154, acc: 1.0)
[2025-01-06 01:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19][root][INFO] - Training Epoch: 5/10, step 69/574 completed (loss: 0.24582798779010773, acc: 0.9444444179534912)
[2025-01-06 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20][root][INFO] - Training Epoch: 5/10, step 70/574 completed (loss: 0.37024009227752686, acc: 0.9090909361839294)
[2025-01-06 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20][root][INFO] - Training Epoch: 5/10, step 71/574 completed (loss: 0.6553786993026733, acc: 0.8308823704719543)
[2025-01-06 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20][root][INFO] - Training Epoch: 5/10, step 72/574 completed (loss: 0.4560847580432892, acc: 0.8095238208770752)
[2025-01-06 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21][root][INFO] - Training Epoch: 5/10, step 73/574 completed (loss: 0.977643609046936, acc: 0.7230769395828247)
[2025-01-06 01:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21][root][INFO] - Training Epoch: 5/10, step 74/574 completed (loss: 0.6762596964836121, acc: 0.8265306353569031)
[2025-01-06 01:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21][root][INFO] - Training Epoch: 5/10, step 75/574 completed (loss: 0.7135188579559326, acc: 0.8059701323509216)
[2025-01-06 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22][root][INFO] - Training Epoch: 5/10, step 76/574 completed (loss: 1.2188321352005005, acc: 0.6788321137428284)
[2025-01-06 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22][root][INFO] - Training Epoch: 5/10, step 77/574 completed (loss: 0.003159929532557726, acc: 1.0)
[2025-01-06 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22][root][INFO] - Training Epoch: 5/10, step 78/574 completed (loss: 0.03271247819066048, acc: 1.0)
[2025-01-06 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23][root][INFO] - Training Epoch: 5/10, step 79/574 completed (loss: 0.027476664632558823, acc: 1.0)
[2025-01-06 01:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23][root][INFO] - Training Epoch: 5/10, step 80/574 completed (loss: 0.012755136936903, acc: 1.0)
[2025-01-06 01:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23][root][INFO] - Training Epoch: 5/10, step 81/574 completed (loss: 0.1662454605102539, acc: 0.9230769276618958)
[2025-01-06 01:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24][root][INFO] - Training Epoch: 5/10, step 82/574 completed (loss: 0.20534609258174896, acc: 0.942307710647583)
[2025-01-06 01:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24][root][INFO] - Training Epoch: 5/10, step 83/574 completed (loss: 0.25221604108810425, acc: 0.90625)
[2025-01-06 01:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24][root][INFO] - Training Epoch: 5/10, step 84/574 completed (loss: 0.0980674996972084, acc: 0.9855072498321533)
[2025-01-06 01:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25][root][INFO] - Training Epoch: 5/10, step 85/574 completed (loss: 0.13819622993469238, acc: 0.9599999785423279)
[2025-01-06 01:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25][root][INFO] - Training Epoch: 5/10, step 86/574 completed (loss: 0.020317066460847855, acc: 1.0)
[2025-01-06 01:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25][root][INFO] - Training Epoch: 5/10, step 87/574 completed (loss: 0.23646265268325806, acc: 0.9599999785423279)
[2025-01-06 01:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:26][root][INFO] - Training Epoch: 5/10, step 88/574 completed (loss: 0.5128151178359985, acc: 0.8737863898277283)
[2025-01-06 01:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:27][root][INFO] - Training Epoch: 5/10, step 89/574 completed (loss: 0.7086708545684814, acc: 0.8300970792770386)
[2025-01-06 01:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:28][root][INFO] - Training Epoch: 5/10, step 90/574 completed (loss: 0.8032523393630981, acc: 0.801075279712677)
[2025-01-06 01:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:28][root][INFO] - Training Epoch: 5/10, step 91/574 completed (loss: 0.6365634799003601, acc: 0.8103448152542114)
[2025-01-06 01:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:29][root][INFO] - Training Epoch: 5/10, step 92/574 completed (loss: 0.5502006411552429, acc: 0.8315789699554443)
[2025-01-06 01:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:30][root][INFO] - Training Epoch: 5/10, step 93/574 completed (loss: 0.6667263507843018, acc: 0.801980197429657)
[2025-01-06 01:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31][root][INFO] - Training Epoch: 5/10, step 94/574 completed (loss: 0.478325217962265, acc: 0.8548387289047241)
[2025-01-06 01:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31][root][INFO] - Training Epoch: 5/10, step 95/574 completed (loss: 0.3856801986694336, acc: 0.8695651888847351)
[2025-01-06 01:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31][root][INFO] - Training Epoch: 5/10, step 96/574 completed (loss: 0.6777008175849915, acc: 0.7983193397521973)
[2025-01-06 01:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32][root][INFO] - Training Epoch: 5/10, step 97/574 completed (loss: 0.6009306311607361, acc: 0.817307710647583)
[2025-01-06 01:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32][root][INFO] - Training Epoch: 5/10, step 98/574 completed (loss: 0.8329910039901733, acc: 0.6934306621551514)
[2025-01-06 01:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32][root][INFO] - Training Epoch: 5/10, step 99/574 completed (loss: 0.4786328077316284, acc: 0.8805969953536987)
[2025-01-06 01:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33][root][INFO] - Training Epoch: 5/10, step 100/574 completed (loss: 0.054519880563020706, acc: 1.0)
[2025-01-06 01:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33][root][INFO] - Training Epoch: 5/10, step 101/574 completed (loss: 0.009121282026171684, acc: 1.0)
[2025-01-06 01:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33][root][INFO] - Training Epoch: 5/10, step 102/574 completed (loss: 0.05760223791003227, acc: 0.95652174949646)
[2025-01-06 01:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34][root][INFO] - Training Epoch: 5/10, step 103/574 completed (loss: 0.0197436660528183, acc: 1.0)
[2025-01-06 01:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34][root][INFO] - Training Epoch: 5/10, step 104/574 completed (loss: 0.10719139128923416, acc: 0.9655172228813171)
[2025-01-06 01:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34][root][INFO] - Training Epoch: 5/10, step 105/574 completed (loss: 0.016512559726834297, acc: 1.0)
[2025-01-06 01:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35][root][INFO] - Training Epoch: 5/10, step 106/574 completed (loss: 0.16944372653961182, acc: 0.9200000166893005)
[2025-01-06 01:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35][root][INFO] - Training Epoch: 5/10, step 107/574 completed (loss: 0.004082603845745325, acc: 1.0)
[2025-01-06 01:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35][root][INFO] - Training Epoch: 5/10, step 108/574 completed (loss: 0.031411804258823395, acc: 0.9615384340286255)
[2025-01-06 01:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36][root][INFO] - Training Epoch: 5/10, step 109/574 completed (loss: 0.22110775113105774, acc: 0.9523809552192688)
[2025-01-06 01:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36][root][INFO] - Training Epoch: 5/10, step 110/574 completed (loss: 0.05001487210392952, acc: 0.9846153855323792)
[2025-01-06 01:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37][root][INFO] - Training Epoch: 5/10, step 111/574 completed (loss: 0.2808919847011566, acc: 0.8771929740905762)
[2025-01-06 01:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37][root][INFO] - Training Epoch: 5/10, step 112/574 completed (loss: 0.3467675447463989, acc: 0.8947368264198303)
[2025-01-06 01:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37][root][INFO] - Training Epoch: 5/10, step 113/574 completed (loss: 0.14315323531627655, acc: 0.9743589758872986)
[2025-01-06 01:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38][root][INFO] - Training Epoch: 5/10, step 114/574 completed (loss: 0.14966316521167755, acc: 0.9591836929321289)
[2025-01-06 01:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38][root][INFO] - Training Epoch: 5/10, step 115/574 completed (loss: 0.0024332546163350344, acc: 1.0)
[2025-01-06 01:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38][root][INFO] - Training Epoch: 5/10, step 116/574 completed (loss: 0.23502126336097717, acc: 0.9523809552192688)
[2025-01-06 01:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39][root][INFO] - Training Epoch: 5/10, step 117/574 completed (loss: 0.3754105567932129, acc: 0.8780487775802612)
[2025-01-06 01:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39][root][INFO] - Training Epoch: 5/10, step 118/574 completed (loss: 0.16276666522026062, acc: 0.9354838728904724)
[2025-01-06 01:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:40][root][INFO] - Training Epoch: 5/10, step 119/574 completed (loss: 0.548801064491272, acc: 0.8403041958808899)
[2025-01-06 01:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:40][root][INFO] - Training Epoch: 5/10, step 120/574 completed (loss: 0.04651183262467384, acc: 1.0)
[2025-01-06 01:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41][root][INFO] - Training Epoch: 5/10, step 121/574 completed (loss: 0.2479906678199768, acc: 0.942307710647583)
[2025-01-06 01:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41][root][INFO] - Training Epoch: 5/10, step 122/574 completed (loss: 0.02578023076057434, acc: 1.0)
[2025-01-06 01:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41][root][INFO] - Training Epoch: 5/10, step 123/574 completed (loss: 0.40915894508361816, acc: 0.8947368264198303)
[2025-01-06 01:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:42][root][INFO] - Training Epoch: 5/10, step 124/574 completed (loss: 0.6898718476295471, acc: 0.8159509301185608)
[2025-01-06 01:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:42][root][INFO] - Training Epoch: 5/10, step 125/574 completed (loss: 0.5420748591423035, acc: 0.8541666865348816)
[2025-01-06 01:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43][root][INFO] - Training Epoch: 5/10, step 126/574 completed (loss: 0.5409641265869141, acc: 0.8333333134651184)
[2025-01-06 01:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43][root][INFO] - Training Epoch: 5/10, step 127/574 completed (loss: 0.3827786147594452, acc: 0.875)
[2025-01-06 01:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43][root][INFO] - Training Epoch: 5/10, step 128/574 completed (loss: 0.4923830032348633, acc: 0.8461538553237915)
[2025-01-06 01:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:44][root][INFO] - Training Epoch: 5/10, step 129/574 completed (loss: 0.5150810480117798, acc: 0.8088235259056091)
[2025-01-06 01:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:44][root][INFO] - Training Epoch: 5/10, step 130/574 completed (loss: 0.0825677216053009, acc: 0.9615384340286255)
[2025-01-06 01:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45][root][INFO] - Training Epoch: 5/10, step 131/574 completed (loss: 0.027748649939894676, acc: 1.0)
[2025-01-06 01:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45][root][INFO] - Training Epoch: 5/10, step 132/574 completed (loss: 0.06266089528799057, acc: 1.0)
[2025-01-06 01:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45][root][INFO] - Training Epoch: 5/10, step 133/574 completed (loss: 0.10445203632116318, acc: 1.0)
[2025-01-06 01:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:46][root][INFO] - Training Epoch: 5/10, step 134/574 completed (loss: 0.27098047733306885, acc: 0.8857142925262451)
[2025-01-06 01:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0965, device='cuda:0') eval_epoch_loss=tensor(0.7403, device='cuda:0') eval_epoch_acc=tensor(0.8324, device='cuda:0')
[2025-01-06 01:25:14][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:25:14][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:25:15][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_135_loss_0.7402848601341248/model.pt
[2025-01-06 01:25:15][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15][root][INFO] - Training Epoch: 5/10, step 135/574 completed (loss: 0.09948931634426117, acc: 1.0)
[2025-01-06 01:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15][root][INFO] - Training Epoch: 5/10, step 136/574 completed (loss: 0.05125853046774864, acc: 1.0)
[2025-01-06 01:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16][root][INFO] - Training Epoch: 5/10, step 137/574 completed (loss: 0.4606732726097107, acc: 0.8333333134651184)
[2025-01-06 01:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16][root][INFO] - Training Epoch: 5/10, step 138/574 completed (loss: 0.25610655546188354, acc: 0.9130434989929199)
[2025-01-06 01:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16][root][INFO] - Training Epoch: 5/10, step 139/574 completed (loss: 0.10828107595443726, acc: 0.9523809552192688)
[2025-01-06 01:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17][root][INFO] - Training Epoch: 5/10, step 140/574 completed (loss: 0.10361329466104507, acc: 0.9615384340286255)
[2025-01-06 01:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17][root][INFO] - Training Epoch: 5/10, step 141/574 completed (loss: 0.0863734781742096, acc: 0.9677419066429138)
[2025-01-06 01:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17][root][INFO] - Training Epoch: 5/10, step 142/574 completed (loss: 0.342519074678421, acc: 0.9459459185600281)
[2025-01-06 01:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18][root][INFO] - Training Epoch: 5/10, step 143/574 completed (loss: 0.4246760904788971, acc: 0.8684210777282715)
[2025-01-06 01:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18][root][INFO] - Training Epoch: 5/10, step 144/574 completed (loss: 0.5570039749145508, acc: 0.8507462739944458)
[2025-01-06 01:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19][root][INFO] - Training Epoch: 5/10, step 145/574 completed (loss: 0.24913369119167328, acc: 0.918367326259613)
[2025-01-06 01:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19][root][INFO] - Training Epoch: 5/10, step 146/574 completed (loss: 0.49812227487564087, acc: 0.8297872543334961)
[2025-01-06 01:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19][root][INFO] - Training Epoch: 5/10, step 147/574 completed (loss: 0.26053836941719055, acc: 0.9285714030265808)
[2025-01-06 01:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20][root][INFO] - Training Epoch: 5/10, step 148/574 completed (loss: 0.06761227548122406, acc: 1.0)
[2025-01-06 01:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20][root][INFO] - Training Epoch: 5/10, step 149/574 completed (loss: 0.17971469461917877, acc: 0.9130434989929199)
[2025-01-06 01:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20][root][INFO] - Training Epoch: 5/10, step 150/574 completed (loss: 0.2619861662387848, acc: 0.8965517282485962)
[2025-01-06 01:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20][root][INFO] - Training Epoch: 5/10, step 151/574 completed (loss: 0.3120729327201843, acc: 0.8695651888847351)
[2025-01-06 01:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21][root][INFO] - Training Epoch: 5/10, step 152/574 completed (loss: 0.3291882574558258, acc: 0.9152542352676392)
[2025-01-06 01:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21][root][INFO] - Training Epoch: 5/10, step 153/574 completed (loss: 0.12507693469524384, acc: 0.9649122953414917)
[2025-01-06 01:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21][root][INFO] - Training Epoch: 5/10, step 154/574 completed (loss: 0.3522273898124695, acc: 0.8918918967247009)
[2025-01-06 01:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22][root][INFO] - Training Epoch: 5/10, step 155/574 completed (loss: 0.05113545060157776, acc: 1.0)
[2025-01-06 01:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22][root][INFO] - Training Epoch: 5/10, step 156/574 completed (loss: 0.056641194969415665, acc: 1.0)
[2025-01-06 01:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22][root][INFO] - Training Epoch: 5/10, step 157/574 completed (loss: 0.4420989751815796, acc: 0.7894737124443054)
[2025-01-06 01:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24][root][INFO] - Training Epoch: 5/10, step 158/574 completed (loss: 0.5278548002243042, acc: 0.8513513803482056)
[2025-01-06 01:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24][root][INFO] - Training Epoch: 5/10, step 159/574 completed (loss: 0.5653854608535767, acc: 0.7777777910232544)
[2025-01-06 01:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25][root][INFO] - Training Epoch: 5/10, step 160/574 completed (loss: 0.5162111520767212, acc: 0.8372092843055725)
[2025-01-06 01:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25][root][INFO] - Training Epoch: 5/10, step 161/574 completed (loss: 0.6692789196968079, acc: 0.8117647171020508)
[2025-01-06 01:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:26][root][INFO] - Training Epoch: 5/10, step 162/574 completed (loss: 0.8138304948806763, acc: 0.8202247023582458)
[2025-01-06 01:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:26][root][INFO] - Training Epoch: 5/10, step 163/574 completed (loss: 0.12693685293197632, acc: 0.9545454382896423)
[2025-01-06 01:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:26][root][INFO] - Training Epoch: 5/10, step 164/574 completed (loss: 0.18714353442192078, acc: 0.9047619104385376)
[2025-01-06 01:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27][root][INFO] - Training Epoch: 5/10, step 165/574 completed (loss: 0.1678994596004486, acc: 0.9655172228813171)
[2025-01-06 01:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27][root][INFO] - Training Epoch: 5/10, step 166/574 completed (loss: 0.08652909845113754, acc: 0.9591836929321289)
[2025-01-06 01:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27][root][INFO] - Training Epoch: 5/10, step 167/574 completed (loss: 0.0699460431933403, acc: 0.9800000190734863)
[2025-01-06 01:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:28][root][INFO] - Training Epoch: 5/10, step 168/574 completed (loss: 0.23883168399333954, acc: 0.9166666865348816)
[2025-01-06 01:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:28][root][INFO] - Training Epoch: 5/10, step 169/574 completed (loss: 0.6741688847541809, acc: 0.8235294222831726)
[2025-01-06 01:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29][root][INFO] - Training Epoch: 5/10, step 170/574 completed (loss: 0.4832727015018463, acc: 0.8424657583236694)
[2025-01-06 01:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29][root][INFO] - Training Epoch: 5/10, step 171/574 completed (loss: 0.24136106669902802, acc: 0.9166666865348816)
[2025-01-06 01:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29][root][INFO] - Training Epoch: 5/10, step 172/574 completed (loss: 0.279418021440506, acc: 0.9259259104728699)
[2025-01-06 01:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30][root][INFO] - Training Epoch: 5/10, step 173/574 completed (loss: 0.28241419792175293, acc: 0.8928571343421936)
[2025-01-06 01:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30][root][INFO] - Training Epoch: 5/10, step 174/574 completed (loss: 0.725368857383728, acc: 0.8141592741012573)
[2025-01-06 01:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:31][root][INFO] - Training Epoch: 5/10, step 175/574 completed (loss: 0.31294044852256775, acc: 0.8985507488250732)
[2025-01-06 01:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:31][root][INFO] - Training Epoch: 5/10, step 176/574 completed (loss: 0.27678382396698, acc: 0.9090909361839294)
[2025-01-06 01:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:32][root][INFO] - Training Epoch: 5/10, step 177/574 completed (loss: 0.8894486427307129, acc: 0.7557252049446106)
[2025-01-06 01:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:32][root][INFO] - Training Epoch: 5/10, step 178/574 completed (loss: 0.6728252172470093, acc: 0.8074073791503906)
[2025-01-06 01:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33][root][INFO] - Training Epoch: 5/10, step 179/574 completed (loss: 0.18929582834243774, acc: 0.9344262480735779)
[2025-01-06 01:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33][root][INFO] - Training Epoch: 5/10, step 180/574 completed (loss: 0.02224622666835785, acc: 1.0)
[2025-01-06 01:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33][root][INFO] - Training Epoch: 5/10, step 181/574 completed (loss: 0.14887766540050507, acc: 0.9599999785423279)
[2025-01-06 01:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34][root][INFO] - Training Epoch: 5/10, step 182/574 completed (loss: 0.03602030500769615, acc: 1.0)
[2025-01-06 01:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34][root][INFO] - Training Epoch: 5/10, step 183/574 completed (loss: 0.1246364414691925, acc: 0.9634146094322205)
[2025-01-06 01:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34][root][INFO] - Training Epoch: 5/10, step 184/574 completed (loss: 0.4223092496395111, acc: 0.903323233127594)
[2025-01-06 01:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:35][root][INFO] - Training Epoch: 5/10, step 185/574 completed (loss: 0.3342105746269226, acc: 0.8991354703903198)
[2025-01-06 01:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:35][root][INFO] - Training Epoch: 5/10, step 186/574 completed (loss: 0.32903796434402466, acc: 0.90625)
[2025-01-06 01:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36][root][INFO] - Training Epoch: 5/10, step 187/574 completed (loss: 0.4311161935329437, acc: 0.8818011283874512)
[2025-01-06 01:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36][root][INFO] - Training Epoch: 5/10, step 188/574 completed (loss: 0.3561861515045166, acc: 0.9110320210456848)
[2025-01-06 01:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37][root][INFO] - Training Epoch: 5/10, step 189/574 completed (loss: 0.16705404222011566, acc: 0.9599999785423279)
[2025-01-06 01:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37][root][INFO] - Training Epoch: 5/10, step 190/574 completed (loss: 0.6014307737350464, acc: 0.7790697813034058)
[2025-01-06 01:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:38][root][INFO] - Training Epoch: 5/10, step 191/574 completed (loss: 0.9525665640830994, acc: 0.7222222089767456)
[2025-01-06 01:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:39][root][INFO] - Training Epoch: 5/10, step 192/574 completed (loss: 0.7344503998756409, acc: 0.7954545617103577)
[2025-01-06 01:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:40][root][INFO] - Training Epoch: 5/10, step 193/574 completed (loss: 0.471028596162796, acc: 0.8588235378265381)
[2025-01-06 01:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:41][root][INFO] - Training Epoch: 5/10, step 194/574 completed (loss: 0.689363956451416, acc: 0.790123462677002)
[2025-01-06 01:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:42][root][INFO] - Training Epoch: 5/10, step 195/574 completed (loss: 0.15424779057502747, acc: 0.9516128897666931)
[2025-01-06 01:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:42][root][INFO] - Training Epoch: 5/10, step 196/574 completed (loss: 0.017183156684041023, acc: 1.0)
[2025-01-06 01:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:42][root][INFO] - Training Epoch: 5/10, step 197/574 completed (loss: 0.22089970111846924, acc: 0.925000011920929)
[2025-01-06 01:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:43][root][INFO] - Training Epoch: 5/10, step 198/574 completed (loss: 0.312086820602417, acc: 0.9411764740943909)
[2025-01-06 01:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:43][root][INFO] - Training Epoch: 5/10, step 199/574 completed (loss: 0.704999566078186, acc: 0.8088235259056091)
[2025-01-06 01:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:43][root][INFO] - Training Epoch: 5/10, step 200/574 completed (loss: 0.4487472474575043, acc: 0.8644067645072937)
[2025-01-06 01:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:44][root][INFO] - Training Epoch: 5/10, step 201/574 completed (loss: 0.4792948365211487, acc: 0.8283582329750061)
[2025-01-06 01:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:44][root][INFO] - Training Epoch: 5/10, step 202/574 completed (loss: 0.450886070728302, acc: 0.8349514603614807)
[2025-01-06 01:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:44][root][INFO] - Training Epoch: 5/10, step 203/574 completed (loss: 0.2656056582927704, acc: 0.9365079402923584)
[2025-01-06 01:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45][root][INFO] - Training Epoch: 5/10, step 204/574 completed (loss: 0.009846214205026627, acc: 1.0)
[2025-01-06 01:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45][root][INFO] - Training Epoch: 5/10, step 205/574 completed (loss: 0.15028609335422516, acc: 0.9372197389602661)
[2025-01-06 01:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45][root][INFO] - Training Epoch: 5/10, step 206/574 completed (loss: 0.3447166383266449, acc: 0.8976377844810486)
[2025-01-06 01:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46][root][INFO] - Training Epoch: 5/10, step 207/574 completed (loss: 0.23373562097549438, acc: 0.9353448152542114)
[2025-01-06 01:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46][root][INFO] - Training Epoch: 5/10, step 208/574 completed (loss: 0.2838641107082367, acc: 0.9311594367027283)
[2025-01-06 01:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46][root][INFO] - Training Epoch: 5/10, step 209/574 completed (loss: 0.3022327125072479, acc: 0.9105058312416077)
[2025-01-06 01:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47][root][INFO] - Training Epoch: 5/10, step 210/574 completed (loss: 0.07576348632574081, acc: 0.967391312122345)
[2025-01-06 01:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47][root][INFO] - Training Epoch: 5/10, step 211/574 completed (loss: 0.020645905286073685, acc: 1.0)
[2025-01-06 01:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47][root][INFO] - Training Epoch: 5/10, step 212/574 completed (loss: 0.022029781714081764, acc: 1.0)
[2025-01-06 01:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:48][root][INFO] - Training Epoch: 5/10, step 213/574 completed (loss: 0.048863485455513, acc: 0.978723406791687)
[2025-01-06 01:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:48][root][INFO] - Training Epoch: 5/10, step 214/574 completed (loss: 0.15887190401554108, acc: 0.9769230484962463)
[2025-01-06 01:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49][root][INFO] - Training Epoch: 5/10, step 215/574 completed (loss: 0.10033900290727615, acc: 0.9729729890823364)
[2025-01-06 01:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49][root][INFO] - Training Epoch: 5/10, step 216/574 completed (loss: 0.05745922774076462, acc: 0.9651162624359131)
[2025-01-06 01:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50][root][INFO] - Training Epoch: 5/10, step 217/574 completed (loss: 0.12909477949142456, acc: 0.9459459185600281)
[2025-01-06 01:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50][root][INFO] - Training Epoch: 5/10, step 218/574 completed (loss: 0.08868568390607834, acc: 0.9555555582046509)
[2025-01-06 01:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50][root][INFO] - Training Epoch: 5/10, step 219/574 completed (loss: 0.013406667858362198, acc: 1.0)
[2025-01-06 01:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51][root][INFO] - Training Epoch: 5/10, step 220/574 completed (loss: 0.0038380224723368883, acc: 1.0)
[2025-01-06 01:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51][root][INFO] - Training Epoch: 5/10, step 221/574 completed (loss: 0.03670699894428253, acc: 1.0)
[2025-01-06 01:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51][root][INFO] - Training Epoch: 5/10, step 222/574 completed (loss: 0.2398536056280136, acc: 0.8846153616905212)
[2025-01-06 01:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:52][root][INFO] - Training Epoch: 5/10, step 223/574 completed (loss: 0.3017447590827942, acc: 0.929347813129425)
[2025-01-06 01:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53][root][INFO] - Training Epoch: 5/10, step 224/574 completed (loss: 0.39195144176483154, acc: 0.8977272510528564)
[2025-01-06 01:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53][root][INFO] - Training Epoch: 5/10, step 225/574 completed (loss: 0.45899882912635803, acc: 0.8829787373542786)
[2025-01-06 01:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53][root][INFO] - Training Epoch: 5/10, step 226/574 completed (loss: 0.1963333934545517, acc: 0.9245283007621765)
[2025-01-06 01:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54][root][INFO] - Training Epoch: 5/10, step 227/574 completed (loss: 0.10307898372411728, acc: 0.9833333492279053)
[2025-01-06 01:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54][root][INFO] - Training Epoch: 5/10, step 228/574 completed (loss: 0.1353224515914917, acc: 0.9534883499145508)
[2025-01-06 01:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54][root][INFO] - Training Epoch: 5/10, step 229/574 completed (loss: 0.32998424768447876, acc: 0.9666666388511658)
[2025-01-06 01:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55][root][INFO] - Training Epoch: 5/10, step 230/574 completed (loss: 0.9734005331993103, acc: 0.7473683953285217)
[2025-01-06 01:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55][root][INFO] - Training Epoch: 5/10, step 231/574 completed (loss: 0.9347737431526184, acc: 0.699999988079071)
[2025-01-06 01:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:56][root][INFO] - Training Epoch: 5/10, step 232/574 completed (loss: 0.9875932335853577, acc: 0.6944444179534912)
[2025-01-06 01:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:56][root][INFO] - Training Epoch: 5/10, step 233/574 completed (loss: 1.3000097274780273, acc: 0.6238532066345215)
[2025-01-06 01:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57][root][INFO] - Training Epoch: 5/10, step 234/574 completed (loss: 0.7758925557136536, acc: 0.7307692170143127)
[2025-01-06 01:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57][root][INFO] - Training Epoch: 5/10, step 235/574 completed (loss: 0.22973023355007172, acc: 0.9473684430122375)
[2025-01-06 01:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57][root][INFO] - Training Epoch: 5/10, step 236/574 completed (loss: 0.013069742359220982, acc: 1.0)
[2025-01-06 01:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57][root][INFO] - Training Epoch: 5/10, step 237/574 completed (loss: 0.15884198248386383, acc: 0.9090909361839294)
[2025-01-06 01:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58][root][INFO] - Training Epoch: 5/10, step 238/574 completed (loss: 0.36364421248435974, acc: 0.9259259104728699)
[2025-01-06 01:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58][root][INFO] - Training Epoch: 5/10, step 239/574 completed (loss: 0.2088516503572464, acc: 0.9142857193946838)
[2025-01-06 01:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58][root][INFO] - Training Epoch: 5/10, step 240/574 completed (loss: 0.5573582053184509, acc: 0.8636363744735718)
[2025-01-06 01:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59][root][INFO] - Training Epoch: 5/10, step 241/574 completed (loss: 0.1920190453529358, acc: 0.9545454382896423)
[2025-01-06 01:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59][root][INFO] - Training Epoch: 5/10, step 242/574 completed (loss: 0.5842257738113403, acc: 0.8387096524238586)
[2025-01-06 01:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:00][root][INFO] - Training Epoch: 5/10, step 243/574 completed (loss: 0.5107370615005493, acc: 0.7727272510528564)
[2025-01-06 01:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:00][root][INFO] - Training Epoch: 5/10, step 244/574 completed (loss: 0.003218521596863866, acc: 1.0)
[2025-01-06 01:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01][root][INFO] - Training Epoch: 5/10, step 245/574 completed (loss: 0.07818812876939774, acc: 0.9615384340286255)
[2025-01-06 01:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01][root][INFO] - Training Epoch: 5/10, step 246/574 completed (loss: 0.01687948778271675, acc: 1.0)
[2025-01-06 01:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01][root][INFO] - Training Epoch: 5/10, step 247/574 completed (loss: 0.01949911192059517, acc: 1.0)
[2025-01-06 01:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02][root][INFO] - Training Epoch: 5/10, step 248/574 completed (loss: 0.18603216111660004, acc: 0.9459459185600281)
[2025-01-06 01:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02][root][INFO] - Training Epoch: 5/10, step 249/574 completed (loss: 0.2098017930984497, acc: 0.8918918967247009)
[2025-01-06 01:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02][root][INFO] - Training Epoch: 5/10, step 250/574 completed (loss: 0.012388368137180805, acc: 1.0)
[2025-01-06 01:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03][root][INFO] - Training Epoch: 5/10, step 251/574 completed (loss: 0.13696187734603882, acc: 0.970588207244873)
[2025-01-06 01:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03][root][INFO] - Training Epoch: 5/10, step 252/574 completed (loss: 0.012535854242742062, acc: 1.0)
[2025-01-06 01:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03][root][INFO] - Training Epoch: 5/10, step 253/574 completed (loss: 0.0045270840637385845, acc: 1.0)
[2025-01-06 01:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04][root][INFO] - Training Epoch: 5/10, step 254/574 completed (loss: 0.0021776207722723484, acc: 1.0)
[2025-01-06 01:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04][root][INFO] - Training Epoch: 5/10, step 255/574 completed (loss: 0.05031290277838707, acc: 1.0)
[2025-01-06 01:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04][root][INFO] - Training Epoch: 5/10, step 256/574 completed (loss: 0.016140755265951157, acc: 1.0)
[2025-01-06 01:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05][root][INFO] - Training Epoch: 5/10, step 257/574 completed (loss: 0.0980723425745964, acc: 0.9714285731315613)
[2025-01-06 01:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05][root][INFO] - Training Epoch: 5/10, step 258/574 completed (loss: 0.06765143573284149, acc: 0.9736841917037964)
[2025-01-06 01:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06][root][INFO] - Training Epoch: 5/10, step 259/574 completed (loss: 0.2562592923641205, acc: 0.9433962106704712)
[2025-01-06 01:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06][root][INFO] - Training Epoch: 5/10, step 260/574 completed (loss: 0.19170226156711578, acc: 0.9416666626930237)
[2025-01-06 01:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06][root][INFO] - Training Epoch: 5/10, step 261/574 completed (loss: 0.14048077166080475, acc: 0.9166666865348816)
[2025-01-06 01:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07][root][INFO] - Training Epoch: 5/10, step 262/574 completed (loss: 0.32756277918815613, acc: 0.9677419066429138)
[2025-01-06 01:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07][root][INFO] - Training Epoch: 5/10, step 263/574 completed (loss: 0.7636575102806091, acc: 0.8266666531562805)
[2025-01-06 01:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08][root][INFO] - Training Epoch: 5/10, step 264/574 completed (loss: 0.35506999492645264, acc: 0.8333333134651184)
[2025-01-06 01:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08][root][INFO] - Training Epoch: 5/10, step 265/574 completed (loss: 0.8403472304344177, acc: 0.7519999742507935)
[2025-01-06 01:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09][root][INFO] - Training Epoch: 5/10, step 266/574 completed (loss: 0.8482652902603149, acc: 0.7191011309623718)
[2025-01-06 01:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09][root][INFO] - Training Epoch: 5/10, step 267/574 completed (loss: 0.4745960235595703, acc: 0.8513513803482056)
[2025-01-06 01:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09][root][INFO] - Training Epoch: 5/10, step 268/574 completed (loss: 0.30627503991127014, acc: 0.931034505367279)
[2025-01-06 01:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:10][root][INFO] - Training Epoch: 5/10, step 269/574 completed (loss: 0.033590421080589294, acc: 1.0)
[2025-01-06 01:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:10][root][INFO] - Training Epoch: 5/10, step 270/574 completed (loss: 0.044903457164764404, acc: 0.9545454382896423)
[2025-01-06 01:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:10][root][INFO] - Training Epoch: 5/10, step 271/574 completed (loss: 0.12447575479745865, acc: 0.9375)
[2025-01-06 01:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11][root][INFO] - Training Epoch: 5/10, step 272/574 completed (loss: 0.10528530925512314, acc: 0.9666666388511658)
[2025-01-06 01:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11][root][INFO] - Training Epoch: 5/10, step 273/574 completed (loss: 0.11632576584815979, acc: 0.9833333492279053)
[2025-01-06 01:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12][root][INFO] - Training Epoch: 5/10, step 274/574 completed (loss: 0.21135729551315308, acc: 0.9375)
[2025-01-06 01:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12][root][INFO] - Training Epoch: 5/10, step 275/574 completed (loss: 0.12337944656610489, acc: 0.9666666388511658)
[2025-01-06 01:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12][root][INFO] - Training Epoch: 5/10, step 276/574 completed (loss: 0.18845978379249573, acc: 0.931034505367279)
[2025-01-06 01:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:13][root][INFO] - Training Epoch: 5/10, step 277/574 completed (loss: 0.011342717334628105, acc: 1.0)
[2025-01-06 01:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:41][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0981, device='cuda:0') eval_epoch_loss=tensor(0.7410, device='cuda:0') eval_epoch_acc=tensor(0.8224, device='cuda:0')
[2025-01-06 01:26:41][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:26:41][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:26:41][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_278_loss_0.7410483360290527/model.pt
[2025-01-06 01:26:41][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42][root][INFO] - Training Epoch: 5/10, step 278/574 completed (loss: 0.09841760993003845, acc: 0.978723406791687)
[2025-01-06 01:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42][root][INFO] - Training Epoch: 5/10, step 279/574 completed (loss: 0.21515560150146484, acc: 0.9166666865348816)
[2025-01-06 01:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42][root][INFO] - Training Epoch: 5/10, step 280/574 completed (loss: 0.03076830692589283, acc: 1.0)
[2025-01-06 01:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43][root][INFO] - Training Epoch: 5/10, step 281/574 completed (loss: 0.2155061960220337, acc: 0.9277108311653137)
[2025-01-06 01:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43][root][INFO] - Training Epoch: 5/10, step 282/574 completed (loss: 0.43983158469200134, acc: 0.8611111044883728)
[2025-01-06 01:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43][root][INFO] - Training Epoch: 5/10, step 283/574 completed (loss: 0.055443018674850464, acc: 0.9736841917037964)
[2025-01-06 01:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44][root][INFO] - Training Epoch: 5/10, step 284/574 completed (loss: 0.07251576334238052, acc: 0.970588207244873)
[2025-01-06 01:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44][root][INFO] - Training Epoch: 5/10, step 285/574 completed (loss: 0.03566237539052963, acc: 1.0)
[2025-01-06 01:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44][root][INFO] - Training Epoch: 5/10, step 286/574 completed (loss: 0.2673368752002716, acc: 0.9140625)
[2025-01-06 01:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45][root][INFO] - Training Epoch: 5/10, step 287/574 completed (loss: 0.2714102864265442, acc: 0.9279999732971191)
[2025-01-06 01:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45][root][INFO] - Training Epoch: 5/10, step 288/574 completed (loss: 0.16167007386684418, acc: 0.9450549483299255)
[2025-01-06 01:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45][root][INFO] - Training Epoch: 5/10, step 289/574 completed (loss: 0.28776121139526367, acc: 0.8944099545478821)
[2025-01-06 01:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46][root][INFO] - Training Epoch: 5/10, step 290/574 completed (loss: 0.39486512541770935, acc: 0.876288652420044)
[2025-01-06 01:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46][root][INFO] - Training Epoch: 5/10, step 291/574 completed (loss: 0.008603562600910664, acc: 1.0)
[2025-01-06 01:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46][root][INFO] - Training Epoch: 5/10, step 292/574 completed (loss: 0.038914937525987625, acc: 1.0)
[2025-01-06 01:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47][root][INFO] - Training Epoch: 5/10, step 293/574 completed (loss: 0.0989656001329422, acc: 0.982758641242981)
[2025-01-06 01:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47][root][INFO] - Training Epoch: 5/10, step 294/574 completed (loss: 0.1423625946044922, acc: 0.9272727370262146)
[2025-01-06 01:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:48][root][INFO] - Training Epoch: 5/10, step 295/574 completed (loss: 0.35886701941490173, acc: 0.907216489315033)
[2025-01-06 01:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:48][root][INFO] - Training Epoch: 5/10, step 296/574 completed (loss: 0.1325782835483551, acc: 0.9482758641242981)
[2025-01-06 01:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:48][root][INFO] - Training Epoch: 5/10, step 297/574 completed (loss: 0.07159997522830963, acc: 0.9629629850387573)
[2025-01-06 01:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49][root][INFO] - Training Epoch: 5/10, step 298/574 completed (loss: 0.04519377276301384, acc: 1.0)
[2025-01-06 01:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49][root][INFO] - Training Epoch: 5/10, step 299/574 completed (loss: 0.03854674845933914, acc: 0.9821428656578064)
[2025-01-06 01:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49][root][INFO] - Training Epoch: 5/10, step 300/574 completed (loss: 0.02324402704834938, acc: 1.0)
[2025-01-06 01:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50][root][INFO] - Training Epoch: 5/10, step 301/574 completed (loss: 0.03246243670582771, acc: 1.0)
[2025-01-06 01:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50][root][INFO] - Training Epoch: 5/10, step 302/574 completed (loss: 0.008988060988485813, acc: 1.0)
[2025-01-06 01:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50][root][INFO] - Training Epoch: 5/10, step 303/574 completed (loss: 0.007685565855354071, acc: 1.0)
[2025-01-06 01:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51][root][INFO] - Training Epoch: 5/10, step 304/574 completed (loss: 0.04933689162135124, acc: 0.96875)
[2025-01-06 01:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51][root][INFO] - Training Epoch: 5/10, step 305/574 completed (loss: 0.14838124811649323, acc: 0.9344262480735779)
[2025-01-06 01:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51][root][INFO] - Training Epoch: 5/10, step 306/574 completed (loss: 0.08371438086032867, acc: 0.9666666388511658)
[2025-01-06 01:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51][root][INFO] - Training Epoch: 5/10, step 307/574 completed (loss: 0.012091594748198986, acc: 1.0)
[2025-01-06 01:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52][root][INFO] - Training Epoch: 5/10, step 308/574 completed (loss: 0.10057847946882248, acc: 0.95652174949646)
[2025-01-06 01:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52][root][INFO] - Training Epoch: 5/10, step 309/574 completed (loss: 0.0778363049030304, acc: 0.9722222089767456)
[2025-01-06 01:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53][root][INFO] - Training Epoch: 5/10, step 310/574 completed (loss: 0.04195217415690422, acc: 1.0)
[2025-01-06 01:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53][root][INFO] - Training Epoch: 5/10, step 311/574 completed (loss: 0.07264120876789093, acc: 0.9871794581413269)
[2025-01-06 01:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53][root][INFO] - Training Epoch: 5/10, step 312/574 completed (loss: 0.05606301501393318, acc: 0.9795918464660645)
[2025-01-06 01:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54][root][INFO] - Training Epoch: 5/10, step 313/574 completed (loss: 0.0021378439851105213, acc: 1.0)
[2025-01-06 01:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54][root][INFO] - Training Epoch: 5/10, step 314/574 completed (loss: 0.004006546456366777, acc: 1.0)
[2025-01-06 01:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54][root][INFO] - Training Epoch: 5/10, step 315/574 completed (loss: 0.007942024618387222, acc: 1.0)
[2025-01-06 01:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55][root][INFO] - Training Epoch: 5/10, step 316/574 completed (loss: 0.2300058901309967, acc: 0.9032257795333862)
[2025-01-06 01:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55][root][INFO] - Training Epoch: 5/10, step 317/574 completed (loss: 0.02409793622791767, acc: 1.0)
[2025-01-06 01:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55][root][INFO] - Training Epoch: 5/10, step 318/574 completed (loss: 0.03777694329619408, acc: 0.9903846383094788)
[2025-01-06 01:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56][root][INFO] - Training Epoch: 5/10, step 319/574 completed (loss: 0.054370563477277756, acc: 0.9777777791023254)
[2025-01-06 01:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56][root][INFO] - Training Epoch: 5/10, step 320/574 completed (loss: 0.022442061454057693, acc: 1.0)
[2025-01-06 01:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56][root][INFO] - Training Epoch: 5/10, step 321/574 completed (loss: 0.02042566053569317, acc: 0.9800000190734863)
[2025-01-06 01:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57][root][INFO] - Training Epoch: 5/10, step 322/574 completed (loss: 0.3985764682292938, acc: 0.8518518805503845)
[2025-01-06 01:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57][root][INFO] - Training Epoch: 5/10, step 323/574 completed (loss: 0.3829922080039978, acc: 0.8571428656578064)
[2025-01-06 01:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57][root][INFO] - Training Epoch: 5/10, step 324/574 completed (loss: 0.1872507780790329, acc: 0.8717948794364929)
[2025-01-06 01:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58][root][INFO] - Training Epoch: 5/10, step 325/574 completed (loss: 0.49379831552505493, acc: 0.8048780560493469)
[2025-01-06 01:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58][root][INFO] - Training Epoch: 5/10, step 326/574 completed (loss: 0.3369772732257843, acc: 0.9210526347160339)
[2025-01-06 01:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58][root][INFO] - Training Epoch: 5/10, step 327/574 completed (loss: 0.009022724814713001, acc: 1.0)
[2025-01-06 01:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59][root][INFO] - Training Epoch: 5/10, step 328/574 completed (loss: 0.06346061080694199, acc: 0.9642857313156128)
[2025-01-06 01:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59][root][INFO] - Training Epoch: 5/10, step 329/574 completed (loss: 0.002022427273914218, acc: 1.0)
[2025-01-06 01:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59][root][INFO] - Training Epoch: 5/10, step 330/574 completed (loss: 0.013742838986217976, acc: 1.0)
[2025-01-06 01:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59][root][INFO] - Training Epoch: 5/10, step 331/574 completed (loss: 0.10784904658794403, acc: 0.9838709831237793)
[2025-01-06 01:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00][root][INFO] - Training Epoch: 5/10, step 332/574 completed (loss: 0.15693975985050201, acc: 0.9473684430122375)
[2025-01-06 01:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00][root][INFO] - Training Epoch: 5/10, step 333/574 completed (loss: 0.030967257916927338, acc: 1.0)
[2025-01-06 01:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00][root][INFO] - Training Epoch: 5/10, step 334/574 completed (loss: 0.01908538118004799, acc: 1.0)
[2025-01-06 01:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01][root][INFO] - Training Epoch: 5/10, step 335/574 completed (loss: 0.004602448549121618, acc: 1.0)
[2025-01-06 01:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01][root][INFO] - Training Epoch: 5/10, step 336/574 completed (loss: 0.29855266213417053, acc: 0.8799999952316284)
[2025-01-06 01:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02][root][INFO] - Training Epoch: 5/10, step 337/574 completed (loss: 0.7991811037063599, acc: 0.7586206793785095)
[2025-01-06 01:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02][root][INFO] - Training Epoch: 5/10, step 338/574 completed (loss: 0.8361055254936218, acc: 0.7446808218955994)
[2025-01-06 01:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02][root][INFO] - Training Epoch: 5/10, step 339/574 completed (loss: 0.5481107831001282, acc: 0.8072289228439331)
[2025-01-06 01:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03][root][INFO] - Training Epoch: 5/10, step 340/574 completed (loss: 0.0021453124936670065, acc: 1.0)
[2025-01-06 01:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03][root][INFO] - Training Epoch: 5/10, step 341/574 completed (loss: 0.0531175397336483, acc: 0.9743589758872986)
[2025-01-06 01:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03][root][INFO] - Training Epoch: 5/10, step 342/574 completed (loss: 0.1995822936296463, acc: 0.9156626462936401)
[2025-01-06 01:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03][root][INFO] - Training Epoch: 5/10, step 343/574 completed (loss: 0.31471776962280273, acc: 0.9245283007621765)
[2025-01-06 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04][root][INFO] - Training Epoch: 5/10, step 344/574 completed (loss: 0.18801189959049225, acc: 0.9620253443717957)
[2025-01-06 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04][root][INFO] - Training Epoch: 5/10, step 345/574 completed (loss: 0.09130245447158813, acc: 0.9411764740943909)
[2025-01-06 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04][root][INFO] - Training Epoch: 5/10, step 346/574 completed (loss: 0.1372736692428589, acc: 0.9253731369972229)
[2025-01-06 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05][root][INFO] - Training Epoch: 5/10, step 347/574 completed (loss: 0.0011219799052923918, acc: 1.0)
[2025-01-06 01:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05][root][INFO] - Training Epoch: 5/10, step 348/574 completed (loss: 0.04960617050528526, acc: 0.9599999785423279)
[2025-01-06 01:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05][root][INFO] - Training Epoch: 5/10, step 349/574 completed (loss: 0.31888872385025024, acc: 0.8888888955116272)
[2025-01-06 01:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06][root][INFO] - Training Epoch: 5/10, step 350/574 completed (loss: 0.23383863270282745, acc: 0.930232584476471)
[2025-01-06 01:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06][root][INFO] - Training Epoch: 5/10, step 351/574 completed (loss: 0.1721477061510086, acc: 0.9230769276618958)
[2025-01-06 01:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06][root][INFO] - Training Epoch: 5/10, step 352/574 completed (loss: 0.14895373582839966, acc: 0.9555555582046509)
[2025-01-06 01:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07][root][INFO] - Training Epoch: 5/10, step 353/574 completed (loss: 0.005822019651532173, acc: 1.0)
[2025-01-06 01:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07][root][INFO] - Training Epoch: 5/10, step 354/574 completed (loss: 0.053257569670677185, acc: 0.9615384340286255)
[2025-01-06 01:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07][root][INFO] - Training Epoch: 5/10, step 355/574 completed (loss: 0.3198077976703644, acc: 0.901098906993866)
[2025-01-06 01:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08][root][INFO] - Training Epoch: 5/10, step 356/574 completed (loss: 0.36088666319847107, acc: 0.904347836971283)
[2025-01-06 01:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08][root][INFO] - Training Epoch: 5/10, step 357/574 completed (loss: 0.17115704715251923, acc: 0.945652186870575)
[2025-01-06 01:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08][root][INFO] - Training Epoch: 5/10, step 358/574 completed (loss: 0.13603730499744415, acc: 0.9795918464660645)
[2025-01-06 01:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09][root][INFO] - Training Epoch: 5/10, step 359/574 completed (loss: 0.0006189599516801536, acc: 1.0)
[2025-01-06 01:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09][root][INFO] - Training Epoch: 5/10, step 360/574 completed (loss: 0.04232315346598625, acc: 1.0)
[2025-01-06 01:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09][root][INFO] - Training Epoch: 5/10, step 361/574 completed (loss: 0.2859255075454712, acc: 0.9024389982223511)
[2025-01-06 01:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10][root][INFO] - Training Epoch: 5/10, step 362/574 completed (loss: 0.18168912827968597, acc: 0.9555555582046509)
[2025-01-06 01:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10][root][INFO] - Training Epoch: 5/10, step 363/574 completed (loss: 0.013363061472773552, acc: 1.0)
[2025-01-06 01:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10][root][INFO] - Training Epoch: 5/10, step 364/574 completed (loss: 0.016092421486973763, acc: 1.0)
[2025-01-06 01:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11][root][INFO] - Training Epoch: 5/10, step 365/574 completed (loss: 0.020817842334508896, acc: 1.0)
[2025-01-06 01:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11][root][INFO] - Training Epoch: 5/10, step 366/574 completed (loss: 0.00029713966068811715, acc: 1.0)
[2025-01-06 01:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11][root][INFO] - Training Epoch: 5/10, step 367/574 completed (loss: 0.00237796432338655, acc: 1.0)
[2025-01-06 01:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12][root][INFO] - Training Epoch: 5/10, step 368/574 completed (loss: 0.01721382513642311, acc: 1.0)
[2025-01-06 01:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12][root][INFO] - Training Epoch: 5/10, step 369/574 completed (loss: 0.027505645528435707, acc: 1.0)
[2025-01-06 01:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12][root][INFO] - Training Epoch: 5/10, step 370/574 completed (loss: 0.43578556180000305, acc: 0.8787878751754761)
[2025-01-06 01:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:13][root][INFO] - Training Epoch: 5/10, step 371/574 completed (loss: 0.20952768623828888, acc: 0.9339622855186462)
[2025-01-06 01:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14][root][INFO] - Training Epoch: 5/10, step 372/574 completed (loss: 0.10694176703691483, acc: 0.9777777791023254)
[2025-01-06 01:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14][root][INFO] - Training Epoch: 5/10, step 373/574 completed (loss: 0.10039450228214264, acc: 0.9642857313156128)
[2025-01-06 01:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14][root][INFO] - Training Epoch: 5/10, step 374/574 completed (loss: 0.12462795525789261, acc: 0.9714285731315613)
[2025-01-06 01:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15][root][INFO] - Training Epoch: 5/10, step 375/574 completed (loss: 0.0021491909865289927, acc: 1.0)
[2025-01-06 01:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15][root][INFO] - Training Epoch: 5/10, step 376/574 completed (loss: 0.001702394220046699, acc: 1.0)
[2025-01-06 01:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15][root][INFO] - Training Epoch: 5/10, step 377/574 completed (loss: 0.06860318034887314, acc: 0.9791666865348816)
[2025-01-06 01:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15][root][INFO] - Training Epoch: 5/10, step 378/574 completed (loss: 0.029737984761595726, acc: 0.9894737005233765)
[2025-01-06 01:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16][root][INFO] - Training Epoch: 5/10, step 379/574 completed (loss: 0.3092438876628876, acc: 0.9221556782722473)
[2025-01-06 01:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16][root][INFO] - Training Epoch: 5/10, step 380/574 completed (loss: 0.1914842575788498, acc: 0.9548872113227844)
[2025-01-06 01:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:17][root][INFO] - Training Epoch: 5/10, step 381/574 completed (loss: 0.3580690324306488, acc: 0.8823529481887817)
[2025-01-06 01:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:18][root][INFO] - Training Epoch: 5/10, step 382/574 completed (loss: 0.047440916299819946, acc: 0.9909909963607788)
[2025-01-06 01:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:18][root][INFO] - Training Epoch: 5/10, step 383/574 completed (loss: 0.10627783089876175, acc: 0.9285714030265808)
[2025-01-06 01:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19][root][INFO] - Training Epoch: 5/10, step 384/574 completed (loss: 0.0309599582105875, acc: 0.9642857313156128)
[2025-01-06 01:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19][root][INFO] - Training Epoch: 5/10, step 385/574 completed (loss: 0.07928617298603058, acc: 0.96875)
[2025-01-06 01:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19][root][INFO] - Training Epoch: 5/10, step 386/574 completed (loss: 0.0024190342519432306, acc: 1.0)
[2025-01-06 01:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19][root][INFO] - Training Epoch: 5/10, step 387/574 completed (loss: 0.0020858810748904943, acc: 1.0)
[2025-01-06 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20][root][INFO] - Training Epoch: 5/10, step 388/574 completed (loss: 0.0012550875544548035, acc: 1.0)
[2025-01-06 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20][root][INFO] - Training Epoch: 5/10, step 389/574 completed (loss: 0.0056808809749782085, acc: 1.0)
[2025-01-06 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20][root][INFO] - Training Epoch: 5/10, step 390/574 completed (loss: 0.16318468749523163, acc: 0.9523809552192688)
[2025-01-06 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21][root][INFO] - Training Epoch: 5/10, step 391/574 completed (loss: 0.2815544903278351, acc: 0.8703703880310059)
[2025-01-06 01:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21][root][INFO] - Training Epoch: 5/10, step 392/574 completed (loss: 0.5557165741920471, acc: 0.844660222530365)
[2025-01-06 01:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:22][root][INFO] - Training Epoch: 5/10, step 393/574 completed (loss: 0.6656886339187622, acc: 0.8014705777168274)
[2025-01-06 01:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:22][root][INFO] - Training Epoch: 5/10, step 394/574 completed (loss: 0.30066901445388794, acc: 0.8999999761581421)
[2025-01-06 01:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:22][root][INFO] - Training Epoch: 5/10, step 395/574 completed (loss: 0.42622601985931396, acc: 0.8472222089767456)
[2025-01-06 01:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23][root][INFO] - Training Epoch: 5/10, step 396/574 completed (loss: 0.23898516595363617, acc: 0.930232584476471)
[2025-01-06 01:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23][root][INFO] - Training Epoch: 5/10, step 397/574 completed (loss: 0.007470321375876665, acc: 1.0)
[2025-01-06 01:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23][root][INFO] - Training Epoch: 5/10, step 398/574 completed (loss: 0.07655225694179535, acc: 1.0)
[2025-01-06 01:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24][root][INFO] - Training Epoch: 5/10, step 399/574 completed (loss: 0.013977003283798695, acc: 1.0)
[2025-01-06 01:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24][root][INFO] - Training Epoch: 5/10, step 400/574 completed (loss: 0.16584360599517822, acc: 0.9558823704719543)
[2025-01-06 01:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25][root][INFO] - Training Epoch: 5/10, step 401/574 completed (loss: 0.27220937609672546, acc: 0.9200000166893005)
[2025-01-06 01:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25][root][INFO] - Training Epoch: 5/10, step 402/574 completed (loss: 0.14876987040042877, acc: 0.939393937587738)
[2025-01-06 01:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25][root][INFO] - Training Epoch: 5/10, step 403/574 completed (loss: 0.12289396673440933, acc: 0.9696969985961914)
[2025-01-06 01:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26][root][INFO] - Training Epoch: 5/10, step 404/574 completed (loss: 0.08876708894968033, acc: 0.9677419066429138)
[2025-01-06 01:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26][root][INFO] - Training Epoch: 5/10, step 405/574 completed (loss: 0.002753455424681306, acc: 1.0)
[2025-01-06 01:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26][root][INFO] - Training Epoch: 5/10, step 406/574 completed (loss: 0.02520223706960678, acc: 1.0)
[2025-01-06 01:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:27][root][INFO] - Training Epoch: 5/10, step 407/574 completed (loss: 0.02466546930372715, acc: 1.0)
[2025-01-06 01:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:27][root][INFO] - Training Epoch: 5/10, step 408/574 completed (loss: 0.009954012930393219, acc: 1.0)
[2025-01-06 01:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:27][root][INFO] - Training Epoch: 5/10, step 409/574 completed (loss: 0.015940317884087563, acc: 1.0)
[2025-01-06 01:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28][root][INFO] - Training Epoch: 5/10, step 410/574 completed (loss: 0.08798902481794357, acc: 0.982758641242981)
[2025-01-06 01:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28][root][INFO] - Training Epoch: 5/10, step 411/574 completed (loss: 0.005843082908540964, acc: 1.0)
[2025-01-06 01:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28][root][INFO] - Training Epoch: 5/10, step 412/574 completed (loss: 0.02417730540037155, acc: 1.0)
[2025-01-06 01:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29][root][INFO] - Training Epoch: 5/10, step 413/574 completed (loss: 0.02283143624663353, acc: 1.0)
[2025-01-06 01:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29][root][INFO] - Training Epoch: 5/10, step 414/574 completed (loss: 0.0018240053905174136, acc: 1.0)
[2025-01-06 01:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29][root][INFO] - Training Epoch: 5/10, step 415/574 completed (loss: 0.05823388695716858, acc: 0.9803921580314636)
[2025-01-06 01:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29][root][INFO] - Training Epoch: 5/10, step 416/574 completed (loss: 0.034528426826000214, acc: 1.0)
[2025-01-06 01:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30][root][INFO] - Training Epoch: 5/10, step 417/574 completed (loss: 0.23003429174423218, acc: 0.9444444179534912)
[2025-01-06 01:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30][root][INFO] - Training Epoch: 5/10, step 418/574 completed (loss: 0.08626978099346161, acc: 0.9750000238418579)
[2025-01-06 01:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31][root][INFO] - Training Epoch: 5/10, step 419/574 completed (loss: 0.013079049065709114, acc: 1.0)
[2025-01-06 01:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31][root][INFO] - Training Epoch: 5/10, step 420/574 completed (loss: 0.011386449448764324, acc: 1.0)
[2025-01-06 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3771, device='cuda:0') eval_epoch_loss=tensor(0.8659, device='cuda:0') eval_epoch_acc=tensor(0.8090, device='cuda:0')
[2025-01-06 01:27:57][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:27:57][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:27:57][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_421_loss_0.8658795952796936/model.pt
[2025-01-06 01:27:57][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57][root][INFO] - Training Epoch: 5/10, step 421/574 completed (loss: 0.05972534418106079, acc: 1.0)
[2025-01-06 01:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58][root][INFO] - Training Epoch: 5/10, step 422/574 completed (loss: 0.32626286149024963, acc: 0.9375)
[2025-01-06 01:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58][root][INFO] - Training Epoch: 5/10, step 423/574 completed (loss: 0.19198353588581085, acc: 0.9444444179534912)
[2025-01-06 01:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59][root][INFO] - Training Epoch: 5/10, step 424/574 completed (loss: 0.1071435809135437, acc: 0.9629629850387573)
[2025-01-06 01:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59][root][INFO] - Training Epoch: 5/10, step 425/574 completed (loss: 0.2531212866306305, acc: 0.9696969985961914)
[2025-01-06 01:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59][root][INFO] - Training Epoch: 5/10, step 426/574 completed (loss: 1.0019358396530151, acc: 0.8695651888847351)
[2025-01-06 01:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00][root][INFO] - Training Epoch: 5/10, step 427/574 completed (loss: 0.2059108465909958, acc: 0.9729729890823364)
[2025-01-06 01:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00][root][INFO] - Training Epoch: 5/10, step 428/574 completed (loss: 0.006504349876195192, acc: 1.0)
[2025-01-06 01:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00][root][INFO] - Training Epoch: 5/10, step 429/574 completed (loss: 0.08881250768899918, acc: 0.95652174949646)
[2025-01-06 01:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01][root][INFO] - Training Epoch: 5/10, step 430/574 completed (loss: 0.1860564798116684, acc: 0.9629629850387573)
[2025-01-06 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01][root][INFO] - Training Epoch: 5/10, step 431/574 completed (loss: 0.09227841347455978, acc: 0.9629629850387573)
[2025-01-06 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01][root][INFO] - Training Epoch: 5/10, step 432/574 completed (loss: 0.0058765895664691925, acc: 1.0)
[2025-01-06 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01][root][INFO] - Training Epoch: 5/10, step 433/574 completed (loss: 0.10948709398508072, acc: 0.9722222089767456)
[2025-01-06 01:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02][root][INFO] - Training Epoch: 5/10, step 434/574 completed (loss: 0.0029089106246829033, acc: 1.0)
[2025-01-06 01:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02][root][INFO] - Training Epoch: 5/10, step 435/574 completed (loss: 0.28545039892196655, acc: 0.9696969985961914)
[2025-01-06 01:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02][root][INFO] - Training Epoch: 5/10, step 436/574 completed (loss: 0.30063608288764954, acc: 0.8888888955116272)
[2025-01-06 01:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03][root][INFO] - Training Epoch: 5/10, step 437/574 completed (loss: 0.13650186359882355, acc: 0.9545454382896423)
[2025-01-06 01:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03][root][INFO] - Training Epoch: 5/10, step 438/574 completed (loss: 0.2887611389160156, acc: 0.9047619104385376)
[2025-01-06 01:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03][root][INFO] - Training Epoch: 5/10, step 439/574 completed (loss: 0.1396147608757019, acc: 0.9487179517745972)
[2025-01-06 01:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:04][root][INFO] - Training Epoch: 5/10, step 440/574 completed (loss: 0.08272864669561386, acc: 0.9545454382896423)
[2025-01-06 01:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:04][root][INFO] - Training Epoch: 5/10, step 441/574 completed (loss: 0.5819900631904602, acc: 0.7839999794960022)
[2025-01-06 01:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:05][root][INFO] - Training Epoch: 5/10, step 442/574 completed (loss: 0.4259142577648163, acc: 0.8870967626571655)
[2025-01-06 01:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06][root][INFO] - Training Epoch: 5/10, step 443/574 completed (loss: 0.44142815470695496, acc: 0.8756219148635864)
[2025-01-06 01:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06][root][INFO] - Training Epoch: 5/10, step 444/574 completed (loss: 0.13079889118671417, acc: 0.9622641801834106)
[2025-01-06 01:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06][root][INFO] - Training Epoch: 5/10, step 445/574 completed (loss: 0.08375641703605652, acc: 0.9772727489471436)
[2025-01-06 01:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07][root][INFO] - Training Epoch: 5/10, step 446/574 completed (loss: 0.054686423391103745, acc: 0.95652174949646)
[2025-01-06 01:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07][root][INFO] - Training Epoch: 5/10, step 447/574 completed (loss: 0.1443122923374176, acc: 0.9230769276618958)
[2025-01-06 01:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07][root][INFO] - Training Epoch: 5/10, step 448/574 completed (loss: 0.11397700756788254, acc: 0.9642857313156128)
[2025-01-06 01:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07][root][INFO] - Training Epoch: 5/10, step 449/574 completed (loss: 0.14293000102043152, acc: 0.9552238583564758)
[2025-01-06 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08][root][INFO] - Training Epoch: 5/10, step 450/574 completed (loss: 0.03458733856678009, acc: 1.0)
[2025-01-06 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08][root][INFO] - Training Epoch: 5/10, step 451/574 completed (loss: 0.14014242589473724, acc: 0.95652174949646)
[2025-01-06 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08][root][INFO] - Training Epoch: 5/10, step 452/574 completed (loss: 0.10566980391740799, acc: 0.9743589758872986)
[2025-01-06 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09][root][INFO] - Training Epoch: 5/10, step 453/574 completed (loss: 0.1797410100698471, acc: 0.9605262875556946)
[2025-01-06 01:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09][root][INFO] - Training Epoch: 5/10, step 454/574 completed (loss: 0.08395107090473175, acc: 0.9795918464660645)
[2025-01-06 01:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09][root][INFO] - Training Epoch: 5/10, step 455/574 completed (loss: 0.03396297246217728, acc: 1.0)
[2025-01-06 01:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10][root][INFO] - Training Epoch: 5/10, step 456/574 completed (loss: 0.29229748249053955, acc: 0.9278350472450256)
[2025-01-06 01:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10][root][INFO] - Training Epoch: 5/10, step 457/574 completed (loss: 0.02803819440305233, acc: 1.0)
[2025-01-06 01:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10][root][INFO] - Training Epoch: 5/10, step 458/574 completed (loss: 0.3107026517391205, acc: 0.9069767594337463)
[2025-01-06 01:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11][root][INFO] - Training Epoch: 5/10, step 459/574 completed (loss: 0.15906789898872375, acc: 0.9285714030265808)
[2025-01-06 01:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11][root][INFO] - Training Epoch: 5/10, step 460/574 completed (loss: 0.08479038625955582, acc: 0.9629629850387573)
[2025-01-06 01:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11][root][INFO] - Training Epoch: 5/10, step 461/574 completed (loss: 0.05863267183303833, acc: 0.9722222089767456)
[2025-01-06 01:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12][root][INFO] - Training Epoch: 5/10, step 462/574 completed (loss: 0.1249641552567482, acc: 0.96875)
[2025-01-06 01:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12][root][INFO] - Training Epoch: 5/10, step 463/574 completed (loss: 0.16094334423542023, acc: 0.9615384340286255)
[2025-01-06 01:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12][root][INFO] - Training Epoch: 5/10, step 464/574 completed (loss: 0.014277730137109756, acc: 1.0)
[2025-01-06 01:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13][root][INFO] - Training Epoch: 5/10, step 465/574 completed (loss: 0.09794668853282928, acc: 0.9642857313156128)
[2025-01-06 01:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13][root][INFO] - Training Epoch: 5/10, step 466/574 completed (loss: 0.25670114159584045, acc: 0.891566276550293)
[2025-01-06 01:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13][root][INFO] - Training Epoch: 5/10, step 467/574 completed (loss: 0.21240496635437012, acc: 0.9369369149208069)
[2025-01-06 01:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14][root][INFO] - Training Epoch: 5/10, step 468/574 completed (loss: 0.22396527230739594, acc: 0.9223300814628601)
[2025-01-06 01:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14][root][INFO] - Training Epoch: 5/10, step 469/574 completed (loss: 0.26671740412712097, acc: 0.9105691313743591)
[2025-01-06 01:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14][root][INFO] - Training Epoch: 5/10, step 470/574 completed (loss: 0.018523069098591805, acc: 1.0)
[2025-01-06 01:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14][root][INFO] - Training Epoch: 5/10, step 471/574 completed (loss: 0.05224587395787239, acc: 0.9642857313156128)
[2025-01-06 01:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15][root][INFO] - Training Epoch: 5/10, step 472/574 completed (loss: 0.18687044084072113, acc: 0.9313725233078003)
[2025-01-06 01:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15][root][INFO] - Training Epoch: 5/10, step 473/574 completed (loss: 0.5569639205932617, acc: 0.8122270703315735)
[2025-01-06 01:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16][root][INFO] - Training Epoch: 5/10, step 474/574 completed (loss: 0.20565026998519897, acc: 0.9583333134651184)
[2025-01-06 01:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16][root][INFO] - Training Epoch: 5/10, step 475/574 completed (loss: 0.1756172925233841, acc: 0.9325153231620789)
[2025-01-06 01:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16][root][INFO] - Training Epoch: 5/10, step 476/574 completed (loss: 0.219427689909935, acc: 0.9136690497398376)
[2025-01-06 01:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16][root][INFO] - Training Epoch: 5/10, step 477/574 completed (loss: 0.3836537003517151, acc: 0.8944723606109619)
[2025-01-06 01:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17][root][INFO] - Training Epoch: 5/10, step 478/574 completed (loss: 0.21776826679706573, acc: 0.9444444179534912)
[2025-01-06 01:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17][root][INFO] - Training Epoch: 5/10, step 479/574 completed (loss: 0.054890722036361694, acc: 1.0)
[2025-01-06 01:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17][root][INFO] - Training Epoch: 5/10, step 480/574 completed (loss: 0.055706318467855453, acc: 0.9629629850387573)
[2025-01-06 01:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18][root][INFO] - Training Epoch: 5/10, step 481/574 completed (loss: 0.021400583907961845, acc: 1.0)
[2025-01-06 01:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18][root][INFO] - Training Epoch: 5/10, step 482/574 completed (loss: 0.11448393017053604, acc: 1.0)
[2025-01-06 01:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18][root][INFO] - Training Epoch: 5/10, step 483/574 completed (loss: 0.3671496510505676, acc: 0.931034505367279)
[2025-01-06 01:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19][root][INFO] - Training Epoch: 5/10, step 484/574 completed (loss: 0.15750494599342346, acc: 0.9354838728904724)
[2025-01-06 01:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19][root][INFO] - Training Epoch: 5/10, step 485/574 completed (loss: 0.04855506122112274, acc: 1.0)
[2025-01-06 01:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19][root][INFO] - Training Epoch: 5/10, step 486/574 completed (loss: 0.08368300646543503, acc: 1.0)
[2025-01-06 01:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:20][root][INFO] - Training Epoch: 5/10, step 487/574 completed (loss: 0.11565173417329788, acc: 0.9523809552192688)
[2025-01-06 01:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:20][root][INFO] - Training Epoch: 5/10, step 488/574 completed (loss: 0.02106049656867981, acc: 1.0)
[2025-01-06 01:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:20][root][INFO] - Training Epoch: 5/10, step 489/574 completed (loss: 0.4538688361644745, acc: 0.800000011920929)
[2025-01-06 01:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21][root][INFO] - Training Epoch: 5/10, step 490/574 completed (loss: 0.13854964077472687, acc: 0.9666666388511658)
[2025-01-06 01:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21][root][INFO] - Training Epoch: 5/10, step 491/574 completed (loss: 0.031529948115348816, acc: 1.0)
[2025-01-06 01:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21][root][INFO] - Training Epoch: 5/10, step 492/574 completed (loss: 0.16328838467597961, acc: 0.9607843160629272)
[2025-01-06 01:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21][root][INFO] - Training Epoch: 5/10, step 493/574 completed (loss: 0.09819270670413971, acc: 0.9655172228813171)
[2025-01-06 01:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22][root][INFO] - Training Epoch: 5/10, step 494/574 completed (loss: 0.04672465845942497, acc: 1.0)
[2025-01-06 01:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22][root][INFO] - Training Epoch: 5/10, step 495/574 completed (loss: 0.046063680201768875, acc: 1.0)
[2025-01-06 01:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22][root][INFO] - Training Epoch: 5/10, step 496/574 completed (loss: 0.4715263843536377, acc: 0.8839285969734192)
[2025-01-06 01:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23][root][INFO] - Training Epoch: 5/10, step 497/574 completed (loss: 0.17339536547660828, acc: 0.932584285736084)
[2025-01-06 01:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23][root][INFO] - Training Epoch: 5/10, step 498/574 completed (loss: 0.36544230580329895, acc: 0.8764045238494873)
[2025-01-06 01:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23][root][INFO] - Training Epoch: 5/10, step 499/574 completed (loss: 0.8275190591812134, acc: 0.7517730593681335)
[2025-01-06 01:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24][root][INFO] - Training Epoch: 5/10, step 500/574 completed (loss: 0.4007751941680908, acc: 0.8913043737411499)
[2025-01-06 01:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24][root][INFO] - Training Epoch: 5/10, step 501/574 completed (loss: 0.0318145789206028, acc: 1.0)
[2025-01-06 01:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24][root][INFO] - Training Epoch: 5/10, step 502/574 completed (loss: 0.041463810950517654, acc: 0.9615384340286255)
[2025-01-06 01:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25][root][INFO] - Training Epoch: 5/10, step 503/574 completed (loss: 0.032365262508392334, acc: 1.0)
[2025-01-06 01:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25][root][INFO] - Training Epoch: 5/10, step 504/574 completed (loss: 0.006329520605504513, acc: 1.0)
[2025-01-06 01:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25][root][INFO] - Training Epoch: 5/10, step 505/574 completed (loss: 0.24671514332294464, acc: 0.9245283007621765)
[2025-01-06 01:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:26][root][INFO] - Training Epoch: 5/10, step 506/574 completed (loss: 0.2117394506931305, acc: 0.931034505367279)
[2025-01-06 01:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:26][root][INFO] - Training Epoch: 5/10, step 507/574 completed (loss: 0.6604424715042114, acc: 0.7747747898101807)
[2025-01-06 01:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27][root][INFO] - Training Epoch: 5/10, step 508/574 completed (loss: 0.3685126006603241, acc: 0.9154929518699646)
[2025-01-06 01:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27][root][INFO] - Training Epoch: 5/10, step 509/574 completed (loss: 0.018906155601143837, acc: 1.0)
[2025-01-06 01:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27][root][INFO] - Training Epoch: 5/10, step 510/574 completed (loss: 0.00624294625595212, acc: 1.0)
[2025-01-06 01:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:28][root][INFO] - Training Epoch: 5/10, step 511/574 completed (loss: 0.007167731877416372, acc: 1.0)
[2025-01-06 01:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:30][root][INFO] - Training Epoch: 5/10, step 512/574 completed (loss: 0.7987813949584961, acc: 0.7642857432365417)
[2025-01-06 01:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31][root][INFO] - Training Epoch: 5/10, step 513/574 completed (loss: 0.08580043911933899, acc: 0.9682539701461792)
[2025-01-06 01:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31][root][INFO] - Training Epoch: 5/10, step 514/574 completed (loss: 0.025043707340955734, acc: 1.0)
[2025-01-06 01:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31][root][INFO] - Training Epoch: 5/10, step 515/574 completed (loss: 0.008436575531959534, acc: 1.0)
[2025-01-06 01:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32][root][INFO] - Training Epoch: 5/10, step 516/574 completed (loss: 0.3482547402381897, acc: 0.8888888955116272)
[2025-01-06 01:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32][root][INFO] - Training Epoch: 5/10, step 517/574 completed (loss: 0.001753759104758501, acc: 1.0)
[2025-01-06 01:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33][root][INFO] - Training Epoch: 5/10, step 518/574 completed (loss: 0.02795582078397274, acc: 1.0)
[2025-01-06 01:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33][root][INFO] - Training Epoch: 5/10, step 519/574 completed (loss: 0.05600149184465408, acc: 1.0)
[2025-01-06 01:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33][root][INFO] - Training Epoch: 5/10, step 520/574 completed (loss: 0.1651598960161209, acc: 0.8888888955116272)
[2025-01-06 01:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34][root][INFO] - Training Epoch: 5/10, step 521/574 completed (loss: 0.4728981554508209, acc: 0.8644067645072937)
[2025-01-06 01:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34][root][INFO] - Training Epoch: 5/10, step 522/574 completed (loss: 0.17295943200588226, acc: 0.9328358173370361)
[2025-01-06 01:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35][root][INFO] - Training Epoch: 5/10, step 523/574 completed (loss: 0.22159600257873535, acc: 0.9416058659553528)
[2025-01-06 01:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35][root][INFO] - Training Epoch: 5/10, step 524/574 completed (loss: 0.45327651500701904, acc: 0.8899999856948853)
[2025-01-06 01:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36][root][INFO] - Training Epoch: 5/10, step 525/574 completed (loss: 0.011081267148256302, acc: 1.0)
[2025-01-06 01:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36][root][INFO] - Training Epoch: 5/10, step 526/574 completed (loss: 0.1757577508687973, acc: 0.942307710647583)
[2025-01-06 01:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36][root][INFO] - Training Epoch: 5/10, step 527/574 completed (loss: 0.4855863153934479, acc: 0.8571428656578064)
[2025-01-06 01:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37][root][INFO] - Training Epoch: 5/10, step 528/574 completed (loss: 0.47921648621559143, acc: 0.8196721076965332)
[2025-01-06 01:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37][root][INFO] - Training Epoch: 5/10, step 529/574 completed (loss: 0.05946027487516403, acc: 0.9830508232116699)
[2025-01-06 01:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37][root][INFO] - Training Epoch: 5/10, step 530/574 completed (loss: 0.3312564194202423, acc: 0.8604651093482971)
[2025-01-06 01:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37][root][INFO] - Training Epoch: 5/10, step 531/574 completed (loss: 0.12837056815624237, acc: 0.9318181872367859)
[2025-01-06 01:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38][root][INFO] - Training Epoch: 5/10, step 532/574 completed (loss: 0.2443954348564148, acc: 0.9056603908538818)
[2025-01-06 01:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38][root][INFO] - Training Epoch: 5/10, step 533/574 completed (loss: 0.23335400223731995, acc: 0.9318181872367859)
[2025-01-06 01:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38][root][INFO] - Training Epoch: 5/10, step 534/574 completed (loss: 0.05668112635612488, acc: 1.0)
[2025-01-06 01:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39][root][INFO] - Training Epoch: 5/10, step 535/574 completed (loss: 0.0570184662938118, acc: 1.0)
[2025-01-06 01:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39][root][INFO] - Training Epoch: 5/10, step 536/574 completed (loss: 0.05597896873950958, acc: 0.9545454382896423)
[2025-01-06 01:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39][root][INFO] - Training Epoch: 5/10, step 537/574 completed (loss: 0.2610642910003662, acc: 0.9230769276618958)
[2025-01-06 01:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40][root][INFO] - Training Epoch: 5/10, step 538/574 completed (loss: 0.29422658681869507, acc: 0.921875)
[2025-01-06 01:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40][root][INFO] - Training Epoch: 5/10, step 539/574 completed (loss: 0.18852569162845612, acc: 0.90625)
[2025-01-06 01:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40][root][INFO] - Training Epoch: 5/10, step 540/574 completed (loss: 0.08925161510705948, acc: 1.0)
[2025-01-06 01:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41][root][INFO] - Training Epoch: 5/10, step 541/574 completed (loss: 0.09762930870056152, acc: 0.9375)
[2025-01-06 01:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41][root][INFO] - Training Epoch: 5/10, step 542/574 completed (loss: 0.002790506463497877, acc: 1.0)
[2025-01-06 01:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41][root][INFO] - Training Epoch: 5/10, step 543/574 completed (loss: 0.00041984787094406784, acc: 1.0)
[2025-01-06 01:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42][root][INFO] - Training Epoch: 5/10, step 544/574 completed (loss: 0.0213253702968359, acc: 1.0)
[2025-01-06 01:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42][root][INFO] - Training Epoch: 5/10, step 545/574 completed (loss: 0.015063999220728874, acc: 1.0)
[2025-01-06 01:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42][root][INFO] - Training Epoch: 5/10, step 546/574 completed (loss: 0.0033401392865926027, acc: 1.0)
[2025-01-06 01:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43][root][INFO] - Training Epoch: 5/10, step 547/574 completed (loss: 0.00243164598941803, acc: 1.0)
[2025-01-06 01:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43][root][INFO] - Training Epoch: 5/10, step 548/574 completed (loss: 0.008094551041722298, acc: 1.0)
[2025-01-06 01:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43][root][INFO] - Training Epoch: 5/10, step 549/574 completed (loss: 0.0011622058227658272, acc: 1.0)
[2025-01-06 01:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43][root][INFO] - Training Epoch: 5/10, step 550/574 completed (loss: 0.13783207535743713, acc: 0.9090909361839294)
[2025-01-06 01:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44][root][INFO] - Training Epoch: 5/10, step 551/574 completed (loss: 0.0215439572930336, acc: 1.0)
[2025-01-06 01:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44][root][INFO] - Training Epoch: 5/10, step 552/574 completed (loss: 0.11768998950719833, acc: 0.9571428298950195)
[2025-01-06 01:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44][root][INFO] - Training Epoch: 5/10, step 553/574 completed (loss: 0.2207862138748169, acc: 0.9270073175430298)
[2025-01-06 01:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45][root][INFO] - Training Epoch: 5/10, step 554/574 completed (loss: 0.1412343531847, acc: 0.951724112033844)
[2025-01-06 01:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45][root][INFO] - Training Epoch: 5/10, step 555/574 completed (loss: 0.18571823835372925, acc: 0.9642857313156128)
[2025-01-06 01:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45][root][INFO] - Training Epoch: 5/10, step 556/574 completed (loss: 0.3549996614456177, acc: 0.9139072895050049)
[2025-01-06 01:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:46][root][INFO] - Training Epoch: 5/10, step 557/574 completed (loss: 0.03925211727619171, acc: 0.9914529919624329)
[2025-01-06 01:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:46][root][INFO] - Training Epoch: 5/10, step 558/574 completed (loss: 0.1896653026342392, acc: 0.9200000166893005)
[2025-01-06 01:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:46][root][INFO] - Training Epoch: 5/10, step 559/574 completed (loss: 0.018321586772799492, acc: 1.0)
[2025-01-06 01:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:47][root][INFO] - Training Epoch: 5/10, step 560/574 completed (loss: 0.0023829187266528606, acc: 1.0)
[2025-01-06 01:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:47][root][INFO] - Training Epoch: 5/10, step 561/574 completed (loss: 0.034811411052942276, acc: 0.9743589758872986)
[2025-01-06 01:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:47][root][INFO] - Training Epoch: 5/10, step 562/574 completed (loss: 0.1748993843793869, acc: 0.9444444179534912)
[2025-01-06 01:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:48][root][INFO] - Training Epoch: 5/10, step 563/574 completed (loss: 0.19431708753108978, acc: 0.9610389471054077)
[2025-01-06 01:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2952, device='cuda:0') eval_epoch_loss=tensor(0.8308, device='cuda:0') eval_epoch_acc=tensor(0.8188, device='cuda:0')
[2025-01-06 01:29:16][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:29:16][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:29:16][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_564_loss_0.8308197259902954/model.pt
[2025-01-06 01:29:16][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16][root][INFO] - Training Epoch: 5/10, step 564/574 completed (loss: 0.06216047331690788, acc: 0.9791666865348816)
[2025-01-06 01:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17][root][INFO] - Training Epoch: 5/10, step 565/574 completed (loss: 0.04380826652050018, acc: 1.0)
[2025-01-06 01:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17][root][INFO] - Training Epoch: 5/10, step 566/574 completed (loss: 0.11544660478830338, acc: 0.9642857313156128)
[2025-01-06 01:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17][root][INFO] - Training Epoch: 5/10, step 567/574 completed (loss: 0.0118095763027668, acc: 1.0)
[2025-01-06 01:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18][root][INFO] - Training Epoch: 5/10, step 568/574 completed (loss: 0.010140984319150448, acc: 1.0)
[2025-01-06 01:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18][root][INFO] - Training Epoch: 5/10, step 569/574 completed (loss: 0.11945288628339767, acc: 0.9679144620895386)
[2025-01-06 01:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19][root][INFO] - Training Epoch: 5/10, step 570/574 completed (loss: 0.012370442971587181, acc: 1.0)
[2025-01-06 01:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19][root][INFO] - Training Epoch: 5/10, step 571/574 completed (loss: 0.08038023859262466, acc: 0.9743589758872986)
[2025-01-06 01:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19][root][INFO] - Training Epoch: 5/10, step 572/574 completed (loss: 0.2993830144405365, acc: 0.9081632494926453)
[2025-01-06 01:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:20][root][INFO] - Training Epoch: 5/10, step 573/574 completed (loss: 0.14194442331790924, acc: 0.9433962106704712)
[2025-01-06 01:29:20][slam_llm.utils.train_utils][INFO] - Epoch 5: train_perplexity=1.2426, train_epoch_loss=0.2172, epoch time 329.9822268038988s
[2025-01-06 01:29:20][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:29:20][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:29:20][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:29:20][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 14
[2025-01-06 01:29:20][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21][root][INFO] - Training Epoch: 6/10, step 0/574 completed (loss: 0.21888138353824615, acc: 0.9259259104728699)
[2025-01-06 01:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21][root][INFO] - Training Epoch: 6/10, step 1/574 completed (loss: 0.05514974892139435, acc: 0.9599999785423279)
[2025-01-06 01:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22][root][INFO] - Training Epoch: 6/10, step 2/574 completed (loss: 0.4016215205192566, acc: 0.837837815284729)
[2025-01-06 01:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22][root][INFO] - Training Epoch: 6/10, step 3/574 completed (loss: 0.015138003043830395, acc: 1.0)
[2025-01-06 01:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22][root][INFO] - Training Epoch: 6/10, step 4/574 completed (loss: 0.04636096581816673, acc: 0.9729729890823364)
[2025-01-06 01:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23][root][INFO] - Training Epoch: 6/10, step 5/574 completed (loss: 0.013430655933916569, acc: 1.0)
[2025-01-06 01:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23][root][INFO] - Training Epoch: 6/10, step 6/574 completed (loss: 0.1335483342409134, acc: 0.9795918464660645)
[2025-01-06 01:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23][root][INFO] - Training Epoch: 6/10, step 7/574 completed (loss: 0.0330473817884922, acc: 1.0)
[2025-01-06 01:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23][root][INFO] - Training Epoch: 6/10, step 8/574 completed (loss: 0.02420175075531006, acc: 1.0)
[2025-01-06 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24][root][INFO] - Training Epoch: 6/10, step 9/574 completed (loss: 0.002954151015728712, acc: 1.0)
[2025-01-06 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24][root][INFO] - Training Epoch: 6/10, step 10/574 completed (loss: 0.0028377408161759377, acc: 1.0)
[2025-01-06 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24][root][INFO] - Training Epoch: 6/10, step 11/574 completed (loss: 0.07534310966730118, acc: 0.9743589758872986)
[2025-01-06 01:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25][root][INFO] - Training Epoch: 6/10, step 12/574 completed (loss: 0.025261929258704185, acc: 1.0)
[2025-01-06 01:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25][root][INFO] - Training Epoch: 6/10, step 13/574 completed (loss: 0.09594035148620605, acc: 0.95652174949646)
[2025-01-06 01:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25][root][INFO] - Training Epoch: 6/10, step 14/574 completed (loss: 0.01945626735687256, acc: 1.0)
[2025-01-06 01:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26][root][INFO] - Training Epoch: 6/10, step 15/574 completed (loss: 0.07145198434591293, acc: 0.9795918464660645)
[2025-01-06 01:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26][root][INFO] - Training Epoch: 6/10, step 16/574 completed (loss: 0.26921346783638, acc: 0.9473684430122375)
[2025-01-06 01:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26][root][INFO] - Training Epoch: 6/10, step 17/574 completed (loss: 0.07026174664497375, acc: 0.9583333134651184)
[2025-01-06 01:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27][root][INFO] - Training Epoch: 6/10, step 18/574 completed (loss: 0.13134869933128357, acc: 0.9444444179534912)
[2025-01-06 01:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27][root][INFO] - Training Epoch: 6/10, step 19/574 completed (loss: 0.3274020254611969, acc: 0.9473684430122375)
[2025-01-06 01:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27][root][INFO] - Training Epoch: 6/10, step 20/574 completed (loss: 0.020866453647613525, acc: 1.0)
[2025-01-06 01:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28][root][INFO] - Training Epoch: 6/10, step 21/574 completed (loss: 0.008114096708595753, acc: 1.0)
[2025-01-06 01:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28][root][INFO] - Training Epoch: 6/10, step 22/574 completed (loss: 0.39773234724998474, acc: 0.9200000166893005)
[2025-01-06 01:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28][root][INFO] - Training Epoch: 6/10, step 23/574 completed (loss: 0.28470227122306824, acc: 0.9047619104385376)
[2025-01-06 01:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29][root][INFO] - Training Epoch: 6/10, step 24/574 completed (loss: 0.0035589663311839104, acc: 1.0)
[2025-01-06 01:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29][root][INFO] - Training Epoch: 6/10, step 25/574 completed (loss: 0.05961878225207329, acc: 1.0)
[2025-01-06 01:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29][root][INFO] - Training Epoch: 6/10, step 26/574 completed (loss: 0.1453183889389038, acc: 0.9863013625144958)
[2025-01-06 01:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31][root][INFO] - Training Epoch: 6/10, step 27/574 completed (loss: 0.5264865159988403, acc: 0.8695651888847351)
[2025-01-06 01:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31][root][INFO] - Training Epoch: 6/10, step 28/574 completed (loss: 0.14593735337257385, acc: 0.930232584476471)
[2025-01-06 01:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31][root][INFO] - Training Epoch: 6/10, step 29/574 completed (loss: 0.19018776714801788, acc: 0.9397590160369873)
[2025-01-06 01:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32][root][INFO] - Training Epoch: 6/10, step 30/574 completed (loss: 0.15909940004348755, acc: 0.9259259104728699)
[2025-01-06 01:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32][root][INFO] - Training Epoch: 6/10, step 31/574 completed (loss: 0.18064646422863007, acc: 0.9642857313156128)
[2025-01-06 01:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32][root][INFO] - Training Epoch: 6/10, step 32/574 completed (loss: 0.07108241319656372, acc: 0.9629629850387573)
[2025-01-06 01:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33][root][INFO] - Training Epoch: 6/10, step 33/574 completed (loss: 0.11650685966014862, acc: 0.95652174949646)
[2025-01-06 01:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33][root][INFO] - Training Epoch: 6/10, step 34/574 completed (loss: 0.20998062193393707, acc: 0.9327731132507324)
[2025-01-06 01:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33][root][INFO] - Training Epoch: 6/10, step 35/574 completed (loss: 0.21726830303668976, acc: 0.9180327653884888)
[2025-01-06 01:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34][root][INFO] - Training Epoch: 6/10, step 36/574 completed (loss: 0.2172539085149765, acc: 0.9523809552192688)
[2025-01-06 01:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34][root][INFO] - Training Epoch: 6/10, step 37/574 completed (loss: 0.09994519501924515, acc: 0.9661017060279846)
[2025-01-06 01:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35][root][INFO] - Training Epoch: 6/10, step 38/574 completed (loss: 0.14160001277923584, acc: 0.9655172228813171)
[2025-01-06 01:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35][root][INFO] - Training Epoch: 6/10, step 39/574 completed (loss: 0.05276970565319061, acc: 1.0)
[2025-01-06 01:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35][root][INFO] - Training Epoch: 6/10, step 40/574 completed (loss: 0.1278104931116104, acc: 0.9615384340286255)
[2025-01-06 01:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36][root][INFO] - Training Epoch: 6/10, step 41/574 completed (loss: 0.19628725945949554, acc: 0.9594594836235046)
[2025-01-06 01:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36][root][INFO] - Training Epoch: 6/10, step 42/574 completed (loss: 0.2649209201335907, acc: 0.9230769276618958)
[2025-01-06 01:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36][root][INFO] - Training Epoch: 6/10, step 43/574 completed (loss: 0.24633866548538208, acc: 0.9191918969154358)
[2025-01-06 01:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37][root][INFO] - Training Epoch: 6/10, step 44/574 completed (loss: 0.15117236971855164, acc: 0.938144326210022)
[2025-01-06 01:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37][root][INFO] - Training Epoch: 6/10, step 45/574 completed (loss: 0.17515113949775696, acc: 0.9411764740943909)
[2025-01-06 01:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38][root][INFO] - Training Epoch: 6/10, step 46/574 completed (loss: 0.015453724190592766, acc: 1.0)
[2025-01-06 01:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38][root][INFO] - Training Epoch: 6/10, step 47/574 completed (loss: 0.004372906405478716, acc: 1.0)
[2025-01-06 01:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38][root][INFO] - Training Epoch: 6/10, step 48/574 completed (loss: 0.056901317089796066, acc: 1.0)
[2025-01-06 01:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38][root][INFO] - Training Epoch: 6/10, step 49/574 completed (loss: 0.005166663788259029, acc: 1.0)
[2025-01-06 01:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39][root][INFO] - Training Epoch: 6/10, step 50/574 completed (loss: 0.2564646601676941, acc: 0.9122806787490845)
[2025-01-06 01:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39][root][INFO] - Training Epoch: 6/10, step 51/574 completed (loss: 0.12657281756401062, acc: 0.9682539701461792)
[2025-01-06 01:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39][root][INFO] - Training Epoch: 6/10, step 52/574 completed (loss: 0.1712491363286972, acc: 0.9436619877815247)
[2025-01-06 01:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40][root][INFO] - Training Epoch: 6/10, step 53/574 completed (loss: 0.8467043042182922, acc: 0.7599999904632568)
[2025-01-06 01:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40][root][INFO] - Training Epoch: 6/10, step 54/574 completed (loss: 0.12734416127204895, acc: 0.9729729890823364)
[2025-01-06 01:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:41][root][INFO] - Training Epoch: 6/10, step 55/574 completed (loss: 0.031512729823589325, acc: 1.0)
[2025-01-06 01:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:44][root][INFO] - Training Epoch: 6/10, step 56/574 completed (loss: 0.7419049739837646, acc: 0.7747440338134766)
[2025-01-06 01:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45][root][INFO] - Training Epoch: 6/10, step 57/574 completed (loss: 0.9413402676582336, acc: 0.7298474907875061)
[2025-01-06 01:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45][root][INFO] - Training Epoch: 6/10, step 58/574 completed (loss: 0.5607545375823975, acc: 0.8181818127632141)
[2025-01-06 01:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:46][root][INFO] - Training Epoch: 6/10, step 59/574 completed (loss: 0.19300802052021027, acc: 0.9191176295280457)
[2025-01-06 01:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47][root][INFO] - Training Epoch: 6/10, step 60/574 completed (loss: 0.5093427896499634, acc: 0.8333333134651184)
[2025-01-06 01:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47][root][INFO] - Training Epoch: 6/10, step 61/574 completed (loss: 0.29592952132225037, acc: 0.8999999761581421)
[2025-01-06 01:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47][root][INFO] - Training Epoch: 6/10, step 62/574 completed (loss: 0.07938626408576965, acc: 0.970588207244873)
[2025-01-06 01:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48][root][INFO] - Training Epoch: 6/10, step 63/574 completed (loss: 0.12015276402235031, acc: 0.9444444179534912)
[2025-01-06 01:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48][root][INFO] - Training Epoch: 6/10, step 64/574 completed (loss: 0.05090772733092308, acc: 0.984375)
[2025-01-06 01:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48][root][INFO] - Training Epoch: 6/10, step 65/574 completed (loss: 0.012652497738599777, acc: 1.0)
[2025-01-06 01:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49][root][INFO] - Training Epoch: 6/10, step 66/574 completed (loss: 0.25892534852027893, acc: 0.9285714030265808)
[2025-01-06 01:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49][root][INFO] - Training Epoch: 6/10, step 67/574 completed (loss: 0.15808187425136566, acc: 0.9666666388511658)
[2025-01-06 01:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49][root][INFO] - Training Epoch: 6/10, step 68/574 completed (loss: 0.001129309879615903, acc: 1.0)
[2025-01-06 01:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50][root][INFO] - Training Epoch: 6/10, step 69/574 completed (loss: 0.050104543566703796, acc: 0.9722222089767456)
[2025-01-06 01:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50][root][INFO] - Training Epoch: 6/10, step 70/574 completed (loss: 0.22325457632541656, acc: 0.939393937587738)
[2025-01-06 01:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50][root][INFO] - Training Epoch: 6/10, step 71/574 completed (loss: 0.4879559278488159, acc: 0.8676470518112183)
[2025-01-06 01:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51][root][INFO] - Training Epoch: 6/10, step 72/574 completed (loss: 0.4293658435344696, acc: 0.8650793433189392)
[2025-01-06 01:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51][root][INFO] - Training Epoch: 6/10, step 73/574 completed (loss: 0.8886668682098389, acc: 0.7538461685180664)
[2025-01-06 01:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51][root][INFO] - Training Epoch: 6/10, step 74/574 completed (loss: 0.4135187864303589, acc: 0.9285714030265808)
[2025-01-06 01:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51][root][INFO] - Training Epoch: 6/10, step 75/574 completed (loss: 0.6652464270591736, acc: 0.7910447716712952)
[2025-01-06 01:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52][root][INFO] - Training Epoch: 6/10, step 76/574 completed (loss: 1.124248743057251, acc: 0.6788321137428284)
[2025-01-06 01:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52][root][INFO] - Training Epoch: 6/10, step 77/574 completed (loss: 0.03660374879837036, acc: 1.0)
[2025-01-06 01:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:53][root][INFO] - Training Epoch: 6/10, step 78/574 completed (loss: 0.06017543748021126, acc: 0.9583333134651184)
[2025-01-06 01:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:53][root][INFO] - Training Epoch: 6/10, step 79/574 completed (loss: 0.016361601650714874, acc: 1.0)
[2025-01-06 01:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:53][root][INFO] - Training Epoch: 6/10, step 80/574 completed (loss: 0.13275542855262756, acc: 0.9615384340286255)
[2025-01-06 01:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54][root][INFO] - Training Epoch: 6/10, step 81/574 completed (loss: 0.1366400420665741, acc: 0.9615384340286255)
[2025-01-06 01:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54][root][INFO] - Training Epoch: 6/10, step 82/574 completed (loss: 0.09327682852745056, acc: 0.9807692170143127)
[2025-01-06 01:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54][root][INFO] - Training Epoch: 6/10, step 83/574 completed (loss: 0.08058641850948334, acc: 0.96875)
[2025-01-06 01:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55][root][INFO] - Training Epoch: 6/10, step 84/574 completed (loss: 0.08864345401525497, acc: 1.0)
[2025-01-06 01:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55][root][INFO] - Training Epoch: 6/10, step 85/574 completed (loss: 0.13095945119857788, acc: 0.9800000190734863)
[2025-01-06 01:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55][root][INFO] - Training Epoch: 6/10, step 86/574 completed (loss: 0.07905779033899307, acc: 0.95652174949646)
[2025-01-06 01:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56][root][INFO] - Training Epoch: 6/10, step 87/574 completed (loss: 0.29912447929382324, acc: 0.8999999761581421)
[2025-01-06 01:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56][root][INFO] - Training Epoch: 6/10, step 88/574 completed (loss: 0.4930056929588318, acc: 0.8737863898277283)
[2025-01-06 01:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:57][root][INFO] - Training Epoch: 6/10, step 89/574 completed (loss: 0.6258963346481323, acc: 0.8349514603614807)
[2025-01-06 01:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:58][root][INFO] - Training Epoch: 6/10, step 90/574 completed (loss: 0.7072142958641052, acc: 0.7903226017951965)
[2025-01-06 01:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:59][root][INFO] - Training Epoch: 6/10, step 91/574 completed (loss: 0.5805702209472656, acc: 0.8318965435028076)
[2025-01-06 01:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:00][root][INFO] - Training Epoch: 6/10, step 92/574 completed (loss: 0.3667292594909668, acc: 0.8947368264198303)
[2025-01-06 01:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01][root][INFO] - Training Epoch: 6/10, step 93/574 completed (loss: 0.6730954051017761, acc: 0.7821782231330872)
[2025-01-06 01:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01][root][INFO] - Training Epoch: 6/10, step 94/574 completed (loss: 0.3822387158870697, acc: 0.9032257795333862)
[2025-01-06 01:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01][root][INFO] - Training Epoch: 6/10, step 95/574 completed (loss: 0.1643986701965332, acc: 0.9710144996643066)
[2025-01-06 01:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01][root][INFO] - Training Epoch: 6/10, step 96/574 completed (loss: 0.45165979862213135, acc: 0.848739504814148)
[2025-01-06 01:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02][root][INFO] - Training Epoch: 6/10, step 97/574 completed (loss: 0.41604140400886536, acc: 0.8653846383094788)
[2025-01-06 01:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02][root][INFO] - Training Epoch: 6/10, step 98/574 completed (loss: 0.561677873134613, acc: 0.8175182342529297)
[2025-01-06 01:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02][root][INFO] - Training Epoch: 6/10, step 99/574 completed (loss: 0.45078936219215393, acc: 0.8805969953536987)
[2025-01-06 01:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03][root][INFO] - Training Epoch: 6/10, step 100/574 completed (loss: 0.09345052391290665, acc: 0.949999988079071)
[2025-01-06 01:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03][root][INFO] - Training Epoch: 6/10, step 101/574 completed (loss: 0.005691051483154297, acc: 1.0)
[2025-01-06 01:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03][root][INFO] - Training Epoch: 6/10, step 102/574 completed (loss: 0.025147324427962303, acc: 1.0)
[2025-01-06 01:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04][root][INFO] - Training Epoch: 6/10, step 103/574 completed (loss: 0.023996680974960327, acc: 0.9772727489471436)
[2025-01-06 01:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04][root][INFO] - Training Epoch: 6/10, step 104/574 completed (loss: 0.10244644433259964, acc: 0.9655172228813171)
[2025-01-06 01:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04][root][INFO] - Training Epoch: 6/10, step 105/574 completed (loss: 0.013453254476189613, acc: 1.0)
[2025-01-06 01:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:05][root][INFO] - Training Epoch: 6/10, step 106/574 completed (loss: 0.07364095747470856, acc: 0.9599999785423279)
[2025-01-06 01:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:05][root][INFO] - Training Epoch: 6/10, step 107/574 completed (loss: 0.001755430013872683, acc: 1.0)
[2025-01-06 01:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:05][root][INFO] - Training Epoch: 6/10, step 108/574 completed (loss: 0.008136148564517498, acc: 1.0)
[2025-01-06 01:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:06][root][INFO] - Training Epoch: 6/10, step 109/574 completed (loss: 0.2572301924228668, acc: 0.976190447807312)
[2025-01-06 01:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:06][root][INFO] - Training Epoch: 6/10, step 110/574 completed (loss: 0.032333455979824066, acc: 1.0)
[2025-01-06 01:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:06][root][INFO] - Training Epoch: 6/10, step 111/574 completed (loss: 0.06297712028026581, acc: 0.9824561476707458)
[2025-01-06 01:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:07][root][INFO] - Training Epoch: 6/10, step 112/574 completed (loss: 0.17518103122711182, acc: 0.9122806787490845)
[2025-01-06 01:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:07][root][INFO] - Training Epoch: 6/10, step 113/574 completed (loss: 0.09128843247890472, acc: 0.9743589758872986)
[2025-01-06 01:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:08][root][INFO] - Training Epoch: 6/10, step 114/574 completed (loss: 0.04069137945771217, acc: 1.0)
[2025-01-06 01:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:08][root][INFO] - Training Epoch: 6/10, step 115/574 completed (loss: 0.006253094878047705, acc: 1.0)
[2025-01-06 01:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:08][root][INFO] - Training Epoch: 6/10, step 116/574 completed (loss: 0.11840491741895676, acc: 0.9523809552192688)
[2025-01-06 01:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:09][root][INFO] - Training Epoch: 6/10, step 117/574 completed (loss: 0.26047417521476746, acc: 0.9105691313743591)
[2025-01-06 01:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:09][root][INFO] - Training Epoch: 6/10, step 118/574 completed (loss: 0.09132316708564758, acc: 0.9677419066429138)
[2025-01-06 01:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10][root][INFO] - Training Epoch: 6/10, step 119/574 completed (loss: 0.3970623314380646, acc: 0.9049429893493652)
[2025-01-06 01:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10][root][INFO] - Training Epoch: 6/10, step 120/574 completed (loss: 0.10154648870229721, acc: 0.9466666579246521)
[2025-01-06 01:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10][root][INFO] - Training Epoch: 6/10, step 121/574 completed (loss: 0.13179215788841248, acc: 0.9615384340286255)
[2025-01-06 01:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11][root][INFO] - Training Epoch: 6/10, step 122/574 completed (loss: 0.054683852940797806, acc: 0.9583333134651184)
[2025-01-06 01:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11][root][INFO] - Training Epoch: 6/10, step 123/574 completed (loss: 0.05686258152127266, acc: 0.9473684430122375)
[2025-01-06 01:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11][root][INFO] - Training Epoch: 6/10, step 124/574 completed (loss: 0.44418784976005554, acc: 0.8650306463241577)
[2025-01-06 01:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12][root][INFO] - Training Epoch: 6/10, step 125/574 completed (loss: 0.4036235213279724, acc: 0.9027777910232544)
[2025-01-06 01:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12][root][INFO] - Training Epoch: 6/10, step 126/574 completed (loss: 0.46258053183555603, acc: 0.8833333253860474)
[2025-01-06 01:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12][root][INFO] - Training Epoch: 6/10, step 127/574 completed (loss: 0.383063942193985, acc: 0.875)
[2025-01-06 01:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13][root][INFO] - Training Epoch: 6/10, step 128/574 completed (loss: 0.4283006191253662, acc: 0.8717948794364929)
[2025-01-06 01:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13][root][INFO] - Training Epoch: 6/10, step 129/574 completed (loss: 0.6536513566970825, acc: 0.8088235259056091)
[2025-01-06 01:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13][root][INFO] - Training Epoch: 6/10, step 130/574 completed (loss: 0.0475090816617012, acc: 0.9615384340286255)
[2025-01-06 01:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14][root][INFO] - Training Epoch: 6/10, step 131/574 completed (loss: 0.23761996626853943, acc: 0.9130434989929199)
[2025-01-06 01:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14][root][INFO] - Training Epoch: 6/10, step 132/574 completed (loss: 0.038512591272592545, acc: 0.96875)
[2025-01-06 01:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:42][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1363, device='cuda:0') eval_epoch_loss=tensor(0.7591, device='cuda:0') eval_epoch_acc=tensor(0.8336, device='cuda:0')
[2025-01-06 01:30:42][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:30:42][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:30:42][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_133_loss_0.7590958476066589/model.pt
[2025-01-06 01:30:42][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:42][root][INFO] - Training Epoch: 6/10, step 133/574 completed (loss: 0.0779557079076767, acc: 0.95652174949646)
[2025-01-06 01:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43][root][INFO] - Training Epoch: 6/10, step 134/574 completed (loss: 0.060113292187452316, acc: 1.0)
[2025-01-06 01:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43][root][INFO] - Training Epoch: 6/10, step 135/574 completed (loss: 0.019597504287958145, acc: 1.0)
[2025-01-06 01:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43][root][INFO] - Training Epoch: 6/10, step 136/574 completed (loss: 0.056328579783439636, acc: 0.976190447807312)
[2025-01-06 01:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44][root][INFO] - Training Epoch: 6/10, step 137/574 completed (loss: 0.15717503428459167, acc: 0.9666666388511658)
[2025-01-06 01:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44][root][INFO] - Training Epoch: 6/10, step 138/574 completed (loss: 0.014266898855566978, acc: 1.0)
[2025-01-06 01:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44][root][INFO] - Training Epoch: 6/10, step 139/574 completed (loss: 0.006443979684263468, acc: 1.0)
[2025-01-06 01:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45][root][INFO] - Training Epoch: 6/10, step 140/574 completed (loss: 0.14067460596561432, acc: 0.9230769276618958)
[2025-01-06 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45][root][INFO] - Training Epoch: 6/10, step 141/574 completed (loss: 0.028109822422266006, acc: 1.0)
[2025-01-06 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45][root][INFO] - Training Epoch: 6/10, step 142/574 completed (loss: 0.26568999886512756, acc: 0.9459459185600281)
[2025-01-06 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46][root][INFO] - Training Epoch: 6/10, step 143/574 completed (loss: 0.24433113634586334, acc: 0.9035087823867798)
[2025-01-06 01:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46][root][INFO] - Training Epoch: 6/10, step 144/574 completed (loss: 0.38461771607398987, acc: 0.8731343150138855)
[2025-01-06 01:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46][root][INFO] - Training Epoch: 6/10, step 145/574 completed (loss: 0.20780403912067413, acc: 0.9489796161651611)
[2025-01-06 01:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47][root][INFO] - Training Epoch: 6/10, step 146/574 completed (loss: 0.5666846632957458, acc: 0.8191489577293396)
[2025-01-06 01:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47][root][INFO] - Training Epoch: 6/10, step 147/574 completed (loss: 0.152830109000206, acc: 0.9714285731315613)
[2025-01-06 01:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48][root][INFO] - Training Epoch: 6/10, step 148/574 completed (loss: 0.16028234362602234, acc: 0.9285714030265808)
[2025-01-06 01:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48][root][INFO] - Training Epoch: 6/10, step 149/574 completed (loss: 0.43213507533073425, acc: 0.9130434989929199)
[2025-01-06 01:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48][root][INFO] - Training Epoch: 6/10, step 150/574 completed (loss: 0.032732971012592316, acc: 1.0)
[2025-01-06 01:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49][root][INFO] - Training Epoch: 6/10, step 151/574 completed (loss: 0.20479299128055573, acc: 0.9347826242446899)
[2025-01-06 01:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49][root][INFO] - Training Epoch: 6/10, step 152/574 completed (loss: 0.27725839614868164, acc: 0.8983050584793091)
[2025-01-06 01:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49][root][INFO] - Training Epoch: 6/10, step 153/574 completed (loss: 0.09957005828619003, acc: 0.9824561476707458)
[2025-01-06 01:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50][root][INFO] - Training Epoch: 6/10, step 154/574 completed (loss: 0.387841135263443, acc: 0.8918918967247009)
[2025-01-06 01:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50][root][INFO] - Training Epoch: 6/10, step 155/574 completed (loss: 0.041424866765737534, acc: 1.0)
[2025-01-06 01:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50][root][INFO] - Training Epoch: 6/10, step 156/574 completed (loss: 0.047443147748708725, acc: 0.95652174949646)
[2025-01-06 01:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:51][root][INFO] - Training Epoch: 6/10, step 157/574 completed (loss: 0.4077311158180237, acc: 0.8947368264198303)
[2025-01-06 01:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:52][root][INFO] - Training Epoch: 6/10, step 158/574 completed (loss: 0.3826907277107239, acc: 0.8918918967247009)
[2025-01-06 01:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53][root][INFO] - Training Epoch: 6/10, step 159/574 completed (loss: 0.515574038028717, acc: 0.8333333134651184)
[2025-01-06 01:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53][root][INFO] - Training Epoch: 6/10, step 160/574 completed (loss: 0.442440003156662, acc: 0.8255813717842102)
[2025-01-06 01:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54][root][INFO] - Training Epoch: 6/10, step 161/574 completed (loss: 0.6461131572723389, acc: 0.800000011920929)
[2025-01-06 01:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54][root][INFO] - Training Epoch: 6/10, step 162/574 completed (loss: 0.5723920464515686, acc: 0.8426966071128845)
[2025-01-06 01:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55][root][INFO] - Training Epoch: 6/10, step 163/574 completed (loss: 0.09172721207141876, acc: 0.9545454382896423)
[2025-01-06 01:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55][root][INFO] - Training Epoch: 6/10, step 164/574 completed (loss: 0.036143794655799866, acc: 1.0)
[2025-01-06 01:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55][root][INFO] - Training Epoch: 6/10, step 165/574 completed (loss: 0.15612515807151794, acc: 0.9655172228813171)
[2025-01-06 01:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56][root][INFO] - Training Epoch: 6/10, step 166/574 completed (loss: 0.025370541960000992, acc: 1.0)
[2025-01-06 01:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56][root][INFO] - Training Epoch: 6/10, step 167/574 completed (loss: 0.06586486101150513, acc: 0.9800000190734863)
[2025-01-06 01:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56][root][INFO] - Training Epoch: 6/10, step 168/574 completed (loss: 0.175674706697464, acc: 0.9305555820465088)
[2025-01-06 01:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:57][root][INFO] - Training Epoch: 6/10, step 169/574 completed (loss: 0.5388196110725403, acc: 0.8039215803146362)
[2025-01-06 01:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58][root][INFO] - Training Epoch: 6/10, step 170/574 completed (loss: 0.3688417077064514, acc: 0.8972602486610413)
[2025-01-06 01:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58][root][INFO] - Training Epoch: 6/10, step 171/574 completed (loss: 0.1980840563774109, acc: 0.9166666865348816)
[2025-01-06 01:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58][root][INFO] - Training Epoch: 6/10, step 172/574 completed (loss: 0.1026303842663765, acc: 0.9629629850387573)
[2025-01-06 01:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59][root][INFO] - Training Epoch: 6/10, step 173/574 completed (loss: 0.14514590799808502, acc: 0.9642857313156128)
[2025-01-06 01:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59][root][INFO] - Training Epoch: 6/10, step 174/574 completed (loss: 0.5314766764640808, acc: 0.8407079577445984)
[2025-01-06 01:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59][root][INFO] - Training Epoch: 6/10, step 175/574 completed (loss: 0.19825021922588348, acc: 0.9710144996643066)
[2025-01-06 01:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:00][root][INFO] - Training Epoch: 6/10, step 176/574 completed (loss: 0.19403886795043945, acc: 0.9318181872367859)
[2025-01-06 01:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01][root][INFO] - Training Epoch: 6/10, step 177/574 completed (loss: 0.5937432050704956, acc: 0.8091602921485901)
[2025-01-06 01:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01][root][INFO] - Training Epoch: 6/10, step 178/574 completed (loss: 0.5934609770774841, acc: 0.8370370268821716)
[2025-01-06 01:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02][root][INFO] - Training Epoch: 6/10, step 179/574 completed (loss: 0.1251375377178192, acc: 0.9672130942344666)
[2025-01-06 01:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02][root][INFO] - Training Epoch: 6/10, step 180/574 completed (loss: 0.013229459524154663, acc: 1.0)
[2025-01-06 01:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02][root][INFO] - Training Epoch: 6/10, step 181/574 completed (loss: 0.01606728695333004, acc: 1.0)
[2025-01-06 01:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03][root][INFO] - Training Epoch: 6/10, step 182/574 completed (loss: 0.04009028896689415, acc: 1.0)
[2025-01-06 01:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03][root][INFO] - Training Epoch: 6/10, step 183/574 completed (loss: 0.08607155829668045, acc: 0.9634146094322205)
[2025-01-06 01:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03][root][INFO] - Training Epoch: 6/10, step 184/574 completed (loss: 0.29930615425109863, acc: 0.9154078364372253)
[2025-01-06 01:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04][root][INFO] - Training Epoch: 6/10, step 185/574 completed (loss: 0.2777082324028015, acc: 0.9221901893615723)
[2025-01-06 01:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04][root][INFO] - Training Epoch: 6/10, step 186/574 completed (loss: 0.27381616830825806, acc: 0.9156249761581421)
[2025-01-06 01:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05][root][INFO] - Training Epoch: 6/10, step 187/574 completed (loss: 0.38136065006256104, acc: 0.8968105316162109)
[2025-01-06 01:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05][root][INFO] - Training Epoch: 6/10, step 188/574 completed (loss: 0.3241918981075287, acc: 0.9145907759666443)
[2025-01-06 01:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05][root][INFO] - Training Epoch: 6/10, step 189/574 completed (loss: 0.511033833026886, acc: 0.9599999785423279)
[2025-01-06 01:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:06][root][INFO] - Training Epoch: 6/10, step 190/574 completed (loss: 0.46623730659484863, acc: 0.8372092843055725)
[2025-01-06 01:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:07][root][INFO] - Training Epoch: 6/10, step 191/574 completed (loss: 0.804137110710144, acc: 0.7698412537574768)
[2025-01-06 01:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08][root][INFO] - Training Epoch: 6/10, step 192/574 completed (loss: 0.6556093096733093, acc: 0.8181818127632141)
[2025-01-06 01:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08][root][INFO] - Training Epoch: 6/10, step 193/574 completed (loss: 0.28295770287513733, acc: 0.9058823585510254)
[2025-01-06 01:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:09][root][INFO] - Training Epoch: 6/10, step 194/574 completed (loss: 0.6869308352470398, acc: 0.7839506268501282)
[2025-01-06 01:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:10][root][INFO] - Training Epoch: 6/10, step 195/574 completed (loss: 0.18959660828113556, acc: 0.9354838728904724)
[2025-01-06 01:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11][root][INFO] - Training Epoch: 6/10, step 196/574 completed (loss: 0.035688288509845734, acc: 1.0)
[2025-01-06 01:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11][root][INFO] - Training Epoch: 6/10, step 197/574 completed (loss: 0.2207464724779129, acc: 0.9750000238418579)
[2025-01-06 01:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11][root][INFO] - Training Epoch: 6/10, step 198/574 completed (loss: 0.230453222990036, acc: 0.9411764740943909)
[2025-01-06 01:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12][root][INFO] - Training Epoch: 6/10, step 199/574 completed (loss: 0.49096012115478516, acc: 0.875)
[2025-01-06 01:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12][root][INFO] - Training Epoch: 6/10, step 200/574 completed (loss: 0.3457854092121124, acc: 0.9067796468734741)
[2025-01-06 01:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12][root][INFO] - Training Epoch: 6/10, step 201/574 completed (loss: 0.47652608156204224, acc: 0.8283582329750061)
[2025-01-06 01:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13][root][INFO] - Training Epoch: 6/10, step 202/574 completed (loss: 0.28190022706985474, acc: 0.9223300814628601)
[2025-01-06 01:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13][root][INFO] - Training Epoch: 6/10, step 203/574 completed (loss: 0.16510623693466187, acc: 0.9523809552192688)
[2025-01-06 01:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13][root][INFO] - Training Epoch: 6/10, step 204/574 completed (loss: 0.02987517975270748, acc: 0.9890109896659851)
[2025-01-06 01:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14][root][INFO] - Training Epoch: 6/10, step 205/574 completed (loss: 0.15038608014583588, acc: 0.9506726264953613)
[2025-01-06 01:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14][root][INFO] - Training Epoch: 6/10, step 206/574 completed (loss: 0.28491467237472534, acc: 0.9251968264579773)
[2025-01-06 01:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15][root][INFO] - Training Epoch: 6/10, step 207/574 completed (loss: 0.1312829703092575, acc: 0.9612069129943848)
[2025-01-06 01:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15][root][INFO] - Training Epoch: 6/10, step 208/574 completed (loss: 0.2637350559234619, acc: 0.9311594367027283)
[2025-01-06 01:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15][root][INFO] - Training Epoch: 6/10, step 209/574 completed (loss: 0.21438539028167725, acc: 0.929961085319519)
[2025-01-06 01:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16][root][INFO] - Training Epoch: 6/10, step 210/574 completed (loss: 0.042562711983919144, acc: 0.989130437374115)
[2025-01-06 01:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16][root][INFO] - Training Epoch: 6/10, step 211/574 completed (loss: 0.0028831923846155405, acc: 1.0)
[2025-01-06 01:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16][root][INFO] - Training Epoch: 6/10, step 212/574 completed (loss: 0.015155358240008354, acc: 1.0)
[2025-01-06 01:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:17][root][INFO] - Training Epoch: 6/10, step 213/574 completed (loss: 0.21261996030807495, acc: 0.957446813583374)
[2025-01-06 01:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:17][root][INFO] - Training Epoch: 6/10, step 214/574 completed (loss: 0.11410751193761826, acc: 0.9615384340286255)
[2025-01-06 01:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18][root][INFO] - Training Epoch: 6/10, step 215/574 completed (loss: 0.06992745399475098, acc: 0.9864864945411682)
[2025-01-06 01:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18][root][INFO] - Training Epoch: 6/10, step 216/574 completed (loss: 0.023017643019557, acc: 0.9883720874786377)
[2025-01-06 01:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18][root][INFO] - Training Epoch: 6/10, step 217/574 completed (loss: 0.10327387601137161, acc: 0.9639639854431152)
[2025-01-06 01:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19][root][INFO] - Training Epoch: 6/10, step 218/574 completed (loss: 0.05072517320513725, acc: 0.9777777791023254)
[2025-01-06 01:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19][root][INFO] - Training Epoch: 6/10, step 219/574 completed (loss: 0.04657178744673729, acc: 0.9696969985961914)
[2025-01-06 01:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19][root][INFO] - Training Epoch: 6/10, step 220/574 completed (loss: 0.003592920256778598, acc: 1.0)
[2025-01-06 01:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20][root][INFO] - Training Epoch: 6/10, step 221/574 completed (loss: 0.005740860011428595, acc: 1.0)
[2025-01-06 01:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20][root][INFO] - Training Epoch: 6/10, step 222/574 completed (loss: 0.16849538683891296, acc: 0.9230769276618958)
[2025-01-06 01:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:21][root][INFO] - Training Epoch: 6/10, step 223/574 completed (loss: 0.21051475405693054, acc: 0.9239130616188049)
[2025-01-06 01:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:21][root][INFO] - Training Epoch: 6/10, step 224/574 completed (loss: 0.32223525643348694, acc: 0.8806818127632141)
[2025-01-06 01:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:22][root][INFO] - Training Epoch: 6/10, step 225/574 completed (loss: 0.38848021626472473, acc: 0.8829787373542786)
[2025-01-06 01:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:22][root][INFO] - Training Epoch: 6/10, step 226/574 completed (loss: 0.11646464467048645, acc: 0.9622641801834106)
[2025-01-06 01:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23][root][INFO] - Training Epoch: 6/10, step 227/574 completed (loss: 0.027816182002425194, acc: 1.0)
[2025-01-06 01:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23][root][INFO] - Training Epoch: 6/10, step 228/574 completed (loss: 0.1285197138786316, acc: 0.9534883499145508)
[2025-01-06 01:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23][root][INFO] - Training Epoch: 6/10, step 229/574 completed (loss: 0.15877015888690948, acc: 0.9333333373069763)
[2025-01-06 01:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24][root][INFO] - Training Epoch: 6/10, step 230/574 completed (loss: 0.8282449245452881, acc: 0.7473683953285217)
[2025-01-06 01:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24][root][INFO] - Training Epoch: 6/10, step 231/574 completed (loss: 0.8163486123085022, acc: 0.8222222328186035)
[2025-01-06 01:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24][root][INFO] - Training Epoch: 6/10, step 232/574 completed (loss: 0.7376903891563416, acc: 0.7722222208976746)
[2025-01-06 01:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25][root][INFO] - Training Epoch: 6/10, step 233/574 completed (loss: 1.1338272094726562, acc: 0.6880733966827393)
[2025-01-06 01:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25][root][INFO] - Training Epoch: 6/10, step 234/574 completed (loss: 0.6762408018112183, acc: 0.807692289352417)
[2025-01-06 01:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26][root][INFO] - Training Epoch: 6/10, step 235/574 completed (loss: 0.0035942625254392624, acc: 1.0)
[2025-01-06 01:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26][root][INFO] - Training Epoch: 6/10, step 236/574 completed (loss: 0.26397934556007385, acc: 0.9583333134651184)
[2025-01-06 01:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26][root][INFO] - Training Epoch: 6/10, step 237/574 completed (loss: 0.28278326988220215, acc: 0.9090909361839294)
[2025-01-06 01:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:27][root][INFO] - Training Epoch: 6/10, step 238/574 completed (loss: 0.20942114293575287, acc: 0.9259259104728699)
[2025-01-06 01:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:27][root][INFO] - Training Epoch: 6/10, step 239/574 completed (loss: 0.06354011595249176, acc: 1.0)
[2025-01-06 01:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:27][root][INFO] - Training Epoch: 6/10, step 240/574 completed (loss: 0.2127404361963272, acc: 0.9545454382896423)
[2025-01-06 01:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:27][root][INFO] - Training Epoch: 6/10, step 241/574 completed (loss: 0.2103433758020401, acc: 0.9318181872367859)
[2025-01-06 01:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:28][root][INFO] - Training Epoch: 6/10, step 242/574 completed (loss: 0.45572972297668457, acc: 0.8387096524238586)
[2025-01-06 01:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29][root][INFO] - Training Epoch: 6/10, step 243/574 completed (loss: 0.2938190698623657, acc: 0.9318181872367859)
[2025-01-06 01:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29][root][INFO] - Training Epoch: 6/10, step 244/574 completed (loss: 0.006969009060412645, acc: 1.0)
[2025-01-06 01:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29][root][INFO] - Training Epoch: 6/10, step 245/574 completed (loss: 0.13091318309307098, acc: 0.9230769276618958)
[2025-01-06 01:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:30][root][INFO] - Training Epoch: 6/10, step 246/574 completed (loss: 0.016925601288676262, acc: 1.0)
[2025-01-06 01:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:30][root][INFO] - Training Epoch: 6/10, step 247/574 completed (loss: 0.06637770682573318, acc: 0.949999988079071)
[2025-01-06 01:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:30][root][INFO] - Training Epoch: 6/10, step 248/574 completed (loss: 0.031183596700429916, acc: 1.0)
[2025-01-06 01:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31][root][INFO] - Training Epoch: 6/10, step 249/574 completed (loss: 0.028674107044935226, acc: 1.0)
[2025-01-06 01:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31][root][INFO] - Training Epoch: 6/10, step 250/574 completed (loss: 0.06855552643537521, acc: 0.9729729890823364)
[2025-01-06 01:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31][root][INFO] - Training Epoch: 6/10, step 251/574 completed (loss: 0.22927060723304749, acc: 0.970588207244873)
[2025-01-06 01:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32][root][INFO] - Training Epoch: 6/10, step 252/574 completed (loss: 0.03812296688556671, acc: 0.9756097793579102)
[2025-01-06 01:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32][root][INFO] - Training Epoch: 6/10, step 253/574 completed (loss: 0.006790949031710625, acc: 1.0)
[2025-01-06 01:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32][root][INFO] - Training Epoch: 6/10, step 254/574 completed (loss: 0.0009361742995679379, acc: 1.0)
[2025-01-06 01:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33][root][INFO] - Training Epoch: 6/10, step 255/574 completed (loss: 0.032426707446575165, acc: 1.0)
[2025-01-06 01:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33][root][INFO] - Training Epoch: 6/10, step 256/574 completed (loss: 0.009874869138002396, acc: 1.0)
[2025-01-06 01:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33][root][INFO] - Training Epoch: 6/10, step 257/574 completed (loss: 0.026600833982229233, acc: 1.0)
[2025-01-06 01:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:34][root][INFO] - Training Epoch: 6/10, step 258/574 completed (loss: 0.052565958350896835, acc: 0.9868420958518982)
[2025-01-06 01:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:34][root][INFO] - Training Epoch: 6/10, step 259/574 completed (loss: 0.1877337396144867, acc: 0.9433962106704712)
[2025-01-06 01:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35][root][INFO] - Training Epoch: 6/10, step 260/574 completed (loss: 0.12243154644966125, acc: 0.9666666388511658)
[2025-01-06 01:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35][root][INFO] - Training Epoch: 6/10, step 261/574 completed (loss: 0.02732921950519085, acc: 1.0)
[2025-01-06 01:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:36][root][INFO] - Training Epoch: 6/10, step 262/574 completed (loss: 0.05077493190765381, acc: 1.0)
[2025-01-06 01:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:36][root][INFO] - Training Epoch: 6/10, step 263/574 completed (loss: 0.3536781966686249, acc: 0.9066666960716248)
[2025-01-06 01:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:36][root][INFO] - Training Epoch: 6/10, step 264/574 completed (loss: 0.22232739627361298, acc: 0.9375)
[2025-01-06 01:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:37][root][INFO] - Training Epoch: 6/10, step 265/574 completed (loss: 0.7181040048599243, acc: 0.7760000228881836)
[2025-01-06 01:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38][root][INFO] - Training Epoch: 6/10, step 266/574 completed (loss: 0.5372829437255859, acc: 0.7977527976036072)
[2025-01-06 01:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38][root][INFO] - Training Epoch: 6/10, step 267/574 completed (loss: 0.23607346415519714, acc: 0.9189189076423645)
[2025-01-06 01:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38][root][INFO] - Training Epoch: 6/10, step 268/574 completed (loss: 0.19720104336738586, acc: 0.9655172228813171)
[2025-01-06 01:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:39][root][INFO] - Training Epoch: 6/10, step 269/574 completed (loss: 0.007797488942742348, acc: 1.0)
[2025-01-06 01:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:39][root][INFO] - Training Epoch: 6/10, step 270/574 completed (loss: 0.005554614122956991, acc: 1.0)
[2025-01-06 01:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:39][root][INFO] - Training Epoch: 6/10, step 271/574 completed (loss: 0.1283840388059616, acc: 0.96875)
[2025-01-06 01:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40][root][INFO] - Training Epoch: 6/10, step 272/574 completed (loss: 0.006357570644468069, acc: 1.0)
[2025-01-06 01:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40][root][INFO] - Training Epoch: 6/10, step 273/574 completed (loss: 0.13909223675727844, acc: 0.9666666388511658)
[2025-01-06 01:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40][root][INFO] - Training Epoch: 6/10, step 274/574 completed (loss: 0.03245265409350395, acc: 0.96875)
[2025-01-06 01:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:41][root][INFO] - Training Epoch: 6/10, step 275/574 completed (loss: 0.025295283645391464, acc: 1.0)
[2025-01-06 01:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:08][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2607, device='cuda:0') eval_epoch_loss=tensor(0.8157, device='cuda:0') eval_epoch_acc=tensor(0.8236, device='cuda:0')
[2025-01-06 01:32:08][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:32:08][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:32:08][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_276_loss_0.8156688809394836/model.pt
[2025-01-06 01:32:08][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:08][root][INFO] - Training Epoch: 6/10, step 276/574 completed (loss: 0.005763411521911621, acc: 1.0)
[2025-01-06 01:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09][root][INFO] - Training Epoch: 6/10, step 277/574 completed (loss: 0.015476396307349205, acc: 1.0)
[2025-01-06 01:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09][root][INFO] - Training Epoch: 6/10, step 278/574 completed (loss: 0.1768302172422409, acc: 0.936170220375061)
[2025-01-06 01:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09][root][INFO] - Training Epoch: 6/10, step 279/574 completed (loss: 0.22318506240844727, acc: 0.9166666865348816)
[2025-01-06 01:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10][root][INFO] - Training Epoch: 6/10, step 280/574 completed (loss: 0.01001564972102642, acc: 1.0)
[2025-01-06 01:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10][root][INFO] - Training Epoch: 6/10, step 281/574 completed (loss: 0.1427396684885025, acc: 0.9759036302566528)
[2025-01-06 01:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11][root][INFO] - Training Epoch: 6/10, step 282/574 completed (loss: 0.37845122814178467, acc: 0.8981481194496155)
[2025-01-06 01:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11][root][INFO] - Training Epoch: 6/10, step 283/574 completed (loss: 0.03351602703332901, acc: 0.9736841917037964)
[2025-01-06 01:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11][root][INFO] - Training Epoch: 6/10, step 284/574 completed (loss: 0.013815764337778091, acc: 1.0)
[2025-01-06 01:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:12][root][INFO] - Training Epoch: 6/10, step 285/574 completed (loss: 0.03370141237974167, acc: 1.0)
[2025-01-06 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:12][root][INFO] - Training Epoch: 6/10, step 286/574 completed (loss: 0.21082167327404022, acc: 0.9453125)
[2025-01-06 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:12][root][INFO] - Training Epoch: 6/10, step 287/574 completed (loss: 0.31449130177497864, acc: 0.9039999842643738)
[2025-01-06 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13][root][INFO] - Training Epoch: 6/10, step 288/574 completed (loss: 0.1012192890048027, acc: 0.9780219793319702)
[2025-01-06 01:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13][root][INFO] - Training Epoch: 6/10, step 289/574 completed (loss: 0.13336485624313354, acc: 0.9378882050514221)
[2025-01-06 01:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13][root][INFO] - Training Epoch: 6/10, step 290/574 completed (loss: 0.35086125135421753, acc: 0.8865979313850403)
[2025-01-06 01:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14][root][INFO] - Training Epoch: 6/10, step 291/574 completed (loss: 0.003912276588380337, acc: 1.0)
[2025-01-06 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14][root][INFO] - Training Epoch: 6/10, step 292/574 completed (loss: 0.027593862265348434, acc: 1.0)
[2025-01-06 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14][root][INFO] - Training Epoch: 6/10, step 293/574 completed (loss: 0.19372811913490295, acc: 0.9137930870056152)
[2025-01-06 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15][root][INFO] - Training Epoch: 6/10, step 294/574 completed (loss: 0.17389222979545593, acc: 0.9272727370262146)
[2025-01-06 01:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15][root][INFO] - Training Epoch: 6/10, step 295/574 completed (loss: 0.2477002888917923, acc: 0.9278350472450256)
[2025-01-06 01:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16][root][INFO] - Training Epoch: 6/10, step 296/574 completed (loss: 0.17616969347000122, acc: 0.9655172228813171)
[2025-01-06 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16][root][INFO] - Training Epoch: 6/10, step 297/574 completed (loss: 0.10144788026809692, acc: 0.9629629850387573)
[2025-01-06 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16][root][INFO] - Training Epoch: 6/10, step 298/574 completed (loss: 0.03784778714179993, acc: 1.0)
[2025-01-06 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17][root][INFO] - Training Epoch: 6/10, step 299/574 completed (loss: 0.005145073402673006, acc: 1.0)
[2025-01-06 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17][root][INFO] - Training Epoch: 6/10, step 300/574 completed (loss: 0.003425614908337593, acc: 1.0)
[2025-01-06 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17][root][INFO] - Training Epoch: 6/10, step 301/574 completed (loss: 0.03610197827219963, acc: 0.9811320900917053)
[2025-01-06 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18][root][INFO] - Training Epoch: 6/10, step 302/574 completed (loss: 0.004722012672573328, acc: 1.0)
[2025-01-06 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18][root][INFO] - Training Epoch: 6/10, step 303/574 completed (loss: 0.0033644018694758415, acc: 1.0)
[2025-01-06 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18][root][INFO] - Training Epoch: 6/10, step 304/574 completed (loss: 0.006991189904510975, acc: 1.0)
[2025-01-06 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19][root][INFO] - Training Epoch: 6/10, step 305/574 completed (loss: 0.0957733541727066, acc: 0.9836065769195557)
[2025-01-06 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19][root][INFO] - Training Epoch: 6/10, step 306/574 completed (loss: 0.04249138385057449, acc: 1.0)
[2025-01-06 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19][root][INFO] - Training Epoch: 6/10, step 307/574 completed (loss: 0.002535092644393444, acc: 1.0)
[2025-01-06 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20][root][INFO] - Training Epoch: 6/10, step 308/574 completed (loss: 0.09179080277681351, acc: 0.95652174949646)
[2025-01-06 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20][root][INFO] - Training Epoch: 6/10, step 309/574 completed (loss: 0.04804740846157074, acc: 0.9861111044883728)
[2025-01-06 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20][root][INFO] - Training Epoch: 6/10, step 310/574 completed (loss: 0.09222641587257385, acc: 0.9638554453849792)
[2025-01-06 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21][root][INFO] - Training Epoch: 6/10, step 311/574 completed (loss: 0.10806500911712646, acc: 0.9358974099159241)
[2025-01-06 01:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21][root][INFO] - Training Epoch: 6/10, step 312/574 completed (loss: 0.08888798952102661, acc: 0.9693877696990967)
[2025-01-06 01:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21][root][INFO] - Training Epoch: 6/10, step 313/574 completed (loss: 0.0014229664811864495, acc: 1.0)
[2025-01-06 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22][root][INFO] - Training Epoch: 6/10, step 314/574 completed (loss: 0.004773115273565054, acc: 1.0)
[2025-01-06 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22][root][INFO] - Training Epoch: 6/10, step 315/574 completed (loss: 0.06744540482759476, acc: 0.9677419066429138)
[2025-01-06 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22][root][INFO] - Training Epoch: 6/10, step 316/574 completed (loss: 0.2560786008834839, acc: 0.9354838728904724)
[2025-01-06 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23][root][INFO] - Training Epoch: 6/10, step 317/574 completed (loss: 0.02808419056236744, acc: 0.9850746393203735)
[2025-01-06 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23][root][INFO] - Training Epoch: 6/10, step 318/574 completed (loss: 0.01299215480685234, acc: 1.0)
[2025-01-06 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24][root][INFO] - Training Epoch: 6/10, step 319/574 completed (loss: 0.007974879816174507, acc: 1.0)
[2025-01-06 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24][root][INFO] - Training Epoch: 6/10, step 320/574 completed (loss: 0.012079799547791481, acc: 1.0)
[2025-01-06 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24][root][INFO] - Training Epoch: 6/10, step 321/574 completed (loss: 0.013506009243428707, acc: 1.0)
[2025-01-06 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25][root][INFO] - Training Epoch: 6/10, step 322/574 completed (loss: 0.5633297562599182, acc: 0.8518518805503845)
[2025-01-06 01:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25][root][INFO] - Training Epoch: 6/10, step 323/574 completed (loss: 0.12609542906284332, acc: 0.9714285731315613)
[2025-01-06 01:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25][root][INFO] - Training Epoch: 6/10, step 324/574 completed (loss: 0.22955884039402008, acc: 0.9487179517745972)
[2025-01-06 01:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26][root][INFO] - Training Epoch: 6/10, step 325/574 completed (loss: 0.4156932830810547, acc: 0.8536585569381714)
[2025-01-06 01:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26][root][INFO] - Training Epoch: 6/10, step 326/574 completed (loss: 0.1344403773546219, acc: 0.9736841917037964)
[2025-01-06 01:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26][root][INFO] - Training Epoch: 6/10, step 327/574 completed (loss: 0.03269059956073761, acc: 1.0)
[2025-01-06 01:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27][root][INFO] - Training Epoch: 6/10, step 328/574 completed (loss: 0.04411491006612778, acc: 0.9642857313156128)
[2025-01-06 01:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27][root][INFO] - Training Epoch: 6/10, step 329/574 completed (loss: 0.1703193634748459, acc: 0.9629629850387573)
[2025-01-06 01:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27][root][INFO] - Training Epoch: 6/10, step 330/574 completed (loss: 0.003447245806455612, acc: 1.0)
[2025-01-06 01:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28][root][INFO] - Training Epoch: 6/10, step 331/574 completed (loss: 0.10775730013847351, acc: 0.9838709831237793)
[2025-01-06 01:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28][root][INFO] - Training Epoch: 6/10, step 332/574 completed (loss: 0.023916255682706833, acc: 1.0)
[2025-01-06 01:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28][root][INFO] - Training Epoch: 6/10, step 333/574 completed (loss: 0.0050874315202236176, acc: 1.0)
[2025-01-06 01:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29][root][INFO] - Training Epoch: 6/10, step 334/574 completed (loss: 0.01674572005867958, acc: 1.0)
[2025-01-06 01:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29][root][INFO] - Training Epoch: 6/10, step 335/574 completed (loss: 0.004517257213592529, acc: 1.0)
[2025-01-06 01:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29][root][INFO] - Training Epoch: 6/10, step 336/574 completed (loss: 0.09665070474147797, acc: 0.9800000190734863)
[2025-01-06 01:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30][root][INFO] - Training Epoch: 6/10, step 337/574 completed (loss: 0.5003653168678284, acc: 0.8275862336158752)
[2025-01-06 01:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30][root][INFO] - Training Epoch: 6/10, step 338/574 completed (loss: 0.4610433876514435, acc: 0.8617021441459656)
[2025-01-06 01:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31][root][INFO] - Training Epoch: 6/10, step 339/574 completed (loss: 0.4535311460494995, acc: 0.8674699068069458)
[2025-01-06 01:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31][root][INFO] - Training Epoch: 6/10, step 340/574 completed (loss: 0.0018577385926619172, acc: 1.0)
[2025-01-06 01:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31][root][INFO] - Training Epoch: 6/10, step 341/574 completed (loss: 0.01157098263502121, acc: 1.0)
[2025-01-06 01:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32][root][INFO] - Training Epoch: 6/10, step 342/574 completed (loss: 0.09806327521800995, acc: 0.9397590160369873)
[2025-01-06 01:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32][root][INFO] - Training Epoch: 6/10, step 343/574 completed (loss: 0.14356866478919983, acc: 0.9622641801834106)
[2025-01-06 01:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32][root][INFO] - Training Epoch: 6/10, step 344/574 completed (loss: 0.041525643318891525, acc: 0.9873417615890503)
[2025-01-06 01:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33][root][INFO] - Training Epoch: 6/10, step 345/574 completed (loss: 0.06908929347991943, acc: 0.9803921580314636)
[2025-01-06 01:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33][root][INFO] - Training Epoch: 6/10, step 346/574 completed (loss: 0.011423836462199688, acc: 1.0)
[2025-01-06 01:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33][root][INFO] - Training Epoch: 6/10, step 347/574 completed (loss: 0.0010790886590257287, acc: 1.0)
[2025-01-06 01:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34][root][INFO] - Training Epoch: 6/10, step 348/574 completed (loss: 0.010681762360036373, acc: 1.0)
[2025-01-06 01:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34][root][INFO] - Training Epoch: 6/10, step 349/574 completed (loss: 0.24386848509311676, acc: 0.9444444179534912)
[2025-01-06 01:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34][root][INFO] - Training Epoch: 6/10, step 350/574 completed (loss: 0.0562131293118, acc: 1.0)
[2025-01-06 01:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35][root][INFO] - Training Epoch: 6/10, step 351/574 completed (loss: 0.17146849632263184, acc: 0.9743589758872986)
[2025-01-06 01:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35][root][INFO] - Training Epoch: 6/10, step 352/574 completed (loss: 0.10091692954301834, acc: 0.9777777791023254)
[2025-01-06 01:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35][root][INFO] - Training Epoch: 6/10, step 353/574 completed (loss: 0.003162674605846405, acc: 1.0)
[2025-01-06 01:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36][root][INFO] - Training Epoch: 6/10, step 354/574 completed (loss: 0.16553565859794617, acc: 0.9615384340286255)
[2025-01-06 01:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36][root][INFO] - Training Epoch: 6/10, step 355/574 completed (loss: 0.30526354908943176, acc: 0.901098906993866)
[2025-01-06 01:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37][root][INFO] - Training Epoch: 6/10, step 356/574 completed (loss: 0.2943090796470642, acc: 0.8608695864677429)
[2025-01-06 01:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37][root][INFO] - Training Epoch: 6/10, step 357/574 completed (loss: 0.11500327289104462, acc: 0.97826087474823)
[2025-01-06 01:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37][root][INFO] - Training Epoch: 6/10, step 358/574 completed (loss: 0.216099351644516, acc: 0.9387755393981934)
[2025-01-06 01:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38][root][INFO] - Training Epoch: 6/10, step 359/574 completed (loss: 0.0011403512908145785, acc: 1.0)
[2025-01-06 01:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38][root][INFO] - Training Epoch: 6/10, step 360/574 completed (loss: 0.01290606614202261, acc: 1.0)
[2025-01-06 01:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38][root][INFO] - Training Epoch: 6/10, step 361/574 completed (loss: 0.08943907916545868, acc: 0.9512194991111755)
[2025-01-06 01:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39][root][INFO] - Training Epoch: 6/10, step 362/574 completed (loss: 0.14631672203540802, acc: 0.9777777791023254)
[2025-01-06 01:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39][root][INFO] - Training Epoch: 6/10, step 363/574 completed (loss: 0.009582366794347763, acc: 1.0)
[2025-01-06 01:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39][root][INFO] - Training Epoch: 6/10, step 364/574 completed (loss: 0.004893295466899872, acc: 1.0)
[2025-01-06 01:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40][root][INFO] - Training Epoch: 6/10, step 365/574 completed (loss: 0.01092990767210722, acc: 1.0)
[2025-01-06 01:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40][root][INFO] - Training Epoch: 6/10, step 366/574 completed (loss: 0.0006319808890111744, acc: 1.0)
[2025-01-06 01:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40][root][INFO] - Training Epoch: 6/10, step 367/574 completed (loss: 0.0006503549520857632, acc: 1.0)
[2025-01-06 01:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41][root][INFO] - Training Epoch: 6/10, step 368/574 completed (loss: 0.010306780226528645, acc: 1.0)
[2025-01-06 01:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41][root][INFO] - Training Epoch: 6/10, step 369/574 completed (loss: 0.04824299365282059, acc: 1.0)
[2025-01-06 01:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:42][root][INFO] - Training Epoch: 6/10, step 370/574 completed (loss: 0.4249679148197174, acc: 0.8848484754562378)
[2025-01-06 01:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:42][root][INFO] - Training Epoch: 6/10, step 371/574 completed (loss: 0.12748026847839355, acc: 0.9622641801834106)
[2025-01-06 01:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43][root][INFO] - Training Epoch: 6/10, step 372/574 completed (loss: 0.10234220325946808, acc: 0.9666666388511658)
[2025-01-06 01:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43][root][INFO] - Training Epoch: 6/10, step 373/574 completed (loss: 0.029473092406988144, acc: 1.0)
[2025-01-06 01:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43][root][INFO] - Training Epoch: 6/10, step 374/574 completed (loss: 0.12187951058149338, acc: 0.9428571462631226)
[2025-01-06 01:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44][root][INFO] - Training Epoch: 6/10, step 375/574 completed (loss: 0.0024024026934057474, acc: 1.0)
[2025-01-06 01:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44][root][INFO] - Training Epoch: 6/10, step 376/574 completed (loss: 0.0011791711440309882, acc: 1.0)
[2025-01-06 01:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44][root][INFO] - Training Epoch: 6/10, step 377/574 completed (loss: 0.026005079969763756, acc: 1.0)
[2025-01-06 01:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45][root][INFO] - Training Epoch: 6/10, step 378/574 completed (loss: 0.036788828670978546, acc: 0.9789473414421082)
[2025-01-06 01:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45][root][INFO] - Training Epoch: 6/10, step 379/574 completed (loss: 0.22637973725795746, acc: 0.9221556782722473)
[2025-01-06 01:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:46][root][INFO] - Training Epoch: 6/10, step 380/574 completed (loss: 0.34817296266555786, acc: 0.9172932505607605)
[2025-01-06 01:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47][root][INFO] - Training Epoch: 6/10, step 381/574 completed (loss: 0.6029040217399597, acc: 0.8395721912384033)
[2025-01-06 01:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47][root][INFO] - Training Epoch: 6/10, step 382/574 completed (loss: 0.2258436381816864, acc: 0.954954981803894)
[2025-01-06 01:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48][root][INFO] - Training Epoch: 6/10, step 383/574 completed (loss: 0.06360651552677155, acc: 1.0)
[2025-01-06 01:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48][root][INFO] - Training Epoch: 6/10, step 384/574 completed (loss: 0.006333875935524702, acc: 1.0)
[2025-01-06 01:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48][root][INFO] - Training Epoch: 6/10, step 385/574 completed (loss: 0.002457950497046113, acc: 1.0)
[2025-01-06 01:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49][root][INFO] - Training Epoch: 6/10, step 386/574 completed (loss: 0.002728078281506896, acc: 1.0)
[2025-01-06 01:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49][root][INFO] - Training Epoch: 6/10, step 387/574 completed (loss: 0.003034438006579876, acc: 1.0)
[2025-01-06 01:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49][root][INFO] - Training Epoch: 6/10, step 388/574 completed (loss: 0.0013589821755886078, acc: 1.0)
[2025-01-06 01:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50][root][INFO] - Training Epoch: 6/10, step 389/574 completed (loss: 0.0016884414944797754, acc: 1.0)
[2025-01-06 01:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50][root][INFO] - Training Epoch: 6/10, step 390/574 completed (loss: 0.14206185936927795, acc: 0.9047619104385376)
[2025-01-06 01:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50][root][INFO] - Training Epoch: 6/10, step 391/574 completed (loss: 0.18531154096126556, acc: 0.9259259104728699)
[2025-01-06 01:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51][root][INFO] - Training Epoch: 6/10, step 392/574 completed (loss: 0.39421766996383667, acc: 0.8543689250946045)
[2025-01-06 01:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51][root][INFO] - Training Epoch: 6/10, step 393/574 completed (loss: 0.5814769864082336, acc: 0.8529411554336548)
[2025-01-06 01:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52][root][INFO] - Training Epoch: 6/10, step 394/574 completed (loss: 0.33364495635032654, acc: 0.9066666960716248)
[2025-01-06 01:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52][root][INFO] - Training Epoch: 6/10, step 395/574 completed (loss: 0.41575613617897034, acc: 0.8888888955116272)
[2025-01-06 01:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52][root][INFO] - Training Epoch: 6/10, step 396/574 completed (loss: 0.09238838404417038, acc: 0.9767441749572754)
[2025-01-06 01:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53][root][INFO] - Training Epoch: 6/10, step 397/574 completed (loss: 0.1354547142982483, acc: 0.9583333134651184)
[2025-01-06 01:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53][root][INFO] - Training Epoch: 6/10, step 398/574 completed (loss: 0.06422214210033417, acc: 0.9767441749572754)
[2025-01-06 01:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53][root][INFO] - Training Epoch: 6/10, step 399/574 completed (loss: 0.03512624278664589, acc: 1.0)
[2025-01-06 01:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:54][root][INFO] - Training Epoch: 6/10, step 400/574 completed (loss: 0.06848934292793274, acc: 1.0)
[2025-01-06 01:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:54][root][INFO] - Training Epoch: 6/10, step 401/574 completed (loss: 0.14486336708068848, acc: 0.9466666579246521)
[2025-01-06 01:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55][root][INFO] - Training Epoch: 6/10, step 402/574 completed (loss: 0.11121248453855515, acc: 0.939393937587738)
[2025-01-06 01:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55][root][INFO] - Training Epoch: 6/10, step 403/574 completed (loss: 0.08614037185907364, acc: 0.939393937587738)
[2025-01-06 01:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55][root][INFO] - Training Epoch: 6/10, step 404/574 completed (loss: 0.02276015281677246, acc: 1.0)
[2025-01-06 01:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56][root][INFO] - Training Epoch: 6/10, step 405/574 completed (loss: 0.002504399511963129, acc: 1.0)
[2025-01-06 01:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56][root][INFO] - Training Epoch: 6/10, step 406/574 completed (loss: 0.01828124187886715, acc: 1.0)
[2025-01-06 01:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56][root][INFO] - Training Epoch: 6/10, step 407/574 completed (loss: 0.0025414940901100636, acc: 1.0)
[2025-01-06 01:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57][root][INFO] - Training Epoch: 6/10, step 408/574 completed (loss: 0.003438895335420966, acc: 1.0)
[2025-01-06 01:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57][root][INFO] - Training Epoch: 6/10, step 409/574 completed (loss: 0.001754772150889039, acc: 1.0)
[2025-01-06 01:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57][root][INFO] - Training Epoch: 6/10, step 410/574 completed (loss: 0.019883345812559128, acc: 0.982758641242981)
[2025-01-06 01:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58][root][INFO] - Training Epoch: 6/10, step 411/574 completed (loss: 0.012037183158099651, acc: 1.0)
[2025-01-06 01:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58][root][INFO] - Training Epoch: 6/10, step 412/574 completed (loss: 0.0005653815460391343, acc: 1.0)
[2025-01-06 01:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58][root][INFO] - Training Epoch: 6/10, step 413/574 completed (loss: 0.053919773548841476, acc: 0.9696969985961914)
[2025-01-06 01:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59][root][INFO] - Training Epoch: 6/10, step 414/574 completed (loss: 0.10186150670051575, acc: 0.9090909361839294)
[2025-01-06 01:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59][root][INFO] - Training Epoch: 6/10, step 415/574 completed (loss: 0.1357509046792984, acc: 0.9411764740943909)
[2025-01-06 01:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59][root][INFO] - Training Epoch: 6/10, step 416/574 completed (loss: 0.029259197413921356, acc: 1.0)
[2025-01-06 01:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00][root][INFO] - Training Epoch: 6/10, step 417/574 completed (loss: 0.014004609547555447, acc: 1.0)
[2025-01-06 01:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00][root][INFO] - Training Epoch: 6/10, step 418/574 completed (loss: 0.04022429510951042, acc: 0.9750000238418579)
[2025-01-06 01:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2416, device='cuda:0') eval_epoch_loss=tensor(0.8072, device='cuda:0') eval_epoch_acc=tensor(0.8267, device='cuda:0')
[2025-01-06 01:33:28][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:33:28][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:33:29][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_419_loss_0.807183027267456/model.pt
[2025-01-06 01:33:29][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:29][root][INFO] - Training Epoch: 6/10, step 419/574 completed (loss: 0.010616546496748924, acc: 1.0)
[2025-01-06 01:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30][root][INFO] - Training Epoch: 6/10, step 420/574 completed (loss: 0.0018867915496230125, acc: 1.0)
[2025-01-06 01:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30][root][INFO] - Training Epoch: 6/10, step 421/574 completed (loss: 0.030319280922412872, acc: 1.0)
[2025-01-06 01:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30][root][INFO] - Training Epoch: 6/10, step 422/574 completed (loss: 0.03232948109507561, acc: 0.96875)
[2025-01-06 01:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30][root][INFO] - Training Epoch: 6/10, step 423/574 completed (loss: 0.0469970777630806, acc: 0.9722222089767456)
[2025-01-06 01:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31][root][INFO] - Training Epoch: 6/10, step 424/574 completed (loss: 0.009267818182706833, acc: 1.0)
[2025-01-06 01:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31][root][INFO] - Training Epoch: 6/10, step 425/574 completed (loss: 0.2311953902244568, acc: 0.939393937587738)
[2025-01-06 01:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31][root][INFO] - Training Epoch: 6/10, step 426/574 completed (loss: 0.06312844902276993, acc: 0.95652174949646)
[2025-01-06 01:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32][root][INFO] - Training Epoch: 6/10, step 427/574 completed (loss: 0.01834234595298767, acc: 1.0)
[2025-01-06 01:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32][root][INFO] - Training Epoch: 6/10, step 428/574 completed (loss: 0.01301872730255127, acc: 1.0)
[2025-01-06 01:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32][root][INFO] - Training Epoch: 6/10, step 429/574 completed (loss: 0.056513480842113495, acc: 0.95652174949646)
[2025-01-06 01:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33][root][INFO] - Training Epoch: 6/10, step 430/574 completed (loss: 0.004747921135276556, acc: 1.0)
[2025-01-06 01:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33][root][INFO] - Training Epoch: 6/10, step 431/574 completed (loss: 0.001115156919695437, acc: 1.0)
[2025-01-06 01:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33][root][INFO] - Training Epoch: 6/10, step 432/574 completed (loss: 0.0006635895115323365, acc: 1.0)
[2025-01-06 01:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34][root][INFO] - Training Epoch: 6/10, step 433/574 completed (loss: 0.21234025061130524, acc: 0.9722222089767456)
[2025-01-06 01:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34][root][INFO] - Training Epoch: 6/10, step 434/574 completed (loss: 0.003720261389389634, acc: 1.0)
[2025-01-06 01:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34][root][INFO] - Training Epoch: 6/10, step 435/574 completed (loss: 0.009692751802504063, acc: 1.0)
[2025-01-06 01:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35][root][INFO] - Training Epoch: 6/10, step 436/574 completed (loss: 0.12499748170375824, acc: 0.9444444179534912)
[2025-01-06 01:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35][root][INFO] - Training Epoch: 6/10, step 437/574 completed (loss: 0.028762727975845337, acc: 0.9772727489471436)
[2025-01-06 01:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:36][root][INFO] - Training Epoch: 6/10, step 438/574 completed (loss: 0.010404475033283234, acc: 1.0)
[2025-01-06 01:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:36][root][INFO] - Training Epoch: 6/10, step 439/574 completed (loss: 0.009487626142799854, acc: 1.0)
[2025-01-06 01:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:36][root][INFO] - Training Epoch: 6/10, step 440/574 completed (loss: 0.07950321584939957, acc: 0.9696969985961914)
[2025-01-06 01:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37][root][INFO] - Training Epoch: 6/10, step 441/574 completed (loss: 0.603901743888855, acc: 0.8399999737739563)
[2025-01-06 01:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37][root][INFO] - Training Epoch: 6/10, step 442/574 completed (loss: 0.4691774845123291, acc: 0.8629032373428345)
[2025-01-06 01:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38][root][INFO] - Training Epoch: 6/10, step 443/574 completed (loss: 0.345438688993454, acc: 0.89552241563797)
[2025-01-06 01:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38][root][INFO] - Training Epoch: 6/10, step 444/574 completed (loss: 0.05567646771669388, acc: 0.9622641801834106)
[2025-01-06 01:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:39][root][INFO] - Training Epoch: 6/10, step 445/574 completed (loss: 0.024734877049922943, acc: 1.0)
[2025-01-06 01:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:39][root][INFO] - Training Epoch: 6/10, step 446/574 completed (loss: 0.0027741771191358566, acc: 1.0)
[2025-01-06 01:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40][root][INFO] - Training Epoch: 6/10, step 447/574 completed (loss: 0.07900816947221756, acc: 0.9615384340286255)
[2025-01-06 01:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40][root][INFO] - Training Epoch: 6/10, step 448/574 completed (loss: 0.0008572966908104718, acc: 1.0)
[2025-01-06 01:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40][root][INFO] - Training Epoch: 6/10, step 449/574 completed (loss: 0.08268040418624878, acc: 0.9552238583564758)
[2025-01-06 01:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41][root][INFO] - Training Epoch: 6/10, step 450/574 completed (loss: 0.02939728833734989, acc: 0.9861111044883728)
[2025-01-06 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41][root][INFO] - Training Epoch: 6/10, step 451/574 completed (loss: 0.010211379267275333, acc: 1.0)
[2025-01-06 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41][root][INFO] - Training Epoch: 6/10, step 452/574 completed (loss: 0.07107924669981003, acc: 0.9615384340286255)
[2025-01-06 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42][root][INFO] - Training Epoch: 6/10, step 453/574 completed (loss: 0.18717704713344574, acc: 0.9736841917037964)
[2025-01-06 01:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42][root][INFO] - Training Epoch: 6/10, step 454/574 completed (loss: 0.021258467808365822, acc: 1.0)
[2025-01-06 01:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42][root][INFO] - Training Epoch: 6/10, step 455/574 completed (loss: 0.004541350528597832, acc: 1.0)
[2025-01-06 01:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43][root][INFO] - Training Epoch: 6/10, step 456/574 completed (loss: 0.17082896828651428, acc: 0.9484536051750183)
[2025-01-06 01:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43][root][INFO] - Training Epoch: 6/10, step 457/574 completed (loss: 0.018320199102163315, acc: 1.0)
[2025-01-06 01:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43][root][INFO] - Training Epoch: 6/10, step 458/574 completed (loss: 0.14770573377609253, acc: 0.9476743936538696)
[2025-01-06 01:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44][root][INFO] - Training Epoch: 6/10, step 459/574 completed (loss: 0.05257153511047363, acc: 0.9642857313156128)
[2025-01-06 01:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44][root][INFO] - Training Epoch: 6/10, step 460/574 completed (loss: 0.02993132174015045, acc: 1.0)
[2025-01-06 01:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44][root][INFO] - Training Epoch: 6/10, step 461/574 completed (loss: 0.005491858813911676, acc: 1.0)
[2025-01-06 01:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45][root][INFO] - Training Epoch: 6/10, step 462/574 completed (loss: 0.02607303485274315, acc: 1.0)
[2025-01-06 01:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45][root][INFO] - Training Epoch: 6/10, step 463/574 completed (loss: 0.0603378564119339, acc: 0.9615384340286255)
[2025-01-06 01:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45][root][INFO] - Training Epoch: 6/10, step 464/574 completed (loss: 0.011584766209125519, acc: 1.0)
[2025-01-06 01:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46][root][INFO] - Training Epoch: 6/10, step 465/574 completed (loss: 0.07360707968473434, acc: 0.9642857313156128)
[2025-01-06 01:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46][root][INFO] - Training Epoch: 6/10, step 466/574 completed (loss: 0.31737449765205383, acc: 0.9156626462936401)
[2025-01-06 01:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46][root][INFO] - Training Epoch: 6/10, step 467/574 completed (loss: 0.0829029306769371, acc: 0.9819819927215576)
[2025-01-06 01:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47][root][INFO] - Training Epoch: 6/10, step 468/574 completed (loss: 0.21082670986652374, acc: 0.9611650705337524)
[2025-01-06 01:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47][root][INFO] - Training Epoch: 6/10, step 469/574 completed (loss: 0.175923153758049, acc: 0.9430894255638123)
[2025-01-06 01:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47][root][INFO] - Training Epoch: 6/10, step 470/574 completed (loss: 0.0026744662318378687, acc: 1.0)
[2025-01-06 01:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47][root][INFO] - Training Epoch: 6/10, step 471/574 completed (loss: 0.003870378015562892, acc: 1.0)
[2025-01-06 01:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48][root][INFO] - Training Epoch: 6/10, step 472/574 completed (loss: 0.26681050658226013, acc: 0.9215686321258545)
[2025-01-06 01:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48][root][INFO] - Training Epoch: 6/10, step 473/574 completed (loss: 0.40137985348701477, acc: 0.8777292370796204)
[2025-01-06 01:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49][root][INFO] - Training Epoch: 6/10, step 474/574 completed (loss: 0.13319921493530273, acc: 0.9583333134651184)
[2025-01-06 01:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49][root][INFO] - Training Epoch: 6/10, step 475/574 completed (loss: 0.18933561444282532, acc: 0.9386503100395203)
[2025-01-06 01:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49][root][INFO] - Training Epoch: 6/10, step 476/574 completed (loss: 0.16108451783657074, acc: 0.9424460530281067)
[2025-01-06 01:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50][root][INFO] - Training Epoch: 6/10, step 477/574 completed (loss: 0.31599050760269165, acc: 0.8944723606109619)
[2025-01-06 01:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50][root][INFO] - Training Epoch: 6/10, step 478/574 completed (loss: 0.2539205253124237, acc: 0.8888888955116272)
[2025-01-06 01:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50][root][INFO] - Training Epoch: 6/10, step 479/574 completed (loss: 0.053331948816776276, acc: 1.0)
[2025-01-06 01:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51][root][INFO] - Training Epoch: 6/10, step 480/574 completed (loss: 0.005189498886466026, acc: 1.0)
[2025-01-06 01:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51][root][INFO] - Training Epoch: 6/10, step 481/574 completed (loss: 0.023550357669591904, acc: 1.0)
[2025-01-06 01:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51][root][INFO] - Training Epoch: 6/10, step 482/574 completed (loss: 0.04158687964081764, acc: 1.0)
[2025-01-06 01:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52][root][INFO] - Training Epoch: 6/10, step 483/574 completed (loss: 0.17082834243774414, acc: 0.9482758641242981)
[2025-01-06 01:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52][root][INFO] - Training Epoch: 6/10, step 484/574 completed (loss: 0.0013146803248673677, acc: 1.0)
[2025-01-06 01:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52][root][INFO] - Training Epoch: 6/10, step 485/574 completed (loss: 0.016828736290335655, acc: 1.0)
[2025-01-06 01:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52][root][INFO] - Training Epoch: 6/10, step 486/574 completed (loss: 0.10116251558065414, acc: 0.9629629850387573)
[2025-01-06 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53][root][INFO] - Training Epoch: 6/10, step 487/574 completed (loss: 0.024482110515236855, acc: 1.0)
[2025-01-06 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53][root][INFO] - Training Epoch: 6/10, step 488/574 completed (loss: 0.015758462250232697, acc: 1.0)
[2025-01-06 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53][root][INFO] - Training Epoch: 6/10, step 489/574 completed (loss: 0.27294617891311646, acc: 0.892307698726654)
[2025-01-06 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54][root][INFO] - Training Epoch: 6/10, step 490/574 completed (loss: 0.06006141006946564, acc: 0.9666666388511658)
[2025-01-06 01:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54][root][INFO] - Training Epoch: 6/10, step 491/574 completed (loss: 0.009046370163559914, acc: 1.0)
[2025-01-06 01:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54][root][INFO] - Training Epoch: 6/10, step 492/574 completed (loss: 0.06803401559591293, acc: 0.9803921580314636)
[2025-01-06 01:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55][root][INFO] - Training Epoch: 6/10, step 493/574 completed (loss: 0.2953330874443054, acc: 0.931034505367279)
[2025-01-06 01:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55][root][INFO] - Training Epoch: 6/10, step 494/574 completed (loss: 0.026796404272317886, acc: 1.0)
[2025-01-06 01:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55][root][INFO] - Training Epoch: 6/10, step 495/574 completed (loss: 0.011508679017424583, acc: 1.0)
[2025-01-06 01:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55][root][INFO] - Training Epoch: 6/10, step 496/574 completed (loss: 0.39062389731407166, acc: 0.9107142686843872)
[2025-01-06 01:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:56][root][INFO] - Training Epoch: 6/10, step 497/574 completed (loss: 0.22615297138690948, acc: 0.9213483333587646)
[2025-01-06 01:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:56][root][INFO] - Training Epoch: 6/10, step 498/574 completed (loss: 0.24724359810352325, acc: 0.932584285736084)
[2025-01-06 01:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57][root][INFO] - Training Epoch: 6/10, step 499/574 completed (loss: 0.6741212606430054, acc: 0.7943262457847595)
[2025-01-06 01:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57][root][INFO] - Training Epoch: 6/10, step 500/574 completed (loss: 0.16968375444412231, acc: 0.945652186870575)
[2025-01-06 01:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57][root][INFO] - Training Epoch: 6/10, step 501/574 completed (loss: 0.02928829938173294, acc: 1.0)
[2025-01-06 01:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57][root][INFO] - Training Epoch: 6/10, step 502/574 completed (loss: 0.00034281532862223685, acc: 1.0)
[2025-01-06 01:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58][root][INFO] - Training Epoch: 6/10, step 503/574 completed (loss: 0.015507025644183159, acc: 1.0)
[2025-01-06 01:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58][root][INFO] - Training Epoch: 6/10, step 504/574 completed (loss: 0.024567551910877228, acc: 1.0)
[2025-01-06 01:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58][root][INFO] - Training Epoch: 6/10, step 505/574 completed (loss: 0.3239670395851135, acc: 0.8867924809455872)
[2025-01-06 01:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59][root][INFO] - Training Epoch: 6/10, step 506/574 completed (loss: 0.04711231216788292, acc: 0.9655172228813171)
[2025-01-06 01:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59][root][INFO] - Training Epoch: 6/10, step 507/574 completed (loss: 0.8142815828323364, acc: 0.7747747898101807)
[2025-01-06 01:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00][root][INFO] - Training Epoch: 6/10, step 508/574 completed (loss: 0.3378192186355591, acc: 0.8732394576072693)
[2025-01-06 01:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00][root][INFO] - Training Epoch: 6/10, step 509/574 completed (loss: 0.0027735293842852116, acc: 1.0)
[2025-01-06 01:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00][root][INFO] - Training Epoch: 6/10, step 510/574 completed (loss: 0.003914599772542715, acc: 1.0)
[2025-01-06 01:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:01][root][INFO] - Training Epoch: 6/10, step 511/574 completed (loss: 0.03872049227356911, acc: 0.9615384340286255)
[2025-01-06 01:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:03][root][INFO] - Training Epoch: 6/10, step 512/574 completed (loss: 0.5453023910522461, acc: 0.8785714507102966)
[2025-01-06 01:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04][root][INFO] - Training Epoch: 6/10, step 513/574 completed (loss: 0.08605310320854187, acc: 0.976190447807312)
[2025-01-06 01:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04][root][INFO] - Training Epoch: 6/10, step 514/574 completed (loss: 0.19505982100963593, acc: 0.9642857313156128)
[2025-01-06 01:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:05][root][INFO] - Training Epoch: 6/10, step 515/574 completed (loss: 0.06680455803871155, acc: 0.9833333492279053)
[2025-01-06 01:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:05][root][INFO] - Training Epoch: 6/10, step 516/574 completed (loss: 0.18831540644168854, acc: 0.9305555820465088)
[2025-01-06 01:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:06][root][INFO] - Training Epoch: 6/10, step 517/574 completed (loss: 0.0001894915767479688, acc: 1.0)
[2025-01-06 01:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:06][root][INFO] - Training Epoch: 6/10, step 518/574 completed (loss: 0.00784983765333891, acc: 1.0)
[2025-01-06 01:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:06][root][INFO] - Training Epoch: 6/10, step 519/574 completed (loss: 0.10656355321407318, acc: 0.949999988079071)
[2025-01-06 01:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:07][root][INFO] - Training Epoch: 6/10, step 520/574 completed (loss: 0.0030714343301951885, acc: 1.0)
[2025-01-06 01:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08][root][INFO] - Training Epoch: 6/10, step 521/574 completed (loss: 0.5308884978294373, acc: 0.8432203531265259)
[2025-01-06 01:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08][root][INFO] - Training Epoch: 6/10, step 522/574 completed (loss: 0.18745732307434082, acc: 0.9402984976768494)
[2025-01-06 01:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08][root][INFO] - Training Epoch: 6/10, step 523/574 completed (loss: 0.14031729102134705, acc: 0.956204354763031)
[2025-01-06 01:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09][root][INFO] - Training Epoch: 6/10, step 524/574 completed (loss: 0.3421371579170227, acc: 0.8999999761581421)
[2025-01-06 01:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09][root][INFO] - Training Epoch: 6/10, step 525/574 completed (loss: 0.030333423987030983, acc: 1.0)
[2025-01-06 01:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10][root][INFO] - Training Epoch: 6/10, step 526/574 completed (loss: 0.07727991789579391, acc: 0.9807692170143127)
[2025-01-06 01:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10][root][INFO] - Training Epoch: 6/10, step 527/574 completed (loss: 0.0081782890483737, acc: 1.0)
[2025-01-06 01:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10][root][INFO] - Training Epoch: 6/10, step 528/574 completed (loss: 0.2575919032096863, acc: 0.9672130942344666)
[2025-01-06 01:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11][root][INFO] - Training Epoch: 6/10, step 529/574 completed (loss: 0.15430274605751038, acc: 0.9322034120559692)
[2025-01-06 01:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11][root][INFO] - Training Epoch: 6/10, step 530/574 completed (loss: 0.4568032920360565, acc: 0.8372092843055725)
[2025-01-06 01:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11][root][INFO] - Training Epoch: 6/10, step 531/574 completed (loss: 0.31241375207901, acc: 0.8636363744735718)
[2025-01-06 01:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12][root][INFO] - Training Epoch: 6/10, step 532/574 completed (loss: 0.35009321570396423, acc: 0.9245283007621765)
[2025-01-06 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12][root][INFO] - Training Epoch: 6/10, step 533/574 completed (loss: 0.04220636188983917, acc: 1.0)
[2025-01-06 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12][root][INFO] - Training Epoch: 6/10, step 534/574 completed (loss: 0.03806332126259804, acc: 1.0)
[2025-01-06 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13][root][INFO] - Training Epoch: 6/10, step 535/574 completed (loss: 0.05072270706295967, acc: 0.949999988079071)
[2025-01-06 01:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13][root][INFO] - Training Epoch: 6/10, step 536/574 completed (loss: 0.003374268766492605, acc: 1.0)
[2025-01-06 01:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13][root][INFO] - Training Epoch: 6/10, step 537/574 completed (loss: 0.15685094892978668, acc: 0.9538461565971375)
[2025-01-06 01:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:14][root][INFO] - Training Epoch: 6/10, step 538/574 completed (loss: 0.156529501080513, acc: 0.953125)
[2025-01-06 01:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:14][root][INFO] - Training Epoch: 6/10, step 539/574 completed (loss: 0.1716322898864746, acc: 0.90625)
[2025-01-06 01:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:14][root][INFO] - Training Epoch: 6/10, step 540/574 completed (loss: 0.5171963572502136, acc: 0.8484848737716675)
[2025-01-06 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15][root][INFO] - Training Epoch: 6/10, step 541/574 completed (loss: 0.010688583366572857, acc: 1.0)
[2025-01-06 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15][root][INFO] - Training Epoch: 6/10, step 542/574 completed (loss: 0.01519641000777483, acc: 1.0)
[2025-01-06 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15][root][INFO] - Training Epoch: 6/10, step 543/574 completed (loss: 0.004670177586376667, acc: 1.0)
[2025-01-06 01:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16][root][INFO] - Training Epoch: 6/10, step 544/574 completed (loss: 0.006316056475043297, acc: 1.0)
[2025-01-06 01:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16][root][INFO] - Training Epoch: 6/10, step 545/574 completed (loss: 0.20214857161045074, acc: 0.9756097793579102)
[2025-01-06 01:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16][root][INFO] - Training Epoch: 6/10, step 546/574 completed (loss: 0.01599574089050293, acc: 1.0)
[2025-01-06 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17][root][INFO] - Training Epoch: 6/10, step 547/574 completed (loss: 0.004126070998609066, acc: 1.0)
[2025-01-06 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17][root][INFO] - Training Epoch: 6/10, step 548/574 completed (loss: 0.02888917177915573, acc: 1.0)
[2025-01-06 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17][root][INFO] - Training Epoch: 6/10, step 549/574 completed (loss: 0.0011078729294240475, acc: 1.0)
[2025-01-06 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18][root][INFO] - Training Epoch: 6/10, step 550/574 completed (loss: 0.29548174142837524, acc: 0.8484848737716675)
[2025-01-06 01:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18][root][INFO] - Training Epoch: 6/10, step 551/574 completed (loss: 0.08210422098636627, acc: 0.9750000238418579)
[2025-01-06 01:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18][root][INFO] - Training Epoch: 6/10, step 552/574 completed (loss: 0.03925974667072296, acc: 0.9857142567634583)
[2025-01-06 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19][root][INFO] - Training Epoch: 6/10, step 553/574 completed (loss: 0.13954895734786987, acc: 0.9635036587715149)
[2025-01-06 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19][root][INFO] - Training Epoch: 6/10, step 554/574 completed (loss: 0.04128769040107727, acc: 0.9931034445762634)
[2025-01-06 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19][root][INFO] - Training Epoch: 6/10, step 555/574 completed (loss: 0.1299060732126236, acc: 0.949999988079071)
[2025-01-06 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20][root][INFO] - Training Epoch: 6/10, step 556/574 completed (loss: 0.17315785586833954, acc: 0.9602649211883545)
[2025-01-06 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20][root][INFO] - Training Epoch: 6/10, step 557/574 completed (loss: 0.030606016516685486, acc: 1.0)
[2025-01-06 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20][root][INFO] - Training Epoch: 6/10, step 558/574 completed (loss: 0.2112058848142624, acc: 0.9599999785423279)
[2025-01-06 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21][root][INFO] - Training Epoch: 6/10, step 559/574 completed (loss: 0.22577546536922455, acc: 0.9230769276618958)
[2025-01-06 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21][root][INFO] - Training Epoch: 6/10, step 560/574 completed (loss: 0.0010589960729703307, acc: 1.0)
[2025-01-06 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21][root][INFO] - Training Epoch: 6/10, step 561/574 completed (loss: 0.0715101808309555, acc: 0.9743589758872986)
[2025-01-06 01:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:49][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3942, device='cuda:0') eval_epoch_loss=tensor(0.8730, device='cuda:0') eval_epoch_acc=tensor(0.8214, device='cuda:0')
[2025-01-06 01:34:49][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:34:49][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:34:49][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_562_loss_0.8730292916297913/model.pt
[2025-01-06 01:34:49][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50][root][INFO] - Training Epoch: 6/10, step 562/574 completed (loss: 0.18322928249835968, acc: 0.9444444179534912)
[2025-01-06 01:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50][root][INFO] - Training Epoch: 6/10, step 563/574 completed (loss: 0.1722952425479889, acc: 0.9350649118423462)
[2025-01-06 01:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50][root][INFO] - Training Epoch: 6/10, step 564/574 completed (loss: 0.1499340683221817, acc: 0.9166666865348816)
[2025-01-06 01:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51][root][INFO] - Training Epoch: 6/10, step 565/574 completed (loss: 0.01855316571891308, acc: 1.0)
[2025-01-06 01:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51][root][INFO] - Training Epoch: 6/10, step 566/574 completed (loss: 0.09485635161399841, acc: 0.976190447807312)
[2025-01-06 01:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51][root][INFO] - Training Epoch: 6/10, step 567/574 completed (loss: 0.009251946583390236, acc: 1.0)
[2025-01-06 01:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52][root][INFO] - Training Epoch: 6/10, step 568/574 completed (loss: 0.06165238097310066, acc: 0.9629629850387573)
[2025-01-06 01:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52][root][INFO] - Training Epoch: 6/10, step 569/574 completed (loss: 0.10810335725545883, acc: 0.9732620120048523)
[2025-01-06 01:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52][root][INFO] - Training Epoch: 6/10, step 570/574 completed (loss: 0.013071156106889248, acc: 1.0)
[2025-01-06 01:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53][root][INFO] - Training Epoch: 6/10, step 571/574 completed (loss: 0.08009839057922363, acc: 0.9572649598121643)
[2025-01-06 01:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53][root][INFO] - Training Epoch: 6/10, step 572/574 completed (loss: 0.18555691838264465, acc: 0.954081654548645)
[2025-01-06 01:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53][root][INFO] - Training Epoch: 6/10, step 573/574 completed (loss: 0.12100633233785629, acc: 0.9748427867889404)
[2025-01-06 01:34:54][slam_llm.utils.train_utils][INFO] - Epoch 6: train_perplexity=1.1797, train_epoch_loss=0.1653, epoch time 333.7258621901274s
[2025-01-06 01:34:54][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:34:54][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:34:54][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:34:54][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 17
[2025-01-06 01:34:54][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55][root][INFO] - Training Epoch: 7/10, step 0/574 completed (loss: 0.008611262775957584, acc: 1.0)
[2025-01-06 01:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55][root][INFO] - Training Epoch: 7/10, step 1/574 completed (loss: 0.04492209106683731, acc: 1.0)
[2025-01-06 01:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55][root][INFO] - Training Epoch: 7/10, step 2/574 completed (loss: 0.13652461767196655, acc: 0.9459459185600281)
[2025-01-06 01:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56][root][INFO] - Training Epoch: 7/10, step 3/574 completed (loss: 0.12535855174064636, acc: 0.9473684430122375)
[2025-01-06 01:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56][root][INFO] - Training Epoch: 7/10, step 4/574 completed (loss: 0.05399925634264946, acc: 0.9729729890823364)
[2025-01-06 01:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56][root][INFO] - Training Epoch: 7/10, step 5/574 completed (loss: 0.024929210543632507, acc: 1.0)
[2025-01-06 01:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57][root][INFO] - Training Epoch: 7/10, step 6/574 completed (loss: 0.16702358424663544, acc: 0.918367326259613)
[2025-01-06 01:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57][root][INFO] - Training Epoch: 7/10, step 7/574 completed (loss: 0.050304777920246124, acc: 1.0)
[2025-01-06 01:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57][root][INFO] - Training Epoch: 7/10, step 8/574 completed (loss: 0.001623420394025743, acc: 1.0)
[2025-01-06 01:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57][root][INFO] - Training Epoch: 7/10, step 9/574 completed (loss: 0.004372281953692436, acc: 1.0)
[2025-01-06 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58][root][INFO] - Training Epoch: 7/10, step 10/574 completed (loss: 0.0030760338995605707, acc: 1.0)
[2025-01-06 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58][root][INFO] - Training Epoch: 7/10, step 11/574 completed (loss: 0.04023264721035957, acc: 0.9743589758872986)
[2025-01-06 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58][root][INFO] - Training Epoch: 7/10, step 12/574 completed (loss: 0.02077857032418251, acc: 1.0)
[2025-01-06 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59][root][INFO] - Training Epoch: 7/10, step 13/574 completed (loss: 0.029862938448786736, acc: 0.97826087474823)
[2025-01-06 01:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59][root][INFO] - Training Epoch: 7/10, step 14/574 completed (loss: 0.045195575803518295, acc: 0.9803921580314636)
[2025-01-06 01:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59][root][INFO] - Training Epoch: 7/10, step 15/574 completed (loss: 0.042509160935878754, acc: 1.0)
[2025-01-06 01:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00][root][INFO] - Training Epoch: 7/10, step 16/574 completed (loss: 0.009572403505444527, acc: 1.0)
[2025-01-06 01:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00][root][INFO] - Training Epoch: 7/10, step 17/574 completed (loss: 0.003962119575589895, acc: 1.0)
[2025-01-06 01:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00][root][INFO] - Training Epoch: 7/10, step 18/574 completed (loss: 0.08420254290103912, acc: 0.9722222089767456)
[2025-01-06 01:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01][root][INFO] - Training Epoch: 7/10, step 19/574 completed (loss: 0.011062594130635262, acc: 1.0)
[2025-01-06 01:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01][root][INFO] - Training Epoch: 7/10, step 20/574 completed (loss: 0.012713223695755005, acc: 1.0)
[2025-01-06 01:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01][root][INFO] - Training Epoch: 7/10, step 21/574 completed (loss: 0.05400039255619049, acc: 0.9655172228813171)
[2025-01-06 01:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:02][root][INFO] - Training Epoch: 7/10, step 22/574 completed (loss: 0.047967538237571716, acc: 0.9599999785423279)
[2025-01-06 01:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:02][root][INFO] - Training Epoch: 7/10, step 23/574 completed (loss: 0.004906706977635622, acc: 1.0)
[2025-01-06 01:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:02][root][INFO] - Training Epoch: 7/10, step 24/574 completed (loss: 0.006794271059334278, acc: 1.0)
[2025-01-06 01:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03][root][INFO] - Training Epoch: 7/10, step 25/574 completed (loss: 0.036085087805986404, acc: 0.9811320900917053)
[2025-01-06 01:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03][root][INFO] - Training Epoch: 7/10, step 26/574 completed (loss: 0.14997388422489166, acc: 0.931506872177124)
[2025-01-06 01:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04][root][INFO] - Training Epoch: 7/10, step 27/574 completed (loss: 0.5491489768028259, acc: 0.8221343755722046)
[2025-01-06 01:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04][root][INFO] - Training Epoch: 7/10, step 28/574 completed (loss: 0.022655662149190903, acc: 1.0)
[2025-01-06 01:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05][root][INFO] - Training Epoch: 7/10, step 29/574 completed (loss: 0.09424316138029099, acc: 0.9638554453849792)
[2025-01-06 01:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05][root][INFO] - Training Epoch: 7/10, step 30/574 completed (loss: 0.046476107090711594, acc: 0.9753086566925049)
[2025-01-06 01:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05][root][INFO] - Training Epoch: 7/10, step 31/574 completed (loss: 0.10567981749773026, acc: 0.9642857313156128)
[2025-01-06 01:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06][root][INFO] - Training Epoch: 7/10, step 32/574 completed (loss: 0.0020309702958911657, acc: 1.0)
[2025-01-06 01:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06][root][INFO] - Training Epoch: 7/10, step 33/574 completed (loss: 0.002814924344420433, acc: 1.0)
[2025-01-06 01:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06][root][INFO] - Training Epoch: 7/10, step 34/574 completed (loss: 0.075599305331707, acc: 0.9747899174690247)
[2025-01-06 01:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07][root][INFO] - Training Epoch: 7/10, step 35/574 completed (loss: 0.03883478790521622, acc: 0.9836065769195557)
[2025-01-06 01:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07][root][INFO] - Training Epoch: 7/10, step 36/574 completed (loss: 0.05823792517185211, acc: 0.9841269850730896)
[2025-01-06 01:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07][root][INFO] - Training Epoch: 7/10, step 37/574 completed (loss: 0.015441126190125942, acc: 1.0)
[2025-01-06 01:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08][root][INFO] - Training Epoch: 7/10, step 38/574 completed (loss: 0.08139043301343918, acc: 0.9655172228813171)
[2025-01-06 01:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08][root][INFO] - Training Epoch: 7/10, step 39/574 completed (loss: 0.24215716123580933, acc: 0.9523809552192688)
[2025-01-06 01:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08][root][INFO] - Training Epoch: 7/10, step 40/574 completed (loss: 0.09184591472148895, acc: 0.9615384340286255)
[2025-01-06 01:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09][root][INFO] - Training Epoch: 7/10, step 41/574 completed (loss: 0.09111896902322769, acc: 0.9594594836235046)
[2025-01-06 01:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09][root][INFO] - Training Epoch: 7/10, step 42/574 completed (loss: 0.18494157493114471, acc: 0.9538461565971375)
[2025-01-06 01:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10][root][INFO] - Training Epoch: 7/10, step 43/574 completed (loss: 0.170636385679245, acc: 0.9494949579238892)
[2025-01-06 01:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10][root][INFO] - Training Epoch: 7/10, step 44/574 completed (loss: 0.10995933413505554, acc: 0.9484536051750183)
[2025-01-06 01:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10][root][INFO] - Training Epoch: 7/10, step 45/574 completed (loss: 0.1081048771739006, acc: 0.970588207244873)
[2025-01-06 01:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11][root][INFO] - Training Epoch: 7/10, step 46/574 completed (loss: 0.01087137870490551, acc: 1.0)
[2025-01-06 01:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11][root][INFO] - Training Epoch: 7/10, step 47/574 completed (loss: 0.0025567507836967707, acc: 1.0)
[2025-01-06 01:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11][root][INFO] - Training Epoch: 7/10, step 48/574 completed (loss: 0.0031841027084738016, acc: 1.0)
[2025-01-06 01:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12][root][INFO] - Training Epoch: 7/10, step 49/574 completed (loss: 0.0030204772483557463, acc: 1.0)
[2025-01-06 01:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12][root][INFO] - Training Epoch: 7/10, step 50/574 completed (loss: 0.15646547079086304, acc: 0.9473684430122375)
[2025-01-06 01:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12][root][INFO] - Training Epoch: 7/10, step 51/574 completed (loss: 0.0811251550912857, acc: 0.9841269850730896)
[2025-01-06 01:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13][root][INFO] - Training Epoch: 7/10, step 52/574 completed (loss: 0.1576269418001175, acc: 0.9295774698257446)
[2025-01-06 01:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13][root][INFO] - Training Epoch: 7/10, step 53/574 completed (loss: 0.675944447517395, acc: 0.7666666507720947)
[2025-01-06 01:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13][root][INFO] - Training Epoch: 7/10, step 54/574 completed (loss: 0.0695224180817604, acc: 0.9729729890823364)
[2025-01-06 01:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:14][root][INFO] - Training Epoch: 7/10, step 55/574 completed (loss: 0.013515312224626541, acc: 1.0)
[2025-01-06 01:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:17][root][INFO] - Training Epoch: 7/10, step 56/574 completed (loss: 0.7208430171012878, acc: 0.7952218651771545)
[2025-01-06 01:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18][root][INFO] - Training Epoch: 7/10, step 57/574 completed (loss: 0.948214590549469, acc: 0.7254902124404907)
[2025-01-06 01:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18][root][INFO] - Training Epoch: 7/10, step 58/574 completed (loss: 0.4078512489795685, acc: 0.8465909361839294)
[2025-01-06 01:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:19][root][INFO] - Training Epoch: 7/10, step 59/574 completed (loss: 0.12048635631799698, acc: 0.9485294222831726)
[2025-01-06 01:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20][root][INFO] - Training Epoch: 7/10, step 60/574 completed (loss: 0.41678640246391296, acc: 0.8695651888847351)
[2025-01-06 01:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20][root][INFO] - Training Epoch: 7/10, step 61/574 completed (loss: 0.17407551407814026, acc: 0.949999988079071)
[2025-01-06 01:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20][root][INFO] - Training Epoch: 7/10, step 62/574 completed (loss: 0.016828546300530434, acc: 1.0)
[2025-01-06 01:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21][root][INFO] - Training Epoch: 7/10, step 63/574 completed (loss: 0.044172197580337524, acc: 1.0)
[2025-01-06 01:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21][root][INFO] - Training Epoch: 7/10, step 64/574 completed (loss: 0.2077743113040924, acc: 0.96875)
[2025-01-06 01:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21][root][INFO] - Training Epoch: 7/10, step 65/574 completed (loss: 0.03916146978735924, acc: 1.0)
[2025-01-06 01:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22][root][INFO] - Training Epoch: 7/10, step 66/574 completed (loss: 0.05443638190627098, acc: 0.9821428656578064)
[2025-01-06 01:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22][root][INFO] - Training Epoch: 7/10, step 67/574 completed (loss: 0.07475534826517105, acc: 0.9666666388511658)
[2025-01-06 01:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22][root][INFO] - Training Epoch: 7/10, step 68/574 completed (loss: 0.004897124134004116, acc: 1.0)
[2025-01-06 01:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23][root][INFO] - Training Epoch: 7/10, step 69/574 completed (loss: 0.04406460374593735, acc: 1.0)
[2025-01-06 01:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23][root][INFO] - Training Epoch: 7/10, step 70/574 completed (loss: 0.0611802376806736, acc: 0.9696969985961914)
[2025-01-06 01:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23][root][INFO] - Training Epoch: 7/10, step 71/574 completed (loss: 0.36002057790756226, acc: 0.8676470518112183)
[2025-01-06 01:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24][root][INFO] - Training Epoch: 7/10, step 72/574 completed (loss: 0.19169022142887115, acc: 0.9285714030265808)
[2025-01-06 01:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24][root][INFO] - Training Epoch: 7/10, step 73/574 completed (loss: 0.6809706687927246, acc: 0.8307692408561707)
[2025-01-06 01:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24][root][INFO] - Training Epoch: 7/10, step 74/574 completed (loss: 0.34492048621177673, acc: 0.9081632494926453)
[2025-01-06 01:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25][root][INFO] - Training Epoch: 7/10, step 75/574 completed (loss: 0.3766469657421112, acc: 0.888059675693512)
[2025-01-06 01:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25][root][INFO] - Training Epoch: 7/10, step 76/574 completed (loss: 0.9450035095214844, acc: 0.7518247961997986)
[2025-01-06 01:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25][root][INFO] - Training Epoch: 7/10, step 77/574 completed (loss: 0.004178621806204319, acc: 1.0)
[2025-01-06 01:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26][root][INFO] - Training Epoch: 7/10, step 78/574 completed (loss: 0.003068129299208522, acc: 1.0)
[2025-01-06 01:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26][root][INFO] - Training Epoch: 7/10, step 79/574 completed (loss: 0.007396772038191557, acc: 1.0)
[2025-01-06 01:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26][root][INFO] - Training Epoch: 7/10, step 80/574 completed (loss: 0.001419399632140994, acc: 1.0)
[2025-01-06 01:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27][root][INFO] - Training Epoch: 7/10, step 81/574 completed (loss: 0.05226580426096916, acc: 1.0)
[2025-01-06 01:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27][root][INFO] - Training Epoch: 7/10, step 82/574 completed (loss: 0.07510416209697723, acc: 0.9807692170143127)
[2025-01-06 01:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27][root][INFO] - Training Epoch: 7/10, step 83/574 completed (loss: 0.0436381995677948, acc: 1.0)
[2025-01-06 01:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28][root][INFO] - Training Epoch: 7/10, step 84/574 completed (loss: 0.0393863171339035, acc: 1.0)
[2025-01-06 01:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28][root][INFO] - Training Epoch: 7/10, step 85/574 completed (loss: 0.09344083815813065, acc: 1.0)
[2025-01-06 01:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28][root][INFO] - Training Epoch: 7/10, step 86/574 completed (loss: 0.007867999374866486, acc: 1.0)
[2025-01-06 01:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29][root][INFO] - Training Epoch: 7/10, step 87/574 completed (loss: 0.21156752109527588, acc: 0.9599999785423279)
[2025-01-06 01:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29][root][INFO] - Training Epoch: 7/10, step 88/574 completed (loss: 0.30888569355010986, acc: 0.893203854560852)
[2025-01-06 01:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:30][root][INFO] - Training Epoch: 7/10, step 89/574 completed (loss: 0.4925156831741333, acc: 0.8689320683479309)
[2025-01-06 01:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:31][root][INFO] - Training Epoch: 7/10, step 90/574 completed (loss: 0.4551818072795868, acc: 0.8709677457809448)
[2025-01-06 01:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:32][root][INFO] - Training Epoch: 7/10, step 91/574 completed (loss: 0.5213495492935181, acc: 0.8491379022598267)
[2025-01-06 01:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:32][root][INFO] - Training Epoch: 7/10, step 92/574 completed (loss: 0.38135334849357605, acc: 0.8842105269432068)
[2025-01-06 01:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:33][root][INFO] - Training Epoch: 7/10, step 93/574 completed (loss: 0.337309330701828, acc: 0.8712871074676514)
[2025-01-06 01:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34][root][INFO] - Training Epoch: 7/10, step 94/574 completed (loss: 0.3079391419887543, acc: 0.8870967626571655)
[2025-01-06 01:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34][root][INFO] - Training Epoch: 7/10, step 95/574 completed (loss: 0.21906302869319916, acc: 0.95652174949646)
[2025-01-06 01:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34][root][INFO] - Training Epoch: 7/10, step 96/574 completed (loss: 0.3232272267341614, acc: 0.8907563090324402)
[2025-01-06 01:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35][root][INFO] - Training Epoch: 7/10, step 97/574 completed (loss: 0.30108463764190674, acc: 0.8846153616905212)
[2025-01-06 01:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35][root][INFO] - Training Epoch: 7/10, step 98/574 completed (loss: 0.38185954093933105, acc: 0.8686131238937378)
[2025-01-06 01:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35][root][INFO] - Training Epoch: 7/10, step 99/574 completed (loss: 0.27770787477493286, acc: 0.9552238583564758)
[2025-01-06 01:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36][root][INFO] - Training Epoch: 7/10, step 100/574 completed (loss: 0.0025447760708630085, acc: 1.0)
[2025-01-06 01:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36][root][INFO] - Training Epoch: 7/10, step 101/574 completed (loss: 0.003563457168638706, acc: 1.0)
[2025-01-06 01:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36][root][INFO] - Training Epoch: 7/10, step 102/574 completed (loss: 0.03735300898551941, acc: 1.0)
[2025-01-06 01:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37][root][INFO] - Training Epoch: 7/10, step 103/574 completed (loss: 0.0033564353361725807, acc: 1.0)
[2025-01-06 01:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37][root][INFO] - Training Epoch: 7/10, step 104/574 completed (loss: 0.06969104707241058, acc: 0.982758641242981)
[2025-01-06 01:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37][root][INFO] - Training Epoch: 7/10, step 105/574 completed (loss: 0.0030898465774953365, acc: 1.0)
[2025-01-06 01:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:38][root][INFO] - Training Epoch: 7/10, step 106/574 completed (loss: 0.009862677194178104, acc: 1.0)
[2025-01-06 01:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:38][root][INFO] - Training Epoch: 7/10, step 107/574 completed (loss: 0.00813988782465458, acc: 1.0)
[2025-01-06 01:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39][root][INFO] - Training Epoch: 7/10, step 108/574 completed (loss: 0.0007828667294234037, acc: 1.0)
[2025-01-06 01:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39][root][INFO] - Training Epoch: 7/10, step 109/574 completed (loss: 0.004611083306372166, acc: 1.0)
[2025-01-06 01:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39][root][INFO] - Training Epoch: 7/10, step 110/574 completed (loss: 0.03423811122775078, acc: 0.9846153855323792)
[2025-01-06 01:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40][root][INFO] - Training Epoch: 7/10, step 111/574 completed (loss: 0.03766053915023804, acc: 1.0)
[2025-01-06 01:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40][root][INFO] - Training Epoch: 7/10, step 112/574 completed (loss: 0.19344238936901093, acc: 0.9649122953414917)
[2025-01-06 01:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40][root][INFO] - Training Epoch: 7/10, step 113/574 completed (loss: 0.04013235867023468, acc: 1.0)
[2025-01-06 01:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41][root][INFO] - Training Epoch: 7/10, step 114/574 completed (loss: 0.17811378836631775, acc: 0.9591836929321289)
[2025-01-06 01:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41][root][INFO] - Training Epoch: 7/10, step 115/574 completed (loss: 0.010346971452236176, acc: 1.0)
[2025-01-06 01:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41][root][INFO] - Training Epoch: 7/10, step 116/574 completed (loss: 0.2491113245487213, acc: 0.9523809552192688)
[2025-01-06 01:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:42][root][INFO] - Training Epoch: 7/10, step 117/574 completed (loss: 0.1662766933441162, acc: 0.9430894255638123)
[2025-01-06 01:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:42][root][INFO] - Training Epoch: 7/10, step 118/574 completed (loss: 0.11246758699417114, acc: 0.9677419066429138)
[2025-01-06 01:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43][root][INFO] - Training Epoch: 7/10, step 119/574 completed (loss: 0.3168603181838989, acc: 0.9201520681381226)
[2025-01-06 01:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43][root][INFO] - Training Epoch: 7/10, step 120/574 completed (loss: 0.1910942792892456, acc: 0.9599999785423279)
[2025-01-06 01:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44][root][INFO] - Training Epoch: 7/10, step 121/574 completed (loss: 0.0688038021326065, acc: 0.942307710647583)
[2025-01-06 01:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44][root][INFO] - Training Epoch: 7/10, step 122/574 completed (loss: 0.007112998049706221, acc: 1.0)
[2025-01-06 01:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45][root][INFO] - Training Epoch: 7/10, step 123/574 completed (loss: 0.0029035997577011585, acc: 1.0)
[2025-01-06 01:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45][root][INFO] - Training Epoch: 7/10, step 124/574 completed (loss: 0.43226760625839233, acc: 0.8650306463241577)
[2025-01-06 01:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45][root][INFO] - Training Epoch: 7/10, step 125/574 completed (loss: 0.41424429416656494, acc: 0.8819444179534912)
[2025-01-06 01:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45][root][INFO] - Training Epoch: 7/10, step 126/574 completed (loss: 0.45941364765167236, acc: 0.8333333134651184)
[2025-01-06 01:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46][root][INFO] - Training Epoch: 7/10, step 127/574 completed (loss: 0.24616625905036926, acc: 0.9226190447807312)
[2025-01-06 01:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46][root][INFO] - Training Epoch: 7/10, step 128/574 completed (loss: 0.37599295377731323, acc: 0.9025641083717346)
[2025-01-06 01:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47][root][INFO] - Training Epoch: 7/10, step 129/574 completed (loss: 0.3830764591693878, acc: 0.8970588445663452)
[2025-01-06 01:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47][root][INFO] - Training Epoch: 7/10, step 130/574 completed (loss: 0.0185730941593647, acc: 1.0)
[2025-01-06 01:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:15][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2741, device='cuda:0') eval_epoch_loss=tensor(0.8216, device='cuda:0') eval_epoch_acc=tensor(0.8304, device='cuda:0')
[2025-01-06 01:36:15][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:36:15][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:36:15][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_131_loss_0.8215715289115906/model.pt
[2025-01-06 01:36:15][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16][root][INFO] - Training Epoch: 7/10, step 131/574 completed (loss: 0.01643170416355133, acc: 1.0)
[2025-01-06 01:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16][root][INFO] - Training Epoch: 7/10, step 132/574 completed (loss: 0.24116280674934387, acc: 0.875)
[2025-01-06 01:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16][root][INFO] - Training Epoch: 7/10, step 133/574 completed (loss: 0.1345646232366562, acc: 0.95652174949646)
[2025-01-06 01:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:17][root][INFO] - Training Epoch: 7/10, step 134/574 completed (loss: 0.03838033601641655, acc: 0.9714285731315613)
[2025-01-06 01:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:17][root][INFO] - Training Epoch: 7/10, step 135/574 completed (loss: 0.01135534979403019, acc: 1.0)
[2025-01-06 01:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:17][root][INFO] - Training Epoch: 7/10, step 136/574 completed (loss: 0.0781264528632164, acc: 0.976190447807312)
[2025-01-06 01:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:18][root][INFO] - Training Epoch: 7/10, step 137/574 completed (loss: 0.11003577709197998, acc: 0.9666666388511658)
[2025-01-06 01:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:18][root][INFO] - Training Epoch: 7/10, step 138/574 completed (loss: 0.0163691695779562, acc: 1.0)
[2025-01-06 01:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:18][root][INFO] - Training Epoch: 7/10, step 139/574 completed (loss: 0.015326546505093575, acc: 1.0)
[2025-01-06 01:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19][root][INFO] - Training Epoch: 7/10, step 140/574 completed (loss: 0.005089260172098875, acc: 1.0)
[2025-01-06 01:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19][root][INFO] - Training Epoch: 7/10, step 141/574 completed (loss: 0.12333638966083527, acc: 0.9677419066429138)
[2025-01-06 01:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19][root][INFO] - Training Epoch: 7/10, step 142/574 completed (loss: 0.3184846043586731, acc: 0.9459459185600281)
[2025-01-06 01:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:20][root][INFO] - Training Epoch: 7/10, step 143/574 completed (loss: 0.21254034340381622, acc: 0.9210526347160339)
[2025-01-06 01:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:20][root][INFO] - Training Epoch: 7/10, step 144/574 completed (loss: 0.4064680337905884, acc: 0.8507462739944458)
[2025-01-06 01:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21][root][INFO] - Training Epoch: 7/10, step 145/574 completed (loss: 0.37581878900527954, acc: 0.8775510191917419)
[2025-01-06 01:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21][root][INFO] - Training Epoch: 7/10, step 146/574 completed (loss: 0.29378801584243774, acc: 0.914893627166748)
[2025-01-06 01:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21][root][INFO] - Training Epoch: 7/10, step 147/574 completed (loss: 0.20369461178779602, acc: 0.9285714030265808)
[2025-01-06 01:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22][root][INFO] - Training Epoch: 7/10, step 148/574 completed (loss: 0.13071571290493011, acc: 0.9642857313156128)
[2025-01-06 01:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22][root][INFO] - Training Epoch: 7/10, step 149/574 completed (loss: 0.15155360102653503, acc: 0.95652174949646)
[2025-01-06 01:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22][root][INFO] - Training Epoch: 7/10, step 150/574 completed (loss: 0.05402267724275589, acc: 0.9655172228813171)
[2025-01-06 01:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23][root][INFO] - Training Epoch: 7/10, step 151/574 completed (loss: 0.2280484288930893, acc: 0.95652174949646)
[2025-01-06 01:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23][root][INFO] - Training Epoch: 7/10, step 152/574 completed (loss: 0.3245638608932495, acc: 0.9152542352676392)
[2025-01-06 01:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23][root][INFO] - Training Epoch: 7/10, step 153/574 completed (loss: 0.0884610041975975, acc: 0.9824561476707458)
[2025-01-06 01:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24][root][INFO] - Training Epoch: 7/10, step 154/574 completed (loss: 0.1073869839310646, acc: 0.9594594836235046)
[2025-01-06 01:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24][root][INFO] - Training Epoch: 7/10, step 155/574 completed (loss: 0.01943925954401493, acc: 1.0)
[2025-01-06 01:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24][root][INFO] - Training Epoch: 7/10, step 156/574 completed (loss: 0.002422221004962921, acc: 1.0)
[2025-01-06 01:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24][root][INFO] - Training Epoch: 7/10, step 157/574 completed (loss: 0.31721460819244385, acc: 0.8947368264198303)
[2025-01-06 01:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:26][root][INFO] - Training Epoch: 7/10, step 158/574 completed (loss: 0.3330450654029846, acc: 0.8918918967247009)
[2025-01-06 01:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:26][root][INFO] - Training Epoch: 7/10, step 159/574 completed (loss: 0.3241678476333618, acc: 0.9074074029922485)
[2025-01-06 01:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27][root][INFO] - Training Epoch: 7/10, step 160/574 completed (loss: 0.2905013859272003, acc: 0.9418604373931885)
[2025-01-06 01:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27][root][INFO] - Training Epoch: 7/10, step 161/574 completed (loss: 0.45708218216896057, acc: 0.8470588326454163)
[2025-01-06 01:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28][root][INFO] - Training Epoch: 7/10, step 162/574 completed (loss: 0.5151668190956116, acc: 0.8876404762268066)
[2025-01-06 01:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28][root][INFO] - Training Epoch: 7/10, step 163/574 completed (loss: 0.10221819579601288, acc: 0.9772727489471436)
[2025-01-06 01:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29][root][INFO] - Training Epoch: 7/10, step 164/574 completed (loss: 0.42192980647087097, acc: 0.9047619104385376)
[2025-01-06 01:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29][root][INFO] - Training Epoch: 7/10, step 165/574 completed (loss: 0.21392059326171875, acc: 0.9655172228813171)
[2025-01-06 01:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29][root][INFO] - Training Epoch: 7/10, step 166/574 completed (loss: 0.01514999009668827, acc: 1.0)
[2025-01-06 01:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:30][root][INFO] - Training Epoch: 7/10, step 167/574 completed (loss: 0.0622778981924057, acc: 0.9599999785423279)
[2025-01-06 01:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:30][root][INFO] - Training Epoch: 7/10, step 168/574 completed (loss: 0.15540280938148499, acc: 0.9444444179534912)
[2025-01-06 01:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:31][root][INFO] - Training Epoch: 7/10, step 169/574 completed (loss: 0.3630545735359192, acc: 0.9019607901573181)
[2025-01-06 01:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32][root][INFO] - Training Epoch: 7/10, step 170/574 completed (loss: 0.2787523567676544, acc: 0.9178082346916199)
[2025-01-06 01:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32][root][INFO] - Training Epoch: 7/10, step 171/574 completed (loss: 0.0035816107410937548, acc: 1.0)
[2025-01-06 01:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32][root][INFO] - Training Epoch: 7/10, step 172/574 completed (loss: 0.1462947130203247, acc: 0.9259259104728699)
[2025-01-06 01:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33][root][INFO] - Training Epoch: 7/10, step 173/574 completed (loss: 0.09986448287963867, acc: 0.9285714030265808)
[2025-01-06 01:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33][root][INFO] - Training Epoch: 7/10, step 174/574 completed (loss: 0.4533800780773163, acc: 0.8584070801734924)
[2025-01-06 01:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33][root][INFO] - Training Epoch: 7/10, step 175/574 completed (loss: 0.2868666648864746, acc: 0.9130434989929199)
[2025-01-06 01:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:34][root][INFO] - Training Epoch: 7/10, step 176/574 completed (loss: 0.10542468726634979, acc: 0.9772727489471436)
[2025-01-06 01:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:35][root][INFO] - Training Epoch: 7/10, step 177/574 completed (loss: 0.45361557602882385, acc: 0.8549618124961853)
[2025-01-06 01:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:35][root][INFO] - Training Epoch: 7/10, step 178/574 completed (loss: 0.3971753418445587, acc: 0.8962963223457336)
[2025-01-06 01:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:36][root][INFO] - Training Epoch: 7/10, step 179/574 completed (loss: 0.060721270740032196, acc: 0.9836065769195557)
[2025-01-06 01:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:36][root][INFO] - Training Epoch: 7/10, step 180/574 completed (loss: 0.011002481915056705, acc: 1.0)
[2025-01-06 01:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37][root][INFO] - Training Epoch: 7/10, step 181/574 completed (loss: 0.0038733475375920534, acc: 1.0)
[2025-01-06 01:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37][root][INFO] - Training Epoch: 7/10, step 182/574 completed (loss: 0.005207199603319168, acc: 1.0)
[2025-01-06 01:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37][root][INFO] - Training Epoch: 7/10, step 183/574 completed (loss: 0.0433916300535202, acc: 0.9878048896789551)
[2025-01-06 01:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38][root][INFO] - Training Epoch: 7/10, step 184/574 completed (loss: 0.23951898515224457, acc: 0.939577043056488)
[2025-01-06 01:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38][root][INFO] - Training Epoch: 7/10, step 185/574 completed (loss: 0.23440389335155487, acc: 0.9250720739364624)
[2025-01-06 01:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38][root][INFO] - Training Epoch: 7/10, step 186/574 completed (loss: 0.212742879986763, acc: 0.9125000238418579)
[2025-01-06 01:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39][root][INFO] - Training Epoch: 7/10, step 187/574 completed (loss: 0.3399502635002136, acc: 0.9024389982223511)
[2025-01-06 01:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39][root][INFO] - Training Epoch: 7/10, step 188/574 completed (loss: 0.24383263289928436, acc: 0.9252669215202332)
[2025-01-06 01:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:40][root][INFO] - Training Epoch: 7/10, step 189/574 completed (loss: 0.45733898878097534, acc: 0.8799999952316284)
[2025-01-06 01:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:40][root][INFO] - Training Epoch: 7/10, step 190/574 completed (loss: 0.25754037499427795, acc: 0.8720930218696594)
[2025-01-06 01:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:41][root][INFO] - Training Epoch: 7/10, step 191/574 completed (loss: 0.52273029088974, acc: 0.841269850730896)
[2025-01-06 01:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:42][root][INFO] - Training Epoch: 7/10, step 192/574 completed (loss: 0.46480557322502136, acc: 0.8712121248245239)
[2025-01-06 01:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:43][root][INFO] - Training Epoch: 7/10, step 193/574 completed (loss: 0.2984318137168884, acc: 0.929411768913269)
[2025-01-06 01:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:44][root][INFO] - Training Epoch: 7/10, step 194/574 completed (loss: 0.4645494520664215, acc: 0.8888888955116272)
[2025-01-06 01:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45][root][INFO] - Training Epoch: 7/10, step 195/574 completed (loss: 0.1913248896598816, acc: 0.9354838728904724)
[2025-01-06 01:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45][root][INFO] - Training Epoch: 7/10, step 196/574 completed (loss: 0.0295404102653265, acc: 1.0)
[2025-01-06 01:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45][root][INFO] - Training Epoch: 7/10, step 197/574 completed (loss: 0.38598787784576416, acc: 0.9750000238418579)
[2025-01-06 01:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46][root][INFO] - Training Epoch: 7/10, step 198/574 completed (loss: 0.19664731621742249, acc: 0.9411764740943909)
[2025-01-06 01:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46][root][INFO] - Training Epoch: 7/10, step 199/574 completed (loss: 0.4027273654937744, acc: 0.875)
[2025-01-06 01:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46][root][INFO] - Training Epoch: 7/10, step 200/574 completed (loss: 0.3227888345718384, acc: 0.8898305296897888)
[2025-01-06 01:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47][root][INFO] - Training Epoch: 7/10, step 201/574 completed (loss: 0.35526448488235474, acc: 0.8656716346740723)
[2025-01-06 01:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47][root][INFO] - Training Epoch: 7/10, step 202/574 completed (loss: 0.31312936544418335, acc: 0.893203854560852)
[2025-01-06 01:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47][root][INFO] - Training Epoch: 7/10, step 203/574 completed (loss: 0.16978785395622253, acc: 0.9523809552192688)
[2025-01-06 01:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48][root][INFO] - Training Epoch: 7/10, step 204/574 completed (loss: 0.0286257266998291, acc: 0.9890109896659851)
[2025-01-06 01:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48][root][INFO] - Training Epoch: 7/10, step 205/574 completed (loss: 0.10138386487960815, acc: 0.9596412777900696)
[2025-01-06 01:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49][root][INFO] - Training Epoch: 7/10, step 206/574 completed (loss: 0.1801345944404602, acc: 0.9488189220428467)
[2025-01-06 01:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49][root][INFO] - Training Epoch: 7/10, step 207/574 completed (loss: 0.12298641353845596, acc: 0.9655172228813171)
[2025-01-06 01:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49][root][INFO] - Training Epoch: 7/10, step 208/574 completed (loss: 0.19958843290805817, acc: 0.95652174949646)
[2025-01-06 01:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50][root][INFO] - Training Epoch: 7/10, step 209/574 completed (loss: 0.137994185090065, acc: 0.957198441028595)
[2025-01-06 01:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50][root][INFO] - Training Epoch: 7/10, step 210/574 completed (loss: 0.07592028379440308, acc: 0.967391312122345)
[2025-01-06 01:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50][root][INFO] - Training Epoch: 7/10, step 211/574 completed (loss: 0.007457793224602938, acc: 1.0)
[2025-01-06 01:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51][root][INFO] - Training Epoch: 7/10, step 212/574 completed (loss: 0.004288928117603064, acc: 1.0)
[2025-01-06 01:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51][root][INFO] - Training Epoch: 7/10, step 213/574 completed (loss: 0.06255467981100082, acc: 0.978723406791687)
[2025-01-06 01:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52][root][INFO] - Training Epoch: 7/10, step 214/574 completed (loss: 0.06998766958713531, acc: 0.9846153855323792)
[2025-01-06 01:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52][root][INFO] - Training Epoch: 7/10, step 215/574 completed (loss: 0.11417503654956818, acc: 0.9729729890823364)
[2025-01-06 01:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53][root][INFO] - Training Epoch: 7/10, step 216/574 completed (loss: 0.036987099796533585, acc: 0.9883720874786377)
[2025-01-06 01:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53][root][INFO] - Training Epoch: 7/10, step 217/574 completed (loss: 0.12547707557678223, acc: 0.9639639854431152)
[2025-01-06 01:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53][root][INFO] - Training Epoch: 7/10, step 218/574 completed (loss: 0.05314306169748306, acc: 0.9888888597488403)
[2025-01-06 01:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54][root][INFO] - Training Epoch: 7/10, step 219/574 completed (loss: 0.06415921449661255, acc: 0.939393937587738)
[2025-01-06 01:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54][root][INFO] - Training Epoch: 7/10, step 220/574 completed (loss: 0.0026775312144309282, acc: 1.0)
[2025-01-06 01:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54][root][INFO] - Training Epoch: 7/10, step 221/574 completed (loss: 0.0027899830602109432, acc: 1.0)
[2025-01-06 01:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55][root][INFO] - Training Epoch: 7/10, step 222/574 completed (loss: 0.2789788842201233, acc: 0.8653846383094788)
[2025-01-06 01:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55][root][INFO] - Training Epoch: 7/10, step 223/574 completed (loss: 0.19748540222644806, acc: 0.9347826242446899)
[2025-01-06 01:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56][root][INFO] - Training Epoch: 7/10, step 224/574 completed (loss: 0.24003607034683228, acc: 0.9204545617103577)
[2025-01-06 01:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56][root][INFO] - Training Epoch: 7/10, step 225/574 completed (loss: 0.2836637794971466, acc: 0.914893627166748)
[2025-01-06 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57][root][INFO] - Training Epoch: 7/10, step 226/574 completed (loss: 0.051245130598545074, acc: 1.0)
[2025-01-06 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57][root][INFO] - Training Epoch: 7/10, step 227/574 completed (loss: 0.17994651198387146, acc: 0.949999988079071)
[2025-01-06 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57][root][INFO] - Training Epoch: 7/10, step 228/574 completed (loss: 0.09437494724988937, acc: 0.9767441749572754)
[2025-01-06 01:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58][root][INFO] - Training Epoch: 7/10, step 229/574 completed (loss: 0.16118206083774567, acc: 0.9333333373069763)
[2025-01-06 01:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58][root][INFO] - Training Epoch: 7/10, step 230/574 completed (loss: 0.657036542892456, acc: 0.800000011920929)
[2025-01-06 01:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58][root][INFO] - Training Epoch: 7/10, step 231/574 completed (loss: 0.38743358850479126, acc: 0.8444444537162781)
[2025-01-06 01:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59][root][INFO] - Training Epoch: 7/10, step 232/574 completed (loss: 0.5330512523651123, acc: 0.8333333134651184)
[2025-01-06 01:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59][root][INFO] - Training Epoch: 7/10, step 233/574 completed (loss: 0.9635981321334839, acc: 0.6972476840019226)
[2025-01-06 01:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00][root][INFO] - Training Epoch: 7/10, step 234/574 completed (loss: 0.5343745946884155, acc: 0.8692307472229004)
[2025-01-06 01:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00][root][INFO] - Training Epoch: 7/10, step 235/574 completed (loss: 0.008722317405045033, acc: 1.0)
[2025-01-06 01:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00][root][INFO] - Training Epoch: 7/10, step 236/574 completed (loss: 0.004636658355593681, acc: 1.0)
[2025-01-06 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01][root][INFO] - Training Epoch: 7/10, step 237/574 completed (loss: 0.14544834196567535, acc: 0.9545454382896423)
[2025-01-06 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01][root][INFO] - Training Epoch: 7/10, step 238/574 completed (loss: 0.04004714637994766, acc: 1.0)
[2025-01-06 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01][root][INFO] - Training Epoch: 7/10, step 239/574 completed (loss: 0.036895908415317535, acc: 1.0)
[2025-01-06 01:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02][root][INFO] - Training Epoch: 7/10, step 240/574 completed (loss: 0.22922629117965698, acc: 0.9090909361839294)
[2025-01-06 01:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02][root][INFO] - Training Epoch: 7/10, step 241/574 completed (loss: 0.05580630525946617, acc: 0.9772727489471436)
[2025-01-06 01:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03][root][INFO] - Training Epoch: 7/10, step 242/574 completed (loss: 0.3091917634010315, acc: 0.9032257795333862)
[2025-01-06 01:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03][root][INFO] - Training Epoch: 7/10, step 243/574 completed (loss: 0.31784677505493164, acc: 0.9318181872367859)
[2025-01-06 01:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04][root][INFO] - Training Epoch: 7/10, step 244/574 completed (loss: 0.0014583454467356205, acc: 1.0)
[2025-01-06 01:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04][root][INFO] - Training Epoch: 7/10, step 245/574 completed (loss: 0.12459555268287659, acc: 0.9615384340286255)
[2025-01-06 01:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04][root][INFO] - Training Epoch: 7/10, step 246/574 completed (loss: 0.0033767970744520426, acc: 1.0)
[2025-01-06 01:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05][root][INFO] - Training Epoch: 7/10, step 247/574 completed (loss: 0.01118964422494173, acc: 1.0)
[2025-01-06 01:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05][root][INFO] - Training Epoch: 7/10, step 248/574 completed (loss: 0.03641626983880997, acc: 1.0)
[2025-01-06 01:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05][root][INFO] - Training Epoch: 7/10, step 249/574 completed (loss: 0.014736318960785866, acc: 1.0)
[2025-01-06 01:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06][root][INFO] - Training Epoch: 7/10, step 250/574 completed (loss: 0.04697149619460106, acc: 0.9729729890823364)
[2025-01-06 01:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06][root][INFO] - Training Epoch: 7/10, step 251/574 completed (loss: 0.014817750081419945, acc: 1.0)
[2025-01-06 01:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06][root][INFO] - Training Epoch: 7/10, step 252/574 completed (loss: 0.011463929899036884, acc: 1.0)
[2025-01-06 01:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07][root][INFO] - Training Epoch: 7/10, step 253/574 completed (loss: 0.005001759622246027, acc: 1.0)
[2025-01-06 01:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07][root][INFO] - Training Epoch: 7/10, step 254/574 completed (loss: 0.0023169550113379955, acc: 1.0)
[2025-01-06 01:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07][root][INFO] - Training Epoch: 7/10, step 255/574 completed (loss: 0.0007837950834073126, acc: 1.0)
[2025-01-06 01:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07][root][INFO] - Training Epoch: 7/10, step 256/574 completed (loss: 0.014782757498323917, acc: 1.0)
[2025-01-06 01:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08][root][INFO] - Training Epoch: 7/10, step 257/574 completed (loss: 0.08322738111019135, acc: 0.9714285731315613)
[2025-01-06 01:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08][root][INFO] - Training Epoch: 7/10, step 258/574 completed (loss: 0.0216116551309824, acc: 1.0)
[2025-01-06 01:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09][root][INFO] - Training Epoch: 7/10, step 259/574 completed (loss: 0.10413976013660431, acc: 0.9622641801834106)
[2025-01-06 01:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09][root][INFO] - Training Epoch: 7/10, step 260/574 completed (loss: 0.14562971889972687, acc: 0.9583333134651184)
[2025-01-06 01:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10][root][INFO] - Training Epoch: 7/10, step 261/574 completed (loss: 0.0020293465349823236, acc: 1.0)
[2025-01-06 01:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10][root][INFO] - Training Epoch: 7/10, step 262/574 completed (loss: 0.011556466110050678, acc: 1.0)
[2025-01-06 01:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10][root][INFO] - Training Epoch: 7/10, step 263/574 completed (loss: 0.2373652458190918, acc: 0.9333333373069763)
[2025-01-06 01:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11][root][INFO] - Training Epoch: 7/10, step 264/574 completed (loss: 0.09172595292329788, acc: 0.9791666865348816)
[2025-01-06 01:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11][root][INFO] - Training Epoch: 7/10, step 265/574 completed (loss: 0.6702795624732971, acc: 0.8240000009536743)
[2025-01-06 01:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:12][root][INFO] - Training Epoch: 7/10, step 266/574 completed (loss: 0.5444737076759338, acc: 0.8426966071128845)
[2025-01-06 01:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:12][root][INFO] - Training Epoch: 7/10, step 267/574 completed (loss: 0.1451970934867859, acc: 0.9324324131011963)
[2025-01-06 01:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13][root][INFO] - Training Epoch: 7/10, step 268/574 completed (loss: 0.14993537962436676, acc: 0.9482758641242981)
[2025-01-06 01:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13][root][INFO] - Training Epoch: 7/10, step 269/574 completed (loss: 0.0032659766729921103, acc: 1.0)
[2025-01-06 01:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13][root][INFO] - Training Epoch: 7/10, step 270/574 completed (loss: 0.1662442535161972, acc: 0.9545454382896423)
[2025-01-06 01:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13][root][INFO] - Training Epoch: 7/10, step 271/574 completed (loss: 0.01061232015490532, acc: 1.0)
[2025-01-06 01:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14][root][INFO] - Training Epoch: 7/10, step 272/574 completed (loss: 0.0019970217254012823, acc: 1.0)
[2025-01-06 01:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14][root][INFO] - Training Epoch: 7/10, step 273/574 completed (loss: 0.1482616364955902, acc: 0.9666666388511658)
[2025-01-06 01:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3881, device='cuda:0') eval_epoch_loss=tensor(0.8705, device='cuda:0') eval_epoch_acc=tensor(0.8280, device='cuda:0')
[2025-01-06 01:37:43][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:37:43][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:37:44][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_274_loss_0.8704822659492493/model.pt
[2025-01-06 01:37:44][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:44][root][INFO] - Training Epoch: 7/10, step 274/574 completed (loss: 0.038926105946302414, acc: 0.96875)
[2025-01-06 01:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:44][root][INFO] - Training Epoch: 7/10, step 275/574 completed (loss: 0.0010524929966777563, acc: 1.0)
[2025-01-06 01:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45][root][INFO] - Training Epoch: 7/10, step 276/574 completed (loss: 0.02589508518576622, acc: 1.0)
[2025-01-06 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45][root][INFO] - Training Epoch: 7/10, step 277/574 completed (loss: 0.007457309868186712, acc: 1.0)
[2025-01-06 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45][root][INFO] - Training Epoch: 7/10, step 278/574 completed (loss: 0.04737091436982155, acc: 0.978723406791687)
[2025-01-06 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:46][root][INFO] - Training Epoch: 7/10, step 279/574 completed (loss: 0.13322947919368744, acc: 0.9583333134651184)
[2025-01-06 01:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:46][root][INFO] - Training Epoch: 7/10, step 280/574 completed (loss: 0.010564812459051609, acc: 1.0)
[2025-01-06 01:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:46][root][INFO] - Training Epoch: 7/10, step 281/574 completed (loss: 0.1746392697095871, acc: 0.9518072009086609)
[2025-01-06 01:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47][root][INFO] - Training Epoch: 7/10, step 282/574 completed (loss: 0.4492625594139099, acc: 0.8703703880310059)
[2025-01-06 01:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47][root][INFO] - Training Epoch: 7/10, step 283/574 completed (loss: 0.011017757467925549, acc: 1.0)
[2025-01-06 01:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47][root][INFO] - Training Epoch: 7/10, step 284/574 completed (loss: 0.015371195040643215, acc: 1.0)
[2025-01-06 01:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48][root][INFO] - Training Epoch: 7/10, step 285/574 completed (loss: 0.09239808470010757, acc: 0.9750000238418579)
[2025-01-06 01:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48][root][INFO] - Training Epoch: 7/10, step 286/574 completed (loss: 0.11584127694368362, acc: 0.96875)
[2025-01-06 01:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48][root][INFO] - Training Epoch: 7/10, step 287/574 completed (loss: 0.09765751659870148, acc: 0.9679999947547913)
[2025-01-06 01:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49][root][INFO] - Training Epoch: 7/10, step 288/574 completed (loss: 0.06875767558813095, acc: 0.9890109896659851)
[2025-01-06 01:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49][root][INFO] - Training Epoch: 7/10, step 289/574 completed (loss: 0.0610881932079792, acc: 0.9875776171684265)
[2025-01-06 01:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50][root][INFO] - Training Epoch: 7/10, step 290/574 completed (loss: 0.18814000487327576, acc: 0.9226804375648499)
[2025-01-06 01:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50][root][INFO] - Training Epoch: 7/10, step 291/574 completed (loss: 0.02668800950050354, acc: 1.0)
[2025-01-06 01:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50][root][INFO] - Training Epoch: 7/10, step 292/574 completed (loss: 0.09542439132928848, acc: 0.976190447807312)
[2025-01-06 01:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51][root][INFO] - Training Epoch: 7/10, step 293/574 completed (loss: 0.06358332931995392, acc: 0.982758641242981)
[2025-01-06 01:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51][root][INFO] - Training Epoch: 7/10, step 294/574 completed (loss: 0.2334928661584854, acc: 0.9090909361839294)
[2025-01-06 01:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52][root][INFO] - Training Epoch: 7/10, step 295/574 completed (loss: 0.18453122675418854, acc: 0.9329897165298462)
[2025-01-06 01:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52][root][INFO] - Training Epoch: 7/10, step 296/574 completed (loss: 0.04105153679847717, acc: 1.0)
[2025-01-06 01:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52][root][INFO] - Training Epoch: 7/10, step 297/574 completed (loss: 0.3157653510570526, acc: 0.9629629850387573)
[2025-01-06 01:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53][root][INFO] - Training Epoch: 7/10, step 298/574 completed (loss: 0.023686086758971214, acc: 1.0)
[2025-01-06 01:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53][root][INFO] - Training Epoch: 7/10, step 299/574 completed (loss: 0.0030399058014154434, acc: 1.0)
[2025-01-06 01:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53][root][INFO] - Training Epoch: 7/10, step 300/574 completed (loss: 0.008016648702323437, acc: 1.0)
[2025-01-06 01:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54][root][INFO] - Training Epoch: 7/10, step 301/574 completed (loss: 0.08740837872028351, acc: 0.9811320900917053)
[2025-01-06 01:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54][root][INFO] - Training Epoch: 7/10, step 302/574 completed (loss: 0.016789529472589493, acc: 1.0)
[2025-01-06 01:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54][root][INFO] - Training Epoch: 7/10, step 303/574 completed (loss: 0.011139913462102413, acc: 1.0)
[2025-01-06 01:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55][root][INFO] - Training Epoch: 7/10, step 304/574 completed (loss: 0.02584584802389145, acc: 1.0)
[2025-01-06 01:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55][root][INFO] - Training Epoch: 7/10, step 305/574 completed (loss: 0.04470890760421753, acc: 0.9836065769195557)
[2025-01-06 01:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55][root][INFO] - Training Epoch: 7/10, step 306/574 completed (loss: 0.01399324182420969, acc: 1.0)
[2025-01-06 01:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56][root][INFO] - Training Epoch: 7/10, step 307/574 completed (loss: 0.0005150206270627677, acc: 1.0)
[2025-01-06 01:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56][root][INFO] - Training Epoch: 7/10, step 308/574 completed (loss: 0.11644606292247772, acc: 0.9710144996643066)
[2025-01-06 01:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56][root][INFO] - Training Epoch: 7/10, step 309/574 completed (loss: 0.06772691011428833, acc: 0.9722222089767456)
[2025-01-06 01:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57][root][INFO] - Training Epoch: 7/10, step 310/574 completed (loss: 0.03097519837319851, acc: 1.0)
[2025-01-06 01:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57][root][INFO] - Training Epoch: 7/10, step 311/574 completed (loss: 0.09556501358747482, acc: 0.9615384340286255)
[2025-01-06 01:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57][root][INFO] - Training Epoch: 7/10, step 312/574 completed (loss: 0.08701283484697342, acc: 0.9591836929321289)
[2025-01-06 01:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58][root][INFO] - Training Epoch: 7/10, step 313/574 completed (loss: 0.001414785161614418, acc: 1.0)
[2025-01-06 01:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58][root][INFO] - Training Epoch: 7/10, step 314/574 completed (loss: 0.012248971499502659, acc: 1.0)
[2025-01-06 01:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59][root][INFO] - Training Epoch: 7/10, step 315/574 completed (loss: 0.361261248588562, acc: 0.9032257795333862)
[2025-01-06 01:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59][root][INFO] - Training Epoch: 7/10, step 316/574 completed (loss: 0.3475467264652252, acc: 0.9354838728904724)
[2025-01-06 01:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59][root][INFO] - Training Epoch: 7/10, step 317/574 completed (loss: 0.020893925800919533, acc: 1.0)
[2025-01-06 01:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00][root][INFO] - Training Epoch: 7/10, step 318/574 completed (loss: 0.03055967204272747, acc: 0.9903846383094788)
[2025-01-06 01:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00][root][INFO] - Training Epoch: 7/10, step 319/574 completed (loss: 0.003565158462151885, acc: 1.0)
[2025-01-06 01:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00][root][INFO] - Training Epoch: 7/10, step 320/574 completed (loss: 0.015774080529808998, acc: 1.0)
[2025-01-06 01:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01][root][INFO] - Training Epoch: 7/10, step 321/574 completed (loss: 0.0027496751863509417, acc: 1.0)
[2025-01-06 01:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01][root][INFO] - Training Epoch: 7/10, step 322/574 completed (loss: 0.5074787735939026, acc: 0.8148148059844971)
[2025-01-06 01:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01][root][INFO] - Training Epoch: 7/10, step 323/574 completed (loss: 0.16885706782341003, acc: 0.9714285731315613)
[2025-01-06 01:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02][root][INFO] - Training Epoch: 7/10, step 324/574 completed (loss: 0.38449743390083313, acc: 0.8717948794364929)
[2025-01-06 01:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02][root][INFO] - Training Epoch: 7/10, step 325/574 completed (loss: 0.25736886262893677, acc: 0.9024389982223511)
[2025-01-06 01:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02][root][INFO] - Training Epoch: 7/10, step 326/574 completed (loss: 0.4639725387096405, acc: 0.8421052694320679)
[2025-01-06 01:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03][root][INFO] - Training Epoch: 7/10, step 327/574 completed (loss: 0.00892880093306303, acc: 1.0)
[2025-01-06 01:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03][root][INFO] - Training Epoch: 7/10, step 328/574 completed (loss: 0.0028261446859687567, acc: 1.0)
[2025-01-06 01:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03][root][INFO] - Training Epoch: 7/10, step 329/574 completed (loss: 0.006417158525437117, acc: 1.0)
[2025-01-06 01:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04][root][INFO] - Training Epoch: 7/10, step 330/574 completed (loss: 0.006465176120400429, acc: 1.0)
[2025-01-06 01:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04][root][INFO] - Training Epoch: 7/10, step 331/574 completed (loss: 0.06879011541604996, acc: 0.9838709831237793)
[2025-01-06 01:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04][root][INFO] - Training Epoch: 7/10, step 332/574 completed (loss: 0.036559686064720154, acc: 0.9824561476707458)
[2025-01-06 01:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05][root][INFO] - Training Epoch: 7/10, step 333/574 completed (loss: 0.2405463010072708, acc: 0.9375)
[2025-01-06 01:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05][root][INFO] - Training Epoch: 7/10, step 334/574 completed (loss: 0.025650683790445328, acc: 1.0)
[2025-01-06 01:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05][root][INFO] - Training Epoch: 7/10, step 335/574 completed (loss: 0.006090047303587198, acc: 1.0)
[2025-01-06 01:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06][root][INFO] - Training Epoch: 7/10, step 336/574 completed (loss: 0.15225572884082794, acc: 0.9399999976158142)
[2025-01-06 01:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06][root][INFO] - Training Epoch: 7/10, step 337/574 completed (loss: 0.37422534823417664, acc: 0.8965517282485962)
[2025-01-06 01:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06][root][INFO] - Training Epoch: 7/10, step 338/574 completed (loss: 0.43233174085617065, acc: 0.8297872543334961)
[2025-01-06 01:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07][root][INFO] - Training Epoch: 7/10, step 339/574 completed (loss: 0.29486650228500366, acc: 0.9036144614219666)
[2025-01-06 01:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07][root][INFO] - Training Epoch: 7/10, step 340/574 completed (loss: 0.008973948657512665, acc: 1.0)
[2025-01-06 01:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07][root][INFO] - Training Epoch: 7/10, step 341/574 completed (loss: 0.008379094302654266, acc: 1.0)
[2025-01-06 01:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08][root][INFO] - Training Epoch: 7/10, step 342/574 completed (loss: 0.08712157607078552, acc: 0.9879518151283264)
[2025-01-06 01:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08][root][INFO] - Training Epoch: 7/10, step 343/574 completed (loss: 0.2479565441608429, acc: 0.9245283007621765)
[2025-01-06 01:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08][root][INFO] - Training Epoch: 7/10, step 344/574 completed (loss: 0.03701899200677872, acc: 0.9873417615890503)
[2025-01-06 01:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09][root][INFO] - Training Epoch: 7/10, step 345/574 completed (loss: 0.0016885651275515556, acc: 1.0)
[2025-01-06 01:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09][root][INFO] - Training Epoch: 7/10, step 346/574 completed (loss: 0.11466087400913239, acc: 0.9701492786407471)
[2025-01-06 01:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09][root][INFO] - Training Epoch: 7/10, step 347/574 completed (loss: 0.0015352300833910704, acc: 1.0)
[2025-01-06 01:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09][root][INFO] - Training Epoch: 7/10, step 348/574 completed (loss: 0.0277949720621109, acc: 1.0)
[2025-01-06 01:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10][root][INFO] - Training Epoch: 7/10, step 349/574 completed (loss: 0.29039114713668823, acc: 0.9166666865348816)
[2025-01-06 01:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10][root][INFO] - Training Epoch: 7/10, step 350/574 completed (loss: 0.12747101485729218, acc: 0.9534883499145508)
[2025-01-06 01:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11][root][INFO] - Training Epoch: 7/10, step 351/574 completed (loss: 0.023831067606806755, acc: 1.0)
[2025-01-06 01:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11][root][INFO] - Training Epoch: 7/10, step 352/574 completed (loss: 0.23864910006523132, acc: 0.8888888955116272)
[2025-01-06 01:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11][root][INFO] - Training Epoch: 7/10, step 353/574 completed (loss: 0.008806436322629452, acc: 1.0)
[2025-01-06 01:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12][root][INFO] - Training Epoch: 7/10, step 354/574 completed (loss: 0.024983061477541924, acc: 1.0)
[2025-01-06 01:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12][root][INFO] - Training Epoch: 7/10, step 355/574 completed (loss: 0.24193094670772552, acc: 0.901098906993866)
[2025-01-06 01:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12][root][INFO] - Training Epoch: 7/10, step 356/574 completed (loss: 0.17892558872699738, acc: 0.947826087474823)
[2025-01-06 01:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:13][root][INFO] - Training Epoch: 7/10, step 357/574 completed (loss: 0.12078475207090378, acc: 0.9347826242446899)
[2025-01-06 01:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:13][root][INFO] - Training Epoch: 7/10, step 358/574 completed (loss: 0.09984544664621353, acc: 0.9591836929321289)
[2025-01-06 01:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14][root][INFO] - Training Epoch: 7/10, step 359/574 completed (loss: 0.0020606820471584797, acc: 1.0)
[2025-01-06 01:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14][root][INFO] - Training Epoch: 7/10, step 360/574 completed (loss: 0.0119114238768816, acc: 1.0)
[2025-01-06 01:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14][root][INFO] - Training Epoch: 7/10, step 361/574 completed (loss: 0.03795052319765091, acc: 1.0)
[2025-01-06 01:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14][root][INFO] - Training Epoch: 7/10, step 362/574 completed (loss: 0.13548098504543304, acc: 0.9555555582046509)
[2025-01-06 01:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15][root][INFO] - Training Epoch: 7/10, step 363/574 completed (loss: 0.01574571244418621, acc: 1.0)
[2025-01-06 01:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15][root][INFO] - Training Epoch: 7/10, step 364/574 completed (loss: 0.005388016812503338, acc: 1.0)
[2025-01-06 01:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16][root][INFO] - Training Epoch: 7/10, step 365/574 completed (loss: 0.013072693720459938, acc: 1.0)
[2025-01-06 01:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16][root][INFO] - Training Epoch: 7/10, step 366/574 completed (loss: 0.00023374044394586235, acc: 1.0)
[2025-01-06 01:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16][root][INFO] - Training Epoch: 7/10, step 367/574 completed (loss: 0.0016938933404162526, acc: 1.0)
[2025-01-06 01:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17][root][INFO] - Training Epoch: 7/10, step 368/574 completed (loss: 0.03158440440893173, acc: 0.9642857313156128)
[2025-01-06 01:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17][root][INFO] - Training Epoch: 7/10, step 369/574 completed (loss: 0.046533919870853424, acc: 0.96875)
[2025-01-06 01:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18][root][INFO] - Training Epoch: 7/10, step 370/574 completed (loss: 0.27015307545661926, acc: 0.939393937587738)
[2025-01-06 01:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18][root][INFO] - Training Epoch: 7/10, step 371/574 completed (loss: 0.07194836437702179, acc: 0.9622641801834106)
[2025-01-06 01:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19][root][INFO] - Training Epoch: 7/10, step 372/574 completed (loss: 0.1067931056022644, acc: 0.9777777791023254)
[2025-01-06 01:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19][root][INFO] - Training Epoch: 7/10, step 373/574 completed (loss: 0.047390520572662354, acc: 0.9821428656578064)
[2025-01-06 01:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19][root][INFO] - Training Epoch: 7/10, step 374/574 completed (loss: 0.03402157872915268, acc: 0.9714285731315613)
[2025-01-06 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20][root][INFO] - Training Epoch: 7/10, step 375/574 completed (loss: 0.0015509299701079726, acc: 1.0)
[2025-01-06 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20][root][INFO] - Training Epoch: 7/10, step 376/574 completed (loss: 0.005480203311890364, acc: 1.0)
[2025-01-06 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20][root][INFO] - Training Epoch: 7/10, step 377/574 completed (loss: 0.012700878083705902, acc: 1.0)
[2025-01-06 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:21][root][INFO] - Training Epoch: 7/10, step 378/574 completed (loss: 0.004921074956655502, acc: 1.0)
[2025-01-06 01:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:21][root][INFO] - Training Epoch: 7/10, step 379/574 completed (loss: 0.08896220475435257, acc: 0.9640718698501587)
[2025-01-06 01:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:22][root][INFO] - Training Epoch: 7/10, step 380/574 completed (loss: 0.17268703877925873, acc: 0.9624060392379761)
[2025-01-06 01:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23][root][INFO] - Training Epoch: 7/10, step 381/574 completed (loss: 0.2675914466381073, acc: 0.9251337051391602)
[2025-01-06 01:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23][root][INFO] - Training Epoch: 7/10, step 382/574 completed (loss: 0.05664496496319771, acc: 0.9729729890823364)
[2025-01-06 01:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24][root][INFO] - Training Epoch: 7/10, step 383/574 completed (loss: 0.15299741923809052, acc: 0.9642857313156128)
[2025-01-06 01:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24][root][INFO] - Training Epoch: 7/10, step 384/574 completed (loss: 0.0007182666449807584, acc: 1.0)
[2025-01-06 01:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25][root][INFO] - Training Epoch: 7/10, step 385/574 completed (loss: 0.0016329745994880795, acc: 1.0)
[2025-01-06 01:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25][root][INFO] - Training Epoch: 7/10, step 386/574 completed (loss: 0.001223264029249549, acc: 1.0)
[2025-01-06 01:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25][root][INFO] - Training Epoch: 7/10, step 387/574 completed (loss: 0.0014239007141441107, acc: 1.0)
[2025-01-06 01:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26][root][INFO] - Training Epoch: 7/10, step 388/574 completed (loss: 0.00042504406883381307, acc: 1.0)
[2025-01-06 01:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26][root][INFO] - Training Epoch: 7/10, step 389/574 completed (loss: 0.029691431671380997, acc: 1.0)
[2025-01-06 01:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26][root][INFO] - Training Epoch: 7/10, step 390/574 completed (loss: 0.38588082790374756, acc: 0.9523809552192688)
[2025-01-06 01:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27][root][INFO] - Training Epoch: 7/10, step 391/574 completed (loss: 0.20574434101581573, acc: 0.9444444179534912)
[2025-01-06 01:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27][root][INFO] - Training Epoch: 7/10, step 392/574 completed (loss: 0.34017762541770935, acc: 0.8640776872634888)
[2025-01-06 01:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28][root][INFO] - Training Epoch: 7/10, step 393/574 completed (loss: 0.34874799847602844, acc: 0.8897058963775635)
[2025-01-06 01:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28][root][INFO] - Training Epoch: 7/10, step 394/574 completed (loss: 0.21154899895191193, acc: 0.9266666769981384)
[2025-01-06 01:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28][root][INFO] - Training Epoch: 7/10, step 395/574 completed (loss: 0.25214850902557373, acc: 0.9375)
[2025-01-06 01:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29][root][INFO] - Training Epoch: 7/10, step 396/574 completed (loss: 0.0174713134765625, acc: 1.0)
[2025-01-06 01:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29][root][INFO] - Training Epoch: 7/10, step 397/574 completed (loss: 0.18801690638065338, acc: 0.9583333134651184)
[2025-01-06 01:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29][root][INFO] - Training Epoch: 7/10, step 398/574 completed (loss: 0.1383812427520752, acc: 0.9534883499145508)
[2025-01-06 01:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:30][root][INFO] - Training Epoch: 7/10, step 399/574 completed (loss: 0.011574434116482735, acc: 1.0)
[2025-01-06 01:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:30][root][INFO] - Training Epoch: 7/10, step 400/574 completed (loss: 0.061109669506549835, acc: 0.9852941036224365)
[2025-01-06 01:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31][root][INFO] - Training Epoch: 7/10, step 401/574 completed (loss: 0.051311150193214417, acc: 0.9866666793823242)
[2025-01-06 01:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31][root][INFO] - Training Epoch: 7/10, step 402/574 completed (loss: 0.00777143519371748, acc: 1.0)
[2025-01-06 01:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31][root][INFO] - Training Epoch: 7/10, step 403/574 completed (loss: 0.27552974224090576, acc: 0.939393937587738)
[2025-01-06 01:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32][root][INFO] - Training Epoch: 7/10, step 404/574 completed (loss: 0.007743990980088711, acc: 1.0)
[2025-01-06 01:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32][root][INFO] - Training Epoch: 7/10, step 405/574 completed (loss: 0.0006737704388797283, acc: 1.0)
[2025-01-06 01:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32][root][INFO] - Training Epoch: 7/10, step 406/574 completed (loss: 0.0826185867190361, acc: 0.9599999785423279)
[2025-01-06 01:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33][root][INFO] - Training Epoch: 7/10, step 407/574 completed (loss: 0.002069121226668358, acc: 1.0)
[2025-01-06 01:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33][root][INFO] - Training Epoch: 7/10, step 408/574 completed (loss: 0.04401325434446335, acc: 1.0)
[2025-01-06 01:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33][root][INFO] - Training Epoch: 7/10, step 409/574 completed (loss: 0.013003666885197163, acc: 1.0)
[2025-01-06 01:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34][root][INFO] - Training Epoch: 7/10, step 410/574 completed (loss: 0.01118084229528904, acc: 1.0)
[2025-01-06 01:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34][root][INFO] - Training Epoch: 7/10, step 411/574 completed (loss: 0.05120069533586502, acc: 0.9642857313156128)
[2025-01-06 01:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34][root][INFO] - Training Epoch: 7/10, step 412/574 completed (loss: 0.0367853008210659, acc: 0.9666666388511658)
[2025-01-06 01:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35][root][INFO] - Training Epoch: 7/10, step 413/574 completed (loss: 0.17997856438159943, acc: 0.939393937587738)
[2025-01-06 01:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35][root][INFO] - Training Epoch: 7/10, step 414/574 completed (loss: 0.18947868049144745, acc: 0.9090909361839294)
[2025-01-06 01:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35][root][INFO] - Training Epoch: 7/10, step 415/574 completed (loss: 0.040370356291532516, acc: 0.9803921580314636)
[2025-01-06 01:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:36][root][INFO] - Training Epoch: 7/10, step 416/574 completed (loss: 0.015810783952474594, acc: 1.0)
[2025-01-06 01:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2687, device='cuda:0') eval_epoch_loss=tensor(0.8192, device='cuda:0') eval_epoch_acc=tensor(0.8242, device='cuda:0')
[2025-01-06 01:39:07][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:39:07][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:39:07][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_417_loss_0.8192003965377808/model.pt
[2025-01-06 01:39:07][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07][root][INFO] - Training Epoch: 7/10, step 417/574 completed (loss: 0.02430754154920578, acc: 1.0)
[2025-01-06 01:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08][root][INFO] - Training Epoch: 7/10, step 418/574 completed (loss: 0.05182063579559326, acc: 0.9750000238418579)
[2025-01-06 01:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08][root][INFO] - Training Epoch: 7/10, step 419/574 completed (loss: 0.0027610096149146557, acc: 1.0)
[2025-01-06 01:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08][root][INFO] - Training Epoch: 7/10, step 420/574 completed (loss: 0.03377407416701317, acc: 1.0)
[2025-01-06 01:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09][root][INFO] - Training Epoch: 7/10, step 421/574 completed (loss: 0.08945906907320023, acc: 0.9333333373069763)
[2025-01-06 01:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09][root][INFO] - Training Epoch: 7/10, step 422/574 completed (loss: 0.025877060368657112, acc: 1.0)
[2025-01-06 01:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09][root][INFO] - Training Epoch: 7/10, step 423/574 completed (loss: 0.05723774433135986, acc: 0.9722222089767456)
[2025-01-06 01:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10][root][INFO] - Training Epoch: 7/10, step 424/574 completed (loss: 0.006575744599103928, acc: 1.0)
[2025-01-06 01:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10][root][INFO] - Training Epoch: 7/10, step 425/574 completed (loss: 0.00894590001553297, acc: 1.0)
[2025-01-06 01:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10][root][INFO] - Training Epoch: 7/10, step 426/574 completed (loss: 0.0014888247242197394, acc: 1.0)
[2025-01-06 01:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11][root][INFO] - Training Epoch: 7/10, step 427/574 completed (loss: 0.056365907192230225, acc: 0.9729729890823364)
[2025-01-06 01:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11][root][INFO] - Training Epoch: 7/10, step 428/574 completed (loss: 0.07487208396196365, acc: 0.9629629850387573)
[2025-01-06 01:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11][root][INFO] - Training Epoch: 7/10, step 429/574 completed (loss: 0.055715981870889664, acc: 0.95652174949646)
[2025-01-06 01:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12][root][INFO] - Training Epoch: 7/10, step 430/574 completed (loss: 0.00014396915503311902, acc: 1.0)
[2025-01-06 01:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12][root][INFO] - Training Epoch: 7/10, step 431/574 completed (loss: 0.0009150683763436973, acc: 1.0)
[2025-01-06 01:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12][root][INFO] - Training Epoch: 7/10, step 432/574 completed (loss: 0.0015245031099766493, acc: 1.0)
[2025-01-06 01:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:13][root][INFO] - Training Epoch: 7/10, step 433/574 completed (loss: 0.07178317755460739, acc: 0.9722222089767456)
[2025-01-06 01:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:13][root][INFO] - Training Epoch: 7/10, step 434/574 completed (loss: 0.0011615519179031253, acc: 1.0)
[2025-01-06 01:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:13][root][INFO] - Training Epoch: 7/10, step 435/574 completed (loss: 0.022792406380176544, acc: 0.9696969985961914)
[2025-01-06 01:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14][root][INFO] - Training Epoch: 7/10, step 436/574 completed (loss: 0.0028675785288214684, acc: 1.0)
[2025-01-06 01:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14][root][INFO] - Training Epoch: 7/10, step 437/574 completed (loss: 0.010672952979803085, acc: 1.0)
[2025-01-06 01:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14][root][INFO] - Training Epoch: 7/10, step 438/574 completed (loss: 0.0019292045617476106, acc: 1.0)
[2025-01-06 01:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15][root][INFO] - Training Epoch: 7/10, step 439/574 completed (loss: 0.14076213538646698, acc: 0.9487179517745972)
[2025-01-06 01:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15][root][INFO] - Training Epoch: 7/10, step 440/574 completed (loss: 0.04261541739106178, acc: 0.9848484992980957)
[2025-01-06 01:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:16][root][INFO] - Training Epoch: 7/10, step 441/574 completed (loss: 0.43896010518074036, acc: 0.8799999952316284)
[2025-01-06 01:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:16][root][INFO] - Training Epoch: 7/10, step 442/574 completed (loss: 0.2338228076696396, acc: 0.9354838728904724)
[2025-01-06 01:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:17][root][INFO] - Training Epoch: 7/10, step 443/574 completed (loss: 0.18823586404323578, acc: 0.9402984976768494)
[2025-01-06 01:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:17][root][INFO] - Training Epoch: 7/10, step 444/574 completed (loss: 0.06557556986808777, acc: 0.9811320900917053)
[2025-01-06 01:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18][root][INFO] - Training Epoch: 7/10, step 445/574 completed (loss: 0.04324403032660484, acc: 0.9772727489471436)
[2025-01-06 01:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18][root][INFO] - Training Epoch: 7/10, step 446/574 completed (loss: 0.016078801825642586, acc: 1.0)
[2025-01-06 01:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18][root][INFO] - Training Epoch: 7/10, step 447/574 completed (loss: 0.15601414442062378, acc: 0.9615384340286255)
[2025-01-06 01:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19][root][INFO] - Training Epoch: 7/10, step 448/574 completed (loss: 0.07478725165128708, acc: 0.9642857313156128)
[2025-01-06 01:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19][root][INFO] - Training Epoch: 7/10, step 449/574 completed (loss: 0.010830319486558437, acc: 1.0)
[2025-01-06 01:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19][root][INFO] - Training Epoch: 7/10, step 450/574 completed (loss: 0.010986610315740108, acc: 1.0)
[2025-01-06 01:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20][root][INFO] - Training Epoch: 7/10, step 451/574 completed (loss: 0.008812569081783295, acc: 1.0)
[2025-01-06 01:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20][root][INFO] - Training Epoch: 7/10, step 452/574 completed (loss: 0.06623559445142746, acc: 0.9743589758872986)
[2025-01-06 01:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20][root][INFO] - Training Epoch: 7/10, step 453/574 completed (loss: 0.1445326954126358, acc: 0.9605262875556946)
[2025-01-06 01:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21][root][INFO] - Training Epoch: 7/10, step 454/574 completed (loss: 0.012575823813676834, acc: 1.0)
[2025-01-06 01:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21][root][INFO] - Training Epoch: 7/10, step 455/574 completed (loss: 0.012621162459254265, acc: 1.0)
[2025-01-06 01:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22][root][INFO] - Training Epoch: 7/10, step 456/574 completed (loss: 0.09823998808860779, acc: 0.9587628841400146)
[2025-01-06 01:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22][root][INFO] - Training Epoch: 7/10, step 457/574 completed (loss: 0.06028330698609352, acc: 0.9857142567634583)
[2025-01-06 01:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22][root][INFO] - Training Epoch: 7/10, step 458/574 completed (loss: 0.11305248737335205, acc: 0.9825581312179565)
[2025-01-06 01:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23][root][INFO] - Training Epoch: 7/10, step 459/574 completed (loss: 0.021173149347305298, acc: 1.0)
[2025-01-06 01:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23][root][INFO] - Training Epoch: 7/10, step 460/574 completed (loss: 0.027409465983510017, acc: 0.9876543283462524)
[2025-01-06 01:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23][root][INFO] - Training Epoch: 7/10, step 461/574 completed (loss: 0.03905295580625534, acc: 1.0)
[2025-01-06 01:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24][root][INFO] - Training Epoch: 7/10, step 462/574 completed (loss: 0.02136213146150112, acc: 1.0)
[2025-01-06 01:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24][root][INFO] - Training Epoch: 7/10, step 463/574 completed (loss: 0.01988237164914608, acc: 1.0)
[2025-01-06 01:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24][root][INFO] - Training Epoch: 7/10, step 464/574 completed (loss: 0.03485110029578209, acc: 0.97826087474823)
[2025-01-06 01:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:25][root][INFO] - Training Epoch: 7/10, step 465/574 completed (loss: 0.07535403221845627, acc: 0.976190447807312)
[2025-01-06 01:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:25][root][INFO] - Training Epoch: 7/10, step 466/574 completed (loss: 0.2803567051887512, acc: 0.9397590160369873)
[2025-01-06 01:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26][root][INFO] - Training Epoch: 7/10, step 467/574 completed (loss: 0.027633678168058395, acc: 0.9909909963607788)
[2025-01-06 01:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26][root][INFO] - Training Epoch: 7/10, step 468/574 completed (loss: 0.0982569232583046, acc: 0.9611650705337524)
[2025-01-06 01:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26][root][INFO] - Training Epoch: 7/10, step 469/574 completed (loss: 0.16119255125522614, acc: 0.9674796462059021)
[2025-01-06 01:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27][root][INFO] - Training Epoch: 7/10, step 470/574 completed (loss: 0.0037954242434352636, acc: 1.0)
[2025-01-06 01:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27][root][INFO] - Training Epoch: 7/10, step 471/574 completed (loss: 0.019257059320807457, acc: 1.0)
[2025-01-06 01:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27][root][INFO] - Training Epoch: 7/10, step 472/574 completed (loss: 0.22895070910453796, acc: 0.9313725233078003)
[2025-01-06 01:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28][root][INFO] - Training Epoch: 7/10, step 473/574 completed (loss: 0.41869401931762695, acc: 0.8689956068992615)
[2025-01-06 01:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28][root][INFO] - Training Epoch: 7/10, step 474/574 completed (loss: 0.13754205405712128, acc: 0.9479166865348816)
[2025-01-06 01:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28][root][INFO] - Training Epoch: 7/10, step 475/574 completed (loss: 0.10049731284379959, acc: 0.9693251252174377)
[2025-01-06 01:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29][root][INFO] - Training Epoch: 7/10, step 476/574 completed (loss: 0.1706475019454956, acc: 0.9496402740478516)
[2025-01-06 01:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29][root][INFO] - Training Epoch: 7/10, step 477/574 completed (loss: 0.3163467049598694, acc: 0.8844221234321594)
[2025-01-06 01:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29][root][INFO] - Training Epoch: 7/10, step 478/574 completed (loss: 0.06443988531827927, acc: 0.9722222089767456)
[2025-01-06 01:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30][root][INFO] - Training Epoch: 7/10, step 479/574 completed (loss: 0.12623506784439087, acc: 0.939393937587738)
[2025-01-06 01:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30][root][INFO] - Training Epoch: 7/10, step 480/574 completed (loss: 0.05278433859348297, acc: 1.0)
[2025-01-06 01:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30][root][INFO] - Training Epoch: 7/10, step 481/574 completed (loss: 0.11466874182224274, acc: 0.949999988079071)
[2025-01-06 01:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:31][root][INFO] - Training Epoch: 7/10, step 482/574 completed (loss: 0.011088071390986443, acc: 1.0)
[2025-01-06 01:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:31][root][INFO] - Training Epoch: 7/10, step 483/574 completed (loss: 0.12186878174543381, acc: 0.9482758641242981)
[2025-01-06 01:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:31][root][INFO] - Training Epoch: 7/10, step 484/574 completed (loss: 0.004843076225370169, acc: 1.0)
[2025-01-06 01:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32][root][INFO] - Training Epoch: 7/10, step 485/574 completed (loss: 0.13381646573543549, acc: 0.9473684430122375)
[2025-01-06 01:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32][root][INFO] - Training Epoch: 7/10, step 486/574 completed (loss: 0.06674356758594513, acc: 1.0)
[2025-01-06 01:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32][root][INFO] - Training Epoch: 7/10, step 487/574 completed (loss: 0.0016562881646677852, acc: 1.0)
[2025-01-06 01:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33][root][INFO] - Training Epoch: 7/10, step 488/574 completed (loss: 0.04703845828771591, acc: 1.0)
[2025-01-06 01:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33][root][INFO] - Training Epoch: 7/10, step 489/574 completed (loss: 0.12601742148399353, acc: 0.9384615421295166)
[2025-01-06 01:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33][root][INFO] - Training Epoch: 7/10, step 490/574 completed (loss: 0.044530585408210754, acc: 0.9666666388511658)
[2025-01-06 01:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34][root][INFO] - Training Epoch: 7/10, step 491/574 completed (loss: 0.009194308891892433, acc: 1.0)
[2025-01-06 01:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34][root][INFO] - Training Epoch: 7/10, step 492/574 completed (loss: 0.25539761781692505, acc: 0.9215686321258545)
[2025-01-06 01:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34][root][INFO] - Training Epoch: 7/10, step 493/574 completed (loss: 0.13381220400333405, acc: 0.9655172228813171)
[2025-01-06 01:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35][root][INFO] - Training Epoch: 7/10, step 494/574 completed (loss: 0.1769278198480606, acc: 0.9473684430122375)
[2025-01-06 01:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35][root][INFO] - Training Epoch: 7/10, step 495/574 completed (loss: 0.2552395164966583, acc: 0.8947368264198303)
[2025-01-06 01:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36][root][INFO] - Training Epoch: 7/10, step 496/574 completed (loss: 0.37431758642196655, acc: 0.9107142686843872)
[2025-01-06 01:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36][root][INFO] - Training Epoch: 7/10, step 497/574 completed (loss: 0.045059606432914734, acc: 1.0)
[2025-01-06 01:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36][root][INFO] - Training Epoch: 7/10, step 498/574 completed (loss: 0.356868714094162, acc: 0.9213483333587646)
[2025-01-06 01:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37][root][INFO] - Training Epoch: 7/10, step 499/574 completed (loss: 0.5889760255813599, acc: 0.8085106611251831)
[2025-01-06 01:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37][root][INFO] - Training Epoch: 7/10, step 500/574 completed (loss: 0.16537804901599884, acc: 0.9347826242446899)
[2025-01-06 01:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37][root][INFO] - Training Epoch: 7/10, step 501/574 completed (loss: 0.007495464291423559, acc: 1.0)
[2025-01-06 01:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38][root][INFO] - Training Epoch: 7/10, step 502/574 completed (loss: 0.0007716961554251611, acc: 1.0)
[2025-01-06 01:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38][root][INFO] - Training Epoch: 7/10, step 503/574 completed (loss: 0.10156777501106262, acc: 0.9629629850387573)
[2025-01-06 01:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38][root][INFO] - Training Epoch: 7/10, step 504/574 completed (loss: 0.002506471937522292, acc: 1.0)
[2025-01-06 01:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39][root][INFO] - Training Epoch: 7/10, step 505/574 completed (loss: 0.23970744013786316, acc: 0.9056603908538818)
[2025-01-06 01:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39][root][INFO] - Training Epoch: 7/10, step 506/574 completed (loss: 0.18315771222114563, acc: 0.9655172228813171)
[2025-01-06 01:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40][root][INFO] - Training Epoch: 7/10, step 507/574 completed (loss: 0.5728712677955627, acc: 0.8648648858070374)
[2025-01-06 01:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40][root][INFO] - Training Epoch: 7/10, step 508/574 completed (loss: 0.23787151277065277, acc: 0.9436619877815247)
[2025-01-06 01:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40][root][INFO] - Training Epoch: 7/10, step 509/574 completed (loss: 0.1925070583820343, acc: 0.949999988079071)
[2025-01-06 01:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41][root][INFO] - Training Epoch: 7/10, step 510/574 completed (loss: 0.03211014345288277, acc: 0.9666666388511658)
[2025-01-06 01:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41][root][INFO] - Training Epoch: 7/10, step 511/574 completed (loss: 0.22922572493553162, acc: 0.9230769276618958)
[2025-01-06 01:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44][root][INFO] - Training Epoch: 7/10, step 512/574 completed (loss: 0.4075082540512085, acc: 0.8642857074737549)
[2025-01-06 01:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44][root][INFO] - Training Epoch: 7/10, step 513/574 completed (loss: 0.09419792145490646, acc: 0.9603174328804016)
[2025-01-06 01:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:45][root][INFO] - Training Epoch: 7/10, step 514/574 completed (loss: 0.16109272837638855, acc: 0.9285714030265808)
[2025-01-06 01:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:45][root][INFO] - Training Epoch: 7/10, step 515/574 completed (loss: 0.003447932191193104, acc: 1.0)
[2025-01-06 01:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46][root][INFO] - Training Epoch: 7/10, step 516/574 completed (loss: 0.1853807419538498, acc: 0.9444444179534912)
[2025-01-06 01:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46][root][INFO] - Training Epoch: 7/10, step 517/574 completed (loss: 0.00023271415557246655, acc: 1.0)
[2025-01-06 01:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46][root][INFO] - Training Epoch: 7/10, step 518/574 completed (loss: 0.013525388203561306, acc: 1.0)
[2025-01-06 01:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47][root][INFO] - Training Epoch: 7/10, step 519/574 completed (loss: 0.06154017522931099, acc: 0.949999988079071)
[2025-01-06 01:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47][root][INFO] - Training Epoch: 7/10, step 520/574 completed (loss: 0.025319255888462067, acc: 1.0)
[2025-01-06 01:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48][root][INFO] - Training Epoch: 7/10, step 521/574 completed (loss: 0.3257550895214081, acc: 0.8983050584793091)
[2025-01-06 01:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48][root][INFO] - Training Epoch: 7/10, step 522/574 completed (loss: 0.12728986144065857, acc: 0.9701492786407471)
[2025-01-06 01:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49][root][INFO] - Training Epoch: 7/10, step 523/574 completed (loss: 0.14063704013824463, acc: 0.9635036587715149)
[2025-01-06 01:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49][root][INFO] - Training Epoch: 7/10, step 524/574 completed (loss: 0.3719164729118347, acc: 0.9049999713897705)
[2025-01-06 01:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50][root][INFO] - Training Epoch: 7/10, step 525/574 completed (loss: 0.003975576255470514, acc: 1.0)
[2025-01-06 01:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50][root][INFO] - Training Epoch: 7/10, step 526/574 completed (loss: 0.08314640820026398, acc: 0.9807692170143127)
[2025-01-06 01:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50][root][INFO] - Training Epoch: 7/10, step 527/574 completed (loss: 0.0273798368871212, acc: 1.0)
[2025-01-06 01:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51][root][INFO] - Training Epoch: 7/10, step 528/574 completed (loss: 0.2587313652038574, acc: 0.9180327653884888)
[2025-01-06 01:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51][root][INFO] - Training Epoch: 7/10, step 529/574 completed (loss: 0.03171389177441597, acc: 1.0)
[2025-01-06 01:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51][root][INFO] - Training Epoch: 7/10, step 530/574 completed (loss: 0.5333653092384338, acc: 0.8604651093482971)
[2025-01-06 01:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52][root][INFO] - Training Epoch: 7/10, step 531/574 completed (loss: 0.20691746473312378, acc: 0.9545454382896423)
[2025-01-06 01:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52][root][INFO] - Training Epoch: 7/10, step 532/574 completed (loss: 0.04151274636387825, acc: 1.0)
[2025-01-06 01:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52][root][INFO] - Training Epoch: 7/10, step 533/574 completed (loss: 0.1632836014032364, acc: 0.9772727489471436)
[2025-01-06 01:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53][root][INFO] - Training Epoch: 7/10, step 534/574 completed (loss: 0.04709320887923241, acc: 1.0)
[2025-01-06 01:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53][root][INFO] - Training Epoch: 7/10, step 535/574 completed (loss: 0.014911708422005177, acc: 1.0)
[2025-01-06 01:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53][root][INFO] - Training Epoch: 7/10, step 536/574 completed (loss: 0.01140549685806036, acc: 1.0)
[2025-01-06 01:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:54][root][INFO] - Training Epoch: 7/10, step 537/574 completed (loss: 0.10966157913208008, acc: 0.9692307710647583)
[2025-01-06 01:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:54][root][INFO] - Training Epoch: 7/10, step 538/574 completed (loss: 0.18152324855327606, acc: 0.9375)
[2025-01-06 01:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55][root][INFO] - Training Epoch: 7/10, step 539/574 completed (loss: 0.09117332845926285, acc: 1.0)
[2025-01-06 01:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55][root][INFO] - Training Epoch: 7/10, step 540/574 completed (loss: 0.03396732360124588, acc: 1.0)
[2025-01-06 01:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55][root][INFO] - Training Epoch: 7/10, step 541/574 completed (loss: 0.037553563714027405, acc: 1.0)
[2025-01-06 01:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56][root][INFO] - Training Epoch: 7/10, step 542/574 completed (loss: 0.06722762435674667, acc: 0.9677419066429138)
[2025-01-06 01:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56][root][INFO] - Training Epoch: 7/10, step 543/574 completed (loss: 0.002715194830670953, acc: 1.0)
[2025-01-06 01:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57][root][INFO] - Training Epoch: 7/10, step 544/574 completed (loss: 0.03232460841536522, acc: 1.0)
[2025-01-06 01:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57][root][INFO] - Training Epoch: 7/10, step 545/574 completed (loss: 0.14309900999069214, acc: 0.9512194991111755)
[2025-01-06 01:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57][root][INFO] - Training Epoch: 7/10, step 546/574 completed (loss: 0.002651113085448742, acc: 1.0)
[2025-01-06 01:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58][root][INFO] - Training Epoch: 7/10, step 547/574 completed (loss: 0.0039793215692043304, acc: 1.0)
[2025-01-06 01:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58][root][INFO] - Training Epoch: 7/10, step 548/574 completed (loss: 0.007745253387838602, acc: 1.0)
[2025-01-06 01:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58][root][INFO] - Training Epoch: 7/10, step 549/574 completed (loss: 0.0012849484337493777, acc: 1.0)
[2025-01-06 01:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59][root][INFO] - Training Epoch: 7/10, step 550/574 completed (loss: 0.07086117565631866, acc: 0.939393937587738)
[2025-01-06 01:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59][root][INFO] - Training Epoch: 7/10, step 551/574 completed (loss: 0.09715841710567474, acc: 0.9750000238418579)
[2025-01-06 01:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59][root][INFO] - Training Epoch: 7/10, step 552/574 completed (loss: 0.15669263899326324, acc: 0.9571428298950195)
[2025-01-06 01:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00][root][INFO] - Training Epoch: 7/10, step 553/574 completed (loss: 0.0783904418349266, acc: 0.956204354763031)
[2025-01-06 01:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00][root][INFO] - Training Epoch: 7/10, step 554/574 completed (loss: 0.029421178624033928, acc: 0.9862068891525269)
[2025-01-06 01:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00][root][INFO] - Training Epoch: 7/10, step 555/574 completed (loss: 0.12086974829435349, acc: 0.949999988079071)
[2025-01-06 01:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:01][root][INFO] - Training Epoch: 7/10, step 556/574 completed (loss: 0.15027207136154175, acc: 0.9735099077224731)
[2025-01-06 01:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:01][root][INFO] - Training Epoch: 7/10, step 557/574 completed (loss: 0.05085071921348572, acc: 0.9829059839248657)
[2025-01-06 01:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:01][root][INFO] - Training Epoch: 7/10, step 558/574 completed (loss: 0.0032025391701608896, acc: 1.0)
[2025-01-06 01:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:02][root][INFO] - Training Epoch: 7/10, step 559/574 completed (loss: 0.0979900062084198, acc: 0.9615384340286255)
[2025-01-06 01:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:31][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4268, device='cuda:0') eval_epoch_loss=tensor(0.8866, device='cuda:0') eval_epoch_acc=tensor(0.8266, device='cuda:0')
[2025-01-06 01:40:31][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:40:31][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:40:31][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_560_loss_0.8865754008293152/model.pt
[2025-01-06 01:40:31][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32][root][INFO] - Training Epoch: 7/10, step 560/574 completed (loss: 0.0017870725132524967, acc: 1.0)
[2025-01-06 01:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32][root][INFO] - Training Epoch: 7/10, step 561/574 completed (loss: 0.1821829229593277, acc: 0.9743589758872986)
[2025-01-06 01:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32][root][INFO] - Training Epoch: 7/10, step 562/574 completed (loss: 0.18281981348991394, acc: 0.9666666388511658)
[2025-01-06 01:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33][root][INFO] - Training Epoch: 7/10, step 563/574 completed (loss: 0.06237214058637619, acc: 0.9740259647369385)
[2025-01-06 01:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33][root][INFO] - Training Epoch: 7/10, step 564/574 completed (loss: 0.14517821371555328, acc: 0.9583333134651184)
[2025-01-06 01:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33][root][INFO] - Training Epoch: 7/10, step 565/574 completed (loss: 0.03420271724462509, acc: 0.982758641242981)
[2025-01-06 01:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34][root][INFO] - Training Epoch: 7/10, step 566/574 completed (loss: 0.03653784468770027, acc: 0.988095223903656)
[2025-01-06 01:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34][root][INFO] - Training Epoch: 7/10, step 567/574 completed (loss: 0.051581721752882004, acc: 0.9736841917037964)
[2025-01-06 01:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34][root][INFO] - Training Epoch: 7/10, step 568/574 completed (loss: 0.0027295583859086037, acc: 1.0)
[2025-01-06 01:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35][root][INFO] - Training Epoch: 7/10, step 569/574 completed (loss: 0.09639792144298553, acc: 0.9625668525695801)
[2025-01-06 01:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35][root][INFO] - Training Epoch: 7/10, step 570/574 completed (loss: 0.006320232525467873, acc: 1.0)
[2025-01-06 01:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35][root][INFO] - Training Epoch: 7/10, step 571/574 completed (loss: 0.03594614192843437, acc: 0.9914529919624329)
[2025-01-06 01:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:36][root][INFO] - Training Epoch: 7/10, step 572/574 completed (loss: 0.16131781041622162, acc: 0.9489796161651611)
[2025-01-06 01:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:36][root][INFO] - Training Epoch: 7/10, step 573/574 completed (loss: 0.07000480592250824, acc: 0.9874213933944702)
[2025-01-06 01:40:37][slam_llm.utils.train_utils][INFO] - Epoch 7: train_perplexity=1.1399, train_epoch_loss=0.1309, epoch time 342.75879050791264s
[2025-01-06 01:40:37][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:40:37][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 18 GB
[2025-01-06 01:40:37][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:40:37][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 19
[2025-01-06 01:40:37][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:37][root][INFO] - Training Epoch: 8/10, step 0/574 completed (loss: 0.024650931358337402, acc: 1.0)
[2025-01-06 01:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:38][root][INFO] - Training Epoch: 8/10, step 1/574 completed (loss: 0.050760265439748764, acc: 1.0)
[2025-01-06 01:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:38][root][INFO] - Training Epoch: 8/10, step 2/574 completed (loss: 0.24290455877780914, acc: 0.9459459185600281)
[2025-01-06 01:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:38][root][INFO] - Training Epoch: 8/10, step 3/574 completed (loss: 0.01974107138812542, acc: 1.0)
[2025-01-06 01:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:39][root][INFO] - Training Epoch: 8/10, step 4/574 completed (loss: 0.09718291461467743, acc: 0.9459459185600281)
[2025-01-06 01:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:39][root][INFO] - Training Epoch: 8/10, step 5/574 completed (loss: 0.032515257596969604, acc: 1.0)
[2025-01-06 01:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40][root][INFO] - Training Epoch: 8/10, step 6/574 completed (loss: 0.04831629991531372, acc: 0.9795918464660645)
[2025-01-06 01:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40][root][INFO] - Training Epoch: 8/10, step 7/574 completed (loss: 0.03772889822721481, acc: 0.9666666388511658)
[2025-01-06 01:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40][root][INFO] - Training Epoch: 8/10, step 8/574 completed (loss: 0.2929380238056183, acc: 0.9545454382896423)
[2025-01-06 01:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40][root][INFO] - Training Epoch: 8/10, step 9/574 completed (loss: 0.006488691549748182, acc: 1.0)
[2025-01-06 01:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41][root][INFO] - Training Epoch: 8/10, step 10/574 completed (loss: 0.49928396940231323, acc: 0.9629629850387573)
[2025-01-06 01:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41][root][INFO] - Training Epoch: 8/10, step 11/574 completed (loss: 0.13527970016002655, acc: 0.9487179517745972)
[2025-01-06 01:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41][root][INFO] - Training Epoch: 8/10, step 12/574 completed (loss: 0.005344809498637915, acc: 1.0)
[2025-01-06 01:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42][root][INFO] - Training Epoch: 8/10, step 13/574 completed (loss: 0.018739234656095505, acc: 1.0)
[2025-01-06 01:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42][root][INFO] - Training Epoch: 8/10, step 14/574 completed (loss: 0.010526611469686031, acc: 1.0)
[2025-01-06 01:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42][root][INFO] - Training Epoch: 8/10, step 15/574 completed (loss: 0.013876528479158878, acc: 1.0)
[2025-01-06 01:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43][root][INFO] - Training Epoch: 8/10, step 16/574 completed (loss: 0.00758409732952714, acc: 1.0)
[2025-01-06 01:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43][root][INFO] - Training Epoch: 8/10, step 17/574 completed (loss: 0.01152967382222414, acc: 1.0)
[2025-01-06 01:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43][root][INFO] - Training Epoch: 8/10, step 18/574 completed (loss: 0.15421409904956818, acc: 0.9444444179534912)
[2025-01-06 01:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44][root][INFO] - Training Epoch: 8/10, step 19/574 completed (loss: 0.05632776394486427, acc: 1.0)
[2025-01-06 01:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44][root][INFO] - Training Epoch: 8/10, step 20/574 completed (loss: 0.01983962208032608, acc: 1.0)
[2025-01-06 01:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44][root][INFO] - Training Epoch: 8/10, step 21/574 completed (loss: 0.08127110451459885, acc: 0.9655172228813171)
[2025-01-06 01:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45][root][INFO] - Training Epoch: 8/10, step 22/574 completed (loss: 0.011005543172359467, acc: 1.0)
[2025-01-06 01:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45][root][INFO] - Training Epoch: 8/10, step 23/574 completed (loss: 0.006926718167960644, acc: 1.0)
[2025-01-06 01:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45][root][INFO] - Training Epoch: 8/10, step 24/574 completed (loss: 0.006737479940056801, acc: 1.0)
[2025-01-06 01:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46][root][INFO] - Training Epoch: 8/10, step 25/574 completed (loss: 0.2322961837053299, acc: 0.9245283007621765)
[2025-01-06 01:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46][root][INFO] - Training Epoch: 8/10, step 26/574 completed (loss: 0.10205422341823578, acc: 0.9726027250289917)
[2025-01-06 01:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47][root][INFO] - Training Epoch: 8/10, step 27/574 completed (loss: 0.3980765640735626, acc: 0.8774703741073608)
[2025-01-06 01:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47][root][INFO] - Training Epoch: 8/10, step 28/574 completed (loss: 0.0320376418530941, acc: 1.0)
[2025-01-06 01:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48][root][INFO] - Training Epoch: 8/10, step 29/574 completed (loss: 0.08271926641464233, acc: 0.9638554453849792)
[2025-01-06 01:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48][root][INFO] - Training Epoch: 8/10, step 30/574 completed (loss: 0.0378243587911129, acc: 1.0)
[2025-01-06 01:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48][root][INFO] - Training Epoch: 8/10, step 31/574 completed (loss: 0.2628006935119629, acc: 0.9285714030265808)
[2025-01-06 01:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49][root][INFO] - Training Epoch: 8/10, step 32/574 completed (loss: 0.01686270534992218, acc: 1.0)
[2025-01-06 01:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49][root][INFO] - Training Epoch: 8/10, step 33/574 completed (loss: 0.003642983501777053, acc: 1.0)
[2025-01-06 01:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49][root][INFO] - Training Epoch: 8/10, step 34/574 completed (loss: 0.0910114273428917, acc: 0.9663865566253662)
[2025-01-06 01:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50][root][INFO] - Training Epoch: 8/10, step 35/574 completed (loss: 0.11572108417749405, acc: 0.9836065769195557)
[2025-01-06 01:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50][root][INFO] - Training Epoch: 8/10, step 36/574 completed (loss: 0.09692147374153137, acc: 0.9682539701461792)
[2025-01-06 01:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50][root][INFO] - Training Epoch: 8/10, step 37/574 completed (loss: 0.09280568361282349, acc: 0.9491525292396545)
[2025-01-06 01:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51][root][INFO] - Training Epoch: 8/10, step 38/574 completed (loss: 0.07948688417673111, acc: 0.9655172228813171)
[2025-01-06 01:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51][root][INFO] - Training Epoch: 8/10, step 39/574 completed (loss: 0.03676338866353035, acc: 1.0)
[2025-01-06 01:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51][root][INFO] - Training Epoch: 8/10, step 40/574 completed (loss: 0.014878841117024422, acc: 1.0)
[2025-01-06 01:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52][root][INFO] - Training Epoch: 8/10, step 41/574 completed (loss: 0.26559653878211975, acc: 0.8918918967247009)
[2025-01-06 01:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52][root][INFO] - Training Epoch: 8/10, step 42/574 completed (loss: 0.17207838594913483, acc: 0.9384615421295166)
[2025-01-06 01:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52][root][INFO] - Training Epoch: 8/10, step 43/574 completed (loss: 0.15140597522258759, acc: 0.9494949579238892)
[2025-01-06 01:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53][root][INFO] - Training Epoch: 8/10, step 44/574 completed (loss: 0.14979976415634155, acc: 0.9484536051750183)
[2025-01-06 01:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53][root][INFO] - Training Epoch: 8/10, step 45/574 completed (loss: 0.09391175955533981, acc: 0.9779411554336548)
[2025-01-06 01:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54][root][INFO] - Training Epoch: 8/10, step 46/574 completed (loss: 0.037893153727054596, acc: 1.0)
[2025-01-06 01:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54][root][INFO] - Training Epoch: 8/10, step 47/574 completed (loss: 0.1521957963705063, acc: 0.9629629850387573)
[2025-01-06 01:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54][root][INFO] - Training Epoch: 8/10, step 48/574 completed (loss: 0.011204337701201439, acc: 1.0)
[2025-01-06 01:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54][root][INFO] - Training Epoch: 8/10, step 49/574 completed (loss: 0.0030293345917016268, acc: 1.0)
[2025-01-06 01:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55][root][INFO] - Training Epoch: 8/10, step 50/574 completed (loss: 0.056967973709106445, acc: 1.0)
[2025-01-06 01:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55][root][INFO] - Training Epoch: 8/10, step 51/574 completed (loss: 0.11215487122535706, acc: 0.9682539701461792)
[2025-01-06 01:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55][root][INFO] - Training Epoch: 8/10, step 52/574 completed (loss: 0.24692277610301971, acc: 0.8873239159584045)
[2025-01-06 01:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:56][root][INFO] - Training Epoch: 8/10, step 53/574 completed (loss: 0.4940071702003479, acc: 0.8266666531562805)
[2025-01-06 01:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:56][root][INFO] - Training Epoch: 8/10, step 54/574 completed (loss: 0.23361964523792267, acc: 0.9729729890823364)
[2025-01-06 01:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:57][root][INFO] - Training Epoch: 8/10, step 55/574 completed (loss: 0.09206220507621765, acc: 0.9615384340286255)
[2025-01-06 01:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:59][root][INFO] - Training Epoch: 8/10, step 56/574 completed (loss: 0.4807559549808502, acc: 0.8532423377037048)
[2025-01-06 01:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:01][root][INFO] - Training Epoch: 8/10, step 57/574 completed (loss: 0.766803503036499, acc: 0.7625272274017334)
[2025-01-06 01:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:01][root][INFO] - Training Epoch: 8/10, step 58/574 completed (loss: 0.3257201910018921, acc: 0.9034090638160706)
[2025-01-06 01:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02][root][INFO] - Training Epoch: 8/10, step 59/574 completed (loss: 0.11999915540218353, acc: 0.9632353186607361)
[2025-01-06 01:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02][root][INFO] - Training Epoch: 8/10, step 60/574 completed (loss: 0.3996804356575012, acc: 0.8695651888847351)
[2025-01-06 01:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03][root][INFO] - Training Epoch: 8/10, step 61/574 completed (loss: 0.1591884195804596, acc: 0.949999988079071)
[2025-01-06 01:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03][root][INFO] - Training Epoch: 8/10, step 62/574 completed (loss: 0.03739231824874878, acc: 1.0)
[2025-01-06 01:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04][root][INFO] - Training Epoch: 8/10, step 63/574 completed (loss: 0.03328123316168785, acc: 1.0)
[2025-01-06 01:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04][root][INFO] - Training Epoch: 8/10, step 64/574 completed (loss: 0.01799294352531433, acc: 1.0)
[2025-01-06 01:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04][root][INFO] - Training Epoch: 8/10, step 65/574 completed (loss: 0.044222306460142136, acc: 0.9655172228813171)
[2025-01-06 01:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04][root][INFO] - Training Epoch: 8/10, step 66/574 completed (loss: 0.12267600744962692, acc: 0.9642857313156128)
[2025-01-06 01:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05][root][INFO] - Training Epoch: 8/10, step 67/574 completed (loss: 0.12523190677165985, acc: 0.949999988079071)
[2025-01-06 01:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05][root][INFO] - Training Epoch: 8/10, step 68/574 completed (loss: 0.004268779885023832, acc: 1.0)
[2025-01-06 01:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05][root][INFO] - Training Epoch: 8/10, step 69/574 completed (loss: 0.03020535223186016, acc: 1.0)
[2025-01-06 01:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06][root][INFO] - Training Epoch: 8/10, step 70/574 completed (loss: 0.12228167057037354, acc: 0.9696969985961914)
[2025-01-06 01:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06][root][INFO] - Training Epoch: 8/10, step 71/574 completed (loss: 0.2834249436855316, acc: 0.9191176295280457)
[2025-01-06 01:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06][root][INFO] - Training Epoch: 8/10, step 72/574 completed (loss: 0.17083531618118286, acc: 0.9682539701461792)
[2025-01-06 01:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07][root][INFO] - Training Epoch: 8/10, step 73/574 completed (loss: 0.6844009160995483, acc: 0.8256410360336304)
[2025-01-06 01:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07][root][INFO] - Training Epoch: 8/10, step 74/574 completed (loss: 0.3520187735557556, acc: 0.8877550959587097)
[2025-01-06 01:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07][root][INFO] - Training Epoch: 8/10, step 75/574 completed (loss: 0.34010788798332214, acc: 0.89552241563797)
[2025-01-06 01:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08][root][INFO] - Training Epoch: 8/10, step 76/574 completed (loss: 0.7362626194953918, acc: 0.7737226486206055)
[2025-01-06 01:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08][root][INFO] - Training Epoch: 8/10, step 77/574 completed (loss: 0.007866952568292618, acc: 1.0)
[2025-01-06 01:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08][root][INFO] - Training Epoch: 8/10, step 78/574 completed (loss: 0.00383321032859385, acc: 1.0)
[2025-01-06 01:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09][root][INFO] - Training Epoch: 8/10, step 79/574 completed (loss: 0.006900681648403406, acc: 1.0)
[2025-01-06 01:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09][root][INFO] - Training Epoch: 8/10, step 80/574 completed (loss: 0.24056309461593628, acc: 0.9230769276618958)
[2025-01-06 01:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09][root][INFO] - Training Epoch: 8/10, step 81/574 completed (loss: 0.023477228358387947, acc: 1.0)
[2025-01-06 01:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10][root][INFO] - Training Epoch: 8/10, step 82/574 completed (loss: 0.2856416702270508, acc: 0.9615384340286255)
[2025-01-06 01:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10][root][INFO] - Training Epoch: 8/10, step 83/574 completed (loss: 0.04095231741666794, acc: 1.0)
[2025-01-06 01:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10][root][INFO] - Training Epoch: 8/10, step 84/574 completed (loss: 0.06159322336316109, acc: 0.9855072498321533)
[2025-01-06 01:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11][root][INFO] - Training Epoch: 8/10, step 85/574 completed (loss: 0.06481005996465683, acc: 0.9800000190734863)
[2025-01-06 01:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11][root][INFO] - Training Epoch: 8/10, step 86/574 completed (loss: 0.0219110194593668, acc: 1.0)
[2025-01-06 01:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11][root][INFO] - Training Epoch: 8/10, step 87/574 completed (loss: 0.2689208984375, acc: 0.9399999976158142)
[2025-01-06 01:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:12][root][INFO] - Training Epoch: 8/10, step 88/574 completed (loss: 0.24238230288028717, acc: 0.9320388436317444)
[2025-01-06 01:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:13][root][INFO] - Training Epoch: 8/10, step 89/574 completed (loss: 0.379447340965271, acc: 0.8737863898277283)
[2025-01-06 01:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14][root][INFO] - Training Epoch: 8/10, step 90/574 completed (loss: 0.34783533215522766, acc: 0.8817204236984253)
[2025-01-06 01:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14][root][INFO] - Training Epoch: 8/10, step 91/574 completed (loss: 0.35053837299346924, acc: 0.8965517282485962)
[2025-01-06 01:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:15][root][INFO] - Training Epoch: 8/10, step 92/574 completed (loss: 0.2709995210170746, acc: 0.9052631855010986)
[2025-01-06 01:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16][root][INFO] - Training Epoch: 8/10, step 93/574 completed (loss: 0.361123651266098, acc: 0.9108911156654358)
[2025-01-06 01:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16][root][INFO] - Training Epoch: 8/10, step 94/574 completed (loss: 0.1310790479183197, acc: 0.9838709831237793)
[2025-01-06 01:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17][root][INFO] - Training Epoch: 8/10, step 95/574 completed (loss: 0.17726510763168335, acc: 0.9420289993286133)
[2025-01-06 01:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17][root][INFO] - Training Epoch: 8/10, step 96/574 completed (loss: 0.22455421090126038, acc: 0.9411764740943909)
[2025-01-06 01:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17][root][INFO] - Training Epoch: 8/10, step 97/574 completed (loss: 0.15735021233558655, acc: 0.9615384340286255)
[2025-01-06 01:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18][root][INFO] - Training Epoch: 8/10, step 98/574 completed (loss: 0.393633633852005, acc: 0.8759124279022217)
[2025-01-06 01:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18][root][INFO] - Training Epoch: 8/10, step 99/574 completed (loss: 0.25333061814308167, acc: 0.9104477763175964)
[2025-01-06 01:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18][root][INFO] - Training Epoch: 8/10, step 100/574 completed (loss: 0.03602786362171173, acc: 1.0)
[2025-01-06 01:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19][root][INFO] - Training Epoch: 8/10, step 101/574 completed (loss: 0.005032481625676155, acc: 1.0)
[2025-01-06 01:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19][root][INFO] - Training Epoch: 8/10, step 102/574 completed (loss: 0.5152036547660828, acc: 0.9130434989929199)
[2025-01-06 01:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19][root][INFO] - Training Epoch: 8/10, step 103/574 completed (loss: 0.02259145863354206, acc: 0.9772727489471436)
[2025-01-06 01:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20][root][INFO] - Training Epoch: 8/10, step 104/574 completed (loss: 0.26770761609077454, acc: 0.982758641242981)
[2025-01-06 01:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20][root][INFO] - Training Epoch: 8/10, step 105/574 completed (loss: 0.01817660965025425, acc: 1.0)
[2025-01-06 01:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20][root][INFO] - Training Epoch: 8/10, step 106/574 completed (loss: 0.06426442414522171, acc: 0.9599999785423279)
[2025-01-06 01:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21][root][INFO] - Training Epoch: 8/10, step 107/574 completed (loss: 0.0022600735537707806, acc: 1.0)
[2025-01-06 01:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21][root][INFO] - Training Epoch: 8/10, step 108/574 completed (loss: 0.015891743823885918, acc: 1.0)
[2025-01-06 01:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21][root][INFO] - Training Epoch: 8/10, step 109/574 completed (loss: 0.054543476551771164, acc: 0.9523809552192688)
[2025-01-06 01:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22][root][INFO] - Training Epoch: 8/10, step 110/574 completed (loss: 0.022757943719625473, acc: 1.0)
[2025-01-06 01:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22][root][INFO] - Training Epoch: 8/10, step 111/574 completed (loss: 0.034680016338825226, acc: 0.9824561476707458)
[2025-01-06 01:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23][root][INFO] - Training Epoch: 8/10, step 112/574 completed (loss: 0.3918715715408325, acc: 0.9122806787490845)
[2025-01-06 01:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23][root][INFO] - Training Epoch: 8/10, step 113/574 completed (loss: 0.02649998664855957, acc: 1.0)
[2025-01-06 01:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23][root][INFO] - Training Epoch: 8/10, step 114/574 completed (loss: 0.08050717413425446, acc: 0.9591836929321289)
[2025-01-06 01:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24][root][INFO] - Training Epoch: 8/10, step 115/574 completed (loss: 0.004632908385246992, acc: 1.0)
[2025-01-06 01:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24][root][INFO] - Training Epoch: 8/10, step 116/574 completed (loss: 0.01611805334687233, acc: 1.0)
[2025-01-06 01:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24][root][INFO] - Training Epoch: 8/10, step 117/574 completed (loss: 0.08393282443284988, acc: 0.9756097793579102)
[2025-01-06 01:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:25][root][INFO] - Training Epoch: 8/10, step 118/574 completed (loss: 0.022323567420244217, acc: 0.9838709831237793)
[2025-01-06 01:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26][root][INFO] - Training Epoch: 8/10, step 119/574 completed (loss: 0.2935182750225067, acc: 0.9201520681381226)
[2025-01-06 01:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26][root][INFO] - Training Epoch: 8/10, step 120/574 completed (loss: 0.04266061633825302, acc: 0.9733333587646484)
[2025-01-06 01:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26][root][INFO] - Training Epoch: 8/10, step 121/574 completed (loss: 0.12144359201192856, acc: 0.9615384340286255)
[2025-01-06 01:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27][root][INFO] - Training Epoch: 8/10, step 122/574 completed (loss: 0.004346610978245735, acc: 1.0)
[2025-01-06 01:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27][root][INFO] - Training Epoch: 8/10, step 123/574 completed (loss: 0.008665991015732288, acc: 1.0)
[2025-01-06 01:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27][root][INFO] - Training Epoch: 8/10, step 124/574 completed (loss: 0.23949608206748962, acc: 0.9325153231620789)
[2025-01-06 01:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28][root][INFO] - Training Epoch: 8/10, step 125/574 completed (loss: 0.36680713295936584, acc: 0.8819444179534912)
[2025-01-06 01:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28][root][INFO] - Training Epoch: 8/10, step 126/574 completed (loss: 0.3642045557498932, acc: 0.8916666507720947)
[2025-01-06 01:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28][root][INFO] - Training Epoch: 8/10, step 127/574 completed (loss: 0.18115365505218506, acc: 0.9345238208770752)
[2025-01-06 01:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:29][root][INFO] - Training Epoch: 8/10, step 128/574 completed (loss: 0.30057671666145325, acc: 0.9076923131942749)
[2025-01-06 01:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:56][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3654, device='cuda:0') eval_epoch_loss=tensor(0.8609, device='cuda:0') eval_epoch_acc=tensor(0.8309, device='cuda:0')
[2025-01-06 01:41:56][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:41:56][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:41:56][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_129_loss_0.8609286546707153/model.pt
[2025-01-06 01:41:56][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:57][root][INFO] - Training Epoch: 8/10, step 129/574 completed (loss: 0.3849679231643677, acc: 0.8897058963775635)
[2025-01-06 01:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:57][root][INFO] - Training Epoch: 8/10, step 130/574 completed (loss: 0.10991000384092331, acc: 0.9615384340286255)
[2025-01-06 01:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:57][root][INFO] - Training Epoch: 8/10, step 131/574 completed (loss: 0.0582256056368351, acc: 0.95652174949646)
[2025-01-06 01:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58][root][INFO] - Training Epoch: 8/10, step 132/574 completed (loss: 0.017022376880049706, acc: 1.0)
[2025-01-06 01:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58][root][INFO] - Training Epoch: 8/10, step 133/574 completed (loss: 0.015200737863779068, acc: 1.0)
[2025-01-06 01:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58][root][INFO] - Training Epoch: 8/10, step 134/574 completed (loss: 0.05525171756744385, acc: 1.0)
[2025-01-06 01:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58][root][INFO] - Training Epoch: 8/10, step 135/574 completed (loss: 0.045774415135383606, acc: 1.0)
[2025-01-06 01:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59][root][INFO] - Training Epoch: 8/10, step 136/574 completed (loss: 0.09125235676765442, acc: 0.976190447807312)
[2025-01-06 01:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59][root][INFO] - Training Epoch: 8/10, step 137/574 completed (loss: 0.04885842278599739, acc: 1.0)
[2025-01-06 01:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59][root][INFO] - Training Epoch: 8/10, step 138/574 completed (loss: 0.00647937273606658, acc: 1.0)
[2025-01-06 01:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00][root][INFO] - Training Epoch: 8/10, step 139/574 completed (loss: 0.005421584006398916, acc: 1.0)
[2025-01-06 01:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00][root][INFO] - Training Epoch: 8/10, step 140/574 completed (loss: 0.08052247017621994, acc: 0.9615384340286255)
[2025-01-06 01:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00][root][INFO] - Training Epoch: 8/10, step 141/574 completed (loss: 0.028414985164999962, acc: 1.0)
[2025-01-06 01:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01][root][INFO] - Training Epoch: 8/10, step 142/574 completed (loss: 0.26824864745140076, acc: 0.8918918967247009)
[2025-01-06 01:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01][root][INFO] - Training Epoch: 8/10, step 143/574 completed (loss: 0.1910126507282257, acc: 0.9210526347160339)
[2025-01-06 01:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01][root][INFO] - Training Epoch: 8/10, step 144/574 completed (loss: 0.24612028896808624, acc: 0.9626865386962891)
[2025-01-06 01:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02][root][INFO] - Training Epoch: 8/10, step 145/574 completed (loss: 0.23483118414878845, acc: 0.9285714030265808)
[2025-01-06 01:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02][root][INFO] - Training Epoch: 8/10, step 146/574 completed (loss: 0.32760095596313477, acc: 0.8617021441459656)
[2025-01-06 01:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03][root][INFO] - Training Epoch: 8/10, step 147/574 completed (loss: 0.09142474085092545, acc: 0.9571428298950195)
[2025-01-06 01:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03][root][INFO] - Training Epoch: 8/10, step 148/574 completed (loss: 0.1305272877216339, acc: 0.9285714030265808)
[2025-01-06 01:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03][root][INFO] - Training Epoch: 8/10, step 149/574 completed (loss: 0.044442400336265564, acc: 0.95652174949646)
[2025-01-06 01:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03][root][INFO] - Training Epoch: 8/10, step 150/574 completed (loss: 0.011084314435720444, acc: 1.0)
[2025-01-06 01:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04][root][INFO] - Training Epoch: 8/10, step 151/574 completed (loss: 0.11734996736049652, acc: 0.95652174949646)
[2025-01-06 01:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04][root][INFO] - Training Epoch: 8/10, step 152/574 completed (loss: 0.1696661114692688, acc: 0.9322034120559692)
[2025-01-06 01:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04][root][INFO] - Training Epoch: 8/10, step 153/574 completed (loss: 0.14414052665233612, acc: 0.9298245906829834)
[2025-01-06 01:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05][root][INFO] - Training Epoch: 8/10, step 154/574 completed (loss: 0.13951164484024048, acc: 0.9594594836235046)
[2025-01-06 01:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05][root][INFO] - Training Epoch: 8/10, step 155/574 completed (loss: 0.0028845726046711206, acc: 1.0)
[2025-01-06 01:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05][root][INFO] - Training Epoch: 8/10, step 156/574 completed (loss: 0.02275003492832184, acc: 1.0)
[2025-01-06 01:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:06][root][INFO] - Training Epoch: 8/10, step 157/574 completed (loss: 0.16385363042354584, acc: 0.9473684430122375)
[2025-01-06 01:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:07][root][INFO] - Training Epoch: 8/10, step 158/574 completed (loss: 0.1030183881521225, acc: 0.9594594836235046)
[2025-01-06 01:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:08][root][INFO] - Training Epoch: 8/10, step 159/574 completed (loss: 0.18249700963497162, acc: 0.9259259104728699)
[2025-01-06 01:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:08][root][INFO] - Training Epoch: 8/10, step 160/574 completed (loss: 0.1275772899389267, acc: 0.9534883499145508)
[2025-01-06 01:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09][root][INFO] - Training Epoch: 8/10, step 161/574 completed (loss: 0.48159533739089966, acc: 0.8941176533699036)
[2025-01-06 01:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09][root][INFO] - Training Epoch: 8/10, step 162/574 completed (loss: 0.41518285870552063, acc: 0.8876404762268066)
[2025-01-06 01:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09][root][INFO] - Training Epoch: 8/10, step 163/574 completed (loss: 0.060063425451517105, acc: 0.9772727489471436)
[2025-01-06 01:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:10][root][INFO] - Training Epoch: 8/10, step 164/574 completed (loss: 0.16064903140068054, acc: 0.9523809552192688)
[2025-01-06 01:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:10][root][INFO] - Training Epoch: 8/10, step 165/574 completed (loss: 0.08398151397705078, acc: 0.9655172228813171)
[2025-01-06 01:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:10][root][INFO] - Training Epoch: 8/10, step 166/574 completed (loss: 0.033348940312862396, acc: 1.0)
[2025-01-06 01:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:11][root][INFO] - Training Epoch: 8/10, step 167/574 completed (loss: 0.17289398610591888, acc: 0.9200000166893005)
[2025-01-06 01:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:11][root][INFO] - Training Epoch: 8/10, step 168/574 completed (loss: 0.06524788588285446, acc: 0.9861111044883728)
[2025-01-06 01:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:12][root][INFO] - Training Epoch: 8/10, step 169/574 completed (loss: 0.3342537581920624, acc: 0.9313725233078003)
[2025-01-06 01:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13][root][INFO] - Training Epoch: 8/10, step 170/574 completed (loss: 0.24660994112491608, acc: 0.9178082346916199)
[2025-01-06 01:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13][root][INFO] - Training Epoch: 8/10, step 171/574 completed (loss: 0.03184181824326515, acc: 0.9583333134651184)
[2025-01-06 01:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13][root][INFO] - Training Epoch: 8/10, step 172/574 completed (loss: 0.038590360432863235, acc: 1.0)
[2025-01-06 01:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:14][root][INFO] - Training Epoch: 8/10, step 173/574 completed (loss: 0.05037887021899223, acc: 0.9642857313156128)
[2025-01-06 01:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:14][root][INFO] - Training Epoch: 8/10, step 174/574 completed (loss: 0.3316101133823395, acc: 0.9026548862457275)
[2025-01-06 01:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:14][root][INFO] - Training Epoch: 8/10, step 175/574 completed (loss: 0.24169319868087769, acc: 0.9275362491607666)
[2025-01-06 01:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:15][root][INFO] - Training Epoch: 8/10, step 176/574 completed (loss: 0.05610295757651329, acc: 0.9772727489471436)
[2025-01-06 01:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:16][root][INFO] - Training Epoch: 8/10, step 177/574 completed (loss: 0.4228670597076416, acc: 0.885496199131012)
[2025-01-06 01:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:16][root][INFO] - Training Epoch: 8/10, step 178/574 completed (loss: 0.27843448519706726, acc: 0.9259259104728699)
[2025-01-06 01:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17][root][INFO] - Training Epoch: 8/10, step 179/574 completed (loss: 0.06533005833625793, acc: 0.9836065769195557)
[2025-01-06 01:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17][root][INFO] - Training Epoch: 8/10, step 180/574 completed (loss: 0.005081685725599527, acc: 1.0)
[2025-01-06 01:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17][root][INFO] - Training Epoch: 8/10, step 181/574 completed (loss: 0.0004182456177659333, acc: 1.0)
[2025-01-06 01:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18][root][INFO] - Training Epoch: 8/10, step 182/574 completed (loss: 0.02031443826854229, acc: 1.0)
[2025-01-06 01:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18][root][INFO] - Training Epoch: 8/10, step 183/574 completed (loss: 0.03344447910785675, acc: 1.0)
[2025-01-06 01:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19][root][INFO] - Training Epoch: 8/10, step 184/574 completed (loss: 0.24062174558639526, acc: 0.9335347414016724)
[2025-01-06 01:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19][root][INFO] - Training Epoch: 8/10, step 185/574 completed (loss: 0.18315403163433075, acc: 0.9365994334220886)
[2025-01-06 01:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19][root][INFO] - Training Epoch: 8/10, step 186/574 completed (loss: 0.15710128843784332, acc: 0.953125)
[2025-01-06 01:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20][root][INFO] - Training Epoch: 8/10, step 187/574 completed (loss: 0.25296762585639954, acc: 0.9193245768547058)
[2025-01-06 01:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20][root][INFO] - Training Epoch: 8/10, step 188/574 completed (loss: 0.27638664841651917, acc: 0.918149471282959)
[2025-01-06 01:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:21][root][INFO] - Training Epoch: 8/10, step 189/574 completed (loss: 0.2854507863521576, acc: 0.9599999785423279)
[2025-01-06 01:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:21][root][INFO] - Training Epoch: 8/10, step 190/574 completed (loss: 0.14251424372196198, acc: 0.9534883499145508)
[2025-01-06 01:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:22][root][INFO] - Training Epoch: 8/10, step 191/574 completed (loss: 0.40436726808547974, acc: 0.8571428656578064)
[2025-01-06 01:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:23][root][INFO] - Training Epoch: 8/10, step 192/574 completed (loss: 0.33113381266593933, acc: 0.8939393758773804)
[2025-01-06 01:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:24][root][INFO] - Training Epoch: 8/10, step 193/574 completed (loss: 0.158001109957695, acc: 0.929411768913269)
[2025-01-06 01:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:25][root][INFO] - Training Epoch: 8/10, step 194/574 completed (loss: 0.47775721549987793, acc: 0.8888888955116272)
[2025-01-06 01:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26][root][INFO] - Training Epoch: 8/10, step 195/574 completed (loss: 0.04732285067439079, acc: 0.9838709831237793)
[2025-01-06 01:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26][root][INFO] - Training Epoch: 8/10, step 196/574 completed (loss: 0.003712752368301153, acc: 1.0)
[2025-01-06 01:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26][root][INFO] - Training Epoch: 8/10, step 197/574 completed (loss: 0.01710064709186554, acc: 1.0)
[2025-01-06 01:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27][root][INFO] - Training Epoch: 8/10, step 198/574 completed (loss: 0.10972170531749725, acc: 0.970588207244873)
[2025-01-06 01:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27][root][INFO] - Training Epoch: 8/10, step 199/574 completed (loss: 0.31630071997642517, acc: 0.8897058963775635)
[2025-01-06 01:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27][root][INFO] - Training Epoch: 8/10, step 200/574 completed (loss: 0.10611085593700409, acc: 0.9745762944221497)
[2025-01-06 01:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:28][root][INFO] - Training Epoch: 8/10, step 201/574 completed (loss: 0.18975886702537537, acc: 0.9328358173370361)
[2025-01-06 01:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:28][root][INFO] - Training Epoch: 8/10, step 202/574 completed (loss: 0.11512531340122223, acc: 0.9417475461959839)
[2025-01-06 01:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:28][root][INFO] - Training Epoch: 8/10, step 203/574 completed (loss: 0.03659132868051529, acc: 1.0)
[2025-01-06 01:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:29][root][INFO] - Training Epoch: 8/10, step 204/574 completed (loss: 0.007752655074000359, acc: 1.0)
[2025-01-06 01:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:29][root][INFO] - Training Epoch: 8/10, step 205/574 completed (loss: 0.09307248145341873, acc: 0.9506726264953613)
[2025-01-06 01:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30][root][INFO] - Training Epoch: 8/10, step 206/574 completed (loss: 0.14783787727355957, acc: 0.9527559280395508)
[2025-01-06 01:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30][root][INFO] - Training Epoch: 8/10, step 207/574 completed (loss: 0.09971287846565247, acc: 0.9698275923728943)
[2025-01-06 01:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30][root][INFO] - Training Epoch: 8/10, step 208/574 completed (loss: 0.23044291138648987, acc: 0.9275362491607666)
[2025-01-06 01:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31][root][INFO] - Training Epoch: 8/10, step 209/574 completed (loss: 0.1094890758395195, acc: 0.9533073902130127)
[2025-01-06 01:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31][root][INFO] - Training Epoch: 8/10, step 210/574 completed (loss: 0.03839975595474243, acc: 0.97826087474823)
[2025-01-06 01:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31][root][INFO] - Training Epoch: 8/10, step 211/574 completed (loss: 0.0091606630012393, acc: 1.0)
[2025-01-06 01:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31][root][INFO] - Training Epoch: 8/10, step 212/574 completed (loss: 0.05004812404513359, acc: 0.9642857313156128)
[2025-01-06 01:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32][root][INFO] - Training Epoch: 8/10, step 213/574 completed (loss: 0.0277533121407032, acc: 1.0)
[2025-01-06 01:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32][root][INFO] - Training Epoch: 8/10, step 214/574 completed (loss: 0.07642871141433716, acc: 0.9692307710647583)
[2025-01-06 01:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:33][root][INFO] - Training Epoch: 8/10, step 215/574 completed (loss: 0.05386996641755104, acc: 0.9864864945411682)
[2025-01-06 01:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:33][root][INFO] - Training Epoch: 8/10, step 216/574 completed (loss: 0.006723704747855663, acc: 1.0)
[2025-01-06 01:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34][root][INFO] - Training Epoch: 8/10, step 217/574 completed (loss: 0.13269801437854767, acc: 0.9639639854431152)
[2025-01-06 01:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34][root][INFO] - Training Epoch: 8/10, step 218/574 completed (loss: 0.04397976025938988, acc: 0.9777777791023254)
[2025-01-06 01:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34][root][INFO] - Training Epoch: 8/10, step 219/574 completed (loss: 0.032058145850896835, acc: 1.0)
[2025-01-06 01:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35][root][INFO] - Training Epoch: 8/10, step 220/574 completed (loss: 0.0005277091986499727, acc: 1.0)
[2025-01-06 01:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35][root][INFO] - Training Epoch: 8/10, step 221/574 completed (loss: 0.011461309157311916, acc: 1.0)
[2025-01-06 01:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35][root][INFO] - Training Epoch: 8/10, step 222/574 completed (loss: 0.044275227934122086, acc: 0.9807692170143127)
[2025-01-06 01:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:36][root][INFO] - Training Epoch: 8/10, step 223/574 completed (loss: 0.1021774560213089, acc: 0.9728260636329651)
[2025-01-06 01:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37][root][INFO] - Training Epoch: 8/10, step 224/574 completed (loss: 0.17797024548053741, acc: 0.9488636255264282)
[2025-01-06 01:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37][root][INFO] - Training Epoch: 8/10, step 225/574 completed (loss: 0.1999196857213974, acc: 0.9468085169792175)
[2025-01-06 01:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37][root][INFO] - Training Epoch: 8/10, step 226/574 completed (loss: 0.08776164799928665, acc: 0.9622641801834106)
[2025-01-06 01:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38][root][INFO] - Training Epoch: 8/10, step 227/574 completed (loss: 0.1660614013671875, acc: 0.9666666388511658)
[2025-01-06 01:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38][root][INFO] - Training Epoch: 8/10, step 228/574 completed (loss: 0.12453290075063705, acc: 0.9534883499145508)
[2025-01-06 01:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38][root][INFO] - Training Epoch: 8/10, step 229/574 completed (loss: 0.47861555218696594, acc: 0.8999999761581421)
[2025-01-06 01:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39][root][INFO] - Training Epoch: 8/10, step 230/574 completed (loss: 0.5375162959098816, acc: 0.8526315689086914)
[2025-01-06 01:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39][root][INFO] - Training Epoch: 8/10, step 231/574 completed (loss: 0.2739321291446686, acc: 0.9222221970558167)
[2025-01-06 01:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39][root][INFO] - Training Epoch: 8/10, step 232/574 completed (loss: 0.541994571685791, acc: 0.8222222328186035)
[2025-01-06 01:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40][root][INFO] - Training Epoch: 8/10, step 233/574 completed (loss: 0.7920323014259338, acc: 0.7706422209739685)
[2025-01-06 01:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40][root][INFO] - Training Epoch: 8/10, step 234/574 completed (loss: 0.36243465542793274, acc: 0.892307698726654)
[2025-01-06 01:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41][root][INFO] - Training Epoch: 8/10, step 235/574 completed (loss: 0.005952051375061274, acc: 1.0)
[2025-01-06 01:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41][root][INFO] - Training Epoch: 8/10, step 236/574 completed (loss: 0.006194393616169691, acc: 1.0)
[2025-01-06 01:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41][root][INFO] - Training Epoch: 8/10, step 237/574 completed (loss: 0.03253305330872536, acc: 1.0)
[2025-01-06 01:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42][root][INFO] - Training Epoch: 8/10, step 238/574 completed (loss: 0.16671635210514069, acc: 0.9629629850387573)
[2025-01-06 01:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42][root][INFO] - Training Epoch: 8/10, step 239/574 completed (loss: 0.02603250928223133, acc: 1.0)
[2025-01-06 01:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42][root][INFO] - Training Epoch: 8/10, step 240/574 completed (loss: 0.13511845469474792, acc: 0.9545454382896423)
[2025-01-06 01:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43][root][INFO] - Training Epoch: 8/10, step 241/574 completed (loss: 0.19903001189231873, acc: 0.9772727489471436)
[2025-01-06 01:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43][root][INFO] - Training Epoch: 8/10, step 242/574 completed (loss: 0.12125416100025177, acc: 0.9838709831237793)
[2025-01-06 01:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44][root][INFO] - Training Epoch: 8/10, step 243/574 completed (loss: 0.11962330341339111, acc: 0.9772727489471436)
[2025-01-06 01:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44][root][INFO] - Training Epoch: 8/10, step 244/574 completed (loss: 0.0004790894454345107, acc: 1.0)
[2025-01-06 01:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44][root][INFO] - Training Epoch: 8/10, step 245/574 completed (loss: 0.10266898572444916, acc: 0.9615384340286255)
[2025-01-06 01:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45][root][INFO] - Training Epoch: 8/10, step 246/574 completed (loss: 0.013061102479696274, acc: 1.0)
[2025-01-06 01:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45][root][INFO] - Training Epoch: 8/10, step 247/574 completed (loss: 0.012060213834047318, acc: 1.0)
[2025-01-06 01:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45][root][INFO] - Training Epoch: 8/10, step 248/574 completed (loss: 0.021615153178572655, acc: 1.0)
[2025-01-06 01:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46][root][INFO] - Training Epoch: 8/10, step 249/574 completed (loss: 0.020049266517162323, acc: 1.0)
[2025-01-06 01:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46][root][INFO] - Training Epoch: 8/10, step 250/574 completed (loss: 0.0045240200124681, acc: 1.0)
[2025-01-06 01:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46][root][INFO] - Training Epoch: 8/10, step 251/574 completed (loss: 0.03893671929836273, acc: 0.9852941036224365)
[2025-01-06 01:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47][root][INFO] - Training Epoch: 8/10, step 252/574 completed (loss: 0.006977087818086147, acc: 1.0)
[2025-01-06 01:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47][root][INFO] - Training Epoch: 8/10, step 253/574 completed (loss: 0.004838174674659967, acc: 1.0)
[2025-01-06 01:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47][root][INFO] - Training Epoch: 8/10, step 254/574 completed (loss: 0.00053030886920169, acc: 1.0)
[2025-01-06 01:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48][root][INFO] - Training Epoch: 8/10, step 255/574 completed (loss: 0.024779560044407845, acc: 1.0)
[2025-01-06 01:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48][root][INFO] - Training Epoch: 8/10, step 256/574 completed (loss: 0.0020464416593313217, acc: 1.0)
[2025-01-06 01:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48][root][INFO] - Training Epoch: 8/10, step 257/574 completed (loss: 0.14156872034072876, acc: 0.9714285731315613)
[2025-01-06 01:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:49][root][INFO] - Training Epoch: 8/10, step 258/574 completed (loss: 0.007149716839194298, acc: 1.0)
[2025-01-06 01:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:49][root][INFO] - Training Epoch: 8/10, step 259/574 completed (loss: 0.11561931669712067, acc: 0.9528301954269409)
[2025-01-06 01:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50][root][INFO] - Training Epoch: 8/10, step 260/574 completed (loss: 0.08496116101741791, acc: 0.9750000238418579)
[2025-01-06 01:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50][root][INFO] - Training Epoch: 8/10, step 261/574 completed (loss: 0.007569076493382454, acc: 1.0)
[2025-01-06 01:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50][root][INFO] - Training Epoch: 8/10, step 262/574 completed (loss: 0.06283717602491379, acc: 0.9677419066429138)
[2025-01-06 01:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51][root][INFO] - Training Epoch: 8/10, step 263/574 completed (loss: 0.14175420999526978, acc: 0.9466666579246521)
[2025-01-06 01:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51][root][INFO] - Training Epoch: 8/10, step 264/574 completed (loss: 0.08009471744298935, acc: 0.9791666865348816)
[2025-01-06 01:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:52][root][INFO] - Training Epoch: 8/10, step 265/574 completed (loss: 0.4268088936805725, acc: 0.8320000171661377)
[2025-01-06 01:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:52][root][INFO] - Training Epoch: 8/10, step 266/574 completed (loss: 0.4793510138988495, acc: 0.898876428604126)
[2025-01-06 01:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53][root][INFO] - Training Epoch: 8/10, step 267/574 completed (loss: 0.12340471148490906, acc: 0.9594594836235046)
[2025-01-06 01:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53][root][INFO] - Training Epoch: 8/10, step 268/574 completed (loss: 0.1592126488685608, acc: 0.9137930870056152)
[2025-01-06 01:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53][root][INFO] - Training Epoch: 8/10, step 269/574 completed (loss: 0.0036744363605976105, acc: 1.0)
[2025-01-06 01:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54][root][INFO] - Training Epoch: 8/10, step 270/574 completed (loss: 0.023534482344985008, acc: 1.0)
[2025-01-06 01:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54][root][INFO] - Training Epoch: 8/10, step 271/574 completed (loss: 0.0037943816278129816, acc: 1.0)
[2025-01-06 01:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:23][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3515, device='cuda:0') eval_epoch_loss=tensor(0.8551, device='cuda:0') eval_epoch_acc=tensor(0.8251, device='cuda:0')
[2025-01-06 01:43:23][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:43:23][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:43:23][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_272_loss_0.8550664782524109/model.pt
[2025-01-06 01:43:23][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:23][root][INFO] - Training Epoch: 8/10, step 272/574 completed (loss: 0.0006515998393297195, acc: 1.0)
[2025-01-06 01:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:24][root][INFO] - Training Epoch: 8/10, step 273/574 completed (loss: 0.09177563339471817, acc: 0.9666666388511658)
[2025-01-06 01:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:24][root][INFO] - Training Epoch: 8/10, step 274/574 completed (loss: 0.027822226285934448, acc: 1.0)
[2025-01-06 01:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:25][root][INFO] - Training Epoch: 8/10, step 275/574 completed (loss: 0.06854072213172913, acc: 0.9666666388511658)
[2025-01-06 01:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:25][root][INFO] - Training Epoch: 8/10, step 276/574 completed (loss: 0.07102590799331665, acc: 0.9655172228813171)
[2025-01-06 01:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:25][root][INFO] - Training Epoch: 8/10, step 277/574 completed (loss: 0.015490638092160225, acc: 1.0)
[2025-01-06 01:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26][root][INFO] - Training Epoch: 8/10, step 278/574 completed (loss: 0.01346651278436184, acc: 1.0)
[2025-01-06 01:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26][root][INFO] - Training Epoch: 8/10, step 279/574 completed (loss: 0.12021932750940323, acc: 0.9583333134651184)
[2025-01-06 01:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26][root][INFO] - Training Epoch: 8/10, step 280/574 completed (loss: 0.04517355188727379, acc: 0.9772727489471436)
[2025-01-06 01:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:27][root][INFO] - Training Epoch: 8/10, step 281/574 completed (loss: 0.23913443088531494, acc: 0.9156626462936401)
[2025-01-06 01:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:27][root][INFO] - Training Epoch: 8/10, step 282/574 completed (loss: 0.41727617383003235, acc: 0.9259259104728699)
[2025-01-06 01:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:27][root][INFO] - Training Epoch: 8/10, step 283/574 completed (loss: 0.0020147126633673906, acc: 1.0)
[2025-01-06 01:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:28][root][INFO] - Training Epoch: 8/10, step 284/574 completed (loss: 0.03145173564553261, acc: 1.0)
[2025-01-06 01:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:28][root][INFO] - Training Epoch: 8/10, step 285/574 completed (loss: 0.044633232057094574, acc: 0.9750000238418579)
[2025-01-06 01:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:28][root][INFO] - Training Epoch: 8/10, step 286/574 completed (loss: 0.09787125140428543, acc: 0.96875)
[2025-01-06 01:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29][root][INFO] - Training Epoch: 8/10, step 287/574 completed (loss: 0.09705469012260437, acc: 0.9599999785423279)
[2025-01-06 01:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29][root][INFO] - Training Epoch: 8/10, step 288/574 completed (loss: 0.042497240006923676, acc: 0.9890109896659851)
[2025-01-06 01:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29][root][INFO] - Training Epoch: 8/10, step 289/574 completed (loss: 0.05851555988192558, acc: 0.9689440727233887)
[2025-01-06 01:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30][root][INFO] - Training Epoch: 8/10, step 290/574 completed (loss: 0.179770365357399, acc: 0.9587628841400146)
[2025-01-06 01:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30][root][INFO] - Training Epoch: 8/10, step 291/574 completed (loss: 0.001776452292688191, acc: 1.0)
[2025-01-06 01:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30][root][INFO] - Training Epoch: 8/10, step 292/574 completed (loss: 0.012304826639592648, acc: 1.0)
[2025-01-06 01:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31][root][INFO] - Training Epoch: 8/10, step 293/574 completed (loss: 0.05550353229045868, acc: 0.982758641242981)
[2025-01-06 01:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31][root][INFO] - Training Epoch: 8/10, step 294/574 completed (loss: 0.09512338787317276, acc: 0.9636363387107849)
[2025-01-06 01:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32][root][INFO] - Training Epoch: 8/10, step 295/574 completed (loss: 0.20896457135677338, acc: 0.9432989954948425)
[2025-01-06 01:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32][root][INFO] - Training Epoch: 8/10, step 296/574 completed (loss: 0.02978205680847168, acc: 1.0)
[2025-01-06 01:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32][root][INFO] - Training Epoch: 8/10, step 297/574 completed (loss: 0.00850393995642662, acc: 1.0)
[2025-01-06 01:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33][root][INFO] - Training Epoch: 8/10, step 298/574 completed (loss: 0.010642990469932556, acc: 1.0)
[2025-01-06 01:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33][root][INFO] - Training Epoch: 8/10, step 299/574 completed (loss: 0.006509265396744013, acc: 1.0)
[2025-01-06 01:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34][root][INFO] - Training Epoch: 8/10, step 300/574 completed (loss: 0.008511669933795929, acc: 1.0)
[2025-01-06 01:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34][root][INFO] - Training Epoch: 8/10, step 301/574 completed (loss: 0.03154073655605316, acc: 1.0)
[2025-01-06 01:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34][root][INFO] - Training Epoch: 8/10, step 302/574 completed (loss: 0.019794290885329247, acc: 1.0)
[2025-01-06 01:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:35][root][INFO] - Training Epoch: 8/10, step 303/574 completed (loss: 0.035710737109184265, acc: 0.970588207244873)
[2025-01-06 01:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:35][root][INFO] - Training Epoch: 8/10, step 304/574 completed (loss: 0.04018844664096832, acc: 0.96875)
[2025-01-06 01:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:35][root][INFO] - Training Epoch: 8/10, step 305/574 completed (loss: 0.08784795552492142, acc: 0.9672130942344666)
[2025-01-06 01:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36][root][INFO] - Training Epoch: 8/10, step 306/574 completed (loss: 0.0020657412242144346, acc: 1.0)
[2025-01-06 01:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36][root][INFO] - Training Epoch: 8/10, step 307/574 completed (loss: 0.0005579013377428055, acc: 1.0)
[2025-01-06 01:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36][root][INFO] - Training Epoch: 8/10, step 308/574 completed (loss: 0.12216682732105255, acc: 0.9710144996643066)
[2025-01-06 01:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37][root][INFO] - Training Epoch: 8/10, step 309/574 completed (loss: 0.04133713245391846, acc: 0.9722222089767456)
[2025-01-06 01:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37][root][INFO] - Training Epoch: 8/10, step 310/574 completed (loss: 0.011366677470505238, acc: 1.0)
[2025-01-06 01:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37][root][INFO] - Training Epoch: 8/10, step 311/574 completed (loss: 0.035618364810943604, acc: 0.9871794581413269)
[2025-01-06 01:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38][root][INFO] - Training Epoch: 8/10, step 312/574 completed (loss: 0.004934428259730339, acc: 1.0)
[2025-01-06 01:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38][root][INFO] - Training Epoch: 8/10, step 313/574 completed (loss: 0.0004294599639251828, acc: 1.0)
[2025-01-06 01:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38][root][INFO] - Training Epoch: 8/10, step 314/574 completed (loss: 0.0015623763902112842, acc: 1.0)
[2025-01-06 01:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39][root][INFO] - Training Epoch: 8/10, step 315/574 completed (loss: 0.05105762183666229, acc: 0.9677419066429138)
[2025-01-06 01:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39][root][INFO] - Training Epoch: 8/10, step 316/574 completed (loss: 0.19400601089000702, acc: 0.9677419066429138)
[2025-01-06 01:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39][root][INFO] - Training Epoch: 8/10, step 317/574 completed (loss: 0.014615724794566631, acc: 1.0)
[2025-01-06 01:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40][root][INFO] - Training Epoch: 8/10, step 318/574 completed (loss: 0.024163778871297836, acc: 0.9903846383094788)
[2025-01-06 01:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40][root][INFO] - Training Epoch: 8/10, step 319/574 completed (loss: 0.06277136504650116, acc: 0.9555555582046509)
[2025-01-06 01:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40][root][INFO] - Training Epoch: 8/10, step 320/574 completed (loss: 0.008830498903989792, acc: 1.0)
[2025-01-06 01:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41][root][INFO] - Training Epoch: 8/10, step 321/574 completed (loss: 0.009697643108665943, acc: 1.0)
[2025-01-06 01:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41][root][INFO] - Training Epoch: 8/10, step 322/574 completed (loss: 0.0339345820248127, acc: 1.0)
[2025-01-06 01:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41][root][INFO] - Training Epoch: 8/10, step 323/574 completed (loss: 0.09869298338890076, acc: 0.9142857193946838)
[2025-01-06 01:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41][root][INFO] - Training Epoch: 8/10, step 324/574 completed (loss: 0.12093450874090195, acc: 1.0)
[2025-01-06 01:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42][root][INFO] - Training Epoch: 8/10, step 325/574 completed (loss: 0.23275557160377502, acc: 0.9024389982223511)
[2025-01-06 01:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42][root][INFO] - Training Epoch: 8/10, step 326/574 completed (loss: 0.18818853795528412, acc: 0.9473684430122375)
[2025-01-06 01:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42][root][INFO] - Training Epoch: 8/10, step 327/574 completed (loss: 0.046854302287101746, acc: 1.0)
[2025-01-06 01:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43][root][INFO] - Training Epoch: 8/10, step 328/574 completed (loss: 0.0005373828462325037, acc: 1.0)
[2025-01-06 01:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43][root][INFO] - Training Epoch: 8/10, step 329/574 completed (loss: 0.023003557696938515, acc: 1.0)
[2025-01-06 01:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43][root][INFO] - Training Epoch: 8/10, step 330/574 completed (loss: 0.09334233403205872, acc: 0.96875)
[2025-01-06 01:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44][root][INFO] - Training Epoch: 8/10, step 331/574 completed (loss: 0.022227060049772263, acc: 1.0)
[2025-01-06 01:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44][root][INFO] - Training Epoch: 8/10, step 332/574 completed (loss: 0.04546278342604637, acc: 0.9824561476707458)
[2025-01-06 01:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44][root][INFO] - Training Epoch: 8/10, step 333/574 completed (loss: 0.017215630039572716, acc: 1.0)
[2025-01-06 01:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45][root][INFO] - Training Epoch: 8/10, step 334/574 completed (loss: 0.0011172355152666569, acc: 1.0)
[2025-01-06 01:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45][root][INFO] - Training Epoch: 8/10, step 335/574 completed (loss: 0.0006876468542031944, acc: 1.0)
[2025-01-06 01:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45][root][INFO] - Training Epoch: 8/10, step 336/574 completed (loss: 0.05351201072335243, acc: 0.9800000190734863)
[2025-01-06 01:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46][root][INFO] - Training Epoch: 8/10, step 337/574 completed (loss: 0.17982877790927887, acc: 0.931034505367279)
[2025-01-06 01:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46][root][INFO] - Training Epoch: 8/10, step 338/574 completed (loss: 0.22087684273719788, acc: 0.8829787373542786)
[2025-01-06 01:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:47][root][INFO] - Training Epoch: 8/10, step 339/574 completed (loss: 0.2607513666152954, acc: 0.9277108311653137)
[2025-01-06 01:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:47][root][INFO] - Training Epoch: 8/10, step 340/574 completed (loss: 0.0009600886260159314, acc: 1.0)
[2025-01-06 01:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:47][root][INFO] - Training Epoch: 8/10, step 341/574 completed (loss: 0.022765086963772774, acc: 1.0)
[2025-01-06 01:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48][root][INFO] - Training Epoch: 8/10, step 342/574 completed (loss: 0.29022088646888733, acc: 0.9277108311653137)
[2025-01-06 01:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48][root][INFO] - Training Epoch: 8/10, step 343/574 completed (loss: 0.046528469771146774, acc: 0.9811320900917053)
[2025-01-06 01:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48][root][INFO] - Training Epoch: 8/10, step 344/574 completed (loss: 0.0056954496540129185, acc: 1.0)
[2025-01-06 01:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49][root][INFO] - Training Epoch: 8/10, step 345/574 completed (loss: 0.002555759157985449, acc: 1.0)
[2025-01-06 01:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49][root][INFO] - Training Epoch: 8/10, step 346/574 completed (loss: 0.04482788220047951, acc: 0.9850746393203735)
[2025-01-06 01:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49][root][INFO] - Training Epoch: 8/10, step 347/574 completed (loss: 0.38232332468032837, acc: 0.949999988079071)
[2025-01-06 01:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50][root][INFO] - Training Epoch: 8/10, step 348/574 completed (loss: 0.003446233458817005, acc: 1.0)
[2025-01-06 01:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50][root][INFO] - Training Epoch: 8/10, step 349/574 completed (loss: 0.17171329259872437, acc: 0.9166666865348816)
[2025-01-06 01:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50][root][INFO] - Training Epoch: 8/10, step 350/574 completed (loss: 0.16989606618881226, acc: 0.9767441749572754)
[2025-01-06 01:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51][root][INFO] - Training Epoch: 8/10, step 351/574 completed (loss: 0.0031643761321902275, acc: 1.0)
[2025-01-06 01:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51][root][INFO] - Training Epoch: 8/10, step 352/574 completed (loss: 0.0652906522154808, acc: 0.9555555582046509)
[2025-01-06 01:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51][root][INFO] - Training Epoch: 8/10, step 353/574 completed (loss: 0.0037087840028107166, acc: 1.0)
[2025-01-06 01:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52][root][INFO] - Training Epoch: 8/10, step 354/574 completed (loss: 0.017571208998560905, acc: 1.0)
[2025-01-06 01:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52][root][INFO] - Training Epoch: 8/10, step 355/574 completed (loss: 0.12127593159675598, acc: 0.9670329689979553)
[2025-01-06 01:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53][root][INFO] - Training Epoch: 8/10, step 356/574 completed (loss: 0.10596702992916107, acc: 0.95652174949646)
[2025-01-06 01:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53][root][INFO] - Training Epoch: 8/10, step 357/574 completed (loss: 0.08839039504528046, acc: 0.95652174949646)
[2025-01-06 01:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53][root][INFO] - Training Epoch: 8/10, step 358/574 completed (loss: 0.017104635015130043, acc: 1.0)
[2025-01-06 01:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54][root][INFO] - Training Epoch: 8/10, step 359/574 completed (loss: 0.0009854643139988184, acc: 1.0)
[2025-01-06 01:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54][root][INFO] - Training Epoch: 8/10, step 360/574 completed (loss: 0.006582377012819052, acc: 1.0)
[2025-01-06 01:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54][root][INFO] - Training Epoch: 8/10, step 361/574 completed (loss: 0.07277669757604599, acc: 0.9756097793579102)
[2025-01-06 01:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55][root][INFO] - Training Epoch: 8/10, step 362/574 completed (loss: 0.14718832075595856, acc: 0.9777777791023254)
[2025-01-06 01:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55][root][INFO] - Training Epoch: 8/10, step 363/574 completed (loss: 0.015062997117638588, acc: 1.0)
[2025-01-06 01:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55][root][INFO] - Training Epoch: 8/10, step 364/574 completed (loss: 0.2338636964559555, acc: 0.9268292784690857)
[2025-01-06 01:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:56][root][INFO] - Training Epoch: 8/10, step 365/574 completed (loss: 0.027227139100432396, acc: 1.0)
[2025-01-06 01:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:56][root][INFO] - Training Epoch: 8/10, step 366/574 completed (loss: 0.018694687634706497, acc: 1.0)
[2025-01-06 01:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:56][root][INFO] - Training Epoch: 8/10, step 367/574 completed (loss: 0.0016755079850554466, acc: 1.0)
[2025-01-06 01:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57][root][INFO] - Training Epoch: 8/10, step 368/574 completed (loss: 0.012827148661017418, acc: 1.0)
[2025-01-06 01:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57][root][INFO] - Training Epoch: 8/10, step 369/574 completed (loss: 0.004325381480157375, acc: 1.0)
[2025-01-06 01:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:58][root][INFO] - Training Epoch: 8/10, step 370/574 completed (loss: 0.26740702986717224, acc: 0.9090909361839294)
[2025-01-06 01:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:59][root][INFO] - Training Epoch: 8/10, step 371/574 completed (loss: 0.06643573939800262, acc: 0.9716981053352356)
[2025-01-06 01:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:59][root][INFO] - Training Epoch: 8/10, step 372/574 completed (loss: 0.026720557361841202, acc: 0.9888888597488403)
[2025-01-06 01:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:59][root][INFO] - Training Epoch: 8/10, step 373/574 completed (loss: 0.07776711136102676, acc: 0.9821428656578064)
[2025-01-06 01:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00][root][INFO] - Training Epoch: 8/10, step 374/574 completed (loss: 0.02193385176360607, acc: 0.9714285731315613)
[2025-01-06 01:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00][root][INFO] - Training Epoch: 8/10, step 375/574 completed (loss: 0.011318780481815338, acc: 1.0)
[2025-01-06 01:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00][root][INFO] - Training Epoch: 8/10, step 376/574 completed (loss: 0.0005035865469835699, acc: 1.0)
[2025-01-06 01:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01][root][INFO] - Training Epoch: 8/10, step 377/574 completed (loss: 0.014661087654531002, acc: 1.0)
[2025-01-06 01:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01][root][INFO] - Training Epoch: 8/10, step 378/574 completed (loss: 0.024779262021183968, acc: 0.9894737005233765)
[2025-01-06 01:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:02][root][INFO] - Training Epoch: 8/10, step 379/574 completed (loss: 0.09616132080554962, acc: 0.976047933101654)
[2025-01-06 01:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:02][root][INFO] - Training Epoch: 8/10, step 380/574 completed (loss: 0.12678015232086182, acc: 0.9624060392379761)
[2025-01-06 01:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:03][root][INFO] - Training Epoch: 8/10, step 381/574 completed (loss: 0.17245809733867645, acc: 0.9465240836143494)
[2025-01-06 01:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:04][root][INFO] - Training Epoch: 8/10, step 382/574 completed (loss: 0.02205885760486126, acc: 0.9909909963607788)
[2025-01-06 01:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:04][root][INFO] - Training Epoch: 8/10, step 383/574 completed (loss: 0.010326902382075787, acc: 1.0)
[2025-01-06 01:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05][root][INFO] - Training Epoch: 8/10, step 384/574 completed (loss: 0.0024580468889325857, acc: 1.0)
[2025-01-06 01:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05][root][INFO] - Training Epoch: 8/10, step 385/574 completed (loss: 0.01523963175714016, acc: 1.0)
[2025-01-06 01:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05][root][INFO] - Training Epoch: 8/10, step 386/574 completed (loss: 0.00043740789988078177, acc: 1.0)
[2025-01-06 01:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06][root][INFO] - Training Epoch: 8/10, step 387/574 completed (loss: 0.014054905623197556, acc: 1.0)
[2025-01-06 01:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06][root][INFO] - Training Epoch: 8/10, step 388/574 completed (loss: 0.0004145036218687892, acc: 1.0)
[2025-01-06 01:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06][root][INFO] - Training Epoch: 8/10, step 389/574 completed (loss: 0.002900910098105669, acc: 1.0)
[2025-01-06 01:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07][root][INFO] - Training Epoch: 8/10, step 390/574 completed (loss: 0.06288546323776245, acc: 0.9523809552192688)
[2025-01-06 01:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07][root][INFO] - Training Epoch: 8/10, step 391/574 completed (loss: 0.0701407864689827, acc: 1.0)
[2025-01-06 01:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07][root][INFO] - Training Epoch: 8/10, step 392/574 completed (loss: 0.2431638240814209, acc: 0.9223300814628601)
[2025-01-06 01:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08][root][INFO] - Training Epoch: 8/10, step 393/574 completed (loss: 0.25913214683532715, acc: 0.8823529481887817)
[2025-01-06 01:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08][root][INFO] - Training Epoch: 8/10, step 394/574 completed (loss: 0.24515947699546814, acc: 0.9200000166893005)
[2025-01-06 01:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09][root][INFO] - Training Epoch: 8/10, step 395/574 completed (loss: 0.1423952579498291, acc: 0.9513888955116272)
[2025-01-06 01:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09][root][INFO] - Training Epoch: 8/10, step 396/574 completed (loss: 0.04373763129115105, acc: 0.9767441749572754)
[2025-01-06 01:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09][root][INFO] - Training Epoch: 8/10, step 397/574 completed (loss: 0.01368799526244402, acc: 1.0)
[2025-01-06 01:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10][root][INFO] - Training Epoch: 8/10, step 398/574 completed (loss: 0.018073678016662598, acc: 1.0)
[2025-01-06 01:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10][root][INFO] - Training Epoch: 8/10, step 399/574 completed (loss: 0.0008414802723564208, acc: 1.0)
[2025-01-06 01:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10][root][INFO] - Training Epoch: 8/10, step 400/574 completed (loss: 0.024734975770115852, acc: 1.0)
[2025-01-06 01:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:11][root][INFO] - Training Epoch: 8/10, step 401/574 completed (loss: 0.0583343431353569, acc: 0.9866666793823242)
[2025-01-06 01:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:11][root][INFO] - Training Epoch: 8/10, step 402/574 completed (loss: 0.004743139259517193, acc: 1.0)
[2025-01-06 01:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12][root][INFO] - Training Epoch: 8/10, step 403/574 completed (loss: 0.21513113379478455, acc: 0.9696969985961914)
[2025-01-06 01:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12][root][INFO] - Training Epoch: 8/10, step 404/574 completed (loss: 0.023579007014632225, acc: 1.0)
[2025-01-06 01:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12][root][INFO] - Training Epoch: 8/10, step 405/574 completed (loss: 0.0005718565080314875, acc: 1.0)
[2025-01-06 01:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13][root][INFO] - Training Epoch: 8/10, step 406/574 completed (loss: 0.00037468699156306684, acc: 1.0)
[2025-01-06 01:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13][root][INFO] - Training Epoch: 8/10, step 407/574 completed (loss: 0.0007783980690874159, acc: 1.0)
[2025-01-06 01:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13][root][INFO] - Training Epoch: 8/10, step 408/574 completed (loss: 0.004143433645367622, acc: 1.0)
[2025-01-06 01:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14][root][INFO] - Training Epoch: 8/10, step 409/574 completed (loss: 0.0013068022672086954, acc: 1.0)
[2025-01-06 01:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14][root][INFO] - Training Epoch: 8/10, step 410/574 completed (loss: 0.005224870517849922, acc: 1.0)
[2025-01-06 01:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14][root][INFO] - Training Epoch: 8/10, step 411/574 completed (loss: 0.00621033413335681, acc: 1.0)
[2025-01-06 01:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15][root][INFO] - Training Epoch: 8/10, step 412/574 completed (loss: 0.00062104023527354, acc: 1.0)
[2025-01-06 01:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15][root][INFO] - Training Epoch: 8/10, step 413/574 completed (loss: 0.016679586842656136, acc: 1.0)
[2025-01-06 01:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15][root][INFO] - Training Epoch: 8/10, step 414/574 completed (loss: 0.0019784022588282824, acc: 1.0)
[2025-01-06 01:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4004, device='cuda:0') eval_epoch_loss=tensor(0.8756, device='cuda:0') eval_epoch_acc=tensor(0.8333, device='cuda:0')
[2025-01-06 01:44:45][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:44:45][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:44:46][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_415_loss_0.875615119934082/model.pt
[2025-01-06 01:44:46][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46][root][INFO] - Training Epoch: 8/10, step 415/574 completed (loss: 0.05253737419843674, acc: 0.9803921580314636)
[2025-01-06 01:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46][root][INFO] - Training Epoch: 8/10, step 416/574 completed (loss: 0.01548805832862854, acc: 1.0)
[2025-01-06 01:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47][root][INFO] - Training Epoch: 8/10, step 417/574 completed (loss: 0.010564562864601612, acc: 1.0)
[2025-01-06 01:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47][root][INFO] - Training Epoch: 8/10, step 418/574 completed (loss: 0.003922495059669018, acc: 1.0)
[2025-01-06 01:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48][root][INFO] - Training Epoch: 8/10, step 419/574 completed (loss: 0.004616984631866217, acc: 1.0)
[2025-01-06 01:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48][root][INFO] - Training Epoch: 8/10, step 420/574 completed (loss: 0.013904799707233906, acc: 1.0)
[2025-01-06 01:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48][root][INFO] - Training Epoch: 8/10, step 421/574 completed (loss: 0.0031020641326904297, acc: 1.0)
[2025-01-06 01:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49][root][INFO] - Training Epoch: 8/10, step 422/574 completed (loss: 0.10857438296079636, acc: 0.96875)
[2025-01-06 01:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49][root][INFO] - Training Epoch: 8/10, step 423/574 completed (loss: 0.028919700533151627, acc: 1.0)
[2025-01-06 01:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49][root][INFO] - Training Epoch: 8/10, step 424/574 completed (loss: 0.005413890816271305, acc: 1.0)
[2025-01-06 01:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50][root][INFO] - Training Epoch: 8/10, step 425/574 completed (loss: 0.007981711998581886, acc: 1.0)
[2025-01-06 01:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50][root][INFO] - Training Epoch: 8/10, step 426/574 completed (loss: 0.0015813293866813183, acc: 1.0)
[2025-01-06 01:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50][root][INFO] - Training Epoch: 8/10, step 427/574 completed (loss: 0.12966422736644745, acc: 0.9729729890823364)
[2025-01-06 01:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51][root][INFO] - Training Epoch: 8/10, step 428/574 completed (loss: 0.009132716804742813, acc: 1.0)
[2025-01-06 01:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51][root][INFO] - Training Epoch: 8/10, step 429/574 completed (loss: 0.0019295752281323075, acc: 1.0)
[2025-01-06 01:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51][root][INFO] - Training Epoch: 8/10, step 430/574 completed (loss: 0.0001374860294163227, acc: 1.0)
[2025-01-06 01:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52][root][INFO] - Training Epoch: 8/10, step 431/574 completed (loss: 0.0003038463764823973, acc: 1.0)
[2025-01-06 01:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52][root][INFO] - Training Epoch: 8/10, step 432/574 completed (loss: 0.0008451014873571694, acc: 1.0)
[2025-01-06 01:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52][root][INFO] - Training Epoch: 8/10, step 433/574 completed (loss: 0.017629636451601982, acc: 1.0)
[2025-01-06 01:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53][root][INFO] - Training Epoch: 8/10, step 434/574 completed (loss: 0.000318007922032848, acc: 1.0)
[2025-01-06 01:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53][root][INFO] - Training Epoch: 8/10, step 435/574 completed (loss: 0.004199683200567961, acc: 1.0)
[2025-01-06 01:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53][root][INFO] - Training Epoch: 8/10, step 436/574 completed (loss: 0.007366574369370937, acc: 1.0)
[2025-01-06 01:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54][root][INFO] - Training Epoch: 8/10, step 437/574 completed (loss: 0.001219998230226338, acc: 1.0)
[2025-01-06 01:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54][root][INFO] - Training Epoch: 8/10, step 438/574 completed (loss: 0.0003927531943190843, acc: 1.0)
[2025-01-06 01:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54][root][INFO] - Training Epoch: 8/10, step 439/574 completed (loss: 0.008834217675030231, acc: 1.0)
[2025-01-06 01:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:55][root][INFO] - Training Epoch: 8/10, step 440/574 completed (loss: 0.12383202463388443, acc: 0.939393937587738)
[2025-01-06 01:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56][root][INFO] - Training Epoch: 8/10, step 441/574 completed (loss: 0.1869276762008667, acc: 0.9279999732971191)
[2025-01-06 01:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56][root][INFO] - Training Epoch: 8/10, step 442/574 completed (loss: 0.18176525831222534, acc: 0.9354838728904724)
[2025-01-06 01:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57][root][INFO] - Training Epoch: 8/10, step 443/574 completed (loss: 0.21001669764518738, acc: 0.9452736377716064)
[2025-01-06 01:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57][root][INFO] - Training Epoch: 8/10, step 444/574 completed (loss: 0.012197710573673248, acc: 1.0)
[2025-01-06 01:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57][root][INFO] - Training Epoch: 8/10, step 445/574 completed (loss: 0.010095751844346523, acc: 1.0)
[2025-01-06 01:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58][root][INFO] - Training Epoch: 8/10, step 446/574 completed (loss: 0.0009599950863048434, acc: 1.0)
[2025-01-06 01:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58][root][INFO] - Training Epoch: 8/10, step 447/574 completed (loss: 0.0019823883194476366, acc: 1.0)
[2025-01-06 01:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58][root][INFO] - Training Epoch: 8/10, step 448/574 completed (loss: 0.0024998653680086136, acc: 1.0)
[2025-01-06 01:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59][root][INFO] - Training Epoch: 8/10, step 449/574 completed (loss: 0.043217483907938004, acc: 0.9850746393203735)
[2025-01-06 01:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59][root][INFO] - Training Epoch: 8/10, step 450/574 completed (loss: 0.013749532401561737, acc: 1.0)
[2025-01-06 01:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59][root][INFO] - Training Epoch: 8/10, step 451/574 completed (loss: 0.004992376081645489, acc: 1.0)
[2025-01-06 01:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00][root][INFO] - Training Epoch: 8/10, step 452/574 completed (loss: 0.009993808344006538, acc: 1.0)
[2025-01-06 01:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00][root][INFO] - Training Epoch: 8/10, step 453/574 completed (loss: 0.014624533243477345, acc: 1.0)
[2025-01-06 01:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00][root][INFO] - Training Epoch: 8/10, step 454/574 completed (loss: 0.020113827660679817, acc: 0.9795918464660645)
[2025-01-06 01:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01][root][INFO] - Training Epoch: 8/10, step 455/574 completed (loss: 0.013709623366594315, acc: 1.0)
[2025-01-06 01:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01][root][INFO] - Training Epoch: 8/10, step 456/574 completed (loss: 0.1374206393957138, acc: 0.969072163105011)
[2025-01-06 01:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01][root][INFO] - Training Epoch: 8/10, step 457/574 completed (loss: 0.002549816621467471, acc: 1.0)
[2025-01-06 01:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02][root][INFO] - Training Epoch: 8/10, step 458/574 completed (loss: 0.06214716657996178, acc: 0.9883720874786377)
[2025-01-06 01:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02][root][INFO] - Training Epoch: 8/10, step 459/574 completed (loss: 0.027819517999887466, acc: 0.9821428656578064)
[2025-01-06 01:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02][root][INFO] - Training Epoch: 8/10, step 460/574 completed (loss: 0.019974209368228912, acc: 1.0)
[2025-01-06 01:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03][root][INFO] - Training Epoch: 8/10, step 461/574 completed (loss: 0.02112283557653427, acc: 1.0)
[2025-01-06 01:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03][root][INFO] - Training Epoch: 8/10, step 462/574 completed (loss: 0.0010306001640856266, acc: 1.0)
[2025-01-06 01:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03][root][INFO] - Training Epoch: 8/10, step 463/574 completed (loss: 0.0010599142406135798, acc: 1.0)
[2025-01-06 01:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04][root][INFO] - Training Epoch: 8/10, step 464/574 completed (loss: 0.002950676716864109, acc: 1.0)
[2025-01-06 01:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04][root][INFO] - Training Epoch: 8/10, step 465/574 completed (loss: 0.01738390140235424, acc: 0.988095223903656)
[2025-01-06 01:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04][root][INFO] - Training Epoch: 8/10, step 466/574 completed (loss: 0.13364413380622864, acc: 0.9518072009086609)
[2025-01-06 01:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05][root][INFO] - Training Epoch: 8/10, step 467/574 completed (loss: 0.016371294856071472, acc: 1.0)
[2025-01-06 01:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05][root][INFO] - Training Epoch: 8/10, step 468/574 completed (loss: 0.13412582874298096, acc: 0.9611650705337524)
[2025-01-06 01:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05][root][INFO] - Training Epoch: 8/10, step 469/574 completed (loss: 0.04342063143849373, acc: 0.9918699264526367)
[2025-01-06 01:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06][root][INFO] - Training Epoch: 8/10, step 470/574 completed (loss: 0.008276172913610935, acc: 1.0)
[2025-01-06 01:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06][root][INFO] - Training Epoch: 8/10, step 471/574 completed (loss: 0.17591747641563416, acc: 0.9285714030265808)
[2025-01-06 01:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06][root][INFO] - Training Epoch: 8/10, step 472/574 completed (loss: 0.14273284375667572, acc: 0.9509803652763367)
[2025-01-06 01:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07][root][INFO] - Training Epoch: 8/10, step 473/574 completed (loss: 0.21040169894695282, acc: 0.9388646483421326)
[2025-01-06 01:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07][root][INFO] - Training Epoch: 8/10, step 474/574 completed (loss: 0.06255025416612625, acc: 0.9791666865348816)
[2025-01-06 01:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07][root][INFO] - Training Epoch: 8/10, step 475/574 completed (loss: 0.05578870698809624, acc: 0.9815950989723206)
[2025-01-06 01:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08][root][INFO] - Training Epoch: 8/10, step 476/574 completed (loss: 0.07703353464603424, acc: 0.971222996711731)
[2025-01-06 01:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08][root][INFO] - Training Epoch: 8/10, step 477/574 completed (loss: 0.16522136330604553, acc: 0.9396985173225403)
[2025-01-06 01:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08][root][INFO] - Training Epoch: 8/10, step 478/574 completed (loss: 0.11545904725790024, acc: 0.9722222089767456)
[2025-01-06 01:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:09][root][INFO] - Training Epoch: 8/10, step 479/574 completed (loss: 0.024220174178481102, acc: 1.0)
[2025-01-06 01:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:09][root][INFO] - Training Epoch: 8/10, step 480/574 completed (loss: 0.2488476037979126, acc: 0.9629629850387573)
[2025-01-06 01:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:09][root][INFO] - Training Epoch: 8/10, step 481/574 completed (loss: 0.10975410789251328, acc: 0.949999988079071)
[2025-01-06 01:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:10][root][INFO] - Training Epoch: 8/10, step 482/574 completed (loss: 0.08501529693603516, acc: 1.0)
[2025-01-06 01:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:10][root][INFO] - Training Epoch: 8/10, step 483/574 completed (loss: 0.3634440302848816, acc: 0.9482758641242981)
[2025-01-06 01:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11][root][INFO] - Training Epoch: 8/10, step 484/574 completed (loss: 0.0023318594321608543, acc: 1.0)
[2025-01-06 01:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11][root][INFO] - Training Epoch: 8/10, step 485/574 completed (loss: 0.010384238325059414, acc: 1.0)
[2025-01-06 01:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11][root][INFO] - Training Epoch: 8/10, step 486/574 completed (loss: 0.0029422107618302107, acc: 1.0)
[2025-01-06 01:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12][root][INFO] - Training Epoch: 8/10, step 487/574 completed (loss: 0.06781740486621857, acc: 0.9523809552192688)
[2025-01-06 01:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12][root][INFO] - Training Epoch: 8/10, step 488/574 completed (loss: 0.04446737840771675, acc: 1.0)
[2025-01-06 01:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12][root][INFO] - Training Epoch: 8/10, step 489/574 completed (loss: 0.17782460153102875, acc: 0.9692307710647583)
[2025-01-06 01:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13][root][INFO] - Training Epoch: 8/10, step 490/574 completed (loss: 0.0041846176609396935, acc: 1.0)
[2025-01-06 01:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13][root][INFO] - Training Epoch: 8/10, step 491/574 completed (loss: 0.0087058050557971, acc: 1.0)
[2025-01-06 01:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13][root][INFO] - Training Epoch: 8/10, step 492/574 completed (loss: 0.07996616512537003, acc: 0.9607843160629272)
[2025-01-06 01:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14][root][INFO] - Training Epoch: 8/10, step 493/574 completed (loss: 0.026258382946252823, acc: 1.0)
[2025-01-06 01:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14][root][INFO] - Training Epoch: 8/10, step 494/574 completed (loss: 0.4085206091403961, acc: 0.9473684430122375)
[2025-01-06 01:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14][root][INFO] - Training Epoch: 8/10, step 495/574 completed (loss: 0.013150992803275585, acc: 1.0)
[2025-01-06 01:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15][root][INFO] - Training Epoch: 8/10, step 496/574 completed (loss: 0.22549569606781006, acc: 0.9285714030265808)
[2025-01-06 01:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15][root][INFO] - Training Epoch: 8/10, step 497/574 completed (loss: 0.12532104551792145, acc: 0.9438202381134033)
[2025-01-06 01:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15][root][INFO] - Training Epoch: 8/10, step 498/574 completed (loss: 0.1041581779718399, acc: 0.966292142868042)
[2025-01-06 01:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16][root][INFO] - Training Epoch: 8/10, step 499/574 completed (loss: 0.3128999173641205, acc: 0.9078013896942139)
[2025-01-06 01:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16][root][INFO] - Training Epoch: 8/10, step 500/574 completed (loss: 0.11902116984128952, acc: 0.967391312122345)
[2025-01-06 01:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16][root][INFO] - Training Epoch: 8/10, step 501/574 completed (loss: 0.001201135921292007, acc: 1.0)
[2025-01-06 01:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17][root][INFO] - Training Epoch: 8/10, step 502/574 completed (loss: 0.04206934943795204, acc: 0.9615384340286255)
[2025-01-06 01:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17][root][INFO] - Training Epoch: 8/10, step 503/574 completed (loss: 0.015259206295013428, acc: 1.0)
[2025-01-06 01:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17][root][INFO] - Training Epoch: 8/10, step 504/574 completed (loss: 0.002767583355307579, acc: 1.0)
[2025-01-06 01:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18][root][INFO] - Training Epoch: 8/10, step 505/574 completed (loss: 0.12816829979419708, acc: 0.9433962106704712)
[2025-01-06 01:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18][root][INFO] - Training Epoch: 8/10, step 506/574 completed (loss: 0.15372127294540405, acc: 0.9655172228813171)
[2025-01-06 01:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19][root][INFO] - Training Epoch: 8/10, step 507/574 completed (loss: 0.31892523169517517, acc: 0.9009009003639221)
[2025-01-06 01:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19][root][INFO] - Training Epoch: 8/10, step 508/574 completed (loss: 0.1436275690793991, acc: 0.9718309640884399)
[2025-01-06 01:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19][root][INFO] - Training Epoch: 8/10, step 509/574 completed (loss: 0.05543253570795059, acc: 0.949999988079071)
[2025-01-06 01:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20][root][INFO] - Training Epoch: 8/10, step 510/574 completed (loss: 0.5240922570228577, acc: 0.9333333373069763)
[2025-01-06 01:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20][root][INFO] - Training Epoch: 8/10, step 511/574 completed (loss: 0.17516930401325226, acc: 0.9230769276618958)
[2025-01-06 01:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:23][root][INFO] - Training Epoch: 8/10, step 512/574 completed (loss: 0.2799805998802185, acc: 0.9142857193946838)
[2025-01-06 01:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:23][root][INFO] - Training Epoch: 8/10, step 513/574 completed (loss: 0.030923688784241676, acc: 0.976190447807312)
[2025-01-06 01:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24][root][INFO] - Training Epoch: 8/10, step 514/574 completed (loss: 0.020678410306572914, acc: 1.0)
[2025-01-06 01:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24][root][INFO] - Training Epoch: 8/10, step 515/574 completed (loss: 0.020132310688495636, acc: 0.9833333492279053)
[2025-01-06 01:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25][root][INFO] - Training Epoch: 8/10, step 516/574 completed (loss: 0.07733353227376938, acc: 0.9722222089767456)
[2025-01-06 01:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25][root][INFO] - Training Epoch: 8/10, step 517/574 completed (loss: 0.008541987277567387, acc: 1.0)
[2025-01-06 01:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25][root][INFO] - Training Epoch: 8/10, step 518/574 completed (loss: 0.01050260104238987, acc: 1.0)
[2025-01-06 01:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26][root][INFO] - Training Epoch: 8/10, step 519/574 completed (loss: 0.028481388464570045, acc: 1.0)
[2025-01-06 01:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26][root][INFO] - Training Epoch: 8/10, step 520/574 completed (loss: 0.1226106509566307, acc: 0.9259259104728699)
[2025-01-06 01:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:27][root][INFO] - Training Epoch: 8/10, step 521/574 completed (loss: 0.29503133893013, acc: 0.8813559412956238)
[2025-01-06 01:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28][root][INFO] - Training Epoch: 8/10, step 522/574 completed (loss: 0.08530739694833755, acc: 0.9850746393203735)
[2025-01-06 01:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28][root][INFO] - Training Epoch: 8/10, step 523/574 completed (loss: 0.06849198788404465, acc: 0.9781022071838379)
[2025-01-06 01:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28][root][INFO] - Training Epoch: 8/10, step 524/574 completed (loss: 0.2560397982597351, acc: 0.9300000071525574)
[2025-01-06 01:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29][root][INFO] - Training Epoch: 8/10, step 525/574 completed (loss: 0.0060165049508214, acc: 1.0)
[2025-01-06 01:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29][root][INFO] - Training Epoch: 8/10, step 526/574 completed (loss: 0.05098313093185425, acc: 0.9807692170143127)
[2025-01-06 01:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29][root][INFO] - Training Epoch: 8/10, step 527/574 completed (loss: 0.004581585060805082, acc: 1.0)
[2025-01-06 01:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30][root][INFO] - Training Epoch: 8/10, step 528/574 completed (loss: 0.1627453863620758, acc: 0.9508196711540222)
[2025-01-06 01:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30][root][INFO] - Training Epoch: 8/10, step 529/574 completed (loss: 0.09836050868034363, acc: 0.9830508232116699)
[2025-01-06 01:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30][root][INFO] - Training Epoch: 8/10, step 530/574 completed (loss: 0.14237593114376068, acc: 0.9534883499145508)
[2025-01-06 01:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31][root][INFO] - Training Epoch: 8/10, step 531/574 completed (loss: 0.20280112326145172, acc: 0.9772727489471436)
[2025-01-06 01:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31][root][INFO] - Training Epoch: 8/10, step 532/574 completed (loss: 0.30319318175315857, acc: 0.9245283007621765)
[2025-01-06 01:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31][root][INFO] - Training Epoch: 8/10, step 533/574 completed (loss: 0.08980674296617508, acc: 0.9545454382896423)
[2025-01-06 01:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32][root][INFO] - Training Epoch: 8/10, step 534/574 completed (loss: 0.006846800912171602, acc: 1.0)
[2025-01-06 01:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32][root][INFO] - Training Epoch: 8/10, step 535/574 completed (loss: 0.07740315794944763, acc: 0.949999988079071)
[2025-01-06 01:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32][root][INFO] - Training Epoch: 8/10, step 536/574 completed (loss: 0.3508436977863312, acc: 0.9545454382896423)
[2025-01-06 01:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33][root][INFO] - Training Epoch: 8/10, step 537/574 completed (loss: 0.08808094263076782, acc: 0.9384615421295166)
[2025-01-06 01:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33][root][INFO] - Training Epoch: 8/10, step 538/574 completed (loss: 0.05409912392497063, acc: 0.984375)
[2025-01-06 01:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33][root][INFO] - Training Epoch: 8/10, step 539/574 completed (loss: 0.020626122131943703, acc: 1.0)
[2025-01-06 01:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34][root][INFO] - Training Epoch: 8/10, step 540/574 completed (loss: 0.04479951038956642, acc: 1.0)
[2025-01-06 01:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34][root][INFO] - Training Epoch: 8/10, step 541/574 completed (loss: 0.002290884265676141, acc: 1.0)
[2025-01-06 01:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34][root][INFO] - Training Epoch: 8/10, step 542/574 completed (loss: 0.0014477769145742059, acc: 1.0)
[2025-01-06 01:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35][root][INFO] - Training Epoch: 8/10, step 543/574 completed (loss: 0.002150175627321005, acc: 1.0)
[2025-01-06 01:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35][root][INFO] - Training Epoch: 8/10, step 544/574 completed (loss: 0.008452219888567924, acc: 1.0)
[2025-01-06 01:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35][root][INFO] - Training Epoch: 8/10, step 545/574 completed (loss: 0.03425387293100357, acc: 1.0)
[2025-01-06 01:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36][root][INFO] - Training Epoch: 8/10, step 546/574 completed (loss: 0.017639964818954468, acc: 1.0)
[2025-01-06 01:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36][root][INFO] - Training Epoch: 8/10, step 547/574 completed (loss: 0.0028531362768262625, acc: 1.0)
[2025-01-06 01:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36][root][INFO] - Training Epoch: 8/10, step 548/574 completed (loss: 0.003444151720032096, acc: 1.0)
[2025-01-06 01:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36][root][INFO] - Training Epoch: 8/10, step 549/574 completed (loss: 0.08821287006139755, acc: 0.9599999785423279)
[2025-01-06 01:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37][root][INFO] - Training Epoch: 8/10, step 550/574 completed (loss: 0.013781487941741943, acc: 1.0)
[2025-01-06 01:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37][root][INFO] - Training Epoch: 8/10, step 551/574 completed (loss: 0.03047911450266838, acc: 0.9750000238418579)
[2025-01-06 01:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37][root][INFO] - Training Epoch: 8/10, step 552/574 completed (loss: 0.023470452055335045, acc: 0.9857142567634583)
[2025-01-06 01:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38][root][INFO] - Training Epoch: 8/10, step 553/574 completed (loss: 0.03693694621324539, acc: 0.985401451587677)
[2025-01-06 01:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38][root][INFO] - Training Epoch: 8/10, step 554/574 completed (loss: 0.02932824194431305, acc: 0.9931034445762634)
[2025-01-06 01:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38][root][INFO] - Training Epoch: 8/10, step 555/574 completed (loss: 0.07431233674287796, acc: 0.9714285731315613)
[2025-01-06 01:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39][root][INFO] - Training Epoch: 8/10, step 556/574 completed (loss: 0.17117930948734283, acc: 0.9470198750495911)
[2025-01-06 01:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39][root][INFO] - Training Epoch: 8/10, step 557/574 completed (loss: 0.016205325722694397, acc: 0.9914529919624329)
[2025-01-06 01:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3869, device='cuda:0') eval_epoch_loss=tensor(0.8700, device='cuda:0') eval_epoch_acc=tensor(0.8252, device='cuda:0')
[2025-01-06 01:46:09][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:46:09][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:46:09][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_558_loss_0.8699989318847656/model.pt
[2025-01-06 01:46:09][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09][root][INFO] - Training Epoch: 8/10, step 558/574 completed (loss: 0.22078262269496918, acc: 0.9599999785423279)
[2025-01-06 01:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10][root][INFO] - Training Epoch: 8/10, step 559/574 completed (loss: 0.09316980838775635, acc: 0.9615384340286255)
[2025-01-06 01:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10][root][INFO] - Training Epoch: 8/10, step 560/574 completed (loss: 0.0015077551361173391, acc: 1.0)
[2025-01-06 01:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10][root][INFO] - Training Epoch: 8/10, step 561/574 completed (loss: 0.025434641167521477, acc: 1.0)
[2025-01-06 01:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11][root][INFO] - Training Epoch: 8/10, step 562/574 completed (loss: 0.09363455325365067, acc: 0.9666666388511658)
[2025-01-06 01:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11][root][INFO] - Training Epoch: 8/10, step 563/574 completed (loss: 0.02273234724998474, acc: 1.0)
[2025-01-06 01:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11][root][INFO] - Training Epoch: 8/10, step 564/574 completed (loss: 0.053603217005729675, acc: 0.9791666865348816)
[2025-01-06 01:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12][root][INFO] - Training Epoch: 8/10, step 565/574 completed (loss: 0.17835168540477753, acc: 0.9655172228813171)
[2025-01-06 01:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12][root][INFO] - Training Epoch: 8/10, step 566/574 completed (loss: 0.017618227750062943, acc: 1.0)
[2025-01-06 01:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12][root][INFO] - Training Epoch: 8/10, step 567/574 completed (loss: 0.0023814064916223288, acc: 1.0)
[2025-01-06 01:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12][root][INFO] - Training Epoch: 8/10, step 568/574 completed (loss: 0.014460033737123013, acc: 1.0)
[2025-01-06 01:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13][root][INFO] - Training Epoch: 8/10, step 569/574 completed (loss: 0.1058628037571907, acc: 0.9732620120048523)
[2025-01-06 01:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13][root][INFO] - Training Epoch: 8/10, step 570/574 completed (loss: 0.030436189845204353, acc: 0.9838709831237793)
[2025-01-06 01:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13][root][INFO] - Training Epoch: 8/10, step 571/574 completed (loss: 0.0064058150164783, acc: 1.0)
[2025-01-06 01:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14][root][INFO] - Training Epoch: 8/10, step 572/574 completed (loss: 0.11485200375318527, acc: 0.9642857313156128)
[2025-01-06 01:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14][root][INFO] - Training Epoch: 8/10, step 573/574 completed (loss: 0.12422963231801987, acc: 0.9496855139732361)
[2025-01-06 01:46:15][slam_llm.utils.train_utils][INFO] - Epoch 8: train_perplexity=1.1084, train_epoch_loss=0.1029, epoch time 338.0241457410157s
[2025-01-06 01:46:15][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:46:15][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:46:15][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:46:15][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 22
[2025-01-06 01:46:15][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:15][root][INFO] - Training Epoch: 9/10, step 0/574 completed (loss: 0.03730924427509308, acc: 1.0)
[2025-01-06 01:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16][root][INFO] - Training Epoch: 9/10, step 1/574 completed (loss: 0.0319896936416626, acc: 1.0)
[2025-01-06 01:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16][root][INFO] - Training Epoch: 9/10, step 2/574 completed (loss: 0.02973325550556183, acc: 1.0)
[2025-01-06 01:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16][root][INFO] - Training Epoch: 9/10, step 3/574 completed (loss: 0.0347835011780262, acc: 0.9736841917037964)
[2025-01-06 01:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17][root][INFO] - Training Epoch: 9/10, step 4/574 completed (loss: 0.032957158982753754, acc: 1.0)
[2025-01-06 01:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17][root][INFO] - Training Epoch: 9/10, step 5/574 completed (loss: 0.013520709238946438, acc: 1.0)
[2025-01-06 01:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17][root][INFO] - Training Epoch: 9/10, step 6/574 completed (loss: 0.08898144960403442, acc: 0.9591836929321289)
[2025-01-06 01:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18][root][INFO] - Training Epoch: 9/10, step 7/574 completed (loss: 0.10831990838050842, acc: 0.9666666388511658)
[2025-01-06 01:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18][root][INFO] - Training Epoch: 9/10, step 8/574 completed (loss: 0.00352409016340971, acc: 1.0)
[2025-01-06 01:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18][root][INFO] - Training Epoch: 9/10, step 9/574 completed (loss: 0.0026145498268306255, acc: 1.0)
[2025-01-06 01:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19][root][INFO] - Training Epoch: 9/10, step 10/574 completed (loss: 0.002656396944075823, acc: 1.0)
[2025-01-06 01:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19][root][INFO] - Training Epoch: 9/10, step 11/574 completed (loss: 0.006779918447136879, acc: 1.0)
[2025-01-06 01:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19][root][INFO] - Training Epoch: 9/10, step 12/574 completed (loss: 0.006834798492491245, acc: 1.0)
[2025-01-06 01:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20][root][INFO] - Training Epoch: 9/10, step 13/574 completed (loss: 0.013865218497812748, acc: 1.0)
[2025-01-06 01:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20][root][INFO] - Training Epoch: 9/10, step 14/574 completed (loss: 0.008055884391069412, acc: 1.0)
[2025-01-06 01:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20][root][INFO] - Training Epoch: 9/10, step 15/574 completed (loss: 0.021174980327486992, acc: 1.0)
[2025-01-06 01:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21][root][INFO] - Training Epoch: 9/10, step 16/574 completed (loss: 0.0017022539395838976, acc: 1.0)
[2025-01-06 01:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21][root][INFO] - Training Epoch: 9/10, step 17/574 completed (loss: 0.04435183107852936, acc: 0.9583333134651184)
[2025-01-06 01:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21][root][INFO] - Training Epoch: 9/10, step 18/574 completed (loss: 0.04859549552202225, acc: 1.0)
[2025-01-06 01:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22][root][INFO] - Training Epoch: 9/10, step 19/574 completed (loss: 0.0005866107530891895, acc: 1.0)
[2025-01-06 01:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22][root][INFO] - Training Epoch: 9/10, step 20/574 completed (loss: 0.020416323095560074, acc: 1.0)
[2025-01-06 01:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22][root][INFO] - Training Epoch: 9/10, step 21/574 completed (loss: 0.004257923923432827, acc: 1.0)
[2025-01-06 01:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23][root][INFO] - Training Epoch: 9/10, step 22/574 completed (loss: 0.03255883604288101, acc: 1.0)
[2025-01-06 01:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23][root][INFO] - Training Epoch: 9/10, step 23/574 completed (loss: 0.013089385814964771, acc: 1.0)
[2025-01-06 01:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23][root][INFO] - Training Epoch: 9/10, step 24/574 completed (loss: 0.012575939297676086, acc: 1.0)
[2025-01-06 01:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24][root][INFO] - Training Epoch: 9/10, step 25/574 completed (loss: 0.22368718683719635, acc: 0.9622641801834106)
[2025-01-06 01:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24][root][INFO] - Training Epoch: 9/10, step 26/574 completed (loss: 0.04846687987446785, acc: 0.9863013625144958)
[2025-01-06 01:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:25][root][INFO] - Training Epoch: 9/10, step 27/574 completed (loss: 0.273049920797348, acc: 0.8853754997253418)
[2025-01-06 01:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26][root][INFO] - Training Epoch: 9/10, step 28/574 completed (loss: 0.11171254515647888, acc: 0.9534883499145508)
[2025-01-06 01:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26][root][INFO] - Training Epoch: 9/10, step 29/574 completed (loss: 0.08692032098770142, acc: 0.9759036302566528)
[2025-01-06 01:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26][root][INFO] - Training Epoch: 9/10, step 30/574 completed (loss: 0.09950103610754013, acc: 0.9753086566925049)
[2025-01-06 01:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27][root][INFO] - Training Epoch: 9/10, step 31/574 completed (loss: 0.0287090502679348, acc: 1.0)
[2025-01-06 01:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27][root][INFO] - Training Epoch: 9/10, step 32/574 completed (loss: 0.06250554323196411, acc: 0.9629629850387573)
[2025-01-06 01:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27][root][INFO] - Training Epoch: 9/10, step 33/574 completed (loss: 0.008959085680544376, acc: 1.0)
[2025-01-06 01:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27][root][INFO] - Training Epoch: 9/10, step 34/574 completed (loss: 0.08232340961694717, acc: 0.9663865566253662)
[2025-01-06 01:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28][root][INFO] - Training Epoch: 9/10, step 35/574 completed (loss: 0.08843110501766205, acc: 0.9672130942344666)
[2025-01-06 01:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28][root][INFO] - Training Epoch: 9/10, step 36/574 completed (loss: 0.03856136277318001, acc: 0.9841269850730896)
[2025-01-06 01:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28][root][INFO] - Training Epoch: 9/10, step 37/574 completed (loss: 0.009550128132104874, acc: 1.0)
[2025-01-06 01:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:29][root][INFO] - Training Epoch: 9/10, step 38/574 completed (loss: 0.07365070283412933, acc: 0.977011501789093)
[2025-01-06 01:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:29][root][INFO] - Training Epoch: 9/10, step 39/574 completed (loss: 0.02732616290450096, acc: 1.0)
[2025-01-06 01:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:30][root][INFO] - Training Epoch: 9/10, step 40/574 completed (loss: 0.0883350819349289, acc: 0.9230769276618958)
[2025-01-06 01:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:30][root][INFO] - Training Epoch: 9/10, step 41/574 completed (loss: 0.20758621394634247, acc: 0.9459459185600281)
[2025-01-06 01:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:30][root][INFO] - Training Epoch: 9/10, step 42/574 completed (loss: 0.13176201283931732, acc: 0.9538461565971375)
[2025-01-06 01:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:31][root][INFO] - Training Epoch: 9/10, step 43/574 completed (loss: 0.0861232653260231, acc: 0.9595959782600403)
[2025-01-06 01:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:31][root][INFO] - Training Epoch: 9/10, step 44/574 completed (loss: 0.09138700366020203, acc: 0.969072163105011)
[2025-01-06 01:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32][root][INFO] - Training Epoch: 9/10, step 45/574 completed (loss: 0.07367419451475143, acc: 0.9779411554336548)
[2025-01-06 01:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32][root][INFO] - Training Epoch: 9/10, step 46/574 completed (loss: 0.0039873202331364155, acc: 1.0)
[2025-01-06 01:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32][root][INFO] - Training Epoch: 9/10, step 47/574 completed (loss: 0.08886586874723434, acc: 0.9629629850387573)
[2025-01-06 01:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32][root][INFO] - Training Epoch: 9/10, step 48/574 completed (loss: 0.005023537669330835, acc: 1.0)
[2025-01-06 01:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33][root][INFO] - Training Epoch: 9/10, step 49/574 completed (loss: 0.16469168663024902, acc: 0.9722222089767456)
[2025-01-06 01:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33][root][INFO] - Training Epoch: 9/10, step 50/574 completed (loss: 0.1327718049287796, acc: 0.9473684430122375)
[2025-01-06 01:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33][root][INFO] - Training Epoch: 9/10, step 51/574 completed (loss: 0.1081632524728775, acc: 0.9365079402923584)
[2025-01-06 01:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34][root][INFO] - Training Epoch: 9/10, step 52/574 completed (loss: 0.2905113995075226, acc: 0.9014084339141846)
[2025-01-06 01:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34][root][INFO] - Training Epoch: 9/10, step 53/574 completed (loss: 0.516643762588501, acc: 0.8333333134651184)
[2025-01-06 01:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34][root][INFO] - Training Epoch: 9/10, step 54/574 completed (loss: 0.1228170394897461, acc: 0.9729729890823364)
[2025-01-06 01:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:35][root][INFO] - Training Epoch: 9/10, step 55/574 completed (loss: 0.0022825351916253567, acc: 1.0)
[2025-01-06 01:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:38][root][INFO] - Training Epoch: 9/10, step 56/574 completed (loss: 0.4045547544956207, acc: 0.8532423377037048)
[2025-01-06 01:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:39][root][INFO] - Training Epoch: 9/10, step 57/574 completed (loss: 0.7825831770896912, acc: 0.7973856329917908)
[2025-01-06 01:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40][root][INFO] - Training Epoch: 9/10, step 58/574 completed (loss: 0.25134825706481934, acc: 0.8977272510528564)
[2025-01-06 01:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40][root][INFO] - Training Epoch: 9/10, step 59/574 completed (loss: 0.08115781843662262, acc: 0.9779411554336548)
[2025-01-06 01:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:41][root][INFO] - Training Epoch: 9/10, step 60/574 completed (loss: 0.22564280033111572, acc: 0.9202898740768433)
[2025-01-06 01:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:41][root][INFO] - Training Epoch: 9/10, step 61/574 completed (loss: 0.10662670433521271, acc: 0.9624999761581421)
[2025-01-06 01:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42][root][INFO] - Training Epoch: 9/10, step 62/574 completed (loss: 0.008022364228963852, acc: 1.0)
[2025-01-06 01:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42][root][INFO] - Training Epoch: 9/10, step 63/574 completed (loss: 0.010211491957306862, acc: 1.0)
[2025-01-06 01:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42][root][INFO] - Training Epoch: 9/10, step 64/574 completed (loss: 0.1581019163131714, acc: 0.96875)
[2025-01-06 01:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43][root][INFO] - Training Epoch: 9/10, step 65/574 completed (loss: 0.003000300144776702, acc: 1.0)
[2025-01-06 01:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43][root][INFO] - Training Epoch: 9/10, step 66/574 completed (loss: 0.11327844858169556, acc: 0.9464285969734192)
[2025-01-06 01:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43][root][INFO] - Training Epoch: 9/10, step 67/574 completed (loss: 0.07494962215423584, acc: 0.9666666388511658)
[2025-01-06 01:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44][root][INFO] - Training Epoch: 9/10, step 68/574 completed (loss: 0.001955389278009534, acc: 1.0)
[2025-01-06 01:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44][root][INFO] - Training Epoch: 9/10, step 69/574 completed (loss: 0.1523786187171936, acc: 0.9722222089767456)
[2025-01-06 01:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44][root][INFO] - Training Epoch: 9/10, step 70/574 completed (loss: 0.05865238606929779, acc: 1.0)
[2025-01-06 01:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45][root][INFO] - Training Epoch: 9/10, step 71/574 completed (loss: 0.24901247024536133, acc: 0.9264705777168274)
[2025-01-06 01:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45][root][INFO] - Training Epoch: 9/10, step 72/574 completed (loss: 0.25808286666870117, acc: 0.9523809552192688)
[2025-01-06 01:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45][root][INFO] - Training Epoch: 9/10, step 73/574 completed (loss: 0.6382179260253906, acc: 0.7897436022758484)
[2025-01-06 01:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:46][root][INFO] - Training Epoch: 9/10, step 74/574 completed (loss: 0.22226141393184662, acc: 0.918367326259613)
[2025-01-06 01:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:46][root][INFO] - Training Epoch: 9/10, step 75/574 completed (loss: 0.3032158315181732, acc: 0.9253731369972229)
[2025-01-06 01:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47][root][INFO] - Training Epoch: 9/10, step 76/574 completed (loss: 0.6757789254188538, acc: 0.8138686418533325)
[2025-01-06 01:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47][root][INFO] - Training Epoch: 9/10, step 77/574 completed (loss: 0.0010923473164439201, acc: 1.0)
[2025-01-06 01:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47][root][INFO] - Training Epoch: 9/10, step 78/574 completed (loss: 0.06791345775127411, acc: 0.9583333134651184)
[2025-01-06 01:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48][root][INFO] - Training Epoch: 9/10, step 79/574 completed (loss: 0.018732022494077682, acc: 1.0)
[2025-01-06 01:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48][root][INFO] - Training Epoch: 9/10, step 80/574 completed (loss: 0.0009962237672880292, acc: 1.0)
[2025-01-06 01:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48][root][INFO] - Training Epoch: 9/10, step 81/574 completed (loss: 0.038174450397491455, acc: 0.9807692170143127)
[2025-01-06 01:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49][root][INFO] - Training Epoch: 9/10, step 82/574 completed (loss: 0.06661369651556015, acc: 0.9807692170143127)
[2025-01-06 01:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49][root][INFO] - Training Epoch: 9/10, step 83/574 completed (loss: 0.016311844810843468, acc: 1.0)
[2025-01-06 01:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49][root][INFO] - Training Epoch: 9/10, step 84/574 completed (loss: 0.12095089256763458, acc: 0.95652174949646)
[2025-01-06 01:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50][root][INFO] - Training Epoch: 9/10, step 85/574 completed (loss: 0.038242071866989136, acc: 0.9800000190734863)
[2025-01-06 01:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50][root][INFO] - Training Epoch: 9/10, step 86/574 completed (loss: 0.07727295905351639, acc: 0.95652174949646)
[2025-01-06 01:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:51][root][INFO] - Training Epoch: 9/10, step 87/574 completed (loss: 0.06722473353147507, acc: 0.9800000190734863)
[2025-01-06 01:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:51][root][INFO] - Training Epoch: 9/10, step 88/574 completed (loss: 0.16082410514354706, acc: 0.9223300814628601)
[2025-01-06 01:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:52][root][INFO] - Training Epoch: 9/10, step 89/574 completed (loss: 0.32791706919670105, acc: 0.8834951519966125)
[2025-01-06 01:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:53][root][INFO] - Training Epoch: 9/10, step 90/574 completed (loss: 0.38495874404907227, acc: 0.8870967626571655)
[2025-01-06 01:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:54][root][INFO] - Training Epoch: 9/10, step 91/574 completed (loss: 0.2950054109096527, acc: 0.9051724076271057)
[2025-01-06 01:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:55][root][INFO] - Training Epoch: 9/10, step 92/574 completed (loss: 0.14695870876312256, acc: 0.9368420839309692)
[2025-01-06 01:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56][root][INFO] - Training Epoch: 9/10, step 93/574 completed (loss: 0.19272591173648834, acc: 0.9504950642585754)
[2025-01-06 01:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56][root][INFO] - Training Epoch: 9/10, step 94/574 completed (loss: 0.1310328096151352, acc: 0.9516128897666931)
[2025-01-06 01:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56][root][INFO] - Training Epoch: 9/10, step 95/574 completed (loss: 0.15903019905090332, acc: 0.95652174949646)
[2025-01-06 01:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57][root][INFO] - Training Epoch: 9/10, step 96/574 completed (loss: 0.2388136386871338, acc: 0.9411764740943909)
[2025-01-06 01:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57][root][INFO] - Training Epoch: 9/10, step 97/574 completed (loss: 0.2824530303478241, acc: 0.9230769276618958)
[2025-01-06 01:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57][root][INFO] - Training Epoch: 9/10, step 98/574 completed (loss: 0.250299334526062, acc: 0.9343065619468689)
[2025-01-06 01:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58][root][INFO] - Training Epoch: 9/10, step 99/574 completed (loss: 0.3022834360599518, acc: 0.8805969953536987)
[2025-01-06 01:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58][root][INFO] - Training Epoch: 9/10, step 100/574 completed (loss: 0.004724212922155857, acc: 1.0)
[2025-01-06 01:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58][root][INFO] - Training Epoch: 9/10, step 101/574 completed (loss: 0.010127994231879711, acc: 1.0)
[2025-01-06 01:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59][root][INFO] - Training Epoch: 9/10, step 102/574 completed (loss: 0.01078624464571476, acc: 1.0)
[2025-01-06 01:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59][root][INFO] - Training Epoch: 9/10, step 103/574 completed (loss: 0.01404072716832161, acc: 1.0)
[2025-01-06 01:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59][root][INFO] - Training Epoch: 9/10, step 104/574 completed (loss: 0.07792455703020096, acc: 0.9482758641242981)
[2025-01-06 01:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00][root][INFO] - Training Epoch: 9/10, step 105/574 completed (loss: 0.01254297699779272, acc: 1.0)
[2025-01-06 01:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00][root][INFO] - Training Epoch: 9/10, step 106/574 completed (loss: 0.04951851814985275, acc: 1.0)
[2025-01-06 01:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00][root][INFO] - Training Epoch: 9/10, step 107/574 completed (loss: 0.0012913485988974571, acc: 1.0)
[2025-01-06 01:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01][root][INFO] - Training Epoch: 9/10, step 108/574 completed (loss: 0.018290013074874878, acc: 1.0)
[2025-01-06 01:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01][root][INFO] - Training Epoch: 9/10, step 109/574 completed (loss: 0.1401011049747467, acc: 0.9523809552192688)
[2025-01-06 01:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01][root][INFO] - Training Epoch: 9/10, step 110/574 completed (loss: 0.20256778597831726, acc: 0.9538461565971375)
[2025-01-06 01:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02][root][INFO] - Training Epoch: 9/10, step 111/574 completed (loss: 0.15650363266468048, acc: 0.9298245906829834)
[2025-01-06 01:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02][root][INFO] - Training Epoch: 9/10, step 112/574 completed (loss: 0.06661728024482727, acc: 1.0)
[2025-01-06 01:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03][root][INFO] - Training Epoch: 9/10, step 113/574 completed (loss: 0.04862198606133461, acc: 1.0)
[2025-01-06 01:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03][root][INFO] - Training Epoch: 9/10, step 114/574 completed (loss: 0.11452563107013702, acc: 0.9795918464660645)
[2025-01-06 01:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03][root][INFO] - Training Epoch: 9/10, step 115/574 completed (loss: 0.0006754989153705537, acc: 1.0)
[2025-01-06 01:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:04][root][INFO] - Training Epoch: 9/10, step 116/574 completed (loss: 0.032407987862825394, acc: 1.0)
[2025-01-06 01:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:04][root][INFO] - Training Epoch: 9/10, step 117/574 completed (loss: 0.08256760239601135, acc: 0.9593495726585388)
[2025-01-06 01:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05][root][INFO] - Training Epoch: 9/10, step 118/574 completed (loss: 0.020856443792581558, acc: 0.9838709831237793)
[2025-01-06 01:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05][root][INFO] - Training Epoch: 9/10, step 119/574 completed (loss: 0.3592732548713684, acc: 0.8935361504554749)
[2025-01-06 01:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06][root][INFO] - Training Epoch: 9/10, step 120/574 completed (loss: 0.026855656877160072, acc: 1.0)
[2025-01-06 01:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06][root][INFO] - Training Epoch: 9/10, step 121/574 completed (loss: 0.3298376500606537, acc: 0.942307710647583)
[2025-01-06 01:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06][root][INFO] - Training Epoch: 9/10, step 122/574 completed (loss: 0.009143687784671783, acc: 1.0)
[2025-01-06 01:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07][root][INFO] - Training Epoch: 9/10, step 123/574 completed (loss: 0.029828393831849098, acc: 1.0)
[2025-01-06 01:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07][root][INFO] - Training Epoch: 9/10, step 124/574 completed (loss: 0.2715388238430023, acc: 0.9141104221343994)
[2025-01-06 01:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08][root][INFO] - Training Epoch: 9/10, step 125/574 completed (loss: 0.2986984848976135, acc: 0.8958333134651184)
[2025-01-06 01:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08][root][INFO] - Training Epoch: 9/10, step 126/574 completed (loss: 0.17155471444129944, acc: 0.9750000238418579)
[2025-01-06 01:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:36][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4904, device='cuda:0') eval_epoch_loss=tensor(0.9124, device='cuda:0') eval_epoch_acc=tensor(0.8245, device='cuda:0')
[2025-01-06 01:47:36][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:47:36][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:47:36][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_127_loss_0.9124407768249512/model.pt
[2025-01-06 01:47:36][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37][root][INFO] - Training Epoch: 9/10, step 127/574 completed (loss: 0.15495184063911438, acc: 0.9404761791229248)
[2025-01-06 01:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37][root][INFO] - Training Epoch: 9/10, step 128/574 completed (loss: 0.21429917216300964, acc: 0.9333333373069763)
[2025-01-06 01:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38][root][INFO] - Training Epoch: 9/10, step 129/574 completed (loss: 0.20413319766521454, acc: 0.9191176295280457)
[2025-01-06 01:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38][root][INFO] - Training Epoch: 9/10, step 130/574 completed (loss: 0.0052903336472809315, acc: 1.0)
[2025-01-06 01:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38][root][INFO] - Training Epoch: 9/10, step 131/574 completed (loss: 0.043946344405412674, acc: 1.0)
[2025-01-06 01:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38][root][INFO] - Training Epoch: 9/10, step 132/574 completed (loss: 0.03748499974608421, acc: 0.96875)
[2025-01-06 01:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39][root][INFO] - Training Epoch: 9/10, step 133/574 completed (loss: 0.06942372769117355, acc: 0.95652174949646)
[2025-01-06 01:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39][root][INFO] - Training Epoch: 9/10, step 134/574 completed (loss: 0.22732731699943542, acc: 0.9428571462631226)
[2025-01-06 01:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39][root][INFO] - Training Epoch: 9/10, step 135/574 completed (loss: 0.009632022120058537, acc: 1.0)
[2025-01-06 01:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40][root][INFO] - Training Epoch: 9/10, step 136/574 completed (loss: 0.02295033074915409, acc: 0.976190447807312)
[2025-01-06 01:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40][root][INFO] - Training Epoch: 9/10, step 137/574 completed (loss: 0.11129564791917801, acc: 0.9333333373069763)
[2025-01-06 01:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40][root][INFO] - Training Epoch: 9/10, step 138/574 completed (loss: 0.058950480073690414, acc: 0.95652174949646)
[2025-01-06 01:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41][root][INFO] - Training Epoch: 9/10, step 139/574 completed (loss: 0.0019478531321510673, acc: 1.0)
[2025-01-06 01:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41][root][INFO] - Training Epoch: 9/10, step 140/574 completed (loss: 0.05371994152665138, acc: 1.0)
[2025-01-06 01:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41][root][INFO] - Training Epoch: 9/10, step 141/574 completed (loss: 0.040349122136831284, acc: 0.9677419066429138)
[2025-01-06 01:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42][root][INFO] - Training Epoch: 9/10, step 142/574 completed (loss: 0.28265368938446045, acc: 0.9189189076423645)
[2025-01-06 01:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42][root][INFO] - Training Epoch: 9/10, step 143/574 completed (loss: 0.16323857009410858, acc: 0.9385964870452881)
[2025-01-06 01:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42][root][INFO] - Training Epoch: 9/10, step 144/574 completed (loss: 0.1932765394449234, acc: 0.9402984976768494)
[2025-01-06 01:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43][root][INFO] - Training Epoch: 9/10, step 145/574 completed (loss: 0.20429284870624542, acc: 0.9489796161651611)
[2025-01-06 01:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43][root][INFO] - Training Epoch: 9/10, step 146/574 completed (loss: 0.12745703756809235, acc: 0.957446813583374)
[2025-01-06 01:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43][root][INFO] - Training Epoch: 9/10, step 147/574 completed (loss: 0.22562289237976074, acc: 0.9571428298950195)
[2025-01-06 01:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44][root][INFO] - Training Epoch: 9/10, step 148/574 completed (loss: 0.017756149172782898, acc: 1.0)
[2025-01-06 01:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44][root][INFO] - Training Epoch: 9/10, step 149/574 completed (loss: 0.005525671411305666, acc: 1.0)
[2025-01-06 01:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44][root][INFO] - Training Epoch: 9/10, step 150/574 completed (loss: 0.005056342575699091, acc: 1.0)
[2025-01-06 01:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45][root][INFO] - Training Epoch: 9/10, step 151/574 completed (loss: 0.04918980970978737, acc: 0.97826087474823)
[2025-01-06 01:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45][root][INFO] - Training Epoch: 9/10, step 152/574 completed (loss: 0.07168181240558624, acc: 0.9830508232116699)
[2025-01-06 01:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45][root][INFO] - Training Epoch: 9/10, step 153/574 completed (loss: 0.12413059920072556, acc: 0.9649122953414917)
[2025-01-06 01:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46][root][INFO] - Training Epoch: 9/10, step 154/574 completed (loss: 0.16532683372497559, acc: 0.9729729890823364)
[2025-01-06 01:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46][root][INFO] - Training Epoch: 9/10, step 155/574 completed (loss: 0.014588266611099243, acc: 1.0)
[2025-01-06 01:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46][root][INFO] - Training Epoch: 9/10, step 156/574 completed (loss: 0.07669320702552795, acc: 0.95652174949646)
[2025-01-06 01:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:47][root][INFO] - Training Epoch: 9/10, step 157/574 completed (loss: 0.21494179964065552, acc: 0.9473684430122375)
[2025-01-06 01:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:48][root][INFO] - Training Epoch: 9/10, step 158/574 completed (loss: 0.7090420126914978, acc: 0.8243243098258972)
[2025-01-06 01:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:49][root][INFO] - Training Epoch: 9/10, step 159/574 completed (loss: 1.044543743133545, acc: 0.7777777910232544)
[2025-01-06 01:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:49][root][INFO] - Training Epoch: 9/10, step 160/574 completed (loss: 0.31208422780036926, acc: 0.8720930218696594)
[2025-01-06 01:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:50][root][INFO] - Training Epoch: 9/10, step 161/574 completed (loss: 0.13661697506904602, acc: 0.9764705896377563)
[2025-01-06 01:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:50][root][INFO] - Training Epoch: 9/10, step 162/574 completed (loss: 0.5358391404151917, acc: 0.8426966071128845)
[2025-01-06 01:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:50][root][INFO] - Training Epoch: 9/10, step 163/574 completed (loss: 0.03628089651465416, acc: 1.0)
[2025-01-06 01:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:51][root][INFO] - Training Epoch: 9/10, step 164/574 completed (loss: 0.002822066191583872, acc: 1.0)
[2025-01-06 01:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:51][root][INFO] - Training Epoch: 9/10, step 165/574 completed (loss: 0.12818729877471924, acc: 0.931034505367279)
[2025-01-06 01:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:51][root][INFO] - Training Epoch: 9/10, step 166/574 completed (loss: 0.010053071193397045, acc: 1.0)
[2025-01-06 01:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52][root][INFO] - Training Epoch: 9/10, step 167/574 completed (loss: 0.1043315902352333, acc: 0.9599999785423279)
[2025-01-06 01:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52][root][INFO] - Training Epoch: 9/10, step 168/574 completed (loss: 0.17381040751934052, acc: 0.9583333134651184)
[2025-01-06 01:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52][root][INFO] - Training Epoch: 9/10, step 169/574 completed (loss: 0.28590282797813416, acc: 0.9215686321258545)
[2025-01-06 01:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:53][root][INFO] - Training Epoch: 9/10, step 170/574 completed (loss: 0.2389741986989975, acc: 0.931506872177124)
[2025-01-06 01:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54][root][INFO] - Training Epoch: 9/10, step 171/574 completed (loss: 0.0017197391716763377, acc: 1.0)
[2025-01-06 01:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54][root][INFO] - Training Epoch: 9/10, step 172/574 completed (loss: 0.001575378468260169, acc: 1.0)
[2025-01-06 01:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54][root][INFO] - Training Epoch: 9/10, step 173/574 completed (loss: 0.010582885704934597, acc: 1.0)
[2025-01-06 01:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:55][root][INFO] - Training Epoch: 9/10, step 174/574 completed (loss: 0.41538307070732117, acc: 0.8761062026023865)
[2025-01-06 01:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:55][root][INFO] - Training Epoch: 9/10, step 175/574 completed (loss: 0.17180465161800385, acc: 0.9420289993286133)
[2025-01-06 01:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56][root][INFO] - Training Epoch: 9/10, step 176/574 completed (loss: 0.1732996255159378, acc: 0.9431818127632141)
[2025-01-06 01:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56][root][INFO] - Training Epoch: 9/10, step 177/574 completed (loss: 0.24090923368930817, acc: 0.9236640930175781)
[2025-01-06 01:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57][root][INFO] - Training Epoch: 9/10, step 178/574 completed (loss: 0.38341522216796875, acc: 0.8814814686775208)
[2025-01-06 01:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57][root][INFO] - Training Epoch: 9/10, step 179/574 completed (loss: 0.06260107457637787, acc: 0.9508196711540222)
[2025-01-06 01:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58][root][INFO] - Training Epoch: 9/10, step 180/574 completed (loss: 0.005518214777112007, acc: 1.0)
[2025-01-06 01:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58][root][INFO] - Training Epoch: 9/10, step 181/574 completed (loss: 0.0011832095915451646, acc: 1.0)
[2025-01-06 01:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58][root][INFO] - Training Epoch: 9/10, step 182/574 completed (loss: 0.011319617740809917, acc: 1.0)
[2025-01-06 01:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59][root][INFO] - Training Epoch: 9/10, step 183/574 completed (loss: 0.023521585389971733, acc: 1.0)
[2025-01-06 01:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59][root][INFO] - Training Epoch: 9/10, step 184/574 completed (loss: 0.18229468166828156, acc: 0.9516616463661194)
[2025-01-06 01:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59][root][INFO] - Training Epoch: 9/10, step 185/574 completed (loss: 0.22749055922031403, acc: 0.9221901893615723)
[2025-01-06 01:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00][root][INFO] - Training Epoch: 9/10, step 186/574 completed (loss: 0.16336564719676971, acc: 0.9468749761581421)
[2025-01-06 01:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00][root][INFO] - Training Epoch: 9/10, step 187/574 completed (loss: 0.2299875169992447, acc: 0.9380863308906555)
[2025-01-06 01:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01][root][INFO] - Training Epoch: 9/10, step 188/574 completed (loss: 0.20257830619812012, acc: 0.9501779079437256)
[2025-01-06 01:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01][root][INFO] - Training Epoch: 9/10, step 189/574 completed (loss: 0.22015707194805145, acc: 0.8799999952316284)
[2025-01-06 01:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:02][root][INFO] - Training Epoch: 9/10, step 190/574 completed (loss: 0.22174522280693054, acc: 0.930232584476471)
[2025-01-06 01:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:02][root][INFO] - Training Epoch: 9/10, step 191/574 completed (loss: 0.2908616364002228, acc: 0.89682537317276)
[2025-01-06 01:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:03][root][INFO] - Training Epoch: 9/10, step 192/574 completed (loss: 0.27352142333984375, acc: 0.9242424368858337)
[2025-01-06 01:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:04][root][INFO] - Training Epoch: 9/10, step 193/574 completed (loss: 0.09892155975103378, acc: 0.9882352948188782)
[2025-01-06 01:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:05][root][INFO] - Training Epoch: 9/10, step 194/574 completed (loss: 0.4053444564342499, acc: 0.8827160596847534)
[2025-01-06 01:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:06][root][INFO] - Training Epoch: 9/10, step 195/574 completed (loss: 0.07325991243124008, acc: 0.9677419066429138)
[2025-01-06 01:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:06][root][INFO] - Training Epoch: 9/10, step 196/574 completed (loss: 0.06029237061738968, acc: 0.9642857313156128)
[2025-01-06 01:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:07][root][INFO] - Training Epoch: 9/10, step 197/574 completed (loss: 0.10090933740139008, acc: 0.949999988079071)
[2025-01-06 01:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:07][root][INFO] - Training Epoch: 9/10, step 198/574 completed (loss: 0.18845416605472565, acc: 0.9558823704719543)
[2025-01-06 01:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08][root][INFO] - Training Epoch: 9/10, step 199/574 completed (loss: 0.20685355365276337, acc: 0.9338235259056091)
[2025-01-06 01:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08][root][INFO] - Training Epoch: 9/10, step 200/574 completed (loss: 0.21322238445281982, acc: 0.9237288236618042)
[2025-01-06 01:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08][root][INFO] - Training Epoch: 9/10, step 201/574 completed (loss: 0.12708614766597748, acc: 0.9701492786407471)
[2025-01-06 01:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09][root][INFO] - Training Epoch: 9/10, step 202/574 completed (loss: 0.11235000193119049, acc: 0.9514563083648682)
[2025-01-06 01:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09][root][INFO] - Training Epoch: 9/10, step 203/574 completed (loss: 0.02961895987391472, acc: 1.0)
[2025-01-06 01:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09][root][INFO] - Training Epoch: 9/10, step 204/574 completed (loss: 0.005414175800979137, acc: 1.0)
[2025-01-06 01:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09][root][INFO] - Training Epoch: 9/10, step 205/574 completed (loss: 0.08157932013273239, acc: 0.9775784611701965)
[2025-01-06 01:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10][root][INFO] - Training Epoch: 9/10, step 206/574 completed (loss: 0.11219470947980881, acc: 0.9685039520263672)
[2025-01-06 01:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10][root][INFO] - Training Epoch: 9/10, step 207/574 completed (loss: 0.11138926446437836, acc: 0.9784482717514038)
[2025-01-06 01:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10][root][INFO] - Training Epoch: 9/10, step 208/574 completed (loss: 0.18200495839118958, acc: 0.945652186870575)
[2025-01-06 01:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11][root][INFO] - Training Epoch: 9/10, step 209/574 completed (loss: 0.09856713563203812, acc: 0.9610894918441772)
[2025-01-06 01:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11][root][INFO] - Training Epoch: 9/10, step 210/574 completed (loss: 0.050104472786188126, acc: 0.97826087474823)
[2025-01-06 01:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11][root][INFO] - Training Epoch: 9/10, step 211/574 completed (loss: 0.007754837162792683, acc: 1.0)
[2025-01-06 01:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12][root][INFO] - Training Epoch: 9/10, step 212/574 completed (loss: 0.008579655550420284, acc: 1.0)
[2025-01-06 01:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12][root][INFO] - Training Epoch: 9/10, step 213/574 completed (loss: 0.0076357596553862095, acc: 1.0)
[2025-01-06 01:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13][root][INFO] - Training Epoch: 9/10, step 214/574 completed (loss: 0.0318545438349247, acc: 0.9846153855323792)
[2025-01-06 01:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13][root][INFO] - Training Epoch: 9/10, step 215/574 completed (loss: 0.03377619758248329, acc: 0.9729729890823364)
[2025-01-06 01:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13][root][INFO] - Training Epoch: 9/10, step 216/574 completed (loss: 0.059209950268268585, acc: 0.9883720874786377)
[2025-01-06 01:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14][root][INFO] - Training Epoch: 9/10, step 217/574 completed (loss: 0.08912704885005951, acc: 0.9819819927215576)
[2025-01-06 01:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14][root][INFO] - Training Epoch: 9/10, step 218/574 completed (loss: 0.036172136664390564, acc: 0.9777777791023254)
[2025-01-06 01:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15][root][INFO] - Training Epoch: 9/10, step 219/574 completed (loss: 0.0023221485316753387, acc: 1.0)
[2025-01-06 01:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15][root][INFO] - Training Epoch: 9/10, step 220/574 completed (loss: 0.04941648617386818, acc: 0.9629629850387573)
[2025-01-06 01:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15][root][INFO] - Training Epoch: 9/10, step 221/574 completed (loss: 0.00712280860170722, acc: 1.0)
[2025-01-06 01:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15][root][INFO] - Training Epoch: 9/10, step 222/574 completed (loss: 0.07344119250774384, acc: 0.9807692170143127)
[2025-01-06 01:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:16][root][INFO] - Training Epoch: 9/10, step 223/574 completed (loss: 0.08634795248508453, acc: 0.967391312122345)
[2025-01-06 01:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:17][root][INFO] - Training Epoch: 9/10, step 224/574 completed (loss: 0.11515757441520691, acc: 0.9715909361839294)
[2025-01-06 01:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:17][root][INFO] - Training Epoch: 9/10, step 225/574 completed (loss: 0.06700345128774643, acc: 0.9893617033958435)
[2025-01-06 01:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:17][root][INFO] - Training Epoch: 9/10, step 226/574 completed (loss: 0.03761034458875656, acc: 1.0)
[2025-01-06 01:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18][root][INFO] - Training Epoch: 9/10, step 227/574 completed (loss: 0.04231591522693634, acc: 1.0)
[2025-01-06 01:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18][root][INFO] - Training Epoch: 9/10, step 228/574 completed (loss: 0.026810402050614357, acc: 1.0)
[2025-01-06 01:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19][root][INFO] - Training Epoch: 9/10, step 229/574 completed (loss: 0.08755362033843994, acc: 0.9666666388511658)
[2025-01-06 01:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19][root][INFO] - Training Epoch: 9/10, step 230/574 completed (loss: 0.3972838819026947, acc: 0.8947368264198303)
[2025-01-06 01:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19][root][INFO] - Training Epoch: 9/10, step 231/574 completed (loss: 0.43114030361175537, acc: 0.8444444537162781)
[2025-01-06 01:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:20][root][INFO] - Training Epoch: 9/10, step 232/574 completed (loss: 0.44466620683670044, acc: 0.8722222447395325)
[2025-01-06 01:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:20][root][INFO] - Training Epoch: 9/10, step 233/574 completed (loss: 0.8061618208885193, acc: 0.747706413269043)
[2025-01-06 01:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21][root][INFO] - Training Epoch: 9/10, step 234/574 completed (loss: 0.35951393842697144, acc: 0.9076923131942749)
[2025-01-06 01:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21][root][INFO] - Training Epoch: 9/10, step 235/574 completed (loss: 0.023803314194083214, acc: 1.0)
[2025-01-06 01:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21][root][INFO] - Training Epoch: 9/10, step 236/574 completed (loss: 0.006193581968545914, acc: 1.0)
[2025-01-06 01:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21][root][INFO] - Training Epoch: 9/10, step 237/574 completed (loss: 0.044847723096609116, acc: 1.0)
[2025-01-06 01:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22][root][INFO] - Training Epoch: 9/10, step 238/574 completed (loss: 0.15910175442695618, acc: 0.9629629850387573)
[2025-01-06 01:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22][root][INFO] - Training Epoch: 9/10, step 239/574 completed (loss: 0.03855368122458458, acc: 1.0)
[2025-01-06 01:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22][root][INFO] - Training Epoch: 9/10, step 240/574 completed (loss: 0.2471858561038971, acc: 0.9545454382896423)
[2025-01-06 01:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23][root][INFO] - Training Epoch: 9/10, step 241/574 completed (loss: 0.06224854290485382, acc: 1.0)
[2025-01-06 01:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23][root][INFO] - Training Epoch: 9/10, step 242/574 completed (loss: 0.11718442291021347, acc: 0.9516128897666931)
[2025-01-06 01:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24][root][INFO] - Training Epoch: 9/10, step 243/574 completed (loss: 0.1566675305366516, acc: 0.9545454382896423)
[2025-01-06 01:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24][root][INFO] - Training Epoch: 9/10, step 244/574 completed (loss: 0.017433037981390953, acc: 1.0)
[2025-01-06 01:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24][root][INFO] - Training Epoch: 9/10, step 245/574 completed (loss: 0.2580822706222534, acc: 0.9615384340286255)
[2025-01-06 01:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25][root][INFO] - Training Epoch: 9/10, step 246/574 completed (loss: 0.14228682219982147, acc: 0.9677419066429138)
[2025-01-06 01:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25][root][INFO] - Training Epoch: 9/10, step 247/574 completed (loss: 0.004656163044273853, acc: 1.0)
[2025-01-06 01:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25][root][INFO] - Training Epoch: 9/10, step 248/574 completed (loss: 0.017399946227669716, acc: 1.0)
[2025-01-06 01:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26][root][INFO] - Training Epoch: 9/10, step 249/574 completed (loss: 0.036253832280635834, acc: 0.9729729890823364)
[2025-01-06 01:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26][root][INFO] - Training Epoch: 9/10, step 250/574 completed (loss: 0.002996870782226324, acc: 1.0)
[2025-01-06 01:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26][root][INFO] - Training Epoch: 9/10, step 251/574 completed (loss: 0.028757674619555473, acc: 1.0)
[2025-01-06 01:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27][root][INFO] - Training Epoch: 9/10, step 252/574 completed (loss: 0.058482006192207336, acc: 0.9756097793579102)
[2025-01-06 01:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27][root][INFO] - Training Epoch: 9/10, step 253/574 completed (loss: 0.015833009034395218, acc: 1.0)
[2025-01-06 01:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27][root][INFO] - Training Epoch: 9/10, step 254/574 completed (loss: 0.0001600670802872628, acc: 1.0)
[2025-01-06 01:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28][root][INFO] - Training Epoch: 9/10, step 255/574 completed (loss: 0.0010151425376534462, acc: 1.0)
[2025-01-06 01:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28][root][INFO] - Training Epoch: 9/10, step 256/574 completed (loss: 0.007225082255899906, acc: 1.0)
[2025-01-06 01:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28][root][INFO] - Training Epoch: 9/10, step 257/574 completed (loss: 0.006562829948961735, acc: 1.0)
[2025-01-06 01:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28][root][INFO] - Training Epoch: 9/10, step 258/574 completed (loss: 0.12475623935461044, acc: 0.9605262875556946)
[2025-01-06 01:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:29][root][INFO] - Training Epoch: 9/10, step 259/574 completed (loss: 0.02138112671673298, acc: 1.0)
[2025-01-06 01:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30][root][INFO] - Training Epoch: 9/10, step 260/574 completed (loss: 0.037408795207738876, acc: 0.9916666746139526)
[2025-01-06 01:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30][root][INFO] - Training Epoch: 9/10, step 261/574 completed (loss: 0.013069923967123032, acc: 1.0)
[2025-01-06 01:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30][root][INFO] - Training Epoch: 9/10, step 262/574 completed (loss: 0.041982393711805344, acc: 0.9677419066429138)
[2025-01-06 01:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:31][root][INFO] - Training Epoch: 9/10, step 263/574 completed (loss: 0.18293648958206177, acc: 0.9466666579246521)
[2025-01-06 01:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:31][root][INFO] - Training Epoch: 9/10, step 264/574 completed (loss: 0.09115774184465408, acc: 0.9791666865348816)
[2025-01-06 01:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32][root][INFO] - Training Epoch: 9/10, step 265/574 completed (loss: 0.28887706995010376, acc: 0.9200000166893005)
[2025-01-06 01:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32][root][INFO] - Training Epoch: 9/10, step 266/574 completed (loss: 0.26280421018600464, acc: 0.9438202381134033)
[2025-01-06 01:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32][root][INFO] - Training Epoch: 9/10, step 267/574 completed (loss: 0.07786864787340164, acc: 0.9864864945411682)
[2025-01-06 01:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33][root][INFO] - Training Epoch: 9/10, step 268/574 completed (loss: 0.03185277059674263, acc: 1.0)
[2025-01-06 01:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33][root][INFO] - Training Epoch: 9/10, step 269/574 completed (loss: 0.002837412292137742, acc: 1.0)
[2025-01-06 01:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:01][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5092, device='cuda:0') eval_epoch_loss=tensor(0.9199, device='cuda:0') eval_epoch_acc=tensor(0.8232, device='cuda:0')
[2025-01-06 01:49:01][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:49:01][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:49:01][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_270_loss_0.9199445843696594/model.pt
[2025-01-06 01:49:01][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02][root][INFO] - Training Epoch: 9/10, step 270/574 completed (loss: 0.0033226122613996267, acc: 1.0)
[2025-01-06 01:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02][root][INFO] - Training Epoch: 9/10, step 271/574 completed (loss: 0.034376125782728195, acc: 0.96875)
[2025-01-06 01:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02][root][INFO] - Training Epoch: 9/10, step 272/574 completed (loss: 0.014521409757435322, acc: 1.0)
[2025-01-06 01:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:03][root][INFO] - Training Epoch: 9/10, step 273/574 completed (loss: 0.21866287291049957, acc: 0.9166666865348816)
[2025-01-06 01:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:03][root][INFO] - Training Epoch: 9/10, step 274/574 completed (loss: 0.03229515999555588, acc: 1.0)
[2025-01-06 01:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:03][root][INFO] - Training Epoch: 9/10, step 275/574 completed (loss: 0.001950367004610598, acc: 1.0)
[2025-01-06 01:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04][root][INFO] - Training Epoch: 9/10, step 276/574 completed (loss: 0.0013889132533222437, acc: 1.0)
[2025-01-06 01:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04][root][INFO] - Training Epoch: 9/10, step 277/574 completed (loss: 0.0026577108073979616, acc: 1.0)
[2025-01-06 01:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04][root][INFO] - Training Epoch: 9/10, step 278/574 completed (loss: 0.009314077906310558, acc: 1.0)
[2025-01-06 01:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05][root][INFO] - Training Epoch: 9/10, step 279/574 completed (loss: 0.015233281068503857, acc: 1.0)
[2025-01-06 01:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05][root][INFO] - Training Epoch: 9/10, step 280/574 completed (loss: 0.007322547025978565, acc: 1.0)
[2025-01-06 01:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05][root][INFO] - Training Epoch: 9/10, step 281/574 completed (loss: 0.21733742952346802, acc: 0.9397590160369873)
[2025-01-06 01:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06][root][INFO] - Training Epoch: 9/10, step 282/574 completed (loss: 0.13373495638370514, acc: 0.9444444179534912)
[2025-01-06 01:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06][root][INFO] - Training Epoch: 9/10, step 283/574 completed (loss: 0.2564164102077484, acc: 0.9473684430122375)
[2025-01-06 01:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06][root][INFO] - Training Epoch: 9/10, step 284/574 completed (loss: 0.182519793510437, acc: 0.970588207244873)
[2025-01-06 01:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:07][root][INFO] - Training Epoch: 9/10, step 285/574 completed (loss: 0.00564234796911478, acc: 1.0)
[2025-01-06 01:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:07][root][INFO] - Training Epoch: 9/10, step 286/574 completed (loss: 0.06661907583475113, acc: 0.984375)
[2025-01-06 01:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08][root][INFO] - Training Epoch: 9/10, step 287/574 completed (loss: 0.11254991590976715, acc: 0.9520000219345093)
[2025-01-06 01:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08][root][INFO] - Training Epoch: 9/10, step 288/574 completed (loss: 0.19480526447296143, acc: 0.9670329689979553)
[2025-01-06 01:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08][root][INFO] - Training Epoch: 9/10, step 289/574 completed (loss: 0.07154016196727753, acc: 0.9689440727233887)
[2025-01-06 01:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09][root][INFO] - Training Epoch: 9/10, step 290/574 completed (loss: 0.14933398365974426, acc: 0.9639175534248352)
[2025-01-06 01:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09][root][INFO] - Training Epoch: 9/10, step 291/574 completed (loss: 0.0041465419344604015, acc: 1.0)
[2025-01-06 01:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09][root][INFO] - Training Epoch: 9/10, step 292/574 completed (loss: 0.008162298239767551, acc: 1.0)
[2025-01-06 01:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:10][root][INFO] - Training Epoch: 9/10, step 293/574 completed (loss: 0.002707928419113159, acc: 1.0)
[2025-01-06 01:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:10][root][INFO] - Training Epoch: 9/10, step 294/574 completed (loss: 0.019225023686885834, acc: 1.0)
[2025-01-06 01:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11][root][INFO] - Training Epoch: 9/10, step 295/574 completed (loss: 0.16114337742328644, acc: 0.9639175534248352)
[2025-01-06 01:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11][root][INFO] - Training Epoch: 9/10, step 296/574 completed (loss: 0.03266581892967224, acc: 1.0)
[2025-01-06 01:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11][root][INFO] - Training Epoch: 9/10, step 297/574 completed (loss: 0.003024518257007003, acc: 1.0)
[2025-01-06 01:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11][root][INFO] - Training Epoch: 9/10, step 298/574 completed (loss: 0.0101160304620862, acc: 1.0)
[2025-01-06 01:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:12][root][INFO] - Training Epoch: 9/10, step 299/574 completed (loss: 0.1142028197646141, acc: 0.9821428656578064)
[2025-01-06 01:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:12][root][INFO] - Training Epoch: 9/10, step 300/574 completed (loss: 0.020174624398350716, acc: 1.0)
[2025-01-06 01:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:12][root][INFO] - Training Epoch: 9/10, step 301/574 completed (loss: 0.014420125633478165, acc: 1.0)
[2025-01-06 01:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13][root][INFO] - Training Epoch: 9/10, step 302/574 completed (loss: 0.0057917372323572636, acc: 1.0)
[2025-01-06 01:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13][root][INFO] - Training Epoch: 9/10, step 303/574 completed (loss: 0.0010357660939916968, acc: 1.0)
[2025-01-06 01:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13][root][INFO] - Training Epoch: 9/10, step 304/574 completed (loss: 0.028772631660103798, acc: 1.0)
[2025-01-06 01:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14][root][INFO] - Training Epoch: 9/10, step 305/574 completed (loss: 0.07502831518650055, acc: 0.9836065769195557)
[2025-01-06 01:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14][root][INFO] - Training Epoch: 9/10, step 306/574 completed (loss: 0.009022674523293972, acc: 1.0)
[2025-01-06 01:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14][root][INFO] - Training Epoch: 9/10, step 307/574 completed (loss: 0.000489480618853122, acc: 1.0)
[2025-01-06 01:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15][root][INFO] - Training Epoch: 9/10, step 308/574 completed (loss: 0.03181818127632141, acc: 1.0)
[2025-01-06 01:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15][root][INFO] - Training Epoch: 9/10, step 309/574 completed (loss: 0.04436575248837471, acc: 0.9861111044883728)
[2025-01-06 01:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16][root][INFO] - Training Epoch: 9/10, step 310/574 completed (loss: 0.09303351491689682, acc: 0.9759036302566528)
[2025-01-06 01:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16][root][INFO] - Training Epoch: 9/10, step 311/574 completed (loss: 0.027325797826051712, acc: 0.9871794581413269)
[2025-01-06 01:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16][root][INFO] - Training Epoch: 9/10, step 312/574 completed (loss: 0.05070962756872177, acc: 0.9795918464660645)
[2025-01-06 01:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17][root][INFO] - Training Epoch: 9/10, step 313/574 completed (loss: 0.0018906063633039594, acc: 1.0)
[2025-01-06 01:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17][root][INFO] - Training Epoch: 9/10, step 314/574 completed (loss: 0.057928476482629776, acc: 0.9583333134651184)
[2025-01-06 01:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17][root][INFO] - Training Epoch: 9/10, step 315/574 completed (loss: 0.0027484381571412086, acc: 1.0)
[2025-01-06 01:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18][root][INFO] - Training Epoch: 9/10, step 316/574 completed (loss: 0.16962219774723053, acc: 0.9677419066429138)
[2025-01-06 01:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18][root][INFO] - Training Epoch: 9/10, step 317/574 completed (loss: 0.02991306222975254, acc: 0.9850746393203735)
[2025-01-06 01:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18][root][INFO] - Training Epoch: 9/10, step 318/574 completed (loss: 0.0034330817870795727, acc: 1.0)
[2025-01-06 01:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:19][root][INFO] - Training Epoch: 9/10, step 319/574 completed (loss: 0.006345882546156645, acc: 1.0)
[2025-01-06 01:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:19][root][INFO] - Training Epoch: 9/10, step 320/574 completed (loss: 0.043688077479600906, acc: 0.9838709831237793)
[2025-01-06 01:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:19][root][INFO] - Training Epoch: 9/10, step 321/574 completed (loss: 0.0005384897231124341, acc: 1.0)
[2025-01-06 01:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:20][root][INFO] - Training Epoch: 9/10, step 322/574 completed (loss: 0.08204015344381332, acc: 1.0)
[2025-01-06 01:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:20][root][INFO] - Training Epoch: 9/10, step 323/574 completed (loss: 0.1135789081454277, acc: 0.9714285731315613)
[2025-01-06 01:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:20][root][INFO] - Training Epoch: 9/10, step 324/574 completed (loss: 0.07925249636173248, acc: 0.9487179517745972)
[2025-01-06 01:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:21][root][INFO] - Training Epoch: 9/10, step 325/574 completed (loss: 0.13615530729293823, acc: 0.9268292784690857)
[2025-01-06 01:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:21][root][INFO] - Training Epoch: 9/10, step 326/574 completed (loss: 0.2026943415403366, acc: 0.9736841917037964)
[2025-01-06 01:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:21][root][INFO] - Training Epoch: 9/10, step 327/574 completed (loss: 0.009997415356338024, acc: 1.0)
[2025-01-06 01:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22][root][INFO] - Training Epoch: 9/10, step 328/574 completed (loss: 0.028846191242337227, acc: 1.0)
[2025-01-06 01:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22][root][INFO] - Training Epoch: 9/10, step 329/574 completed (loss: 0.056901175528764725, acc: 0.9629629850387573)
[2025-01-06 01:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22][root][INFO] - Training Epoch: 9/10, step 330/574 completed (loss: 0.05504335090517998, acc: 0.96875)
[2025-01-06 01:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23][root][INFO] - Training Epoch: 9/10, step 331/574 completed (loss: 0.026153435930609703, acc: 0.9838709831237793)
[2025-01-06 01:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23][root][INFO] - Training Epoch: 9/10, step 332/574 completed (loss: 0.004630747716873884, acc: 1.0)
[2025-01-06 01:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23][root][INFO] - Training Epoch: 9/10, step 333/574 completed (loss: 0.006191552616655827, acc: 1.0)
[2025-01-06 01:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24][root][INFO] - Training Epoch: 9/10, step 334/574 completed (loss: 0.04429163411259651, acc: 1.0)
[2025-01-06 01:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24][root][INFO] - Training Epoch: 9/10, step 335/574 completed (loss: 0.0022742259316146374, acc: 1.0)
[2025-01-06 01:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24][root][INFO] - Training Epoch: 9/10, step 336/574 completed (loss: 0.04283278062939644, acc: 1.0)
[2025-01-06 01:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25][root][INFO] - Training Epoch: 9/10, step 337/574 completed (loss: 0.12853188812732697, acc: 0.9655172228813171)
[2025-01-06 01:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25][root][INFO] - Training Epoch: 9/10, step 338/574 completed (loss: 0.24005214869976044, acc: 0.9468085169792175)
[2025-01-06 01:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25][root][INFO] - Training Epoch: 9/10, step 339/574 completed (loss: 0.3953699767589569, acc: 0.9156626462936401)
[2025-01-06 01:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26][root][INFO] - Training Epoch: 9/10, step 340/574 completed (loss: 0.00422151992097497, acc: 1.0)
[2025-01-06 01:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26][root][INFO] - Training Epoch: 9/10, step 341/574 completed (loss: 0.009356711059808731, acc: 1.0)
[2025-01-06 01:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26][root][INFO] - Training Epoch: 9/10, step 342/574 completed (loss: 0.14295239746570587, acc: 0.9638554453849792)
[2025-01-06 01:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27][root][INFO] - Training Epoch: 9/10, step 343/574 completed (loss: 0.09119585901498795, acc: 0.9433962106704712)
[2025-01-06 01:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27][root][INFO] - Training Epoch: 9/10, step 344/574 completed (loss: 0.02001519687473774, acc: 1.0)
[2025-01-06 01:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27][root][INFO] - Training Epoch: 9/10, step 345/574 completed (loss: 0.0019303846638649702, acc: 1.0)
[2025-01-06 01:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28][root][INFO] - Training Epoch: 9/10, step 346/574 completed (loss: 0.030088087543845177, acc: 0.9850746393203735)
[2025-01-06 01:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28][root][INFO] - Training Epoch: 9/10, step 347/574 completed (loss: 0.0003096677246503532, acc: 1.0)
[2025-01-06 01:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28][root][INFO] - Training Epoch: 9/10, step 348/574 completed (loss: 0.003622096963226795, acc: 1.0)
[2025-01-06 01:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29][root][INFO] - Training Epoch: 9/10, step 349/574 completed (loss: 0.17726004123687744, acc: 0.9166666865348816)
[2025-01-06 01:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29][root][INFO] - Training Epoch: 9/10, step 350/574 completed (loss: 0.48583173751831055, acc: 0.9069767594337463)
[2025-01-06 01:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29][root][INFO] - Training Epoch: 9/10, step 351/574 completed (loss: 0.007488385774195194, acc: 1.0)
[2025-01-06 01:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30][root][INFO] - Training Epoch: 9/10, step 352/574 completed (loss: 0.07972703129053116, acc: 0.9555555582046509)
[2025-01-06 01:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30][root][INFO] - Training Epoch: 9/10, step 353/574 completed (loss: 0.015060567297041416, acc: 1.0)
[2025-01-06 01:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31][root][INFO] - Training Epoch: 9/10, step 354/574 completed (loss: 0.0072332420386374, acc: 1.0)
[2025-01-06 01:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31][root][INFO] - Training Epoch: 9/10, step 355/574 completed (loss: 0.13288284838199615, acc: 0.9450549483299255)
[2025-01-06 01:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31][root][INFO] - Training Epoch: 9/10, step 356/574 completed (loss: 0.07379597425460815, acc: 0.9739130139350891)
[2025-01-06 01:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32][root][INFO] - Training Epoch: 9/10, step 357/574 completed (loss: 0.06390725076198578, acc: 0.97826087474823)
[2025-01-06 01:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32][root][INFO] - Training Epoch: 9/10, step 358/574 completed (loss: 0.04644423723220825, acc: 0.9795918464660645)
[2025-01-06 01:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32][root][INFO] - Training Epoch: 9/10, step 359/574 completed (loss: 0.002019907347857952, acc: 1.0)
[2025-01-06 01:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:33][root][INFO] - Training Epoch: 9/10, step 360/574 completed (loss: 0.007172172423452139, acc: 1.0)
[2025-01-06 01:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:33][root][INFO] - Training Epoch: 9/10, step 361/574 completed (loss: 0.007049932144582272, acc: 1.0)
[2025-01-06 01:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:33][root][INFO] - Training Epoch: 9/10, step 362/574 completed (loss: 0.022161297500133514, acc: 1.0)
[2025-01-06 01:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34][root][INFO] - Training Epoch: 9/10, step 363/574 completed (loss: 0.052681803703308105, acc: 0.9736841917037964)
[2025-01-06 01:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34][root][INFO] - Training Epoch: 9/10, step 364/574 completed (loss: 0.03726160526275635, acc: 0.9756097793579102)
[2025-01-06 01:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34][root][INFO] - Training Epoch: 9/10, step 365/574 completed (loss: 0.41651296615600586, acc: 0.939393937587738)
[2025-01-06 01:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35][root][INFO] - Training Epoch: 9/10, step 366/574 completed (loss: 0.00010611756442813203, acc: 1.0)
[2025-01-06 01:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35][root][INFO] - Training Epoch: 9/10, step 367/574 completed (loss: 0.0041528199799358845, acc: 1.0)
[2025-01-06 01:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35][root][INFO] - Training Epoch: 9/10, step 368/574 completed (loss: 0.032702427357435226, acc: 1.0)
[2025-01-06 01:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:36][root][INFO] - Training Epoch: 9/10, step 369/574 completed (loss: 0.008341647684574127, acc: 1.0)
[2025-01-06 01:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:36][root][INFO] - Training Epoch: 9/10, step 370/574 completed (loss: 0.12891097366809845, acc: 0.9575757384300232)
[2025-01-06 01:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:37][root][INFO] - Training Epoch: 9/10, step 371/574 completed (loss: 0.04758840054273605, acc: 0.9811320900917053)
[2025-01-06 01:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:37][root][INFO] - Training Epoch: 9/10, step 372/574 completed (loss: 0.041297025978565216, acc: 0.9888888597488403)
[2025-01-06 01:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38][root][INFO] - Training Epoch: 9/10, step 373/574 completed (loss: 0.009746755473315716, acc: 1.0)
[2025-01-06 01:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38][root][INFO] - Training Epoch: 9/10, step 374/574 completed (loss: 0.009459763765335083, acc: 1.0)
[2025-01-06 01:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38][root][INFO] - Training Epoch: 9/10, step 375/574 completed (loss: 0.00017834268510341644, acc: 1.0)
[2025-01-06 01:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39][root][INFO] - Training Epoch: 9/10, step 376/574 completed (loss: 0.0003672648163046688, acc: 1.0)
[2025-01-06 01:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39][root][INFO] - Training Epoch: 9/10, step 377/574 completed (loss: 0.011914008297026157, acc: 1.0)
[2025-01-06 01:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39][root][INFO] - Training Epoch: 9/10, step 378/574 completed (loss: 0.0019803636241704226, acc: 1.0)
[2025-01-06 01:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:40][root][INFO] - Training Epoch: 9/10, step 379/574 completed (loss: 0.13162513077259064, acc: 0.9520958065986633)
[2025-01-06 01:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:40][root][INFO] - Training Epoch: 9/10, step 380/574 completed (loss: 0.0745573565363884, acc: 0.9774436354637146)
[2025-01-06 01:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42][root][INFO] - Training Epoch: 9/10, step 381/574 completed (loss: 0.16890084743499756, acc: 0.9251337051391602)
[2025-01-06 01:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42][root][INFO] - Training Epoch: 9/10, step 382/574 completed (loss: 0.09595734626054764, acc: 0.9819819927215576)
[2025-01-06 01:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43][root][INFO] - Training Epoch: 9/10, step 383/574 completed (loss: 0.01706506311893463, acc: 1.0)
[2025-01-06 01:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43][root][INFO] - Training Epoch: 9/10, step 384/574 completed (loss: 0.0016062009381130338, acc: 1.0)
[2025-01-06 01:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43][root][INFO] - Training Epoch: 9/10, step 385/574 completed (loss: 0.005911483895033598, acc: 1.0)
[2025-01-06 01:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44][root][INFO] - Training Epoch: 9/10, step 386/574 completed (loss: 0.009740266017615795, acc: 1.0)
[2025-01-06 01:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44][root][INFO] - Training Epoch: 9/10, step 387/574 completed (loss: 0.0022641511168330908, acc: 1.0)
[2025-01-06 01:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44][root][INFO] - Training Epoch: 9/10, step 388/574 completed (loss: 0.00031112474971450865, acc: 1.0)
[2025-01-06 01:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45][root][INFO] - Training Epoch: 9/10, step 389/574 completed (loss: 0.0003006465267390013, acc: 1.0)
[2025-01-06 01:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45][root][INFO] - Training Epoch: 9/10, step 390/574 completed (loss: 0.06704557687044144, acc: 0.9523809552192688)
[2025-01-06 01:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45][root][INFO] - Training Epoch: 9/10, step 391/574 completed (loss: 0.14115045964717865, acc: 0.9629629850387573)
[2025-01-06 01:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:46][root][INFO] - Training Epoch: 9/10, step 392/574 completed (loss: 0.23486587405204773, acc: 0.893203854560852)
[2025-01-06 01:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:46][root][INFO] - Training Epoch: 9/10, step 393/574 completed (loss: 0.41142481565475464, acc: 0.8602941036224365)
[2025-01-06 01:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:47][root][INFO] - Training Epoch: 9/10, step 394/574 completed (loss: 0.199760302901268, acc: 0.9466666579246521)
[2025-01-06 01:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:47][root][INFO] - Training Epoch: 9/10, step 395/574 completed (loss: 0.3295583724975586, acc: 0.9027777910232544)
[2025-01-06 01:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48][root][INFO] - Training Epoch: 9/10, step 396/574 completed (loss: 0.0390082485973835, acc: 0.9767441749572754)
[2025-01-06 01:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48][root][INFO] - Training Epoch: 9/10, step 397/574 completed (loss: 0.007235990837216377, acc: 1.0)
[2025-01-06 01:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48][root][INFO] - Training Epoch: 9/10, step 398/574 completed (loss: 0.04048573598265648, acc: 1.0)
[2025-01-06 01:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:49][root][INFO] - Training Epoch: 9/10, step 399/574 completed (loss: 0.14490878582000732, acc: 0.9599999785423279)
[2025-01-06 01:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:49][root][INFO] - Training Epoch: 9/10, step 400/574 completed (loss: 0.0605289451777935, acc: 0.970588207244873)
[2025-01-06 01:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50][root][INFO] - Training Epoch: 9/10, step 401/574 completed (loss: 0.03690589219331741, acc: 0.9866666793823242)
[2025-01-06 01:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50][root][INFO] - Training Epoch: 9/10, step 402/574 completed (loss: 0.0033190613612532616, acc: 1.0)
[2025-01-06 01:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50][root][INFO] - Training Epoch: 9/10, step 403/574 completed (loss: 0.007591390050947666, acc: 1.0)
[2025-01-06 01:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51][root][INFO] - Training Epoch: 9/10, step 404/574 completed (loss: 0.018361886963248253, acc: 1.0)
[2025-01-06 01:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51][root][INFO] - Training Epoch: 9/10, step 405/574 completed (loss: 0.0019573408644646406, acc: 1.0)
[2025-01-06 01:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51][root][INFO] - Training Epoch: 9/10, step 406/574 completed (loss: 0.0014949525939300656, acc: 1.0)
[2025-01-06 01:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51][root][INFO] - Training Epoch: 9/10, step 407/574 completed (loss: 0.006712730508297682, acc: 1.0)
[2025-01-06 01:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:52][root][INFO] - Training Epoch: 9/10, step 408/574 completed (loss: 0.04193837568163872, acc: 0.9629629850387573)
[2025-01-06 01:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:52][root][INFO] - Training Epoch: 9/10, step 409/574 completed (loss: 0.004213918931782246, acc: 1.0)
[2025-01-06 01:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:52][root][INFO] - Training Epoch: 9/10, step 410/574 completed (loss: 0.004652881994843483, acc: 1.0)
[2025-01-06 01:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53][root][INFO] - Training Epoch: 9/10, step 411/574 completed (loss: 0.00830634031444788, acc: 1.0)
[2025-01-06 01:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53][root][INFO] - Training Epoch: 9/10, step 412/574 completed (loss: 0.06095750629901886, acc: 0.9666666388511658)
[2025-01-06 01:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:23][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5254, device='cuda:0') eval_epoch_loss=tensor(0.9264, device='cuda:0') eval_epoch_acc=tensor(0.8239, device='cuda:0')
[2025-01-06 01:50:23][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:50:23][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:50:23][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_413_loss_0.9263991713523865/model.pt
[2025-01-06 01:50:23][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:23][root][INFO] - Training Epoch: 9/10, step 413/574 completed (loss: 0.0022779677528887987, acc: 1.0)
[2025-01-06 01:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24][root][INFO] - Training Epoch: 9/10, step 414/574 completed (loss: 0.006961076054722071, acc: 1.0)
[2025-01-06 01:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24][root][INFO] - Training Epoch: 9/10, step 415/574 completed (loss: 0.09160646796226501, acc: 0.9607843160629272)
[2025-01-06 01:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24][root][INFO] - Training Epoch: 9/10, step 416/574 completed (loss: 0.16216722130775452, acc: 0.9230769276618958)
[2025-01-06 01:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25][root][INFO] - Training Epoch: 9/10, step 417/574 completed (loss: 0.028828339651226997, acc: 1.0)
[2025-01-06 01:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25][root][INFO] - Training Epoch: 9/10, step 418/574 completed (loss: 0.028244774788618088, acc: 0.9750000238418579)
[2025-01-06 01:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25][root][INFO] - Training Epoch: 9/10, step 419/574 completed (loss: 0.007939593866467476, acc: 1.0)
[2025-01-06 01:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26][root][INFO] - Training Epoch: 9/10, step 420/574 completed (loss: 0.0009848015615716577, acc: 1.0)
[2025-01-06 01:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26][root][INFO] - Training Epoch: 9/10, step 421/574 completed (loss: 0.11926867812871933, acc: 0.9666666388511658)
[2025-01-06 01:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26][root][INFO] - Training Epoch: 9/10, step 422/574 completed (loss: 0.47740408778190613, acc: 0.96875)
[2025-01-06 01:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27][root][INFO] - Training Epoch: 9/10, step 423/574 completed (loss: 0.045279283076524734, acc: 0.9722222089767456)
[2025-01-06 01:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27][root][INFO] - Training Epoch: 9/10, step 424/574 completed (loss: 0.002748813247308135, acc: 1.0)
[2025-01-06 01:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27][root][INFO] - Training Epoch: 9/10, step 425/574 completed (loss: 0.011101865209639072, acc: 1.0)
[2025-01-06 01:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28][root][INFO] - Training Epoch: 9/10, step 426/574 completed (loss: 0.0007808012305758893, acc: 1.0)
[2025-01-06 01:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28][root][INFO] - Training Epoch: 9/10, step 427/574 completed (loss: 0.008383318781852722, acc: 1.0)
[2025-01-06 01:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28][root][INFO] - Training Epoch: 9/10, step 428/574 completed (loss: 0.02958701364696026, acc: 0.9629629850387573)
[2025-01-06 01:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29][root][INFO] - Training Epoch: 9/10, step 429/574 completed (loss: 0.05428589880466461, acc: 0.95652174949646)
[2025-01-06 01:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29][root][INFO] - Training Epoch: 9/10, step 430/574 completed (loss: 0.002190006896853447, acc: 1.0)
[2025-01-06 01:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29][root][INFO] - Training Epoch: 9/10, step 431/574 completed (loss: 0.0010860210750252008, acc: 1.0)
[2025-01-06 01:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30][root][INFO] - Training Epoch: 9/10, step 432/574 completed (loss: 0.0030077307019382715, acc: 1.0)
[2025-01-06 01:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30][root][INFO] - Training Epoch: 9/10, step 433/574 completed (loss: 0.027545427903532982, acc: 1.0)
[2025-01-06 01:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30][root][INFO] - Training Epoch: 9/10, step 434/574 completed (loss: 0.0015246898401528597, acc: 1.0)
[2025-01-06 01:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31][root][INFO] - Training Epoch: 9/10, step 435/574 completed (loss: 0.0037833205424249172, acc: 1.0)
[2025-01-06 01:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31][root][INFO] - Training Epoch: 9/10, step 436/574 completed (loss: 0.04871132969856262, acc: 0.9722222089767456)
[2025-01-06 01:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31][root][INFO] - Training Epoch: 9/10, step 437/574 completed (loss: 0.04770375415682793, acc: 0.9772727489471436)
[2025-01-06 01:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32][root][INFO] - Training Epoch: 9/10, step 438/574 completed (loss: 0.0003087612276431173, acc: 1.0)
[2025-01-06 01:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32][root][INFO] - Training Epoch: 9/10, step 439/574 completed (loss: 0.06352224200963974, acc: 0.9743589758872986)
[2025-01-06 01:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33][root][INFO] - Training Epoch: 9/10, step 440/574 completed (loss: 0.0975685715675354, acc: 0.9696969985961914)
[2025-01-06 01:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33][root][INFO] - Training Epoch: 9/10, step 441/574 completed (loss: 0.20863068103790283, acc: 0.9359999895095825)
[2025-01-06 01:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34][root][INFO] - Training Epoch: 9/10, step 442/574 completed (loss: 0.11566136032342911, acc: 0.9274193644523621)
[2025-01-06 01:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34][root][INFO] - Training Epoch: 9/10, step 443/574 completed (loss: 0.16983933746814728, acc: 0.9402984976768494)
[2025-01-06 01:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35][root][INFO] - Training Epoch: 9/10, step 444/574 completed (loss: 0.031821850687265396, acc: 0.9811320900917053)
[2025-01-06 01:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35][root][INFO] - Training Epoch: 9/10, step 445/574 completed (loss: 0.011796051636338234, acc: 1.0)
[2025-01-06 01:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36][root][INFO] - Training Epoch: 9/10, step 446/574 completed (loss: 0.003954008687287569, acc: 1.0)
[2025-01-06 01:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36][root][INFO] - Training Epoch: 9/10, step 447/574 completed (loss: 0.0071325963363051414, acc: 1.0)
[2025-01-06 01:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36][root][INFO] - Training Epoch: 9/10, step 448/574 completed (loss: 0.0021663832012563944, acc: 1.0)
[2025-01-06 01:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37][root][INFO] - Training Epoch: 9/10, step 449/574 completed (loss: 0.04763694480061531, acc: 0.9850746393203735)
[2025-01-06 01:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37][root][INFO] - Training Epoch: 9/10, step 450/574 completed (loss: 0.021924959495663643, acc: 0.9861111044883728)
[2025-01-06 01:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37][root][INFO] - Training Epoch: 9/10, step 451/574 completed (loss: 0.026148172095417976, acc: 0.989130437374115)
[2025-01-06 01:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38][root][INFO] - Training Epoch: 9/10, step 452/574 completed (loss: 0.023901253938674927, acc: 0.9871794581413269)
[2025-01-06 01:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38][root][INFO] - Training Epoch: 9/10, step 453/574 completed (loss: 0.0641958937048912, acc: 0.9736841917037964)
[2025-01-06 01:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38][root][INFO] - Training Epoch: 9/10, step 454/574 completed (loss: 0.7900601029396057, acc: 0.9387755393981934)
[2025-01-06 01:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39][root][INFO] - Training Epoch: 9/10, step 455/574 completed (loss: 0.0065656546503305435, acc: 1.0)
[2025-01-06 01:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39][root][INFO] - Training Epoch: 9/10, step 456/574 completed (loss: 0.0834830105304718, acc: 0.9793814420700073)
[2025-01-06 01:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39][root][INFO] - Training Epoch: 9/10, step 457/574 completed (loss: 0.005965917371213436, acc: 1.0)
[2025-01-06 01:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40][root][INFO] - Training Epoch: 9/10, step 458/574 completed (loss: 0.07007963955402374, acc: 0.9767441749572754)
[2025-01-06 01:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40][root][INFO] - Training Epoch: 9/10, step 459/574 completed (loss: 0.008266710676252842, acc: 1.0)
[2025-01-06 01:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41][root][INFO] - Training Epoch: 9/10, step 460/574 completed (loss: 0.02637396939098835, acc: 1.0)
[2025-01-06 01:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41][root][INFO] - Training Epoch: 9/10, step 461/574 completed (loss: 0.10779157280921936, acc: 0.9722222089767456)
[2025-01-06 01:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41][root][INFO] - Training Epoch: 9/10, step 462/574 completed (loss: 0.021210718899965286, acc: 1.0)
[2025-01-06 01:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42][root][INFO] - Training Epoch: 9/10, step 463/574 completed (loss: 0.008815483190119267, acc: 1.0)
[2025-01-06 01:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42][root][INFO] - Training Epoch: 9/10, step 464/574 completed (loss: 0.036324676126241684, acc: 0.97826087474823)
[2025-01-06 01:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42][root][INFO] - Training Epoch: 9/10, step 465/574 completed (loss: 0.14960525929927826, acc: 0.9523809552192688)
[2025-01-06 01:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43][root][INFO] - Training Epoch: 9/10, step 466/574 completed (loss: 0.09715685993432999, acc: 0.9759036302566528)
[2025-01-06 01:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43][root][INFO] - Training Epoch: 9/10, step 467/574 completed (loss: 0.02505825087428093, acc: 0.9909909963607788)
[2025-01-06 01:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44][root][INFO] - Training Epoch: 9/10, step 468/574 completed (loss: 0.06136972829699516, acc: 0.9805825352668762)
[2025-01-06 01:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44][root][INFO] - Training Epoch: 9/10, step 469/574 completed (loss: 0.13473092019557953, acc: 0.9593495726585388)
[2025-01-06 01:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44][root][INFO] - Training Epoch: 9/10, step 470/574 completed (loss: 0.007322714198380709, acc: 1.0)
[2025-01-06 01:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45][root][INFO] - Training Epoch: 9/10, step 471/574 completed (loss: 0.10049986094236374, acc: 0.9642857313156128)
[2025-01-06 01:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45][root][INFO] - Training Epoch: 9/10, step 472/574 completed (loss: 0.3353343904018402, acc: 0.9117646813392639)
[2025-01-06 01:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45][root][INFO] - Training Epoch: 9/10, step 473/574 completed (loss: 0.2623966634273529, acc: 0.9082969427108765)
[2025-01-06 01:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46][root][INFO] - Training Epoch: 9/10, step 474/574 completed (loss: 0.055600568652153015, acc: 0.9895833134651184)
[2025-01-06 01:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46][root][INFO] - Training Epoch: 9/10, step 475/574 completed (loss: 0.0752723291516304, acc: 0.9754601120948792)
[2025-01-06 01:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47][root][INFO] - Training Epoch: 9/10, step 476/574 completed (loss: 0.05152662470936775, acc: 0.971222996711731)
[2025-01-06 01:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47][root][INFO] - Training Epoch: 9/10, step 477/574 completed (loss: 0.20346127450466156, acc: 0.9396985173225403)
[2025-01-06 01:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47][root][INFO] - Training Epoch: 9/10, step 478/574 completed (loss: 0.028422756120562553, acc: 1.0)
[2025-01-06 01:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48][root][INFO] - Training Epoch: 9/10, step 479/574 completed (loss: 0.08974802494049072, acc: 0.9696969985961914)
[2025-01-06 01:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48][root][INFO] - Training Epoch: 9/10, step 480/574 completed (loss: 0.007896014489233494, acc: 1.0)
[2025-01-06 01:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48][root][INFO] - Training Epoch: 9/10, step 481/574 completed (loss: 0.07775597274303436, acc: 0.949999988079071)
[2025-01-06 01:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49][root][INFO] - Training Epoch: 9/10, step 482/574 completed (loss: 0.1270350068807602, acc: 0.949999988079071)
[2025-01-06 01:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49][root][INFO] - Training Epoch: 9/10, step 483/574 completed (loss: 0.44434377551078796, acc: 0.8965517282485962)
[2025-01-06 01:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50][root][INFO] - Training Epoch: 9/10, step 484/574 completed (loss: 0.006906169932335615, acc: 1.0)
[2025-01-06 01:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50][root][INFO] - Training Epoch: 9/10, step 485/574 completed (loss: 0.0033248686231672764, acc: 1.0)
[2025-01-06 01:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50][root][INFO] - Training Epoch: 9/10, step 486/574 completed (loss: 0.08616122603416443, acc: 0.9629629850387573)
[2025-01-06 01:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51][root][INFO] - Training Epoch: 9/10, step 487/574 completed (loss: 0.051786672323942184, acc: 1.0)
[2025-01-06 01:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51][root][INFO] - Training Epoch: 9/10, step 488/574 completed (loss: 0.027958063408732414, acc: 1.0)
[2025-01-06 01:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51][root][INFO] - Training Epoch: 9/10, step 489/574 completed (loss: 0.18380078673362732, acc: 0.9230769276618958)
[2025-01-06 01:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52][root][INFO] - Training Epoch: 9/10, step 490/574 completed (loss: 0.010791878215968609, acc: 1.0)
[2025-01-06 01:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52][root][INFO] - Training Epoch: 9/10, step 491/574 completed (loss: 0.19138798117637634, acc: 0.931034505367279)
[2025-01-06 01:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52][root][INFO] - Training Epoch: 9/10, step 492/574 completed (loss: 0.052229490131139755, acc: 0.9803921580314636)
[2025-01-06 01:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53][root][INFO] - Training Epoch: 9/10, step 493/574 completed (loss: 0.04936307296156883, acc: 0.9655172228813171)
[2025-01-06 01:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53][root][INFO] - Training Epoch: 9/10, step 494/574 completed (loss: 0.04939533397555351, acc: 1.0)
[2025-01-06 01:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53][root][INFO] - Training Epoch: 9/10, step 495/574 completed (loss: 0.0024947163183242083, acc: 1.0)
[2025-01-06 01:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54][root][INFO] - Training Epoch: 9/10, step 496/574 completed (loss: 0.1629745215177536, acc: 0.9375)
[2025-01-06 01:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54][root][INFO] - Training Epoch: 9/10, step 497/574 completed (loss: 0.09570563584566116, acc: 0.9550561904907227)
[2025-01-06 01:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54][root][INFO] - Training Epoch: 9/10, step 498/574 completed (loss: 0.20530788600444794, acc: 0.9101123809814453)
[2025-01-06 01:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55][root][INFO] - Training Epoch: 9/10, step 499/574 completed (loss: 0.23510222136974335, acc: 0.9219858050346375)
[2025-01-06 01:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55][root][INFO] - Training Epoch: 9/10, step 500/574 completed (loss: 0.16433799266815186, acc: 0.9347826242446899)
[2025-01-06 01:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55][root][INFO] - Training Epoch: 9/10, step 501/574 completed (loss: 0.0022139602806419134, acc: 1.0)
[2025-01-06 01:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56][root][INFO] - Training Epoch: 9/10, step 502/574 completed (loss: 0.0006281250389292836, acc: 1.0)
[2025-01-06 01:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56][root][INFO] - Training Epoch: 9/10, step 503/574 completed (loss: 0.0021063191816210747, acc: 1.0)
[2025-01-06 01:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56][root][INFO] - Training Epoch: 9/10, step 504/574 completed (loss: 0.015586423687636852, acc: 1.0)
[2025-01-06 01:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57][root][INFO] - Training Epoch: 9/10, step 505/574 completed (loss: 0.12184742093086243, acc: 0.9433962106704712)
[2025-01-06 01:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57][root][INFO] - Training Epoch: 9/10, step 506/574 completed (loss: 0.01988268457353115, acc: 1.0)
[2025-01-06 01:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58][root][INFO] - Training Epoch: 9/10, step 507/574 completed (loss: 0.3294852375984192, acc: 0.8918918967247009)
[2025-01-06 01:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58][root][INFO] - Training Epoch: 9/10, step 508/574 completed (loss: 0.29141414165496826, acc: 0.8873239159584045)
[2025-01-06 01:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58][root][INFO] - Training Epoch: 9/10, step 509/574 completed (loss: 0.2239793837070465, acc: 0.8999999761581421)
[2025-01-06 01:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:59][root][INFO] - Training Epoch: 9/10, step 510/574 completed (loss: 0.05881012603640556, acc: 0.9666666388511658)
[2025-01-06 01:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:59][root][INFO] - Training Epoch: 9/10, step 511/574 completed (loss: 0.0035750193055719137, acc: 1.0)
[2025-01-06 01:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:02][root][INFO] - Training Epoch: 9/10, step 512/574 completed (loss: 0.3144038915634155, acc: 0.9071428775787354)
[2025-01-06 01:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:02][root][INFO] - Training Epoch: 9/10, step 513/574 completed (loss: 0.036923084408044815, acc: 0.9920634627342224)
[2025-01-06 01:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:03][root][INFO] - Training Epoch: 9/10, step 514/574 completed (loss: 0.05551266297698021, acc: 0.9642857313156128)
[2025-01-06 01:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:03][root][INFO] - Training Epoch: 9/10, step 515/574 completed (loss: 0.006471381522715092, acc: 1.0)
[2025-01-06 01:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04][root][INFO] - Training Epoch: 9/10, step 516/574 completed (loss: 0.04279400408267975, acc: 0.9861111044883728)
[2025-01-06 01:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04][root][INFO] - Training Epoch: 9/10, step 517/574 completed (loss: 0.0007216455996967852, acc: 1.0)
[2025-01-06 01:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04][root][INFO] - Training Epoch: 9/10, step 518/574 completed (loss: 0.29992184042930603, acc: 0.9677419066429138)
[2025-01-06 01:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05][root][INFO] - Training Epoch: 9/10, step 519/574 completed (loss: 0.04758646711707115, acc: 1.0)
[2025-01-06 01:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05][root][INFO] - Training Epoch: 9/10, step 520/574 completed (loss: 0.15520793199539185, acc: 0.9629629850387573)
[2025-01-06 01:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:06][root][INFO] - Training Epoch: 9/10, step 521/574 completed (loss: 0.22454401850700378, acc: 0.9194915294647217)
[2025-01-06 01:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07][root][INFO] - Training Epoch: 9/10, step 522/574 completed (loss: 0.05352320894598961, acc: 0.9776119589805603)
[2025-01-06 01:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07][root][INFO] - Training Epoch: 9/10, step 523/574 completed (loss: 0.09243744611740112, acc: 0.9489051103591919)
[2025-01-06 01:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08][root][INFO] - Training Epoch: 9/10, step 524/574 completed (loss: 0.24407584965229034, acc: 0.925000011920929)
[2025-01-06 01:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08][root][INFO] - Training Epoch: 9/10, step 525/574 completed (loss: 0.006838236469775438, acc: 1.0)
[2025-01-06 01:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08][root][INFO] - Training Epoch: 9/10, step 526/574 completed (loss: 0.0743292048573494, acc: 0.9807692170143127)
[2025-01-06 01:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09][root][INFO] - Training Epoch: 9/10, step 527/574 completed (loss: 0.038201406598091125, acc: 1.0)
[2025-01-06 01:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09][root][INFO] - Training Epoch: 9/10, step 528/574 completed (loss: 0.28325462341308594, acc: 0.8852459192276001)
[2025-01-06 01:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09][root][INFO] - Training Epoch: 9/10, step 529/574 completed (loss: 0.08494538813829422, acc: 0.9661017060279846)
[2025-01-06 01:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10][root][INFO] - Training Epoch: 9/10, step 530/574 completed (loss: 0.1391192525625229, acc: 0.9767441749572754)
[2025-01-06 01:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10][root][INFO] - Training Epoch: 9/10, step 531/574 completed (loss: 0.03929637372493744, acc: 0.9772727489471436)
[2025-01-06 01:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10][root][INFO] - Training Epoch: 9/10, step 532/574 completed (loss: 0.16368624567985535, acc: 0.9433962106704712)
[2025-01-06 01:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11][root][INFO] - Training Epoch: 9/10, step 533/574 completed (loss: 0.0957125723361969, acc: 0.9545454382896423)
[2025-01-06 01:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11][root][INFO] - Training Epoch: 9/10, step 534/574 completed (loss: 0.11928479373455048, acc: 0.9599999785423279)
[2025-01-06 01:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11][root][INFO] - Training Epoch: 9/10, step 535/574 completed (loss: 0.00963643379509449, acc: 1.0)
[2025-01-06 01:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11][root][INFO] - Training Epoch: 9/10, step 536/574 completed (loss: 0.10987844318151474, acc: 0.9545454382896423)
[2025-01-06 01:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12][root][INFO] - Training Epoch: 9/10, step 537/574 completed (loss: 0.034607212990522385, acc: 1.0)
[2025-01-06 01:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12][root][INFO] - Training Epoch: 9/10, step 538/574 completed (loss: 0.12044724076986313, acc: 0.953125)
[2025-01-06 01:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13][root][INFO] - Training Epoch: 9/10, step 539/574 completed (loss: 0.29247188568115234, acc: 0.96875)
[2025-01-06 01:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13][root][INFO] - Training Epoch: 9/10, step 540/574 completed (loss: 0.012203403748571873, acc: 1.0)
[2025-01-06 01:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13][root][INFO] - Training Epoch: 9/10, step 541/574 completed (loss: 0.01114597823470831, acc: 1.0)
[2025-01-06 01:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14][root][INFO] - Training Epoch: 9/10, step 542/574 completed (loss: 0.0011466349242255092, acc: 1.0)
[2025-01-06 01:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14][root][INFO] - Training Epoch: 9/10, step 543/574 completed (loss: 0.00201849895529449, acc: 1.0)
[2025-01-06 01:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14][root][INFO] - Training Epoch: 9/10, step 544/574 completed (loss: 0.006485192105174065, acc: 1.0)
[2025-01-06 01:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15][root][INFO] - Training Epoch: 9/10, step 545/574 completed (loss: 0.014250234700739384, acc: 1.0)
[2025-01-06 01:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15][root][INFO] - Training Epoch: 9/10, step 546/574 completed (loss: 0.002946473192423582, acc: 1.0)
[2025-01-06 01:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15][root][INFO] - Training Epoch: 9/10, step 547/574 completed (loss: 0.06054137274622917, acc: 0.9736841917037964)
[2025-01-06 01:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:16][root][INFO] - Training Epoch: 9/10, step 548/574 completed (loss: 0.0013463551877066493, acc: 1.0)
[2025-01-06 01:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:16][root][INFO] - Training Epoch: 9/10, step 549/574 completed (loss: 0.02395135723054409, acc: 1.0)
[2025-01-06 01:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:16][root][INFO] - Training Epoch: 9/10, step 550/574 completed (loss: 0.0016810602974146605, acc: 1.0)
[2025-01-06 01:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17][root][INFO] - Training Epoch: 9/10, step 551/574 completed (loss: 0.1773119866847992, acc: 0.9750000238418579)
[2025-01-06 01:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17][root][INFO] - Training Epoch: 9/10, step 552/574 completed (loss: 0.010686351917684078, acc: 1.0)
[2025-01-06 01:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17][root][INFO] - Training Epoch: 9/10, step 553/574 completed (loss: 0.04717741906642914, acc: 0.985401451587677)
[2025-01-06 01:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18][root][INFO] - Training Epoch: 9/10, step 554/574 completed (loss: 0.027950406074523926, acc: 0.9931034445762634)
[2025-01-06 01:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18][root][INFO] - Training Epoch: 9/10, step 555/574 completed (loss: 0.18519286811351776, acc: 0.9571428298950195)
[2025-01-06 01:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:47][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5681, device='cuda:0') eval_epoch_loss=tensor(0.9432, device='cuda:0') eval_epoch_acc=tensor(0.8211, device='cuda:0')
[2025-01-06 01:51:47][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:51:47][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:51:48][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_556_loss_0.9431648850440979/model.pt
[2025-01-06 01:51:48][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:48][root][INFO] - Training Epoch: 9/10, step 556/574 completed (loss: 0.14410777390003204, acc: 0.9735099077224731)
[2025-01-06 01:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49][root][INFO] - Training Epoch: 9/10, step 557/574 completed (loss: 0.02405565045773983, acc: 0.9914529919624329)
[2025-01-06 01:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49][root][INFO] - Training Epoch: 9/10, step 558/574 completed (loss: 0.0008561814320273697, acc: 1.0)
[2025-01-06 01:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49][root][INFO] - Training Epoch: 9/10, step 559/574 completed (loss: 0.013955062255263329, acc: 1.0)
[2025-01-06 01:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50][root][INFO] - Training Epoch: 9/10, step 560/574 completed (loss: 0.0019524513045325875, acc: 1.0)
[2025-01-06 01:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50][root][INFO] - Training Epoch: 9/10, step 561/574 completed (loss: 0.02129381150007248, acc: 1.0)
[2025-01-06 01:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50][root][INFO] - Training Epoch: 9/10, step 562/574 completed (loss: 0.13203229010105133, acc: 0.9777777791023254)
[2025-01-06 01:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51][root][INFO] - Training Epoch: 9/10, step 563/574 completed (loss: 0.07974859327077866, acc: 0.9740259647369385)
[2025-01-06 01:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51][root][INFO] - Training Epoch: 9/10, step 564/574 completed (loss: 0.025776132941246033, acc: 1.0)
[2025-01-06 01:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51][root][INFO] - Training Epoch: 9/10, step 565/574 completed (loss: 0.06329319626092911, acc: 0.982758641242981)
[2025-01-06 01:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52][root][INFO] - Training Epoch: 9/10, step 566/574 completed (loss: 0.11691033095121384, acc: 0.9404761791229248)
[2025-01-06 01:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52][root][INFO] - Training Epoch: 9/10, step 567/574 completed (loss: 0.0022535643074661493, acc: 1.0)
[2025-01-06 01:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52][root][INFO] - Training Epoch: 9/10, step 568/574 completed (loss: 0.05184226483106613, acc: 0.9629629850387573)
[2025-01-06 01:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53][root][INFO] - Training Epoch: 9/10, step 569/574 completed (loss: 0.09701031446456909, acc: 0.9679144620895386)
[2025-01-06 01:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53][root][INFO] - Training Epoch: 9/10, step 570/574 completed (loss: 0.014742698520421982, acc: 0.9838709831237793)
[2025-01-06 01:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53][root][INFO] - Training Epoch: 9/10, step 571/574 completed (loss: 0.017617426812648773, acc: 0.9914529919624329)
[2025-01-06 01:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54][root][INFO] - Training Epoch: 9/10, step 572/574 completed (loss: 0.08994581550359726, acc: 0.9744898080825806)
[2025-01-06 01:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54][root][INFO] - Training Epoch: 9/10, step 573/574 completed (loss: 0.055712878704071045, acc: 0.9811320900917053)
[2025-01-06 01:51:55][slam_llm.utils.train_utils][INFO] - Epoch 9: train_perplexity=1.1013, train_epoch_loss=0.0965, epoch time 340.00066924095154s
[2025-01-06 01:51:55][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:51:55][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2025-01-06 01:51:55][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:51:55][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 25
[2025-01-06 01:51:55][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56][root][INFO] - Training Epoch: 10/10, step 0/574 completed (loss: 0.003817417658865452, acc: 1.0)
[2025-01-06 01:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56][root][INFO] - Training Epoch: 10/10, step 1/574 completed (loss: 0.030667442828416824, acc: 1.0)
[2025-01-06 01:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56][root][INFO] - Training Epoch: 10/10, step 2/574 completed (loss: 0.0446324460208416, acc: 0.9729729890823364)
[2025-01-06 01:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57][root][INFO] - Training Epoch: 10/10, step 3/574 completed (loss: 0.006421281024813652, acc: 1.0)
[2025-01-06 01:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57][root][INFO] - Training Epoch: 10/10, step 4/574 completed (loss: 0.032710276544094086, acc: 1.0)
[2025-01-06 01:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57][root][INFO] - Training Epoch: 10/10, step 5/574 completed (loss: 0.005741732660681009, acc: 1.0)
[2025-01-06 01:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58][root][INFO] - Training Epoch: 10/10, step 6/574 completed (loss: 0.014209314249455929, acc: 1.0)
[2025-01-06 01:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58][root][INFO] - Training Epoch: 10/10, step 7/574 completed (loss: 0.004341354593634605, acc: 1.0)
[2025-01-06 01:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58][root][INFO] - Training Epoch: 10/10, step 8/574 completed (loss: 0.005412768106907606, acc: 1.0)
[2025-01-06 01:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59][root][INFO] - Training Epoch: 10/10, step 9/574 completed (loss: 0.00374178821220994, acc: 1.0)
[2025-01-06 01:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59][root][INFO] - Training Epoch: 10/10, step 10/574 completed (loss: 0.007820110768079758, acc: 1.0)
[2025-01-06 01:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59][root][INFO] - Training Epoch: 10/10, step 11/574 completed (loss: 0.04409017786383629, acc: 0.9743589758872986)
[2025-01-06 01:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00][root][INFO] - Training Epoch: 10/10, step 12/574 completed (loss: 0.01653021015226841, acc: 1.0)
[2025-01-06 01:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00][root][INFO] - Training Epoch: 10/10, step 13/574 completed (loss: 0.04300805181264877, acc: 0.95652174949646)
[2025-01-06 01:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00][root][INFO] - Training Epoch: 10/10, step 14/574 completed (loss: 0.004821576178073883, acc: 1.0)
[2025-01-06 01:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01][root][INFO] - Training Epoch: 10/10, step 15/574 completed (loss: 0.10419268161058426, acc: 0.9795918464660645)
[2025-01-06 01:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01][root][INFO] - Training Epoch: 10/10, step 16/574 completed (loss: 0.016155367717146873, acc: 1.0)
[2025-01-06 01:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02][root][INFO] - Training Epoch: 10/10, step 17/574 completed (loss: 0.005869880318641663, acc: 1.0)
[2025-01-06 01:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02][root][INFO] - Training Epoch: 10/10, step 18/574 completed (loss: 0.04990755394101143, acc: 0.9722222089767456)
[2025-01-06 01:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02][root][INFO] - Training Epoch: 10/10, step 19/574 completed (loss: 0.0216143187135458, acc: 1.0)
[2025-01-06 01:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02][root][INFO] - Training Epoch: 10/10, step 20/574 completed (loss: 0.004785494413226843, acc: 1.0)
[2025-01-06 01:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03][root][INFO] - Training Epoch: 10/10, step 21/574 completed (loss: 0.010196588933467865, acc: 1.0)
[2025-01-06 01:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03][root][INFO] - Training Epoch: 10/10, step 22/574 completed (loss: 0.01785830222070217, acc: 1.0)
[2025-01-06 01:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04][root][INFO] - Training Epoch: 10/10, step 23/574 completed (loss: 0.04640844464302063, acc: 0.9523809552192688)
[2025-01-06 01:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04][root][INFO] - Training Epoch: 10/10, step 24/574 completed (loss: 0.07731453329324722, acc: 0.9375)
[2025-01-06 01:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04][root][INFO] - Training Epoch: 10/10, step 25/574 completed (loss: 0.3574341833591461, acc: 0.9433962106704712)
[2025-01-06 01:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:05][root][INFO] - Training Epoch: 10/10, step 26/574 completed (loss: 0.03253665566444397, acc: 1.0)
[2025-01-06 01:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06][root][INFO] - Training Epoch: 10/10, step 27/574 completed (loss: 0.3113143742084503, acc: 0.9090909361839294)
[2025-01-06 01:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06][root][INFO] - Training Epoch: 10/10, step 28/574 completed (loss: 0.004146228544414043, acc: 1.0)
[2025-01-06 01:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07][root][INFO] - Training Epoch: 10/10, step 29/574 completed (loss: 0.10839460790157318, acc: 0.9518072009086609)
[2025-01-06 01:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07][root][INFO] - Training Epoch: 10/10, step 30/574 completed (loss: 0.036989737302064896, acc: 1.0)
[2025-01-06 01:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07][root][INFO] - Training Epoch: 10/10, step 31/574 completed (loss: 0.001309012295678258, acc: 1.0)
[2025-01-06 01:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08][root][INFO] - Training Epoch: 10/10, step 32/574 completed (loss: 0.014692266471683979, acc: 1.0)
[2025-01-06 01:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08][root][INFO] - Training Epoch: 10/10, step 33/574 completed (loss: 0.0024062504526227713, acc: 1.0)
[2025-01-06 01:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08][root][INFO] - Training Epoch: 10/10, step 34/574 completed (loss: 0.07388797402381897, acc: 0.9747899174690247)
[2025-01-06 01:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09][root][INFO] - Training Epoch: 10/10, step 35/574 completed (loss: 0.028985487297177315, acc: 0.9836065769195557)
[2025-01-06 01:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09][root][INFO] - Training Epoch: 10/10, step 36/574 completed (loss: 0.0648711547255516, acc: 0.9523809552192688)
[2025-01-06 01:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09][root][INFO] - Training Epoch: 10/10, step 37/574 completed (loss: 0.05151297152042389, acc: 0.9830508232116699)
[2025-01-06 01:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10][root][INFO] - Training Epoch: 10/10, step 38/574 completed (loss: 0.10531816631555557, acc: 0.954023003578186)
[2025-01-06 01:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10][root][INFO] - Training Epoch: 10/10, step 39/574 completed (loss: 0.04063151404261589, acc: 1.0)
[2025-01-06 01:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10][root][INFO] - Training Epoch: 10/10, step 40/574 completed (loss: 0.06320558488368988, acc: 0.9615384340286255)
[2025-01-06 01:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11][root][INFO] - Training Epoch: 10/10, step 41/574 completed (loss: 0.2325502336025238, acc: 0.9594594836235046)
[2025-01-06 01:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11][root][INFO] - Training Epoch: 10/10, step 42/574 completed (loss: 0.06559684872627258, acc: 0.9692307710647583)
[2025-01-06 01:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12][root][INFO] - Training Epoch: 10/10, step 43/574 completed (loss: 0.03452355042099953, acc: 1.0)
[2025-01-06 01:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12][root][INFO] - Training Epoch: 10/10, step 44/574 completed (loss: 0.0429309606552124, acc: 0.9896907210350037)
[2025-01-06 01:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12][root][INFO] - Training Epoch: 10/10, step 45/574 completed (loss: 0.10541146248579025, acc: 0.9632353186607361)
[2025-01-06 01:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13][root][INFO] - Training Epoch: 10/10, step 46/574 completed (loss: 0.06261304020881653, acc: 1.0)
[2025-01-06 01:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13][root][INFO] - Training Epoch: 10/10, step 47/574 completed (loss: 0.0012266597477719188, acc: 1.0)
[2025-01-06 01:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13][root][INFO] - Training Epoch: 10/10, step 48/574 completed (loss: 0.0006920580635778606, acc: 1.0)
[2025-01-06 01:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14][root][INFO] - Training Epoch: 10/10, step 49/574 completed (loss: 0.02172750048339367, acc: 1.0)
[2025-01-06 01:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14][root][INFO] - Training Epoch: 10/10, step 50/574 completed (loss: 0.07623951882123947, acc: 0.9649122953414917)
[2025-01-06 01:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14][root][INFO] - Training Epoch: 10/10, step 51/574 completed (loss: 0.020186929032206535, acc: 1.0)
[2025-01-06 01:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15][root][INFO] - Training Epoch: 10/10, step 52/574 completed (loss: 0.20756591856479645, acc: 0.9436619877815247)
[2025-01-06 01:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15][root][INFO] - Training Epoch: 10/10, step 53/574 completed (loss: 0.35100531578063965, acc: 0.8799999952316284)
[2025-01-06 01:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16][root][INFO] - Training Epoch: 10/10, step 54/574 completed (loss: 0.06509733945131302, acc: 0.9729729890823364)
[2025-01-06 01:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16][root][INFO] - Training Epoch: 10/10, step 55/574 completed (loss: 0.00041347125079482794, acc: 1.0)
[2025-01-06 01:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:19][root][INFO] - Training Epoch: 10/10, step 56/574 completed (loss: 0.4284701943397522, acc: 0.8634812235832214)
[2025-01-06 01:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:20][root][INFO] - Training Epoch: 10/10, step 57/574 completed (loss: 0.7049659490585327, acc: 0.7995642423629761)
[2025-01-06 01:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:21][root][INFO] - Training Epoch: 10/10, step 58/574 completed (loss: 0.26039519906044006, acc: 0.9090909361839294)
[2025-01-06 01:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:21][root][INFO] - Training Epoch: 10/10, step 59/574 completed (loss: 0.07133016735315323, acc: 0.970588207244873)
[2025-01-06 01:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:22][root][INFO] - Training Epoch: 10/10, step 60/574 completed (loss: 0.25783008337020874, acc: 0.9202898740768433)
[2025-01-06 01:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:22][root][INFO] - Training Epoch: 10/10, step 61/574 completed (loss: 0.10825717449188232, acc: 0.9750000238418579)
[2025-01-06 01:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23][root][INFO] - Training Epoch: 10/10, step 62/574 completed (loss: 0.030448345467448235, acc: 1.0)
[2025-01-06 01:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23][root][INFO] - Training Epoch: 10/10, step 63/574 completed (loss: 0.010226748883724213, acc: 1.0)
[2025-01-06 01:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23][root][INFO] - Training Epoch: 10/10, step 64/574 completed (loss: 0.009304523468017578, acc: 1.0)
[2025-01-06 01:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:24][root][INFO] - Training Epoch: 10/10, step 65/574 completed (loss: 0.026504892855882645, acc: 0.9655172228813171)
[2025-01-06 01:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:24][root][INFO] - Training Epoch: 10/10, step 66/574 completed (loss: 0.07975289970636368, acc: 0.9821428656578064)
[2025-01-06 01:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:24][root][INFO] - Training Epoch: 10/10, step 67/574 completed (loss: 0.052774712443351746, acc: 0.9833333492279053)
[2025-01-06 01:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:25][root][INFO] - Training Epoch: 10/10, step 68/574 completed (loss: 0.0017782412469387054, acc: 1.0)
[2025-01-06 01:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:25][root][INFO] - Training Epoch: 10/10, step 69/574 completed (loss: 0.6090773344039917, acc: 0.8888888955116272)
[2025-01-06 01:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:26][root][INFO] - Training Epoch: 10/10, step 70/574 completed (loss: 0.017206665128469467, acc: 1.0)
[2025-01-06 01:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:26][root][INFO] - Training Epoch: 10/10, step 71/574 completed (loss: 0.16334308683872223, acc: 0.9485294222831726)
[2025-01-06 01:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:26][root][INFO] - Training Epoch: 10/10, step 72/574 completed (loss: 0.2119714617729187, acc: 0.920634925365448)
[2025-01-06 01:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27][root][INFO] - Training Epoch: 10/10, step 73/574 completed (loss: 0.4169222414493561, acc: 0.8564102649688721)
[2025-01-06 01:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27][root][INFO] - Training Epoch: 10/10, step 74/574 completed (loss: 0.1573115736246109, acc: 0.9285714030265808)
[2025-01-06 01:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27][root][INFO] - Training Epoch: 10/10, step 75/574 completed (loss: 0.2253594547510147, acc: 0.9179104566574097)
[2025-01-06 01:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28][root][INFO] - Training Epoch: 10/10, step 76/574 completed (loss: 0.5332384705543518, acc: 0.8284671306610107)
[2025-01-06 01:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28][root][INFO] - Training Epoch: 10/10, step 77/574 completed (loss: 0.000696299655828625, acc: 1.0)
[2025-01-06 01:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28][root][INFO] - Training Epoch: 10/10, step 78/574 completed (loss: 0.004382019396871328, acc: 1.0)
[2025-01-06 01:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29][root][INFO] - Training Epoch: 10/10, step 79/574 completed (loss: 0.06517574191093445, acc: 0.9696969985961914)
[2025-01-06 01:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29][root][INFO] - Training Epoch: 10/10, step 80/574 completed (loss: 0.0026802148204296827, acc: 1.0)
[2025-01-06 01:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29][root][INFO] - Training Epoch: 10/10, step 81/574 completed (loss: 0.020856481045484543, acc: 1.0)
[2025-01-06 01:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30][root][INFO] - Training Epoch: 10/10, step 82/574 completed (loss: 0.02115444280207157, acc: 1.0)
[2025-01-06 01:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30][root][INFO] - Training Epoch: 10/10, step 83/574 completed (loss: 0.007653599604964256, acc: 1.0)
[2025-01-06 01:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30][root][INFO] - Training Epoch: 10/10, step 84/574 completed (loss: 0.04119795933365822, acc: 1.0)
[2025-01-06 01:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:31][root][INFO] - Training Epoch: 10/10, step 85/574 completed (loss: 0.06528039276599884, acc: 0.9800000190734863)
[2025-01-06 01:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:31][root][INFO] - Training Epoch: 10/10, step 86/574 completed (loss: 0.10681913048028946, acc: 0.95652174949646)
[2025-01-06 01:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:32][root][INFO] - Training Epoch: 10/10, step 87/574 completed (loss: 0.052402883768081665, acc: 1.0)
[2025-01-06 01:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:32][root][INFO] - Training Epoch: 10/10, step 88/574 completed (loss: 0.095291867852211, acc: 0.9611650705337524)
[2025-01-06 01:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:33][root][INFO] - Training Epoch: 10/10, step 89/574 completed (loss: 0.2710117995738983, acc: 0.9029126167297363)
[2025-01-06 01:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:34][root][INFO] - Training Epoch: 10/10, step 90/574 completed (loss: 0.4038931429386139, acc: 0.8870967626571655)
[2025-01-06 01:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35][root][INFO] - Training Epoch: 10/10, step 91/574 completed (loss: 0.2385319620370865, acc: 0.9267241358757019)
[2025-01-06 01:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35][root][INFO] - Training Epoch: 10/10, step 92/574 completed (loss: 0.1411653310060501, acc: 0.9578947424888611)
[2025-01-06 01:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:36][root][INFO] - Training Epoch: 10/10, step 93/574 completed (loss: 0.2795782685279846, acc: 0.9009901285171509)
[2025-01-06 01:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37][root][INFO] - Training Epoch: 10/10, step 94/574 completed (loss: 0.08702971041202545, acc: 0.9838709831237793)
[2025-01-06 01:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37][root][INFO] - Training Epoch: 10/10, step 95/574 completed (loss: 0.11270437389612198, acc: 0.9710144996643066)
[2025-01-06 01:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38][root][INFO] - Training Epoch: 10/10, step 96/574 completed (loss: 0.20558109879493713, acc: 0.9327731132507324)
[2025-01-06 01:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38][root][INFO] - Training Epoch: 10/10, step 97/574 completed (loss: 0.2731117606163025, acc: 0.9134615659713745)
[2025-01-06 01:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38][root][INFO] - Training Epoch: 10/10, step 98/574 completed (loss: 0.183450385928154, acc: 0.9416058659553528)
[2025-01-06 01:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39][root][INFO] - Training Epoch: 10/10, step 99/574 completed (loss: 0.1987525075674057, acc: 0.9253731369972229)
[2025-01-06 01:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39][root][INFO] - Training Epoch: 10/10, step 100/574 completed (loss: 0.018648816272616386, acc: 1.0)
[2025-01-06 01:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39][root][INFO] - Training Epoch: 10/10, step 101/574 completed (loss: 0.0015536946011707187, acc: 1.0)
[2025-01-06 01:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40][root][INFO] - Training Epoch: 10/10, step 102/574 completed (loss: 0.0014432872412726283, acc: 1.0)
[2025-01-06 01:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40][root][INFO] - Training Epoch: 10/10, step 103/574 completed (loss: 0.047563586384058, acc: 0.9772727489471436)
[2025-01-06 01:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40][root][INFO] - Training Epoch: 10/10, step 104/574 completed (loss: 0.09618422389030457, acc: 0.9655172228813171)
[2025-01-06 01:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41][root][INFO] - Training Epoch: 10/10, step 105/574 completed (loss: 0.018483392894268036, acc: 1.0)
[2025-01-06 01:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41][root][INFO] - Training Epoch: 10/10, step 106/574 completed (loss: 0.0011304154759272933, acc: 1.0)
[2025-01-06 01:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41][root][INFO] - Training Epoch: 10/10, step 107/574 completed (loss: 0.0006513636326417327, acc: 1.0)
[2025-01-06 01:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41][root][INFO] - Training Epoch: 10/10, step 108/574 completed (loss: 0.0002846403804142028, acc: 1.0)
[2025-01-06 01:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42][root][INFO] - Training Epoch: 10/10, step 109/574 completed (loss: 0.003940212074667215, acc: 1.0)
[2025-01-06 01:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42][root][INFO] - Training Epoch: 10/10, step 110/574 completed (loss: 0.02207338437438011, acc: 1.0)
[2025-01-06 01:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42][root][INFO] - Training Epoch: 10/10, step 111/574 completed (loss: 0.21766360104084015, acc: 0.9473684430122375)
[2025-01-06 01:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:43][root][INFO] - Training Epoch: 10/10, step 112/574 completed (loss: 0.10790599882602692, acc: 0.9649122953414917)
[2025-01-06 01:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:43][root][INFO] - Training Epoch: 10/10, step 113/574 completed (loss: 0.11174452304840088, acc: 0.9487179517745972)
[2025-01-06 01:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44][root][INFO] - Training Epoch: 10/10, step 114/574 completed (loss: 0.16531071066856384, acc: 0.9591836929321289)
[2025-01-06 01:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44][root][INFO] - Training Epoch: 10/10, step 115/574 completed (loss: 0.003741685301065445, acc: 1.0)
[2025-01-06 01:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44][root][INFO] - Training Epoch: 10/10, step 116/574 completed (loss: 0.04632866010069847, acc: 0.9841269850730896)
[2025-01-06 01:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44][root][INFO] - Training Epoch: 10/10, step 117/574 completed (loss: 0.05160781368613243, acc: 0.9918699264526367)
[2025-01-06 01:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:45][root][INFO] - Training Epoch: 10/10, step 118/574 completed (loss: 0.04386235773563385, acc: 0.9838709831237793)
[2025-01-06 01:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46][root][INFO] - Training Epoch: 10/10, step 119/574 completed (loss: 0.3468726873397827, acc: 0.9087452292442322)
[2025-01-06 01:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46][root][INFO] - Training Epoch: 10/10, step 120/574 completed (loss: 0.05130112171173096, acc: 0.9733333587646484)
[2025-01-06 01:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46][root][INFO] - Training Epoch: 10/10, step 121/574 completed (loss: 0.0108101861551404, acc: 1.0)
[2025-01-06 01:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47][root][INFO] - Training Epoch: 10/10, step 122/574 completed (loss: 0.0009707671706564724, acc: 1.0)
[2025-01-06 01:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47][root][INFO] - Training Epoch: 10/10, step 123/574 completed (loss: 0.05547238513827324, acc: 1.0)
[2025-01-06 01:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47][root][INFO] - Training Epoch: 10/10, step 124/574 completed (loss: 0.22091437876224518, acc: 0.907975435256958)
[2025-01-06 01:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:17][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3489, device='cuda:0') eval_epoch_loss=tensor(0.8539, device='cuda:0') eval_epoch_acc=tensor(0.8315, device='cuda:0')
[2025-01-06 01:53:17][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:53:17][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:53:17][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_125_loss_0.8539376854896545/model.pt
[2025-01-06 01:53:17][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18][root][INFO] - Training Epoch: 10/10, step 125/574 completed (loss: 0.23181165754795074, acc: 0.9513888955116272)
[2025-01-06 01:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18][root][INFO] - Training Epoch: 10/10, step 126/574 completed (loss: 0.21959072351455688, acc: 0.8999999761581421)
[2025-01-06 01:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18][root][INFO] - Training Epoch: 10/10, step 127/574 completed (loss: 0.15337613224983215, acc: 0.9464285969734192)
[2025-01-06 01:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19][root][INFO] - Training Epoch: 10/10, step 128/574 completed (loss: 0.1616414338350296, acc: 0.9589743614196777)
[2025-01-06 01:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19][root][INFO] - Training Epoch: 10/10, step 129/574 completed (loss: 0.19169548153877258, acc: 0.9485294222831726)
[2025-01-06 01:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20][root][INFO] - Training Epoch: 10/10, step 130/574 completed (loss: 0.006777138449251652, acc: 1.0)
[2025-01-06 01:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20][root][INFO] - Training Epoch: 10/10, step 131/574 completed (loss: 0.015708204358816147, acc: 1.0)
[2025-01-06 01:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20][root][INFO] - Training Epoch: 10/10, step 132/574 completed (loss: 0.09781084209680557, acc: 0.9375)
[2025-01-06 01:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20][root][INFO] - Training Epoch: 10/10, step 133/574 completed (loss: 0.09886274486780167, acc: 0.95652174949646)
[2025-01-06 01:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:21][root][INFO] - Training Epoch: 10/10, step 134/574 completed (loss: 0.0142797427251935, acc: 1.0)
[2025-01-06 01:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:21][root][INFO] - Training Epoch: 10/10, step 135/574 completed (loss: 0.005969928577542305, acc: 1.0)
[2025-01-06 01:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:21][root][INFO] - Training Epoch: 10/10, step 136/574 completed (loss: 0.013569601811468601, acc: 1.0)
[2025-01-06 01:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22][root][INFO] - Training Epoch: 10/10, step 137/574 completed (loss: 0.15081140398979187, acc: 0.9666666388511658)
[2025-01-06 01:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22][root][INFO] - Training Epoch: 10/10, step 138/574 completed (loss: 0.023412736132740974, acc: 1.0)
[2025-01-06 01:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22][root][INFO] - Training Epoch: 10/10, step 139/574 completed (loss: 0.009865380823612213, acc: 1.0)
[2025-01-06 01:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23][root][INFO] - Training Epoch: 10/10, step 140/574 completed (loss: 0.01695379614830017, acc: 1.0)
[2025-01-06 01:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23][root][INFO] - Training Epoch: 10/10, step 141/574 completed (loss: 0.07725439965724945, acc: 0.9677419066429138)
[2025-01-06 01:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23][root][INFO] - Training Epoch: 10/10, step 142/574 completed (loss: 0.1502417027950287, acc: 0.9729729890823364)
[2025-01-06 01:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24][root][INFO] - Training Epoch: 10/10, step 143/574 completed (loss: 0.18142522871494293, acc: 0.9473684430122375)
[2025-01-06 01:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24][root][INFO] - Training Epoch: 10/10, step 144/574 completed (loss: 0.15436863899230957, acc: 0.9552238583564758)
[2025-01-06 01:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24][root][INFO] - Training Epoch: 10/10, step 145/574 completed (loss: 0.13552075624465942, acc: 0.9693877696990967)
[2025-01-06 01:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25][root][INFO] - Training Epoch: 10/10, step 146/574 completed (loss: 0.11777279525995255, acc: 0.957446813583374)
[2025-01-06 01:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25][root][INFO] - Training Epoch: 10/10, step 147/574 completed (loss: 0.4412004053592682, acc: 0.9285714030265808)
[2025-01-06 01:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25][root][INFO] - Training Epoch: 10/10, step 148/574 completed (loss: 0.022220997139811516, acc: 1.0)
[2025-01-06 01:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26][root][INFO] - Training Epoch: 10/10, step 149/574 completed (loss: 0.004843032453209162, acc: 1.0)
[2025-01-06 01:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26][root][INFO] - Training Epoch: 10/10, step 150/574 completed (loss: 0.004113806411623955, acc: 1.0)
[2025-01-06 01:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26][root][INFO] - Training Epoch: 10/10, step 151/574 completed (loss: 0.15280620753765106, acc: 0.97826087474823)
[2025-01-06 01:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27][root][INFO] - Training Epoch: 10/10, step 152/574 completed (loss: 0.07895714044570923, acc: 0.9661017060279846)
[2025-01-06 01:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27][root][INFO] - Training Epoch: 10/10, step 153/574 completed (loss: 0.05968476086854935, acc: 0.9649122953414917)
[2025-01-06 01:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27][root][INFO] - Training Epoch: 10/10, step 154/574 completed (loss: 0.10597417503595352, acc: 0.9729729890823364)
[2025-01-06 01:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28][root][INFO] - Training Epoch: 10/10, step 155/574 completed (loss: 0.010820678435266018, acc: 1.0)
[2025-01-06 01:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28][root][INFO] - Training Epoch: 10/10, step 156/574 completed (loss: 0.0028445085044950247, acc: 1.0)
[2025-01-06 01:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28][root][INFO] - Training Epoch: 10/10, step 157/574 completed (loss: 0.264160692691803, acc: 0.8947368264198303)
[2025-01-06 01:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:30][root][INFO] - Training Epoch: 10/10, step 158/574 completed (loss: 0.3174183964729309, acc: 0.9054054021835327)
[2025-01-06 01:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:30][root][INFO] - Training Epoch: 10/10, step 159/574 completed (loss: 0.22497698664665222, acc: 0.9629629850387573)
[2025-01-06 01:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:31][root][INFO] - Training Epoch: 10/10, step 160/574 completed (loss: 0.23756073415279388, acc: 0.9534883499145508)
[2025-01-06 01:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:31][root][INFO] - Training Epoch: 10/10, step 161/574 completed (loss: 0.1978272795677185, acc: 0.9411764740943909)
[2025-01-06 01:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:32][root][INFO] - Training Epoch: 10/10, step 162/574 completed (loss: 0.19096526503562927, acc: 0.9101123809814453)
[2025-01-06 01:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:32][root][INFO] - Training Epoch: 10/10, step 163/574 completed (loss: 0.03226327523589134, acc: 1.0)
[2025-01-06 01:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33][root][INFO] - Training Epoch: 10/10, step 164/574 completed (loss: 0.010486537590622902, acc: 1.0)
[2025-01-06 01:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33][root][INFO] - Training Epoch: 10/10, step 165/574 completed (loss: 0.1082039475440979, acc: 0.9655172228813171)
[2025-01-06 01:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33][root][INFO] - Training Epoch: 10/10, step 166/574 completed (loss: 0.009363319724798203, acc: 1.0)
[2025-01-06 01:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34][root][INFO] - Training Epoch: 10/10, step 167/574 completed (loss: 0.030724814161658287, acc: 0.9800000190734863)
[2025-01-06 01:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34][root][INFO] - Training Epoch: 10/10, step 168/574 completed (loss: 0.2710230350494385, acc: 0.9166666865348816)
[2025-01-06 01:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34][root][INFO] - Training Epoch: 10/10, step 169/574 completed (loss: 0.14425447583198547, acc: 0.9803921580314636)
[2025-01-06 01:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:35][root][INFO] - Training Epoch: 10/10, step 170/574 completed (loss: 0.16990354657173157, acc: 0.965753436088562)
[2025-01-06 01:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36][root][INFO] - Training Epoch: 10/10, step 171/574 completed (loss: 0.005470253061503172, acc: 1.0)
[2025-01-06 01:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36][root][INFO] - Training Epoch: 10/10, step 172/574 completed (loss: 0.1973331719636917, acc: 0.9629629850387573)
[2025-01-06 01:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36][root][INFO] - Training Epoch: 10/10, step 173/574 completed (loss: 0.014948160387575626, acc: 1.0)
[2025-01-06 01:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37][root][INFO] - Training Epoch: 10/10, step 174/574 completed (loss: 0.24473394453525543, acc: 0.9026548862457275)
[2025-01-06 01:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37][root][INFO] - Training Epoch: 10/10, step 175/574 completed (loss: 0.17281846702098846, acc: 0.9420289993286133)
[2025-01-06 01:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37][root][INFO] - Training Epoch: 10/10, step 176/574 completed (loss: 0.07446009665727615, acc: 0.9772727489471436)
[2025-01-06 01:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:38][root][INFO] - Training Epoch: 10/10, step 177/574 completed (loss: 0.1566278338432312, acc: 0.9618320465087891)
[2025-01-06 01:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39][root][INFO] - Training Epoch: 10/10, step 178/574 completed (loss: 0.246014803647995, acc: 0.9259259104728699)
[2025-01-06 01:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39][root][INFO] - Training Epoch: 10/10, step 179/574 completed (loss: 0.054983995854854584, acc: 0.9836065769195557)
[2025-01-06 01:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40][root][INFO] - Training Epoch: 10/10, step 180/574 completed (loss: 0.09896694868803024, acc: 0.9583333134651184)
[2025-01-06 01:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40][root][INFO] - Training Epoch: 10/10, step 181/574 completed (loss: 0.0005254211137071252, acc: 1.0)
[2025-01-06 01:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40][root][INFO] - Training Epoch: 10/10, step 182/574 completed (loss: 0.0037628503050655127, acc: 1.0)
[2025-01-06 01:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40][root][INFO] - Training Epoch: 10/10, step 183/574 completed (loss: 0.01167871430516243, acc: 1.0)
[2025-01-06 01:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41][root][INFO] - Training Epoch: 10/10, step 184/574 completed (loss: 0.1251111626625061, acc: 0.9607250690460205)
[2025-01-06 01:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41][root][INFO] - Training Epoch: 10/10, step 185/574 completed (loss: 0.10966311395168304, acc: 0.9596541523933411)
[2025-01-06 01:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42][root][INFO] - Training Epoch: 10/10, step 186/574 completed (loss: 0.11737243086099625, acc: 0.9468749761581421)
[2025-01-06 01:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42][root][INFO] - Training Epoch: 10/10, step 187/574 completed (loss: 0.2259722799062729, acc: 0.9268292784690857)
[2025-01-06 01:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:43][root][INFO] - Training Epoch: 10/10, step 188/574 completed (loss: 0.13139501214027405, acc: 0.9537366628646851)
[2025-01-06 01:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:43][root][INFO] - Training Epoch: 10/10, step 189/574 completed (loss: 0.03395995497703552, acc: 1.0)
[2025-01-06 01:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:43][root][INFO] - Training Epoch: 10/10, step 190/574 completed (loss: 0.26043450832366943, acc: 0.930232584476471)
[2025-01-06 01:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:44][root][INFO] - Training Epoch: 10/10, step 191/574 completed (loss: 0.3288562297821045, acc: 0.9126983880996704)
[2025-01-06 01:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:45][root][INFO] - Training Epoch: 10/10, step 192/574 completed (loss: 0.22860205173492432, acc: 0.939393937587738)
[2025-01-06 01:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:46][root][INFO] - Training Epoch: 10/10, step 193/574 completed (loss: 0.09297024458646774, acc: 0.9764705896377563)
[2025-01-06 01:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:47][root][INFO] - Training Epoch: 10/10, step 194/574 completed (loss: 0.26489025354385376, acc: 0.9259259104728699)
[2025-01-06 01:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:48][root][INFO] - Training Epoch: 10/10, step 195/574 completed (loss: 0.08090926706790924, acc: 0.9677419066429138)
[2025-01-06 01:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:48][root][INFO] - Training Epoch: 10/10, step 196/574 completed (loss: 0.0009974415879696608, acc: 1.0)
[2025-01-06 01:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49][root][INFO] - Training Epoch: 10/10, step 197/574 completed (loss: 0.06604646146297455, acc: 0.9750000238418579)
[2025-01-06 01:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49][root][INFO] - Training Epoch: 10/10, step 198/574 completed (loss: 0.08469010144472122, acc: 0.970588207244873)
[2025-01-06 01:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49][root][INFO] - Training Epoch: 10/10, step 199/574 completed (loss: 0.15125153958797455, acc: 0.9485294222831726)
[2025-01-06 01:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49][root][INFO] - Training Epoch: 10/10, step 200/574 completed (loss: 0.1631798893213272, acc: 0.9322034120559692)
[2025-01-06 01:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50][root][INFO] - Training Epoch: 10/10, step 201/574 completed (loss: 0.1527126282453537, acc: 0.9328358173370361)
[2025-01-06 01:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50][root][INFO] - Training Epoch: 10/10, step 202/574 completed (loss: 0.22095970809459686, acc: 0.9223300814628601)
[2025-01-06 01:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50][root][INFO] - Training Epoch: 10/10, step 203/574 completed (loss: 0.0780513733625412, acc: 0.9523809552192688)
[2025-01-06 01:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51][root][INFO] - Training Epoch: 10/10, step 204/574 completed (loss: 0.044441863894462585, acc: 0.9890109896659851)
[2025-01-06 01:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51][root][INFO] - Training Epoch: 10/10, step 205/574 completed (loss: 0.03739798441529274, acc: 0.9910314083099365)
[2025-01-06 01:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51][root][INFO] - Training Epoch: 10/10, step 206/574 completed (loss: 0.1427232325077057, acc: 0.960629940032959)
[2025-01-06 01:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52][root][INFO] - Training Epoch: 10/10, step 207/574 completed (loss: 0.040588997304439545, acc: 0.9870689511299133)
[2025-01-06 01:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52][root][INFO] - Training Epoch: 10/10, step 208/574 completed (loss: 0.1066877543926239, acc: 0.9601449370384216)
[2025-01-06 01:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52][root][INFO] - Training Epoch: 10/10, step 209/574 completed (loss: 0.09720294177532196, acc: 0.9766536951065063)
[2025-01-06 01:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53][root][INFO] - Training Epoch: 10/10, step 210/574 completed (loss: 0.02013726532459259, acc: 1.0)
[2025-01-06 01:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53][root][INFO] - Training Epoch: 10/10, step 211/574 completed (loss: 0.0027512097731232643, acc: 1.0)
[2025-01-06 01:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53][root][INFO] - Training Epoch: 10/10, step 212/574 completed (loss: 0.00254674069583416, acc: 1.0)
[2025-01-06 01:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:54][root][INFO] - Training Epoch: 10/10, step 213/574 completed (loss: 0.012003383599221706, acc: 1.0)
[2025-01-06 01:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:54][root][INFO] - Training Epoch: 10/10, step 214/574 completed (loss: 0.05592238903045654, acc: 0.9769230484962463)
[2025-01-06 01:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55][root][INFO] - Training Epoch: 10/10, step 215/574 completed (loss: 0.0664779469370842, acc: 0.9729729890823364)
[2025-01-06 01:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55][root][INFO] - Training Epoch: 10/10, step 216/574 completed (loss: 0.09044839441776276, acc: 0.9883720874786377)
[2025-01-06 01:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56][root][INFO] - Training Epoch: 10/10, step 217/574 completed (loss: 0.03728214651346207, acc: 0.9819819927215576)
[2025-01-06 01:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56][root][INFO] - Training Epoch: 10/10, step 218/574 completed (loss: 0.02411808632314205, acc: 1.0)
[2025-01-06 01:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56][root][INFO] - Training Epoch: 10/10, step 219/574 completed (loss: 0.003968869801610708, acc: 1.0)
[2025-01-06 01:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57][root][INFO] - Training Epoch: 10/10, step 220/574 completed (loss: 0.018989643082022667, acc: 1.0)
[2025-01-06 01:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57][root][INFO] - Training Epoch: 10/10, step 221/574 completed (loss: 0.00521797826513648, acc: 1.0)
[2025-01-06 01:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57][root][INFO] - Training Epoch: 10/10, step 222/574 completed (loss: 0.03537282347679138, acc: 0.9807692170143127)
[2025-01-06 01:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:58][root][INFO] - Training Epoch: 10/10, step 223/574 completed (loss: 0.11345580220222473, acc: 0.9619565010070801)
[2025-01-06 01:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:59][root][INFO] - Training Epoch: 10/10, step 224/574 completed (loss: 0.14001686871051788, acc: 0.9431818127632141)
[2025-01-06 01:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:59][root][INFO] - Training Epoch: 10/10, step 225/574 completed (loss: 0.0875762403011322, acc: 0.9680851101875305)
[2025-01-06 01:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:59][root][INFO] - Training Epoch: 10/10, step 226/574 completed (loss: 0.08814442902803421, acc: 0.9811320900917053)
[2025-01-06 01:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:00][root][INFO] - Training Epoch: 10/10, step 227/574 completed (loss: 0.03226375952363014, acc: 0.9833333492279053)
[2025-01-06 01:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:00][root][INFO] - Training Epoch: 10/10, step 228/574 completed (loss: 0.010597793385386467, acc: 1.0)
[2025-01-06 01:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:00][root][INFO] - Training Epoch: 10/10, step 229/574 completed (loss: 0.12837310135364532, acc: 0.9333333373069763)
[2025-01-06 01:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:01][root][INFO] - Training Epoch: 10/10, step 230/574 completed (loss: 0.3118368089199066, acc: 0.9157894849777222)
[2025-01-06 01:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:01][root][INFO] - Training Epoch: 10/10, step 231/574 completed (loss: 0.26398032903671265, acc: 0.9111111164093018)
[2025-01-06 01:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:02][root][INFO] - Training Epoch: 10/10, step 232/574 completed (loss: 0.2508622109889984, acc: 0.9111111164093018)
[2025-01-06 01:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:02][root][INFO] - Training Epoch: 10/10, step 233/574 completed (loss: 0.6110925674438477, acc: 0.8211008906364441)
[2025-01-06 01:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:02][root][INFO] - Training Epoch: 10/10, step 234/574 completed (loss: 0.2679115831851959, acc: 0.9153845906257629)
[2025-01-06 01:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03][root][INFO] - Training Epoch: 10/10, step 235/574 completed (loss: 0.0035276212729513645, acc: 1.0)
[2025-01-06 01:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03][root][INFO] - Training Epoch: 10/10, step 236/574 completed (loss: 0.00604877807199955, acc: 1.0)
[2025-01-06 01:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03][root][INFO] - Training Epoch: 10/10, step 237/574 completed (loss: 0.04681262746453285, acc: 1.0)
[2025-01-06 01:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04][root][INFO] - Training Epoch: 10/10, step 238/574 completed (loss: 0.018152417615056038, acc: 1.0)
[2025-01-06 01:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04][root][INFO] - Training Epoch: 10/10, step 239/574 completed (loss: 0.4216828942298889, acc: 0.9142857193946838)
[2025-01-06 01:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04][root][INFO] - Training Epoch: 10/10, step 240/574 completed (loss: 0.06898539513349533, acc: 0.9772727489471436)
[2025-01-06 01:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05][root][INFO] - Training Epoch: 10/10, step 241/574 completed (loss: 0.012701147235929966, acc: 1.0)
[2025-01-06 01:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05][root][INFO] - Training Epoch: 10/10, step 242/574 completed (loss: 0.10067229717969894, acc: 0.9838709831237793)
[2025-01-06 01:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06][root][INFO] - Training Epoch: 10/10, step 243/574 completed (loss: 0.3171849846839905, acc: 0.9318181872367859)
[2025-01-06 01:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06][root][INFO] - Training Epoch: 10/10, step 244/574 completed (loss: 0.00021518695575650781, acc: 1.0)
[2025-01-06 01:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06][root][INFO] - Training Epoch: 10/10, step 245/574 completed (loss: 0.09245322644710541, acc: 0.9615384340286255)
[2025-01-06 01:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07][root][INFO] - Training Epoch: 10/10, step 246/574 completed (loss: 0.021839434280991554, acc: 1.0)
[2025-01-06 01:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07][root][INFO] - Training Epoch: 10/10, step 247/574 completed (loss: 0.001511573907919228, acc: 1.0)
[2025-01-06 01:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07][root][INFO] - Training Epoch: 10/10, step 248/574 completed (loss: 0.03996927663683891, acc: 1.0)
[2025-01-06 01:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08][root][INFO] - Training Epoch: 10/10, step 249/574 completed (loss: 0.0867520421743393, acc: 0.9729729890823364)
[2025-01-06 01:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08][root][INFO] - Training Epoch: 10/10, step 250/574 completed (loss: 0.0031029940582811832, acc: 1.0)
[2025-01-06 01:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08][root][INFO] - Training Epoch: 10/10, step 251/574 completed (loss: 0.05223897472023964, acc: 0.9852941036224365)
[2025-01-06 01:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09][root][INFO] - Training Epoch: 10/10, step 252/574 completed (loss: 0.04289090633392334, acc: 0.9756097793579102)
[2025-01-06 01:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09][root][INFO] - Training Epoch: 10/10, step 253/574 completed (loss: 0.0018644691444933414, acc: 1.0)
[2025-01-06 01:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09][root][INFO] - Training Epoch: 10/10, step 254/574 completed (loss: 0.0002071126364171505, acc: 1.0)
[2025-01-06 01:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10][root][INFO] - Training Epoch: 10/10, step 255/574 completed (loss: 0.0019167247228324413, acc: 1.0)
[2025-01-06 01:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10][root][INFO] - Training Epoch: 10/10, step 256/574 completed (loss: 0.008823375217616558, acc: 1.0)
[2025-01-06 01:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11][root][INFO] - Training Epoch: 10/10, step 257/574 completed (loss: 0.05124467983841896, acc: 0.9714285731315613)
[2025-01-06 01:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11][root][INFO] - Training Epoch: 10/10, step 258/574 completed (loss: 0.0054073818027973175, acc: 1.0)
[2025-01-06 01:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11][root][INFO] - Training Epoch: 10/10, step 259/574 completed (loss: 0.02324472926557064, acc: 1.0)
[2025-01-06 01:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:12][root][INFO] - Training Epoch: 10/10, step 260/574 completed (loss: 0.0590260811150074, acc: 0.9750000238418579)
[2025-01-06 01:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:12][root][INFO] - Training Epoch: 10/10, step 261/574 completed (loss: 0.006166630890220404, acc: 1.0)
[2025-01-06 01:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13][root][INFO] - Training Epoch: 10/10, step 262/574 completed (loss: 0.026631999760866165, acc: 1.0)
[2025-01-06 01:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13][root][INFO] - Training Epoch: 10/10, step 263/574 completed (loss: 0.20297956466674805, acc: 0.9200000166893005)
[2025-01-06 01:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13][root][INFO] - Training Epoch: 10/10, step 264/574 completed (loss: 0.0260633435100317, acc: 1.0)
[2025-01-06 01:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:14][root][INFO] - Training Epoch: 10/10, step 265/574 completed (loss: 0.24728496372699738, acc: 0.9200000166893005)
[2025-01-06 01:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:15][root][INFO] - Training Epoch: 10/10, step 266/574 completed (loss: 0.1668594479560852, acc: 0.932584285736084)
[2025-01-06 01:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:15][root][INFO] - Training Epoch: 10/10, step 267/574 completed (loss: 0.2011512666940689, acc: 0.9459459185600281)
[2025-01-06 01:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:43][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4545, device='cuda:0') eval_epoch_loss=tensor(0.8979, device='cuda:0') eval_epoch_acc=tensor(0.8272, device='cuda:0')
[2025-01-06 01:54:43][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:54:43][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:54:43][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_268_loss_0.8979087471961975/model.pt
[2025-01-06 01:54:43][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:44][root][INFO] - Training Epoch: 10/10, step 268/574 completed (loss: 0.0138937309384346, acc: 1.0)
[2025-01-06 01:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:44][root][INFO] - Training Epoch: 10/10, step 269/574 completed (loss: 0.004369869362562895, acc: 1.0)
[2025-01-06 01:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45][root][INFO] - Training Epoch: 10/10, step 270/574 completed (loss: 0.005492184776812792, acc: 1.0)
[2025-01-06 01:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45][root][INFO] - Training Epoch: 10/10, step 271/574 completed (loss: 0.0014612630475312471, acc: 1.0)
[2025-01-06 01:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45][root][INFO] - Training Epoch: 10/10, step 272/574 completed (loss: 0.001147949369624257, acc: 1.0)
[2025-01-06 01:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46][root][INFO] - Training Epoch: 10/10, step 273/574 completed (loss: 0.25909677147865295, acc: 0.949999988079071)
[2025-01-06 01:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46][root][INFO] - Training Epoch: 10/10, step 274/574 completed (loss: 0.004296892788261175, acc: 1.0)
[2025-01-06 01:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46][root][INFO] - Training Epoch: 10/10, step 275/574 completed (loss: 0.0006052662502042949, acc: 1.0)
[2025-01-06 01:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46][root][INFO] - Training Epoch: 10/10, step 276/574 completed (loss: 0.0008211668464355171, acc: 1.0)
[2025-01-06 01:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47][root][INFO] - Training Epoch: 10/10, step 277/574 completed (loss: 0.003979371394962072, acc: 1.0)
[2025-01-06 01:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47][root][INFO] - Training Epoch: 10/10, step 278/574 completed (loss: 0.0166175439953804, acc: 1.0)
[2025-01-06 01:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48][root][INFO] - Training Epoch: 10/10, step 279/574 completed (loss: 0.008725925348699093, acc: 1.0)
[2025-01-06 01:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48][root][INFO] - Training Epoch: 10/10, step 280/574 completed (loss: 0.004164222162216902, acc: 1.0)
[2025-01-06 01:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48][root][INFO] - Training Epoch: 10/10, step 281/574 completed (loss: 0.10814622044563293, acc: 0.9759036302566528)
[2025-01-06 01:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49][root][INFO] - Training Epoch: 10/10, step 282/574 completed (loss: 0.31132087111473083, acc: 0.9166666865348816)
[2025-01-06 01:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49][root][INFO] - Training Epoch: 10/10, step 283/574 completed (loss: 0.09834109246730804, acc: 0.9736841917037964)
[2025-01-06 01:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49][root][INFO] - Training Epoch: 10/10, step 284/574 completed (loss: 0.020310774445533752, acc: 1.0)
[2025-01-06 01:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50][root][INFO] - Training Epoch: 10/10, step 285/574 completed (loss: 0.015782300382852554, acc: 1.0)
[2025-01-06 01:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50][root][INFO] - Training Epoch: 10/10, step 286/574 completed (loss: 0.06972289830446243, acc: 0.96875)
[2025-01-06 01:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50][root][INFO] - Training Epoch: 10/10, step 287/574 completed (loss: 0.0644063949584961, acc: 0.9760000109672546)
[2025-01-06 01:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51][root][INFO] - Training Epoch: 10/10, step 288/574 completed (loss: 0.40223944187164307, acc: 0.9340659379959106)
[2025-01-06 01:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51][root][INFO] - Training Epoch: 10/10, step 289/574 completed (loss: 0.05083048716187477, acc: 0.9813664555549622)
[2025-01-06 01:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51][root][INFO] - Training Epoch: 10/10, step 290/574 completed (loss: 0.104835644364357, acc: 0.9639175534248352)
[2025-01-06 01:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51][root][INFO] - Training Epoch: 10/10, step 291/574 completed (loss: 0.0007908565457910299, acc: 1.0)
[2025-01-06 01:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52][root][INFO] - Training Epoch: 10/10, step 292/574 completed (loss: 0.01203838735818863, acc: 1.0)
[2025-01-06 01:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52][root][INFO] - Training Epoch: 10/10, step 293/574 completed (loss: 0.022648204118013382, acc: 1.0)
[2025-01-06 01:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53][root][INFO] - Training Epoch: 10/10, step 294/574 completed (loss: 0.3358578383922577, acc: 0.9636363387107849)
[2025-01-06 01:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53][root][INFO] - Training Epoch: 10/10, step 295/574 completed (loss: 0.12656646966934204, acc: 0.9742268323898315)
[2025-01-06 01:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53][root][INFO] - Training Epoch: 10/10, step 296/574 completed (loss: 0.01921319216489792, acc: 1.0)
[2025-01-06 01:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54][root][INFO] - Training Epoch: 10/10, step 297/574 completed (loss: 0.012054918333888054, acc: 1.0)
[2025-01-06 01:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54][root][INFO] - Training Epoch: 10/10, step 298/574 completed (loss: 0.018414286896586418, acc: 1.0)
[2025-01-06 01:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54][root][INFO] - Training Epoch: 10/10, step 299/574 completed (loss: 0.006886411923915148, acc: 1.0)
[2025-01-06 01:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55][root][INFO] - Training Epoch: 10/10, step 300/574 completed (loss: 0.0020124120637774467, acc: 1.0)
[2025-01-06 01:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55][root][INFO] - Training Epoch: 10/10, step 301/574 completed (loss: 0.00538622448220849, acc: 1.0)
[2025-01-06 01:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55][root][INFO] - Training Epoch: 10/10, step 302/574 completed (loss: 0.015115195885300636, acc: 1.0)
[2025-01-06 01:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56][root][INFO] - Training Epoch: 10/10, step 303/574 completed (loss: 0.0008552666404284537, acc: 1.0)
[2025-01-06 01:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56][root][INFO] - Training Epoch: 10/10, step 304/574 completed (loss: 0.0006909049698151648, acc: 1.0)
[2025-01-06 01:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56][root][INFO] - Training Epoch: 10/10, step 305/574 completed (loss: 0.15778948366641998, acc: 0.9672130942344666)
[2025-01-06 01:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57][root][INFO] - Training Epoch: 10/10, step 306/574 completed (loss: 0.025607379153370857, acc: 0.9666666388511658)
[2025-01-06 01:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57][root][INFO] - Training Epoch: 10/10, step 307/574 completed (loss: 0.0004543899849522859, acc: 1.0)
[2025-01-06 01:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57][root][INFO] - Training Epoch: 10/10, step 308/574 completed (loss: 0.07556329667568207, acc: 0.9855072498321533)
[2025-01-06 01:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:58][root][INFO] - Training Epoch: 10/10, step 309/574 completed (loss: 0.003596734721213579, acc: 1.0)
[2025-01-06 01:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:58][root][INFO] - Training Epoch: 10/10, step 310/574 completed (loss: 0.039346739649772644, acc: 0.9879518151283264)
[2025-01-06 01:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:58][root][INFO] - Training Epoch: 10/10, step 311/574 completed (loss: 0.014809470623731613, acc: 1.0)
[2025-01-06 01:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:59][root][INFO] - Training Epoch: 10/10, step 312/574 completed (loss: 0.014972114935517311, acc: 1.0)
[2025-01-06 01:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:59][root][INFO] - Training Epoch: 10/10, step 313/574 completed (loss: 0.006188841070979834, acc: 1.0)
[2025-01-06 01:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00][root][INFO] - Training Epoch: 10/10, step 314/574 completed (loss: 0.000785829673986882, acc: 1.0)
[2025-01-06 01:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00][root][INFO] - Training Epoch: 10/10, step 315/574 completed (loss: 0.012693346478044987, acc: 1.0)
[2025-01-06 01:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00][root][INFO] - Training Epoch: 10/10, step 316/574 completed (loss: 0.06357283890247345, acc: 0.9677419066429138)
[2025-01-06 01:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01][root][INFO] - Training Epoch: 10/10, step 317/574 completed (loss: 0.03103059157729149, acc: 0.9701492786407471)
[2025-01-06 01:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01][root][INFO] - Training Epoch: 10/10, step 318/574 completed (loss: 0.015414949506521225, acc: 0.9903846383094788)
[2025-01-06 01:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01][root][INFO] - Training Epoch: 10/10, step 319/574 completed (loss: 0.030995558947324753, acc: 0.9777777791023254)
[2025-01-06 01:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02][root][INFO] - Training Epoch: 10/10, step 320/574 completed (loss: 0.005656094290316105, acc: 1.0)
[2025-01-06 01:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02][root][INFO] - Training Epoch: 10/10, step 321/574 completed (loss: 0.0004580017412081361, acc: 1.0)
[2025-01-06 01:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02][root][INFO] - Training Epoch: 10/10, step 322/574 completed (loss: 0.03597478196024895, acc: 1.0)
[2025-01-06 01:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:03][root][INFO] - Training Epoch: 10/10, step 323/574 completed (loss: 0.1479877233505249, acc: 0.9714285731315613)
[2025-01-06 01:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:03][root][INFO] - Training Epoch: 10/10, step 324/574 completed (loss: 0.021657973527908325, acc: 1.0)
[2025-01-06 01:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:04][root][INFO] - Training Epoch: 10/10, step 325/574 completed (loss: 0.16170896589756012, acc: 0.9512194991111755)
[2025-01-06 01:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:04][root][INFO] - Training Epoch: 10/10, step 326/574 completed (loss: 0.27642521262168884, acc: 0.9210526347160339)
[2025-01-06 01:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:04][root][INFO] - Training Epoch: 10/10, step 327/574 completed (loss: 0.11615663021802902, acc: 0.9473684430122375)
[2025-01-06 01:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05][root][INFO] - Training Epoch: 10/10, step 328/574 completed (loss: 0.043734222650527954, acc: 1.0)
[2025-01-06 01:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05][root][INFO] - Training Epoch: 10/10, step 329/574 completed (loss: 0.013664423488080502, acc: 1.0)
[2025-01-06 01:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05][root][INFO] - Training Epoch: 10/10, step 330/574 completed (loss: 0.0006986599764786661, acc: 1.0)
[2025-01-06 01:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06][root][INFO] - Training Epoch: 10/10, step 331/574 completed (loss: 0.08819244056940079, acc: 0.9838709831237793)
[2025-01-06 01:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06][root][INFO] - Training Epoch: 10/10, step 332/574 completed (loss: 0.011557579971849918, acc: 1.0)
[2025-01-06 01:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06][root][INFO] - Training Epoch: 10/10, step 333/574 completed (loss: 0.09544223546981812, acc: 0.96875)
[2025-01-06 01:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:07][root][INFO] - Training Epoch: 10/10, step 334/574 completed (loss: 0.003336310852319002, acc: 1.0)
[2025-01-06 01:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:07][root][INFO] - Training Epoch: 10/10, step 335/574 completed (loss: 0.0651099681854248, acc: 0.9473684430122375)
[2025-01-06 01:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:07][root][INFO] - Training Epoch: 10/10, step 336/574 completed (loss: 0.06084883585572243, acc: 0.9800000190734863)
[2025-01-06 01:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:08][root][INFO] - Training Epoch: 10/10, step 337/574 completed (loss: 0.16445417702198029, acc: 0.9425287246704102)
[2025-01-06 01:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:08][root][INFO] - Training Epoch: 10/10, step 338/574 completed (loss: 0.1326507329940796, acc: 0.978723406791687)
[2025-01-06 01:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09][root][INFO] - Training Epoch: 10/10, step 339/574 completed (loss: 0.1657623052597046, acc: 0.9277108311653137)
[2025-01-06 01:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09][root][INFO] - Training Epoch: 10/10, step 340/574 completed (loss: 0.000755057786591351, acc: 1.0)
[2025-01-06 01:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09][root][INFO] - Training Epoch: 10/10, step 341/574 completed (loss: 0.020332328975200653, acc: 1.0)
[2025-01-06 01:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10][root][INFO] - Training Epoch: 10/10, step 342/574 completed (loss: 0.03878398984670639, acc: 0.9879518151283264)
[2025-01-06 01:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10][root][INFO] - Training Epoch: 10/10, step 343/574 completed (loss: 0.04624544084072113, acc: 0.9811320900917053)
[2025-01-06 01:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10][root][INFO] - Training Epoch: 10/10, step 344/574 completed (loss: 0.019220968708395958, acc: 0.9873417615890503)
[2025-01-06 01:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11][root][INFO] - Training Epoch: 10/10, step 345/574 completed (loss: 0.010705864988267422, acc: 1.0)
[2025-01-06 01:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11][root][INFO] - Training Epoch: 10/10, step 346/574 completed (loss: 0.014242012985050678, acc: 1.0)
[2025-01-06 01:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11][root][INFO] - Training Epoch: 10/10, step 347/574 completed (loss: 0.0006427918560802937, acc: 1.0)
[2025-01-06 01:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11][root][INFO] - Training Epoch: 10/10, step 348/574 completed (loss: 0.011037877760827541, acc: 1.0)
[2025-01-06 01:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12][root][INFO] - Training Epoch: 10/10, step 349/574 completed (loss: 0.10335138440132141, acc: 0.9444444179534912)
[2025-01-06 01:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12][root][INFO] - Training Epoch: 10/10, step 350/574 completed (loss: 0.0799228847026825, acc: 0.9534883499145508)
[2025-01-06 01:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12][root][INFO] - Training Epoch: 10/10, step 351/574 completed (loss: 0.06276217848062515, acc: 0.9743589758872986)
[2025-01-06 01:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:13][root][INFO] - Training Epoch: 10/10, step 352/574 completed (loss: 0.04025936871767044, acc: 1.0)
[2025-01-06 01:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:13][root][INFO] - Training Epoch: 10/10, step 353/574 completed (loss: 0.5019205212593079, acc: 0.95652174949646)
[2025-01-06 01:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:13][root][INFO] - Training Epoch: 10/10, step 354/574 completed (loss: 0.030620107427239418, acc: 1.0)
[2025-01-06 01:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:14][root][INFO] - Training Epoch: 10/10, step 355/574 completed (loss: 0.25160813331604004, acc: 0.9340659379959106)
[2025-01-06 01:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:14][root][INFO] - Training Epoch: 10/10, step 356/574 completed (loss: 0.14294148981571198, acc: 0.9739130139350891)
[2025-01-06 01:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15][root][INFO] - Training Epoch: 10/10, step 357/574 completed (loss: 0.11227109283208847, acc: 0.967391312122345)
[2025-01-06 01:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15][root][INFO] - Training Epoch: 10/10, step 358/574 completed (loss: 0.10971644520759583, acc: 0.9387755393981934)
[2025-01-06 01:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15][root][INFO] - Training Epoch: 10/10, step 359/574 completed (loss: 0.36208924651145935, acc: 0.9583333134651184)
[2025-01-06 01:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:16][root][INFO] - Training Epoch: 10/10, step 360/574 completed (loss: 0.007788127288222313, acc: 1.0)
[2025-01-06 01:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:16][root][INFO] - Training Epoch: 10/10, step 361/574 completed (loss: 0.01577867940068245, acc: 1.0)
[2025-01-06 01:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:16][root][INFO] - Training Epoch: 10/10, step 362/574 completed (loss: 0.021898098289966583, acc: 0.9777777791023254)
[2025-01-06 01:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:17][root][INFO] - Training Epoch: 10/10, step 363/574 completed (loss: 0.015677565708756447, acc: 1.0)
[2025-01-06 01:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:17][root][INFO] - Training Epoch: 10/10, step 364/574 completed (loss: 0.005696884356439114, acc: 1.0)
[2025-01-06 01:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:17][root][INFO] - Training Epoch: 10/10, step 365/574 completed (loss: 0.5257164835929871, acc: 0.9090909361839294)
[2025-01-06 01:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18][root][INFO] - Training Epoch: 10/10, step 366/574 completed (loss: 0.00024106947239488363, acc: 1.0)
[2025-01-06 01:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18][root][INFO] - Training Epoch: 10/10, step 367/574 completed (loss: 0.08859841525554657, acc: 0.95652174949646)
[2025-01-06 01:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18][root][INFO] - Training Epoch: 10/10, step 368/574 completed (loss: 0.0006399091216735542, acc: 1.0)
[2025-01-06 01:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19][root][INFO] - Training Epoch: 10/10, step 369/574 completed (loss: 0.03079815022647381, acc: 0.96875)
[2025-01-06 01:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19][root][INFO] - Training Epoch: 10/10, step 370/574 completed (loss: 0.12374231964349747, acc: 0.9575757384300232)
[2025-01-06 01:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:20][root][INFO] - Training Epoch: 10/10, step 371/574 completed (loss: 0.060270484536886215, acc: 0.9905660152435303)
[2025-01-06 01:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21][root][INFO] - Training Epoch: 10/10, step 372/574 completed (loss: 0.021383265033364296, acc: 0.9888888597488403)
[2025-01-06 01:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21][root][INFO] - Training Epoch: 10/10, step 373/574 completed (loss: 0.006984509527683258, acc: 1.0)
[2025-01-06 01:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21][root][INFO] - Training Epoch: 10/10, step 374/574 completed (loss: 0.004067061934620142, acc: 1.0)
[2025-01-06 01:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22][root][INFO] - Training Epoch: 10/10, step 375/574 completed (loss: 0.0035384008660912514, acc: 1.0)
[2025-01-06 01:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22][root][INFO] - Training Epoch: 10/10, step 376/574 completed (loss: 0.016278821974992752, acc: 1.0)
[2025-01-06 01:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23][root][INFO] - Training Epoch: 10/10, step 377/574 completed (loss: 0.0039995573461055756, acc: 1.0)
[2025-01-06 01:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23][root][INFO] - Training Epoch: 10/10, step 378/574 completed (loss: 0.004809204023331404, acc: 1.0)
[2025-01-06 01:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:24][root][INFO] - Training Epoch: 10/10, step 379/574 completed (loss: 0.09237967431545258, acc: 0.976047933101654)
[2025-01-06 01:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:24][root][INFO] - Training Epoch: 10/10, step 380/574 completed (loss: 0.07998200505971909, acc: 0.9849624037742615)
[2025-01-06 01:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:25][root][INFO] - Training Epoch: 10/10, step 381/574 completed (loss: 0.13465042412281036, acc: 0.9411764740943909)
[2025-01-06 01:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26][root][INFO] - Training Epoch: 10/10, step 382/574 completed (loss: 0.025512922555208206, acc: 0.9909909963607788)
[2025-01-06 01:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26][root][INFO] - Training Epoch: 10/10, step 383/574 completed (loss: 0.027012387290596962, acc: 1.0)
[2025-01-06 01:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27][root][INFO] - Training Epoch: 10/10, step 384/574 completed (loss: 0.001211080583743751, acc: 1.0)
[2025-01-06 01:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27][root][INFO] - Training Epoch: 10/10, step 385/574 completed (loss: 0.035005372017621994, acc: 1.0)
[2025-01-06 01:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27][root][INFO] - Training Epoch: 10/10, step 386/574 completed (loss: 0.0010224414290860295, acc: 1.0)
[2025-01-06 01:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28][root][INFO] - Training Epoch: 10/10, step 387/574 completed (loss: 0.017376506701111794, acc: 1.0)
[2025-01-06 01:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28][root][INFO] - Training Epoch: 10/10, step 388/574 completed (loss: 0.0007137765642255545, acc: 1.0)
[2025-01-06 01:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28][root][INFO] - Training Epoch: 10/10, step 389/574 completed (loss: 0.0011289836838841438, acc: 1.0)
[2025-01-06 01:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:29][root][INFO] - Training Epoch: 10/10, step 390/574 completed (loss: 0.05079558491706848, acc: 1.0)
[2025-01-06 01:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:29][root][INFO] - Training Epoch: 10/10, step 391/574 completed (loss: 0.1286470741033554, acc: 0.9629629850387573)
[2025-01-06 01:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30][root][INFO] - Training Epoch: 10/10, step 392/574 completed (loss: 0.21363845467567444, acc: 0.9223300814628601)
[2025-01-06 01:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30][root][INFO] - Training Epoch: 10/10, step 393/574 completed (loss: 0.31845319271087646, acc: 0.904411792755127)
[2025-01-06 01:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31][root][INFO] - Training Epoch: 10/10, step 394/574 completed (loss: 0.2057441920042038, acc: 0.9200000166893005)
[2025-01-06 01:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31][root][INFO] - Training Epoch: 10/10, step 395/574 completed (loss: 0.11852872371673584, acc: 0.9513888955116272)
[2025-01-06 01:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31][root][INFO] - Training Epoch: 10/10, step 396/574 completed (loss: 0.020680774003267288, acc: 1.0)
[2025-01-06 01:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32][root][INFO] - Training Epoch: 10/10, step 397/574 completed (loss: 0.031018784269690514, acc: 1.0)
[2025-01-06 01:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32][root][INFO] - Training Epoch: 10/10, step 398/574 completed (loss: 0.011737529188394547, acc: 1.0)
[2025-01-06 01:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32][root][INFO] - Training Epoch: 10/10, step 399/574 completed (loss: 0.0018222479848191142, acc: 1.0)
[2025-01-06 01:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33][root][INFO] - Training Epoch: 10/10, step 400/574 completed (loss: 0.042258646339178085, acc: 0.9852941036224365)
[2025-01-06 01:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33][root][INFO] - Training Epoch: 10/10, step 401/574 completed (loss: 0.021641064435243607, acc: 1.0)
[2025-01-06 01:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34][root][INFO] - Training Epoch: 10/10, step 402/574 completed (loss: 0.014623326249420643, acc: 1.0)
[2025-01-06 01:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34][root][INFO] - Training Epoch: 10/10, step 403/574 completed (loss: 0.0890992134809494, acc: 0.939393937587738)
[2025-01-06 01:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34][root][INFO] - Training Epoch: 10/10, step 404/574 completed (loss: 0.08726204931735992, acc: 0.9677419066429138)
[2025-01-06 01:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:35][root][INFO] - Training Epoch: 10/10, step 405/574 completed (loss: 0.0016316147521138191, acc: 1.0)
[2025-01-06 01:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:35][root][INFO] - Training Epoch: 10/10, step 406/574 completed (loss: 0.006104819942265749, acc: 1.0)
[2025-01-06 01:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36][root][INFO] - Training Epoch: 10/10, step 407/574 completed (loss: 0.0007221458363346756, acc: 1.0)
[2025-01-06 01:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36][root][INFO] - Training Epoch: 10/10, step 408/574 completed (loss: 0.006749085616320372, acc: 1.0)
[2025-01-06 01:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36][root][INFO] - Training Epoch: 10/10, step 409/574 completed (loss: 0.013005981221795082, acc: 1.0)
[2025-01-06 01:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:37][root][INFO] - Training Epoch: 10/10, step 410/574 completed (loss: 0.05918871611356735, acc: 0.9655172228813171)
[2025-01-06 01:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5081, device='cuda:0') eval_epoch_loss=tensor(0.9195, device='cuda:0') eval_epoch_acc=tensor(0.8136, device='cuda:0')
[2025-01-06 01:56:05][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:56:05][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:56:05][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_411_loss_0.9195123314857483/model.pt
[2025-01-06 01:56:05][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05][root][INFO] - Training Epoch: 10/10, step 411/574 completed (loss: 0.18451867997646332, acc: 0.9642857313156128)
[2025-01-06 01:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:06][root][INFO] - Training Epoch: 10/10, step 412/574 completed (loss: 0.001244276762008667, acc: 1.0)
[2025-01-06 01:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:06][root][INFO] - Training Epoch: 10/10, step 413/574 completed (loss: 0.007978950627148151, acc: 1.0)
[2025-01-06 01:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07][root][INFO] - Training Epoch: 10/10, step 414/574 completed (loss: 0.003507158951833844, acc: 1.0)
[2025-01-06 01:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07][root][INFO] - Training Epoch: 10/10, step 415/574 completed (loss: 0.17161285877227783, acc: 0.9607843160629272)
[2025-01-06 01:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07][root][INFO] - Training Epoch: 10/10, step 416/574 completed (loss: 0.27790161967277527, acc: 0.9230769276618958)
[2025-01-06 01:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08][root][INFO] - Training Epoch: 10/10, step 417/574 completed (loss: 0.1131029948592186, acc: 0.9444444179534912)
[2025-01-06 01:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08][root][INFO] - Training Epoch: 10/10, step 418/574 completed (loss: 0.028782326728105545, acc: 1.0)
[2025-01-06 01:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08][root][INFO] - Training Epoch: 10/10, step 419/574 completed (loss: 0.028659483417868614, acc: 1.0)
[2025-01-06 01:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09][root][INFO] - Training Epoch: 10/10, step 420/574 completed (loss: 0.0015793743077665567, acc: 1.0)
[2025-01-06 01:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09][root][INFO] - Training Epoch: 10/10, step 421/574 completed (loss: 0.0013978219358250499, acc: 1.0)
[2025-01-06 01:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09][root][INFO] - Training Epoch: 10/10, step 422/574 completed (loss: 0.09826447069644928, acc: 0.96875)
[2025-01-06 01:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09][root][INFO] - Training Epoch: 10/10, step 423/574 completed (loss: 0.012519057840108871, acc: 1.0)
[2025-01-06 01:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10][root][INFO] - Training Epoch: 10/10, step 424/574 completed (loss: 0.0038503168616443872, acc: 1.0)
[2025-01-06 01:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10][root][INFO] - Training Epoch: 10/10, step 425/574 completed (loss: 0.018657390028238297, acc: 1.0)
[2025-01-06 01:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10][root][INFO] - Training Epoch: 10/10, step 426/574 completed (loss: 0.001017970615066588, acc: 1.0)
[2025-01-06 01:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11][root][INFO] - Training Epoch: 10/10, step 427/574 completed (loss: 0.007695780601352453, acc: 1.0)
[2025-01-06 01:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11][root][INFO] - Training Epoch: 10/10, step 428/574 completed (loss: 0.005044391844421625, acc: 1.0)
[2025-01-06 01:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11][root][INFO] - Training Epoch: 10/10, step 429/574 completed (loss: 0.0015657087787985802, acc: 1.0)
[2025-01-06 01:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12][root][INFO] - Training Epoch: 10/10, step 430/574 completed (loss: 0.00027974124532192945, acc: 1.0)
[2025-01-06 01:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12][root][INFO] - Training Epoch: 10/10, step 431/574 completed (loss: 0.0075985523872077465, acc: 1.0)
[2025-01-06 01:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13][root][INFO] - Training Epoch: 10/10, step 432/574 completed (loss: 0.00604929169639945, acc: 1.0)
[2025-01-06 01:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13][root][INFO] - Training Epoch: 10/10, step 433/574 completed (loss: 0.03326382488012314, acc: 1.0)
[2025-01-06 01:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13][root][INFO] - Training Epoch: 10/10, step 434/574 completed (loss: 0.010414049960672855, acc: 1.0)
[2025-01-06 01:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14][root][INFO] - Training Epoch: 10/10, step 435/574 completed (loss: 0.2005184441804886, acc: 0.9696969985961914)
[2025-01-06 01:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14][root][INFO] - Training Epoch: 10/10, step 436/574 completed (loss: 0.011847566813230515, acc: 1.0)
[2025-01-06 01:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14][root][INFO] - Training Epoch: 10/10, step 437/574 completed (loss: 0.030076952651143074, acc: 0.9772727489471436)
[2025-01-06 01:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:15][root][INFO] - Training Epoch: 10/10, step 438/574 completed (loss: 0.01086769625544548, acc: 1.0)
[2025-01-06 01:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:15][root][INFO] - Training Epoch: 10/10, step 439/574 completed (loss: 0.05325588211417198, acc: 0.9743589758872986)
[2025-01-06 01:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:15][root][INFO] - Training Epoch: 10/10, step 440/574 completed (loss: 0.1828332096338272, acc: 0.9848484992980957)
[2025-01-06 01:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:16][root][INFO] - Training Epoch: 10/10, step 441/574 completed (loss: 0.25562289357185364, acc: 0.9279999732971191)
[2025-01-06 01:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17][root][INFO] - Training Epoch: 10/10, step 442/574 completed (loss: 0.1419522911310196, acc: 0.9596773982048035)
[2025-01-06 01:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17][root][INFO] - Training Epoch: 10/10, step 443/574 completed (loss: 0.1735547035932541, acc: 0.9353233575820923)
[2025-01-06 01:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18][root][INFO] - Training Epoch: 10/10, step 444/574 completed (loss: 0.03299744799733162, acc: 0.9811320900917053)
[2025-01-06 01:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18][root][INFO] - Training Epoch: 10/10, step 445/574 completed (loss: 0.03302169591188431, acc: 0.9772727489471436)
[2025-01-06 01:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18][root][INFO] - Training Epoch: 10/10, step 446/574 completed (loss: 0.005735873710364103, acc: 1.0)
[2025-01-06 01:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19][root][INFO] - Training Epoch: 10/10, step 447/574 completed (loss: 0.040324218571186066, acc: 1.0)
[2025-01-06 01:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19][root][INFO] - Training Epoch: 10/10, step 448/574 completed (loss: 0.01728280819952488, acc: 1.0)
[2025-01-06 01:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19][root][INFO] - Training Epoch: 10/10, step 449/574 completed (loss: 0.040708400309085846, acc: 0.9850746393203735)
[2025-01-06 01:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20][root][INFO] - Training Epoch: 10/10, step 450/574 completed (loss: 0.0144288819283247, acc: 1.0)
[2025-01-06 01:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20][root][INFO] - Training Epoch: 10/10, step 451/574 completed (loss: 0.01201546285301447, acc: 1.0)
[2025-01-06 01:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20][root][INFO] - Training Epoch: 10/10, step 452/574 completed (loss: 0.019741173833608627, acc: 1.0)
[2025-01-06 01:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21][root][INFO] - Training Epoch: 10/10, step 453/574 completed (loss: 0.12924370169639587, acc: 0.9473684430122375)
[2025-01-06 01:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21][root][INFO] - Training Epoch: 10/10, step 454/574 completed (loss: 0.01945592276751995, acc: 1.0)
[2025-01-06 01:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21][root][INFO] - Training Epoch: 10/10, step 455/574 completed (loss: 0.029885856434702873, acc: 1.0)
[2025-01-06 01:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22][root][INFO] - Training Epoch: 10/10, step 456/574 completed (loss: 0.07815586030483246, acc: 0.9896907210350037)
[2025-01-06 01:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22][root][INFO] - Training Epoch: 10/10, step 457/574 completed (loss: 0.021196147426962852, acc: 0.9857142567634583)
[2025-01-06 01:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22][root][INFO] - Training Epoch: 10/10, step 458/574 completed (loss: 0.11808202415704727, acc: 0.9651162624359131)
[2025-01-06 01:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23][root][INFO] - Training Epoch: 10/10, step 459/574 completed (loss: 0.05621980503201485, acc: 0.9821428656578064)
[2025-01-06 01:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23][root][INFO] - Training Epoch: 10/10, step 460/574 completed (loss: 0.01512508187443018, acc: 1.0)
[2025-01-06 01:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23][root][INFO] - Training Epoch: 10/10, step 461/574 completed (loss: 0.0023687700740993023, acc: 1.0)
[2025-01-06 01:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24][root][INFO] - Training Epoch: 10/10, step 462/574 completed (loss: 0.0047901044599711895, acc: 1.0)
[2025-01-06 01:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24][root][INFO] - Training Epoch: 10/10, step 463/574 completed (loss: 0.005559317767620087, acc: 1.0)
[2025-01-06 01:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24][root][INFO] - Training Epoch: 10/10, step 464/574 completed (loss: 0.019072147086262703, acc: 1.0)
[2025-01-06 01:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25][root][INFO] - Training Epoch: 10/10, step 465/574 completed (loss: 0.032244641333818436, acc: 0.988095223903656)
[2025-01-06 01:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25][root][INFO] - Training Epoch: 10/10, step 466/574 completed (loss: 0.11540864408016205, acc: 0.9638554453849792)
[2025-01-06 01:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25][root][INFO] - Training Epoch: 10/10, step 467/574 completed (loss: 0.011096267960965633, acc: 1.0)
[2025-01-06 01:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26][root][INFO] - Training Epoch: 10/10, step 468/574 completed (loss: 0.09044354408979416, acc: 0.9611650705337524)
[2025-01-06 01:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26][root][INFO] - Training Epoch: 10/10, step 469/574 completed (loss: 0.020242126658558846, acc: 1.0)
[2025-01-06 01:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27][root][INFO] - Training Epoch: 10/10, step 470/574 completed (loss: 0.025998413562774658, acc: 1.0)
[2025-01-06 01:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27][root][INFO] - Training Epoch: 10/10, step 471/574 completed (loss: 0.3300933837890625, acc: 0.9285714030265808)
[2025-01-06 01:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27][root][INFO] - Training Epoch: 10/10, step 472/574 completed (loss: 0.27049848437309265, acc: 0.9215686321258545)
[2025-01-06 01:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28][root][INFO] - Training Epoch: 10/10, step 473/574 completed (loss: 0.25868120789527893, acc: 0.9213973879814148)
[2025-01-06 01:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28][root][INFO] - Training Epoch: 10/10, step 474/574 completed (loss: 0.039398193359375, acc: 0.9895833134651184)
[2025-01-06 01:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28][root][INFO] - Training Epoch: 10/10, step 475/574 completed (loss: 0.09426474571228027, acc: 0.9447852969169617)
[2025-01-06 01:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29][root][INFO] - Training Epoch: 10/10, step 476/574 completed (loss: 0.026705531403422356, acc: 1.0)
[2025-01-06 01:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29][root][INFO] - Training Epoch: 10/10, step 477/574 completed (loss: 0.1347474455833435, acc: 0.9497487545013428)
[2025-01-06 01:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29][root][INFO] - Training Epoch: 10/10, step 478/574 completed (loss: 0.06855777651071548, acc: 0.9444444179534912)
[2025-01-06 01:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30][root][INFO] - Training Epoch: 10/10, step 479/574 completed (loss: 0.024219060316681862, acc: 1.0)
[2025-01-06 01:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30][root][INFO] - Training Epoch: 10/10, step 480/574 completed (loss: 0.020885927602648735, acc: 1.0)
[2025-01-06 01:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30][root][INFO] - Training Epoch: 10/10, step 481/574 completed (loss: 0.0038783657364547253, acc: 1.0)
[2025-01-06 01:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31][root][INFO] - Training Epoch: 10/10, step 482/574 completed (loss: 0.13649742305278778, acc: 0.949999988079071)
[2025-01-06 01:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31][root][INFO] - Training Epoch: 10/10, step 483/574 completed (loss: 0.09034910053014755, acc: 0.9482758641242981)
[2025-01-06 01:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31][root][INFO] - Training Epoch: 10/10, step 484/574 completed (loss: 0.035569705069065094, acc: 1.0)
[2025-01-06 01:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32][root][INFO] - Training Epoch: 10/10, step 485/574 completed (loss: 0.01090280432254076, acc: 1.0)
[2025-01-06 01:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32][root][INFO] - Training Epoch: 10/10, step 486/574 completed (loss: 0.04968878626823425, acc: 1.0)
[2025-01-06 01:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32][root][INFO] - Training Epoch: 10/10, step 487/574 completed (loss: 0.05974661558866501, acc: 0.9523809552192688)
[2025-01-06 01:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33][root][INFO] - Training Epoch: 10/10, step 488/574 completed (loss: 0.12405648827552795, acc: 0.9545454382896423)
[2025-01-06 01:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33][root][INFO] - Training Epoch: 10/10, step 489/574 completed (loss: 0.07774417847394943, acc: 0.9692307710647583)
[2025-01-06 01:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33][root][INFO] - Training Epoch: 10/10, step 490/574 completed (loss: 0.005311909597367048, acc: 1.0)
[2025-01-06 01:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34][root][INFO] - Training Epoch: 10/10, step 491/574 completed (loss: 0.29063740372657776, acc: 0.9655172228813171)
[2025-01-06 01:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34][root][INFO] - Training Epoch: 10/10, step 492/574 completed (loss: 0.04560975357890129, acc: 0.9803921580314636)
[2025-01-06 01:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34][root][INFO] - Training Epoch: 10/10, step 493/574 completed (loss: 0.01795397698879242, acc: 1.0)
[2025-01-06 01:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35][root][INFO] - Training Epoch: 10/10, step 494/574 completed (loss: 0.013676249422132969, acc: 1.0)
[2025-01-06 01:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35][root][INFO] - Training Epoch: 10/10, step 495/574 completed (loss: 0.02530744858086109, acc: 1.0)
[2025-01-06 01:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35][root][INFO] - Training Epoch: 10/10, step 496/574 completed (loss: 0.12633323669433594, acc: 0.9732142686843872)
[2025-01-06 01:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36][root][INFO] - Training Epoch: 10/10, step 497/574 completed (loss: 0.14512403309345245, acc: 0.966292142868042)
[2025-01-06 01:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36][root][INFO] - Training Epoch: 10/10, step 498/574 completed (loss: 0.07726941257715225, acc: 0.9775280952453613)
[2025-01-06 01:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36][root][INFO] - Training Epoch: 10/10, step 499/574 completed (loss: 0.27392876148223877, acc: 0.9078013896942139)
[2025-01-06 01:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37][root][INFO] - Training Epoch: 10/10, step 500/574 completed (loss: 0.14631405472755432, acc: 0.945652186870575)
[2025-01-06 01:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37][root][INFO] - Training Epoch: 10/10, step 501/574 completed (loss: 0.01250426098704338, acc: 1.0)
[2025-01-06 01:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37][root][INFO] - Training Epoch: 10/10, step 502/574 completed (loss: 0.0012305793352425098, acc: 1.0)
[2025-01-06 01:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38][root][INFO] - Training Epoch: 10/10, step 503/574 completed (loss: 0.3762723207473755, acc: 0.9629629850387573)
[2025-01-06 01:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38][root][INFO] - Training Epoch: 10/10, step 504/574 completed (loss: 0.030666057020425797, acc: 1.0)
[2025-01-06 01:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38][root][INFO] - Training Epoch: 10/10, step 505/574 completed (loss: 0.0489119254052639, acc: 0.9622641801834106)
[2025-01-06 01:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38][root][INFO] - Training Epoch: 10/10, step 506/574 completed (loss: 0.022535081952810287, acc: 1.0)
[2025-01-06 01:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39][root][INFO] - Training Epoch: 10/10, step 507/574 completed (loss: 0.13569524884223938, acc: 0.9819819927215576)
[2025-01-06 01:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39][root][INFO] - Training Epoch: 10/10, step 508/574 completed (loss: 0.16514089703559875, acc: 0.9577465057373047)
[2025-01-06 01:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40][root][INFO] - Training Epoch: 10/10, step 509/574 completed (loss: 0.1821666657924652, acc: 0.949999988079071)
[2025-01-06 01:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40][root][INFO] - Training Epoch: 10/10, step 510/574 completed (loss: 0.000563219771720469, acc: 1.0)
[2025-01-06 01:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40][root][INFO] - Training Epoch: 10/10, step 511/574 completed (loss: 0.023373078554868698, acc: 1.0)
[2025-01-06 01:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:43][root][INFO] - Training Epoch: 10/10, step 512/574 completed (loss: 0.19799166917800903, acc: 0.949999988079071)
[2025-01-06 01:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44][root][INFO] - Training Epoch: 10/10, step 513/574 completed (loss: 0.09968572854995728, acc: 0.9523809552192688)
[2025-01-06 01:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44][root][INFO] - Training Epoch: 10/10, step 514/574 completed (loss: 0.03074568137526512, acc: 1.0)
[2025-01-06 01:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44][root][INFO] - Training Epoch: 10/10, step 515/574 completed (loss: 0.03145505487918854, acc: 0.9833333492279053)
[2025-01-06 01:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:45][root][INFO] - Training Epoch: 10/10, step 516/574 completed (loss: 0.06643521785736084, acc: 0.9861111044883728)
[2025-01-06 01:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:45][root][INFO] - Training Epoch: 10/10, step 517/574 completed (loss: 0.0016926816897466779, acc: 1.0)
[2025-01-06 01:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46][root][INFO] - Training Epoch: 10/10, step 518/574 completed (loss: 0.012689149007201195, acc: 1.0)
[2025-01-06 01:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46][root][INFO] - Training Epoch: 10/10, step 519/574 completed (loss: 0.001419594045728445, acc: 1.0)
[2025-01-06 01:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:47][root][INFO] - Training Epoch: 10/10, step 520/574 completed (loss: 0.2082298994064331, acc: 0.9259259104728699)
[2025-01-06 01:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48][root][INFO] - Training Epoch: 10/10, step 521/574 completed (loss: 0.21741966903209686, acc: 0.9364407062530518)
[2025-01-06 01:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48][root][INFO] - Training Epoch: 10/10, step 522/574 completed (loss: 0.05190269276499748, acc: 0.9925373196601868)
[2025-01-06 01:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48][root][INFO] - Training Epoch: 10/10, step 523/574 completed (loss: 0.049583278596401215, acc: 0.9781022071838379)
[2025-01-06 01:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49][root][INFO] - Training Epoch: 10/10, step 524/574 completed (loss: 0.2142442762851715, acc: 0.9350000023841858)
[2025-01-06 01:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49][root][INFO] - Training Epoch: 10/10, step 525/574 completed (loss: 0.04350357875227928, acc: 0.9814814925193787)
[2025-01-06 01:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50][root][INFO] - Training Epoch: 10/10, step 526/574 completed (loss: 0.30956414341926575, acc: 0.9807692170143127)
[2025-01-06 01:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50][root][INFO] - Training Epoch: 10/10, step 527/574 completed (loss: 0.03570469841361046, acc: 1.0)
[2025-01-06 01:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50][root][INFO] - Training Epoch: 10/10, step 528/574 completed (loss: 0.36068087816238403, acc: 0.8852459192276001)
[2025-01-06 01:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51][root][INFO] - Training Epoch: 10/10, step 529/574 completed (loss: 0.026688717305660248, acc: 1.0)
[2025-01-06 01:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51][root][INFO] - Training Epoch: 10/10, step 530/574 completed (loss: 0.17607001960277557, acc: 0.9534883499145508)
[2025-01-06 01:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51][root][INFO] - Training Epoch: 10/10, step 531/574 completed (loss: 0.1335829645395279, acc: 0.9545454382896423)
[2025-01-06 01:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51][root][INFO] - Training Epoch: 10/10, step 532/574 completed (loss: 0.12336279451847076, acc: 0.9622641801834106)
[2025-01-06 01:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52][root][INFO] - Training Epoch: 10/10, step 533/574 completed (loss: 0.17130865156650543, acc: 0.9318181872367859)
[2025-01-06 01:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52][root][INFO] - Training Epoch: 10/10, step 534/574 completed (loss: 0.03115815483033657, acc: 1.0)
[2025-01-06 01:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52][root][INFO] - Training Epoch: 10/10, step 535/574 completed (loss: 0.0018660681089386344, acc: 1.0)
[2025-01-06 01:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:53][root][INFO] - Training Epoch: 10/10, step 536/574 completed (loss: 0.003932999912649393, acc: 1.0)
[2025-01-06 01:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:53][root][INFO] - Training Epoch: 10/10, step 537/574 completed (loss: 0.03283492475748062, acc: 1.0)
[2025-01-06 01:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:53][root][INFO] - Training Epoch: 10/10, step 538/574 completed (loss: 0.16252736747264862, acc: 0.96875)
[2025-01-06 01:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54][root][INFO] - Training Epoch: 10/10, step 539/574 completed (loss: 0.09487929940223694, acc: 0.9375)
[2025-01-06 01:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54][root][INFO] - Training Epoch: 10/10, step 540/574 completed (loss: 0.07817808538675308, acc: 0.9696969985961914)
[2025-01-06 01:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54][root][INFO] - Training Epoch: 10/10, step 541/574 completed (loss: 0.09425029158592224, acc: 0.9375)
[2025-01-06 01:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55][root][INFO] - Training Epoch: 10/10, step 542/574 completed (loss: 0.00493236817419529, acc: 1.0)
[2025-01-06 01:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55][root][INFO] - Training Epoch: 10/10, step 543/574 completed (loss: 0.005399125162512064, acc: 1.0)
[2025-01-06 01:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55][root][INFO] - Training Epoch: 10/10, step 544/574 completed (loss: 0.009137260727584362, acc: 1.0)
[2025-01-06 01:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56][root][INFO] - Training Epoch: 10/10, step 545/574 completed (loss: 0.004357474856078625, acc: 1.0)
[2025-01-06 01:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56][root][INFO] - Training Epoch: 10/10, step 546/574 completed (loss: 0.0019031282281503081, acc: 1.0)
[2025-01-06 01:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56][root][INFO] - Training Epoch: 10/10, step 547/574 completed (loss: 0.00040794885717332363, acc: 1.0)
[2025-01-06 01:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57][root][INFO] - Training Epoch: 10/10, step 548/574 completed (loss: 0.00419752299785614, acc: 1.0)
[2025-01-06 01:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57][root][INFO] - Training Epoch: 10/10, step 549/574 completed (loss: 0.00047594140050932765, acc: 1.0)
[2025-01-06 01:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57][root][INFO] - Training Epoch: 10/10, step 550/574 completed (loss: 0.02038729004561901, acc: 1.0)
[2025-01-06 01:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:58][root][INFO] - Training Epoch: 10/10, step 551/574 completed (loss: 0.0012247228296473622, acc: 1.0)
[2025-01-06 01:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:58][root][INFO] - Training Epoch: 10/10, step 552/574 completed (loss: 0.07515456527471542, acc: 0.9857142567634583)
[2025-01-06 01:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:58][root][INFO] - Training Epoch: 10/10, step 553/574 completed (loss: 0.03811318799853325, acc: 0.985401451587677)
[2025-01-06 01:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.5755, device='cuda:0') eval_epoch_loss=tensor(0.9460, device='cuda:0') eval_epoch_acc=tensor(0.8212, device='cuda:0')
[2025-01-06 01:57:28][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:57:28][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:57:29][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_554_loss_0.9460422396659851/model.pt
[2025-01-06 01:57:29][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.059103503823280334, acc: 0.9724137783050537)
[2025-01-06 01:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.0789204090833664, acc: 0.9785714149475098)
[2025-01-06 01:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.06326834857463837, acc: 0.9867549538612366)
[2025-01-06 01:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.016167787835001945, acc: 1.0)
[2025-01-06 01:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.00135647167917341, acc: 1.0)
[2025-01-06 01:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.007840147241950035, acc: 1.0)
[2025-01-06 01:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.0023139948025345802, acc: 1.0)
[2025-01-06 01:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.0018500704318284988, acc: 1.0)
[2025-01-06 01:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.10426461696624756, acc: 0.9444444179534912)
[2025-01-06 01:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.04002651572227478, acc: 0.9870129823684692)
[2025-01-06 01:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.06151224672794342, acc: 0.9791666865348816)
[2025-01-06 01:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.039465393871068954, acc: 0.9655172228813171)
[2025-01-06 01:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.009441657923161983, acc: 1.0)
[2025-01-06 01:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.0036802683025598526, acc: 1.0)
[2025-01-06 01:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.021918579936027527, acc: 1.0)
[2025-01-06 01:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.044884759932756424, acc: 0.9839572310447693)
[2025-01-06 01:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.0013453500578179955, acc: 1.0)
[2025-01-06 01:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.010372593998908997, acc: 1.0)
[2025-01-06 01:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.12877157330513, acc: 0.9642857313156128)
[2025-01-06 01:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.06368528306484222, acc: 0.9748427867889404)
[2025-01-06 01:57:36][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.0859, train_epoch_loss=0.0824, epoch time 340.9976419173181s
[2025-01-06 01:57:36][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:57:36][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 20 GB
[2025-01-06 01:57:36][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:57:36][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 28
[2025-01-06 01:57:36][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:57:36][root][INFO] - Key: avg_train_prep, Value: 1.5500922203063965
[2025-01-06 01:57:36][root][INFO] - Key: avg_train_loss, Value: 0.3510302007198334
[2025-01-06 01:57:36][root][INFO] - Key: avg_train_acc, Value: 0.9069973230361938
[2025-01-06 01:57:36][root][INFO] - Key: avg_eval_prep, Value: 2.2773172855377197
[2025-01-06 01:57:36][root][INFO] - Key: avg_eval_loss, Value: 0.8162105679512024
[2025-01-06 01:57:36][root][INFO] - Key: avg_eval_acc, Value: 0.8183140754699707
[2025-01-06 01:57:36][root][INFO] - Key: avg_epoch_time, Value: 346.2848273772746
[2025-01-06 01:57:36][root][INFO] - Key: avg_checkpoint_time, Value: 0.2840960045345128
Selected lowest loss checkpoint: asr_epoch_3_step_139_loss_0.6237507462501526
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_139_loss_0.6237507462501526/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_139_loss_0.6237507462501526
[2025-01-06 01:57:59][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-01-06 01:57:59][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-06 01:57:59][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-06 01:58:00][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-06 01:58:06][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 01:58:06][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-06 01:58:06][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 01:58:06][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-06 01:58:10][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 01:58:10][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-06 01:58:10][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-01-06 01:58:10][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 01:58:10][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-06 01:58:11][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-06 01:58:11][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-06 01:58:11][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_139_loss_0.6237507462501526/model.pt
[2025-01-06 01:58:11][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-06 01:58:11][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-06 01:58:12][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-06 01:58:13][root][INFO] - --> Training Set Length = 652
[2025-01-06 01:58:13][root][INFO] - =====================================
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-01-06 01:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:41][slam_llm.models.slam_model][INFO] - modality encoder
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/decode_test_beam4_gt_20250102_005108
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/decode_test_beam4_pred_20250102_005108
Combined WER: 0.7543511450381679

Filtering repeated words...

Found 10 repeated lines in total.
Repeated lines are:
- SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH SH AH
- AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil> AH <sil>
- AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil> AH M <sil>
- AH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA HH AA
- OW AH M IY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY Y EY
- AH M <sil> AH M <sil> AH M <sil> T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T
- AH M N EH S AH N IH Z D UH R IH NG AH D IY <sil> D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D IY D
- AH M AH M <sil> S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S
- AH AH M <sil> OW AH M OW N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N AO R N
- AH Y EH S Y UW Y AE AH HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA R HH AA
Filtered Combined WER: 0.5289348171701113
