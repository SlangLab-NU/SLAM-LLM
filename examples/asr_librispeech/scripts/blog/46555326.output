/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
encoder_config: wavlm-mono
num_epochs: 2
batch_size_training: 4
train_data_folder: aphasia
test_data_folder: aphasia
use_peft: true
seed: 
llm_name: llama32_1b
debug: 
Is test_run? 
freeze_encoder: true
encoder_projector: cov1d-linear
encoder_projector_ds_rate: 5
Is save_embedding? false
projector_transfer_learning: false
transfer_data_folder: 
----------
----------
Final identifier: aphasia_wavlm_llama32_1b_cov1d-linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft/asr_epoch_2_step_23834_loss_1.2332391738891602
Resume epoch: 2
Resume step: 23834
[2025-02-03 23:40:37][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 23834, 'resume_epoch': 2, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-03 23:40:37][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-03 23:40:37][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'cov1d-linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_wavlm_llama32_1b_cov1d-linear_peft'}
[2025-02-03 23:40:37][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_wavlm_llama32_1b_cov1d-linear_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-03_23-40-37.txt', 'log_interval': 5}
[2025-02-03 23:41:06][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-03 23:41:12][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-03 23:41:12][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-03 23:41:12][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-03 23:41:12][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-03 23:41:24][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-03 23:41:24][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-03 23:41:24][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-03 23:41:25][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-03 23:41:25][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-03 23:41:25][slam_llm.utils.train_utils][INFO] - --> Module cov1d-linear
[2025-02-03 23:41:25][slam_llm.utils.train_utils][INFO] - --> cov1d-linear has 11.539456 Million params

[2025-02-03 23:41:25][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft/asr_epoch_2_step_23834_loss_1.2332391738891602/model.pt
[2025-02-03 23:41:25][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-03 23:41:25][slam_llm.utils.train_utils][INFO] - --> asr has 17.175552 Million params

[2025-02-03 23:41:28][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-03 23:41:32][root][INFO] - --> Training Set Length = 95353
[2025-02-03 23:41:32][root][INFO] - --> Validation Set Length = 13162
[2025-02-03 23:41:32][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-03 23:41:32][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-03 23:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:41:37][root][INFO] - Training Epoch: 2/2, step 23834/23838 completed (loss: 1.2349156141281128, acc: 0.8666666746139526)
[2025-02-03 23:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:41:37][root][INFO] - Training Epoch: 2/2, step 23835/23838 completed (loss: 0.9858067035675049, acc: 0.7916666865348816)
[2025-02-03 23:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:41:38][root][INFO] - Training Epoch: 2/2, step 23836/23838 completed (loss: 0.26700642704963684, acc: 0.9599999785423279)
[2025-02-03 23:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:41:38][root][INFO] - Training Epoch: 2/2, step 23837/23838 completed (loss: 0.7220996022224426, acc: 0.8823529481887817)
[2025-02-03 23:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:41:39][root][INFO] - Training Epoch: 2/2, step 23838/23838 completed (loss: 0.5835102796554565, acc: 0.8888888955116272)
[2025-02-03 23:41:40][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.0002, train_epoch_loss=0.0002, epoch time 7.857898876070976s
[2025-02-03 23:41:40][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-02-03 23:41:40][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2025-02-03 23:41:40][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-02-03 23:41:40][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-03 23:41:40][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
Selected lowest loss checkpoint: asr_epoch_2_step_23834_loss_1.2332391738891602
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft/asr_epoch_2_step_23834_loss_1.2332391738891602/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft/asr_epoch_2_step_23834_loss_1.2332391738891602
[2025-02-03 23:42:10][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-03 23:42:10][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-03 23:42:10][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'cov1d-linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_wavlm_llama32_1b_cov1d-linear_peft'}
[2025-02-03 23:42:12][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-03 23:42:18][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-03 23:42:18][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-03 23:42:18][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-03 23:42:18][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-03 23:42:23][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-03 23:42:23][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-03 23:42:23][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-03 23:42:23][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-03 23:42:23][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-03 23:42:24][slam_llm.utils.train_utils][INFO] - --> Module cov1d-linear
[2025-02-03 23:42:24][slam_llm.utils.train_utils][INFO] - --> cov1d-linear has 11.539456 Million params

[2025-02-03 23:42:24][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft/asr_epoch_2_step_23834_loss_1.2332391738891602/model.pt
[2025-02-03 23:42:24][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-03 23:42:24][slam_llm.utils.train_utils][INFO] - --> asr has 17.175552 Million params

[2025-02-03 23:42:26][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-03 23:42:26][root][INFO] - --> Training Set Length = 12232
[2025-02-03 23:42:26][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/test_config.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-02-03 23:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-03 23:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:31:39][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft/decode_test_beam4_pred_20250203_234226
[2025-02-04 00:31:39][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft/decode_test_beam4_gt_20250203_234226
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft/decode_test_beam4_gt_20250203_234226
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_cov1d-linear_peft/decode_test_beam4_pred_20250203_234226
Combined WER: 0.3485410824034847

Filtering repeated words...

Found 37 repeated lines in total.
Repeated lines are:
- but but that was all on a a desk and that kind of stuff or or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk or a desk
- umbrella umbrella umbrella umbrella umbrella umbrella
- well kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick
- her hair is is made up and the the the the
- he's kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick
- oh oh oh oh
- guess we were pretty and pink the film with a million ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo ringo
- and and jelly and and and and and putting and putting and putting and putting
- but he's got this rescue rescue rescue rescue
- okay kicks kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick
- hey you sisters sisters sisters sisters
- so what i learned to do is keep going to more and more and more and more and more and more
- and and and and
- okay okay okay okay okay okay okay okay okay
- and he's kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick
- and now two boys are kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick kick
- he hadta return his crown and go to another conference and and then start right putting feet feet feet feet fits fits fits fits
- he she's pretty sure that what he really wanted is the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
- so he's looking and cleaning and but the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
- and had a ladder broken ladder ladder ladder ladder ladder ladder ladder ladder ladder
- and people walking for a car probably probably probably probably probably probably probably probably probably probably probably probably probably probably probably probably probably probably
- oh i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
- it's it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it
- he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he
- that's hospital and i the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
- here the umbrella or the umbrella umbrella umbrella umbrella
- oh i so it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it
- oh no no no no no
- and barking barking barking barking
- and she would have two two two two
- firstnamej firstnamej firstnamej firstnamej firstnamej firstnamej firstnamej
- and two three three five five five five five
- bench bench bench bench
- four oh oh oh oh
- and i'd seen it before the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
- and i'm just doing what i can to try to heal and realize on normal place to be able to take four take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take take
- and and it hit window window window window
Filtered Combined WER: 0.31929698278447605
task_flag: all
encoder_config: wavlm-mono
num_epochs: 2
batch_size_training: 4
train_data_folder: aphasia
test_data_folder: aphasia
use_peft: true
seed: 
llm_name: llama32_1b
debug: 
Is test_run? 
freeze_encoder: true
encoder_projector: q-former
encoder_projector_ds_rate: 5
Is save_embedding? false
projector_transfer_learning: false
transfer_data_folder: 
----------
----------
Final identifier: aphasia_wavlm_llama32_1b_q-former_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_3.3814234733581543
Resume epoch: 2
Resume step: 17875
[2025-02-04 00:32:23][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 17875, 'resume_epoch': 2, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-04 00:32:23][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-04 00:32:23][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'q-former', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_wavlm_llama32_1b_q-former_peft'}
[2025-02-04 00:32:23][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_wavlm_llama32_1b_q-former_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-04_00-32-22.txt', 'log_interval': 5}
[2025-02-04 00:32:49][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-04 00:32:54][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-04 00:32:54][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-04 00:32:54][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-04 00:32:54][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-04 00:33:00][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-04 00:33:00][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-04 00:33:00][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-04 00:33:01][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-04 00:33:01][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-04 00:33:02][slam_llm.utils.train_utils][INFO] - --> Module q-former
[2025-02-04 00:33:02][slam_llm.utils.train_utils][INFO] - --> q-former has 69.361152 Million params

[2025-02-04 00:33:02][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_3.3814234733581543/model.pt
[2025-02-04 00:33:03][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-04 00:33:03][slam_llm.utils.train_utils][INFO] - --> asr has 74.997248 Million params

[2025-02-04 00:33:05][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-04 00:33:08][root][INFO] - --> Training Set Length = 95353
[2025-02-04 00:33:08][root][INFO] - --> Validation Set Length = 13162
[2025-02-04 00:33:08][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-04 00:33:08][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-04 00:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:11][root][INFO] - Training Epoch: 2/2, step 17875/23838 completed (loss: 3.1601145267486572, acc: 0.4000000059604645)
[2025-02-04 00:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:12][root][INFO] - Training Epoch: 2/2, step 17876/23838 completed (loss: 2.949953317642212, acc: 0.5833333134651184)
[2025-02-04 00:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:12][root][INFO] - Training Epoch: 2/2, step 17877/23838 completed (loss: 1.7279748916625977, acc: 0.6000000238418579)
[2025-02-04 00:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:13][root][INFO] - Training Epoch: 2/2, step 17878/23838 completed (loss: 3.922466516494751, acc: 0.3529411852359772)
[2025-02-04 00:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:13][root][INFO] - Training Epoch: 2/2, step 17879/23838 completed (loss: 3.2067272663116455, acc: 0.4166666567325592)
[2025-02-04 00:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:14][root][INFO] - Training Epoch: 2/2, step 17880/23838 completed (loss: 3.4104201793670654, acc: 0.5)
[2025-02-04 00:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:14][root][INFO] - Training Epoch: 2/2, step 17881/23838 completed (loss: 3.474069118499756, acc: 0.4864864945411682)
[2025-02-04 00:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:15][root][INFO] - Training Epoch: 2/2, step 17882/23838 completed (loss: 4.208179950714111, acc: 0.3488371968269348)
[2025-02-04 00:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:16][root][INFO] - Training Epoch: 2/2, step 17883/23838 completed (loss: 3.729008913040161, acc: 0.41025641560554504)
[2025-02-04 00:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:16][root][INFO] - Training Epoch: 2/2, step 17884/23838 completed (loss: 3.6850967407226562, acc: 0.4285714328289032)
[2025-02-04 00:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:16][root][INFO] - Training Epoch: 2/2, step 17885/23838 completed (loss: 4.3476762771606445, acc: 0.3333333432674408)
[2025-02-04 00:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:17][root][INFO] - Training Epoch: 2/2, step 17886/23838 completed (loss: 2.3866348266601562, acc: 0.6666666865348816)
[2025-02-04 00:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:17][root][INFO] - Training Epoch: 2/2, step 17887/23838 completed (loss: 1.2950515747070312, acc: 0.7142857313156128)
[2025-02-04 00:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:18][root][INFO] - Training Epoch: 2/2, step 17888/23838 completed (loss: 1.7739180326461792, acc: 0.4166666567325592)
[2025-02-04 00:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:19][root][INFO] - Training Epoch: 2/2, step 17889/23838 completed (loss: 1.991407036781311, acc: 0.6666666865348816)
[2025-02-04 00:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:19][root][INFO] - Training Epoch: 2/2, step 17890/23838 completed (loss: 3.684579849243164, acc: 0.4545454680919647)
[2025-02-04 00:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:20][root][INFO] - Training Epoch: 2/2, step 17891/23838 completed (loss: 2.4077095985412598, acc: 0.42105263471603394)
[2025-02-04 00:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:20][root][INFO] - Training Epoch: 2/2, step 17892/23838 completed (loss: 1.3286033868789673, acc: 0.75)
[2025-02-04 00:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:21][root][INFO] - Training Epoch: 2/2, step 17893/23838 completed (loss: 2.7447590827941895, acc: 0.5)
[2025-02-04 00:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:21][root][INFO] - Training Epoch: 2/2, step 17894/23838 completed (loss: 3.9709739685058594, acc: 0.375)
[2025-02-04 00:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:21][root][INFO] - Training Epoch: 2/2, step 17895/23838 completed (loss: 2.4637181758880615, acc: 0.48148149251937866)
[2025-02-04 00:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:22][root][INFO] - Training Epoch: 2/2, step 17896/23838 completed (loss: 4.233656883239746, acc: 0.4285714328289032)
[2025-02-04 00:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:22][root][INFO] - Training Epoch: 2/2, step 17897/23838 completed (loss: 1.918121099472046, acc: 0.5263158082962036)
[2025-02-04 00:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:23][root][INFO] - Training Epoch: 2/2, step 17898/23838 completed (loss: 2.9676430225372314, acc: 0.46875)
[2025-02-04 00:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:23][root][INFO] - Training Epoch: 2/2, step 17899/23838 completed (loss: 3.2740046977996826, acc: 0.4482758641242981)
[2025-02-04 00:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:24][root][INFO] - Training Epoch: 2/2, step 17900/23838 completed (loss: 2.2729856967926025, acc: 0.5185185074806213)
[2025-02-04 00:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:24][root][INFO] - Training Epoch: 2/2, step 17901/23838 completed (loss: 2.2653019428253174, acc: 0.5333333611488342)
[2025-02-04 00:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:25][root][INFO] - Training Epoch: 2/2, step 17902/23838 completed (loss: 2.090662956237793, acc: 0.5199999809265137)
[2025-02-04 00:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:25][root][INFO] - Training Epoch: 2/2, step 17903/23838 completed (loss: 2.7257790565490723, acc: 0.3478260934352875)
[2025-02-04 00:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:26][root][INFO] - Training Epoch: 2/2, step 17904/23838 completed (loss: 1.834114670753479, acc: 0.6666666865348816)
[2025-02-04 00:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:26][root][INFO] - Training Epoch: 2/2, step 17905/23838 completed (loss: 3.0406103134155273, acc: 0.6153846383094788)
[2025-02-04 00:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:26][root][INFO] - Training Epoch: 2/2, step 17906/23838 completed (loss: 2.4678473472595215, acc: 0.6666666865348816)
[2025-02-04 00:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:27][root][INFO] - Training Epoch: 2/2, step 17907/23838 completed (loss: 3.0536558628082275, acc: 0.5)
[2025-02-04 00:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:27][root][INFO] - Training Epoch: 2/2, step 17908/23838 completed (loss: 2.2196648120880127, acc: 0.6363636255264282)
[2025-02-04 00:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:28][root][INFO] - Training Epoch: 2/2, step 17909/23838 completed (loss: 2.6950109004974365, acc: 0.4166666567325592)
[2025-02-04 00:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:28][root][INFO] - Training Epoch: 2/2, step 17910/23838 completed (loss: 3.5129783153533936, acc: 0.5)
[2025-02-04 00:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:29][root][INFO] - Training Epoch: 2/2, step 17911/23838 completed (loss: 5.199635982513428, acc: 0.25)
[2025-02-04 00:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:29][root][INFO] - Training Epoch: 2/2, step 17912/23838 completed (loss: 7.172077178955078, acc: 0.13793103396892548)
[2025-02-04 00:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:30][root][INFO] - Training Epoch: 2/2, step 17913/23838 completed (loss: 5.081422328948975, acc: 0.31578946113586426)
[2025-02-04 00:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:31][root][INFO] - Training Epoch: 2/2, step 17914/23838 completed (loss: 0.41378018260002136, acc: 0.8333333134651184)
[2025-02-04 00:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:31][root][INFO] - Training Epoch: 2/2, step 17915/23838 completed (loss: 0.01637965254485607, acc: 1.0)
[2025-02-04 00:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:32][root][INFO] - Training Epoch: 2/2, step 17916/23838 completed (loss: 3.620734691619873, acc: 0.5333333611488342)
[2025-02-04 00:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:32][root][INFO] - Training Epoch: 2/2, step 17917/23838 completed (loss: 2.5829203128814697, acc: 0.4000000059604645)
[2025-02-04 00:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:33][root][INFO] - Training Epoch: 2/2, step 17918/23838 completed (loss: 2.7945451736450195, acc: 0.4117647111415863)
[2025-02-04 00:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:33][root][INFO] - Training Epoch: 2/2, step 17919/23838 completed (loss: 2.302061080932617, acc: 0.5882353186607361)
[2025-02-04 00:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:33][root][INFO] - Training Epoch: 2/2, step 17920/23838 completed (loss: 2.1136014461517334, acc: 0.52173912525177)
[2025-02-04 00:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:34][root][INFO] - Training Epoch: 2/2, step 17921/23838 completed (loss: 1.5918668508529663, acc: 0.6153846383094788)
[2025-02-04 00:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:34][root][INFO] - Training Epoch: 2/2, step 17922/23838 completed (loss: 2.4658944606781006, acc: 0.5833333134651184)
[2025-02-04 00:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:35][root][INFO] - Training Epoch: 2/2, step 17923/23838 completed (loss: 1.816508412361145, acc: 0.6666666865348816)
[2025-02-04 00:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:35][root][INFO] - Training Epoch: 2/2, step 17924/23838 completed (loss: 4.206663608551025, acc: 0.18918919563293457)
[2025-02-04 00:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:36][root][INFO] - Training Epoch: 2/2, step 17925/23838 completed (loss: 3.6279456615448, acc: 0.46666666865348816)
[2025-02-04 00:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:36][root][INFO] - Training Epoch: 2/2, step 17926/23838 completed (loss: 2.8735475540161133, acc: 0.4680851101875305)
[2025-02-04 00:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:37][root][INFO] - Training Epoch: 2/2, step 17927/23838 completed (loss: 2.879315137863159, acc: 0.3571428656578064)
[2025-02-04 00:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:37][root][INFO] - Training Epoch: 2/2, step 17928/23838 completed (loss: 3.3279099464416504, acc: 0.3571428656578064)
[2025-02-04 00:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:38][root][INFO] - Training Epoch: 2/2, step 17929/23838 completed (loss: 1.9097516536712646, acc: 0.5161290168762207)
[2025-02-04 00:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:38][root][INFO] - Training Epoch: 2/2, step 17930/23838 completed (loss: 3.1860291957855225, acc: 0.4166666567325592)
[2025-02-04 00:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:39][root][INFO] - Training Epoch: 2/2, step 17931/23838 completed (loss: 2.7340521812438965, acc: 0.44999998807907104)
[2025-02-04 00:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:39][root][INFO] - Training Epoch: 2/2, step 17932/23838 completed (loss: 2.505352735519409, acc: 0.375)
[2025-02-04 00:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:40][root][INFO] - Training Epoch: 2/2, step 17933/23838 completed (loss: 2.6423861980438232, acc: 0.4516128897666931)
[2025-02-04 00:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:40][root][INFO] - Training Epoch: 2/2, step 17934/23838 completed (loss: 2.645228624343872, acc: 0.4516128897666931)
[2025-02-04 00:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:41][root][INFO] - Training Epoch: 2/2, step 17935/23838 completed (loss: 2.8462398052215576, acc: 0.4615384638309479)
[2025-02-04 00:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:41][root][INFO] - Training Epoch: 2/2, step 17936/23838 completed (loss: 3.119943380355835, acc: 0.29411765933036804)
[2025-02-04 00:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:42][root][INFO] - Training Epoch: 2/2, step 17937/23838 completed (loss: 3.087261438369751, acc: 0.36666667461395264)
[2025-02-04 00:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:42][root][INFO] - Training Epoch: 2/2, step 17938/23838 completed (loss: 3.7345709800720215, acc: 0.4000000059604645)
[2025-02-04 00:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:43][root][INFO] - Training Epoch: 2/2, step 17939/23838 completed (loss: 2.369232654571533, acc: 0.517241358757019)
[2025-02-04 00:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:43][root][INFO] - Training Epoch: 2/2, step 17940/23838 completed (loss: 2.731884002685547, acc: 0.4285714328289032)
[2025-02-04 00:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:44][root][INFO] - Training Epoch: 2/2, step 17941/23838 completed (loss: 1.1735471487045288, acc: 0.699999988079071)
[2025-02-04 00:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:44][root][INFO] - Training Epoch: 2/2, step 17942/23838 completed (loss: 1.7039151191711426, acc: 0.5882353186607361)
[2025-02-04 00:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:44][root][INFO] - Training Epoch: 2/2, step 17943/23838 completed (loss: 1.8302580118179321, acc: 0.6666666865348816)
[2025-02-04 00:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:45][root][INFO] - Training Epoch: 2/2, step 17944/23838 completed (loss: 0.9917858242988586, acc: 0.6363636255264282)
[2025-02-04 00:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:46][root][INFO] - Training Epoch: 2/2, step 17945/23838 completed (loss: 3.3721394538879395, acc: 0.4285714328289032)
[2025-02-04 00:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:46][root][INFO] - Training Epoch: 2/2, step 17946/23838 completed (loss: 2.232848644256592, acc: 0.6000000238418579)
[2025-02-04 00:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:47][root][INFO] - Training Epoch: 2/2, step 17947/23838 completed (loss: 4.5453972816467285, acc: 0.4166666567325592)
[2025-02-04 00:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:47][root][INFO] - Training Epoch: 2/2, step 17948/23838 completed (loss: 3.8905189037323, acc: 0.4000000059604645)
[2025-02-04 00:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:48][root][INFO] - Training Epoch: 2/2, step 17949/23838 completed (loss: 3.2346303462982178, acc: 0.4375)
[2025-02-04 00:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:48][root][INFO] - Training Epoch: 2/2, step 17950/23838 completed (loss: 2.3937346935272217, acc: 0.5)
[2025-02-04 00:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:49][root][INFO] - Training Epoch: 2/2, step 17951/23838 completed (loss: 2.1583731174468994, acc: 0.4545454680919647)
[2025-02-04 00:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:49][root][INFO] - Training Epoch: 2/2, step 17952/23838 completed (loss: 4.784778118133545, acc: 0.31578946113586426)
[2025-02-04 00:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:49][root][INFO] - Training Epoch: 2/2, step 17953/23838 completed (loss: 1.8391743898391724, acc: 0.5833333134651184)
[2025-02-04 00:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:50][root][INFO] - Training Epoch: 2/2, step 17954/23838 completed (loss: 2.0757529735565186, acc: 0.5833333134651184)
[2025-02-04 00:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:50][root][INFO] - Training Epoch: 2/2, step 17955/23838 completed (loss: 1.8770583868026733, acc: 0.75)
[2025-02-04 00:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:51][root][INFO] - Training Epoch: 2/2, step 17956/23838 completed (loss: 2.2959675788879395, acc: 0.42105263471603394)
[2025-02-04 00:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:51][root][INFO] - Training Epoch: 2/2, step 17957/23838 completed (loss: 1.4068759679794312, acc: 0.7142857313156128)
[2025-02-04 00:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:52][root][INFO] - Training Epoch: 2/2, step 17958/23838 completed (loss: 2.9784340858459473, acc: 0.4375)
[2025-02-04 00:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:52][root][INFO] - Training Epoch: 2/2, step 17959/23838 completed (loss: 1.878912091255188, acc: 0.4761904776096344)
[2025-02-04 00:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:53][root][INFO] - Training Epoch: 2/2, step 17960/23838 completed (loss: 2.624180316925049, acc: 0.375)
[2025-02-04 00:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:53][root][INFO] - Training Epoch: 2/2, step 17961/23838 completed (loss: 2.3789098262786865, acc: 0.5333333611488342)
[2025-02-04 00:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:54][root][INFO] - Training Epoch: 2/2, step 17962/23838 completed (loss: 2.2676503658294678, acc: 0.4583333432674408)
[2025-02-04 00:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:54][root][INFO] - Training Epoch: 2/2, step 17963/23838 completed (loss: 1.5527470111846924, acc: 0.7647058963775635)
[2025-02-04 00:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:55][root][INFO] - Training Epoch: 2/2, step 17964/23838 completed (loss: 4.078198432922363, acc: 0.40740740299224854)
[2025-02-04 00:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:55][root][INFO] - Training Epoch: 2/2, step 17965/23838 completed (loss: 4.235384941101074, acc: 0.3636363744735718)
[2025-02-04 00:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:56][root][INFO] - Training Epoch: 2/2, step 17966/23838 completed (loss: 1.9361048936843872, acc: 0.53125)
[2025-02-04 00:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:56][root][INFO] - Training Epoch: 2/2, step 17967/23838 completed (loss: 1.4328933954238892, acc: 0.6071428656578064)
[2025-02-04 00:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:57][root][INFO] - Training Epoch: 2/2, step 17968/23838 completed (loss: 2.0051767826080322, acc: 0.5757575631141663)
[2025-02-04 00:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:57][root][INFO] - Training Epoch: 2/2, step 17969/23838 completed (loss: 1.9083335399627686, acc: 0.5769230723381042)
[2025-02-04 00:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:58][root][INFO] - Training Epoch: 2/2, step 17970/23838 completed (loss: 1.3902961015701294, acc: 0.75)
[2025-02-04 00:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:58][root][INFO] - Training Epoch: 2/2, step 17971/23838 completed (loss: 0.4070869982242584, acc: 0.8571428656578064)
[2025-02-04 00:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:58][root][INFO] - Training Epoch: 2/2, step 17972/23838 completed (loss: 0.3142077326774597, acc: 1.0)
[2025-02-04 00:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:59][root][INFO] - Training Epoch: 2/2, step 17973/23838 completed (loss: 2.191521406173706, acc: 0.5)
[2025-02-04 00:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:33:59][root][INFO] - Training Epoch: 2/2, step 17974/23838 completed (loss: 0.2799527943134308, acc: 0.9166666865348816)
[2025-02-04 00:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:00][root][INFO] - Training Epoch: 2/2, step 17975/23838 completed (loss: 1.3823212385177612, acc: 0.875)
[2025-02-04 00:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:00][root][INFO] - Training Epoch: 2/2, step 17976/23838 completed (loss: 2.4058055877685547, acc: 0.6111111044883728)
[2025-02-04 00:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:01][root][INFO] - Training Epoch: 2/2, step 17977/23838 completed (loss: 1.2657133340835571, acc: 0.8181818127632141)
[2025-02-04 00:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:01][root][INFO] - Training Epoch: 2/2, step 17978/23838 completed (loss: 1.8732966184616089, acc: 0.6000000238418579)
[2025-02-04 00:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:02][root][INFO] - Training Epoch: 2/2, step 17979/23838 completed (loss: 1.4701294898986816, acc: 0.5)
[2025-02-04 00:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:02][root][INFO] - Training Epoch: 2/2, step 17980/23838 completed (loss: 1.1599171161651611, acc: 0.6428571343421936)
[2025-02-04 00:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:03][root][INFO] - Training Epoch: 2/2, step 17981/23838 completed (loss: 0.7244929671287537, acc: 0.7692307829856873)
[2025-02-04 00:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:03][root][INFO] - Training Epoch: 2/2, step 17982/23838 completed (loss: 0.6355807781219482, acc: 0.875)
[2025-02-04 00:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:03][root][INFO] - Training Epoch: 2/2, step 17983/23838 completed (loss: 0.6825371980667114, acc: 0.8666666746139526)
[2025-02-04 00:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:04][root][INFO] - Training Epoch: 2/2, step 17984/23838 completed (loss: 1.2271254062652588, acc: 0.6000000238418579)
[2025-02-04 00:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:04][root][INFO] - Training Epoch: 2/2, step 17985/23838 completed (loss: 1.7951027154922485, acc: 0.6666666865348816)
[2025-02-04 00:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:05][root][INFO] - Training Epoch: 2/2, step 17986/23838 completed (loss: 1.4664450883865356, acc: 0.7857142686843872)
[2025-02-04 00:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:05][root][INFO] - Training Epoch: 2/2, step 17987/23838 completed (loss: 1.834684133529663, acc: 0.4285714328289032)
[2025-02-04 00:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:06][root][INFO] - Training Epoch: 2/2, step 17988/23838 completed (loss: 0.5068826079368591, acc: 0.8181818127632141)
[2025-02-04 00:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:06][root][INFO] - Training Epoch: 2/2, step 17989/23838 completed (loss: 2.1021323204040527, acc: 0.6428571343421936)
[2025-02-04 00:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:06][root][INFO] - Training Epoch: 2/2, step 17990/23838 completed (loss: 5.256330966949463, acc: 0.15000000596046448)
[2025-02-04 00:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:07][root][INFO] - Training Epoch: 2/2, step 17991/23838 completed (loss: 3.977905035018921, acc: 0.260869562625885)
[2025-02-04 00:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:07][root][INFO] - Training Epoch: 2/2, step 17992/23838 completed (loss: 4.091241359710693, acc: 0.5)
[2025-02-04 00:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:08][root][INFO] - Training Epoch: 2/2, step 17993/23838 completed (loss: 2.922973871231079, acc: 0.5714285969734192)
[2025-02-04 00:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:08][root][INFO] - Training Epoch: 2/2, step 17994/23838 completed (loss: 4.453887939453125, acc: 0.4000000059604645)
[2025-02-04 00:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:09][root][INFO] - Training Epoch: 2/2, step 17995/23838 completed (loss: 5.384612083435059, acc: 0.20000000298023224)
[2025-02-04 00:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:09][root][INFO] - Training Epoch: 2/2, step 17996/23838 completed (loss: 3.800976276397705, acc: 0.4615384638309479)
[2025-02-04 00:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:10][root][INFO] - Training Epoch: 2/2, step 17997/23838 completed (loss: 2.658282995223999, acc: 0.375)
[2025-02-04 00:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:10][root][INFO] - Training Epoch: 2/2, step 17998/23838 completed (loss: 2.8970963954925537, acc: 0.5)
[2025-02-04 00:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:11][root][INFO] - Training Epoch: 2/2, step 17999/23838 completed (loss: 3.423631429672241, acc: 0.38461539149284363)
[2025-02-04 00:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:11][root][INFO] - Training Epoch: 2/2, step 18000/23838 completed (loss: 4.603497505187988, acc: 0.38461539149284363)
[2025-02-04 00:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:12][root][INFO] - Training Epoch: 2/2, step 18001/23838 completed (loss: 5.3001322746276855, acc: 0.2380952388048172)
[2025-02-04 00:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:12][root][INFO] - Training Epoch: 2/2, step 18002/23838 completed (loss: 4.696179389953613, acc: 0.23999999463558197)
[2025-02-04 00:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:13][root][INFO] - Training Epoch: 2/2, step 18003/23838 completed (loss: 5.118954658508301, acc: 0.36000001430511475)
[2025-02-04 00:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:13][root][INFO] - Training Epoch: 2/2, step 18004/23838 completed (loss: 2.7468652725219727, acc: 0.47058823704719543)
[2025-02-04 00:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:14][root][INFO] - Training Epoch: 2/2, step 18005/23838 completed (loss: 3.0642616748809814, acc: 0.4444444477558136)
[2025-02-04 00:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:14][root][INFO] - Training Epoch: 2/2, step 18006/23838 completed (loss: 4.6080851554870605, acc: 0.3333333432674408)
[2025-02-04 00:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:15][root][INFO] - Training Epoch: 2/2, step 18007/23838 completed (loss: 2.064572811126709, acc: 0.5384615659713745)
[2025-02-04 00:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:15][root][INFO] - Training Epoch: 2/2, step 18008/23838 completed (loss: 4.763803482055664, acc: 0.25)
[2025-02-04 00:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:16][root][INFO] - Training Epoch: 2/2, step 18009/23838 completed (loss: 2.5325794219970703, acc: 0.4117647111415863)
[2025-02-04 00:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:16][root][INFO] - Training Epoch: 2/2, step 18010/23838 completed (loss: 3.001063108444214, acc: 0.4375)
[2025-02-04 00:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:17][root][INFO] - Training Epoch: 2/2, step 18011/23838 completed (loss: 5.170321941375732, acc: 0.375)
[2025-02-04 00:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:17][root][INFO] - Training Epoch: 2/2, step 18012/23838 completed (loss: 3.1168322563171387, acc: 0.4761904776096344)
[2025-02-04 00:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:18][root][INFO] - Training Epoch: 2/2, step 18013/23838 completed (loss: 4.13809871673584, acc: 0.5384615659713745)
[2025-02-04 00:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:18][root][INFO] - Training Epoch: 2/2, step 18014/23838 completed (loss: 3.001774549484253, acc: 0.5)
[2025-02-04 00:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:19][root][INFO] - Training Epoch: 2/2, step 18015/23838 completed (loss: 4.648831844329834, acc: 0.25)
[2025-02-04 00:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:19][root][INFO] - Training Epoch: 2/2, step 18016/23838 completed (loss: 2.8898236751556396, acc: 0.4166666567325592)
[2025-02-04 00:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:19][root][INFO] - Training Epoch: 2/2, step 18017/23838 completed (loss: 3.9294185638427734, acc: 0.2631579041481018)
[2025-02-04 00:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:20][root][INFO] - Training Epoch: 2/2, step 18018/23838 completed (loss: 2.5949954986572266, acc: 0.4285714328289032)
[2025-02-04 00:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:20][root][INFO] - Training Epoch: 2/2, step 18019/23838 completed (loss: 1.9174394607543945, acc: 0.6666666865348816)
[2025-02-04 00:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:21][root][INFO] - Training Epoch: 2/2, step 18020/23838 completed (loss: 3.692321300506592, acc: 0.32258063554763794)
[2025-02-04 00:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:21][root][INFO] - Training Epoch: 2/2, step 18021/23838 completed (loss: 3.4780755043029785, acc: 0.3499999940395355)
[2025-02-04 00:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:22][root][INFO] - Training Epoch: 2/2, step 18022/23838 completed (loss: 5.698095798492432, acc: 0.27272728085517883)
[2025-02-04 00:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:22][root][INFO] - Training Epoch: 2/2, step 18023/23838 completed (loss: 3.2731447219848633, acc: 0.5)
[2025-02-04 00:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:23][root][INFO] - Training Epoch: 2/2, step 18024/23838 completed (loss: 1.2978222370147705, acc: 0.699999988079071)
[2025-02-04 00:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:23][root][INFO] - Training Epoch: 2/2, step 18025/23838 completed (loss: 4.508849620819092, acc: 0.40909090638160706)
[2025-02-04 00:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:24][root][INFO] - Training Epoch: 2/2, step 18026/23838 completed (loss: 4.121969223022461, acc: 0.4166666567325592)
[2025-02-04 00:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:24][root][INFO] - Training Epoch: 2/2, step 18027/23838 completed (loss: 3.361011266708374, acc: 0.4000000059604645)
[2025-02-04 00:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:24][root][INFO] - Training Epoch: 2/2, step 18028/23838 completed (loss: 2.8389976024627686, acc: 0.42105263471603394)
[2025-02-04 00:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:25][root][INFO] - Training Epoch: 2/2, step 18029/23838 completed (loss: 3.561084032058716, acc: 0.3499999940395355)
[2025-02-04 00:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:25][root][INFO] - Training Epoch: 2/2, step 18030/23838 completed (loss: 3.9416468143463135, acc: 0.48148149251937866)
[2025-02-04 00:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:26][root][INFO] - Training Epoch: 2/2, step 18031/23838 completed (loss: 3.786264419555664, acc: 0.47058823704719543)
[2025-02-04 00:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:26][root][INFO] - Training Epoch: 2/2, step 18032/23838 completed (loss: 3.172755718231201, acc: 0.3571428656578064)
[2025-02-04 00:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:27][root][INFO] - Training Epoch: 2/2, step 18033/23838 completed (loss: 2.4848251342773438, acc: 0.5833333134651184)
[2025-02-04 00:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:27][root][INFO] - Training Epoch: 2/2, step 18034/23838 completed (loss: 4.279090404510498, acc: 0.4545454680919647)
[2025-02-04 00:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:28][root][INFO] - Training Epoch: 2/2, step 18035/23838 completed (loss: 3.813530921936035, acc: 0.5)
[2025-02-04 00:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:28][root][INFO] - Training Epoch: 2/2, step 18036/23838 completed (loss: 3.3128743171691895, acc: 0.4000000059604645)
[2025-02-04 00:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:29][root][INFO] - Training Epoch: 2/2, step 18037/23838 completed (loss: 3.1557416915893555, acc: 0.5)
[2025-02-04 00:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:29][root][INFO] - Training Epoch: 2/2, step 18038/23838 completed (loss: 3.926161050796509, acc: 0.30434781312942505)
[2025-02-04 00:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:30][root][INFO] - Training Epoch: 2/2, step 18039/23838 completed (loss: 4.7024126052856445, acc: 0.22857142984867096)
[2025-02-04 00:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:30][root][INFO] - Training Epoch: 2/2, step 18040/23838 completed (loss: 3.8738253116607666, acc: 0.3333333432674408)
[2025-02-04 00:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:30][root][INFO] - Training Epoch: 2/2, step 18041/23838 completed (loss: 4.250957489013672, acc: 0.27272728085517883)
[2025-02-04 00:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:31][root][INFO] - Training Epoch: 2/2, step 18042/23838 completed (loss: 5.5398945808410645, acc: 0.21212121844291687)
[2025-02-04 00:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:31][root][INFO] - Training Epoch: 2/2, step 18043/23838 completed (loss: 2.727048635482788, acc: 0.3333333432674408)
[2025-02-04 00:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:32][root][INFO] - Training Epoch: 2/2, step 18044/23838 completed (loss: 1.9875155687332153, acc: 0.5882353186607361)
[2025-02-04 00:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:32][root][INFO] - Training Epoch: 2/2, step 18045/23838 completed (loss: 2.9818506240844727, acc: 0.48148149251937866)
[2025-02-04 00:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:33][root][INFO] - Training Epoch: 2/2, step 18046/23838 completed (loss: 2.812370777130127, acc: 0.4761904776096344)
[2025-02-04 00:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:33][root][INFO] - Training Epoch: 2/2, step 18047/23838 completed (loss: 2.0531020164489746, acc: 0.800000011920929)
[2025-02-04 00:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:34][root][INFO] - Training Epoch: 2/2, step 18048/23838 completed (loss: 3.7856171131134033, acc: 0.3913043439388275)
[2025-02-04 00:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:34][root][INFO] - Training Epoch: 2/2, step 18049/23838 completed (loss: 1.710304856300354, acc: 0.5)
[2025-02-04 00:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:35][root][INFO] - Training Epoch: 2/2, step 18050/23838 completed (loss: 2.8718180656433105, acc: 0.5)
[2025-02-04 00:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:35][root][INFO] - Training Epoch: 2/2, step 18051/23838 completed (loss: 2.6598970890045166, acc: 0.5185185074806213)
[2025-02-04 00:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:36][root][INFO] - Training Epoch: 2/2, step 18052/23838 completed (loss: 2.1105141639709473, acc: 0.6363636255264282)
[2025-02-04 00:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:36][root][INFO] - Training Epoch: 2/2, step 18053/23838 completed (loss: 2.3520262241363525, acc: 0.529411792755127)
[2025-02-04 00:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:36][root][INFO] - Training Epoch: 2/2, step 18054/23838 completed (loss: 3.4175829887390137, acc: 0.4848484992980957)
[2025-02-04 00:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:37][root][INFO] - Training Epoch: 2/2, step 18055/23838 completed (loss: 1.77381432056427, acc: 0.5714285969734192)
[2025-02-04 00:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:37][root][INFO] - Training Epoch: 2/2, step 18056/23838 completed (loss: 2.404374599456787, acc: 0.523809552192688)
[2025-02-04 00:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:38][root][INFO] - Training Epoch: 2/2, step 18057/23838 completed (loss: 3.2878496646881104, acc: 0.42307692766189575)
[2025-02-04 00:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:38][root][INFO] - Training Epoch: 2/2, step 18058/23838 completed (loss: 3.126713514328003, acc: 0.3913043439388275)
[2025-02-04 00:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:39][root][INFO] - Training Epoch: 2/2, step 18059/23838 completed (loss: 3.251647710800171, acc: 0.5862069129943848)
[2025-02-04 00:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:39][root][INFO] - Training Epoch: 2/2, step 18060/23838 completed (loss: 3.577617883682251, acc: 0.4333333373069763)
[2025-02-04 00:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:40][root][INFO] - Training Epoch: 2/2, step 18061/23838 completed (loss: 3.7136833667755127, acc: 0.4000000059604645)
[2025-02-04 00:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:40][root][INFO] - Training Epoch: 2/2, step 18062/23838 completed (loss: 4.133038520812988, acc: 0.4000000059604645)
[2025-02-04 00:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:40][root][INFO] - Training Epoch: 2/2, step 18063/23838 completed (loss: 3.636831521987915, acc: 0.5)
[2025-02-04 00:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:41][root][INFO] - Training Epoch: 2/2, step 18064/23838 completed (loss: 2.640468120574951, acc: 0.5263158082962036)
[2025-02-04 00:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:41][root][INFO] - Training Epoch: 2/2, step 18065/23838 completed (loss: 2.971404552459717, acc: 0.5)
[2025-02-04 00:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:42][root][INFO] - Training Epoch: 2/2, step 18066/23838 completed (loss: 3.7308995723724365, acc: 0.3571428656578064)
[2025-02-04 00:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:42][root][INFO] - Training Epoch: 2/2, step 18067/23838 completed (loss: 3.6632614135742188, acc: 0.5384615659713745)
[2025-02-04 00:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:43][root][INFO] - Training Epoch: 2/2, step 18068/23838 completed (loss: 3.967088222503662, acc: 0.40740740299224854)
[2025-02-04 00:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:43][root][INFO] - Training Epoch: 2/2, step 18069/23838 completed (loss: 1.977165699005127, acc: 0.6153846383094788)
[2025-02-04 00:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:44][root][INFO] - Training Epoch: 2/2, step 18070/23838 completed (loss: 3.21640944480896, acc: 0.3636363744735718)
[2025-02-04 00:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:44][root][INFO] - Training Epoch: 2/2, step 18071/23838 completed (loss: 4.444031715393066, acc: 0.23999999463558197)
[2025-02-04 00:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:45][root][INFO] - Training Epoch: 2/2, step 18072/23838 completed (loss: 2.7522544860839844, acc: 0.5384615659713745)
[2025-02-04 00:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:45][root][INFO] - Training Epoch: 2/2, step 18073/23838 completed (loss: 3.3138427734375, acc: 0.5833333134651184)
[2025-02-04 00:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:46][root][INFO] - Training Epoch: 2/2, step 18074/23838 completed (loss: 3.0646636486053467, acc: 0.4000000059604645)
[2025-02-04 00:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:46][root][INFO] - Training Epoch: 2/2, step 18075/23838 completed (loss: 2.5845184326171875, acc: 0.4000000059604645)
[2025-02-04 00:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:46][root][INFO] - Training Epoch: 2/2, step 18076/23838 completed (loss: 4.016485214233398, acc: 0.3076923191547394)
[2025-02-04 00:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:47][root][INFO] - Training Epoch: 2/2, step 18077/23838 completed (loss: 3.0680124759674072, acc: 0.5)
[2025-02-04 00:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:47][root][INFO] - Training Epoch: 2/2, step 18078/23838 completed (loss: 2.880183696746826, acc: 0.5)
[2025-02-04 00:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:48][root][INFO] - Training Epoch: 2/2, step 18079/23838 completed (loss: 4.6406097412109375, acc: 0.3499999940395355)
[2025-02-04 00:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:48][root][INFO] - Training Epoch: 2/2, step 18080/23838 completed (loss: 2.720980167388916, acc: 0.5384615659713745)
[2025-02-04 00:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:49][root][INFO] - Training Epoch: 2/2, step 18081/23838 completed (loss: 3.1176106929779053, acc: 0.6153846383094788)
[2025-02-04 00:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:49][root][INFO] - Training Epoch: 2/2, step 18082/23838 completed (loss: 4.818318843841553, acc: 0.4000000059604645)
[2025-02-04 00:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:50][root][INFO] - Training Epoch: 2/2, step 18083/23838 completed (loss: 3.178651809692383, acc: 0.5)
[2025-02-04 00:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:50][root][INFO] - Training Epoch: 2/2, step 18084/23838 completed (loss: 2.830275774002075, acc: 0.4166666567325592)
[2025-02-04 00:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:50][root][INFO] - Training Epoch: 2/2, step 18085/23838 completed (loss: 2.0681533813476562, acc: 0.5)
[2025-02-04 00:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:51][root][INFO] - Training Epoch: 2/2, step 18086/23838 completed (loss: 3.30363392829895, acc: 0.4000000059604645)
[2025-02-04 00:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:51][root][INFO] - Training Epoch: 2/2, step 18087/23838 completed (loss: 4.589637279510498, acc: 0.25)
[2025-02-04 00:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:52][root][INFO] - Training Epoch: 2/2, step 18088/23838 completed (loss: 1.522019386291504, acc: 0.7777777910232544)
[2025-02-04 00:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:52][root][INFO] - Training Epoch: 2/2, step 18089/23838 completed (loss: 3.0420424938201904, acc: 0.5)
[2025-02-04 00:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:53][root][INFO] - Training Epoch: 2/2, step 18090/23838 completed (loss: 2.42323899269104, acc: 0.5454545617103577)
[2025-02-04 00:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:53][root][INFO] - Training Epoch: 2/2, step 18091/23838 completed (loss: 2.425273895263672, acc: 0.5)
[2025-02-04 00:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:54][root][INFO] - Training Epoch: 2/2, step 18092/23838 completed (loss: 3.9505341053009033, acc: 0.3333333432674408)
[2025-02-04 00:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:54][root][INFO] - Training Epoch: 2/2, step 18093/23838 completed (loss: 2.5525970458984375, acc: 0.5714285969734192)
[2025-02-04 00:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:54][root][INFO] - Training Epoch: 2/2, step 18094/23838 completed (loss: 2.2146008014678955, acc: 0.5882353186607361)
[2025-02-04 00:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:55][root][INFO] - Training Epoch: 2/2, step 18095/23838 completed (loss: 3.3963208198547363, acc: 0.5)
[2025-02-04 00:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:55][root][INFO] - Training Epoch: 2/2, step 18096/23838 completed (loss: 2.612666130065918, acc: 0.46666666865348816)
[2025-02-04 00:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:56][root][INFO] - Training Epoch: 2/2, step 18097/23838 completed (loss: 3.536951780319214, acc: 0.4000000059604645)
[2025-02-04 00:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:56][root][INFO] - Training Epoch: 2/2, step 18098/23838 completed (loss: 3.7777013778686523, acc: 0.375)
[2025-02-04 00:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:57][root][INFO] - Training Epoch: 2/2, step 18099/23838 completed (loss: 4.796354293823242, acc: 0.375)
[2025-02-04 00:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:57][root][INFO] - Training Epoch: 2/2, step 18100/23838 completed (loss: 4.160439968109131, acc: 0.3125)
[2025-02-04 00:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:58][root][INFO] - Training Epoch: 2/2, step 18101/23838 completed (loss: 3.2806396484375, acc: 0.5625)
[2025-02-04 00:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:58][root][INFO] - Training Epoch: 2/2, step 18102/23838 completed (loss: 2.1100974082946777, acc: 0.5)
[2025-02-04 00:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:59][root][INFO] - Training Epoch: 2/2, step 18103/23838 completed (loss: 3.5752310752868652, acc: 0.4375)
[2025-02-04 00:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:34:59][root][INFO] - Training Epoch: 2/2, step 18104/23838 completed (loss: 3.753723382949829, acc: 0.4117647111415863)
[2025-02-04 00:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:00][root][INFO] - Training Epoch: 2/2, step 18105/23838 completed (loss: 2.3375155925750732, acc: 0.6000000238418579)
[2025-02-04 00:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:00][root][INFO] - Training Epoch: 2/2, step 18106/23838 completed (loss: 3.582538604736328, acc: 0.4166666567325592)
[2025-02-04 00:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:00][root][INFO] - Training Epoch: 2/2, step 18107/23838 completed (loss: 3.9783828258514404, acc: 0.3214285671710968)
[2025-02-04 00:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:01][root][INFO] - Training Epoch: 2/2, step 18108/23838 completed (loss: 4.106258869171143, acc: 0.3888888955116272)
[2025-02-04 00:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:01][root][INFO] - Training Epoch: 2/2, step 18109/23838 completed (loss: 3.8292276859283447, acc: 0.375)
[2025-02-04 00:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:02][root][INFO] - Training Epoch: 2/2, step 18110/23838 completed (loss: 3.99664306640625, acc: 0.4117647111415863)
[2025-02-04 00:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:02][root][INFO] - Training Epoch: 2/2, step 18111/23838 completed (loss: 3.4486377239227295, acc: 0.5)
[2025-02-04 00:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:03][root][INFO] - Training Epoch: 2/2, step 18112/23838 completed (loss: 1.9690823554992676, acc: 0.27272728085517883)
[2025-02-04 00:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:04][root][INFO] - Training Epoch: 2/2, step 18113/23838 completed (loss: 3.1064186096191406, acc: 0.3333333432674408)
[2025-02-04 00:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:04][root][INFO] - Training Epoch: 2/2, step 18114/23838 completed (loss: 2.1259679794311523, acc: 0.550000011920929)
[2025-02-04 00:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:04][root][INFO] - Training Epoch: 2/2, step 18115/23838 completed (loss: 0.5835459232330322, acc: 0.9090909361839294)
[2025-02-04 00:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:05][root][INFO] - Training Epoch: 2/2, step 18116/23838 completed (loss: 2.3816332817077637, acc: 0.6666666865348816)
[2025-02-04 00:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:05][root][INFO] - Training Epoch: 2/2, step 18117/23838 completed (loss: 3.2860517501831055, acc: 0.40740740299224854)
[2025-02-04 00:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:06][root][INFO] - Training Epoch: 2/2, step 18118/23838 completed (loss: 1.4794987440109253, acc: 0.8333333134651184)
[2025-02-04 00:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:06][root][INFO] - Training Epoch: 2/2, step 18119/23838 completed (loss: 4.493931770324707, acc: 0.4615384638309479)
[2025-02-04 00:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:07][root][INFO] - Training Epoch: 2/2, step 18120/23838 completed (loss: 4.081040859222412, acc: 0.46666666865348816)
[2025-02-04 00:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:07][root][INFO] - Training Epoch: 2/2, step 18121/23838 completed (loss: 2.9532687664031982, acc: 0.5384615659713745)
[2025-02-04 00:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:08][root][INFO] - Training Epoch: 2/2, step 18122/23838 completed (loss: 1.8652254343032837, acc: 0.6666666865348816)
[2025-02-04 00:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:08][root][INFO] - Training Epoch: 2/2, step 18123/23838 completed (loss: 2.963083505630493, acc: 0.3684210479259491)
[2025-02-04 00:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:08][root][INFO] - Training Epoch: 2/2, step 18124/23838 completed (loss: 3.7893645763397217, acc: 0.3529411852359772)
[2025-02-04 00:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:09][root][INFO] - Training Epoch: 2/2, step 18125/23838 completed (loss: 4.690234184265137, acc: 0.38461539149284363)
[2025-02-04 00:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:09][root][INFO] - Training Epoch: 2/2, step 18126/23838 completed (loss: 2.216069459915161, acc: 0.6875)
[2025-02-04 00:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:10][root][INFO] - Training Epoch: 2/2, step 18127/23838 completed (loss: 3.4934940338134766, acc: 0.375)
[2025-02-04 00:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:10][root][INFO] - Training Epoch: 2/2, step 18128/23838 completed (loss: 2.278348207473755, acc: 0.6666666865348816)
[2025-02-04 00:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:11][root][INFO] - Training Epoch: 2/2, step 18129/23838 completed (loss: 3.400904417037964, acc: 0.4166666567325592)
[2025-02-04 00:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:11][root][INFO] - Training Epoch: 2/2, step 18130/23838 completed (loss: 0.24899238348007202, acc: 0.9090909361839294)
[2025-02-04 00:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:12][root][INFO] - Training Epoch: 2/2, step 18131/23838 completed (loss: 2.3942291736602783, acc: 0.6428571343421936)
[2025-02-04 00:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:12][root][INFO] - Training Epoch: 2/2, step 18132/23838 completed (loss: 1.8297324180603027, acc: 0.5555555820465088)
[2025-02-04 00:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:13][root][INFO] - Training Epoch: 2/2, step 18133/23838 completed (loss: 2.3568005561828613, acc: 0.5333333611488342)
[2025-02-04 00:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:13][root][INFO] - Training Epoch: 2/2, step 18134/23838 completed (loss: 2.5898733139038086, acc: 0.29411765933036804)
[2025-02-04 00:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:14][root][INFO] - Training Epoch: 2/2, step 18135/23838 completed (loss: 2.8189427852630615, acc: 0.40625)
[2025-02-04 00:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:14][root][INFO] - Training Epoch: 2/2, step 18136/23838 completed (loss: 3.0200507640838623, acc: 0.27586206793785095)
[2025-02-04 00:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:15][root][INFO] - Training Epoch: 2/2, step 18137/23838 completed (loss: 2.182178020477295, acc: 0.6111111044883728)
[2025-02-04 00:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:15][root][INFO] - Training Epoch: 2/2, step 18138/23838 completed (loss: 3.79740309715271, acc: 0.4545454680919647)
[2025-02-04 00:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:15][root][INFO] - Training Epoch: 2/2, step 18139/23838 completed (loss: 5.915562152862549, acc: 0.25)
[2025-02-04 00:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:16][root][INFO] - Training Epoch: 2/2, step 18140/23838 completed (loss: 3.2690908908843994, acc: 0.45945945382118225)
[2025-02-04 00:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:16][root][INFO] - Training Epoch: 2/2, step 18141/23838 completed (loss: 3.1585521697998047, acc: 0.5263158082962036)
[2025-02-04 00:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:17][root][INFO] - Training Epoch: 2/2, step 18142/23838 completed (loss: 1.8919721841812134, acc: 0.5)
[2025-02-04 00:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:17][root][INFO] - Training Epoch: 2/2, step 18143/23838 completed (loss: 1.5147403478622437, acc: 0.5789473652839661)
[2025-02-04 00:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:18][root][INFO] - Training Epoch: 2/2, step 18144/23838 completed (loss: 2.8368566036224365, acc: 0.4583333432674408)
[2025-02-04 00:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:18][root][INFO] - Training Epoch: 2/2, step 18145/23838 completed (loss: 2.283313035964966, acc: 0.46666666865348816)
[2025-02-04 00:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:19][root][INFO] - Training Epoch: 2/2, step 18146/23838 completed (loss: 2.86614727973938, acc: 0.3636363744735718)
[2025-02-04 00:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:19][root][INFO] - Training Epoch: 2/2, step 18147/23838 completed (loss: 2.6515088081359863, acc: 0.4761904776096344)
[2025-02-04 00:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:20][root][INFO] - Training Epoch: 2/2, step 18148/23838 completed (loss: 1.4611029624938965, acc: 0.75)
[2025-02-04 00:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:20][root][INFO] - Training Epoch: 2/2, step 18149/23838 completed (loss: 1.6788341999053955, acc: 0.6000000238418579)
[2025-02-04 00:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:20][root][INFO] - Training Epoch: 2/2, step 18150/23838 completed (loss: 3.3408801555633545, acc: 0.375)
[2025-02-04 00:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:21][root][INFO] - Training Epoch: 2/2, step 18151/23838 completed (loss: 1.073505163192749, acc: 0.8888888955116272)
[2025-02-04 00:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:21][root][INFO] - Training Epoch: 2/2, step 18152/23838 completed (loss: 1.118698000907898, acc: 0.7272727489471436)
[2025-02-04 00:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:22][root][INFO] - Training Epoch: 2/2, step 18153/23838 completed (loss: 2.3551418781280518, acc: 0.550000011920929)
[2025-02-04 00:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:22][root][INFO] - Training Epoch: 2/2, step 18154/23838 completed (loss: 2.390151262283325, acc: 0.5483871102333069)
[2025-02-04 00:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:23][root][INFO] - Training Epoch: 2/2, step 18155/23838 completed (loss: 2.2369272708892822, acc: 0.6111111044883728)
[2025-02-04 00:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:23][root][INFO] - Training Epoch: 2/2, step 18156/23838 completed (loss: 2.265212297439575, acc: 0.5365853905677795)
[2025-02-04 00:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:23][root][INFO] - Training Epoch: 2/2, step 18157/23838 completed (loss: 1.349249243736267, acc: 0.6315789222717285)
[2025-02-04 00:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:24][root][INFO] - Training Epoch: 2/2, step 18158/23838 completed (loss: 1.8754643201828003, acc: 0.6363636255264282)
[2025-02-04 00:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:24][root][INFO] - Training Epoch: 2/2, step 18159/23838 completed (loss: 1.863136887550354, acc: 0.6842105388641357)
[2025-02-04 00:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:25][root][INFO] - Training Epoch: 2/2, step 18160/23838 completed (loss: 3.02101731300354, acc: 0.39024388790130615)
[2025-02-04 00:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:25][root][INFO] - Training Epoch: 2/2, step 18161/23838 completed (loss: 1.6954779624938965, acc: 0.5789473652839661)
[2025-02-04 00:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:26][root][INFO] - Training Epoch: 2/2, step 18162/23838 completed (loss: 3.4648287296295166, acc: 0.375)
[2025-02-04 00:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:26][root][INFO] - Training Epoch: 2/2, step 18163/23838 completed (loss: 2.7692182064056396, acc: 0.42307692766189575)
[2025-02-04 00:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:26][root][INFO] - Training Epoch: 2/2, step 18164/23838 completed (loss: 2.5741803646087646, acc: 0.5)
[2025-02-04 00:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:27][root][INFO] - Training Epoch: 2/2, step 18165/23838 completed (loss: 3.3686139583587646, acc: 0.3333333432674408)
[2025-02-04 00:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:27][root][INFO] - Training Epoch: 2/2, step 18166/23838 completed (loss: 2.8518452644348145, acc: 0.3235294222831726)
[2025-02-04 00:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:28][root][INFO] - Training Epoch: 2/2, step 18167/23838 completed (loss: 3.615410804748535, acc: 0.2800000011920929)
[2025-02-04 00:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:28][root][INFO] - Training Epoch: 2/2, step 18168/23838 completed (loss: 1.9131561517715454, acc: 0.6499999761581421)
[2025-02-04 00:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:28][root][INFO] - Training Epoch: 2/2, step 18169/23838 completed (loss: 3.987278461456299, acc: 0.2222222238779068)
[2025-02-04 00:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:29][root][INFO] - Training Epoch: 2/2, step 18170/23838 completed (loss: 3.597733736038208, acc: 0.4117647111415863)
[2025-02-04 00:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:29][root][INFO] - Training Epoch: 2/2, step 18171/23838 completed (loss: 2.855668783187866, acc: 0.4285714328289032)
[2025-02-04 00:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:30][root][INFO] - Training Epoch: 2/2, step 18172/23838 completed (loss: 3.711986780166626, acc: 0.29411765933036804)
[2025-02-04 00:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:30][root][INFO] - Training Epoch: 2/2, step 18173/23838 completed (loss: 4.617937088012695, acc: 0.3513513505458832)
[2025-02-04 00:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:31][root][INFO] - Training Epoch: 2/2, step 18174/23838 completed (loss: 4.35954475402832, acc: 0.2978723347187042)
[2025-02-04 00:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:31][root][INFO] - Training Epoch: 2/2, step 18175/23838 completed (loss: 3.7779765129089355, acc: 0.2750000059604645)
[2025-02-04 00:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:32][root][INFO] - Training Epoch: 2/2, step 18176/23838 completed (loss: 3.561976432800293, acc: 0.40740740299224854)
[2025-02-04 00:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:32][root][INFO] - Training Epoch: 2/2, step 18177/23838 completed (loss: 4.001189708709717, acc: 0.3030303120613098)
[2025-02-04 00:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:33][root][INFO] - Training Epoch: 2/2, step 18178/23838 completed (loss: 3.1408116817474365, acc: 0.3333333432674408)
[2025-02-04 00:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:33][root][INFO] - Training Epoch: 2/2, step 18179/23838 completed (loss: 3.3672995567321777, acc: 0.3870967626571655)
[2025-02-04 00:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:34][root][INFO] - Training Epoch: 2/2, step 18180/23838 completed (loss: 2.302251100540161, acc: 0.3333333432674408)
[2025-02-04 00:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:34][root][INFO] - Training Epoch: 2/2, step 18181/23838 completed (loss: 4.198109149932861, acc: 0.3333333432674408)
[2025-02-04 00:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:34][root][INFO] - Training Epoch: 2/2, step 18182/23838 completed (loss: 2.58010196685791, acc: 0.5)
[2025-02-04 00:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:35][root][INFO] - Training Epoch: 2/2, step 18183/23838 completed (loss: 3.5087356567382812, acc: 0.24242424964904785)
[2025-02-04 00:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:35][root][INFO] - Training Epoch: 2/2, step 18184/23838 completed (loss: 3.5448262691497803, acc: 0.260869562625885)
[2025-02-04 00:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:36][root][INFO] - Training Epoch: 2/2, step 18185/23838 completed (loss: 3.4644854068756104, acc: 0.3636363744735718)
[2025-02-04 00:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:36][root][INFO] - Training Epoch: 2/2, step 18186/23838 completed (loss: 3.833221197128296, acc: 0.3478260934352875)
[2025-02-04 00:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:36][root][INFO] - Training Epoch: 2/2, step 18187/23838 completed (loss: 3.642069101333618, acc: 0.3611111044883728)
[2025-02-04 00:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:37][root][INFO] - Training Epoch: 2/2, step 18188/23838 completed (loss: 1.7675038576126099, acc: 0.5454545617103577)
[2025-02-04 00:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:37][root][INFO] - Training Epoch: 2/2, step 18189/23838 completed (loss: 3.4859588146209717, acc: 0.5)
[2025-02-04 00:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:38][root][INFO] - Training Epoch: 2/2, step 18190/23838 completed (loss: 3.391202211380005, acc: 0.36666667461395264)
[2025-02-04 00:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:38][root][INFO] - Training Epoch: 2/2, step 18191/23838 completed (loss: 4.608808517456055, acc: 0.1860465109348297)
[2025-02-04 00:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:39][root][INFO] - Training Epoch: 2/2, step 18192/23838 completed (loss: 3.3654544353485107, acc: 0.29629629850387573)
[2025-02-04 00:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:39][root][INFO] - Training Epoch: 2/2, step 18193/23838 completed (loss: 2.3988194465637207, acc: 0.6000000238418579)
[2025-02-04 00:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:39][root][INFO] - Training Epoch: 2/2, step 18194/23838 completed (loss: 2.6009674072265625, acc: 0.5652173757553101)
[2025-02-04 00:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:40][root][INFO] - Training Epoch: 2/2, step 18195/23838 completed (loss: 2.2903127670288086, acc: 0.4761904776096344)
[2025-02-04 00:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:40][root][INFO] - Training Epoch: 2/2, step 18196/23838 completed (loss: 3.110288381576538, acc: 0.375)
[2025-02-04 00:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:41][root][INFO] - Training Epoch: 2/2, step 18197/23838 completed (loss: 1.726113200187683, acc: 0.3888888955116272)
[2025-02-04 00:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:41][root][INFO] - Training Epoch: 2/2, step 18198/23838 completed (loss: 3.259606122970581, acc: 0.3684210479259491)
[2025-02-04 00:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:42][root][INFO] - Training Epoch: 2/2, step 18199/23838 completed (loss: 2.0773799419403076, acc: 0.5555555820465088)
[2025-02-04 00:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:42][root][INFO] - Training Epoch: 2/2, step 18200/23838 completed (loss: 3.026902675628662, acc: 0.3181818127632141)
[2025-02-04 00:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:43][root][INFO] - Training Epoch: 2/2, step 18201/23838 completed (loss: 2.3283443450927734, acc: 0.5199999809265137)
[2025-02-04 00:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:43][root][INFO] - Training Epoch: 2/2, step 18202/23838 completed (loss: 3.2976126670837402, acc: 0.21739129722118378)
[2025-02-04 00:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:43][root][INFO] - Training Epoch: 2/2, step 18203/23838 completed (loss: 2.522944211959839, acc: 0.4000000059604645)
[2025-02-04 00:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:44][root][INFO] - Training Epoch: 2/2, step 18204/23838 completed (loss: 3.1541922092437744, acc: 0.4583333432674408)
[2025-02-04 00:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:44][root][INFO] - Training Epoch: 2/2, step 18205/23838 completed (loss: 3.0652079582214355, acc: 0.38461539149284363)
[2025-02-04 00:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:45][root][INFO] - Training Epoch: 2/2, step 18206/23838 completed (loss: 3.0530850887298584, acc: 0.5)
[2025-02-04 00:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:45][root][INFO] - Training Epoch: 2/2, step 18207/23838 completed (loss: 3.7045392990112305, acc: 0.3103448152542114)
[2025-02-04 00:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:46][root][INFO] - Training Epoch: 2/2, step 18208/23838 completed (loss: 3.4631905555725098, acc: 0.3103448152542114)
[2025-02-04 00:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:46][root][INFO] - Training Epoch: 2/2, step 18209/23838 completed (loss: 3.863128185272217, acc: 0.38461539149284363)
[2025-02-04 00:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:47][root][INFO] - Training Epoch: 2/2, step 18210/23838 completed (loss: 3.201040029525757, acc: 0.5)
[2025-02-04 00:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:47][root][INFO] - Training Epoch: 2/2, step 18211/23838 completed (loss: 3.631704568862915, acc: 0.3448275923728943)
[2025-02-04 00:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:47][root][INFO] - Training Epoch: 2/2, step 18212/23838 completed (loss: 3.767838478088379, acc: 0.29411765933036804)
[2025-02-04 00:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:48][root][INFO] - Training Epoch: 2/2, step 18213/23838 completed (loss: 4.077131748199463, acc: 0.38461539149284363)
[2025-02-04 00:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:48][root][INFO] - Training Epoch: 2/2, step 18214/23838 completed (loss: 4.443030834197998, acc: 0.16129031777381897)
[2025-02-04 00:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:49][root][INFO] - Training Epoch: 2/2, step 18215/23838 completed (loss: 2.9001452922821045, acc: 0.4285714328289032)
[2025-02-04 00:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:49][root][INFO] - Training Epoch: 2/2, step 18216/23838 completed (loss: 2.3381717205047607, acc: 0.4375)
[2025-02-04 00:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:50][root][INFO] - Training Epoch: 2/2, step 18217/23838 completed (loss: 3.308842420578003, acc: 0.3125)
[2025-02-04 00:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:50][root][INFO] - Training Epoch: 2/2, step 18218/23838 completed (loss: 3.424156427383423, acc: 0.39534884691238403)
[2025-02-04 00:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:51][root][INFO] - Training Epoch: 2/2, step 18219/23838 completed (loss: 2.7341647148132324, acc: 0.4444444477558136)
[2025-02-04 00:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:51][root][INFO] - Training Epoch: 2/2, step 18220/23838 completed (loss: 4.335425853729248, acc: 0.2222222238779068)
[2025-02-04 00:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:52][root][INFO] - Training Epoch: 2/2, step 18221/23838 completed (loss: 3.8324365615844727, acc: 0.25806450843811035)
[2025-02-04 00:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:52][root][INFO] - Training Epoch: 2/2, step 18222/23838 completed (loss: 3.8357038497924805, acc: 0.3333333432674408)
[2025-02-04 00:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:53][root][INFO] - Training Epoch: 2/2, step 18223/23838 completed (loss: 3.201984167098999, acc: 0.2857142984867096)
[2025-02-04 00:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:53][root][INFO] - Training Epoch: 2/2, step 18224/23838 completed (loss: 3.6272809505462646, acc: 0.4333333373069763)
[2025-02-04 00:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:53][root][INFO] - Training Epoch: 2/2, step 18225/23838 completed (loss: 3.7310848236083984, acc: 0.40625)
[2025-02-04 00:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:54][root][INFO] - Training Epoch: 2/2, step 18226/23838 completed (loss: 3.771636962890625, acc: 0.3913043439388275)
[2025-02-04 00:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:54][root][INFO] - Training Epoch: 2/2, step 18227/23838 completed (loss: 2.1154892444610596, acc: 0.7142857313156128)
[2025-02-04 00:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:55][root][INFO] - Training Epoch: 2/2, step 18228/23838 completed (loss: 3.433081865310669, acc: 0.3913043439388275)
[2025-02-04 00:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:55][root][INFO] - Training Epoch: 2/2, step 18229/23838 completed (loss: 3.654940605163574, acc: 0.3199999928474426)
[2025-02-04 00:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:55][root][INFO] - Training Epoch: 2/2, step 18230/23838 completed (loss: 3.469193935394287, acc: 0.3333333432674408)
[2025-02-04 00:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:56][root][INFO] - Training Epoch: 2/2, step 18231/23838 completed (loss: 3.479539155960083, acc: 0.3214285671710968)
[2025-02-04 00:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:56][root][INFO] - Training Epoch: 2/2, step 18232/23838 completed (loss: 3.167365789413452, acc: 0.4137931168079376)
[2025-02-04 00:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:57][root][INFO] - Training Epoch: 2/2, step 18233/23838 completed (loss: 4.319007396697998, acc: 0.3333333432674408)
[2025-02-04 00:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:57][root][INFO] - Training Epoch: 2/2, step 18234/23838 completed (loss: 3.2388854026794434, acc: 0.37931033968925476)
[2025-02-04 00:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:58][root][INFO] - Training Epoch: 2/2, step 18235/23838 completed (loss: 3.706362724304199, acc: 0.3103448152542114)
[2025-02-04 00:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:58][root][INFO] - Training Epoch: 2/2, step 18236/23838 completed (loss: 2.4972450733184814, acc: 0.3913043439388275)
[2025-02-04 00:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:58][root][INFO] - Training Epoch: 2/2, step 18237/23838 completed (loss: 1.781782865524292, acc: 0.5454545617103577)
[2025-02-04 00:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:59][root][INFO] - Training Epoch: 2/2, step 18238/23838 completed (loss: 3.399925470352173, acc: 0.36000001430511475)
[2025-02-04 00:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:35:59][root][INFO] - Training Epoch: 2/2, step 18239/23838 completed (loss: 3.8468470573425293, acc: 0.2857142984867096)
[2025-02-04 00:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:00][root][INFO] - Training Epoch: 2/2, step 18240/23838 completed (loss: 1.6645612716674805, acc: 0.75)
[2025-02-04 00:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:00][root][INFO] - Training Epoch: 2/2, step 18241/23838 completed (loss: 3.5357301235198975, acc: 0.27272728085517883)
[2025-02-04 00:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:01][root][INFO] - Training Epoch: 2/2, step 18242/23838 completed (loss: 3.486440896987915, acc: 0.2631579041481018)
[2025-02-04 00:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:01][root][INFO] - Training Epoch: 2/2, step 18243/23838 completed (loss: 4.598362445831299, acc: 0.190476194024086)
[2025-02-04 00:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:02][root][INFO] - Training Epoch: 2/2, step 18244/23838 completed (loss: 3.7181577682495117, acc: 0.2800000011920929)
[2025-02-04 00:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:02][root][INFO] - Training Epoch: 2/2, step 18245/23838 completed (loss: 2.7232306003570557, acc: 0.4166666567325592)
[2025-02-04 00:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:03][root][INFO] - Training Epoch: 2/2, step 18246/23838 completed (loss: 2.900627851486206, acc: 0.42424243688583374)
[2025-02-04 00:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:03][root][INFO] - Training Epoch: 2/2, step 18247/23838 completed (loss: 2.3433048725128174, acc: 0.4864864945411682)
[2025-02-04 00:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:03][root][INFO] - Training Epoch: 2/2, step 18248/23838 completed (loss: 2.938223123550415, acc: 0.3684210479259491)
[2025-02-04 00:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:04][root][INFO] - Training Epoch: 2/2, step 18249/23838 completed (loss: 3.3676156997680664, acc: 0.2666666805744171)
[2025-02-04 00:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:04][root][INFO] - Training Epoch: 2/2, step 18250/23838 completed (loss: 3.0577290058135986, acc: 0.3888888955116272)
[2025-02-04 00:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:05][root][INFO] - Training Epoch: 2/2, step 18251/23838 completed (loss: 2.690894842147827, acc: 0.44999998807907104)
[2025-02-04 00:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:05][root][INFO] - Training Epoch: 2/2, step 18252/23838 completed (loss: 3.2232024669647217, acc: 0.2666666805744171)
[2025-02-04 00:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:06][root][INFO] - Training Epoch: 2/2, step 18253/23838 completed (loss: 3.6031482219696045, acc: 0.2295081913471222)
[2025-02-04 00:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:06][root][INFO] - Training Epoch: 2/2, step 18254/23838 completed (loss: 3.067019462585449, acc: 0.3488371968269348)
[2025-02-04 00:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:07][root][INFO] - Training Epoch: 2/2, step 18255/23838 completed (loss: 4.029831886291504, acc: 0.2982456088066101)
[2025-02-04 00:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:07][root][INFO] - Training Epoch: 2/2, step 18256/23838 completed (loss: 4.021771430969238, acc: 0.3125)
[2025-02-04 00:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:08][root][INFO] - Training Epoch: 2/2, step 18257/23838 completed (loss: 3.869938611984253, acc: 0.3235294222831726)
[2025-02-04 00:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:08][root][INFO] - Training Epoch: 2/2, step 18258/23838 completed (loss: 3.282987594604492, acc: 0.38235294818878174)
[2025-02-04 00:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:09][root][INFO] - Training Epoch: 2/2, step 18259/23838 completed (loss: 3.3507063388824463, acc: 0.35555556416511536)
[2025-02-04 00:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:09][root][INFO] - Training Epoch: 2/2, step 18260/23838 completed (loss: 3.2735414505004883, acc: 0.2380952388048172)
[2025-02-04 00:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:10][root][INFO] - Training Epoch: 2/2, step 18261/23838 completed (loss: 2.7658016681671143, acc: 0.4642857015132904)
[2025-02-04 00:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:10][root][INFO] - Training Epoch: 2/2, step 18262/23838 completed (loss: 2.9614250659942627, acc: 0.42105263471603394)
[2025-02-04 00:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:11][root][INFO] - Training Epoch: 2/2, step 18263/23838 completed (loss: 3.2054717540740967, acc: 0.3030303120613098)
[2025-02-04 00:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:11][root][INFO] - Training Epoch: 2/2, step 18264/23838 completed (loss: 3.0721302032470703, acc: 0.44897958636283875)
[2025-02-04 00:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:12][root][INFO] - Training Epoch: 2/2, step 18265/23838 completed (loss: 3.679828405380249, acc: 0.3030303120613098)
[2025-02-04 00:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:12][root][INFO] - Training Epoch: 2/2, step 18266/23838 completed (loss: 2.1561920642852783, acc: 0.6052631735801697)
[2025-02-04 00:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:13][root][INFO] - Training Epoch: 2/2, step 18267/23838 completed (loss: 2.798173189163208, acc: 0.4375)
[2025-02-04 00:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:13][root][INFO] - Training Epoch: 2/2, step 18268/23838 completed (loss: 2.234795093536377, acc: 0.5416666865348816)
[2025-02-04 00:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:14][root][INFO] - Training Epoch: 2/2, step 18269/23838 completed (loss: 2.7529616355895996, acc: 0.38461539149284363)
[2025-02-04 00:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:14][root][INFO] - Training Epoch: 2/2, step 18270/23838 completed (loss: 2.8914520740509033, acc: 0.4749999940395355)
[2025-02-04 00:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:15][root][INFO] - Training Epoch: 2/2, step 18271/23838 completed (loss: 2.6212501525878906, acc: 0.5)
[2025-02-04 00:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:15][root][INFO] - Training Epoch: 2/2, step 18272/23838 completed (loss: 3.7253198623657227, acc: 0.2545454502105713)
[2025-02-04 00:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:16][root][INFO] - Training Epoch: 2/2, step 18273/23838 completed (loss: 3.1242687702178955, acc: 0.4285714328289032)
[2025-02-04 00:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:16][root][INFO] - Training Epoch: 2/2, step 18274/23838 completed (loss: 3.479440927505493, acc: 0.3404255211353302)
[2025-02-04 00:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:17][root][INFO] - Training Epoch: 2/2, step 18275/23838 completed (loss: 3.1822714805603027, acc: 0.40425533056259155)
[2025-02-04 00:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:17][root][INFO] - Training Epoch: 2/2, step 18276/23838 completed (loss: 3.8336565494537354, acc: 0.28787878155708313)
[2025-02-04 00:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:18][root][INFO] - Training Epoch: 2/2, step 18277/23838 completed (loss: 3.141292095184326, acc: 0.5757575631141663)
[2025-02-04 00:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:18][root][INFO] - Training Epoch: 2/2, step 18278/23838 completed (loss: 2.390709161758423, acc: 0.4761904776096344)
[2025-02-04 00:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:19][root][INFO] - Training Epoch: 2/2, step 18279/23838 completed (loss: 2.314474105834961, acc: 0.5600000023841858)
[2025-02-04 00:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:19][root][INFO] - Training Epoch: 2/2, step 18280/23838 completed (loss: 2.461853504180908, acc: 0.5600000023841858)
[2025-02-04 00:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:20][root][INFO] - Training Epoch: 2/2, step 18281/23838 completed (loss: 3.606544017791748, acc: 0.5)
[2025-02-04 00:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:20][root][INFO] - Training Epoch: 2/2, step 18282/23838 completed (loss: 2.9500653743743896, acc: 0.44999998807907104)
[2025-02-04 00:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:21][root][INFO] - Training Epoch: 2/2, step 18283/23838 completed (loss: 4.235864162445068, acc: 0.4285714328289032)
[2025-02-04 00:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:21][root][INFO] - Training Epoch: 2/2, step 18284/23838 completed (loss: 2.333815813064575, acc: 0.5882353186607361)
[2025-02-04 00:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:22][root][INFO] - Training Epoch: 2/2, step 18285/23838 completed (loss: 2.9790327548980713, acc: 0.5)
[2025-02-04 00:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:22][root][INFO] - Training Epoch: 2/2, step 18286/23838 completed (loss: 3.117164134979248, acc: 0.6190476417541504)
[2025-02-04 00:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:23][root][INFO] - Training Epoch: 2/2, step 18287/23838 completed (loss: 3.163818597793579, acc: 0.6428571343421936)
[2025-02-04 00:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:23][root][INFO] - Training Epoch: 2/2, step 18288/23838 completed (loss: 3.254955530166626, acc: 0.625)
[2025-02-04 00:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:23][root][INFO] - Training Epoch: 2/2, step 18289/23838 completed (loss: 2.533780336380005, acc: 0.517241358757019)
[2025-02-04 00:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:24][root][INFO] - Training Epoch: 2/2, step 18290/23838 completed (loss: 3.496884346008301, acc: 0.6153846383094788)
[2025-02-04 00:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:24][root][INFO] - Training Epoch: 2/2, step 18291/23838 completed (loss: 3.5458297729492188, acc: 0.523809552192688)
[2025-02-04 00:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:25][root][INFO] - Training Epoch: 2/2, step 18292/23838 completed (loss: 3.9967620372772217, acc: 0.3448275923728943)
[2025-02-04 00:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:25][root][INFO] - Training Epoch: 2/2, step 18293/23838 completed (loss: 3.821378707885742, acc: 0.30434781312942505)
[2025-02-04 00:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:26][root][INFO] - Training Epoch: 2/2, step 18294/23838 completed (loss: 3.030519723892212, acc: 0.5)
[2025-02-04 00:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:26][root][INFO] - Training Epoch: 2/2, step 18295/23838 completed (loss: 3.330775737762451, acc: 0.3333333432674408)
[2025-02-04 00:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:27][root][INFO] - Training Epoch: 2/2, step 18296/23838 completed (loss: 3.44647216796875, acc: 0.42307692766189575)
[2025-02-04 00:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:27][root][INFO] - Training Epoch: 2/2, step 18297/23838 completed (loss: 3.84487247467041, acc: 0.5)
[2025-02-04 00:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:28][root][INFO] - Training Epoch: 2/2, step 18298/23838 completed (loss: 4.178769588470459, acc: 0.37037035822868347)
[2025-02-04 00:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:28][root][INFO] - Training Epoch: 2/2, step 18299/23838 completed (loss: 3.4424455165863037, acc: 0.5)
[2025-02-04 00:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:29][root][INFO] - Training Epoch: 2/2, step 18300/23838 completed (loss: 2.695969581604004, acc: 0.5263158082962036)
[2025-02-04 00:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:29][root][INFO] - Training Epoch: 2/2, step 18301/23838 completed (loss: 3.520989179611206, acc: 0.4444444477558136)
[2025-02-04 00:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:30][root][INFO] - Training Epoch: 2/2, step 18302/23838 completed (loss: 3.49003005027771, acc: 0.5)
[2025-02-04 00:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:30][root][INFO] - Training Epoch: 2/2, step 18303/23838 completed (loss: 1.918138027191162, acc: 0.6666666865348816)
[2025-02-04 00:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:31][root][INFO] - Training Epoch: 2/2, step 18304/23838 completed (loss: 4.180949687957764, acc: 0.30000001192092896)
[2025-02-04 00:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:31][root][INFO] - Training Epoch: 2/2, step 18305/23838 completed (loss: 2.0146307945251465, acc: 0.6000000238418579)
[2025-02-04 00:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:32][root][INFO] - Training Epoch: 2/2, step 18306/23838 completed (loss: 2.7353355884552, acc: 0.4516128897666931)
[2025-02-04 00:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:32][root][INFO] - Training Epoch: 2/2, step 18307/23838 completed (loss: 3.5773255825042725, acc: 0.41860464215278625)
[2025-02-04 00:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:33][root][INFO] - Training Epoch: 2/2, step 18308/23838 completed (loss: 2.8071553707122803, acc: 0.45652174949645996)
[2025-02-04 00:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:33][root][INFO] - Training Epoch: 2/2, step 18309/23838 completed (loss: 2.673224687576294, acc: 0.5151515007019043)
[2025-02-04 00:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:34][root][INFO] - Training Epoch: 2/2, step 18310/23838 completed (loss: 2.786919116973877, acc: 0.4615384638309479)
[2025-02-04 00:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:34][root][INFO] - Training Epoch: 2/2, step 18311/23838 completed (loss: 2.552478075027466, acc: 0.5666666626930237)
[2025-02-04 00:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:34][root][INFO] - Training Epoch: 2/2, step 18312/23838 completed (loss: 3.0237536430358887, acc: 0.5249999761581421)
[2025-02-04 00:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:35][root][INFO] - Training Epoch: 2/2, step 18313/23838 completed (loss: 3.6514739990234375, acc: 0.39534884691238403)
[2025-02-04 00:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:35][root][INFO] - Training Epoch: 2/2, step 18314/23838 completed (loss: 2.3638088703155518, acc: 0.4571428596973419)
[2025-02-04 00:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:36][root][INFO] - Training Epoch: 2/2, step 18315/23838 completed (loss: 3.040722131729126, acc: 0.42307692766189575)
[2025-02-04 00:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:36][root][INFO] - Training Epoch: 2/2, step 18316/23838 completed (loss: 4.766801834106445, acc: 0.21052631735801697)
[2025-02-04 00:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:37][root][INFO] - Training Epoch: 2/2, step 18317/23838 completed (loss: 1.67879056930542, acc: 0.6399999856948853)
[2025-02-04 00:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:37][root][INFO] - Training Epoch: 2/2, step 18318/23838 completed (loss: 1.9594104290008545, acc: 0.4545454680919647)
[2025-02-04 00:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:38][root][INFO] - Training Epoch: 2/2, step 18319/23838 completed (loss: 3.666252374649048, acc: 0.3214285671710968)
[2025-02-04 00:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:38][root][INFO] - Training Epoch: 2/2, step 18320/23838 completed (loss: 4.804561614990234, acc: 0.2857142984867096)
[2025-02-04 00:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:39][root][INFO] - Training Epoch: 2/2, step 18321/23838 completed (loss: 3.66377854347229, acc: 0.31707316637039185)
[2025-02-04 00:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:39][root][INFO] - Training Epoch: 2/2, step 18322/23838 completed (loss: 3.0925302505493164, acc: 0.40740740299224854)
[2025-02-04 00:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:40][root][INFO] - Training Epoch: 2/2, step 18323/23838 completed (loss: 3.7687268257141113, acc: 0.375)
[2025-02-04 00:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:40][root][INFO] - Training Epoch: 2/2, step 18324/23838 completed (loss: 2.9740610122680664, acc: 0.44999998807907104)
[2025-02-04 00:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:40][root][INFO] - Training Epoch: 2/2, step 18325/23838 completed (loss: 3.296361207962036, acc: 0.3333333432674408)
[2025-02-04 00:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:41][root][INFO] - Training Epoch: 2/2, step 18326/23838 completed (loss: 3.622258424758911, acc: 0.3030303120613098)
[2025-02-04 00:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:41][root][INFO] - Training Epoch: 2/2, step 18327/23838 completed (loss: 3.3637454509735107, acc: 0.5135135054588318)
[2025-02-04 00:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:42][root][INFO] - Training Epoch: 2/2, step 18328/23838 completed (loss: 3.5519230365753174, acc: 0.3095238208770752)
[2025-02-04 00:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:42][root][INFO] - Training Epoch: 2/2, step 18329/23838 completed (loss: 3.819833755493164, acc: 0.34246575832366943)
[2025-02-04 00:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:43][root][INFO] - Training Epoch: 2/2, step 18330/23838 completed (loss: 3.0740485191345215, acc: 0.23529411852359772)
[2025-02-04 00:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:43][root][INFO] - Training Epoch: 2/2, step 18331/23838 completed (loss: 2.908247232437134, acc: 0.46666666865348816)
[2025-02-04 00:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:44][root][INFO] - Training Epoch: 2/2, step 18332/23838 completed (loss: 3.444739818572998, acc: 0.3333333432674408)
[2025-02-04 00:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:44][root][INFO] - Training Epoch: 2/2, step 18333/23838 completed (loss: 3.746940851211548, acc: 0.3265306055545807)
[2025-02-04 00:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:45][root][INFO] - Training Epoch: 2/2, step 18334/23838 completed (loss: 3.3640363216400146, acc: 0.375)
[2025-02-04 00:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:45][root][INFO] - Training Epoch: 2/2, step 18335/23838 completed (loss: 2.561044454574585, acc: 0.4054054021835327)
[2025-02-04 00:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:46][root][INFO] - Training Epoch: 2/2, step 18336/23838 completed (loss: 3.41268253326416, acc: 0.3611111044883728)
[2025-02-04 00:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:46][root][INFO] - Training Epoch: 2/2, step 18337/23838 completed (loss: 3.7483577728271484, acc: 0.28947368264198303)
[2025-02-04 00:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:47][root][INFO] - Training Epoch: 2/2, step 18338/23838 completed (loss: 2.7832934856414795, acc: 0.41025641560554504)
[2025-02-04 00:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:47][root][INFO] - Training Epoch: 2/2, step 18339/23838 completed (loss: 3.4443624019622803, acc: 0.47999998927116394)
[2025-02-04 00:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:48][root][INFO] - Training Epoch: 2/2, step 18340/23838 completed (loss: 3.8143019676208496, acc: 0.3478260934352875)
[2025-02-04 00:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:48][root][INFO] - Training Epoch: 2/2, step 18341/23838 completed (loss: 4.0312981605529785, acc: 0.3181818127632141)
[2025-02-04 00:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:49][root][INFO] - Training Epoch: 2/2, step 18342/23838 completed (loss: 3.928420305252075, acc: 0.17499999701976776)
[2025-02-04 00:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:49][root][INFO] - Training Epoch: 2/2, step 18343/23838 completed (loss: 4.141129970550537, acc: 0.38235294818878174)
[2025-02-04 00:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:50][root][INFO] - Training Epoch: 2/2, step 18344/23838 completed (loss: 2.7087833881378174, acc: 0.3448275923728943)
[2025-02-04 00:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:50][root][INFO] - Training Epoch: 2/2, step 18345/23838 completed (loss: 2.8484599590301514, acc: 0.4838709533214569)
[2025-02-04 00:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:51][root][INFO] - Training Epoch: 2/2, step 18346/23838 completed (loss: 1.9422012567520142, acc: 0.5833333134651184)
[2025-02-04 00:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:51][root][INFO] - Training Epoch: 2/2, step 18347/23838 completed (loss: 3.1165244579315186, acc: 0.4324324429035187)
[2025-02-04 00:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:51][root][INFO] - Training Epoch: 2/2, step 18348/23838 completed (loss: 3.2995083332061768, acc: 0.44117647409439087)
[2025-02-04 00:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:52][root][INFO] - Training Epoch: 2/2, step 18349/23838 completed (loss: 2.856370687484741, acc: 0.3684210479259491)
[2025-02-04 00:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:52][root][INFO] - Training Epoch: 2/2, step 18350/23838 completed (loss: 3.898031234741211, acc: 0.3947368562221527)
[2025-02-04 00:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:53][root][INFO] - Training Epoch: 2/2, step 18351/23838 completed (loss: 3.8081319332122803, acc: 0.30909091234207153)
[2025-02-04 00:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:54][root][INFO] - Training Epoch: 2/2, step 18352/23838 completed (loss: 2.0834763050079346, acc: 0.5945945978164673)
[2025-02-04 00:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:54][root][INFO] - Training Epoch: 2/2, step 18353/23838 completed (loss: 3.068300485610962, acc: 0.5454545617103577)
[2025-02-04 00:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:55][root][INFO] - Training Epoch: 2/2, step 18354/23838 completed (loss: 2.7232284545898438, acc: 0.4000000059604645)
[2025-02-04 00:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:55][root][INFO] - Training Epoch: 2/2, step 18355/23838 completed (loss: 2.0982725620269775, acc: 0.5681818127632141)
[2025-02-04 00:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:56][root][INFO] - Training Epoch: 2/2, step 18356/23838 completed (loss: 1.774477481842041, acc: 0.6000000238418579)
[2025-02-04 00:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:56][root][INFO] - Training Epoch: 2/2, step 18357/23838 completed (loss: 4.0117974281311035, acc: 0.3103448152542114)
[2025-02-04 00:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:56][root][INFO] - Training Epoch: 2/2, step 18358/23838 completed (loss: 1.997246503829956, acc: 0.52173912525177)
[2025-02-04 00:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:57][root][INFO] - Training Epoch: 2/2, step 18359/23838 completed (loss: 2.5578958988189697, acc: 0.6818181872367859)
[2025-02-04 00:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:57][root][INFO] - Training Epoch: 2/2, step 18360/23838 completed (loss: 3.102437734603882, acc: 0.40909090638160706)
[2025-02-04 00:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:58][root][INFO] - Training Epoch: 2/2, step 18361/23838 completed (loss: 2.0526256561279297, acc: 0.47999998927116394)
[2025-02-04 00:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:58][root][INFO] - Training Epoch: 2/2, step 18362/23838 completed (loss: 2.6494016647338867, acc: 0.46666666865348816)
[2025-02-04 00:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:59][root][INFO] - Training Epoch: 2/2, step 18363/23838 completed (loss: 2.4088168144226074, acc: 0.5517241358757019)
[2025-02-04 00:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:36:59][root][INFO] - Training Epoch: 2/2, step 18364/23838 completed (loss: 2.5095443725585938, acc: 0.47058823704719543)
[2025-02-04 00:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:00][root][INFO] - Training Epoch: 2/2, step 18365/23838 completed (loss: 2.8308281898498535, acc: 0.4838709533214569)
[2025-02-04 00:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:00][root][INFO] - Training Epoch: 2/2, step 18366/23838 completed (loss: 3.172447443008423, acc: 0.42307692766189575)
[2025-02-04 00:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:00][root][INFO] - Training Epoch: 2/2, step 18367/23838 completed (loss: 3.522071123123169, acc: 0.3913043439388275)
[2025-02-04 00:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:01][root][INFO] - Training Epoch: 2/2, step 18368/23838 completed (loss: 2.8876893520355225, acc: 0.4166666567325592)
[2025-02-04 00:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:01][root][INFO] - Training Epoch: 2/2, step 18369/23838 completed (loss: 3.800407648086548, acc: 0.4000000059604645)
[2025-02-04 00:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:02][root][INFO] - Training Epoch: 2/2, step 18370/23838 completed (loss: 3.47743821144104, acc: 0.3529411852359772)
[2025-02-04 00:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:02][root][INFO] - Training Epoch: 2/2, step 18371/23838 completed (loss: 2.100337266921997, acc: 0.6000000238418579)
[2025-02-04 00:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:03][root][INFO] - Training Epoch: 2/2, step 18372/23838 completed (loss: 2.0978620052337646, acc: 0.5199999809265137)
[2025-02-04 00:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:03][root][INFO] - Training Epoch: 2/2, step 18373/23838 completed (loss: 3.057374954223633, acc: 0.5)
[2025-02-04 00:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:04][root][INFO] - Training Epoch: 2/2, step 18374/23838 completed (loss: 3.8909108638763428, acc: 0.4285714328289032)
[2025-02-04 00:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:04][root][INFO] - Training Epoch: 2/2, step 18375/23838 completed (loss: 3.459852933883667, acc: 0.4749999940395355)
[2025-02-04 00:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:05][root][INFO] - Training Epoch: 2/2, step 18376/23838 completed (loss: 2.8679721355438232, acc: 0.2916666567325592)
[2025-02-04 00:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:05][root][INFO] - Training Epoch: 2/2, step 18377/23838 completed (loss: 2.889913320541382, acc: 0.4117647111415863)
[2025-02-04 00:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:06][root][INFO] - Training Epoch: 2/2, step 18378/23838 completed (loss: 2.437215566635132, acc: 0.5714285969734192)
[2025-02-04 00:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:06][root][INFO] - Training Epoch: 2/2, step 18379/23838 completed (loss: 2.0736758708953857, acc: 0.6111111044883728)
[2025-02-04 00:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:06][root][INFO] - Training Epoch: 2/2, step 18380/23838 completed (loss: 3.4661943912506104, acc: 0.4324324429035187)
[2025-02-04 00:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:07][root][INFO] - Training Epoch: 2/2, step 18381/23838 completed (loss: 2.3816213607788086, acc: 0.5769230723381042)
[2025-02-04 00:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:07][root][INFO] - Training Epoch: 2/2, step 18382/23838 completed (loss: 2.6105759143829346, acc: 0.5)
[2025-02-04 00:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:08][root][INFO] - Training Epoch: 2/2, step 18383/23838 completed (loss: 2.679241180419922, acc: 0.6388888955116272)
[2025-02-04 00:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:08][root][INFO] - Training Epoch: 2/2, step 18384/23838 completed (loss: 2.973473072052002, acc: 0.39024388790130615)
[2025-02-04 00:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:09][root][INFO] - Training Epoch: 2/2, step 18385/23838 completed (loss: 2.4588706493377686, acc: 0.4838709533214569)
[2025-02-04 00:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:09][root][INFO] - Training Epoch: 2/2, step 18386/23838 completed (loss: 2.001677989959717, acc: 0.6000000238418579)
[2025-02-04 00:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:10][root][INFO] - Training Epoch: 2/2, step 18387/23838 completed (loss: 1.412429928779602, acc: 0.6428571343421936)
[2025-02-04 00:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:10][root][INFO] - Training Epoch: 2/2, step 18388/23838 completed (loss: 3.008692741394043, acc: 0.48275861144065857)
[2025-02-04 00:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:11][root][INFO] - Training Epoch: 2/2, step 18389/23838 completed (loss: 2.833986759185791, acc: 0.5135135054588318)
[2025-02-04 00:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:11][root][INFO] - Training Epoch: 2/2, step 18390/23838 completed (loss: 3.0908453464508057, acc: 0.5)
[2025-02-04 00:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:12][root][INFO] - Training Epoch: 2/2, step 18391/23838 completed (loss: 2.628970146179199, acc: 0.5277777910232544)
[2025-02-04 00:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:12][root][INFO] - Training Epoch: 2/2, step 18392/23838 completed (loss: 1.9513871669769287, acc: 0.550000011920929)
[2025-02-04 00:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:13][root][INFO] - Training Epoch: 2/2, step 18393/23838 completed (loss: 3.8137474060058594, acc: 0.41860464215278625)
[2025-02-04 00:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:13][root][INFO] - Training Epoch: 2/2, step 18394/23838 completed (loss: 2.8085715770721436, acc: 0.4583333432674408)
[2025-02-04 00:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:14][root][INFO] - Training Epoch: 2/2, step 18395/23838 completed (loss: 3.521568536758423, acc: 0.40816327929496765)
[2025-02-04 00:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:14][root][INFO] - Training Epoch: 2/2, step 18396/23838 completed (loss: 2.0607357025146484, acc: 0.5600000023841858)
[2025-02-04 00:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:15][root][INFO] - Training Epoch: 2/2, step 18397/23838 completed (loss: 2.462479829788208, acc: 0.5)
[2025-02-04 00:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:15][root][INFO] - Training Epoch: 2/2, step 18398/23838 completed (loss: 2.730003595352173, acc: 0.5081967115402222)
[2025-02-04 00:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:16][root][INFO] - Training Epoch: 2/2, step 18399/23838 completed (loss: 1.8781468868255615, acc: 0.5555555820465088)
[2025-02-04 00:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:16][root][INFO] - Training Epoch: 2/2, step 18400/23838 completed (loss: 2.7725093364715576, acc: 0.43589743971824646)
[2025-02-04 00:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:16][root][INFO] - Training Epoch: 2/2, step 18401/23838 completed (loss: 2.200479507446289, acc: 0.4285714328289032)
[2025-02-04 00:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:17][root][INFO] - Training Epoch: 2/2, step 18402/23838 completed (loss: 2.1100685596466064, acc: 0.5)
[2025-02-04 00:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:17][root][INFO] - Training Epoch: 2/2, step 18403/23838 completed (loss: 1.9498926401138306, acc: 0.5869565010070801)
[2025-02-04 00:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:18][root][INFO] - Training Epoch: 2/2, step 18404/23838 completed (loss: 3.179292678833008, acc: 0.38297873735427856)
[2025-02-04 00:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:18][root][INFO] - Training Epoch: 2/2, step 18405/23838 completed (loss: 2.871950626373291, acc: 0.4285714328289032)
[2025-02-04 00:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:19][root][INFO] - Training Epoch: 2/2, step 18406/23838 completed (loss: 2.299145460128784, acc: 0.5625)
[2025-02-04 00:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:19][root][INFO] - Training Epoch: 2/2, step 18407/23838 completed (loss: 2.6155526638031006, acc: 0.5416666865348816)
[2025-02-04 00:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:20][root][INFO] - Training Epoch: 2/2, step 18408/23838 completed (loss: 2.7051002979278564, acc: 0.5909090638160706)
[2025-02-04 00:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:20][root][INFO] - Training Epoch: 2/2, step 18409/23838 completed (loss: 2.7485876083374023, acc: 0.5)
[2025-02-04 00:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:21][root][INFO] - Training Epoch: 2/2, step 18410/23838 completed (loss: 2.3948185443878174, acc: 0.6190476417541504)
[2025-02-04 00:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:21][root][INFO] - Training Epoch: 2/2, step 18411/23838 completed (loss: 3.277188777923584, acc: 0.44999998807907104)
[2025-02-04 00:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:22][root][INFO] - Training Epoch: 2/2, step 18412/23838 completed (loss: 3.2946622371673584, acc: 0.4285714328289032)
[2025-02-04 00:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:22][root][INFO] - Training Epoch: 2/2, step 18413/23838 completed (loss: 3.642937660217285, acc: 0.4615384638309479)
[2025-02-04 00:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:23][root][INFO] - Training Epoch: 2/2, step 18414/23838 completed (loss: 2.431455135345459, acc: 0.4375)
[2025-02-04 00:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:23][root][INFO] - Training Epoch: 2/2, step 18415/23838 completed (loss: 2.558134078979492, acc: 0.43589743971824646)
[2025-02-04 00:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:24][root][INFO] - Training Epoch: 2/2, step 18416/23838 completed (loss: 2.7398464679718018, acc: 0.38333332538604736)
[2025-02-04 00:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:24][root][INFO] - Training Epoch: 2/2, step 18417/23838 completed (loss: 2.853691577911377, acc: 0.4545454680919647)
[2025-02-04 00:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:25][root][INFO] - Training Epoch: 2/2, step 18418/23838 completed (loss: 2.590226411819458, acc: 0.5151515007019043)
[2025-02-04 00:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:25][root][INFO] - Training Epoch: 2/2, step 18419/23838 completed (loss: 1.8162879943847656, acc: 0.6176470518112183)
[2025-02-04 00:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:26][root][INFO] - Training Epoch: 2/2, step 18420/23838 completed (loss: 2.5146543979644775, acc: 0.47826087474823)
[2025-02-04 00:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:26][root][INFO] - Training Epoch: 2/2, step 18421/23838 completed (loss: 2.414022445678711, acc: 0.5526315569877625)
[2025-02-04 00:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:27][root][INFO] - Training Epoch: 2/2, step 18422/23838 completed (loss: 2.27093505859375, acc: 0.5)
[2025-02-04 00:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:27][root][INFO] - Training Epoch: 2/2, step 18423/23838 completed (loss: 1.8965086936950684, acc: 0.6486486196517944)
[2025-02-04 00:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:28][root][INFO] - Training Epoch: 2/2, step 18424/23838 completed (loss: 3.539038896560669, acc: 0.3333333432674408)
[2025-02-04 00:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:28][root][INFO] - Training Epoch: 2/2, step 18425/23838 completed (loss: 3.0484278202056885, acc: 0.42553192377090454)
[2025-02-04 00:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:29][root][INFO] - Training Epoch: 2/2, step 18426/23838 completed (loss: 2.447950839996338, acc: 0.523809552192688)
[2025-02-04 00:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:29][root][INFO] - Training Epoch: 2/2, step 18427/23838 completed (loss: 2.192850351333618, acc: 0.5625)
[2025-02-04 00:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:30][root][INFO] - Training Epoch: 2/2, step 18428/23838 completed (loss: 2.765437602996826, acc: 0.45098039507865906)
[2025-02-04 00:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:30][root][INFO] - Training Epoch: 2/2, step 18429/23838 completed (loss: 2.1473653316497803, acc: 0.6086956262588501)
[2025-02-04 00:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:30][root][INFO] - Training Epoch: 2/2, step 18430/23838 completed (loss: 2.435606002807617, acc: 0.44897958636283875)
[2025-02-04 00:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:31][root][INFO] - Training Epoch: 2/2, step 18431/23838 completed (loss: 1.6619045734405518, acc: 0.6428571343421936)
[2025-02-04 00:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:31][root][INFO] - Training Epoch: 2/2, step 18432/23838 completed (loss: 2.6464180946350098, acc: 0.44999998807907104)
[2025-02-04 00:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:32][root][INFO] - Training Epoch: 2/2, step 18433/23838 completed (loss: 2.8922181129455566, acc: 0.5641025900840759)
[2025-02-04 00:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:32][root][INFO] - Training Epoch: 2/2, step 18434/23838 completed (loss: 2.9470832347869873, acc: 0.4399999976158142)
[2025-02-04 00:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:33][root][INFO] - Training Epoch: 2/2, step 18435/23838 completed (loss: 2.290055751800537, acc: 0.5714285969734192)
[2025-02-04 00:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:33][root][INFO] - Training Epoch: 2/2, step 18436/23838 completed (loss: 2.2253577709198, acc: 0.3888888955116272)
[2025-02-04 00:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:34][root][INFO] - Training Epoch: 2/2, step 18437/23838 completed (loss: 2.9501585960388184, acc: 0.4285714328289032)
[2025-02-04 00:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:34][root][INFO] - Training Epoch: 2/2, step 18438/23838 completed (loss: 1.8845208883285522, acc: 0.7083333134651184)
[2025-02-04 00:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:34][root][INFO] - Training Epoch: 2/2, step 18439/23838 completed (loss: 2.165961265563965, acc: 0.6274510025978088)
[2025-02-04 00:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:35][root][INFO] - Training Epoch: 2/2, step 18440/23838 completed (loss: 1.8905746936798096, acc: 0.6000000238418579)
[2025-02-04 00:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:35][root][INFO] - Training Epoch: 2/2, step 18441/23838 completed (loss: 2.580096960067749, acc: 0.4761904776096344)
[2025-02-04 00:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:36][root][INFO] - Training Epoch: 2/2, step 18442/23838 completed (loss: 2.8295605182647705, acc: 0.4736842215061188)
[2025-02-04 00:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:36][root][INFO] - Training Epoch: 2/2, step 18443/23838 completed (loss: 3.3342132568359375, acc: 0.4864864945411682)
[2025-02-04 00:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:36][root][INFO] - Training Epoch: 2/2, step 18444/23838 completed (loss: 3.515798568725586, acc: 0.4482758641242981)
[2025-02-04 00:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:37][root][INFO] - Training Epoch: 2/2, step 18445/23838 completed (loss: 3.371609926223755, acc: 0.3636363744735718)
[2025-02-04 00:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:37][root][INFO] - Training Epoch: 2/2, step 18446/23838 completed (loss: 2.1246421337127686, acc: 0.6000000238418579)
[2025-02-04 00:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:38][root][INFO] - Training Epoch: 2/2, step 18447/23838 completed (loss: 2.9973556995391846, acc: 0.46875)
[2025-02-04 00:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:38][root][INFO] - Training Epoch: 2/2, step 18448/23838 completed (loss: 3.288745641708374, acc: 0.4399999976158142)
[2025-02-04 00:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:39][root][INFO] - Training Epoch: 2/2, step 18449/23838 completed (loss: 4.273425102233887, acc: 0.3076923191547394)
[2025-02-04 00:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:39][root][INFO] - Training Epoch: 2/2, step 18450/23838 completed (loss: 2.6040570735931396, acc: 0.3199999928474426)
[2025-02-04 00:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:40][root][INFO] - Training Epoch: 2/2, step 18451/23838 completed (loss: 3.243248224258423, acc: 0.3636363744735718)
[2025-02-04 00:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:40][root][INFO] - Training Epoch: 2/2, step 18452/23838 completed (loss: 2.518280029296875, acc: 0.5454545617103577)
[2025-02-04 00:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:41][root][INFO] - Training Epoch: 2/2, step 18453/23838 completed (loss: 2.398541212081909, acc: 0.5333333611488342)
[2025-02-04 00:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:41][root][INFO] - Training Epoch: 2/2, step 18454/23838 completed (loss: 1.746049404144287, acc: 0.4615384638309479)
[2025-02-04 00:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:41][root][INFO] - Training Epoch: 2/2, step 18455/23838 completed (loss: 2.9774434566497803, acc: 0.5714285969734192)
[2025-02-04 00:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:42][root][INFO] - Training Epoch: 2/2, step 18456/23838 completed (loss: 2.158818006515503, acc: 0.6153846383094788)
[2025-02-04 00:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:42][root][INFO] - Training Epoch: 2/2, step 18457/23838 completed (loss: 2.082449436187744, acc: 0.5)
[2025-02-04 00:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:43][root][INFO] - Training Epoch: 2/2, step 18458/23838 completed (loss: 3.9559221267700195, acc: 0.4545454680919647)
[2025-02-04 00:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:43][root][INFO] - Training Epoch: 2/2, step 18459/23838 completed (loss: 3.4208455085754395, acc: 0.3333333432674408)
[2025-02-04 00:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:44][root][INFO] - Training Epoch: 2/2, step 18460/23838 completed (loss: 1.8852736949920654, acc: 0.5333333611488342)
[2025-02-04 00:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:44][root][INFO] - Training Epoch: 2/2, step 18461/23838 completed (loss: 3.9350662231445312, acc: 0.3333333432674408)
[2025-02-04 00:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:45][root][INFO] - Training Epoch: 2/2, step 18462/23838 completed (loss: 2.6803016662597656, acc: 0.5882353186607361)
[2025-02-04 00:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:45][root][INFO] - Training Epoch: 2/2, step 18463/23838 completed (loss: 3.139097213745117, acc: 0.6399999856948853)
[2025-02-04 00:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:46][root][INFO] - Training Epoch: 2/2, step 18464/23838 completed (loss: 1.504128336906433, acc: 0.6666666865348816)
[2025-02-04 00:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:46][root][INFO] - Training Epoch: 2/2, step 18465/23838 completed (loss: 3.322897434234619, acc: 0.380952388048172)
[2025-02-04 00:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:47][root][INFO] - Training Epoch: 2/2, step 18466/23838 completed (loss: 3.341480255126953, acc: 0.4444444477558136)
[2025-02-04 00:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:47][root][INFO] - Training Epoch: 2/2, step 18467/23838 completed (loss: 2.6246931552886963, acc: 0.3928571343421936)
[2025-02-04 00:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:48][root][INFO] - Training Epoch: 2/2, step 18468/23838 completed (loss: 4.3959808349609375, acc: 0.34090909361839294)
[2025-02-04 00:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:48][root][INFO] - Training Epoch: 2/2, step 18469/23838 completed (loss: 3.741041898727417, acc: 0.4054054021835327)
[2025-02-04 00:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:49][root][INFO] - Training Epoch: 2/2, step 18470/23838 completed (loss: 2.96110463142395, acc: 0.4137931168079376)
[2025-02-04 00:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:49][root][INFO] - Training Epoch: 2/2, step 18471/23838 completed (loss: 2.6587297916412354, acc: 0.4375)
[2025-02-04 00:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:50][root][INFO] - Training Epoch: 2/2, step 18472/23838 completed (loss: 3.0709235668182373, acc: 0.37142857909202576)
[2025-02-04 00:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:50][root][INFO] - Training Epoch: 2/2, step 18473/23838 completed (loss: 2.3035833835601807, acc: 0.6000000238418579)
[2025-02-04 00:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:51][root][INFO] - Training Epoch: 2/2, step 18474/23838 completed (loss: 3.7151787281036377, acc: 0.3928571343421936)
[2025-02-04 00:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:51][root][INFO] - Training Epoch: 2/2, step 18475/23838 completed (loss: 2.917266368865967, acc: 0.40625)
[2025-02-04 00:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:52][root][INFO] - Training Epoch: 2/2, step 18476/23838 completed (loss: 2.776912212371826, acc: 0.4761904776096344)
[2025-02-04 00:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:52][root][INFO] - Training Epoch: 2/2, step 18477/23838 completed (loss: 3.2816665172576904, acc: 0.4117647111415863)
[2025-02-04 00:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:52][root][INFO] - Training Epoch: 2/2, step 18478/23838 completed (loss: 2.0316224098205566, acc: 0.5333333611488342)
[2025-02-04 00:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:53][root][INFO] - Training Epoch: 2/2, step 18479/23838 completed (loss: 2.765329599380493, acc: 0.4333333373069763)
[2025-02-04 00:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:54][root][INFO] - Training Epoch: 2/2, step 18480/23838 completed (loss: 3.1625936031341553, acc: 0.3968254029750824)
[2025-02-04 00:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:54][root][INFO] - Training Epoch: 2/2, step 18481/23838 completed (loss: 3.669908285140991, acc: 0.3636363744735718)
[2025-02-04 00:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:54][root][INFO] - Training Epoch: 2/2, step 18482/23838 completed (loss: 2.6854326725006104, acc: 0.47058823704719543)
[2025-02-04 00:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:55][root][INFO] - Training Epoch: 2/2, step 18483/23838 completed (loss: 3.18692684173584, acc: 0.6000000238418579)
[2025-02-04 00:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:55][root][INFO] - Training Epoch: 2/2, step 18484/23838 completed (loss: 3.053266763687134, acc: 0.4482758641242981)
[2025-02-04 00:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:56][root][INFO] - Training Epoch: 2/2, step 18485/23838 completed (loss: 3.562821865081787, acc: 0.3720930218696594)
[2025-02-04 00:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:56][root][INFO] - Training Epoch: 2/2, step 18486/23838 completed (loss: 2.5512940883636475, acc: 0.4642857015132904)
[2025-02-04 00:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:57][root][INFO] - Training Epoch: 2/2, step 18487/23838 completed (loss: 1.8142166137695312, acc: 0.6315789222717285)
[2025-02-04 00:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:57][root][INFO] - Training Epoch: 2/2, step 18488/23838 completed (loss: 3.2617971897125244, acc: 0.35483869910240173)
[2025-02-04 00:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:57][root][INFO] - Training Epoch: 2/2, step 18489/23838 completed (loss: 3.463353395462036, acc: 0.37037035822868347)
[2025-02-04 00:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:58][root][INFO] - Training Epoch: 2/2, step 18490/23838 completed (loss: 3.4133963584899902, acc: 0.23255814611911774)
[2025-02-04 00:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:58][root][INFO] - Training Epoch: 2/2, step 18491/23838 completed (loss: 2.1128556728363037, acc: 0.5909090638160706)
[2025-02-04 00:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:59][root][INFO] - Training Epoch: 2/2, step 18492/23838 completed (loss: 2.651761054992676, acc: 0.49056604504585266)
[2025-02-04 00:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:37:59][root][INFO] - Training Epoch: 2/2, step 18493/23838 completed (loss: 3.0298359394073486, acc: 0.5)
[2025-02-04 00:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:00][root][INFO] - Training Epoch: 2/2, step 18494/23838 completed (loss: 4.171620845794678, acc: 0.31111112236976624)
[2025-02-04 00:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:00][root][INFO] - Training Epoch: 2/2, step 18495/23838 completed (loss: 2.799081563949585, acc: 0.4615384638309479)
[2025-02-04 00:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:01][root][INFO] - Training Epoch: 2/2, step 18496/23838 completed (loss: 2.585340738296509, acc: 0.47058823704719543)
[2025-02-04 00:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:01][root][INFO] - Training Epoch: 2/2, step 18497/23838 completed (loss: 2.722184658050537, acc: 0.4545454680919647)
[2025-02-04 00:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:01][root][INFO] - Training Epoch: 2/2, step 18498/23838 completed (loss: 3.274928569793701, acc: 0.43478259444236755)
[2025-02-04 00:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:02][root][INFO] - Training Epoch: 2/2, step 18499/23838 completed (loss: 1.908781886100769, acc: 0.6000000238418579)
[2025-02-04 00:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:02][root][INFO] - Training Epoch: 2/2, step 18500/23838 completed (loss: 2.7379088401794434, acc: 0.523809552192688)
[2025-02-04 00:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:03][root][INFO] - Training Epoch: 2/2, step 18501/23838 completed (loss: 2.753657579421997, acc: 0.36666667461395264)
[2025-02-04 00:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:03][root][INFO] - Training Epoch: 2/2, step 18502/23838 completed (loss: 2.029317855834961, acc: 0.5416666865348816)
[2025-02-04 00:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:04][root][INFO] - Training Epoch: 2/2, step 18503/23838 completed (loss: 3.1144778728485107, acc: 0.523809552192688)
[2025-02-04 00:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:04][root][INFO] - Training Epoch: 2/2, step 18504/23838 completed (loss: 3.5415685176849365, acc: 0.3214285671710968)
[2025-02-04 00:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:05][root][INFO] - Training Epoch: 2/2, step 18505/23838 completed (loss: 3.9896936416625977, acc: 0.4583333432674408)
[2025-02-04 00:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:05][root][INFO] - Training Epoch: 2/2, step 18506/23838 completed (loss: 1.7194621562957764, acc: 0.6000000238418579)
[2025-02-04 00:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:06][root][INFO] - Training Epoch: 2/2, step 18507/23838 completed (loss: 2.242300033569336, acc: 0.4444444477558136)
[2025-02-04 00:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:06][root][INFO] - Training Epoch: 2/2, step 18508/23838 completed (loss: 2.168727159500122, acc: 0.5882353186607361)
[2025-02-04 00:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:06][root][INFO] - Training Epoch: 2/2, step 18509/23838 completed (loss: 2.0846078395843506, acc: 0.6000000238418579)
[2025-02-04 00:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:07][root][INFO] - Training Epoch: 2/2, step 18510/23838 completed (loss: 1.8904203176498413, acc: 0.6666666865348816)
[2025-02-04 00:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:07][root][INFO] - Training Epoch: 2/2, step 18511/23838 completed (loss: 2.387941837310791, acc: 0.5)
[2025-02-04 00:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:07][root][INFO] - Training Epoch: 2/2, step 18512/23838 completed (loss: 1.9544563293457031, acc: 0.4399999976158142)
[2025-02-04 00:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:08][root][INFO] - Training Epoch: 2/2, step 18513/23838 completed (loss: 1.3426399230957031, acc: 0.6000000238418579)
[2025-02-04 00:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:08][root][INFO] - Training Epoch: 2/2, step 18514/23838 completed (loss: 2.7483625411987305, acc: 0.4545454680919647)
[2025-02-04 00:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:09][root][INFO] - Training Epoch: 2/2, step 18515/23838 completed (loss: 1.7120057344436646, acc: 0.5185185074806213)
[2025-02-04 00:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:09][root][INFO] - Training Epoch: 2/2, step 18516/23838 completed (loss: 1.7621949911117554, acc: 0.5833333134651184)
[2025-02-04 00:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:09][root][INFO] - Training Epoch: 2/2, step 18517/23838 completed (loss: 2.1918087005615234, acc: 0.5185185074806213)
[2025-02-04 00:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:10][root][INFO] - Training Epoch: 2/2, step 18518/23838 completed (loss: 2.556453227996826, acc: 0.44736841320991516)
[2025-02-04 00:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:10][root][INFO] - Training Epoch: 2/2, step 18519/23838 completed (loss: 3.3518903255462646, acc: 0.44736841320991516)
[2025-02-04 00:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:11][root][INFO] - Training Epoch: 2/2, step 18520/23838 completed (loss: 3.042102336883545, acc: 0.46666666865348816)
[2025-02-04 00:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:11][root][INFO] - Training Epoch: 2/2, step 18521/23838 completed (loss: 1.9908075332641602, acc: 0.5757575631141663)
[2025-02-04 00:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:12][root][INFO] - Training Epoch: 2/2, step 18522/23838 completed (loss: 2.598634958267212, acc: 0.5)
[2025-02-04 00:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:12][root][INFO] - Training Epoch: 2/2, step 18523/23838 completed (loss: 2.402245044708252, acc: 0.6071428656578064)
[2025-02-04 00:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:13][root][INFO] - Training Epoch: 2/2, step 18524/23838 completed (loss: 2.809262990951538, acc: 0.5)
[2025-02-04 00:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:13][root][INFO] - Training Epoch: 2/2, step 18525/23838 completed (loss: 3.5740811824798584, acc: 0.2857142984867096)
[2025-02-04 00:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:14][root][INFO] - Training Epoch: 2/2, step 18526/23838 completed (loss: 2.4584367275238037, acc: 0.375)
[2025-02-04 00:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:14][root][INFO] - Training Epoch: 2/2, step 18527/23838 completed (loss: 4.356250762939453, acc: 0.32499998807907104)
[2025-02-04 00:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:15][root][INFO] - Training Epoch: 2/2, step 18528/23838 completed (loss: 3.2154483795166016, acc: 0.380952388048172)
[2025-02-04 00:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:15][root][INFO] - Training Epoch: 2/2, step 18529/23838 completed (loss: 2.9936881065368652, acc: 0.5)
[2025-02-04 00:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:16][root][INFO] - Training Epoch: 2/2, step 18530/23838 completed (loss: 2.094252824783325, acc: 0.5)
[2025-02-04 00:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:16][root][INFO] - Training Epoch: 2/2, step 18531/23838 completed (loss: 1.2756402492523193, acc: 0.5714285969734192)
[2025-02-04 00:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:16][root][INFO] - Training Epoch: 2/2, step 18532/23838 completed (loss: 3.091325521469116, acc: 0.42307692766189575)
[2025-02-04 00:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:17][root][INFO] - Training Epoch: 2/2, step 18533/23838 completed (loss: 1.620652198791504, acc: 0.43478259444236755)
[2025-02-04 00:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:17][root][INFO] - Training Epoch: 2/2, step 18534/23838 completed (loss: 3.0049774646759033, acc: 0.375)
[2025-02-04 00:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:18][root][INFO] - Training Epoch: 2/2, step 18535/23838 completed (loss: 2.0836708545684814, acc: 0.52173912525177)
[2025-02-04 00:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:18][root][INFO] - Training Epoch: 2/2, step 18536/23838 completed (loss: 2.8418397903442383, acc: 0.3611111044883728)
[2025-02-04 00:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:19][root][INFO] - Training Epoch: 2/2, step 18537/23838 completed (loss: 3.1021366119384766, acc: 0.39393940567970276)
[2025-02-04 00:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:19][root][INFO] - Training Epoch: 2/2, step 18538/23838 completed (loss: 2.922607898712158, acc: 0.4000000059604645)
[2025-02-04 00:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:20][root][INFO] - Training Epoch: 2/2, step 18539/23838 completed (loss: 3.097180128097534, acc: 0.3777777850627899)
[2025-02-04 00:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:20][root][INFO] - Training Epoch: 2/2, step 18540/23838 completed (loss: 2.9986958503723145, acc: 0.30188679695129395)
[2025-02-04 00:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:20][root][INFO] - Training Epoch: 2/2, step 18541/23838 completed (loss: 2.133443832397461, acc: 0.5476190447807312)
[2025-02-04 00:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:21][root][INFO] - Training Epoch: 2/2, step 18542/23838 completed (loss: 1.8567824363708496, acc: 0.5714285969734192)
[2025-02-04 00:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:21][root][INFO] - Training Epoch: 2/2, step 18543/23838 completed (loss: 2.628427505493164, acc: 0.2800000011920929)
[2025-02-04 00:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:22][root][INFO] - Training Epoch: 2/2, step 18544/23838 completed (loss: 3.831486225128174, acc: 0.30434781312942505)
[2025-02-04 00:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:22][root][INFO] - Training Epoch: 2/2, step 18545/23838 completed (loss: 1.9804567098617554, acc: 0.5428571701049805)
[2025-02-04 00:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:22][root][INFO] - Training Epoch: 2/2, step 18546/23838 completed (loss: 3.7177486419677734, acc: 0.4117647111415863)
[2025-02-04 00:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:23][root][INFO] - Training Epoch: 2/2, step 18547/23838 completed (loss: 2.420024871826172, acc: 0.4615384638309479)
[2025-02-04 00:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:23][root][INFO] - Training Epoch: 2/2, step 18548/23838 completed (loss: 2.8467772006988525, acc: 0.3947368562221527)
[2025-02-04 00:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:24][root][INFO] - Training Epoch: 2/2, step 18549/23838 completed (loss: 2.6877777576446533, acc: 0.4000000059604645)
[2025-02-04 00:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:24][root][INFO] - Training Epoch: 2/2, step 18550/23838 completed (loss: 3.2539682388305664, acc: 0.3513513505458832)
[2025-02-04 00:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:24][root][INFO] - Training Epoch: 2/2, step 18551/23838 completed (loss: 1.2776691913604736, acc: 0.6551724076271057)
[2025-02-04 00:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:25][root][INFO] - Training Epoch: 2/2, step 18552/23838 completed (loss: 3.0428760051727295, acc: 0.3529411852359772)
[2025-02-04 00:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:25][root][INFO] - Training Epoch: 2/2, step 18553/23838 completed (loss: 1.8133254051208496, acc: 0.6842105388641357)
[2025-02-04 00:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:26][root][INFO] - Training Epoch: 2/2, step 18554/23838 completed (loss: 1.8643819093704224, acc: 0.625)
[2025-02-04 00:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:26][root][INFO] - Training Epoch: 2/2, step 18555/23838 completed (loss: 1.8720687627792358, acc: 0.625)
[2025-02-04 00:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:27][root][INFO] - Training Epoch: 2/2, step 18556/23838 completed (loss: 3.326728582382202, acc: 0.4375)
[2025-02-04 00:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:27][root][INFO] - Training Epoch: 2/2, step 18557/23838 completed (loss: 3.47971248626709, acc: 0.47058823704719543)
[2025-02-04 00:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:28][root][INFO] - Training Epoch: 2/2, step 18558/23838 completed (loss: 2.720720052719116, acc: 0.42222222685813904)
[2025-02-04 00:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:28][root][INFO] - Training Epoch: 2/2, step 18559/23838 completed (loss: 3.391650676727295, acc: 0.3448275923728943)
[2025-02-04 00:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:28][root][INFO] - Training Epoch: 2/2, step 18560/23838 completed (loss: 3.6697540283203125, acc: 0.30909091234207153)
[2025-02-04 00:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:29][root][INFO] - Training Epoch: 2/2, step 18561/23838 completed (loss: 2.815426826477051, acc: 0.4444444477558136)
[2025-02-04 00:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:29][root][INFO] - Training Epoch: 2/2, step 18562/23838 completed (loss: 3.2175991535186768, acc: 0.4390243887901306)
[2025-02-04 00:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:30][root][INFO] - Training Epoch: 2/2, step 18563/23838 completed (loss: 2.9714982509613037, acc: 0.4651162922382355)
[2025-02-04 00:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:30][root][INFO] - Training Epoch: 2/2, step 18564/23838 completed (loss: 1.5186933279037476, acc: 0.65625)
[2025-02-04 00:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:31][root][INFO] - Training Epoch: 2/2, step 18565/23838 completed (loss: 3.1990842819213867, acc: 0.28947368264198303)
[2025-02-04 00:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:31][root][INFO] - Training Epoch: 2/2, step 18566/23838 completed (loss: 3.06834077835083, acc: 0.42307692766189575)
[2025-02-04 00:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:32][root][INFO] - Training Epoch: 2/2, step 18567/23838 completed (loss: 3.1040713787078857, acc: 0.37142857909202576)
[2025-02-04 00:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:32][root][INFO] - Training Epoch: 2/2, step 18568/23838 completed (loss: 1.3652141094207764, acc: 0.7368420958518982)
[2025-02-04 00:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:32][root][INFO] - Training Epoch: 2/2, step 18569/23838 completed (loss: 2.698331832885742, acc: 0.44186046719551086)
[2025-02-04 00:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:33][root][INFO] - Training Epoch: 2/2, step 18570/23838 completed (loss: 3.0049290657043457, acc: 0.4571428596973419)
[2025-02-04 00:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:33][root][INFO] - Training Epoch: 2/2, step 18571/23838 completed (loss: 2.965463161468506, acc: 0.5185185074806213)
[2025-02-04 00:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:34][root][INFO] - Training Epoch: 2/2, step 18572/23838 completed (loss: 2.7359774112701416, acc: 0.44117647409439087)
[2025-02-04 00:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:34][root][INFO] - Training Epoch: 2/2, step 18573/23838 completed (loss: 2.454019546508789, acc: 0.5199999809265137)
[2025-02-04 00:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:35][root][INFO] - Training Epoch: 2/2, step 18574/23838 completed (loss: 3.102879524230957, acc: 0.4516128897666931)
[2025-02-04 00:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:35][root][INFO] - Training Epoch: 2/2, step 18575/23838 completed (loss: 3.050703525543213, acc: 0.5384615659713745)
[2025-02-04 00:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:36][root][INFO] - Training Epoch: 2/2, step 18576/23838 completed (loss: 2.628533124923706, acc: 0.6785714030265808)
[2025-02-04 00:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:36][root][INFO] - Training Epoch: 2/2, step 18577/23838 completed (loss: 1.9876527786254883, acc: 0.5833333134651184)
[2025-02-04 00:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:36][root][INFO] - Training Epoch: 2/2, step 18578/23838 completed (loss: 3.023054599761963, acc: 0.4117647111415863)
[2025-02-04 00:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:37][root][INFO] - Training Epoch: 2/2, step 18579/23838 completed (loss: 0.9861348271369934, acc: 0.6666666865348816)
[2025-02-04 00:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:37][root][INFO] - Training Epoch: 2/2, step 18580/23838 completed (loss: 1.4144142866134644, acc: 0.6818181872367859)
[2025-02-04 00:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:38][root][INFO] - Training Epoch: 2/2, step 18581/23838 completed (loss: 2.6827781200408936, acc: 0.5517241358757019)
[2025-02-04 00:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:38][root][INFO] - Training Epoch: 2/2, step 18582/23838 completed (loss: 2.2375102043151855, acc: 0.5185185074806213)
[2025-02-04 00:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:39][root][INFO] - Training Epoch: 2/2, step 18583/23838 completed (loss: 2.4076743125915527, acc: 0.4482758641242981)
[2025-02-04 00:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:39][root][INFO] - Training Epoch: 2/2, step 18584/23838 completed (loss: 2.812924385070801, acc: 0.4047619104385376)
[2025-02-04 00:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:40][root][INFO] - Training Epoch: 2/2, step 18585/23838 completed (loss: 3.003887176513672, acc: 0.4038461446762085)
[2025-02-04 00:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:40][root][INFO] - Training Epoch: 2/2, step 18586/23838 completed (loss: 2.6715292930603027, acc: 0.47999998927116394)
[2025-02-04 00:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:40][root][INFO] - Training Epoch: 2/2, step 18587/23838 completed (loss: 2.4390599727630615, acc: 0.529411792755127)
[2025-02-04 00:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:41][root][INFO] - Training Epoch: 2/2, step 18588/23838 completed (loss: 2.132453441619873, acc: 0.5714285969734192)
[2025-02-04 00:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:41][root][INFO] - Training Epoch: 2/2, step 18589/23838 completed (loss: 2.5537989139556885, acc: 0.4285714328289032)
[2025-02-04 00:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:42][root][INFO] - Training Epoch: 2/2, step 18590/23838 completed (loss: 1.8034608364105225, acc: 0.6428571343421936)
[2025-02-04 00:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:42][root][INFO] - Training Epoch: 2/2, step 18591/23838 completed (loss: 2.5195887088775635, acc: 0.5)
[2025-02-04 00:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:43][root][INFO] - Training Epoch: 2/2, step 18592/23838 completed (loss: 2.723978281021118, acc: 0.5588235259056091)
[2025-02-04 00:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:43][root][INFO] - Training Epoch: 2/2, step 18593/23838 completed (loss: 2.5446066856384277, acc: 0.45945945382118225)
[2025-02-04 00:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:44][root][INFO] - Training Epoch: 2/2, step 18594/23838 completed (loss: 1.0895527601242065, acc: 0.6666666865348816)
[2025-02-04 00:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:44][root][INFO] - Training Epoch: 2/2, step 18595/23838 completed (loss: 2.3988208770751953, acc: 0.4516128897666931)
[2025-02-04 00:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:45][root][INFO] - Training Epoch: 2/2, step 18596/23838 completed (loss: 2.1598901748657227, acc: 0.4583333432674408)
[2025-02-04 00:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:45][root][INFO] - Training Epoch: 2/2, step 18597/23838 completed (loss: 1.8447102308273315, acc: 0.4838709533214569)
[2025-02-04 00:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:45][root][INFO] - Training Epoch: 2/2, step 18598/23838 completed (loss: 2.2579493522644043, acc: 0.5862069129943848)
[2025-02-04 00:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:46][root][INFO] - Training Epoch: 2/2, step 18599/23838 completed (loss: 1.8180643320083618, acc: 0.6176470518112183)
[2025-02-04 00:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:47][root][INFO] - Training Epoch: 2/2, step 18600/23838 completed (loss: 3.736623764038086, acc: 0.2222222238779068)
[2025-02-04 00:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:47][root][INFO] - Training Epoch: 2/2, step 18601/23838 completed (loss: 3.1831648349761963, acc: 0.22413793206214905)
[2025-02-04 00:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:47][root][INFO] - Training Epoch: 2/2, step 18602/23838 completed (loss: 3.2596631050109863, acc: 0.3928571343421936)
[2025-02-04 00:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:48][root][INFO] - Training Epoch: 2/2, step 18603/23838 completed (loss: 3.172698497772217, acc: 0.3571428656578064)
[2025-02-04 00:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:48][root][INFO] - Training Epoch: 2/2, step 18604/23838 completed (loss: 3.476862907409668, acc: 0.523809552192688)
[2025-02-04 00:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:49][root][INFO] - Training Epoch: 2/2, step 18605/23838 completed (loss: 2.618774652481079, acc: 0.40909090638160706)
[2025-02-04 00:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:49][root][INFO] - Training Epoch: 2/2, step 18606/23838 completed (loss: 2.439277172088623, acc: 0.5161290168762207)
[2025-02-04 00:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:50][root][INFO] - Training Epoch: 2/2, step 18607/23838 completed (loss: 3.344491481781006, acc: 0.3499999940395355)
[2025-02-04 00:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:50][root][INFO] - Training Epoch: 2/2, step 18608/23838 completed (loss: 3.4370384216308594, acc: 0.4000000059604645)
[2025-02-04 00:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:51][root][INFO] - Training Epoch: 2/2, step 18609/23838 completed (loss: 5.564095497131348, acc: 0.19607843458652496)
[2025-02-04 00:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:52][root][INFO] - Training Epoch: 2/2, step 18610/23838 completed (loss: 3.6701390743255615, acc: 0.3928571343421936)
[2025-02-04 00:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:52][root][INFO] - Training Epoch: 2/2, step 18611/23838 completed (loss: 4.008334636688232, acc: 0.37142857909202576)
[2025-02-04 00:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:53][root][INFO] - Training Epoch: 2/2, step 18612/23838 completed (loss: 2.209749698638916, acc: 0.6000000238418579)
[2025-02-04 00:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:53][root][INFO] - Training Epoch: 2/2, step 18613/23838 completed (loss: 2.7538599967956543, acc: 0.5)
[2025-02-04 00:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:54][root][INFO] - Training Epoch: 2/2, step 18614/23838 completed (loss: 3.3357439041137695, acc: 0.3055555522441864)
[2025-02-04 00:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:54][root][INFO] - Training Epoch: 2/2, step 18615/23838 completed (loss: 3.120734214782715, acc: 0.24242424964904785)
[2025-02-04 00:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:55][root][INFO] - Training Epoch: 2/2, step 18616/23838 completed (loss: 2.2656073570251465, acc: 0.517241358757019)
[2025-02-04 00:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:55][root][INFO] - Training Epoch: 2/2, step 18617/23838 completed (loss: 2.7571704387664795, acc: 0.380952388048172)
[2025-02-04 00:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:55][root][INFO] - Training Epoch: 2/2, step 18618/23838 completed (loss: 2.976529121398926, acc: 0.4482758641242981)
[2025-02-04 00:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:56][root][INFO] - Training Epoch: 2/2, step 18619/23838 completed (loss: 2.2860238552093506, acc: 0.6060606241226196)
[2025-02-04 00:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:56][root][INFO] - Training Epoch: 2/2, step 18620/23838 completed (loss: 2.687617063522339, acc: 0.44736841320991516)
[2025-02-04 00:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:57][root][INFO] - Training Epoch: 2/2, step 18621/23838 completed (loss: 3.0927462577819824, acc: 0.3777777850627899)
[2025-02-04 00:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:58][root][INFO] - Training Epoch: 2/2, step 18622/23838 completed (loss: 3.6835780143737793, acc: 0.34285715222358704)
[2025-02-04 00:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:58][root][INFO] - Training Epoch: 2/2, step 18623/23838 completed (loss: 3.3899857997894287, acc: 0.23076923191547394)
[2025-02-04 00:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:59][root][INFO] - Training Epoch: 2/2, step 18624/23838 completed (loss: 3.1145412921905518, acc: 0.30612245202064514)
[2025-02-04 00:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:38:59][root][INFO] - Training Epoch: 2/2, step 18625/23838 completed (loss: 3.7850496768951416, acc: 0.27272728085517883)
[2025-02-04 00:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:00][root][INFO] - Training Epoch: 2/2, step 18626/23838 completed (loss: 2.759216785430908, acc: 0.5)
[2025-02-04 00:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:01][root][INFO] - Training Epoch: 2/2, step 18627/23838 completed (loss: 2.7311582565307617, acc: 0.3478260934352875)
[2025-02-04 00:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:02][root][INFO] - Training Epoch: 2/2, step 18628/23838 completed (loss: 3.225569725036621, acc: 0.3076923191547394)
[2025-02-04 00:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:02][root][INFO] - Training Epoch: 2/2, step 18629/23838 completed (loss: 3.4804539680480957, acc: 0.2926829159259796)
[2025-02-04 00:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:03][root][INFO] - Training Epoch: 2/2, step 18630/23838 completed (loss: 3.256587266921997, acc: 0.4054054021835327)
[2025-02-04 00:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:03][root][INFO] - Training Epoch: 2/2, step 18631/23838 completed (loss: 3.0420539379119873, acc: 0.37837839126586914)
[2025-02-04 00:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:04][root][INFO] - Training Epoch: 2/2, step 18632/23838 completed (loss: 2.355111837387085, acc: 0.5208333134651184)
[2025-02-04 00:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:04][root][INFO] - Training Epoch: 2/2, step 18633/23838 completed (loss: 1.9398345947265625, acc: 0.5333333611488342)
[2025-02-04 00:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:05][root][INFO] - Training Epoch: 2/2, step 18634/23838 completed (loss: 3.6922078132629395, acc: 0.32499998807907104)
[2025-02-04 00:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:05][root][INFO] - Training Epoch: 2/2, step 18635/23838 completed (loss: 2.9731059074401855, acc: 0.4000000059604645)
[2025-02-04 00:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:06][root][INFO] - Training Epoch: 2/2, step 18636/23838 completed (loss: 3.0767252445220947, acc: 0.40909090638160706)
[2025-02-04 00:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:06][root][INFO] - Training Epoch: 2/2, step 18637/23838 completed (loss: 1.8402683734893799, acc: 0.6538461446762085)
[2025-02-04 00:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:07][root][INFO] - Training Epoch: 2/2, step 18638/23838 completed (loss: 2.7980124950408936, acc: 0.4516128897666931)
[2025-02-04 00:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:07][root][INFO] - Training Epoch: 2/2, step 18639/23838 completed (loss: 2.44608211517334, acc: 0.5)
[2025-02-04 00:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:07][root][INFO] - Training Epoch: 2/2, step 18640/23838 completed (loss: 2.817155361175537, acc: 0.5185185074806213)
[2025-02-04 00:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:08][root][INFO] - Training Epoch: 2/2, step 18641/23838 completed (loss: 2.919517993927002, acc: 0.4444444477558136)
[2025-02-04 00:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:08][root][INFO] - Training Epoch: 2/2, step 18642/23838 completed (loss: 2.6880125999450684, acc: 0.3513513505458832)
[2025-02-04 00:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:09][root][INFO] - Training Epoch: 2/2, step 18643/23838 completed (loss: 2.6832547187805176, acc: 0.38983049988746643)
[2025-02-04 00:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:10][root][INFO] - Training Epoch: 2/2, step 18644/23838 completed (loss: 3.58320951461792, acc: 0.43478259444236755)
[2025-02-04 00:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:10][root][INFO] - Training Epoch: 2/2, step 18645/23838 completed (loss: 2.2955093383789062, acc: 0.5909090638160706)
[2025-02-04 00:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:11][root][INFO] - Training Epoch: 2/2, step 18646/23838 completed (loss: 2.3342113494873047, acc: 0.5)
[2025-02-04 00:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:11][root][INFO] - Training Epoch: 2/2, step 18647/23838 completed (loss: 2.2284963130950928, acc: 0.5882353186607361)
[2025-02-04 00:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:11][root][INFO] - Training Epoch: 2/2, step 18648/23838 completed (loss: 3.2118706703186035, acc: 0.3199999928474426)
[2025-02-04 00:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:12][root][INFO] - Training Epoch: 2/2, step 18649/23838 completed (loss: 4.049032211303711, acc: 0.3076923191547394)
[2025-02-04 00:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:12][root][INFO] - Training Epoch: 2/2, step 18650/23838 completed (loss: 2.0110743045806885, acc: 0.47826087474823)
[2025-02-04 00:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:13][root][INFO] - Training Epoch: 2/2, step 18651/23838 completed (loss: 2.80772066116333, acc: 0.529411792755127)
[2025-02-04 00:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:13][root][INFO] - Training Epoch: 2/2, step 18652/23838 completed (loss: 3.9414443969726562, acc: 0.40625)
[2025-02-04 00:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:14][root][INFO] - Training Epoch: 2/2, step 18653/23838 completed (loss: 3.4776859283447266, acc: 0.4285714328289032)
[2025-02-04 00:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:14][root][INFO] - Training Epoch: 2/2, step 18654/23838 completed (loss: 1.3262062072753906, acc: 0.8333333134651184)
[2025-02-04 00:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:15][root][INFO] - Training Epoch: 2/2, step 18655/23838 completed (loss: 0.9633444547653198, acc: 0.800000011920929)
[2025-02-04 00:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:15][root][INFO] - Training Epoch: 2/2, step 18656/23838 completed (loss: 2.5993001461029053, acc: 0.5)
[2025-02-04 00:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:16][root][INFO] - Training Epoch: 2/2, step 18657/23838 completed (loss: 1.3597419261932373, acc: 0.800000011920929)
[2025-02-04 00:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:16][root][INFO] - Training Epoch: 2/2, step 18658/23838 completed (loss: 2.170802116394043, acc: 0.3636363744735718)
[2025-02-04 00:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:16][root][INFO] - Training Epoch: 2/2, step 18659/23838 completed (loss: 3.465500593185425, acc: 0.5)
[2025-02-04 00:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:17][root][INFO] - Training Epoch: 2/2, step 18660/23838 completed (loss: 2.281184673309326, acc: 0.4545454680919647)
[2025-02-04 00:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:17][root][INFO] - Training Epoch: 2/2, step 18661/23838 completed (loss: 3.0339066982269287, acc: 0.46666666865348816)
[2025-02-04 00:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:18][root][INFO] - Training Epoch: 2/2, step 18662/23838 completed (loss: 3.0970089435577393, acc: 0.5)
[2025-02-04 00:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:18][root][INFO] - Training Epoch: 2/2, step 18663/23838 completed (loss: 3.2209622859954834, acc: 0.4444444477558136)
[2025-02-04 00:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:19][root][INFO] - Training Epoch: 2/2, step 18664/23838 completed (loss: 1.864237666130066, acc: 0.5555555820465088)
[2025-02-04 00:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:19][root][INFO] - Training Epoch: 2/2, step 18665/23838 completed (loss: 0.2811017632484436, acc: 0.875)
[2025-02-04 00:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:20][root][INFO] - Training Epoch: 2/2, step 18666/23838 completed (loss: 0.027654273435473442, acc: 1.0)
[2025-02-04 00:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:20][root][INFO] - Training Epoch: 2/2, step 18667/23838 completed (loss: 0.7130981087684631, acc: 0.8999999761581421)
[2025-02-04 00:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:21][root][INFO] - Training Epoch: 2/2, step 18668/23838 completed (loss: 2.674405097961426, acc: 0.5454545617103577)
[2025-02-04 00:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:21][root][INFO] - Training Epoch: 2/2, step 18669/23838 completed (loss: 1.177506923675537, acc: 0.6499999761581421)
[2025-02-04 00:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:22][root][INFO] - Training Epoch: 2/2, step 18670/23838 completed (loss: 0.07137560099363327, acc: 1.0)
[2025-02-04 00:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:22][root][INFO] - Training Epoch: 2/2, step 18671/23838 completed (loss: 3.1778831481933594, acc: 0.5)
[2025-02-04 00:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:23][root][INFO] - Training Epoch: 2/2, step 18672/23838 completed (loss: 2.2251901626586914, acc: 0.6000000238418579)
[2025-02-04 00:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:23][root][INFO] - Training Epoch: 2/2, step 18673/23838 completed (loss: 1.6755157709121704, acc: 0.75)
[2025-02-04 00:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:23][root][INFO] - Training Epoch: 2/2, step 18674/23838 completed (loss: 3.16977858543396, acc: 0.5)
[2025-02-04 00:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:24][root][INFO] - Training Epoch: 2/2, step 18675/23838 completed (loss: 1.2580232620239258, acc: 0.6666666865348816)
[2025-02-04 00:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:24][root][INFO] - Training Epoch: 2/2, step 18676/23838 completed (loss: 2.032012701034546, acc: 0.6666666865348816)
[2025-02-04 00:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:25][root][INFO] - Training Epoch: 2/2, step 18677/23838 completed (loss: 1.963531255722046, acc: 0.6666666865348816)
[2025-02-04 00:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:25][root][INFO] - Training Epoch: 2/2, step 18678/23838 completed (loss: 3.0454254150390625, acc: 0.5333333611488342)
[2025-02-04 00:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:26][root][INFO] - Training Epoch: 2/2, step 18679/23838 completed (loss: 3.0698342323303223, acc: 0.625)
[2025-02-04 00:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:26][root][INFO] - Training Epoch: 2/2, step 18680/23838 completed (loss: 0.6353425979614258, acc: 0.7142857313156128)
[2025-02-04 00:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:26][root][INFO] - Training Epoch: 2/2, step 18681/23838 completed (loss: 2.091243267059326, acc: 0.5)
[2025-02-04 00:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:27][root][INFO] - Training Epoch: 2/2, step 18682/23838 completed (loss: 2.166661262512207, acc: 0.5555555820465088)
[2025-02-04 00:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:27][root][INFO] - Training Epoch: 2/2, step 18683/23838 completed (loss: 3.2785398960113525, acc: 0.43478259444236755)
[2025-02-04 00:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:28][root][INFO] - Training Epoch: 2/2, step 18684/23838 completed (loss: 3.7445945739746094, acc: 0.4166666567325592)
[2025-02-04 00:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:28][root][INFO] - Training Epoch: 2/2, step 18685/23838 completed (loss: 3.431466579437256, acc: 0.29629629850387573)
[2025-02-04 00:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:29][root][INFO] - Training Epoch: 2/2, step 18686/23838 completed (loss: 3.430960178375244, acc: 0.4545454680919647)
[2025-02-04 00:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:29][root][INFO] - Training Epoch: 2/2, step 18687/23838 completed (loss: 3.0531513690948486, acc: 0.375)
[2025-02-04 00:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:30][root][INFO] - Training Epoch: 2/2, step 18688/23838 completed (loss: 3.4389493465423584, acc: 0.3571428656578064)
[2025-02-04 00:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:30][root][INFO] - Training Epoch: 2/2, step 18689/23838 completed (loss: 3.9647183418273926, acc: 0.3199999928474426)
[2025-02-04 00:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:31][root][INFO] - Training Epoch: 2/2, step 18690/23838 completed (loss: 3.6409823894500732, acc: 0.38181817531585693)
[2025-02-04 00:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:31][root][INFO] - Training Epoch: 2/2, step 18691/23838 completed (loss: 3.506385564804077, acc: 0.4000000059604645)
[2025-02-04 00:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:32][root][INFO] - Training Epoch: 2/2, step 18692/23838 completed (loss: 3.202086925506592, acc: 0.4615384638309479)
[2025-02-04 00:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:32][root][INFO] - Training Epoch: 2/2, step 18693/23838 completed (loss: 2.5100932121276855, acc: 0.5185185074806213)
[2025-02-04 00:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:33][root][INFO] - Training Epoch: 2/2, step 18694/23838 completed (loss: 2.754495620727539, acc: 0.43478259444236755)
[2025-02-04 00:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:33][root][INFO] - Training Epoch: 2/2, step 18695/23838 completed (loss: 2.7463159561157227, acc: 0.39534884691238403)
[2025-02-04 00:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:34][root][INFO] - Training Epoch: 2/2, step 18696/23838 completed (loss: 4.005258083343506, acc: 0.42105263471603394)
[2025-02-04 00:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:34][root][INFO] - Training Epoch: 2/2, step 18697/23838 completed (loss: 2.798947811126709, acc: 0.375)
[2025-02-04 00:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:35][root][INFO] - Training Epoch: 2/2, step 18698/23838 completed (loss: 3.8184187412261963, acc: 0.21875)
[2025-02-04 00:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:35][root][INFO] - Training Epoch: 2/2, step 18699/23838 completed (loss: 2.904845952987671, acc: 0.48148149251937866)
[2025-02-04 00:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:35][root][INFO] - Training Epoch: 2/2, step 18700/23838 completed (loss: 2.5339536666870117, acc: 0.5517241358757019)
[2025-02-04 00:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:36][root][INFO] - Training Epoch: 2/2, step 18701/23838 completed (loss: 3.672506809234619, acc: 0.4000000059604645)
[2025-02-04 00:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:36][root][INFO] - Training Epoch: 2/2, step 18702/23838 completed (loss: 3.1200497150421143, acc: 0.5)
[2025-02-04 00:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:37][root][INFO] - Training Epoch: 2/2, step 18703/23838 completed (loss: 2.9251885414123535, acc: 0.4615384638309479)
[2025-02-04 00:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:38][root][INFO] - Training Epoch: 2/2, step 18704/23838 completed (loss: 2.4734628200531006, acc: 0.4444444477558136)
[2025-02-04 00:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:38][root][INFO] - Training Epoch: 2/2, step 18705/23838 completed (loss: 2.344228506088257, acc: 0.5666666626930237)
[2025-02-04 00:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:38][root][INFO] - Training Epoch: 2/2, step 18706/23838 completed (loss: 2.745504140853882, acc: 0.4000000059604645)
[2025-02-04 00:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:39][root][INFO] - Training Epoch: 2/2, step 18707/23838 completed (loss: 3.3379950523376465, acc: 0.2647058963775635)
[2025-02-04 00:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:39][root][INFO] - Training Epoch: 2/2, step 18708/23838 completed (loss: 3.2326674461364746, acc: 0.3333333432674408)
[2025-02-04 00:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:40][root][INFO] - Training Epoch: 2/2, step 18709/23838 completed (loss: 3.3775875568389893, acc: 0.38461539149284363)
[2025-02-04 00:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:40][root][INFO] - Training Epoch: 2/2, step 18710/23838 completed (loss: 2.656320810317993, acc: 0.3636363744735718)
[2025-02-04 00:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:41][root][INFO] - Training Epoch: 2/2, step 18711/23838 completed (loss: 3.116988182067871, acc: 0.4054054021835327)
[2025-02-04 00:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:41][root][INFO] - Training Epoch: 2/2, step 18712/23838 completed (loss: 3.33028507232666, acc: 0.31111112236976624)
[2025-02-04 00:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:42][root][INFO] - Training Epoch: 2/2, step 18713/23838 completed (loss: 3.1332004070281982, acc: 0.5)
[2025-02-04 00:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:42][root][INFO] - Training Epoch: 2/2, step 18714/23838 completed (loss: 2.2502100467681885, acc: 0.5666666626930237)
[2025-02-04 00:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:43][root][INFO] - Training Epoch: 2/2, step 18715/23838 completed (loss: 3.460341453552246, acc: 0.30612245202064514)
[2025-02-04 00:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:43][root][INFO] - Training Epoch: 2/2, step 18716/23838 completed (loss: 2.600048065185547, acc: 0.4615384638309479)
[2025-02-04 00:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:44][root][INFO] - Training Epoch: 2/2, step 18717/23838 completed (loss: 3.2365517616271973, acc: 0.33898305892944336)
[2025-02-04 00:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:44][root][INFO] - Training Epoch: 2/2, step 18718/23838 completed (loss: 2.717639446258545, acc: 0.4285714328289032)
[2025-02-04 00:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:45][root][INFO] - Training Epoch: 2/2, step 18719/23838 completed (loss: 2.1973025798797607, acc: 0.5185185074806213)
[2025-02-04 00:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:45][root][INFO] - Training Epoch: 2/2, step 18720/23838 completed (loss: 2.8084683418273926, acc: 0.4000000059604645)
[2025-02-04 00:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:46][root][INFO] - Training Epoch: 2/2, step 18721/23838 completed (loss: 2.964569330215454, acc: 0.4193548262119293)
[2025-02-04 00:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:46][root][INFO] - Training Epoch: 2/2, step 18722/23838 completed (loss: 2.7843220233917236, acc: 0.5625)
[2025-02-04 00:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:47][root][INFO] - Training Epoch: 2/2, step 18723/23838 completed (loss: 2.1771109104156494, acc: 0.6363636255264282)
[2025-02-04 00:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:47][root][INFO] - Training Epoch: 2/2, step 18724/23838 completed (loss: 1.1725497245788574, acc: 0.8461538553237915)
[2025-02-04 00:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:48][root][INFO] - Training Epoch: 2/2, step 18725/23838 completed (loss: 2.9461967945098877, acc: 0.6000000238418579)
[2025-02-04 00:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:48][root][INFO] - Training Epoch: 2/2, step 18726/23838 completed (loss: 1.6761442422866821, acc: 0.5882353186607361)
[2025-02-04 00:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:49][root][INFO] - Training Epoch: 2/2, step 18727/23838 completed (loss: 2.5349740982055664, acc: 0.5384615659713745)
[2025-02-04 00:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:49][root][INFO] - Training Epoch: 2/2, step 18728/23838 completed (loss: 1.9392329454421997, acc: 0.4285714328289032)
[2025-02-04 00:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:50][root][INFO] - Training Epoch: 2/2, step 18729/23838 completed (loss: 0.23746764659881592, acc: 0.9166666865348816)
[2025-02-04 00:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:50][root][INFO] - Training Epoch: 2/2, step 18730/23838 completed (loss: 1.0012109279632568, acc: 0.8888888955116272)
[2025-02-04 00:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:50][root][INFO] - Training Epoch: 2/2, step 18731/23838 completed (loss: 1.0548650026321411, acc: 0.7142857313156128)
[2025-02-04 00:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:51][root][INFO] - Training Epoch: 2/2, step 18732/23838 completed (loss: 1.2238394021987915, acc: 0.8888888955116272)
[2025-02-04 00:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:51][root][INFO] - Training Epoch: 2/2, step 18733/23838 completed (loss: 1.4437059164047241, acc: 0.7777777910232544)
[2025-02-04 00:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:52][root][INFO] - Training Epoch: 2/2, step 18734/23838 completed (loss: 2.1219711303710938, acc: 0.6000000238418579)
[2025-02-04 00:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:52][root][INFO] - Training Epoch: 2/2, step 18735/23838 completed (loss: 2.8595471382141113, acc: 0.5263158082962036)
[2025-02-04 00:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:53][root][INFO] - Training Epoch: 2/2, step 18736/23838 completed (loss: 3.3985917568206787, acc: 0.4000000059604645)
[2025-02-04 00:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:53][root][INFO] - Training Epoch: 2/2, step 18737/23838 completed (loss: 1.6553434133529663, acc: 0.7222222089767456)
[2025-02-04 00:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:53][root][INFO] - Training Epoch: 2/2, step 18738/23838 completed (loss: 2.29811692237854, acc: 0.5714285969734192)
[2025-02-04 00:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:54][root][INFO] - Training Epoch: 2/2, step 18739/23838 completed (loss: 3.054225444793701, acc: 0.3913043439388275)
[2025-02-04 00:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:54][root][INFO] - Training Epoch: 2/2, step 18740/23838 completed (loss: 0.878965437412262, acc: 0.75)
[2025-02-04 00:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:55][root][INFO] - Training Epoch: 2/2, step 18741/23838 completed (loss: 1.863337755203247, acc: 0.5)
[2025-02-04 00:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:55][root][INFO] - Training Epoch: 2/2, step 18742/23838 completed (loss: 1.69791841506958, acc: 0.7272727489471436)
[2025-02-04 00:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:56][root][INFO] - Training Epoch: 2/2, step 18743/23838 completed (loss: 1.3466110229492188, acc: 0.699999988079071)
[2025-02-04 00:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:56][root][INFO] - Training Epoch: 2/2, step 18744/23838 completed (loss: 1.364643931388855, acc: 0.7142857313156128)
[2025-02-04 00:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:56][root][INFO] - Training Epoch: 2/2, step 18745/23838 completed (loss: 1.098059892654419, acc: 0.8333333134651184)
[2025-02-04 00:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:57][root][INFO] - Training Epoch: 2/2, step 18746/23838 completed (loss: 0.5883113145828247, acc: 0.7777777910232544)
[2025-02-04 00:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:57][root][INFO] - Training Epoch: 2/2, step 18747/23838 completed (loss: 0.18136654794216156, acc: 1.0)
[2025-02-04 00:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:58][root][INFO] - Training Epoch: 2/2, step 18748/23838 completed (loss: 2.092176675796509, acc: 0.4545454680919647)
[2025-02-04 00:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:58][root][INFO] - Training Epoch: 2/2, step 18749/23838 completed (loss: 0.7798494696617126, acc: 0.7647058963775635)
[2025-02-04 00:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:59][root][INFO] - Training Epoch: 2/2, step 18750/23838 completed (loss: 0.22417564690113068, acc: 1.0)
[2025-02-04 00:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:39:59][root][INFO] - Training Epoch: 2/2, step 18751/23838 completed (loss: 0.686079204082489, acc: 0.8999999761581421)
[2025-02-04 00:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:00][root][INFO] - Training Epoch: 2/2, step 18752/23838 completed (loss: 1.824361801147461, acc: 0.5833333134651184)
[2025-02-04 00:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:00][root][INFO] - Training Epoch: 2/2, step 18753/23838 completed (loss: 1.398578405380249, acc: 0.6000000238418579)
[2025-02-04 00:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:00][root][INFO] - Training Epoch: 2/2, step 18754/23838 completed (loss: 1.6378326416015625, acc: 0.6363636255264282)
[2025-02-04 00:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:01][root][INFO] - Training Epoch: 2/2, step 18755/23838 completed (loss: 0.5206676125526428, acc: 0.9166666865348816)
[2025-02-04 00:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:01][root][INFO] - Training Epoch: 2/2, step 18756/23838 completed (loss: 2.060847759246826, acc: 0.5714285969734192)
[2025-02-04 00:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:02][root][INFO] - Training Epoch: 2/2, step 18757/23838 completed (loss: 0.8973132967948914, acc: 0.6666666865348816)
[2025-02-04 00:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:02][root][INFO] - Training Epoch: 2/2, step 18758/23838 completed (loss: 0.026825275272130966, acc: 1.0)
[2025-02-04 00:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:03][root][INFO] - Training Epoch: 2/2, step 18759/23838 completed (loss: 1.1290701627731323, acc: 0.7777777910232544)
[2025-02-04 00:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:03][root][INFO] - Training Epoch: 2/2, step 18760/23838 completed (loss: 1.7008867263793945, acc: 0.5714285969734192)
[2025-02-04 00:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:03][root][INFO] - Training Epoch: 2/2, step 18761/23838 completed (loss: 3.6353392601013184, acc: 0.4285714328289032)
[2025-02-04 00:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:04][root][INFO] - Training Epoch: 2/2, step 18762/23838 completed (loss: 1.2031060457229614, acc: 0.625)
[2025-02-04 00:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:04][root][INFO] - Training Epoch: 2/2, step 18763/23838 completed (loss: 1.9447553157806396, acc: 0.5)
[2025-02-04 00:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:05][root][INFO] - Training Epoch: 2/2, step 18764/23838 completed (loss: 1.1597189903259277, acc: 0.8181818127632141)
[2025-02-04 00:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:05][root][INFO] - Training Epoch: 2/2, step 18765/23838 completed (loss: 2.186255693435669, acc: 0.4615384638309479)
[2025-02-04 00:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:06][root][INFO] - Training Epoch: 2/2, step 18766/23838 completed (loss: 1.5111091136932373, acc: 0.692307710647583)
[2025-02-04 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:06][root][INFO] - Training Epoch: 2/2, step 18767/23838 completed (loss: 3.6048169136047363, acc: 0.47826087474823)
[2025-02-04 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:06][root][INFO] - Training Epoch: 2/2, step 18768/23838 completed (loss: 0.5720468759536743, acc: 0.7777777910232544)
[2025-02-04 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:07][root][INFO] - Training Epoch: 2/2, step 18769/23838 completed (loss: 0.5987635254859924, acc: 0.875)
[2025-02-04 00:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:07][root][INFO] - Training Epoch: 2/2, step 18770/23838 completed (loss: 1.4829169511795044, acc: 0.6000000238418579)
[2025-02-04 00:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:08][root][INFO] - Training Epoch: 2/2, step 18771/23838 completed (loss: 1.914435625076294, acc: 0.5384615659713745)
[2025-02-04 00:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:08][root][INFO] - Training Epoch: 2/2, step 18772/23838 completed (loss: 0.3294537365436554, acc: 1.0)
[2025-02-04 00:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:09][root][INFO] - Training Epoch: 2/2, step 18773/23838 completed (loss: 0.8857936859130859, acc: 0.6000000238418579)
[2025-02-04 00:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:09][root][INFO] - Training Epoch: 2/2, step 18774/23838 completed (loss: 1.8673559427261353, acc: 0.6666666865348816)
[2025-02-04 00:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:10][root][INFO] - Training Epoch: 2/2, step 18775/23838 completed (loss: 0.7293190956115723, acc: 0.8333333134651184)
[2025-02-04 00:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:10][root][INFO] - Training Epoch: 2/2, step 18776/23838 completed (loss: 2.0737369060516357, acc: 0.4166666567325592)
[2025-02-04 00:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:11][root][INFO] - Training Epoch: 2/2, step 18777/23838 completed (loss: 2.1081576347351074, acc: 0.625)
[2025-02-04 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:11][root][INFO] - Training Epoch: 2/2, step 18778/23838 completed (loss: 1.4479496479034424, acc: 0.7142857313156128)
[2025-02-04 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:11][root][INFO] - Training Epoch: 2/2, step 18779/23838 completed (loss: 1.8924901485443115, acc: 0.5714285969734192)
[2025-02-04 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:12][root][INFO] - Training Epoch: 2/2, step 18780/23838 completed (loss: 1.5710800886154175, acc: 0.625)
[2025-02-04 00:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:12][root][INFO] - Training Epoch: 2/2, step 18781/23838 completed (loss: 2.502734661102295, acc: 0.4285714328289032)
[2025-02-04 00:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:13][root][INFO] - Training Epoch: 2/2, step 18782/23838 completed (loss: 2.943969964981079, acc: 0.3333333432674408)
[2025-02-04 00:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:13][root][INFO] - Training Epoch: 2/2, step 18783/23838 completed (loss: 0.46604374051094055, acc: 0.8181818127632141)
[2025-02-04 00:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:13][root][INFO] - Training Epoch: 2/2, step 18784/23838 completed (loss: 1.7120301723480225, acc: 0.6153846383094788)
[2025-02-04 00:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:14][root][INFO] - Training Epoch: 2/2, step 18785/23838 completed (loss: 0.9386328458786011, acc: 0.7692307829856873)
[2025-02-04 00:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:14][root][INFO] - Training Epoch: 2/2, step 18786/23838 completed (loss: 2.8540265560150146, acc: 0.4166666567325592)
[2025-02-04 00:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:15][root][INFO] - Training Epoch: 2/2, step 18787/23838 completed (loss: 3.0042312145233154, acc: 0.3684210479259491)
[2025-02-04 00:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:15][root][INFO] - Training Epoch: 2/2, step 18788/23838 completed (loss: 1.7548834085464478, acc: 0.6000000238418579)
[2025-02-04 00:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:15][root][INFO] - Training Epoch: 2/2, step 18789/23838 completed (loss: 0.8971378207206726, acc: 0.75)
[2025-02-04 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:16][root][INFO] - Training Epoch: 2/2, step 18790/23838 completed (loss: 0.7795273065567017, acc: 0.800000011920929)
[2025-02-04 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:16][root][INFO] - Training Epoch: 2/2, step 18791/23838 completed (loss: 1.3803373575210571, acc: 0.6363636255264282)
[2025-02-04 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:17][root][INFO] - Training Epoch: 2/2, step 18792/23838 completed (loss: 2.078425168991089, acc: 0.46666666865348816)
[2025-02-04 00:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:17][root][INFO] - Training Epoch: 2/2, step 18793/23838 completed (loss: 2.9290223121643066, acc: 0.6000000238418579)
[2025-02-04 00:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:18][root][INFO] - Training Epoch: 2/2, step 18794/23838 completed (loss: 0.5863566398620605, acc: 0.8095238208770752)
[2025-02-04 00:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:18][root][INFO] - Training Epoch: 2/2, step 18795/23838 completed (loss: 1.691798448562622, acc: 0.625)
[2025-02-04 00:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:18][root][INFO] - Training Epoch: 2/2, step 18796/23838 completed (loss: 1.9173011779785156, acc: 0.8181818127632141)
[2025-02-04 00:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:19][root][INFO] - Training Epoch: 2/2, step 18797/23838 completed (loss: 2.0201735496520996, acc: 0.6153846383094788)
[2025-02-04 00:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:19][root][INFO] - Training Epoch: 2/2, step 18798/23838 completed (loss: 1.938256859779358, acc: 0.5)
[2025-02-04 00:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:20][root][INFO] - Training Epoch: 2/2, step 18799/23838 completed (loss: 0.801129162311554, acc: 0.8181818127632141)
[2025-02-04 00:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:20][root][INFO] - Training Epoch: 2/2, step 18800/23838 completed (loss: 1.4751852750778198, acc: 0.6666666865348816)
[2025-02-04 00:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:21][root][INFO] - Training Epoch: 2/2, step 18801/23838 completed (loss: 0.6968989372253418, acc: 0.6666666865348816)
[2025-02-04 00:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:21][root][INFO] - Training Epoch: 2/2, step 18802/23838 completed (loss: 1.7940372228622437, acc: 0.6000000238418579)
[2025-02-04 00:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:21][root][INFO] - Training Epoch: 2/2, step 18803/23838 completed (loss: 1.98031747341156, acc: 0.6428571343421936)
[2025-02-04 00:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:22][root][INFO] - Training Epoch: 2/2, step 18804/23838 completed (loss: 1.669560194015503, acc: 0.699999988079071)
[2025-02-04 00:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:22][root][INFO] - Training Epoch: 2/2, step 18805/23838 completed (loss: 0.808543860912323, acc: 0.8571428656578064)
[2025-02-04 00:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:23][root][INFO] - Training Epoch: 2/2, step 18806/23838 completed (loss: 1.0424649715423584, acc: 0.7142857313156128)
[2025-02-04 00:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:23][root][INFO] - Training Epoch: 2/2, step 18807/23838 completed (loss: 0.9003071188926697, acc: 0.8333333134651184)
[2025-02-04 00:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:23][root][INFO] - Training Epoch: 2/2, step 18808/23838 completed (loss: 1.480295181274414, acc: 0.6363636255264282)
[2025-02-04 00:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:24][root][INFO] - Training Epoch: 2/2, step 18809/23838 completed (loss: 0.4686049818992615, acc: 0.800000011920929)
[2025-02-04 00:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:24][root][INFO] - Training Epoch: 2/2, step 18810/23838 completed (loss: 1.2958399057388306, acc: 0.6666666865348816)
[2025-02-04 00:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:25][root][INFO] - Training Epoch: 2/2, step 18811/23838 completed (loss: 1.7159793376922607, acc: 0.6000000238418579)
[2025-02-04 00:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:25][root][INFO] - Training Epoch: 2/2, step 18812/23838 completed (loss: 2.8364946842193604, acc: 0.5882353186607361)
[2025-02-04 00:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:25][root][INFO] - Training Epoch: 2/2, step 18813/23838 completed (loss: 0.6467906832695007, acc: 0.800000011920929)
[2025-02-04 00:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:26][root][INFO] - Training Epoch: 2/2, step 18814/23838 completed (loss: 2.8779242038726807, acc: 0.4615384638309479)
[2025-02-04 00:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:26][root][INFO] - Training Epoch: 2/2, step 18815/23838 completed (loss: 3.070722818374634, acc: 0.4166666567325592)
[2025-02-04 00:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:27][root][INFO] - Training Epoch: 2/2, step 18816/23838 completed (loss: 4.163950443267822, acc: 0.29032257199287415)
[2025-02-04 00:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:27][root][INFO] - Training Epoch: 2/2, step 18817/23838 completed (loss: 5.84255313873291, acc: 0.20588235557079315)
[2025-02-04 00:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:27][root][INFO] - Training Epoch: 2/2, step 18818/23838 completed (loss: 4.517531394958496, acc: 0.25641027092933655)
[2025-02-04 00:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:28][root][INFO] - Training Epoch: 2/2, step 18819/23838 completed (loss: 2.0301401615142822, acc: 0.5789473652839661)
[2025-02-04 00:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:28][root][INFO] - Training Epoch: 2/2, step 18820/23838 completed (loss: 3.0186703205108643, acc: 0.4375)
[2025-02-04 00:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:29][root][INFO] - Training Epoch: 2/2, step 18821/23838 completed (loss: 4.428730010986328, acc: 0.3030303120613098)
[2025-02-04 00:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:29][root][INFO] - Training Epoch: 2/2, step 18822/23838 completed (loss: 2.9033164978027344, acc: 0.550000011920929)
[2025-02-04 00:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:30][root][INFO] - Training Epoch: 2/2, step 18823/23838 completed (loss: 3.9907169342041016, acc: 0.3499999940395355)
[2025-02-04 00:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:30][root][INFO] - Training Epoch: 2/2, step 18824/23838 completed (loss: 3.320094347000122, acc: 0.4444444477558136)
[2025-02-04 00:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:31][root][INFO] - Training Epoch: 2/2, step 18825/23838 completed (loss: 3.501736879348755, acc: 0.4285714328289032)
[2025-02-04 00:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:31][root][INFO] - Training Epoch: 2/2, step 18826/23838 completed (loss: 3.4949731826782227, acc: 0.46666666865348816)
[2025-02-04 00:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:31][root][INFO] - Training Epoch: 2/2, step 18827/23838 completed (loss: 3.368022918701172, acc: 0.4444444477558136)
[2025-02-04 00:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:32][root][INFO] - Training Epoch: 2/2, step 18828/23838 completed (loss: 3.5029330253601074, acc: 0.2857142984867096)
[2025-02-04 00:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:32][root][INFO] - Training Epoch: 2/2, step 18829/23838 completed (loss: 2.8918089866638184, acc: 0.41304346919059753)
[2025-02-04 00:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:33][root][INFO] - Training Epoch: 2/2, step 18830/23838 completed (loss: 3.5925302505493164, acc: 0.47999998927116394)
[2025-02-04 00:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:33][root][INFO] - Training Epoch: 2/2, step 18831/23838 completed (loss: 3.995598077774048, acc: 0.3076923191547394)
[2025-02-04 00:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:34][root][INFO] - Training Epoch: 2/2, step 18832/23838 completed (loss: 3.1436688899993896, acc: 0.36666667461395264)
[2025-02-04 00:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:34][root][INFO] - Training Epoch: 2/2, step 18833/23838 completed (loss: 3.1664161682128906, acc: 0.3076923191547394)
[2025-02-04 00:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:35][root][INFO] - Training Epoch: 2/2, step 18834/23838 completed (loss: 2.6292884349823, acc: 0.4736842215061188)
[2025-02-04 00:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:35][root][INFO] - Training Epoch: 2/2, step 18835/23838 completed (loss: 3.840667247772217, acc: 0.29032257199287415)
[2025-02-04 00:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:35][root][INFO] - Training Epoch: 2/2, step 18836/23838 completed (loss: 3.889037847518921, acc: 0.4516128897666931)
[2025-02-04 00:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:36][root][INFO] - Training Epoch: 2/2, step 18837/23838 completed (loss: 2.098546028137207, acc: 0.6000000238418579)
[2025-02-04 00:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:36][root][INFO] - Training Epoch: 2/2, step 18838/23838 completed (loss: 3.5197572708129883, acc: 0.4285714328289032)
[2025-02-04 00:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:37][root][INFO] - Training Epoch: 2/2, step 18839/23838 completed (loss: 3.1112334728240967, acc: 0.48571428656578064)
[2025-02-04 00:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:37][root][INFO] - Training Epoch: 2/2, step 18840/23838 completed (loss: 4.397222518920898, acc: 0.4000000059604645)
[2025-02-04 00:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:38][root][INFO] - Training Epoch: 2/2, step 18841/23838 completed (loss: 3.635288715362549, acc: 0.4375)
[2025-02-04 00:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:38][root][INFO] - Training Epoch: 2/2, step 18842/23838 completed (loss: 3.8028106689453125, acc: 0.37037035822868347)
[2025-02-04 00:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:39][root][INFO] - Training Epoch: 2/2, step 18843/23838 completed (loss: 4.049799919128418, acc: 0.4000000059604645)
[2025-02-04 00:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:39][root][INFO] - Training Epoch: 2/2, step 18844/23838 completed (loss: 4.376528739929199, acc: 0.3235294222831726)
[2025-02-04 00:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:39][root][INFO] - Training Epoch: 2/2, step 18845/23838 completed (loss: 4.309849262237549, acc: 0.28260868787765503)
[2025-02-04 00:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:40][root][INFO] - Training Epoch: 2/2, step 18846/23838 completed (loss: 3.2570242881774902, acc: 0.4137931168079376)
[2025-02-04 00:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:40][root][INFO] - Training Epoch: 2/2, step 18847/23838 completed (loss: 3.1585934162139893, acc: 0.41999998688697815)
[2025-02-04 00:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:41][root][INFO] - Training Epoch: 2/2, step 18848/23838 completed (loss: 2.991424083709717, acc: 0.5)
[2025-02-04 00:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:41][root][INFO] - Training Epoch: 2/2, step 18849/23838 completed (loss: 3.47747802734375, acc: 0.28125)
[2025-02-04 00:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:42][root][INFO] - Training Epoch: 2/2, step 18850/23838 completed (loss: 2.817448377609253, acc: 0.5454545617103577)
[2025-02-04 00:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:42][root][INFO] - Training Epoch: 2/2, step 18851/23838 completed (loss: 3.134981870651245, acc: 0.4285714328289032)
[2025-02-04 00:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:42][root][INFO] - Training Epoch: 2/2, step 18852/23838 completed (loss: 3.668813467025757, acc: 0.4444444477558136)
[2025-02-04 00:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:43][root][INFO] - Training Epoch: 2/2, step 18853/23838 completed (loss: 3.8547797203063965, acc: 0.2631579041481018)
[2025-02-04 00:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:43][root][INFO] - Training Epoch: 2/2, step 18854/23838 completed (loss: 3.5484604835510254, acc: 0.31707316637039185)
[2025-02-04 00:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:44][root][INFO] - Training Epoch: 2/2, step 18855/23838 completed (loss: 4.088444232940674, acc: 0.23076923191547394)
[2025-02-04 00:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:44][root][INFO] - Training Epoch: 2/2, step 18856/23838 completed (loss: 4.6495232582092285, acc: 0.28125)
[2025-02-04 00:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:44][root][INFO] - Training Epoch: 2/2, step 18857/23838 completed (loss: 2.0065040588378906, acc: 0.5)
[2025-02-04 00:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:45][root][INFO] - Training Epoch: 2/2, step 18858/23838 completed (loss: 3.2368099689483643, acc: 0.4390243887901306)
[2025-02-04 00:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:45][root][INFO] - Training Epoch: 2/2, step 18859/23838 completed (loss: 4.592074394226074, acc: 0.3103448152542114)
[2025-02-04 00:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:46][root][INFO] - Training Epoch: 2/2, step 18860/23838 completed (loss: 2.212540626525879, acc: 0.5909090638160706)
[2025-02-04 00:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:46][root][INFO] - Training Epoch: 2/2, step 18861/23838 completed (loss: 2.498016834259033, acc: 0.5517241358757019)
[2025-02-04 00:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:47][root][INFO] - Training Epoch: 2/2, step 18862/23838 completed (loss: 3.5014376640319824, acc: 0.2222222238779068)
[2025-02-04 00:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:47][root][INFO] - Training Epoch: 2/2, step 18863/23838 completed (loss: 3.047375440597534, acc: 0.4285714328289032)
[2025-02-04 00:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:47][root][INFO] - Training Epoch: 2/2, step 18864/23838 completed (loss: 3.1008989810943604, acc: 0.3478260934352875)
[2025-02-04 00:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:48][root][INFO] - Training Epoch: 2/2, step 18865/23838 completed (loss: 2.740291118621826, acc: 0.5526315569877625)
[2025-02-04 00:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:48][root][INFO] - Training Epoch: 2/2, step 18866/23838 completed (loss: 3.0332632064819336, acc: 0.4137931168079376)
[2025-02-04 00:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:49][root][INFO] - Training Epoch: 2/2, step 18867/23838 completed (loss: 3.57859206199646, acc: 0.40909090638160706)
[2025-02-04 00:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:49][root][INFO] - Training Epoch: 2/2, step 18868/23838 completed (loss: 3.159390926361084, acc: 0.44117647409439087)
[2025-02-04 00:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:50][root][INFO] - Training Epoch: 2/2, step 18869/23838 completed (loss: 2.310155153274536, acc: 0.6129032373428345)
[2025-02-04 00:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:50][root][INFO] - Training Epoch: 2/2, step 18870/23838 completed (loss: 2.5396437644958496, acc: 0.5)
[2025-02-04 00:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:50][root][INFO] - Training Epoch: 2/2, step 18871/23838 completed (loss: 2.6745872497558594, acc: 0.5)
[2025-02-04 00:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:51][root][INFO] - Training Epoch: 2/2, step 18872/23838 completed (loss: 2.240079879760742, acc: 0.45652174949645996)
[2025-02-04 00:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:51][root][INFO] - Training Epoch: 2/2, step 18873/23838 completed (loss: 2.4654111862182617, acc: 0.5454545617103577)
[2025-02-04 00:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:52][root][INFO] - Training Epoch: 2/2, step 18874/23838 completed (loss: 2.3505687713623047, acc: 0.5652173757553101)
[2025-02-04 00:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:52][root][INFO] - Training Epoch: 2/2, step 18875/23838 completed (loss: 3.0948455333709717, acc: 0.4375)
[2025-02-04 00:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:53][root][INFO] - Training Epoch: 2/2, step 18876/23838 completed (loss: 3.6595418453216553, acc: 0.4137931168079376)
[2025-02-04 00:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:53][root][INFO] - Training Epoch: 2/2, step 18877/23838 completed (loss: 3.663317918777466, acc: 0.3333333432674408)
[2025-02-04 00:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:54][root][INFO] - Training Epoch: 2/2, step 18878/23838 completed (loss: 3.766848087310791, acc: 0.42424243688583374)
[2025-02-04 00:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:54][root][INFO] - Training Epoch: 2/2, step 18879/23838 completed (loss: 3.0202178955078125, acc: 0.5)
[2025-02-04 00:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:54][root][INFO] - Training Epoch: 2/2, step 18880/23838 completed (loss: 4.724856853485107, acc: 0.37931033968925476)
[2025-02-04 00:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:55][root][INFO] - Training Epoch: 2/2, step 18881/23838 completed (loss: 3.766113758087158, acc: 0.30000001192092896)
[2025-02-04 00:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:55][root][INFO] - Training Epoch: 2/2, step 18882/23838 completed (loss: 3.3962037563323975, acc: 0.25806450843811035)
[2025-02-04 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:56][root][INFO] - Training Epoch: 2/2, step 18883/23838 completed (loss: 3.5340371131896973, acc: 0.34375)
[2025-02-04 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:56][root][INFO] - Training Epoch: 2/2, step 18884/23838 completed (loss: 2.459437370300293, acc: 0.4848484992980957)
[2025-02-04 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:57][root][INFO] - Training Epoch: 2/2, step 18885/23838 completed (loss: 2.557203531265259, acc: 0.6086956262588501)
[2025-02-04 00:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:57][root][INFO] - Training Epoch: 2/2, step 18886/23838 completed (loss: 3.4003894329071045, acc: 0.36666667461395264)
[2025-02-04 00:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:58][root][INFO] - Training Epoch: 2/2, step 18887/23838 completed (loss: 3.7380337715148926, acc: 0.3478260934352875)
[2025-02-04 00:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:58][root][INFO] - Training Epoch: 2/2, step 18888/23838 completed (loss: 2.2131800651550293, acc: 0.5416666865348816)
[2025-02-04 00:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:58][root][INFO] - Training Epoch: 2/2, step 18889/23838 completed (loss: 2.948674440383911, acc: 0.4642857015132904)
[2025-02-04 00:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:59][root][INFO] - Training Epoch: 2/2, step 18890/23838 completed (loss: 3.212952136993408, acc: 0.3888888955116272)
[2025-02-04 00:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:40:59][root][INFO] - Training Epoch: 2/2, step 18891/23838 completed (loss: 2.8061106204986572, acc: 0.42424243688583374)
[2025-02-04 00:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:00][root][INFO] - Training Epoch: 2/2, step 18892/23838 completed (loss: 2.4810681343078613, acc: 0.4516128897666931)
[2025-02-04 00:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:00][root][INFO] - Training Epoch: 2/2, step 18893/23838 completed (loss: 3.4508252143859863, acc: 0.3333333432674408)
[2025-02-04 00:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:00][root][INFO] - Training Epoch: 2/2, step 18894/23838 completed (loss: 3.6025547981262207, acc: 0.4749999940395355)
[2025-02-04 00:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:01][root][INFO] - Training Epoch: 2/2, step 18895/23838 completed (loss: 3.197545289993286, acc: 0.5)
[2025-02-04 00:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:01][root][INFO] - Training Epoch: 2/2, step 18896/23838 completed (loss: 3.550698757171631, acc: 0.2777777910232544)
[2025-02-04 00:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:02][root][INFO] - Training Epoch: 2/2, step 18897/23838 completed (loss: 2.5875372886657715, acc: 0.3636363744735718)
[2025-02-04 00:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:02][root][INFO] - Training Epoch: 2/2, step 18898/23838 completed (loss: 1.9306973218917847, acc: 0.5714285969734192)
[2025-02-04 00:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:03][root][INFO] - Training Epoch: 2/2, step 18899/23838 completed (loss: 4.012650489807129, acc: 0.3076923191547394)
[2025-02-04 00:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:03][root][INFO] - Training Epoch: 2/2, step 18900/23838 completed (loss: 3.7158234119415283, acc: 0.29411765933036804)
[2025-02-04 00:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:03][root][INFO] - Training Epoch: 2/2, step 18901/23838 completed (loss: 3.5447182655334473, acc: 0.30000001192092896)
[2025-02-04 00:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:04][root][INFO] - Training Epoch: 2/2, step 18902/23838 completed (loss: 3.42323637008667, acc: 0.4117647111415863)
[2025-02-04 00:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:04][root][INFO] - Training Epoch: 2/2, step 18903/23838 completed (loss: 3.703702688217163, acc: 0.3265306055545807)
[2025-02-04 00:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:05][root][INFO] - Training Epoch: 2/2, step 18904/23838 completed (loss: 3.2739691734313965, acc: 0.46666666865348816)
[2025-02-04 00:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:05][root][INFO] - Training Epoch: 2/2, step 18905/23838 completed (loss: 3.282644748687744, acc: 0.3947368562221527)
[2025-02-04 00:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:06][root][INFO] - Training Epoch: 2/2, step 18906/23838 completed (loss: 3.833099842071533, acc: 0.2631579041481018)
[2025-02-04 00:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:06][root][INFO] - Training Epoch: 2/2, step 18907/23838 completed (loss: 3.548658609390259, acc: 0.3636363744735718)
[2025-02-04 00:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:06][root][INFO] - Training Epoch: 2/2, step 18908/23838 completed (loss: 3.170044183731079, acc: 0.40909090638160706)
[2025-02-04 00:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:07][root][INFO] - Training Epoch: 2/2, step 18909/23838 completed (loss: 3.3603789806365967, acc: 0.36666667461395264)
[2025-02-04 00:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:07][root][INFO] - Training Epoch: 2/2, step 18910/23838 completed (loss: 3.904601812362671, acc: 0.2800000011920929)
[2025-02-04 00:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:08][root][INFO] - Training Epoch: 2/2, step 18911/23838 completed (loss: 3.3375649452209473, acc: 0.34285715222358704)
[2025-02-04 00:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:08][root][INFO] - Training Epoch: 2/2, step 18912/23838 completed (loss: 3.8252716064453125, acc: 0.3191489279270172)
[2025-02-04 00:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:08][root][INFO] - Training Epoch: 2/2, step 18913/23838 completed (loss: 3.0328097343444824, acc: 0.375)
[2025-02-04 00:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:09][root][INFO] - Training Epoch: 2/2, step 18914/23838 completed (loss: 3.70906662940979, acc: 0.30000001192092896)
[2025-02-04 00:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:09][root][INFO] - Training Epoch: 2/2, step 18915/23838 completed (loss: 3.279836654663086, acc: 0.31111112236976624)
[2025-02-04 00:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:10][root][INFO] - Training Epoch: 2/2, step 18916/23838 completed (loss: 2.9119980335235596, acc: 0.43396225571632385)
[2025-02-04 00:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:10][root][INFO] - Training Epoch: 2/2, step 18917/23838 completed (loss: 3.417652130126953, acc: 0.27659574151039124)
[2025-02-04 00:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:11][root][INFO] - Training Epoch: 2/2, step 18918/23838 completed (loss: 3.5005266666412354, acc: 0.2666666805744171)
[2025-02-04 00:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:11][root][INFO] - Training Epoch: 2/2, step 18919/23838 completed (loss: 1.9099429845809937, acc: 0.5652173757553101)
[2025-02-04 00:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:12][root][INFO] - Training Epoch: 2/2, step 18920/23838 completed (loss: 4.007230758666992, acc: 0.42307692766189575)
[2025-02-04 00:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:12][root][INFO] - Training Epoch: 2/2, step 18921/23838 completed (loss: 2.9037938117980957, acc: 0.5)
[2025-02-04 00:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:13][root][INFO] - Training Epoch: 2/2, step 18922/23838 completed (loss: 3.17303466796875, acc: 0.43478259444236755)
[2025-02-04 00:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:13][root][INFO] - Training Epoch: 2/2, step 18923/23838 completed (loss: 2.1813430786132812, acc: 0.53125)
[2025-02-04 00:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:13][root][INFO] - Training Epoch: 2/2, step 18924/23838 completed (loss: 3.452944040298462, acc: 0.37142857909202576)
[2025-02-04 00:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:14][root][INFO] - Training Epoch: 2/2, step 18925/23838 completed (loss: 1.9464681148529053, acc: 0.6111111044883728)
[2025-02-04 00:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:14][root][INFO] - Training Epoch: 2/2, step 18926/23838 completed (loss: 2.927834987640381, acc: 0.3333333432674408)
[2025-02-04 00:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:15][root][INFO] - Training Epoch: 2/2, step 18927/23838 completed (loss: 3.505632162094116, acc: 0.4444444477558136)
[2025-02-04 00:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:15][root][INFO] - Training Epoch: 2/2, step 18928/23838 completed (loss: 3.309675931930542, acc: 0.4285714328289032)
[2025-02-04 00:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:16][root][INFO] - Training Epoch: 2/2, step 18929/23838 completed (loss: 3.260195255279541, acc: 0.37837839126586914)
[2025-02-04 00:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:16][root][INFO] - Training Epoch: 2/2, step 18930/23838 completed (loss: 2.806211471557617, acc: 0.3333333432674408)
[2025-02-04 00:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:17][root][INFO] - Training Epoch: 2/2, step 18931/23838 completed (loss: 2.0481715202331543, acc: 0.4736842215061188)
[2025-02-04 00:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:17][root][INFO] - Training Epoch: 2/2, step 18932/23838 completed (loss: 2.5755138397216797, acc: 0.3333333432674408)
[2025-02-04 00:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:18][root][INFO] - Training Epoch: 2/2, step 18933/23838 completed (loss: 2.9636762142181396, acc: 0.48275861144065857)
[2025-02-04 00:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:18][root][INFO] - Training Epoch: 2/2, step 18934/23838 completed (loss: 2.417149305343628, acc: 0.4871794879436493)
[2025-02-04 00:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:19][root][INFO] - Training Epoch: 2/2, step 18935/23838 completed (loss: 2.9509027004241943, acc: 0.4838709533214569)
[2025-02-04 00:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:19][root][INFO] - Training Epoch: 2/2, step 18936/23838 completed (loss: 3.8383262157440186, acc: 0.3243243098258972)
[2025-02-04 00:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:20][root][INFO] - Training Epoch: 2/2, step 18937/23838 completed (loss: 3.176710605621338, acc: 0.35555556416511536)
[2025-02-04 00:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:20][root][INFO] - Training Epoch: 2/2, step 18938/23838 completed (loss: 2.432957172393799, acc: 0.6000000238418579)
[2025-02-04 00:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:20][root][INFO] - Training Epoch: 2/2, step 18939/23838 completed (loss: 2.19572377204895, acc: 0.5769230723381042)
[2025-02-04 00:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:21][root][INFO] - Training Epoch: 2/2, step 18940/23838 completed (loss: 3.3027071952819824, acc: 0.29411765933036804)
[2025-02-04 00:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:21][root][INFO] - Training Epoch: 2/2, step 18941/23838 completed (loss: 3.0880396366119385, acc: 0.41304346919059753)
[2025-02-04 00:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:22][root][INFO] - Training Epoch: 2/2, step 18942/23838 completed (loss: 2.425168514251709, acc: 0.4749999940395355)
[2025-02-04 00:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:22][root][INFO] - Training Epoch: 2/2, step 18943/23838 completed (loss: 4.069573402404785, acc: 0.40740740299224854)
[2025-02-04 00:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:23][root][INFO] - Training Epoch: 2/2, step 18944/23838 completed (loss: 3.6470487117767334, acc: 0.30000001192092896)
[2025-02-04 00:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:23][root][INFO] - Training Epoch: 2/2, step 18945/23838 completed (loss: 3.6849300861358643, acc: 0.29032257199287415)
[2025-02-04 00:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:23][root][INFO] - Training Epoch: 2/2, step 18946/23838 completed (loss: 2.923903703689575, acc: 0.4047619104385376)
[2025-02-04 00:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:24][root][INFO] - Training Epoch: 2/2, step 18947/23838 completed (loss: 3.3853402137756348, acc: 0.4000000059604645)
[2025-02-04 00:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:24][root][INFO] - Training Epoch: 2/2, step 18948/23838 completed (loss: 3.281085968017578, acc: 0.2666666805744171)
[2025-02-04 00:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:25][root][INFO] - Training Epoch: 2/2, step 18949/23838 completed (loss: 2.744191884994507, acc: 0.3913043439388275)
[2025-02-04 00:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:25][root][INFO] - Training Epoch: 2/2, step 18950/23838 completed (loss: 1.9740450382232666, acc: 0.550000011920929)
[2025-02-04 00:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:26][root][INFO] - Training Epoch: 2/2, step 18951/23838 completed (loss: 3.681847333908081, acc: 0.2711864411830902)
[2025-02-04 00:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:26][root][INFO] - Training Epoch: 2/2, step 18952/23838 completed (loss: 3.198646306991577, acc: 0.3384615480899811)
[2025-02-04 00:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:27][root][INFO] - Training Epoch: 2/2, step 18953/23838 completed (loss: 2.8862557411193848, acc: 0.4166666567325592)
[2025-02-04 00:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:27][root][INFO] - Training Epoch: 2/2, step 18954/23838 completed (loss: 2.6337273120880127, acc: 0.3870967626571655)
[2025-02-04 00:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:27][root][INFO] - Training Epoch: 2/2, step 18955/23838 completed (loss: 2.5206081867218018, acc: 0.3636363744735718)
[2025-02-04 00:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:28][root][INFO] - Training Epoch: 2/2, step 18956/23838 completed (loss: 2.8185441493988037, acc: 0.3684210479259491)
[2025-02-04 00:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:28][root][INFO] - Training Epoch: 2/2, step 18957/23838 completed (loss: 2.65177059173584, acc: 0.47999998927116394)
[2025-02-04 00:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:29][root][INFO] - Training Epoch: 2/2, step 18958/23838 completed (loss: 4.42449951171875, acc: 0.30000001192092896)
[2025-02-04 00:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:29][root][INFO] - Training Epoch: 2/2, step 18959/23838 completed (loss: 3.7724034786224365, acc: 0.2857142984867096)
[2025-02-04 00:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:30][root][INFO] - Training Epoch: 2/2, step 18960/23838 completed (loss: 2.0355613231658936, acc: 0.5789473652839661)
[2025-02-04 00:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:30][root][INFO] - Training Epoch: 2/2, step 18961/23838 completed (loss: 2.7883191108703613, acc: 0.4615384638309479)
[2025-02-04 00:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:31][root][INFO] - Training Epoch: 2/2, step 18962/23838 completed (loss: 3.4206442832946777, acc: 0.5714285969734192)
[2025-02-04 00:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:31][root][INFO] - Training Epoch: 2/2, step 18963/23838 completed (loss: 6.339654445648193, acc: 0.30000001192092896)
[2025-02-04 00:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:32][root][INFO] - Training Epoch: 2/2, step 18964/23838 completed (loss: 3.0439741611480713, acc: 0.5454545617103577)
[2025-02-04 00:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:32][root][INFO] - Training Epoch: 2/2, step 18965/23838 completed (loss: 3.866264581680298, acc: 0.3571428656578064)
[2025-02-04 00:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:32][root][INFO] - Training Epoch: 2/2, step 18966/23838 completed (loss: 3.467749834060669, acc: 0.48275861144065857)
[2025-02-04 00:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:33][root][INFO] - Training Epoch: 2/2, step 18967/23838 completed (loss: 4.334464073181152, acc: 0.3333333432674408)
[2025-02-04 00:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:33][root][INFO] - Training Epoch: 2/2, step 18968/23838 completed (loss: 3.135922908782959, acc: 0.4615384638309479)
[2025-02-04 00:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:34][root][INFO] - Training Epoch: 2/2, step 18969/23838 completed (loss: 3.599492073059082, acc: 0.529411792755127)
[2025-02-04 00:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:34][root][INFO] - Training Epoch: 2/2, step 18970/23838 completed (loss: 3.5027291774749756, acc: 0.4399999976158142)
[2025-02-04 00:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:34][root][INFO] - Training Epoch: 2/2, step 18971/23838 completed (loss: 2.1671009063720703, acc: 0.5882353186607361)
[2025-02-04 00:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:35][root][INFO] - Training Epoch: 2/2, step 18972/23838 completed (loss: 2.855928659439087, acc: 0.4166666567325592)
[2025-02-04 00:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:35][root][INFO] - Training Epoch: 2/2, step 18973/23838 completed (loss: 3.1750524044036865, acc: 0.3333333432674408)
[2025-02-04 00:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:35][root][INFO] - Training Epoch: 2/2, step 18974/23838 completed (loss: 3.750124931335449, acc: 0.2857142984867096)
[2025-02-04 00:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:36][root][INFO] - Training Epoch: 2/2, step 18975/23838 completed (loss: 4.200318336486816, acc: 0.23529411852359772)
[2025-02-04 00:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:36][root][INFO] - Training Epoch: 2/2, step 18976/23838 completed (loss: 3.1959409713745117, acc: 0.3888888955116272)
[2025-02-04 00:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:37][root][INFO] - Training Epoch: 2/2, step 18977/23838 completed (loss: 2.7386527061462402, acc: 0.5714285969734192)
[2025-02-04 00:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:37][root][INFO] - Training Epoch: 2/2, step 18978/23838 completed (loss: 3.823768138885498, acc: 0.47058823704719543)
[2025-02-04 00:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:37][root][INFO] - Training Epoch: 2/2, step 18979/23838 completed (loss: 2.4960451126098633, acc: 0.6111111044883728)
[2025-02-04 00:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:38][root][INFO] - Training Epoch: 2/2, step 18980/23838 completed (loss: 3.6468966007232666, acc: 0.4375)
[2025-02-04 00:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:38][root][INFO] - Training Epoch: 2/2, step 18981/23838 completed (loss: 2.8435330390930176, acc: 0.4000000059604645)
[2025-02-04 00:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:39][root][INFO] - Training Epoch: 2/2, step 18982/23838 completed (loss: 4.515528202056885, acc: 0.4285714328289032)
[2025-02-04 00:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:39][root][INFO] - Training Epoch: 2/2, step 18983/23838 completed (loss: 2.5599918365478516, acc: 0.5789473652839661)
[2025-02-04 00:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:40][root][INFO] - Training Epoch: 2/2, step 18984/23838 completed (loss: 2.5393662452697754, acc: 0.4285714328289032)
[2025-02-04 00:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:40][root][INFO] - Training Epoch: 2/2, step 18985/23838 completed (loss: 2.955686569213867, acc: 0.47058823704719543)
[2025-02-04 00:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:40][root][INFO] - Training Epoch: 2/2, step 18986/23838 completed (loss: 2.3824002742767334, acc: 0.38461539149284363)
[2025-02-04 00:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:41][root][INFO] - Training Epoch: 2/2, step 18987/23838 completed (loss: 4.119964599609375, acc: 0.29629629850387573)
[2025-02-04 00:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:41][root][INFO] - Training Epoch: 2/2, step 18988/23838 completed (loss: 2.4681973457336426, acc: 0.5555555820465088)
[2025-02-04 00:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:42][root][INFO] - Training Epoch: 2/2, step 18989/23838 completed (loss: 3.3775320053100586, acc: 0.4117647111415863)
[2025-02-04 00:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:42][root][INFO] - Training Epoch: 2/2, step 18990/23838 completed (loss: 2.478013753890991, acc: 0.5)
[2025-02-04 00:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:43][root][INFO] - Training Epoch: 2/2, step 18991/23838 completed (loss: 3.5821526050567627, acc: 0.40909090638160706)
[2025-02-04 00:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:43][root][INFO] - Training Epoch: 2/2, step 18992/23838 completed (loss: 3.319459915161133, acc: 0.4583333432674408)
[2025-02-04 00:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:43][root][INFO] - Training Epoch: 2/2, step 18993/23838 completed (loss: 3.711902379989624, acc: 0.3461538553237915)
[2025-02-04 00:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:44][root][INFO] - Training Epoch: 2/2, step 18994/23838 completed (loss: 3.3503851890563965, acc: 0.30434781312942505)
[2025-02-04 00:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:44][root][INFO] - Training Epoch: 2/2, step 18995/23838 completed (loss: 3.566514730453491, acc: 0.2916666567325592)
[2025-02-04 00:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:45][root][INFO] - Training Epoch: 2/2, step 18996/23838 completed (loss: 2.0879719257354736, acc: 0.5)
[2025-02-04 00:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:45][root][INFO] - Training Epoch: 2/2, step 18997/23838 completed (loss: 3.240823268890381, acc: 0.36000001430511475)
[2025-02-04 00:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:46][root][INFO] - Training Epoch: 2/2, step 18998/23838 completed (loss: 4.531366348266602, acc: 0.2068965584039688)
[2025-02-04 00:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:46][root][INFO] - Training Epoch: 2/2, step 18999/23838 completed (loss: 4.039892673492432, acc: 0.3571428656578064)
[2025-02-04 00:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:47][root][INFO] - Training Epoch: 2/2, step 19000/23838 completed (loss: 3.361774444580078, acc: 0.29411765933036804)
[2025-02-04 00:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:47][root][INFO] - Training Epoch: 2/2, step 19001/23838 completed (loss: 3.7590043544769287, acc: 0.375)
[2025-02-04 00:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:48][root][INFO] - Training Epoch: 2/2, step 19002/23838 completed (loss: 4.221132755279541, acc: 0.29032257199287415)
[2025-02-04 00:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:48][root][INFO] - Training Epoch: 2/2, step 19003/23838 completed (loss: 3.9983556270599365, acc: 0.3333333432674408)
[2025-02-04 00:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:48][root][INFO] - Training Epoch: 2/2, step 19004/23838 completed (loss: 1.0678553581237793, acc: 0.7272727489471436)
[2025-02-04 00:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:49][root][INFO] - Training Epoch: 2/2, step 19005/23838 completed (loss: 3.6424477100372314, acc: 0.3913043439388275)
[2025-02-04 00:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:49][root][INFO] - Training Epoch: 2/2, step 19006/23838 completed (loss: 3.5266778469085693, acc: 0.31707316637039185)
[2025-02-04 00:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:50][root][INFO] - Training Epoch: 2/2, step 19007/23838 completed (loss: 2.9307711124420166, acc: 0.47826087474823)
[2025-02-04 00:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:50][root][INFO] - Training Epoch: 2/2, step 19008/23838 completed (loss: 3.2971408367156982, acc: 0.38235294818878174)
[2025-02-04 00:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:51][root][INFO] - Training Epoch: 2/2, step 19009/23838 completed (loss: 3.91284441947937, acc: 0.3235294222831726)
[2025-02-04 00:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:51][root][INFO] - Training Epoch: 2/2, step 19010/23838 completed (loss: 3.0158491134643555, acc: 0.38461539149284363)
[2025-02-04 00:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:52][root][INFO] - Training Epoch: 2/2, step 19011/23838 completed (loss: 2.759744644165039, acc: 0.2916666567325592)
[2025-02-04 00:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:52][root][INFO] - Training Epoch: 2/2, step 19012/23838 completed (loss: 2.6042590141296387, acc: 0.380952388048172)
[2025-02-04 00:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:52][root][INFO] - Training Epoch: 2/2, step 19013/23838 completed (loss: 3.4854745864868164, acc: 0.23999999463558197)
[2025-02-04 00:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:53][root][INFO] - Training Epoch: 2/2, step 19014/23838 completed (loss: 3.33528733253479, acc: 0.35483869910240173)
[2025-02-04 00:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:53][root][INFO] - Training Epoch: 2/2, step 19015/23838 completed (loss: 3.4405629634857178, acc: 0.4117647111415863)
[2025-02-04 00:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:54][root][INFO] - Training Epoch: 2/2, step 19016/23838 completed (loss: 3.158828020095825, acc: 0.3636363744735718)
[2025-02-04 00:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:54][root][INFO] - Training Epoch: 2/2, step 19017/23838 completed (loss: 1.3590357303619385, acc: 0.625)
[2025-02-04 00:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:54][root][INFO] - Training Epoch: 2/2, step 19018/23838 completed (loss: 3.7280876636505127, acc: 0.3076923191547394)
[2025-02-04 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:55][root][INFO] - Training Epoch: 2/2, step 19019/23838 completed (loss: 2.7708804607391357, acc: 0.5263158082962036)
[2025-02-04 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:55][root][INFO] - Training Epoch: 2/2, step 19020/23838 completed (loss: 3.4751110076904297, acc: 0.4285714328289032)
[2025-02-04 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:56][root][INFO] - Training Epoch: 2/2, step 19021/23838 completed (loss: 3.0046567916870117, acc: 0.5384615659713745)
[2025-02-04 00:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:56][root][INFO] - Training Epoch: 2/2, step 19022/23838 completed (loss: 2.7739968299865723, acc: 0.4285714328289032)
[2025-02-04 00:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:57][root][INFO] - Training Epoch: 2/2, step 19023/23838 completed (loss: 3.8190183639526367, acc: 0.3103448152542114)
[2025-02-04 00:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:57][root][INFO] - Training Epoch: 2/2, step 19024/23838 completed (loss: 3.8421483039855957, acc: 0.3333333432674408)
[2025-02-04 00:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:58][root][INFO] - Training Epoch: 2/2, step 19025/23838 completed (loss: 3.898125171661377, acc: 0.25)
[2025-02-04 00:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:58][root][INFO] - Training Epoch: 2/2, step 19026/23838 completed (loss: 4.28727388381958, acc: 0.38235294818878174)
[2025-02-04 00:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:59][root][INFO] - Training Epoch: 2/2, step 19027/23838 completed (loss: 2.4158990383148193, acc: 0.5862069129943848)
[2025-02-04 00:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:41:59][root][INFO] - Training Epoch: 2/2, step 19028/23838 completed (loss: 4.212635517120361, acc: 0.3888888955116272)
[2025-02-04 00:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:00][root][INFO] - Training Epoch: 2/2, step 19029/23838 completed (loss: 3.494969606399536, acc: 0.5)
[2025-02-04 00:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:00][root][INFO] - Training Epoch: 2/2, step 19030/23838 completed (loss: 4.880407810211182, acc: 0.2800000011920929)
[2025-02-04 00:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:01][root][INFO] - Training Epoch: 2/2, step 19031/23838 completed (loss: 3.227769136428833, acc: 0.5714285969734192)
[2025-02-04 00:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:01][root][INFO] - Training Epoch: 2/2, step 19032/23838 completed (loss: 4.229923248291016, acc: 0.44117647409439087)
[2025-02-04 00:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:01][root][INFO] - Training Epoch: 2/2, step 19033/23838 completed (loss: 4.687307357788086, acc: 0.3589743673801422)
[2025-02-04 00:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:02][root][INFO] - Training Epoch: 2/2, step 19034/23838 completed (loss: 3.9070587158203125, acc: 0.37142857909202576)
[2025-02-04 00:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:03][root][INFO] - Training Epoch: 2/2, step 19035/23838 completed (loss: 3.9686622619628906, acc: 0.4000000059604645)
[2025-02-04 00:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:03][root][INFO] - Training Epoch: 2/2, step 19036/23838 completed (loss: 2.2920138835906982, acc: 0.44999998807907104)
[2025-02-04 00:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:04][root][INFO] - Training Epoch: 2/2, step 19037/23838 completed (loss: 3.7755398750305176, acc: 0.4482758641242981)
[2025-02-04 00:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:04][root][INFO] - Training Epoch: 2/2, step 19038/23838 completed (loss: 3.341693639755249, acc: 0.46875)
[2025-02-04 00:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:04][root][INFO] - Training Epoch: 2/2, step 19039/23838 completed (loss: 3.7907614707946777, acc: 0.4545454680919647)
[2025-02-04 00:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:05][root][INFO] - Training Epoch: 2/2, step 19040/23838 completed (loss: 3.7532334327697754, acc: 0.4000000059604645)
[2025-02-04 00:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:05][root][INFO] - Training Epoch: 2/2, step 19041/23838 completed (loss: 4.384274482727051, acc: 0.3636363744735718)
[2025-02-04 00:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:06][root][INFO] - Training Epoch: 2/2, step 19042/23838 completed (loss: 4.312187671661377, acc: 0.34285715222358704)
[2025-02-04 00:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:07][root][INFO] - Training Epoch: 2/2, step 19043/23838 completed (loss: 3.779082775115967, acc: 0.4736842215061188)
[2025-02-04 00:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:07][root][INFO] - Training Epoch: 2/2, step 19044/23838 completed (loss: 4.222774028778076, acc: 0.3181818127632141)
[2025-02-04 00:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:08][root][INFO] - Training Epoch: 2/2, step 19045/23838 completed (loss: 3.8629181385040283, acc: 0.2666666805744171)
[2025-02-04 00:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:08][root][INFO] - Training Epoch: 2/2, step 19046/23838 completed (loss: 4.662195205688477, acc: 0.39534884691238403)
[2025-02-04 00:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:09][root][INFO] - Training Epoch: 2/2, step 19047/23838 completed (loss: 4.086520671844482, acc: 0.4166666567325592)
[2025-02-04 00:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:10][root][INFO] - Training Epoch: 2/2, step 19048/23838 completed (loss: 4.745157241821289, acc: 0.2666666805744171)
[2025-02-04 00:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:10][root][INFO] - Training Epoch: 2/2, step 19049/23838 completed (loss: 2.867475986480713, acc: 0.5185185074806213)
[2025-02-04 00:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:11][root][INFO] - Training Epoch: 2/2, step 19050/23838 completed (loss: 4.216079235076904, acc: 0.4000000059604645)
[2025-02-04 00:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:11][root][INFO] - Training Epoch: 2/2, step 19051/23838 completed (loss: 4.324051856994629, acc: 0.30434781312942505)
[2025-02-04 00:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:12][root][INFO] - Training Epoch: 2/2, step 19052/23838 completed (loss: 3.185657024383545, acc: 0.36666667461395264)
[2025-02-04 00:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:12][root][INFO] - Training Epoch: 2/2, step 19053/23838 completed (loss: 3.8547017574310303, acc: 0.39393940567970276)
[2025-02-04 00:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:13][root][INFO] - Training Epoch: 2/2, step 19054/23838 completed (loss: 3.114550828933716, acc: 0.3478260934352875)
[2025-02-04 00:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:13][root][INFO] - Training Epoch: 2/2, step 19055/23838 completed (loss: 2.847154378890991, acc: 0.4545454680919647)
[2025-02-04 00:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:14][root][INFO] - Training Epoch: 2/2, step 19056/23838 completed (loss: 3.797882080078125, acc: 0.2666666805744171)
[2025-02-04 00:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:14][root][INFO] - Training Epoch: 2/2, step 19057/23838 completed (loss: 3.284735679626465, acc: 0.42222222685813904)
[2025-02-04 00:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:15][root][INFO] - Training Epoch: 2/2, step 19058/23838 completed (loss: 3.829455852508545, acc: 0.47999998927116394)
[2025-02-04 00:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:15][root][INFO] - Training Epoch: 2/2, step 19059/23838 completed (loss: 4.541417598724365, acc: 0.2857142984867096)
[2025-02-04 00:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:16][root][INFO] - Training Epoch: 2/2, step 19060/23838 completed (loss: 5.3549580574035645, acc: 0.21052631735801697)
[2025-02-04 00:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:16][root][INFO] - Training Epoch: 2/2, step 19061/23838 completed (loss: 4.238154411315918, acc: 0.2702702581882477)
[2025-02-04 00:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:17][root][INFO] - Training Epoch: 2/2, step 19062/23838 completed (loss: 2.4377949237823486, acc: 0.47826087474823)
[2025-02-04 00:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:17][root][INFO] - Training Epoch: 2/2, step 19063/23838 completed (loss: 4.401569366455078, acc: 0.25)
[2025-02-04 00:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:18][root][INFO] - Training Epoch: 2/2, step 19064/23838 completed (loss: 2.591866970062256, acc: 0.42307692766189575)
[2025-02-04 00:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:19][root][INFO] - Training Epoch: 2/2, step 19065/23838 completed (loss: 4.582260608673096, acc: 0.3035714328289032)
[2025-02-04 00:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:19][root][INFO] - Training Epoch: 2/2, step 19066/23838 completed (loss: 3.642014980316162, acc: 0.42307692766189575)
[2025-02-04 00:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:19][root][INFO] - Training Epoch: 2/2, step 19067/23838 completed (loss: 4.6742024421691895, acc: 0.4000000059604645)
[2025-02-04 00:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:20][root][INFO] - Training Epoch: 2/2, step 19068/23838 completed (loss: 4.665253639221191, acc: 0.3333333432674408)
[2025-02-04 00:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:21][root][INFO] - Training Epoch: 2/2, step 19069/23838 completed (loss: 3.39017391204834, acc: 0.40909090638160706)
[2025-02-04 00:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:21][root][INFO] - Training Epoch: 2/2, step 19070/23838 completed (loss: 3.9061167240142822, acc: 0.37931033968925476)
[2025-02-04 00:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:22][root][INFO] - Training Epoch: 2/2, step 19071/23838 completed (loss: 3.4972996711730957, acc: 0.3414634168148041)
[2025-02-04 00:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:22][root][INFO] - Training Epoch: 2/2, step 19072/23838 completed (loss: 4.038028717041016, acc: 0.3636363744735718)
[2025-02-04 00:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:22][root][INFO] - Training Epoch: 2/2, step 19073/23838 completed (loss: 1.6271735429763794, acc: 0.6470588445663452)
[2025-02-04 00:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:23][root][INFO] - Training Epoch: 2/2, step 19074/23838 completed (loss: 2.92899227142334, acc: 0.47826087474823)
[2025-02-04 00:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:23][root][INFO] - Training Epoch: 2/2, step 19075/23838 completed (loss: 4.889151573181152, acc: 0.1818181872367859)
[2025-02-04 00:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:24][root][INFO] - Training Epoch: 2/2, step 19076/23838 completed (loss: 6.560891628265381, acc: 0.21052631735801697)
[2025-02-04 00:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:24][root][INFO] - Training Epoch: 2/2, step 19077/23838 completed (loss: 4.349850177764893, acc: 0.3488371968269348)
[2025-02-04 00:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:25][root][INFO] - Training Epoch: 2/2, step 19078/23838 completed (loss: 4.929481029510498, acc: 0.3199999928474426)
[2025-02-04 00:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:25][root][INFO] - Training Epoch: 2/2, step 19079/23838 completed (loss: 5.748372554779053, acc: 0.23076923191547394)
[2025-02-04 00:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:26][root][INFO] - Training Epoch: 2/2, step 19080/23838 completed (loss: 4.984788417816162, acc: 0.4166666567325592)
[2025-02-04 00:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:26][root][INFO] - Training Epoch: 2/2, step 19081/23838 completed (loss: 4.500589370727539, acc: 0.37142857909202576)
[2025-02-04 00:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:27][root][INFO] - Training Epoch: 2/2, step 19082/23838 completed (loss: 4.81336784362793, acc: 0.29729729890823364)
[2025-02-04 00:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:27][root][INFO] - Training Epoch: 2/2, step 19083/23838 completed (loss: 4.00313138961792, acc: 0.3235294222831726)
[2025-02-04 00:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:28][root][INFO] - Training Epoch: 2/2, step 19084/23838 completed (loss: 4.070972919464111, acc: 0.2916666567325592)
[2025-02-04 00:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:28][root][INFO] - Training Epoch: 2/2, step 19085/23838 completed (loss: 3.7390077114105225, acc: 0.38235294818878174)
[2025-02-04 00:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:28][root][INFO] - Training Epoch: 2/2, step 19086/23838 completed (loss: 3.951263904571533, acc: 0.3103448152542114)
[2025-02-04 00:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:29][root][INFO] - Training Epoch: 2/2, step 19087/23838 completed (loss: 3.9404945373535156, acc: 0.46666666865348816)
[2025-02-04 00:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:29][root][INFO] - Training Epoch: 2/2, step 19088/23838 completed (loss: 4.247608184814453, acc: 0.3103448152542114)
[2025-02-04 00:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:30][root][INFO] - Training Epoch: 2/2, step 19089/23838 completed (loss: 5.821191787719727, acc: 0.28205129504203796)
[2025-02-04 00:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:30][root][INFO] - Training Epoch: 2/2, step 19090/23838 completed (loss: 5.471276760101318, acc: 0.25)
[2025-02-04 00:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:31][root][INFO] - Training Epoch: 2/2, step 19091/23838 completed (loss: 3.189328908920288, acc: 0.4761904776096344)
[2025-02-04 00:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:31][root][INFO] - Training Epoch: 2/2, step 19092/23838 completed (loss: 2.5125207901000977, acc: 0.5789473652839661)
[2025-02-04 00:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:32][root][INFO] - Training Epoch: 2/2, step 19093/23838 completed (loss: 2.8805184364318848, acc: 0.4615384638309479)
[2025-02-04 00:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:32][root][INFO] - Training Epoch: 2/2, step 19094/23838 completed (loss: 4.930975914001465, acc: 0.3571428656578064)
[2025-02-04 00:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:32][root][INFO] - Training Epoch: 2/2, step 19095/23838 completed (loss: 4.01121187210083, acc: 0.3199999928474426)
[2025-02-04 00:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:33][root][INFO] - Training Epoch: 2/2, step 19096/23838 completed (loss: 3.90128755569458, acc: 0.380952388048172)
[2025-02-04 00:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:33][root][INFO] - Training Epoch: 2/2, step 19097/23838 completed (loss: 4.288168907165527, acc: 0.3461538553237915)
[2025-02-04 00:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:34][root][INFO] - Training Epoch: 2/2, step 19098/23838 completed (loss: 2.13716459274292, acc: 0.6190476417541504)
[2025-02-04 00:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:34][root][INFO] - Training Epoch: 2/2, step 19099/23838 completed (loss: 3.7510275840759277, acc: 0.2432432472705841)
[2025-02-04 00:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:35][root][INFO] - Training Epoch: 2/2, step 19100/23838 completed (loss: 3.9817967414855957, acc: 0.3636363744735718)
[2025-02-04 00:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:35][root][INFO] - Training Epoch: 2/2, step 19101/23838 completed (loss: 2.8226916790008545, acc: 0.5384615659713745)
[2025-02-04 00:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:35][root][INFO] - Training Epoch: 2/2, step 19102/23838 completed (loss: 2.792900800704956, acc: 0.6666666865348816)
[2025-02-04 00:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:36][root][INFO] - Training Epoch: 2/2, step 19103/23838 completed (loss: 3.9323368072509766, acc: 0.3333333432674408)
[2025-02-04 00:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:36][root][INFO] - Training Epoch: 2/2, step 19104/23838 completed (loss: 3.859351396560669, acc: 0.34285715222358704)
[2025-02-04 00:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:37][root][INFO] - Training Epoch: 2/2, step 19105/23838 completed (loss: 2.545156478881836, acc: 0.5454545617103577)
[2025-02-04 00:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:37][root][INFO] - Training Epoch: 2/2, step 19106/23838 completed (loss: 3.406646490097046, acc: 0.4444444477558136)
[2025-02-04 00:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:38][root][INFO] - Training Epoch: 2/2, step 19107/23838 completed (loss: 3.8327221870422363, acc: 0.35483869910240173)
[2025-02-04 00:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:38][root][INFO] - Training Epoch: 2/2, step 19108/23838 completed (loss: 2.4449079036712646, acc: 0.4285714328289032)
[2025-02-04 00:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:38][root][INFO] - Training Epoch: 2/2, step 19109/23838 completed (loss: 2.4560773372650146, acc: 0.6000000238418579)
[2025-02-04 00:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:39][root][INFO] - Training Epoch: 2/2, step 19110/23838 completed (loss: 3.593492031097412, acc: 0.42307692766189575)
[2025-02-04 00:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:39][root][INFO] - Training Epoch: 2/2, step 19111/23838 completed (loss: 3.6797995567321777, acc: 0.4137931168079376)
[2025-02-04 00:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:40][root][INFO] - Training Epoch: 2/2, step 19112/23838 completed (loss: 2.282616376876831, acc: 0.5)
[2025-02-04 00:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:40][root][INFO] - Training Epoch: 2/2, step 19113/23838 completed (loss: 4.058467864990234, acc: 0.2571428716182709)
[2025-02-04 00:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:41][root][INFO] - Training Epoch: 2/2, step 19114/23838 completed (loss: 3.4110445976257324, acc: 0.4444444477558136)
[2025-02-04 00:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:41][root][INFO] - Training Epoch: 2/2, step 19115/23838 completed (loss: 4.281311511993408, acc: 0.3611111044883728)
[2025-02-04 00:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:42][root][INFO] - Training Epoch: 2/2, step 19116/23838 completed (loss: 2.2123589515686035, acc: 0.529411792755127)
[2025-02-04 00:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:42][root][INFO] - Training Epoch: 2/2, step 19117/23838 completed (loss: 5.288260459899902, acc: 0.38461539149284363)
[2025-02-04 00:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:42][root][INFO] - Training Epoch: 2/2, step 19118/23838 completed (loss: 2.721229076385498, acc: 0.3529411852359772)
[2025-02-04 00:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:43][root][INFO] - Training Epoch: 2/2, step 19119/23838 completed (loss: 2.3393237590789795, acc: 0.5833333134651184)
[2025-02-04 00:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:43][root][INFO] - Training Epoch: 2/2, step 19120/23838 completed (loss: 3.0047476291656494, acc: 0.2857142984867096)
[2025-02-04 00:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:44][root][INFO] - Training Epoch: 2/2, step 19121/23838 completed (loss: 3.4437174797058105, acc: 0.3571428656578064)
[2025-02-04 00:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:44][root][INFO] - Training Epoch: 2/2, step 19122/23838 completed (loss: 2.532118082046509, acc: 0.4444444477558136)
[2025-02-04 00:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:45][root][INFO] - Training Epoch: 2/2, step 19123/23838 completed (loss: 2.1626133918762207, acc: 0.7272727489471436)
[2025-02-04 00:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:45][root][INFO] - Training Epoch: 2/2, step 19124/23838 completed (loss: 3.719324827194214, acc: 0.3333333432674408)
[2025-02-04 00:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:45][root][INFO] - Training Epoch: 2/2, step 19125/23838 completed (loss: 2.884558916091919, acc: 0.4545454680919647)
[2025-02-04 00:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:46][root][INFO] - Training Epoch: 2/2, step 19126/23838 completed (loss: 2.048356533050537, acc: 0.692307710647583)
[2025-02-04 00:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:46][root][INFO] - Training Epoch: 2/2, step 19127/23838 completed (loss: 4.875795364379883, acc: 0.1764705926179886)
[2025-02-04 00:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:47][root][INFO] - Training Epoch: 2/2, step 19128/23838 completed (loss: 4.2385573387146, acc: 0.302325576543808)
[2025-02-04 00:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:47][root][INFO] - Training Epoch: 2/2, step 19129/23838 completed (loss: 4.013651371002197, acc: 0.2142857164144516)
[2025-02-04 00:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:48][root][INFO] - Training Epoch: 2/2, step 19130/23838 completed (loss: 3.4880716800689697, acc: 0.3448275923728943)
[2025-02-04 00:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:48][root][INFO] - Training Epoch: 2/2, step 19131/23838 completed (loss: 2.94168758392334, acc: 0.4117647111415863)
[2025-02-04 00:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:49][root][INFO] - Training Epoch: 2/2, step 19132/23838 completed (loss: 3.0209720134735107, acc: 0.35483869910240173)
[2025-02-04 00:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:49][root][INFO] - Training Epoch: 2/2, step 19133/23838 completed (loss: 3.6729016304016113, acc: 0.25)
[2025-02-04 00:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:49][root][INFO] - Training Epoch: 2/2, step 19134/23838 completed (loss: 3.4434380531311035, acc: 0.3684210479259491)
[2025-02-04 00:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:50][root][INFO] - Training Epoch: 2/2, step 19135/23838 completed (loss: 2.9061806201934814, acc: 0.3142857253551483)
[2025-02-04 00:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:50][root][INFO] - Training Epoch: 2/2, step 19136/23838 completed (loss: 4.160099983215332, acc: 0.27586206793785095)
[2025-02-04 00:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:51][root][INFO] - Training Epoch: 2/2, step 19137/23838 completed (loss: 3.2603108882904053, acc: 0.3513513505458832)
[2025-02-04 00:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:51][root][INFO] - Training Epoch: 2/2, step 19138/23838 completed (loss: 3.7074692249298096, acc: 0.3181818127632141)
[2025-02-04 00:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:52][root][INFO] - Training Epoch: 2/2, step 19139/23838 completed (loss: 3.0133249759674072, acc: 0.5142857432365417)
[2025-02-04 00:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:52][root][INFO] - Training Epoch: 2/2, step 19140/23838 completed (loss: 3.713909149169922, acc: 0.37837839126586914)
[2025-02-04 00:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:53][root][INFO] - Training Epoch: 2/2, step 19141/23838 completed (loss: 3.2603228092193604, acc: 0.39534884691238403)
[2025-02-04 00:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:53][root][INFO] - Training Epoch: 2/2, step 19142/23838 completed (loss: 3.0458555221557617, acc: 0.42424243688583374)
[2025-02-04 00:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:53][root][INFO] - Training Epoch: 2/2, step 19143/23838 completed (loss: 2.805548667907715, acc: 0.3913043439388275)
[2025-02-04 00:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:54][root][INFO] - Training Epoch: 2/2, step 19144/23838 completed (loss: 4.464974880218506, acc: 0.1111111119389534)
[2025-02-04 00:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:54][root][INFO] - Training Epoch: 2/2, step 19145/23838 completed (loss: 2.8476271629333496, acc: 0.4642857015132904)
[2025-02-04 00:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:55][root][INFO] - Training Epoch: 2/2, step 19146/23838 completed (loss: 3.9582698345184326, acc: 0.2195121943950653)
[2025-02-04 00:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:55][root][INFO] - Training Epoch: 2/2, step 19147/23838 completed (loss: 3.90331768989563, acc: 0.3611111044883728)
[2025-02-04 00:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:56][root][INFO] - Training Epoch: 2/2, step 19148/23838 completed (loss: 3.4814608097076416, acc: 0.3448275923728943)
[2025-02-04 00:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:56][root][INFO] - Training Epoch: 2/2, step 19149/23838 completed (loss: 3.082967519760132, acc: 0.4117647111415863)
[2025-02-04 00:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:56][root][INFO] - Training Epoch: 2/2, step 19150/23838 completed (loss: 2.708879232406616, acc: 0.4545454680919647)
[2025-02-04 00:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:57][root][INFO] - Training Epoch: 2/2, step 19151/23838 completed (loss: 2.380457878112793, acc: 0.5)
[2025-02-04 00:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:57][root][INFO] - Training Epoch: 2/2, step 19152/23838 completed (loss: 3.0409951210021973, acc: 0.4545454680919647)
[2025-02-04 00:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:58][root][INFO] - Training Epoch: 2/2, step 19153/23838 completed (loss: 3.2317233085632324, acc: 0.4117647111415863)
[2025-02-04 00:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:58][root][INFO] - Training Epoch: 2/2, step 19154/23838 completed (loss: 1.9435094594955444, acc: 0.6666666865348816)
[2025-02-04 00:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:59][root][INFO] - Training Epoch: 2/2, step 19155/23838 completed (loss: 2.2636396884918213, acc: 0.5454545617103577)
[2025-02-04 00:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:59][root][INFO] - Training Epoch: 2/2, step 19156/23838 completed (loss: 2.7382888793945312, acc: 0.48275861144065857)
[2025-02-04 00:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:42:59][root][INFO] - Training Epoch: 2/2, step 19157/23838 completed (loss: 2.663975715637207, acc: 0.31578946113586426)
[2025-02-04 00:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:00][root][INFO] - Training Epoch: 2/2, step 19158/23838 completed (loss: 4.2522406578063965, acc: 0.1746031790971756)
[2025-02-04 00:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:00][root][INFO] - Training Epoch: 2/2, step 19159/23838 completed (loss: 3.715006113052368, acc: 0.31111112236976624)
[2025-02-04 00:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:01][root][INFO] - Training Epoch: 2/2, step 19160/23838 completed (loss: 3.6947314739227295, acc: 0.3235294222831726)
[2025-02-04 00:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:01][root][INFO] - Training Epoch: 2/2, step 19161/23838 completed (loss: 3.8559091091156006, acc: 0.2884615361690521)
[2025-02-04 00:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:01][root][INFO] - Training Epoch: 2/2, step 19162/23838 completed (loss: 4.032156467437744, acc: 0.28358209133148193)
[2025-02-04 00:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:02][root][INFO] - Training Epoch: 2/2, step 19163/23838 completed (loss: 3.764876127243042, acc: 0.37142857909202576)
[2025-02-04 00:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:02][root][INFO] - Training Epoch: 2/2, step 19164/23838 completed (loss: 3.279590368270874, acc: 0.3888888955116272)
[2025-02-04 00:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:03][root][INFO] - Training Epoch: 2/2, step 19165/23838 completed (loss: 3.4302492141723633, acc: 0.3055555522441864)
[2025-02-04 00:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:03][root][INFO] - Training Epoch: 2/2, step 19166/23838 completed (loss: 3.9282710552215576, acc: 0.34285715222358704)
[2025-02-04 00:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:03][root][INFO] - Training Epoch: 2/2, step 19167/23838 completed (loss: 3.7057454586029053, acc: 0.3333333432674408)
[2025-02-04 00:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:04][root][INFO] - Training Epoch: 2/2, step 19168/23838 completed (loss: 4.212491512298584, acc: 0.3888888955116272)
[2025-02-04 00:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:04][root][INFO] - Training Epoch: 2/2, step 19169/23838 completed (loss: 3.3308956623077393, acc: 0.4117647111415863)
[2025-02-04 00:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:05][root][INFO] - Training Epoch: 2/2, step 19170/23838 completed (loss: 2.4218804836273193, acc: 0.4482758641242981)
[2025-02-04 00:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:05][root][INFO] - Training Epoch: 2/2, step 19171/23838 completed (loss: 3.3454535007476807, acc: 0.4000000059604645)
[2025-02-04 00:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:06][root][INFO] - Training Epoch: 2/2, step 19172/23838 completed (loss: 3.7535812854766846, acc: 0.4736842215061188)
[2025-02-04 00:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:06][root][INFO] - Training Epoch: 2/2, step 19173/23838 completed (loss: 3.038780927658081, acc: 0.4642857015132904)
[2025-02-04 00:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:06][root][INFO] - Training Epoch: 2/2, step 19174/23838 completed (loss: 2.823864459991455, acc: 0.37931033968925476)
[2025-02-04 00:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:07][root][INFO] - Training Epoch: 2/2, step 19175/23838 completed (loss: 3.251467704772949, acc: 0.27906978130340576)
[2025-02-04 00:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:07][root][INFO] - Training Epoch: 2/2, step 19176/23838 completed (loss: 4.129363536834717, acc: 0.2857142984867096)
[2025-02-04 00:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:08][root][INFO] - Training Epoch: 2/2, step 19177/23838 completed (loss: 2.983342170715332, acc: 0.41999998688697815)
[2025-02-04 00:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:08][root][INFO] - Training Epoch: 2/2, step 19178/23838 completed (loss: 4.008242130279541, acc: 0.3095238208770752)
[2025-02-04 00:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:09][root][INFO] - Training Epoch: 2/2, step 19179/23838 completed (loss: 3.6568663120269775, acc: 0.3205128312110901)
[2025-02-04 00:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:09][root][INFO] - Training Epoch: 2/2, step 19180/23838 completed (loss: 4.310734748840332, acc: 0.26153847575187683)
[2025-02-04 00:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:09][root][INFO] - Training Epoch: 2/2, step 19181/23838 completed (loss: 3.22986102104187, acc: 0.31578946113586426)
[2025-02-04 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:10][root][INFO] - Training Epoch: 2/2, step 19182/23838 completed (loss: 4.060414791107178, acc: 0.28125)
[2025-02-04 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:10][root][INFO] - Training Epoch: 2/2, step 19183/23838 completed (loss: 2.8590903282165527, acc: 0.29032257199287415)
[2025-02-04 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:11][root][INFO] - Training Epoch: 2/2, step 19184/23838 completed (loss: 4.006159782409668, acc: 0.3913043439388275)
[2025-02-04 00:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:11][root][INFO] - Training Epoch: 2/2, step 19185/23838 completed (loss: 3.2137608528137207, acc: 0.4545454680919647)
[2025-02-04 00:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:12][root][INFO] - Training Epoch: 2/2, step 19186/23838 completed (loss: 3.1467249393463135, acc: 0.40625)
[2025-02-04 00:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:12][root][INFO] - Training Epoch: 2/2, step 19187/23838 completed (loss: 3.9706289768218994, acc: 0.39534884691238403)
[2025-02-04 00:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:12][root][INFO] - Training Epoch: 2/2, step 19188/23838 completed (loss: 3.71279239654541, acc: 0.37837839126586914)
[2025-02-04 00:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:13][root][INFO] - Training Epoch: 2/2, step 19189/23838 completed (loss: 1.9455146789550781, acc: 0.625)
[2025-02-04 00:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:13][root][INFO] - Training Epoch: 2/2, step 19190/23838 completed (loss: 3.58162522315979, acc: 0.2857142984867096)
[2025-02-04 00:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:14][root][INFO] - Training Epoch: 2/2, step 19191/23838 completed (loss: 2.181710720062256, acc: 0.4482758641242981)
[2025-02-04 00:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:14][root][INFO] - Training Epoch: 2/2, step 19192/23838 completed (loss: 3.4042673110961914, acc: 0.37837839126586914)
[2025-02-04 00:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:14][root][INFO] - Training Epoch: 2/2, step 19193/23838 completed (loss: 4.201254844665527, acc: 0.28205129504203796)
[2025-02-04 00:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:15][root][INFO] - Training Epoch: 2/2, step 19194/23838 completed (loss: 3.451014757156372, acc: 0.27659574151039124)
[2025-02-04 00:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:15][root][INFO] - Training Epoch: 2/2, step 19195/23838 completed (loss: 3.577025890350342, acc: 0.3333333432674408)
[2025-02-04 00:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:16][root][INFO] - Training Epoch: 2/2, step 19196/23838 completed (loss: 2.8014731407165527, acc: 0.23076923191547394)
[2025-02-04 00:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:16][root][INFO] - Training Epoch: 2/2, step 19197/23838 completed (loss: 3.3894758224487305, acc: 0.40740740299224854)
[2025-02-04 00:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:16][root][INFO] - Training Epoch: 2/2, step 19198/23838 completed (loss: 2.808058023452759, acc: 0.5333333611488342)
[2025-02-04 00:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:17][root][INFO] - Training Epoch: 2/2, step 19199/23838 completed (loss: 2.3458640575408936, acc: 0.3888888955116272)
[2025-02-04 00:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:17][root][INFO] - Training Epoch: 2/2, step 19200/23838 completed (loss: 3.5785374641418457, acc: 0.4117647111415863)
[2025-02-04 00:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:18][root][INFO] - Training Epoch: 2/2, step 19201/23838 completed (loss: 3.632551670074463, acc: 0.32258063554763794)
[2025-02-04 00:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:18][root][INFO] - Training Epoch: 2/2, step 19202/23838 completed (loss: 3.085758924484253, acc: 0.3870967626571655)
[2025-02-04 00:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:19][root][INFO] - Training Epoch: 2/2, step 19203/23838 completed (loss: 2.9559640884399414, acc: 0.5)
[2025-02-04 00:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:19][root][INFO] - Training Epoch: 2/2, step 19204/23838 completed (loss: 3.682389259338379, acc: 0.2916666567325592)
[2025-02-04 00:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:19][root][INFO] - Training Epoch: 2/2, step 19205/23838 completed (loss: 2.9840314388275146, acc: 0.40740740299224854)
[2025-02-04 00:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:20][root][INFO] - Training Epoch: 2/2, step 19206/23838 completed (loss: 2.514690399169922, acc: 0.375)
[2025-02-04 00:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:20][root][INFO] - Training Epoch: 2/2, step 19207/23838 completed (loss: 2.954373598098755, acc: 0.36666667461395264)
[2025-02-04 00:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:21][root][INFO] - Training Epoch: 2/2, step 19208/23838 completed (loss: 1.7053483724594116, acc: 0.5555555820465088)
[2025-02-04 00:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:21][root][INFO] - Training Epoch: 2/2, step 19209/23838 completed (loss: 2.520310878753662, acc: 0.25)
[2025-02-04 00:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:21][root][INFO] - Training Epoch: 2/2, step 19210/23838 completed (loss: 2.5653598308563232, acc: 0.5384615659713745)
[2025-02-04 00:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:22][root][INFO] - Training Epoch: 2/2, step 19211/23838 completed (loss: 2.954468011856079, acc: 0.37037035822868347)
[2025-02-04 00:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:22][root][INFO] - Training Epoch: 2/2, step 19212/23838 completed (loss: 2.5369787216186523, acc: 0.40740740299224854)
[2025-02-04 00:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:23][root][INFO] - Training Epoch: 2/2, step 19213/23838 completed (loss: 3.33270001411438, acc: 0.44999998807907104)
[2025-02-04 00:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:23][root][INFO] - Training Epoch: 2/2, step 19214/23838 completed (loss: 4.107633113861084, acc: 0.4000000059604645)
[2025-02-04 00:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:24][root][INFO] - Training Epoch: 2/2, step 19215/23838 completed (loss: 3.700935125350952, acc: 0.37037035822868347)
[2025-02-04 00:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:24][root][INFO] - Training Epoch: 2/2, step 19216/23838 completed (loss: 2.3451263904571533, acc: 0.5384615659713745)
[2025-02-04 00:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:25][root][INFO] - Training Epoch: 2/2, step 19217/23838 completed (loss: 2.1849281787872314, acc: 0.4399999976158142)
[2025-02-04 00:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:25][root][INFO] - Training Epoch: 2/2, step 19218/23838 completed (loss: 2.4820544719696045, acc: 0.3684210479259491)
[2025-02-04 00:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:26][root][INFO] - Training Epoch: 2/2, step 19219/23838 completed (loss: 1.9656038284301758, acc: 0.6666666865348816)
[2025-02-04 00:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:26][root][INFO] - Training Epoch: 2/2, step 19220/23838 completed (loss: 3.108246088027954, acc: 0.5333333611488342)
[2025-02-04 00:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:26][root][INFO] - Training Epoch: 2/2, step 19221/23838 completed (loss: 3.1819140911102295, acc: 0.375)
[2025-02-04 00:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:27][root][INFO] - Training Epoch: 2/2, step 19222/23838 completed (loss: 4.337520599365234, acc: 0.2142857164144516)
[2025-02-04 00:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:27][root][INFO] - Training Epoch: 2/2, step 19223/23838 completed (loss: 4.903520107269287, acc: 0.23999999463558197)
[2025-02-04 00:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:28][root][INFO] - Training Epoch: 2/2, step 19224/23838 completed (loss: 3.5141897201538086, acc: 0.5)
[2025-02-04 00:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:28][root][INFO] - Training Epoch: 2/2, step 19225/23838 completed (loss: 4.583187580108643, acc: 0.26923078298568726)
[2025-02-04 00:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:29][root][INFO] - Training Epoch: 2/2, step 19226/23838 completed (loss: 2.9320714473724365, acc: 0.48571428656578064)
[2025-02-04 00:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:29][root][INFO] - Training Epoch: 2/2, step 19227/23838 completed (loss: 4.042605400085449, acc: 0.3214285671710968)
[2025-02-04 00:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:30][root][INFO] - Training Epoch: 2/2, step 19228/23838 completed (loss: 2.7731547355651855, acc: 0.3499999940395355)
[2025-02-04 00:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:30][root][INFO] - Training Epoch: 2/2, step 19229/23838 completed (loss: 3.077691078186035, acc: 0.26829269528388977)
[2025-02-04 00:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:30][root][INFO] - Training Epoch: 2/2, step 19230/23838 completed (loss: 2.7437620162963867, acc: 0.4166666567325592)
[2025-02-04 00:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:31][root][INFO] - Training Epoch: 2/2, step 19231/23838 completed (loss: 3.689805746078491, acc: 0.3571428656578064)
[2025-02-04 00:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:31][root][INFO] - Training Epoch: 2/2, step 19232/23838 completed (loss: 3.4708974361419678, acc: 0.40909090638160706)
[2025-02-04 00:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:32][root][INFO] - Training Epoch: 2/2, step 19233/23838 completed (loss: 4.088111877441406, acc: 0.27272728085517883)
[2025-02-04 00:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:32][root][INFO] - Training Epoch: 2/2, step 19234/23838 completed (loss: 3.3499865531921387, acc: 0.2068965584039688)
[2025-02-04 00:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:33][root][INFO] - Training Epoch: 2/2, step 19235/23838 completed (loss: 3.3652560710906982, acc: 0.3461538553237915)
[2025-02-04 00:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:33][root][INFO] - Training Epoch: 2/2, step 19236/23838 completed (loss: 2.404294967651367, acc: 0.523809552192688)
[2025-02-04 00:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:34][root][INFO] - Training Epoch: 2/2, step 19237/23838 completed (loss: 2.727339506149292, acc: 0.2857142984867096)
[2025-02-04 00:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:34][root][INFO] - Training Epoch: 2/2, step 19238/23838 completed (loss: 2.0178074836730957, acc: 0.517241358757019)
[2025-02-04 00:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:35][root][INFO] - Training Epoch: 2/2, step 19239/23838 completed (loss: 2.5044851303100586, acc: 0.5)
[2025-02-04 00:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:35][root][INFO] - Training Epoch: 2/2, step 19240/23838 completed (loss: 3.4344937801361084, acc: 0.34090909361839294)
[2025-02-04 00:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:36][root][INFO] - Training Epoch: 2/2, step 19241/23838 completed (loss: 2.6001501083374023, acc: 0.4313725531101227)
[2025-02-04 00:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:36][root][INFO] - Training Epoch: 2/2, step 19242/23838 completed (loss: 3.4338598251342773, acc: 0.4390243887901306)
[2025-02-04 00:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:36][root][INFO] - Training Epoch: 2/2, step 19243/23838 completed (loss: 2.184814691543579, acc: 0.5555555820465088)
[2025-02-04 00:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:37][root][INFO] - Training Epoch: 2/2, step 19244/23838 completed (loss: 3.1425344944000244, acc: 0.2380952388048172)
[2025-02-04 00:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:37][root][INFO] - Training Epoch: 2/2, step 19245/23838 completed (loss: 2.9721670150756836, acc: 0.34285715222358704)
[2025-02-04 00:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:38][root][INFO] - Training Epoch: 2/2, step 19246/23838 completed (loss: 3.055760622024536, acc: 0.4615384638309479)
[2025-02-04 00:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:38][root][INFO] - Training Epoch: 2/2, step 19247/23838 completed (loss: 2.8269705772399902, acc: 0.4736842215061188)
[2025-02-04 00:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:39][root][INFO] - Training Epoch: 2/2, step 19248/23838 completed (loss: 2.0814595222473145, acc: 0.625)
[2025-02-04 00:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:39][root][INFO] - Training Epoch: 2/2, step 19249/23838 completed (loss: 3.2864480018615723, acc: 0.5)
[2025-02-04 00:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:39][root][INFO] - Training Epoch: 2/2, step 19250/23838 completed (loss: 2.5061094760894775, acc: 0.5769230723381042)
[2025-02-04 00:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:40][root][INFO] - Training Epoch: 2/2, step 19251/23838 completed (loss: 2.914731979370117, acc: 0.3571428656578064)
[2025-02-04 00:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:40][root][INFO] - Training Epoch: 2/2, step 19252/23838 completed (loss: 2.581808090209961, acc: 0.5789473652839661)
[2025-02-04 00:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:41][root][INFO] - Training Epoch: 2/2, step 19253/23838 completed (loss: 2.5805420875549316, acc: 0.52173912525177)
[2025-02-04 00:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:41][root][INFO] - Training Epoch: 2/2, step 19254/23838 completed (loss: 2.512770414352417, acc: 0.31578946113586426)
[2025-02-04 00:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:42][root][INFO] - Training Epoch: 2/2, step 19255/23838 completed (loss: 2.9810314178466797, acc: 0.31578946113586426)
[2025-02-04 00:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:42][root][INFO] - Training Epoch: 2/2, step 19256/23838 completed (loss: 4.076715469360352, acc: 0.3513513505458832)
[2025-02-04 00:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:42][root][INFO] - Training Epoch: 2/2, step 19257/23838 completed (loss: 4.270503997802734, acc: 0.2448979616165161)
[2025-02-04 00:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:43][root][INFO] - Training Epoch: 2/2, step 19258/23838 completed (loss: 2.9926600456237793, acc: 0.3499999940395355)
[2025-02-04 00:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:43][root][INFO] - Training Epoch: 2/2, step 19259/23838 completed (loss: 1.9655427932739258, acc: 0.42105263471603394)
[2025-02-04 00:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:44][root][INFO] - Training Epoch: 2/2, step 19260/23838 completed (loss: 3.6547513008117676, acc: 0.3448275923728943)
[2025-02-04 00:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:44][root][INFO] - Training Epoch: 2/2, step 19261/23838 completed (loss: 3.795114755630493, acc: 0.42307692766189575)
[2025-02-04 00:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:45][root][INFO] - Training Epoch: 2/2, step 19262/23838 completed (loss: 4.090023994445801, acc: 0.29411765933036804)
[2025-02-04 00:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:45][root][INFO] - Training Epoch: 2/2, step 19263/23838 completed (loss: 2.734283447265625, acc: 0.3888888955116272)
[2025-02-04 00:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:46][root][INFO] - Training Epoch: 2/2, step 19264/23838 completed (loss: 2.635960578918457, acc: 0.4761904776096344)
[2025-02-04 00:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:46][root][INFO] - Training Epoch: 2/2, step 19265/23838 completed (loss: 4.236600399017334, acc: 0.2698412835597992)
[2025-02-04 00:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:47][root][INFO] - Training Epoch: 2/2, step 19266/23838 completed (loss: 2.4245681762695312, acc: 0.5151515007019043)
[2025-02-04 00:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:47][root][INFO] - Training Epoch: 2/2, step 19267/23838 completed (loss: 2.7730283737182617, acc: 0.375)
[2025-02-04 00:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:48][root][INFO] - Training Epoch: 2/2, step 19268/23838 completed (loss: 2.6968164443969727, acc: 0.23333333432674408)
[2025-02-04 00:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:48][root][INFO] - Training Epoch: 2/2, step 19269/23838 completed (loss: 3.682011127471924, acc: 0.3478260934352875)
[2025-02-04 00:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:49][root][INFO] - Training Epoch: 2/2, step 19270/23838 completed (loss: 2.497117757797241, acc: 0.4375)
[2025-02-04 00:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:49][root][INFO] - Training Epoch: 2/2, step 19271/23838 completed (loss: 3.608469009399414, acc: 0.3684210479259491)
[2025-02-04 00:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:50][root][INFO] - Training Epoch: 2/2, step 19272/23838 completed (loss: 1.758561372756958, acc: 0.6000000238418579)
[2025-02-04 00:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:50][root][INFO] - Training Epoch: 2/2, step 19273/23838 completed (loss: 3.706484317779541, acc: 0.4000000059604645)
[2025-02-04 00:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:50][root][INFO] - Training Epoch: 2/2, step 19274/23838 completed (loss: 3.131404161453247, acc: 0.4642857015132904)
[2025-02-04 00:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:51][root][INFO] - Training Epoch: 2/2, step 19275/23838 completed (loss: 2.411931276321411, acc: 0.3888888955116272)
[2025-02-04 00:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:51][root][INFO] - Training Epoch: 2/2, step 19276/23838 completed (loss: 3.017503023147583, acc: 0.3333333432674408)
[2025-02-04 00:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:52][root][INFO] - Training Epoch: 2/2, step 19277/23838 completed (loss: 3.0150697231292725, acc: 0.35483869910240173)
[2025-02-04 00:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:52][root][INFO] - Training Epoch: 2/2, step 19278/23838 completed (loss: 3.111002206802368, acc: 0.34375)
[2025-02-04 00:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:53][root][INFO] - Training Epoch: 2/2, step 19279/23838 completed (loss: 3.044142484664917, acc: 0.38181817531585693)
[2025-02-04 00:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:53][root][INFO] - Training Epoch: 2/2, step 19280/23838 completed (loss: 2.3917994499206543, acc: 0.5)
[2025-02-04 00:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:54][root][INFO] - Training Epoch: 2/2, step 19281/23838 completed (loss: 2.890411138534546, acc: 0.5)
[2025-02-04 00:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:54][root][INFO] - Training Epoch: 2/2, step 19282/23838 completed (loss: 2.307466983795166, acc: 0.5)
[2025-02-04 00:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:54][root][INFO] - Training Epoch: 2/2, step 19283/23838 completed (loss: 3.4158289432525635, acc: 0.3720930218696594)
[2025-02-04 00:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:55][root][INFO] - Training Epoch: 2/2, step 19284/23838 completed (loss: 2.1706736087799072, acc: 0.5862069129943848)
[2025-02-04 00:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:55][root][INFO] - Training Epoch: 2/2, step 19285/23838 completed (loss: 2.4678006172180176, acc: 0.43478259444236755)
[2025-02-04 00:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:56][root][INFO] - Training Epoch: 2/2, step 19286/23838 completed (loss: 1.6876487731933594, acc: 0.6153846383094788)
[2025-02-04 00:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:56][root][INFO] - Training Epoch: 2/2, step 19287/23838 completed (loss: 2.689363479614258, acc: 0.529411792755127)
[2025-02-04 00:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:56][root][INFO] - Training Epoch: 2/2, step 19288/23838 completed (loss: 2.910045623779297, acc: 0.42424243688583374)
[2025-02-04 00:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:57][root][INFO] - Training Epoch: 2/2, step 19289/23838 completed (loss: 3.395179510116577, acc: 0.3076923191547394)
[2025-02-04 00:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:57][root][INFO] - Training Epoch: 2/2, step 19290/23838 completed (loss: 2.429939031600952, acc: 0.5151515007019043)
[2025-02-04 00:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:58][root][INFO] - Training Epoch: 2/2, step 19291/23838 completed (loss: 2.72971248626709, acc: 0.4482758641242981)
[2025-02-04 00:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:58][root][INFO] - Training Epoch: 2/2, step 19292/23838 completed (loss: 3.9718728065490723, acc: 0.3863636255264282)
[2025-02-04 00:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:59][root][INFO] - Training Epoch: 2/2, step 19293/23838 completed (loss: 2.0481314659118652, acc: 0.5588235259056091)
[2025-02-04 00:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:59][root][INFO] - Training Epoch: 2/2, step 19294/23838 completed (loss: 3.484934091567993, acc: 0.3400000035762787)
[2025-02-04 00:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:43:59][root][INFO] - Training Epoch: 2/2, step 19295/23838 completed (loss: 3.037853956222534, acc: 0.4000000059604645)
[2025-02-04 00:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:00][root][INFO] - Training Epoch: 2/2, step 19296/23838 completed (loss: 3.297520160675049, acc: 0.4642857015132904)
[2025-02-04 00:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:00][root][INFO] - Training Epoch: 2/2, step 19297/23838 completed (loss: 2.9096286296844482, acc: 0.4545454680919647)
[2025-02-04 00:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:01][root][INFO] - Training Epoch: 2/2, step 19298/23838 completed (loss: 3.111578941345215, acc: 0.39393940567970276)
[2025-02-04 00:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:01][root][INFO] - Training Epoch: 2/2, step 19299/23838 completed (loss: 2.7868735790252686, acc: 0.4838709533214569)
[2025-02-04 00:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:02][root][INFO] - Training Epoch: 2/2, step 19300/23838 completed (loss: 3.1202335357666016, acc: 0.46341463923454285)
[2025-02-04 00:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:02][root][INFO] - Training Epoch: 2/2, step 19301/23838 completed (loss: 3.250333547592163, acc: 0.4285714328289032)
[2025-02-04 00:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:02][root][INFO] - Training Epoch: 2/2, step 19302/23838 completed (loss: 3.174393892288208, acc: 0.5625)
[2025-02-04 00:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:03][root][INFO] - Training Epoch: 2/2, step 19303/23838 completed (loss: 2.5804786682128906, acc: 0.46875)
[2025-02-04 00:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:03][root][INFO] - Training Epoch: 2/2, step 19304/23838 completed (loss: 2.5013082027435303, acc: 0.692307710647583)
[2025-02-04 00:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:04][root][INFO] - Training Epoch: 2/2, step 19305/23838 completed (loss: 0.198184072971344, acc: 0.8888888955116272)
[2025-02-04 00:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:04][root][INFO] - Training Epoch: 2/2, step 19306/23838 completed (loss: 2.732790231704712, acc: 0.3888888955116272)
[2025-02-04 00:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:05][root][INFO] - Training Epoch: 2/2, step 19307/23838 completed (loss: 2.327427625656128, acc: 0.46875)
[2025-02-04 00:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:05][root][INFO] - Training Epoch: 2/2, step 19308/23838 completed (loss: 1.6134003400802612, acc: 0.625)
[2025-02-04 00:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:05][root][INFO] - Training Epoch: 2/2, step 19309/23838 completed (loss: 1.8708992004394531, acc: 0.5625)
[2025-02-04 00:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:06][root][INFO] - Training Epoch: 2/2, step 19310/23838 completed (loss: 2.117992401123047, acc: 0.5833333134651184)
[2025-02-04 00:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:06][root][INFO] - Training Epoch: 2/2, step 19311/23838 completed (loss: 0.7659791111946106, acc: 0.6666666865348816)
[2025-02-04 00:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:07][root][INFO] - Training Epoch: 2/2, step 19312/23838 completed (loss: 2.430896282196045, acc: 0.5357142686843872)
[2025-02-04 00:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:07][root][INFO] - Training Epoch: 2/2, step 19313/23838 completed (loss: 3.442884922027588, acc: 0.3529411852359772)
[2025-02-04 00:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:08][root][INFO] - Training Epoch: 2/2, step 19314/23838 completed (loss: 3.6259682178497314, acc: 0.4324324429035187)
[2025-02-04 00:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:08][root][INFO] - Training Epoch: 2/2, step 19315/23838 completed (loss: 3.5716800689697266, acc: 0.3962264060974121)
[2025-02-04 00:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:08][root][INFO] - Training Epoch: 2/2, step 19316/23838 completed (loss: 2.2722952365875244, acc: 0.5454545617103577)
[2025-02-04 00:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:09][root][INFO] - Training Epoch: 2/2, step 19317/23838 completed (loss: 2.7921903133392334, acc: 0.5)
[2025-02-04 00:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:09][root][INFO] - Training Epoch: 2/2, step 19318/23838 completed (loss: 1.603624939918518, acc: 0.65625)
[2025-02-04 00:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:10][root][INFO] - Training Epoch: 2/2, step 19319/23838 completed (loss: 3.1346027851104736, acc: 0.47058823704719543)
[2025-02-04 00:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:10][root][INFO] - Training Epoch: 2/2, step 19320/23838 completed (loss: 2.689680337905884, acc: 0.4375)
[2025-02-04 00:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:11][root][INFO] - Training Epoch: 2/2, step 19321/23838 completed (loss: 1.916114330291748, acc: 0.6086956262588501)
[2025-02-04 00:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:11][root][INFO] - Training Epoch: 2/2, step 19322/23838 completed (loss: 1.3109803199768066, acc: 0.75)
[2025-02-04 00:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:11][root][INFO] - Training Epoch: 2/2, step 19323/23838 completed (loss: 1.6322214603424072, acc: 0.6842105388641357)
[2025-02-04 00:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:12][root][INFO] - Training Epoch: 2/2, step 19324/23838 completed (loss: 1.8722553253173828, acc: 0.5769230723381042)
[2025-02-04 00:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:12][root][INFO] - Training Epoch: 2/2, step 19325/23838 completed (loss: 2.2001559734344482, acc: 0.6000000238418579)
[2025-02-04 00:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:13][root][INFO] - Training Epoch: 2/2, step 19326/23838 completed (loss: 2.2654619216918945, acc: 0.5454545617103577)
[2025-02-04 00:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:13][root][INFO] - Training Epoch: 2/2, step 19327/23838 completed (loss: 4.288806915283203, acc: 0.375)
[2025-02-04 00:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:14][root][INFO] - Training Epoch: 2/2, step 19328/23838 completed (loss: 2.866455078125, acc: 0.5833333134651184)
[2025-02-04 00:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:14][root][INFO] - Training Epoch: 2/2, step 19329/23838 completed (loss: 4.68403959274292, acc: 0.4285714328289032)
[2025-02-04 00:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:15][root][INFO] - Training Epoch: 2/2, step 19330/23838 completed (loss: 5.155563831329346, acc: 0.25)
[2025-02-04 00:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:15][root][INFO] - Training Epoch: 2/2, step 19331/23838 completed (loss: 3.6496410369873047, acc: 0.5714285969734192)
[2025-02-04 00:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:15][root][INFO] - Training Epoch: 2/2, step 19332/23838 completed (loss: 3.7403926849365234, acc: 0.5714285969734192)
[2025-02-04 00:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:16][root][INFO] - Training Epoch: 2/2, step 19333/23838 completed (loss: 2.6389291286468506, acc: 0.5333333611488342)
[2025-02-04 00:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:16][root][INFO] - Training Epoch: 2/2, step 19334/23838 completed (loss: 3.716336727142334, acc: 0.38461539149284363)
[2025-02-04 00:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:16][root][INFO] - Training Epoch: 2/2, step 19335/23838 completed (loss: 2.9306135177612305, acc: 0.5384615659713745)
[2025-02-04 00:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:17][root][INFO] - Training Epoch: 2/2, step 19336/23838 completed (loss: 4.982629299163818, acc: 0.30434781312942505)
[2025-02-04 00:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:17][root][INFO] - Training Epoch: 2/2, step 19337/23838 completed (loss: 3.1858596801757812, acc: 0.2666666805744171)
[2025-02-04 00:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:18][root][INFO] - Training Epoch: 2/2, step 19338/23838 completed (loss: 2.438880443572998, acc: 0.5)
[2025-02-04 00:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:18][root][INFO] - Training Epoch: 2/2, step 19339/23838 completed (loss: 4.009261608123779, acc: 0.3125)
[2025-02-04 00:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:19][root][INFO] - Training Epoch: 2/2, step 19340/23838 completed (loss: 4.2870707511901855, acc: 0.30000001192092896)
[2025-02-04 00:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:19][root][INFO] - Training Epoch: 2/2, step 19341/23838 completed (loss: 2.7055859565734863, acc: 0.5)
[2025-02-04 00:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:19][root][INFO] - Training Epoch: 2/2, step 19342/23838 completed (loss: 4.079205513000488, acc: 0.375)
[2025-02-04 00:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:20][root][INFO] - Training Epoch: 2/2, step 19343/23838 completed (loss: 2.5234577655792236, acc: 0.5555555820465088)
[2025-02-04 00:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:20][root][INFO] - Training Epoch: 2/2, step 19344/23838 completed (loss: 3.109562873840332, acc: 0.4117647111415863)
[2025-02-04 00:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:21][root][INFO] - Training Epoch: 2/2, step 19345/23838 completed (loss: 3.930211067199707, acc: 0.4000000059604645)
[2025-02-04 00:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:21][root][INFO] - Training Epoch: 2/2, step 19346/23838 completed (loss: 4.892027378082275, acc: 0.2857142984867096)
[2025-02-04 00:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:21][root][INFO] - Training Epoch: 2/2, step 19347/23838 completed (loss: 4.839503765106201, acc: 0.36666667461395264)
[2025-02-04 00:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:22][root][INFO] - Training Epoch: 2/2, step 19348/23838 completed (loss: 2.1054937839508057, acc: 0.5555555820465088)
[2025-02-04 00:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:22][root][INFO] - Training Epoch: 2/2, step 19349/23838 completed (loss: 2.8058056831359863, acc: 0.529411792755127)
[2025-02-04 00:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:23][root][INFO] - Training Epoch: 2/2, step 19350/23838 completed (loss: 2.6086456775665283, acc: 0.6111111044883728)
[2025-02-04 00:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:23][root][INFO] - Training Epoch: 2/2, step 19351/23838 completed (loss: 4.721163272857666, acc: 0.4375)
[2025-02-04 00:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:24][root][INFO] - Training Epoch: 2/2, step 19352/23838 completed (loss: 3.705864429473877, acc: 0.3888888955116272)
[2025-02-04 00:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:24][root][INFO] - Training Epoch: 2/2, step 19353/23838 completed (loss: 2.9341676235198975, acc: 0.5)
[2025-02-04 00:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:25][root][INFO] - Training Epoch: 2/2, step 19354/23838 completed (loss: 2.0818567276000977, acc: 0.6875)
[2025-02-04 00:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:25][root][INFO] - Training Epoch: 2/2, step 19355/23838 completed (loss: 1.8962016105651855, acc: 0.625)
[2025-02-04 00:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:26][root][INFO] - Training Epoch: 2/2, step 19356/23838 completed (loss: 2.6669716835021973, acc: 0.4444444477558136)
[2025-02-04 00:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:26][root][INFO] - Training Epoch: 2/2, step 19357/23838 completed (loss: 1.1339237689971924, acc: 0.692307710647583)
[2025-02-04 00:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:27][root][INFO] - Training Epoch: 2/2, step 19358/23838 completed (loss: 1.0382945537567139, acc: 0.7777777910232544)
[2025-02-04 00:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:27][root][INFO] - Training Epoch: 2/2, step 19359/23838 completed (loss: 3.9178149700164795, acc: 0.5)
[2025-02-04 00:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:28][root][INFO] - Training Epoch: 2/2, step 19360/23838 completed (loss: 2.328392505645752, acc: 0.5625)
[2025-02-04 00:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:28][root][INFO] - Training Epoch: 2/2, step 19361/23838 completed (loss: 1.9565321207046509, acc: 0.5789473652839661)
[2025-02-04 00:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:28][root][INFO] - Training Epoch: 2/2, step 19362/23838 completed (loss: 3.7599222660064697, acc: 0.43478259444236755)
[2025-02-04 00:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:29][root][INFO] - Training Epoch: 2/2, step 19363/23838 completed (loss: 4.173013687133789, acc: 0.3636363744735718)
[2025-02-04 00:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:29][root][INFO] - Training Epoch: 2/2, step 19364/23838 completed (loss: 3.216447353363037, acc: 0.4545454680919647)
[2025-02-04 00:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:30][root][INFO] - Training Epoch: 2/2, step 19365/23838 completed (loss: 2.1983439922332764, acc: 0.4736842215061188)
[2025-02-04 00:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:30][root][INFO] - Training Epoch: 2/2, step 19366/23838 completed (loss: 1.6558854579925537, acc: 0.6000000238418579)
[2025-02-04 00:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:30][root][INFO] - Training Epoch: 2/2, step 19367/23838 completed (loss: 2.4401321411132812, acc: 0.6153846383094788)
[2025-02-04 00:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:31][root][INFO] - Training Epoch: 2/2, step 19368/23838 completed (loss: 3.7862765789031982, acc: 0.5)
[2025-02-04 00:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:31][root][INFO] - Training Epoch: 2/2, step 19369/23838 completed (loss: 1.4130538702011108, acc: 0.800000011920929)
[2025-02-04 00:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:32][root][INFO] - Training Epoch: 2/2, step 19370/23838 completed (loss: 2.3395304679870605, acc: 0.4545454680919647)
[2025-02-04 00:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:32][root][INFO] - Training Epoch: 2/2, step 19371/23838 completed (loss: 1.5269176959991455, acc: 0.692307710647583)
[2025-02-04 00:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:32][root][INFO] - Training Epoch: 2/2, step 19372/23838 completed (loss: 2.226513385772705, acc: 0.699999988079071)
[2025-02-04 00:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:33][root][INFO] - Training Epoch: 2/2, step 19373/23838 completed (loss: 2.8199687004089355, acc: 0.47058823704719543)
[2025-02-04 00:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:33][root][INFO] - Training Epoch: 2/2, step 19374/23838 completed (loss: 3.5270004272460938, acc: 0.5384615659713745)
[2025-02-04 00:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:34][root][INFO] - Training Epoch: 2/2, step 19375/23838 completed (loss: 2.153737783432007, acc: 0.7272727489471436)
[2025-02-04 00:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:34][root][INFO] - Training Epoch: 2/2, step 19376/23838 completed (loss: 1.332579493522644, acc: 0.699999988079071)
[2025-02-04 00:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:34][root][INFO] - Training Epoch: 2/2, step 19377/23838 completed (loss: 3.769239902496338, acc: 0.4545454680919647)
[2025-02-04 00:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:35][root][INFO] - Training Epoch: 2/2, step 19378/23838 completed (loss: 4.003565788269043, acc: 0.46666666865348816)
[2025-02-04 00:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:35][root][INFO] - Training Epoch: 2/2, step 19379/23838 completed (loss: 1.6911836862564087, acc: 0.7777777910232544)
[2025-02-04 00:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:35][root][INFO] - Training Epoch: 2/2, step 19380/23838 completed (loss: 1.2465870380401611, acc: 0.7333333492279053)
[2025-02-04 00:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:36][root][INFO] - Training Epoch: 2/2, step 19381/23838 completed (loss: 2.128253221511841, acc: 0.7857142686843872)
[2025-02-04 00:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:36][root][INFO] - Training Epoch: 2/2, step 19382/23838 completed (loss: 4.945037841796875, acc: 0.30000001192092896)
[2025-02-04 00:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:37][root][INFO] - Training Epoch: 2/2, step 19383/23838 completed (loss: 3.573549270629883, acc: 0.47058823704719543)
[2025-02-04 00:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:37][root][INFO] - Training Epoch: 2/2, step 19384/23838 completed (loss: 4.583907127380371, acc: 0.44999998807907104)
[2025-02-04 00:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:38][root][INFO] - Training Epoch: 2/2, step 19385/23838 completed (loss: 1.2724602222442627, acc: 0.6111111044883728)
[2025-02-04 00:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:38][root][INFO] - Training Epoch: 2/2, step 19386/23838 completed (loss: 1.262635350227356, acc: 0.6153846383094788)
[2025-02-04 00:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:39][root][INFO] - Training Epoch: 2/2, step 19387/23838 completed (loss: 1.0445297956466675, acc: 0.8125)
[2025-02-04 00:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:39][root][INFO] - Training Epoch: 2/2, step 19388/23838 completed (loss: 2.231060028076172, acc: 0.5833333134651184)
[2025-02-04 00:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:39][root][INFO] - Training Epoch: 2/2, step 19389/23838 completed (loss: 0.8148321509361267, acc: 0.7333333492279053)
[2025-02-04 00:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:40][root][INFO] - Training Epoch: 2/2, step 19390/23838 completed (loss: 2.1743998527526855, acc: 0.6000000238418579)
[2025-02-04 00:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:40][root][INFO] - Training Epoch: 2/2, step 19391/23838 completed (loss: 1.3850041627883911, acc: 0.8181818127632141)
[2025-02-04 00:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:41][root][INFO] - Training Epoch: 2/2, step 19392/23838 completed (loss: 2.441427707672119, acc: 0.6000000238418579)
[2025-02-04 00:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:41][root][INFO] - Training Epoch: 2/2, step 19393/23838 completed (loss: 3.3936851024627686, acc: 0.4000000059604645)
[2025-02-04 00:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:41][root][INFO] - Training Epoch: 2/2, step 19394/23838 completed (loss: 2.885402202606201, acc: 0.5)
[2025-02-04 00:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:42][root][INFO] - Training Epoch: 2/2, step 19395/23838 completed (loss: 3.697695016860962, acc: 0.375)
[2025-02-04 00:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:42][root][INFO] - Training Epoch: 2/2, step 19396/23838 completed (loss: 3.2669379711151123, acc: 0.4736842215061188)
[2025-02-04 00:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:43][root][INFO] - Training Epoch: 2/2, step 19397/23838 completed (loss: 2.64259934425354, acc: 0.6666666865348816)
[2025-02-04 00:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:43][root][INFO] - Training Epoch: 2/2, step 19398/23838 completed (loss: 4.612205982208252, acc: 0.4444444477558136)
[2025-02-04 00:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:43][root][INFO] - Training Epoch: 2/2, step 19399/23838 completed (loss: 3.0664424896240234, acc: 0.5882353186607361)
[2025-02-04 00:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:44][root][INFO] - Training Epoch: 2/2, step 19400/23838 completed (loss: 0.3619096577167511, acc: 0.8571428656578064)
[2025-02-04 00:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:44][root][INFO] - Training Epoch: 2/2, step 19401/23838 completed (loss: 2.1357340812683105, acc: 0.5384615659713745)
[2025-02-04 00:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:45][root][INFO] - Training Epoch: 2/2, step 19402/23838 completed (loss: 3.9443445205688477, acc: 0.3333333432674408)
[2025-02-04 00:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:45][root][INFO] - Training Epoch: 2/2, step 19403/23838 completed (loss: 2.4983391761779785, acc: 0.47999998927116394)
[2025-02-04 00:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:46][root][INFO] - Training Epoch: 2/2, step 19404/23838 completed (loss: 1.7050672769546509, acc: 0.699999988079071)
[2025-02-04 00:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:46][root][INFO] - Training Epoch: 2/2, step 19405/23838 completed (loss: 3.172478437423706, acc: 0.5)
[2025-02-04 00:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:47][root][INFO] - Training Epoch: 2/2, step 19406/23838 completed (loss: 2.7360661029815674, acc: 0.5384615659713745)
[2025-02-04 00:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:47][root][INFO] - Training Epoch: 2/2, step 19407/23838 completed (loss: 1.778046727180481, acc: 0.6666666865348816)
[2025-02-04 00:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:47][root][INFO] - Training Epoch: 2/2, step 19408/23838 completed (loss: 1.6744358539581299, acc: 0.6153846383094788)
[2025-02-04 00:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:48][root][INFO] - Training Epoch: 2/2, step 19409/23838 completed (loss: 1.9590531587600708, acc: 0.7692307829856873)
[2025-02-04 00:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:48][root][INFO] - Training Epoch: 2/2, step 19410/23838 completed (loss: 3.9383859634399414, acc: 0.3333333432674408)
[2025-02-04 00:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:49][root][INFO] - Training Epoch: 2/2, step 19411/23838 completed (loss: 4.247148513793945, acc: 0.3333333432674408)
[2025-02-04 00:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:49][root][INFO] - Training Epoch: 2/2, step 19412/23838 completed (loss: 2.2391650676727295, acc: 0.5555555820465088)
[2025-02-04 00:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:50][root][INFO] - Training Epoch: 2/2, step 19413/23838 completed (loss: 1.1836215257644653, acc: 0.75)
[2025-02-04 00:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:50][root][INFO] - Training Epoch: 2/2, step 19414/23838 completed (loss: 1.1572527885437012, acc: 0.8181818127632141)
[2025-02-04 00:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:51][root][INFO] - Training Epoch: 2/2, step 19415/23838 completed (loss: 2.0949065685272217, acc: 0.5625)
[2025-02-04 00:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:51][root][INFO] - Training Epoch: 2/2, step 19416/23838 completed (loss: 1.6803756952285767, acc: 0.6666666865348816)
[2025-02-04 00:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:51][root][INFO] - Training Epoch: 2/2, step 19417/23838 completed (loss: 1.9867959022521973, acc: 0.6315789222717285)
[2025-02-04 00:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:52][root][INFO] - Training Epoch: 2/2, step 19418/23838 completed (loss: 2.31781268119812, acc: 0.5714285969734192)
[2025-02-04 00:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:52][root][INFO] - Training Epoch: 2/2, step 19419/23838 completed (loss: 2.14058518409729, acc: 0.5833333134651184)
[2025-02-04 00:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:53][root][INFO] - Training Epoch: 2/2, step 19420/23838 completed (loss: 4.881972312927246, acc: 0.25)
[2025-02-04 00:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:53][root][INFO] - Training Epoch: 2/2, step 19421/23838 completed (loss: 3.9073102474212646, acc: 0.29629629850387573)
[2025-02-04 00:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:54][root][INFO] - Training Epoch: 2/2, step 19422/23838 completed (loss: 3.5566673278808594, acc: 0.3589743673801422)
[2025-02-04 00:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:54][root][INFO] - Training Epoch: 2/2, step 19423/23838 completed (loss: 3.159029245376587, acc: 0.4000000059604645)
[2025-02-04 00:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:55][root][INFO] - Training Epoch: 2/2, step 19424/23838 completed (loss: 3.889652729034424, acc: 0.36666667461395264)
[2025-02-04 00:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:55][root][INFO] - Training Epoch: 2/2, step 19425/23838 completed (loss: 3.267110824584961, acc: 0.37142857909202576)
[2025-02-04 00:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:55][root][INFO] - Training Epoch: 2/2, step 19426/23838 completed (loss: 3.345144748687744, acc: 0.4000000059604645)
[2025-02-04 00:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:56][root][INFO] - Training Epoch: 2/2, step 19427/23838 completed (loss: 4.686859130859375, acc: 0.17391304671764374)
[2025-02-04 00:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:56][root][INFO] - Training Epoch: 2/2, step 19428/23838 completed (loss: 4.540579795837402, acc: 0.2857142984867096)
[2025-02-04 00:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:57][root][INFO] - Training Epoch: 2/2, step 19429/23838 completed (loss: 4.1913557052612305, acc: 0.27272728085517883)
[2025-02-04 00:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:57][root][INFO] - Training Epoch: 2/2, step 19430/23838 completed (loss: 4.141454696655273, acc: 0.34545454382896423)
[2025-02-04 00:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:58][root][INFO] - Training Epoch: 2/2, step 19431/23838 completed (loss: 3.620492935180664, acc: 0.31578946113586426)
[2025-02-04 00:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:58][root][INFO] - Training Epoch: 2/2, step 19432/23838 completed (loss: 3.773540496826172, acc: 0.26923078298568726)
[2025-02-04 00:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:58][root][INFO] - Training Epoch: 2/2, step 19433/23838 completed (loss: 6.109017848968506, acc: 0.3181818127632141)
[2025-02-04 00:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:59][root][INFO] - Training Epoch: 2/2, step 19434/23838 completed (loss: 4.50318717956543, acc: 0.23076923191547394)
[2025-02-04 00:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:44:59][root][INFO] - Training Epoch: 2/2, step 19435/23838 completed (loss: 3.1396894454956055, acc: 0.3461538553237915)
[2025-02-04 00:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:00][root][INFO] - Training Epoch: 2/2, step 19436/23838 completed (loss: 3.142606019973755, acc: 0.52173912525177)
[2025-02-04 00:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:00][root][INFO] - Training Epoch: 2/2, step 19437/23838 completed (loss: 4.620900630950928, acc: 0.40909090638160706)
[2025-02-04 00:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:01][root][INFO] - Training Epoch: 2/2, step 19438/23838 completed (loss: 2.503173589706421, acc: 0.5483871102333069)
[2025-02-04 00:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:01][root][INFO] - Training Epoch: 2/2, step 19439/23838 completed (loss: 3.030780553817749, acc: 0.4166666567325592)
[2025-02-04 00:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:01][root][INFO] - Training Epoch: 2/2, step 19440/23838 completed (loss: 3.5388023853302, acc: 0.25)
[2025-02-04 00:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:02][root][INFO] - Training Epoch: 2/2, step 19441/23838 completed (loss: 3.843414306640625, acc: 0.26923078298568726)
[2025-02-04 00:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:02][root][INFO] - Training Epoch: 2/2, step 19442/23838 completed (loss: 3.8850347995758057, acc: 0.30909091234207153)
[2025-02-04 00:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:03][root][INFO] - Training Epoch: 2/2, step 19443/23838 completed (loss: 3.6691060066223145, acc: 0.302325576543808)
[2025-02-04 00:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:03][root][INFO] - Training Epoch: 2/2, step 19444/23838 completed (loss: 2.8799149990081787, acc: 0.5454545617103577)
[2025-02-04 00:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:04][root][INFO] - Training Epoch: 2/2, step 19445/23838 completed (loss: 4.7845139503479, acc: 0.3684210479259491)
[2025-02-04 00:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:04][root][INFO] - Training Epoch: 2/2, step 19446/23838 completed (loss: 2.886523723602295, acc: 0.4285714328289032)
[2025-02-04 00:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:05][root][INFO] - Training Epoch: 2/2, step 19447/23838 completed (loss: 3.3846588134765625, acc: 0.3571428656578064)
[2025-02-04 00:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:05][root][INFO] - Training Epoch: 2/2, step 19448/23838 completed (loss: 4.113481044769287, acc: 0.3199999928474426)
[2025-02-04 00:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:06][root][INFO] - Training Epoch: 2/2, step 19449/23838 completed (loss: 3.326615810394287, acc: 0.4000000059604645)
[2025-02-04 00:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:06][root][INFO] - Training Epoch: 2/2, step 19450/23838 completed (loss: 3.9304895401000977, acc: 0.32692307233810425)
[2025-02-04 00:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:07][root][INFO] - Training Epoch: 2/2, step 19451/23838 completed (loss: 3.661728620529175, acc: 0.34090909361839294)
[2025-02-04 00:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:07][root][INFO] - Training Epoch: 2/2, step 19452/23838 completed (loss: 4.473822593688965, acc: 0.3333333432674408)
[2025-02-04 00:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:08][root][INFO] - Training Epoch: 2/2, step 19453/23838 completed (loss: 4.690450191497803, acc: 0.2702702581882477)
[2025-02-04 00:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:08][root][INFO] - Training Epoch: 2/2, step 19454/23838 completed (loss: 3.5906107425689697, acc: 0.29032257199287415)
[2025-02-04 00:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:08][root][INFO] - Training Epoch: 2/2, step 19455/23838 completed (loss: 3.0984387397766113, acc: 0.4285714328289032)
[2025-02-04 00:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:09][root][INFO] - Training Epoch: 2/2, step 19456/23838 completed (loss: 3.232008457183838, acc: 0.3513513505458832)
[2025-02-04 00:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:09][root][INFO] - Training Epoch: 2/2, step 19457/23838 completed (loss: 3.2431390285491943, acc: 0.40740740299224854)
[2025-02-04 00:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:10][root][INFO] - Training Epoch: 2/2, step 19458/23838 completed (loss: 3.5771899223327637, acc: 0.2857142984867096)
[2025-02-04 00:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:10][root][INFO] - Training Epoch: 2/2, step 19459/23838 completed (loss: 5.467714309692383, acc: 0.2777777910232544)
[2025-02-04 00:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:11][root][INFO] - Training Epoch: 2/2, step 19460/23838 completed (loss: 2.8112916946411133, acc: 0.523809552192688)
[2025-02-04 00:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:11][root][INFO] - Training Epoch: 2/2, step 19461/23838 completed (loss: 3.9747698307037354, acc: 0.2888889014720917)
[2025-02-04 00:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:11][root][INFO] - Training Epoch: 2/2, step 19462/23838 completed (loss: 3.9402778148651123, acc: 0.5833333134651184)
[2025-02-04 00:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:12][root][INFO] - Training Epoch: 2/2, step 19463/23838 completed (loss: 3.015469789505005, acc: 0.4615384638309479)
[2025-02-04 00:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:12][root][INFO] - Training Epoch: 2/2, step 19464/23838 completed (loss: 3.5831520557403564, acc: 0.34285715222358704)
[2025-02-04 00:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:13][root][INFO] - Training Epoch: 2/2, step 19465/23838 completed (loss: 4.620971202850342, acc: 0.23404255509376526)
[2025-02-04 00:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:13][root][INFO] - Training Epoch: 2/2, step 19466/23838 completed (loss: 3.138078212738037, acc: 0.40625)
[2025-02-04 00:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:14][root][INFO] - Training Epoch: 2/2, step 19467/23838 completed (loss: 3.725992202758789, acc: 0.4444444477558136)
[2025-02-04 00:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:14][root][INFO] - Training Epoch: 2/2, step 19468/23838 completed (loss: 3.7684152126312256, acc: 0.3448275923728943)
[2025-02-04 00:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:14][root][INFO] - Training Epoch: 2/2, step 19469/23838 completed (loss: 2.1289398670196533, acc: 0.5357142686843872)
[2025-02-04 00:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:15][root][INFO] - Training Epoch: 2/2, step 19470/23838 completed (loss: 4.266519546508789, acc: 0.2750000059604645)
[2025-02-04 00:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:15][root][INFO] - Training Epoch: 2/2, step 19471/23838 completed (loss: 3.771108627319336, acc: 0.3142857253551483)
[2025-02-04 00:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:16][root][INFO] - Training Epoch: 2/2, step 19472/23838 completed (loss: 3.57316255569458, acc: 0.2432432472705841)
[2025-02-04 00:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:16][root][INFO] - Training Epoch: 2/2, step 19473/23838 completed (loss: 1.2486425638198853, acc: 0.75)
[2025-02-04 00:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:16][root][INFO] - Training Epoch: 2/2, step 19474/23838 completed (loss: 3.6977851390838623, acc: 0.2800000011920929)
[2025-02-04 00:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:17][root][INFO] - Training Epoch: 2/2, step 19475/23838 completed (loss: 2.19492769241333, acc: 0.4375)
[2025-02-04 00:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:17][root][INFO] - Training Epoch: 2/2, step 19476/23838 completed (loss: 3.5583267211914062, acc: 0.3333333432674408)
[2025-02-04 00:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:18][root][INFO] - Training Epoch: 2/2, step 19477/23838 completed (loss: 2.7616708278656006, acc: 0.5)
[2025-02-04 00:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:18][root][INFO] - Training Epoch: 2/2, step 19478/23838 completed (loss: 3.7830910682678223, acc: 0.3333333432674408)
[2025-02-04 00:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:19][root][INFO] - Training Epoch: 2/2, step 19479/23838 completed (loss: 3.0977909564971924, acc: 0.4761904776096344)
[2025-02-04 00:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:19][root][INFO] - Training Epoch: 2/2, step 19480/23838 completed (loss: 2.2284810543060303, acc: 0.5263158082962036)
[2025-02-04 00:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:19][root][INFO] - Training Epoch: 2/2, step 19481/23838 completed (loss: 4.024196147918701, acc: 0.28125)
[2025-02-04 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:20][root][INFO] - Training Epoch: 2/2, step 19482/23838 completed (loss: 3.66789174079895, acc: 0.3478260934352875)
[2025-02-04 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:20][root][INFO] - Training Epoch: 2/2, step 19483/23838 completed (loss: 4.549501895904541, acc: 0.375)
[2025-02-04 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:21][root][INFO] - Training Epoch: 2/2, step 19484/23838 completed (loss: 4.400146007537842, acc: 0.3142857253551483)
[2025-02-04 00:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:21][root][INFO] - Training Epoch: 2/2, step 19485/23838 completed (loss: 3.8761041164398193, acc: 0.25)
[2025-02-04 00:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:22][root][INFO] - Training Epoch: 2/2, step 19486/23838 completed (loss: 4.582938194274902, acc: 0.3333333432674408)
[2025-02-04 00:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:22][root][INFO] - Training Epoch: 2/2, step 19487/23838 completed (loss: 4.2626423835754395, acc: 0.26923078298568726)
[2025-02-04 00:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:22][root][INFO] - Training Epoch: 2/2, step 19488/23838 completed (loss: 3.1195950508117676, acc: 0.550000011920929)
[2025-02-04 00:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:23][root][INFO] - Training Epoch: 2/2, step 19489/23838 completed (loss: 4.053460597991943, acc: 0.2857142984867096)
[2025-02-04 00:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:23][root][INFO] - Training Epoch: 2/2, step 19490/23838 completed (loss: 5.26890754699707, acc: 0.21568627655506134)
[2025-02-04 00:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:24][root][INFO] - Training Epoch: 2/2, step 19491/23838 completed (loss: 2.9444432258605957, acc: 0.4761904776096344)
[2025-02-04 00:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:24][root][INFO] - Training Epoch: 2/2, step 19492/23838 completed (loss: 2.4304730892181396, acc: 0.4285714328289032)
[2025-02-04 00:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:25][root][INFO] - Training Epoch: 2/2, step 19493/23838 completed (loss: 3.639347791671753, acc: 0.25)
[2025-02-04 00:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:25][root][INFO] - Training Epoch: 2/2, step 19494/23838 completed (loss: 2.8418545722961426, acc: 0.4285714328289032)
[2025-02-04 00:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:26][root][INFO] - Training Epoch: 2/2, step 19495/23838 completed (loss: 3.005311965942383, acc: 0.44897958636283875)
[2025-02-04 00:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:26][root][INFO] - Training Epoch: 2/2, step 19496/23838 completed (loss: 2.349886655807495, acc: 0.4117647111415863)
[2025-02-04 00:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:27][root][INFO] - Training Epoch: 2/2, step 19497/23838 completed (loss: 2.737919807434082, acc: 0.4736842215061188)
[2025-02-04 00:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:27][root][INFO] - Training Epoch: 2/2, step 19498/23838 completed (loss: 3.125962495803833, acc: 0.46666666865348816)
[2025-02-04 00:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:28][root][INFO] - Training Epoch: 2/2, step 19499/23838 completed (loss: 3.3739287853240967, acc: 0.3529411852359772)
[2025-02-04 00:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:28][root][INFO] - Training Epoch: 2/2, step 19500/23838 completed (loss: 3.294553518295288, acc: 0.5)
[2025-02-04 00:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:28][root][INFO] - Training Epoch: 2/2, step 19501/23838 completed (loss: 3.0540990829467773, acc: 0.5151515007019043)
[2025-02-04 00:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:29][root][INFO] - Training Epoch: 2/2, step 19502/23838 completed (loss: 2.6211955547332764, acc: 0.4761904776096344)
[2025-02-04 00:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:29][root][INFO] - Training Epoch: 2/2, step 19503/23838 completed (loss: 3.161227226257324, acc: 0.3255814015865326)
[2025-02-04 00:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:30][root][INFO] - Training Epoch: 2/2, step 19504/23838 completed (loss: 2.0344457626342773, acc: 0.5769230723381042)
[2025-02-04 00:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:30][root][INFO] - Training Epoch: 2/2, step 19505/23838 completed (loss: 2.2441060543060303, acc: 0.5416666865348816)
[2025-02-04 00:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:31][root][INFO] - Training Epoch: 2/2, step 19506/23838 completed (loss: 2.4687860012054443, acc: 0.4642857015132904)
[2025-02-04 00:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:31][root][INFO] - Training Epoch: 2/2, step 19507/23838 completed (loss: 2.7164411544799805, acc: 0.4897959232330322)
[2025-02-04 00:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:31][root][INFO] - Training Epoch: 2/2, step 19508/23838 completed (loss: 2.058406352996826, acc: 0.5)
[2025-02-04 00:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:32][root][INFO] - Training Epoch: 2/2, step 19509/23838 completed (loss: 1.0245330333709717, acc: 0.6153846383094788)
[2025-02-04 00:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:32][root][INFO] - Training Epoch: 2/2, step 19510/23838 completed (loss: 2.874171733856201, acc: 0.41304346919059753)
[2025-02-04 00:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:33][root][INFO] - Training Epoch: 2/2, step 19511/23838 completed (loss: 3.0177252292633057, acc: 0.3777777850627899)
[2025-02-04 00:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:33][root][INFO] - Training Epoch: 2/2, step 19512/23838 completed (loss: 2.410320281982422, acc: 0.47826087474823)
[2025-02-04 00:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:34][root][INFO] - Training Epoch: 2/2, step 19513/23838 completed (loss: 1.9037140607833862, acc: 0.692307710647583)
[2025-02-04 00:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:34][root][INFO] - Training Epoch: 2/2, step 19514/23838 completed (loss: 4.030066013336182, acc: 0.3333333432674408)
[2025-02-04 00:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:35][root][INFO] - Training Epoch: 2/2, step 19515/23838 completed (loss: 1.9980344772338867, acc: 0.6190476417541504)
[2025-02-04 00:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:35][root][INFO] - Training Epoch: 2/2, step 19516/23838 completed (loss: 4.7885589599609375, acc: 0.30434781312942505)
[2025-02-04 00:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:36][root][INFO] - Training Epoch: 2/2, step 19517/23838 completed (loss: 3.670672655105591, acc: 0.3636363744735718)
[2025-02-04 00:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:36][root][INFO] - Training Epoch: 2/2, step 19518/23838 completed (loss: 1.5915958881378174, acc: 0.5652173757553101)
[2025-02-04 00:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:37][root][INFO] - Training Epoch: 2/2, step 19519/23838 completed (loss: 2.6603870391845703, acc: 0.6428571343421936)
[2025-02-04 00:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:37][root][INFO] - Training Epoch: 2/2, step 19520/23838 completed (loss: 2.527299165725708, acc: 0.5)
[2025-02-04 00:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:37][root][INFO] - Training Epoch: 2/2, step 19521/23838 completed (loss: 3.6213555335998535, acc: 0.28260868787765503)
[2025-02-04 00:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:38][root][INFO] - Training Epoch: 2/2, step 19522/23838 completed (loss: 2.2194764614105225, acc: 0.4722222089767456)
[2025-02-04 00:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:38][root][INFO] - Training Epoch: 2/2, step 19523/23838 completed (loss: 2.727419853210449, acc: 0.6111111044883728)
[2025-02-04 00:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:39][root][INFO] - Training Epoch: 2/2, step 19524/23838 completed (loss: 1.5341780185699463, acc: 0.6190476417541504)
[2025-02-04 00:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:39][root][INFO] - Training Epoch: 2/2, step 19525/23838 completed (loss: 2.646942377090454, acc: 0.38235294818878174)
[2025-02-04 00:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:40][root][INFO] - Training Epoch: 2/2, step 19526/23838 completed (loss: 2.778136730194092, acc: 0.36666667461395264)
[2025-02-04 00:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:40][root][INFO] - Training Epoch: 2/2, step 19527/23838 completed (loss: 3.2108960151672363, acc: 0.3478260934352875)
[2025-02-04 00:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:40][root][INFO] - Training Epoch: 2/2, step 19528/23838 completed (loss: 1.7952982187271118, acc: 0.5)
[2025-02-04 00:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:41][root][INFO] - Training Epoch: 2/2, step 19529/23838 completed (loss: 0.11639396846294403, acc: 1.0)
[2025-02-04 00:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:41][root][INFO] - Training Epoch: 2/2, step 19530/23838 completed (loss: 1.5911248922348022, acc: 0.692307710647583)
[2025-02-04 00:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:42][root][INFO] - Training Epoch: 2/2, step 19531/23838 completed (loss: 2.988527536392212, acc: 0.4047619104385376)
[2025-02-04 00:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:42][root][INFO] - Training Epoch: 2/2, step 19532/23838 completed (loss: 1.753448486328125, acc: 0.692307710647583)
[2025-02-04 00:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:42][root][INFO] - Training Epoch: 2/2, step 19533/23838 completed (loss: 1.5280405282974243, acc: 0.6000000238418579)
[2025-02-04 00:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:43][root][INFO] - Training Epoch: 2/2, step 19534/23838 completed (loss: 2.178448438644409, acc: 0.47058823704719543)
[2025-02-04 00:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:43][root][INFO] - Training Epoch: 2/2, step 19535/23838 completed (loss: 1.989537239074707, acc: 0.5555555820465088)
[2025-02-04 00:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:44][root][INFO] - Training Epoch: 2/2, step 19536/23838 completed (loss: 2.767594575881958, acc: 0.4399999976158142)
[2025-02-04 00:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:44][root][INFO] - Training Epoch: 2/2, step 19537/23838 completed (loss: 2.180769920349121, acc: 0.48275861144065857)
[2025-02-04 00:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:44][root][INFO] - Training Epoch: 2/2, step 19538/23838 completed (loss: 1.7392866611480713, acc: 0.6785714030265808)
[2025-02-04 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:45][root][INFO] - Training Epoch: 2/2, step 19539/23838 completed (loss: 1.487068772315979, acc: 0.7142857313156128)
[2025-02-04 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:45][root][INFO] - Training Epoch: 2/2, step 19540/23838 completed (loss: 1.3315662145614624, acc: 0.6111111044883728)
[2025-02-04 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:46][root][INFO] - Training Epoch: 2/2, step 19541/23838 completed (loss: 2.1473705768585205, acc: 0.48571428656578064)
[2025-02-04 00:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:46][root][INFO] - Training Epoch: 2/2, step 19542/23838 completed (loss: 1.9958873987197876, acc: 0.5789473652839661)
[2025-02-04 00:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:47][root][INFO] - Training Epoch: 2/2, step 19543/23838 completed (loss: 1.4551433324813843, acc: 0.6052631735801697)
[2025-02-04 00:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:47][root][INFO] - Training Epoch: 2/2, step 19544/23838 completed (loss: 3.148672342300415, acc: 0.37931033968925476)
[2025-02-04 00:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:48][root][INFO] - Training Epoch: 2/2, step 19545/23838 completed (loss: 0.39845025539398193, acc: 0.8999999761581421)
[2025-02-04 00:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:48][root][INFO] - Training Epoch: 2/2, step 19546/23838 completed (loss: 1.8414634466171265, acc: 0.5454545617103577)
[2025-02-04 00:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:48][root][INFO] - Training Epoch: 2/2, step 19547/23838 completed (loss: 3.0266060829162598, acc: 0.46341463923454285)
[2025-02-04 00:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:49][root][INFO] - Training Epoch: 2/2, step 19548/23838 completed (loss: 2.449972152709961, acc: 0.5517241358757019)
[2025-02-04 00:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:49][root][INFO] - Training Epoch: 2/2, step 19549/23838 completed (loss: 2.640223264694214, acc: 0.5384615659713745)
[2025-02-04 00:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:50][root][INFO] - Training Epoch: 2/2, step 19550/23838 completed (loss: 3.0662057399749756, acc: 0.4838709533214569)
[2025-02-04 00:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:51][root][INFO] - Training Epoch: 2/2, step 19551/23838 completed (loss: 4.371798038482666, acc: 0.30000001192092896)
[2025-02-04 00:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:51][root][INFO] - Training Epoch: 2/2, step 19552/23838 completed (loss: 2.6489522457122803, acc: 0.3863636255264282)
[2025-02-04 00:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:51][root][INFO] - Training Epoch: 2/2, step 19553/23838 completed (loss: 2.901897430419922, acc: 0.3888888955116272)
[2025-02-04 00:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:52][root][INFO] - Training Epoch: 2/2, step 19554/23838 completed (loss: 3.04494047164917, acc: 0.296875)
[2025-02-04 00:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:52][root][INFO] - Training Epoch: 2/2, step 19555/23838 completed (loss: 2.3368639945983887, acc: 0.5531914830207825)
[2025-02-04 00:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:53][root][INFO] - Training Epoch: 2/2, step 19556/23838 completed (loss: 2.7614364624023438, acc: 0.5)
[2025-02-04 00:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:53][root][INFO] - Training Epoch: 2/2, step 19557/23838 completed (loss: 3.2192811965942383, acc: 0.4375)
[2025-02-04 00:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:54][root][INFO] - Training Epoch: 2/2, step 19558/23838 completed (loss: 1.4473016262054443, acc: 0.6428571343421936)
[2025-02-04 00:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:54][root][INFO] - Training Epoch: 2/2, step 19559/23838 completed (loss: 1.7346760034561157, acc: 0.625)
[2025-02-04 00:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:55][root][INFO] - Training Epoch: 2/2, step 19560/23838 completed (loss: 2.9692745208740234, acc: 0.5869565010070801)
[2025-02-04 00:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:55][root][INFO] - Training Epoch: 2/2, step 19561/23838 completed (loss: 3.5092737674713135, acc: 0.43478259444236755)
[2025-02-04 00:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:56][root][INFO] - Training Epoch: 2/2, step 19562/23838 completed (loss: 3.3996670246124268, acc: 0.29629629850387573)
[2025-02-04 00:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:56][root][INFO] - Training Epoch: 2/2, step 19563/23838 completed (loss: 2.587151288986206, acc: 0.41999998688697815)
[2025-02-04 00:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:56][root][INFO] - Training Epoch: 2/2, step 19564/23838 completed (loss: 3.588066816329956, acc: 0.3333333432674408)
[2025-02-04 00:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:57][root][INFO] - Training Epoch: 2/2, step 19565/23838 completed (loss: 3.2975687980651855, acc: 0.2777777910232544)
[2025-02-04 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:57][root][INFO] - Training Epoch: 2/2, step 19566/23838 completed (loss: 3.4284679889678955, acc: 0.3870967626571655)
[2025-02-04 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:58][root][INFO] - Training Epoch: 2/2, step 19567/23838 completed (loss: 2.94598650932312, acc: 0.3191489279270172)
[2025-02-04 00:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:58][root][INFO] - Training Epoch: 2/2, step 19568/23838 completed (loss: 2.767742395401001, acc: 0.4193548262119293)
[2025-02-04 00:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:59][root][INFO] - Training Epoch: 2/2, step 19569/23838 completed (loss: 2.757988214492798, acc: 0.43478259444236755)
[2025-02-04 00:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:45:59][root][INFO] - Training Epoch: 2/2, step 19570/23838 completed (loss: 2.1860859394073486, acc: 0.4285714328289032)
[2025-02-04 00:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:00][root][INFO] - Training Epoch: 2/2, step 19571/23838 completed (loss: 2.7509701251983643, acc: 0.4000000059604645)
[2025-02-04 00:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:00][root][INFO] - Training Epoch: 2/2, step 19572/23838 completed (loss: 3.4400079250335693, acc: 0.34285715222358704)
[2025-02-04 00:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:00][root][INFO] - Training Epoch: 2/2, step 19573/23838 completed (loss: 3.3996646404266357, acc: 0.2702702581882477)
[2025-02-04 00:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:01][root][INFO] - Training Epoch: 2/2, step 19574/23838 completed (loss: 2.7823326587677, acc: 0.4375)
[2025-02-04 00:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:01][root][INFO] - Training Epoch: 2/2, step 19575/23838 completed (loss: 2.9229533672332764, acc: 0.3499999940395355)
[2025-02-04 00:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:02][root][INFO] - Training Epoch: 2/2, step 19576/23838 completed (loss: 3.45092511177063, acc: 0.37837839126586914)
[2025-02-04 00:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:02][root][INFO] - Training Epoch: 2/2, step 19577/23838 completed (loss: 2.7949326038360596, acc: 0.4137931168079376)
[2025-02-04 00:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:02][root][INFO] - Training Epoch: 2/2, step 19578/23838 completed (loss: 2.9649765491485596, acc: 0.25925925374031067)
[2025-02-04 00:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:03][root][INFO] - Training Epoch: 2/2, step 19579/23838 completed (loss: 3.350372076034546, acc: 0.3214285671710968)
[2025-02-04 00:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:03][root][INFO] - Training Epoch: 2/2, step 19580/23838 completed (loss: 2.734961986541748, acc: 0.3235294222831726)
[2025-02-04 00:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:04][root][INFO] - Training Epoch: 2/2, step 19581/23838 completed (loss: 2.4814558029174805, acc: 0.5199999809265137)
[2025-02-04 00:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:04][root][INFO] - Training Epoch: 2/2, step 19582/23838 completed (loss: 2.8505148887634277, acc: 0.37142857909202576)
[2025-02-04 00:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:04][root][INFO] - Training Epoch: 2/2, step 19583/23838 completed (loss: 3.1760711669921875, acc: 0.3095238208770752)
[2025-02-04 00:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:05][root][INFO] - Training Epoch: 2/2, step 19584/23838 completed (loss: 3.0991532802581787, acc: 0.2666666805744171)
[2025-02-04 00:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:05][root][INFO] - Training Epoch: 2/2, step 19585/23838 completed (loss: 2.9748711585998535, acc: 0.4146341383457184)
[2025-02-04 00:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:06][root][INFO] - Training Epoch: 2/2, step 19586/23838 completed (loss: 2.1050097942352295, acc: 0.53125)
[2025-02-04 00:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:06][root][INFO] - Training Epoch: 2/2, step 19587/23838 completed (loss: 2.914262533187866, acc: 0.3478260934352875)
[2025-02-04 00:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:07][root][INFO] - Training Epoch: 2/2, step 19588/23838 completed (loss: 2.8346920013427734, acc: 0.38235294818878174)
[2025-02-04 00:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:07][root][INFO] - Training Epoch: 2/2, step 19589/23838 completed (loss: 3.4023983478546143, acc: 0.37142857909202576)
[2025-02-04 00:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:07][root][INFO] - Training Epoch: 2/2, step 19590/23838 completed (loss: 3.6786959171295166, acc: 0.2800000011920929)
[2025-02-04 00:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:08][root][INFO] - Training Epoch: 2/2, step 19591/23838 completed (loss: 2.686354875564575, acc: 0.3333333432674408)
[2025-02-04 00:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:08][root][INFO] - Training Epoch: 2/2, step 19592/23838 completed (loss: 2.9483582973480225, acc: 0.3076923191547394)
[2025-02-04 00:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:09][root][INFO] - Training Epoch: 2/2, step 19593/23838 completed (loss: 3.4607455730438232, acc: 0.2380952388048172)
[2025-02-04 00:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:09][root][INFO] - Training Epoch: 2/2, step 19594/23838 completed (loss: 2.287109851837158, acc: 0.44999998807907104)
[2025-02-04 00:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:09][root][INFO] - Training Epoch: 2/2, step 19595/23838 completed (loss: 2.6783759593963623, acc: 0.47999998927116394)
[2025-02-04 00:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:10][root][INFO] - Training Epoch: 2/2, step 19596/23838 completed (loss: 3.445117712020874, acc: 0.4000000059604645)
[2025-02-04 00:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:10][root][INFO] - Training Epoch: 2/2, step 19597/23838 completed (loss: 3.7902064323425293, acc: 0.4375)
[2025-02-04 00:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:11][root][INFO] - Training Epoch: 2/2, step 19598/23838 completed (loss: 4.236334800720215, acc: 0.2222222238779068)
[2025-02-04 00:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:11][root][INFO] - Training Epoch: 2/2, step 19599/23838 completed (loss: 2.8347671031951904, acc: 0.4333333373069763)
[2025-02-04 00:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:12][root][INFO] - Training Epoch: 2/2, step 19600/23838 completed (loss: 3.698174238204956, acc: 0.3181818127632141)
[2025-02-04 00:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:12][root][INFO] - Training Epoch: 2/2, step 19601/23838 completed (loss: 2.2087583541870117, acc: 0.5909090638160706)
[2025-02-04 00:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:13][root][INFO] - Training Epoch: 2/2, step 19602/23838 completed (loss: 3.1583380699157715, acc: 0.3913043439388275)
[2025-02-04 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:13][root][INFO] - Training Epoch: 2/2, step 19603/23838 completed (loss: 2.9989418983459473, acc: 0.4761904776096344)
[2025-02-04 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:13][root][INFO] - Training Epoch: 2/2, step 19604/23838 completed (loss: 2.931516408920288, acc: 0.4146341383457184)
[2025-02-04 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:14][root][INFO] - Training Epoch: 2/2, step 19605/23838 completed (loss: 2.9333138465881348, acc: 0.375)
[2025-02-04 00:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:14][root][INFO] - Training Epoch: 2/2, step 19606/23838 completed (loss: 3.546111822128296, acc: 0.37037035822868347)
[2025-02-04 00:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:14][root][INFO] - Training Epoch: 2/2, step 19607/23838 completed (loss: 3.1440255641937256, acc: 0.35483869910240173)
[2025-02-04 00:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:15][root][INFO] - Training Epoch: 2/2, step 19608/23838 completed (loss: 2.7213523387908936, acc: 0.5)
[2025-02-04 00:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:15][root][INFO] - Training Epoch: 2/2, step 19609/23838 completed (loss: 3.421327829360962, acc: 0.31111112236976624)
[2025-02-04 00:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:16][root][INFO] - Training Epoch: 2/2, step 19610/23838 completed (loss: 2.256657361984253, acc: 0.52173912525177)
[2025-02-04 00:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:16][root][INFO] - Training Epoch: 2/2, step 19611/23838 completed (loss: 2.7527048587799072, acc: 0.3636363744735718)
[2025-02-04 00:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:16][root][INFO] - Training Epoch: 2/2, step 19612/23838 completed (loss: 2.675758123397827, acc: 0.44999998807907104)
[2025-02-04 00:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:17][root][INFO] - Training Epoch: 2/2, step 19613/23838 completed (loss: 3.382296323776245, acc: 0.3333333432674408)
[2025-02-04 00:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:17][root][INFO] - Training Epoch: 2/2, step 19614/23838 completed (loss: 3.4868452548980713, acc: 0.3636363744735718)
[2025-02-04 00:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:18][root][INFO] - Training Epoch: 2/2, step 19615/23838 completed (loss: 4.616749286651611, acc: 0.20000000298023224)
[2025-02-04 00:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:18][root][INFO] - Training Epoch: 2/2, step 19616/23838 completed (loss: 2.9835779666900635, acc: 0.4444444477558136)
[2025-02-04 00:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:19][root][INFO] - Training Epoch: 2/2, step 19617/23838 completed (loss: 4.206523895263672, acc: 0.3012048304080963)
[2025-02-04 00:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:19][root][INFO] - Training Epoch: 2/2, step 19618/23838 completed (loss: 3.116255521774292, acc: 0.4000000059604645)
[2025-02-04 00:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:19][root][INFO] - Training Epoch: 2/2, step 19619/23838 completed (loss: 2.1218512058258057, acc: 0.5)
[2025-02-04 00:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:20][root][INFO] - Training Epoch: 2/2, step 19620/23838 completed (loss: 3.387108087539673, acc: 0.4642857015132904)
[2025-02-04 00:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:20][root][INFO] - Training Epoch: 2/2, step 19621/23838 completed (loss: 2.559464454650879, acc: 0.46875)
[2025-02-04 00:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:20][root][INFO] - Training Epoch: 2/2, step 19622/23838 completed (loss: 3.454784393310547, acc: 0.3658536672592163)
[2025-02-04 00:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:21][root][INFO] - Training Epoch: 2/2, step 19623/23838 completed (loss: 4.20965576171875, acc: 0.4146341383457184)
[2025-02-04 00:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:21][root][INFO] - Training Epoch: 2/2, step 19624/23838 completed (loss: 3.232391595840454, acc: 0.2857142984867096)
[2025-02-04 00:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:22][root][INFO] - Training Epoch: 2/2, step 19625/23838 completed (loss: 2.761401414871216, acc: 0.4615384638309479)
[2025-02-04 00:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:22][root][INFO] - Training Epoch: 2/2, step 19626/23838 completed (loss: 2.640183687210083, acc: 0.42424243688583374)
[2025-02-04 00:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:22][root][INFO] - Training Epoch: 2/2, step 19627/23838 completed (loss: 2.975452423095703, acc: 0.375)
[2025-02-04 00:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:23][root][INFO] - Training Epoch: 2/2, step 19628/23838 completed (loss: 2.9399609565734863, acc: 0.4000000059604645)
[2025-02-04 00:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:23][root][INFO] - Training Epoch: 2/2, step 19629/23838 completed (loss: 3.1759376525878906, acc: 0.40625)
[2025-02-04 00:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:24][root][INFO] - Training Epoch: 2/2, step 19630/23838 completed (loss: 3.5386810302734375, acc: 0.42500001192092896)
[2025-02-04 00:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:24][root][INFO] - Training Epoch: 2/2, step 19631/23838 completed (loss: 2.2510101795196533, acc: 0.3636363744735718)
[2025-02-04 00:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:24][root][INFO] - Training Epoch: 2/2, step 19632/23838 completed (loss: 3.132579803466797, acc: 0.2702702581882477)
[2025-02-04 00:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:25][root][INFO] - Training Epoch: 2/2, step 19633/23838 completed (loss: 2.5474679470062256, acc: 0.5)
[2025-02-04 00:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:25][root][INFO] - Training Epoch: 2/2, step 19634/23838 completed (loss: 2.5843703746795654, acc: 0.6000000238418579)
[2025-02-04 00:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:26][root][INFO] - Training Epoch: 2/2, step 19635/23838 completed (loss: 2.963557243347168, acc: 0.375)
[2025-02-04 00:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:26][root][INFO] - Training Epoch: 2/2, step 19636/23838 completed (loss: 1.7960741519927979, acc: 0.6129032373428345)
[2025-02-04 00:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:26][root][INFO] - Training Epoch: 2/2, step 19637/23838 completed (loss: 3.105678081512451, acc: 0.44897958636283875)
[2025-02-04 00:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:27][root][INFO] - Training Epoch: 2/2, step 19638/23838 completed (loss: 2.1285979747772217, acc: 0.4571428596973419)
[2025-02-04 00:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:27][root][INFO] - Training Epoch: 2/2, step 19639/23838 completed (loss: 2.524162769317627, acc: 0.4583333432674408)
[2025-02-04 00:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:28][root][INFO] - Training Epoch: 2/2, step 19640/23838 completed (loss: 2.3889174461364746, acc: 0.4722222089767456)
[2025-02-04 00:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:28][root][INFO] - Training Epoch: 2/2, step 19641/23838 completed (loss: 2.1046090126037598, acc: 0.5476190447807312)
[2025-02-04 00:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:28][root][INFO] - Training Epoch: 2/2, step 19642/23838 completed (loss: 3.361267566680908, acc: 0.37931033968925476)
[2025-02-04 00:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:29][root][INFO] - Training Epoch: 2/2, step 19643/23838 completed (loss: 3.1563608646392822, acc: 0.5)
[2025-02-04 00:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:29][root][INFO] - Training Epoch: 2/2, step 19644/23838 completed (loss: 2.5653998851776123, acc: 0.6000000238418579)
[2025-02-04 00:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:30][root][INFO] - Training Epoch: 2/2, step 19645/23838 completed (loss: 3.2442893981933594, acc: 0.3571428656578064)
[2025-02-04 00:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:30][root][INFO] - Training Epoch: 2/2, step 19646/23838 completed (loss: 3.3291735649108887, acc: 0.38461539149284363)
[2025-02-04 00:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:30][root][INFO] - Training Epoch: 2/2, step 19647/23838 completed (loss: 2.3771657943725586, acc: 0.47058823704719543)
[2025-02-04 00:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:31][root][INFO] - Training Epoch: 2/2, step 19648/23838 completed (loss: 3.065431594848633, acc: 0.3947368562221527)
[2025-02-04 00:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:31][root][INFO] - Training Epoch: 2/2, step 19649/23838 completed (loss: 3.413027048110962, acc: 0.40740740299224854)
[2025-02-04 00:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:32][root][INFO] - Training Epoch: 2/2, step 19650/23838 completed (loss: 3.7002313137054443, acc: 0.3870967626571655)
[2025-02-04 00:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:32][root][INFO] - Training Epoch: 2/2, step 19651/23838 completed (loss: 3.6476423740386963, acc: 0.4166666567325592)
[2025-02-04 00:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:33][root][INFO] - Training Epoch: 2/2, step 19652/23838 completed (loss: 3.748474597930908, acc: 0.2666666805744171)
[2025-02-04 00:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:33][root][INFO] - Training Epoch: 2/2, step 19653/23838 completed (loss: 2.2228779792785645, acc: 0.5714285969734192)
[2025-02-04 00:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:33][root][INFO] - Training Epoch: 2/2, step 19654/23838 completed (loss: 1.8621755838394165, acc: 0.5925925970077515)
[2025-02-04 00:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:34][root][INFO] - Training Epoch: 2/2, step 19655/23838 completed (loss: 2.9362902641296387, acc: 0.3199999928474426)
[2025-02-04 00:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:34][root][INFO] - Training Epoch: 2/2, step 19656/23838 completed (loss: 3.470499038696289, acc: 0.4285714328289032)
[2025-02-04 00:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:35][root][INFO] - Training Epoch: 2/2, step 19657/23838 completed (loss: 3.077258348464966, acc: 0.4193548262119293)
[2025-02-04 00:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:35][root][INFO] - Training Epoch: 2/2, step 19658/23838 completed (loss: 2.7090842723846436, acc: 0.4375)
[2025-02-04 00:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:35][root][INFO] - Training Epoch: 2/2, step 19659/23838 completed (loss: 0.30706849694252014, acc: 1.0)
[2025-02-04 00:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:36][root][INFO] - Training Epoch: 2/2, step 19660/23838 completed (loss: 2.523303270339966, acc: 0.5)
[2025-02-04 00:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:36][root][INFO] - Training Epoch: 2/2, step 19661/23838 completed (loss: 2.0699896812438965, acc: 0.3888888955116272)
[2025-02-04 00:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:37][root][INFO] - Training Epoch: 2/2, step 19662/23838 completed (loss: 2.5780019760131836, acc: 0.5789473652839661)
[2025-02-04 00:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:37][root][INFO] - Training Epoch: 2/2, step 19663/23838 completed (loss: 3.1989896297454834, acc: 0.32258063554763794)
[2025-02-04 00:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:38][root][INFO] - Training Epoch: 2/2, step 19664/23838 completed (loss: 3.392188310623169, acc: 0.2800000011920929)
[2025-02-04 00:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:38][root][INFO] - Training Epoch: 2/2, step 19665/23838 completed (loss: 4.460377216339111, acc: 0.3166666626930237)
[2025-02-04 00:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:38][root][INFO] - Training Epoch: 2/2, step 19666/23838 completed (loss: 4.554726600646973, acc: 0.2857142984867096)
[2025-02-04 00:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:39][root][INFO] - Training Epoch: 2/2, step 19667/23838 completed (loss: 3.7168571949005127, acc: 0.3684210479259491)
[2025-02-04 00:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:39][root][INFO] - Training Epoch: 2/2, step 19668/23838 completed (loss: 2.8746237754821777, acc: 0.5)
[2025-02-04 00:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:40][root][INFO] - Training Epoch: 2/2, step 19669/23838 completed (loss: 2.8318521976470947, acc: 0.5)
[2025-02-04 00:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:40][root][INFO] - Training Epoch: 2/2, step 19670/23838 completed (loss: 2.9819135665893555, acc: 0.375)
[2025-02-04 00:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:40][root][INFO] - Training Epoch: 2/2, step 19671/23838 completed (loss: 2.104757070541382, acc: 0.6086956262588501)
[2025-02-04 00:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:41][root][INFO] - Training Epoch: 2/2, step 19672/23838 completed (loss: 3.0758118629455566, acc: 0.42500001192092896)
[2025-02-04 00:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:41][root][INFO] - Training Epoch: 2/2, step 19673/23838 completed (loss: 2.065009355545044, acc: 0.517241358757019)
[2025-02-04 00:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:42][root][INFO] - Training Epoch: 2/2, step 19674/23838 completed (loss: 3.8846302032470703, acc: 0.34545454382896423)
[2025-02-04 00:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:42][root][INFO] - Training Epoch: 2/2, step 19675/23838 completed (loss: 3.634079694747925, acc: 0.3333333432674408)
[2025-02-04 00:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:43][root][INFO] - Training Epoch: 2/2, step 19676/23838 completed (loss: 3.888983964920044, acc: 0.375)
[2025-02-04 00:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:43][root][INFO] - Training Epoch: 2/2, step 19677/23838 completed (loss: 1.2368453741073608, acc: 0.6666666865348816)
[2025-02-04 00:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:44][root][INFO] - Training Epoch: 2/2, step 19678/23838 completed (loss: 2.6509006023406982, acc: 0.41818180680274963)
[2025-02-04 00:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:44][root][INFO] - Training Epoch: 2/2, step 19679/23838 completed (loss: 2.793842315673828, acc: 0.5)
[2025-02-04 00:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:44][root][INFO] - Training Epoch: 2/2, step 19680/23838 completed (loss: 1.100760817527771, acc: 0.695652186870575)
[2025-02-04 00:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:45][root][INFO] - Training Epoch: 2/2, step 19681/23838 completed (loss: 2.3294849395751953, acc: 0.5)
[2025-02-04 00:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:45][root][INFO] - Training Epoch: 2/2, step 19682/23838 completed (loss: 3.390918731689453, acc: 0.3787878751754761)
[2025-02-04 00:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:46][root][INFO] - Training Epoch: 2/2, step 19683/23838 completed (loss: 2.8834657669067383, acc: 0.4444444477558136)
[2025-02-04 00:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:46][root][INFO] - Training Epoch: 2/2, step 19684/23838 completed (loss: 2.02517032623291, acc: 0.5)
[2025-02-04 00:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:47][root][INFO] - Training Epoch: 2/2, step 19685/23838 completed (loss: 1.8587682247161865, acc: 0.5925925970077515)
[2025-02-04 00:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:47][root][INFO] - Training Epoch: 2/2, step 19686/23838 completed (loss: 2.5803699493408203, acc: 0.40740740299224854)
[2025-02-04 00:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:48][root][INFO] - Training Epoch: 2/2, step 19687/23838 completed (loss: 3.2642602920532227, acc: 0.40909090638160706)
[2025-02-04 00:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:48][root][INFO] - Training Epoch: 2/2, step 19688/23838 completed (loss: 2.4896938800811768, acc: 0.453125)
[2025-02-04 00:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:48][root][INFO] - Training Epoch: 2/2, step 19689/23838 completed (loss: 2.3973000049591064, acc: 0.52173912525177)
[2025-02-04 00:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:49][root][INFO] - Training Epoch: 2/2, step 19690/23838 completed (loss: 1.958717942237854, acc: 0.5277777910232544)
[2025-02-04 00:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:49][root][INFO] - Training Epoch: 2/2, step 19691/23838 completed (loss: 2.207923173904419, acc: 0.5249999761581421)
[2025-02-04 00:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:50][root][INFO] - Training Epoch: 2/2, step 19692/23838 completed (loss: 2.4619944095611572, acc: 0.5454545617103577)
[2025-02-04 00:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:50][root][INFO] - Training Epoch: 2/2, step 19693/23838 completed (loss: 1.514378309249878, acc: 0.6896551847457886)
[2025-02-04 00:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:51][root][INFO] - Training Epoch: 2/2, step 19694/23838 completed (loss: 1.4092814922332764, acc: 0.6399999856948853)
[2025-02-04 00:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:51][root][INFO] - Training Epoch: 2/2, step 19695/23838 completed (loss: 0.4616069793701172, acc: 0.8857142925262451)
[2025-02-04 00:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:52][root][INFO] - Training Epoch: 2/2, step 19696/23838 completed (loss: 1.3058326244354248, acc: 0.7407407164573669)
[2025-02-04 00:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:52][root][INFO] - Training Epoch: 2/2, step 19697/23838 completed (loss: 3.3548059463500977, acc: 0.46666666865348816)
[2025-02-04 00:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:52][root][INFO] - Training Epoch: 2/2, step 19698/23838 completed (loss: 2.4177591800689697, acc: 0.3888888955116272)
[2025-02-04 00:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:53][root][INFO] - Training Epoch: 2/2, step 19699/23838 completed (loss: 3.8158740997314453, acc: 0.5)
[2025-02-04 00:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:53][root][INFO] - Training Epoch: 2/2, step 19700/23838 completed (loss: 3.723972797393799, acc: 0.375)
[2025-02-04 00:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:54][root][INFO] - Training Epoch: 2/2, step 19701/23838 completed (loss: 3.061180353164673, acc: 0.4285714328289032)
[2025-02-04 00:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:54][root][INFO] - Training Epoch: 2/2, step 19702/23838 completed (loss: 2.3568248748779297, acc: 0.5882353186607361)
[2025-02-04 00:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:55][root][INFO] - Training Epoch: 2/2, step 19703/23838 completed (loss: 3.10090970993042, acc: 0.37931033968925476)
[2025-02-04 00:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:55][root][INFO] - Training Epoch: 2/2, step 19704/23838 completed (loss: 4.6989569664001465, acc: 0.2916666567325592)
[2025-02-04 00:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:56][root][INFO] - Training Epoch: 2/2, step 19705/23838 completed (loss: 4.021510124206543, acc: 0.3478260934352875)
[2025-02-04 00:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:56][root][INFO] - Training Epoch: 2/2, step 19706/23838 completed (loss: 2.826080560684204, acc: 0.4117647111415863)
[2025-02-04 00:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:56][root][INFO] - Training Epoch: 2/2, step 19707/23838 completed (loss: 3.594670534133911, acc: 0.3243243098258972)
[2025-02-04 00:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:57][root][INFO] - Training Epoch: 2/2, step 19708/23838 completed (loss: 4.221782207489014, acc: 0.34210526943206787)
[2025-02-04 00:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:57][root][INFO] - Training Epoch: 2/2, step 19709/23838 completed (loss: 4.297184944152832, acc: 0.25)
[2025-02-04 00:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:58][root][INFO] - Training Epoch: 2/2, step 19710/23838 completed (loss: 3.5080406665802, acc: 0.34285715222358704)
[2025-02-04 00:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:58][root][INFO] - Training Epoch: 2/2, step 19711/23838 completed (loss: 4.228868007659912, acc: 0.1818181872367859)
[2025-02-04 00:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:59][root][INFO] - Training Epoch: 2/2, step 19712/23838 completed (loss: 4.966418266296387, acc: 0.24137930572032928)
[2025-02-04 00:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:46:59][root][INFO] - Training Epoch: 2/2, step 19713/23838 completed (loss: 4.050774574279785, acc: 0.25)
[2025-02-04 00:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:00][root][INFO] - Training Epoch: 2/2, step 19714/23838 completed (loss: 4.123485565185547, acc: 0.4117647111415863)
[2025-02-04 00:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:00][root][INFO] - Training Epoch: 2/2, step 19715/23838 completed (loss: 4.273777961730957, acc: 0.3243243098258972)
[2025-02-04 00:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:00][root][INFO] - Training Epoch: 2/2, step 19716/23838 completed (loss: 3.4688711166381836, acc: 0.3255814015865326)
[2025-02-04 00:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:01][root][INFO] - Training Epoch: 2/2, step 19717/23838 completed (loss: 2.435685634613037, acc: 0.59375)
[2025-02-04 00:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:01][root][INFO] - Training Epoch: 2/2, step 19718/23838 completed (loss: 4.1637067794799805, acc: 0.3199999928474426)
[2025-02-04 00:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:02][root][INFO] - Training Epoch: 2/2, step 19719/23838 completed (loss: 4.045912742614746, acc: 0.2750000059604645)
[2025-02-04 00:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:02][root][INFO] - Training Epoch: 2/2, step 19720/23838 completed (loss: 2.0971312522888184, acc: 0.5)
[2025-02-04 00:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:02][root][INFO] - Training Epoch: 2/2, step 19721/23838 completed (loss: 2.277986764907837, acc: 0.4117647111415863)
[2025-02-04 00:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:03][root][INFO] - Training Epoch: 2/2, step 19722/23838 completed (loss: 3.386455774307251, acc: 0.4444444477558136)
[2025-02-04 00:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:03][root][INFO] - Training Epoch: 2/2, step 19723/23838 completed (loss: 3.043071746826172, acc: 0.3125)
[2025-02-04 00:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:04][root][INFO] - Training Epoch: 2/2, step 19724/23838 completed (loss: 2.7573328018188477, acc: 0.5909090638160706)
[2025-02-04 00:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:04][root][INFO] - Training Epoch: 2/2, step 19725/23838 completed (loss: 3.2398061752319336, acc: 0.3888888955116272)
[2025-02-04 00:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:04][root][INFO] - Training Epoch: 2/2, step 19726/23838 completed (loss: 4.639602184295654, acc: 0.2678571343421936)
[2025-02-04 00:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:05][root][INFO] - Training Epoch: 2/2, step 19727/23838 completed (loss: 3.5968949794769287, acc: 0.3888888955116272)
[2025-02-04 00:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:05][root][INFO] - Training Epoch: 2/2, step 19728/23838 completed (loss: 3.1979265213012695, acc: 0.39393940567970276)
[2025-02-04 00:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:06][root][INFO] - Training Epoch: 2/2, step 19729/23838 completed (loss: 2.966684103012085, acc: 0.42553192377090454)
[2025-02-04 00:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:07][root][INFO] - Training Epoch: 2/2, step 19730/23838 completed (loss: 3.282334089279175, acc: 0.30000001192092896)
[2025-02-04 00:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:07][root][INFO] - Training Epoch: 2/2, step 19731/23838 completed (loss: 2.796541690826416, acc: 0.5882353186607361)
[2025-02-04 00:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:07][root][INFO] - Training Epoch: 2/2, step 19732/23838 completed (loss: 2.6584832668304443, acc: 0.4390243887901306)
[2025-02-04 00:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:08][root][INFO] - Training Epoch: 2/2, step 19733/23838 completed (loss: 3.098252773284912, acc: 0.4000000059604645)
[2025-02-04 00:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:08][root][INFO] - Training Epoch: 2/2, step 19734/23838 completed (loss: 3.064565896987915, acc: 0.46875)
[2025-02-04 00:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:09][root][INFO] - Training Epoch: 2/2, step 19735/23838 completed (loss: 2.9538357257843018, acc: 0.3947368562221527)
[2025-02-04 00:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:09][root][INFO] - Training Epoch: 2/2, step 19736/23838 completed (loss: 2.346696615219116, acc: 0.4615384638309479)
[2025-02-04 00:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:10][root][INFO] - Training Epoch: 2/2, step 19737/23838 completed (loss: 3.632185935974121, acc: 0.4117647111415863)
[2025-02-04 00:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:10][root][INFO] - Training Epoch: 2/2, step 19738/23838 completed (loss: 3.2176947593688965, acc: 0.5652173757553101)
[2025-02-04 00:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:10][root][INFO] - Training Epoch: 2/2, step 19739/23838 completed (loss: 2.7534472942352295, acc: 0.40425533056259155)
[2025-02-04 00:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:11][root][INFO] - Training Epoch: 2/2, step 19740/23838 completed (loss: 2.6019957065582275, acc: 0.4615384638309479)
[2025-02-04 00:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:11][root][INFO] - Training Epoch: 2/2, step 19741/23838 completed (loss: 2.530925989151001, acc: 0.42424243688583374)
[2025-02-04 00:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:12][root][INFO] - Training Epoch: 2/2, step 19742/23838 completed (loss: 2.6133291721343994, acc: 0.4444444477558136)
[2025-02-04 00:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:12][root][INFO] - Training Epoch: 2/2, step 19743/23838 completed (loss: 1.4226447343826294, acc: 0.523809552192688)
[2025-02-04 00:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:13][root][INFO] - Training Epoch: 2/2, step 19744/23838 completed (loss: 3.705747604370117, acc: 0.3333333432674408)
[2025-02-04 00:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:13][root][INFO] - Training Epoch: 2/2, step 19745/23838 completed (loss: 1.7940731048583984, acc: 0.6499999761581421)
[2025-02-04 00:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:14][root][INFO] - Training Epoch: 2/2, step 19746/23838 completed (loss: 1.910503625869751, acc: 0.5769230723381042)
[2025-02-04 00:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:14][root][INFO] - Training Epoch: 2/2, step 19747/23838 completed (loss: 2.115614175796509, acc: 0.4516128897666931)
[2025-02-04 00:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:15][root][INFO] - Training Epoch: 2/2, step 19748/23838 completed (loss: 2.1055057048797607, acc: 0.5476190447807312)
[2025-02-04 00:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:15][root][INFO] - Training Epoch: 2/2, step 19749/23838 completed (loss: 3.0437991619110107, acc: 0.42222222685813904)
[2025-02-04 00:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:15][root][INFO] - Training Epoch: 2/2, step 19750/23838 completed (loss: 2.6108689308166504, acc: 0.5833333134651184)
[2025-02-04 00:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:16][root][INFO] - Training Epoch: 2/2, step 19751/23838 completed (loss: 3.106475353240967, acc: 0.4000000059604645)
[2025-02-04 00:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:16][root][INFO] - Training Epoch: 2/2, step 19752/23838 completed (loss: 1.9841465950012207, acc: 0.529411792755127)
[2025-02-04 00:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:17][root][INFO] - Training Epoch: 2/2, step 19753/23838 completed (loss: 2.811978816986084, acc: 0.5)
[2025-02-04 00:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:17][root][INFO] - Training Epoch: 2/2, step 19754/23838 completed (loss: 2.5489869117736816, acc: 0.3888888955116272)
[2025-02-04 00:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:18][root][INFO] - Training Epoch: 2/2, step 19755/23838 completed (loss: 1.9355875253677368, acc: 0.5555555820465088)
[2025-02-04 00:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:18][root][INFO] - Training Epoch: 2/2, step 19756/23838 completed (loss: 2.830259084701538, acc: 0.38461539149284363)
[2025-02-04 00:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:18][root][INFO] - Training Epoch: 2/2, step 19757/23838 completed (loss: 2.31152081489563, acc: 0.6666666865348816)
[2025-02-04 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:19][root][INFO] - Training Epoch: 2/2, step 19758/23838 completed (loss: 2.8637588024139404, acc: 0.375)
[2025-02-04 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:19][root][INFO] - Training Epoch: 2/2, step 19759/23838 completed (loss: 2.6580727100372314, acc: 0.5)
[2025-02-04 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:20][root][INFO] - Training Epoch: 2/2, step 19760/23838 completed (loss: 3.340930223464966, acc: 0.42105263471603394)
[2025-02-04 00:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:20][root][INFO] - Training Epoch: 2/2, step 19761/23838 completed (loss: 3.104300022125244, acc: 0.4000000059604645)
[2025-02-04 00:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:21][root][INFO] - Training Epoch: 2/2, step 19762/23838 completed (loss: 2.846764326095581, acc: 0.5882353186607361)
[2025-02-04 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:21][root][INFO] - Training Epoch: 2/2, step 19763/23838 completed (loss: 1.9477068185806274, acc: 0.5714285969734192)
[2025-02-04 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:21][root][INFO] - Training Epoch: 2/2, step 19764/23838 completed (loss: 3.547363042831421, acc: 0.3636363744735718)
[2025-02-04 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:22][root][INFO] - Training Epoch: 2/2, step 19765/23838 completed (loss: 3.8178794384002686, acc: 0.23333333432674408)
[2025-02-04 00:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:22][root][INFO] - Training Epoch: 2/2, step 19766/23838 completed (loss: 2.836036443710327, acc: 0.3529411852359772)
[2025-02-04 00:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:23][root][INFO] - Training Epoch: 2/2, step 19767/23838 completed (loss: 4.033520698547363, acc: 0.10000000149011612)
[2025-02-04 00:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:23][root][INFO] - Training Epoch: 2/2, step 19768/23838 completed (loss: 2.469569683074951, acc: 0.5384615659713745)
[2025-02-04 00:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:23][root][INFO] - Training Epoch: 2/2, step 19769/23838 completed (loss: 2.591865062713623, acc: 0.47058823704719543)
[2025-02-04 00:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:24][root][INFO] - Training Epoch: 2/2, step 19770/23838 completed (loss: 2.9428350925445557, acc: 0.5)
[2025-02-04 00:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:24][root][INFO] - Training Epoch: 2/2, step 19771/23838 completed (loss: 2.8130197525024414, acc: 0.4000000059604645)
[2025-02-04 00:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:25][root][INFO] - Training Epoch: 2/2, step 19772/23838 completed (loss: 2.0165977478027344, acc: 0.5384615659713745)
[2025-02-04 00:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:25][root][INFO] - Training Epoch: 2/2, step 19773/23838 completed (loss: 2.904123067855835, acc: 0.44999998807907104)
[2025-02-04 00:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:26][root][INFO] - Training Epoch: 2/2, step 19774/23838 completed (loss: 2.7833423614501953, acc: 0.5)
[2025-02-04 00:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:26][root][INFO] - Training Epoch: 2/2, step 19775/23838 completed (loss: 2.3442888259887695, acc: 0.46666666865348816)
[2025-02-04 00:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:26][root][INFO] - Training Epoch: 2/2, step 19776/23838 completed (loss: 2.548356533050537, acc: 0.625)
[2025-02-04 00:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:27][root][INFO] - Training Epoch: 2/2, step 19777/23838 completed (loss: 3.9205949306488037, acc: 0.5)
[2025-02-04 00:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:27][root][INFO] - Training Epoch: 2/2, step 19778/23838 completed (loss: 2.3403468132019043, acc: 0.5199999809265137)
[2025-02-04 00:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:28][root][INFO] - Training Epoch: 2/2, step 19779/23838 completed (loss: 3.022393226623535, acc: 0.46666666865348816)
[2025-02-04 00:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:28][root][INFO] - Training Epoch: 2/2, step 19780/23838 completed (loss: 2.5967228412628174, acc: 0.3461538553237915)
[2025-02-04 00:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:29][root][INFO] - Training Epoch: 2/2, step 19781/23838 completed (loss: 2.5364887714385986, acc: 0.6666666865348816)
[2025-02-04 00:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:29][root][INFO] - Training Epoch: 2/2, step 19782/23838 completed (loss: 3.3128299713134766, acc: 0.5)
[2025-02-04 00:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:30][root][INFO] - Training Epoch: 2/2, step 19783/23838 completed (loss: 3.6459264755249023, acc: 0.2800000011920929)
[2025-02-04 00:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:30][root][INFO] - Training Epoch: 2/2, step 19784/23838 completed (loss: 3.214604377746582, acc: 0.375)
[2025-02-04 00:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:30][root][INFO] - Training Epoch: 2/2, step 19785/23838 completed (loss: 2.782888889312744, acc: 0.4285714328289032)
[2025-02-04 00:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:31][root][INFO] - Training Epoch: 2/2, step 19786/23838 completed (loss: 2.786930561065674, acc: 0.4000000059604645)
[2025-02-04 00:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:31][root][INFO] - Training Epoch: 2/2, step 19787/23838 completed (loss: 3.152944326400757, acc: 0.4545454680919647)
[2025-02-04 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:32][root][INFO] - Training Epoch: 2/2, step 19788/23838 completed (loss: 2.1313061714172363, acc: 0.5)
[2025-02-04 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:32][root][INFO] - Training Epoch: 2/2, step 19789/23838 completed (loss: 2.9126133918762207, acc: 0.5)
[2025-02-04 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:33][root][INFO] - Training Epoch: 2/2, step 19790/23838 completed (loss: 2.5302391052246094, acc: 0.5714285969734192)
[2025-02-04 00:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:33][root][INFO] - Training Epoch: 2/2, step 19791/23838 completed (loss: 4.054755210876465, acc: 0.36000001430511475)
[2025-02-04 00:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:34][root][INFO] - Training Epoch: 2/2, step 19792/23838 completed (loss: 2.964174270629883, acc: 0.529411792755127)
[2025-02-04 00:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:34][root][INFO] - Training Epoch: 2/2, step 19793/23838 completed (loss: 3.914044141769409, acc: 0.35483869910240173)
[2025-02-04 00:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:35][root][INFO] - Training Epoch: 2/2, step 19794/23838 completed (loss: 2.1604645252227783, acc: 0.5789473652839661)
[2025-02-04 00:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:35][root][INFO] - Training Epoch: 2/2, step 19795/23838 completed (loss: 1.818799376487732, acc: 0.7142857313156128)
[2025-02-04 00:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:35][root][INFO] - Training Epoch: 2/2, step 19796/23838 completed (loss: 2.300081491470337, acc: 0.6071428656578064)
[2025-02-04 00:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:36][root][INFO] - Training Epoch: 2/2, step 19797/23838 completed (loss: 3.58023738861084, acc: 0.48148149251937866)
[2025-02-04 00:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:36][root][INFO] - Training Epoch: 2/2, step 19798/23838 completed (loss: 2.5918822288513184, acc: 0.5)
[2025-02-04 00:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:37][root][INFO] - Training Epoch: 2/2, step 19799/23838 completed (loss: 2.2972774505615234, acc: 0.523809552192688)
[2025-02-04 00:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:37][root][INFO] - Training Epoch: 2/2, step 19800/23838 completed (loss: 2.7057011127471924, acc: 0.31578946113586426)
[2025-02-04 00:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:37][root][INFO] - Training Epoch: 2/2, step 19801/23838 completed (loss: 1.6065136194229126, acc: 0.6000000238418579)
[2025-02-04 00:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:38][root][INFO] - Training Epoch: 2/2, step 19802/23838 completed (loss: 4.58181095123291, acc: 0.30434781312942505)
[2025-02-04 00:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:38][root][INFO] - Training Epoch: 2/2, step 19803/23838 completed (loss: 2.10188364982605, acc: 0.6153846383094788)
[2025-02-04 00:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:39][root][INFO] - Training Epoch: 2/2, step 19804/23838 completed (loss: 3.7492835521698, acc: 0.4399999976158142)
[2025-02-04 00:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:39][root][INFO] - Training Epoch: 2/2, step 19805/23838 completed (loss: 2.5271594524383545, acc: 0.625)
[2025-02-04 00:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:40][root][INFO] - Training Epoch: 2/2, step 19806/23838 completed (loss: 2.6292989253997803, acc: 0.4736842215061188)
[2025-02-04 00:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:40][root][INFO] - Training Epoch: 2/2, step 19807/23838 completed (loss: 1.6296758651733398, acc: 0.7647058963775635)
[2025-02-04 00:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:40][root][INFO] - Training Epoch: 2/2, step 19808/23838 completed (loss: 2.2837469577789307, acc: 0.5789473652839661)
[2025-02-04 00:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:41][root][INFO] - Training Epoch: 2/2, step 19809/23838 completed (loss: 2.8770980834960938, acc: 0.47058823704719543)
[2025-02-04 00:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:41][root][INFO] - Training Epoch: 2/2, step 19810/23838 completed (loss: 2.3718268871307373, acc: 0.4736842215061188)
[2025-02-04 00:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:42][root][INFO] - Training Epoch: 2/2, step 19811/23838 completed (loss: 2.8188438415527344, acc: 0.3529411852359772)
[2025-02-04 00:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:42][root][INFO] - Training Epoch: 2/2, step 19812/23838 completed (loss: 2.5603418350219727, acc: 0.5)
[2025-02-04 00:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:42][root][INFO] - Training Epoch: 2/2, step 19813/23838 completed (loss: 3.36625337600708, acc: 0.4285714328289032)
[2025-02-04 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:43][root][INFO] - Training Epoch: 2/2, step 19814/23838 completed (loss: 3.1003572940826416, acc: 0.4375)
[2025-02-04 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:43][root][INFO] - Training Epoch: 2/2, step 19815/23838 completed (loss: 1.424506425857544, acc: 0.7142857313156128)
[2025-02-04 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:44][root][INFO] - Training Epoch: 2/2, step 19816/23838 completed (loss: 4.211059093475342, acc: 0.27586206793785095)
[2025-02-04 00:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:44][root][INFO] - Training Epoch: 2/2, step 19817/23838 completed (loss: 4.465852737426758, acc: 0.2777777910232544)
[2025-02-04 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:45][root][INFO] - Training Epoch: 2/2, step 19818/23838 completed (loss: 3.6685256958007812, acc: 0.4444444477558136)
[2025-02-04 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:45][root][INFO] - Training Epoch: 2/2, step 19819/23838 completed (loss: 5.084104537963867, acc: 0.3636363744735718)
[2025-02-04 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:46][root][INFO] - Training Epoch: 2/2, step 19820/23838 completed (loss: 4.823780059814453, acc: 0.3636363744735718)
[2025-02-04 00:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:46][root][INFO] - Training Epoch: 2/2, step 19821/23838 completed (loss: 4.995764255523682, acc: 0.5)
[2025-02-04 00:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:47][root][INFO] - Training Epoch: 2/2, step 19822/23838 completed (loss: 4.856179237365723, acc: 0.4444444477558136)
[2025-02-04 00:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:47][root][INFO] - Training Epoch: 2/2, step 19823/23838 completed (loss: 4.70506477355957, acc: 0.4000000059604645)
[2025-02-04 00:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:47][root][INFO] - Training Epoch: 2/2, step 19824/23838 completed (loss: 5.010369300842285, acc: 0.3636363744735718)
[2025-02-04 00:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:48][root][INFO] - Training Epoch: 2/2, step 19825/23838 completed (loss: 4.126552104949951, acc: 0.5454545617103577)
[2025-02-04 00:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:48][root][INFO] - Training Epoch: 2/2, step 19826/23838 completed (loss: 4.168234825134277, acc: 0.4000000059604645)
[2025-02-04 00:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:48][root][INFO] - Training Epoch: 2/2, step 19827/23838 completed (loss: 4.309020042419434, acc: 0.4444444477558136)
[2025-02-04 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:49][root][INFO] - Training Epoch: 2/2, step 19828/23838 completed (loss: 4.725856781005859, acc: 0.4000000059604645)
[2025-02-04 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:49][root][INFO] - Training Epoch: 2/2, step 19829/23838 completed (loss: 4.095819473266602, acc: 0.4444444477558136)
[2025-02-04 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:50][root][INFO] - Training Epoch: 2/2, step 19830/23838 completed (loss: 4.585869789123535, acc: 0.4000000059604645)
[2025-02-04 00:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:50][root][INFO] - Training Epoch: 2/2, step 19831/23838 completed (loss: 4.415058135986328, acc: 0.3333333432674408)
[2025-02-04 00:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:50][root][INFO] - Training Epoch: 2/2, step 19832/23838 completed (loss: 4.357834339141846, acc: 0.4444444477558136)
[2025-02-04 00:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:51][root][INFO] - Training Epoch: 2/2, step 19833/23838 completed (loss: 3.3611807823181152, acc: 0.7272727489471436)
[2025-02-04 00:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:51][root][INFO] - Training Epoch: 2/2, step 19834/23838 completed (loss: 4.710068702697754, acc: 0.3333333432674408)
[2025-02-04 00:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:52][root][INFO] - Training Epoch: 2/2, step 19835/23838 completed (loss: 5.539596080780029, acc: 0.3076923191547394)
[2025-02-04 00:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:52][root][INFO] - Training Epoch: 2/2, step 19836/23838 completed (loss: 3.977748155593872, acc: 0.46666666865348816)
[2025-02-04 00:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:52][root][INFO] - Training Epoch: 2/2, step 19837/23838 completed (loss: 2.9642622470855713, acc: 0.37931033968925476)
[2025-02-04 00:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:53][root][INFO] - Training Epoch: 2/2, step 19838/23838 completed (loss: 3.180521249771118, acc: 0.38461539149284363)
[2025-02-04 00:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:53][root][INFO] - Training Epoch: 2/2, step 19839/23838 completed (loss: 2.9170432090759277, acc: 0.4571428596973419)
[2025-02-04 00:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:54][root][INFO] - Training Epoch: 2/2, step 19840/23838 completed (loss: 3.0880839824676514, acc: 0.5)
[2025-02-04 00:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:54][root][INFO] - Training Epoch: 2/2, step 19841/23838 completed (loss: 2.949366331100464, acc: 0.5806451439857483)
[2025-02-04 00:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:55][root][INFO] - Training Epoch: 2/2, step 19842/23838 completed (loss: 3.467498302459717, acc: 0.4146341383457184)
[2025-02-04 00:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:55][root][INFO] - Training Epoch: 2/2, step 19843/23838 completed (loss: 4.773428440093994, acc: 0.25)
[2025-02-04 00:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:56][root][INFO] - Training Epoch: 2/2, step 19844/23838 completed (loss: 2.700242757797241, acc: 0.40909090638160706)
[2025-02-04 00:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:56][root][INFO] - Training Epoch: 2/2, step 19845/23838 completed (loss: 0.6813897490501404, acc: 0.8636363744735718)
[2025-02-04 00:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:57][root][INFO] - Training Epoch: 2/2, step 19846/23838 completed (loss: 1.380763292312622, acc: 0.7352941036224365)
[2025-02-04 00:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:57][root][INFO] - Training Epoch: 2/2, step 19847/23838 completed (loss: 1.2518174648284912, acc: 0.7878788113594055)
[2025-02-04 00:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:57][root][INFO] - Training Epoch: 2/2, step 19848/23838 completed (loss: 2.0710670948028564, acc: 0.5666666626930237)
[2025-02-04 00:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:58][root][INFO] - Training Epoch: 2/2, step 19849/23838 completed (loss: 3.265687942504883, acc: 0.30434781312942505)
[2025-02-04 00:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:58][root][INFO] - Training Epoch: 2/2, step 19850/23838 completed (loss: 2.2357025146484375, acc: 0.5)
[2025-02-04 00:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:59][root][INFO] - Training Epoch: 2/2, step 19851/23838 completed (loss: 2.590353488922119, acc: 0.523809552192688)
[2025-02-04 00:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:47:59][root][INFO] - Training Epoch: 2/2, step 19852/23838 completed (loss: 3.498988389968872, acc: 0.47826087474823)
[2025-02-04 00:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:00][root][INFO] - Training Epoch: 2/2, step 19853/23838 completed (loss: 2.7014739513397217, acc: 0.42105263471603394)
[2025-02-04 00:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:00][root][INFO] - Training Epoch: 2/2, step 19854/23838 completed (loss: 2.681086540222168, acc: 0.46666666865348816)
[2025-02-04 00:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:00][root][INFO] - Training Epoch: 2/2, step 19855/23838 completed (loss: 2.015709638595581, acc: 0.5263158082962036)
[2025-02-04 00:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:01][root][INFO] - Training Epoch: 2/2, step 19856/23838 completed (loss: 2.714428663253784, acc: 0.5789473652839661)
[2025-02-04 00:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:01][root][INFO] - Training Epoch: 2/2, step 19857/23838 completed (loss: 2.7617194652557373, acc: 0.375)
[2025-02-04 00:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:02][root][INFO] - Training Epoch: 2/2, step 19858/23838 completed (loss: 2.767456531524658, acc: 0.3684210479259491)
[2025-02-04 00:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:02][root][INFO] - Training Epoch: 2/2, step 19859/23838 completed (loss: 2.7652814388275146, acc: 0.4444444477558136)
[2025-02-04 00:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:02][root][INFO] - Training Epoch: 2/2, step 19860/23838 completed (loss: 3.024798631668091, acc: 0.5199999809265137)
[2025-02-04 00:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:03][root][INFO] - Training Epoch: 2/2, step 19861/23838 completed (loss: 2.6255288124084473, acc: 0.25)
[2025-02-04 00:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:03][root][INFO] - Training Epoch: 2/2, step 19862/23838 completed (loss: 3.2019407749176025, acc: 0.46666666865348816)
[2025-02-04 00:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:04][root][INFO] - Training Epoch: 2/2, step 19863/23838 completed (loss: 1.8136638402938843, acc: 0.6363636255264282)
[2025-02-04 00:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:04][root][INFO] - Training Epoch: 2/2, step 19864/23838 completed (loss: 4.371509552001953, acc: 0.4000000059604645)
[2025-02-04 00:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:04][root][INFO] - Training Epoch: 2/2, step 19865/23838 completed (loss: 3.2250406742095947, acc: 0.3913043439388275)
[2025-02-04 00:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:05][root][INFO] - Training Epoch: 2/2, step 19866/23838 completed (loss: 3.471917152404785, acc: 0.36000001430511475)
[2025-02-04 00:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:05][root][INFO] - Training Epoch: 2/2, step 19867/23838 completed (loss: 3.6238386631011963, acc: 0.40740740299224854)
[2025-02-04 00:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:06][root][INFO] - Training Epoch: 2/2, step 19868/23838 completed (loss: 4.140988826751709, acc: 0.23529411852359772)
[2025-02-04 00:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:06][root][INFO] - Training Epoch: 2/2, step 19869/23838 completed (loss: 3.2428133487701416, acc: 0.34939759969711304)
[2025-02-04 00:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:07][root][INFO] - Training Epoch: 2/2, step 19870/23838 completed (loss: 3.9121556282043457, acc: 0.375)
[2025-02-04 00:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:07][root][INFO] - Training Epoch: 2/2, step 19871/23838 completed (loss: 3.460963726043701, acc: 0.4000000059604645)
[2025-02-04 00:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:08][root][INFO] - Training Epoch: 2/2, step 19872/23838 completed (loss: 3.1642563343048096, acc: 0.2978723347187042)
[2025-02-04 00:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:08][root][INFO] - Training Epoch: 2/2, step 19873/23838 completed (loss: 3.944781541824341, acc: 0.4000000059604645)
[2025-02-04 00:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:08][root][INFO] - Training Epoch: 2/2, step 19874/23838 completed (loss: 2.96893572807312, acc: 0.42105263471603394)
[2025-02-04 00:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:09][root][INFO] - Training Epoch: 2/2, step 19875/23838 completed (loss: 3.2672555446624756, acc: 0.3076923191547394)
[2025-02-04 00:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:09][root][INFO] - Training Epoch: 2/2, step 19876/23838 completed (loss: 3.5850703716278076, acc: 0.36666667461395264)
[2025-02-04 00:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:10][root][INFO] - Training Epoch: 2/2, step 19877/23838 completed (loss: 2.4912548065185547, acc: 0.4285714328289032)
[2025-02-04 00:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:10][root][INFO] - Training Epoch: 2/2, step 19878/23838 completed (loss: 3.4282586574554443, acc: 0.4150943458080292)
[2025-02-04 00:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:10][root][INFO] - Training Epoch: 2/2, step 19879/23838 completed (loss: 3.416473150253296, acc: 0.3913043439388275)
[2025-02-04 00:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:11][root][INFO] - Training Epoch: 2/2, step 19880/23838 completed (loss: 3.0118749141693115, acc: 0.4571428596973419)
[2025-02-04 00:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:11][root][INFO] - Training Epoch: 2/2, step 19881/23838 completed (loss: 2.8890151977539062, acc: 0.45945945382118225)
[2025-02-04 00:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:12][root][INFO] - Training Epoch: 2/2, step 19882/23838 completed (loss: 3.7515041828155518, acc: 0.28947368264198303)
[2025-02-04 00:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:12][root][INFO] - Training Epoch: 2/2, step 19883/23838 completed (loss: 2.2269980907440186, acc: 0.5405405163764954)
[2025-02-04 00:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:13][root][INFO] - Training Epoch: 2/2, step 19884/23838 completed (loss: 2.83056640625, acc: 0.4047619104385376)
[2025-02-04 00:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:13][root][INFO] - Training Epoch: 2/2, step 19885/23838 completed (loss: 2.5950927734375, acc: 0.4285714328289032)
[2025-02-04 00:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:13][root][INFO] - Training Epoch: 2/2, step 19886/23838 completed (loss: 2.73016357421875, acc: 0.4000000059604645)
[2025-02-04 00:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:14][root][INFO] - Training Epoch: 2/2, step 19887/23838 completed (loss: 3.0762085914611816, acc: 0.37931033968925476)
[2025-02-04 00:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:14][root][INFO] - Training Epoch: 2/2, step 19888/23838 completed (loss: 2.830777406692505, acc: 0.41025641560554504)
[2025-02-04 00:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:15][root][INFO] - Training Epoch: 2/2, step 19889/23838 completed (loss: 2.6324689388275146, acc: 0.4423076808452606)
[2025-02-04 00:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:15][root][INFO] - Training Epoch: 2/2, step 19890/23838 completed (loss: 1.8451839685440063, acc: 0.4285714328289032)
[2025-02-04 00:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:15][root][INFO] - Training Epoch: 2/2, step 19891/23838 completed (loss: 2.5382940769195557, acc: 0.5789473652839661)
[2025-02-04 00:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:16][root][INFO] - Training Epoch: 2/2, step 19892/23838 completed (loss: 2.7050538063049316, acc: 0.5)
[2025-02-04 00:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:16][root][INFO] - Training Epoch: 2/2, step 19893/23838 completed (loss: 3.242644786834717, acc: 0.3414634168148041)
[2025-02-04 00:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:17][root][INFO] - Training Epoch: 2/2, step 19894/23838 completed (loss: 2.632122755050659, acc: 0.3928571343421936)
[2025-02-04 00:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:17][root][INFO] - Training Epoch: 2/2, step 19895/23838 completed (loss: 2.246690511703491, acc: 0.517241358757019)
[2025-02-04 00:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:17][root][INFO] - Training Epoch: 2/2, step 19896/23838 completed (loss: 2.8999977111816406, acc: 0.37837839126586914)
[2025-02-04 00:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:18][root][INFO] - Training Epoch: 2/2, step 19897/23838 completed (loss: 1.4849069118499756, acc: 0.625)
[2025-02-04 00:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:18][root][INFO] - Training Epoch: 2/2, step 19898/23838 completed (loss: 1.8679680824279785, acc: 0.5263158082962036)
[2025-02-04 00:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:19][root][INFO] - Training Epoch: 2/2, step 19899/23838 completed (loss: 3.0205137729644775, acc: 0.4000000059604645)
[2025-02-04 00:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:19][root][INFO] - Training Epoch: 2/2, step 19900/23838 completed (loss: 2.102567672729492, acc: 0.5306122303009033)
[2025-02-04 00:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:20][root][INFO] - Training Epoch: 2/2, step 19901/23838 completed (loss: 1.6018034219741821, acc: 0.4117647111415863)
[2025-02-04 00:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:20][root][INFO] - Training Epoch: 2/2, step 19902/23838 completed (loss: 2.4790844917297363, acc: 0.574999988079071)
[2025-02-04 00:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:20][root][INFO] - Training Epoch: 2/2, step 19903/23838 completed (loss: 1.0146510601043701, acc: 0.6875)
[2025-02-04 00:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:21][root][INFO] - Training Epoch: 2/2, step 19904/23838 completed (loss: 2.1443984508514404, acc: 0.4615384638309479)
[2025-02-04 00:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:21][root][INFO] - Training Epoch: 2/2, step 19905/23838 completed (loss: 2.906778335571289, acc: 0.43589743971824646)
[2025-02-04 00:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:22][root][INFO] - Training Epoch: 2/2, step 19906/23838 completed (loss: 2.4634478092193604, acc: 0.5111111402511597)
[2025-02-04 00:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:22][root][INFO] - Training Epoch: 2/2, step 19907/23838 completed (loss: 2.945650815963745, acc: 0.3333333432674408)
[2025-02-04 00:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:23][root][INFO] - Training Epoch: 2/2, step 19908/23838 completed (loss: 2.6639530658721924, acc: 0.5)
[2025-02-04 00:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:23][root][INFO] - Training Epoch: 2/2, step 19909/23838 completed (loss: 0.916096568107605, acc: 0.8125)
[2025-02-04 00:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:24][root][INFO] - Training Epoch: 2/2, step 19910/23838 completed (loss: 1.021190881729126, acc: 0.738095223903656)
[2025-02-04 00:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:24][root][INFO] - Training Epoch: 2/2, step 19911/23838 completed (loss: 2.4938690662384033, acc: 0.5384615659713745)
[2025-02-04 00:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:24][root][INFO] - Training Epoch: 2/2, step 19912/23838 completed (loss: 2.099571704864502, acc: 0.6666666865348816)
[2025-02-04 00:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:25][root][INFO] - Training Epoch: 2/2, step 19913/23838 completed (loss: 3.356105089187622, acc: 0.42105263471603394)
[2025-02-04 00:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:25][root][INFO] - Training Epoch: 2/2, step 19914/23838 completed (loss: 3.6327314376831055, acc: 0.4117647111415863)
[2025-02-04 00:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:26][root][INFO] - Training Epoch: 2/2, step 19915/23838 completed (loss: 3.3399200439453125, acc: 0.39024388790130615)
[2025-02-04 00:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:26][root][INFO] - Training Epoch: 2/2, step 19916/23838 completed (loss: 4.327072620391846, acc: 0.2888889014720917)
[2025-02-04 00:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:26][root][INFO] - Training Epoch: 2/2, step 19917/23838 completed (loss: 3.5765938758850098, acc: 0.47999998927116394)
[2025-02-04 00:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:27][root][INFO] - Training Epoch: 2/2, step 19918/23838 completed (loss: 1.532838225364685, acc: 0.7222222089767456)
[2025-02-04 00:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:27][root][INFO] - Training Epoch: 2/2, step 19919/23838 completed (loss: 4.128562927246094, acc: 0.29629629850387573)
[2025-02-04 00:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:28][root][INFO] - Training Epoch: 2/2, step 19920/23838 completed (loss: 4.1716156005859375, acc: 0.23529411852359772)
[2025-02-04 00:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:28][root][INFO] - Training Epoch: 2/2, step 19921/23838 completed (loss: 3.978895664215088, acc: 0.3499999940395355)
[2025-02-04 00:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:28][root][INFO] - Training Epoch: 2/2, step 19922/23838 completed (loss: 3.9541256427764893, acc: 0.3055555522441864)
[2025-02-04 00:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:29][root][INFO] - Training Epoch: 2/2, step 19923/23838 completed (loss: 3.66806697845459, acc: 0.32758620381355286)
[2025-02-04 00:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:29][root][INFO] - Training Epoch: 2/2, step 19924/23838 completed (loss: 4.000824928283691, acc: 0.26829269528388977)
[2025-02-04 00:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:30][root][INFO] - Training Epoch: 2/2, step 19925/23838 completed (loss: 3.1537415981292725, acc: 0.43478259444236755)
[2025-02-04 00:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:30][root][INFO] - Training Epoch: 2/2, step 19926/23838 completed (loss: 5.065165042877197, acc: 0.4000000059604645)
[2025-02-04 00:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:30][root][INFO] - Training Epoch: 2/2, step 19927/23838 completed (loss: 2.065796375274658, acc: 0.5600000023841858)
[2025-02-04 00:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:31][root][INFO] - Training Epoch: 2/2, step 19928/23838 completed (loss: 2.6005916595458984, acc: 0.3928571343421936)
[2025-02-04 00:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:31][root][INFO] - Training Epoch: 2/2, step 19929/23838 completed (loss: 3.426509141921997, acc: 0.3414634168148041)
[2025-02-04 00:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:32][root][INFO] - Training Epoch: 2/2, step 19930/23838 completed (loss: 3.377173900604248, acc: 0.2857142984867096)
[2025-02-04 00:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:32][root][INFO] - Training Epoch: 2/2, step 19931/23838 completed (loss: 1.7563254833221436, acc: 0.5882353186607361)
[2025-02-04 00:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:33][root][INFO] - Training Epoch: 2/2, step 19932/23838 completed (loss: 3.013909339904785, acc: 0.3333333432674408)
[2025-02-04 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:33][root][INFO] - Training Epoch: 2/2, step 19933/23838 completed (loss: 2.6438074111938477, acc: 0.39534884691238403)
[2025-02-04 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:33][root][INFO] - Training Epoch: 2/2, step 19934/23838 completed (loss: 2.681396722793579, acc: 0.4333333373069763)
[2025-02-04 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:34][root][INFO] - Training Epoch: 2/2, step 19935/23838 completed (loss: 3.4971249103546143, acc: 0.2631579041481018)
[2025-02-04 00:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:34][root][INFO] - Training Epoch: 2/2, step 19936/23838 completed (loss: 2.7474308013916016, acc: 0.44999998807907104)
[2025-02-04 00:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:35][root][INFO] - Training Epoch: 2/2, step 19937/23838 completed (loss: 2.9124982357025146, acc: 0.2857142984867096)
[2025-02-04 00:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:35][root][INFO] - Training Epoch: 2/2, step 19938/23838 completed (loss: 1.760260820388794, acc: 0.5666666626930237)
[2025-02-04 00:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:35][root][INFO] - Training Epoch: 2/2, step 19939/23838 completed (loss: 2.407726764678955, acc: 0.5151515007019043)
[2025-02-04 00:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:36][root][INFO] - Training Epoch: 2/2, step 19940/23838 completed (loss: 2.521097421646118, acc: 0.3461538553237915)
[2025-02-04 00:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:36][root][INFO] - Training Epoch: 2/2, step 19941/23838 completed (loss: 2.907532215118408, acc: 0.4871794879436493)
[2025-02-04 00:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:37][root][INFO] - Training Epoch: 2/2, step 19942/23838 completed (loss: 3.063372850418091, acc: 0.3777777850627899)
[2025-02-04 00:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:37][root][INFO] - Training Epoch: 2/2, step 19943/23838 completed (loss: 3.3226099014282227, acc: 0.3488371968269348)
[2025-02-04 00:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:37][root][INFO] - Training Epoch: 2/2, step 19944/23838 completed (loss: 2.6511662006378174, acc: 0.41860464215278625)
[2025-02-04 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:38][root][INFO] - Training Epoch: 2/2, step 19945/23838 completed (loss: 2.495638847351074, acc: 0.5208333134651184)
[2025-02-04 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:38][root][INFO] - Training Epoch: 2/2, step 19946/23838 completed (loss: 2.4478282928466797, acc: 0.4545454680919647)
[2025-02-04 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:39][root][INFO] - Training Epoch: 2/2, step 19947/23838 completed (loss: 3.388472557067871, acc: 0.37254902720451355)
[2025-02-04 00:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:39][root][INFO] - Training Epoch: 2/2, step 19948/23838 completed (loss: 2.425222158432007, acc: 0.40625)
[2025-02-04 00:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:40][root][INFO] - Training Epoch: 2/2, step 19949/23838 completed (loss: 3.2499566078186035, acc: 0.3589743673801422)
[2025-02-04 00:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:40][root][INFO] - Training Epoch: 2/2, step 19950/23838 completed (loss: 2.7664380073547363, acc: 0.5384615659713745)
[2025-02-04 00:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:40][root][INFO] - Training Epoch: 2/2, step 19951/23838 completed (loss: 2.233383893966675, acc: 0.5161290168762207)
[2025-02-04 00:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:41][root][INFO] - Training Epoch: 2/2, step 19952/23838 completed (loss: 0.6255360245704651, acc: 0.8235294222831726)
[2025-02-04 00:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:41][root][INFO] - Training Epoch: 2/2, step 19953/23838 completed (loss: 1.3841136693954468, acc: 0.6666666865348816)
[2025-02-04 00:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:42][root][INFO] - Training Epoch: 2/2, step 19954/23838 completed (loss: 2.8214590549468994, acc: 0.6000000238418579)
[2025-02-04 00:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:42][root][INFO] - Training Epoch: 2/2, step 19955/23838 completed (loss: 2.1558947563171387, acc: 0.6071428656578064)
[2025-02-04 00:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:43][root][INFO] - Training Epoch: 2/2, step 19956/23838 completed (loss: 3.0915982723236084, acc: 0.3636363744735718)
[2025-02-04 00:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:43][root][INFO] - Training Epoch: 2/2, step 19957/23838 completed (loss: 2.1402344703674316, acc: 0.6000000238418579)
[2025-02-04 00:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:44][root][INFO] - Training Epoch: 2/2, step 19958/23838 completed (loss: 3.1991071701049805, acc: 0.523809552192688)
[2025-02-04 00:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:44][root][INFO] - Training Epoch: 2/2, step 19959/23838 completed (loss: 2.63568115234375, acc: 0.47058823704719543)
[2025-02-04 00:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:44][root][INFO] - Training Epoch: 2/2, step 19960/23838 completed (loss: 2.8060357570648193, acc: 0.4545454680919647)
[2025-02-04 00:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:45][root][INFO] - Training Epoch: 2/2, step 19961/23838 completed (loss: 4.9989776611328125, acc: 0.3030303120613098)
[2025-02-04 00:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:45][root][INFO] - Training Epoch: 2/2, step 19962/23838 completed (loss: 3.846127510070801, acc: 0.4166666567325592)
[2025-02-04 00:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:46][root][INFO] - Training Epoch: 2/2, step 19963/23838 completed (loss: 3.2434768676757812, acc: 0.4193548262119293)
[2025-02-04 00:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:46][root][INFO] - Training Epoch: 2/2, step 19964/23838 completed (loss: 4.6856303215026855, acc: 0.2857142984867096)
[2025-02-04 00:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:47][root][INFO] - Training Epoch: 2/2, step 19965/23838 completed (loss: 4.391329288482666, acc: 0.46666666865348816)
[2025-02-04 00:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:47][root][INFO] - Training Epoch: 2/2, step 19966/23838 completed (loss: 4.9735212326049805, acc: 0.3214285671710968)
[2025-02-04 00:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:47][root][INFO] - Training Epoch: 2/2, step 19967/23838 completed (loss: 3.0288023948669434, acc: 0.40625)
[2025-02-04 00:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:48][root][INFO] - Training Epoch: 2/2, step 19968/23838 completed (loss: 3.88384747505188, acc: 0.28205129504203796)
[2025-02-04 00:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:48][root][INFO] - Training Epoch: 2/2, step 19969/23838 completed (loss: 5.038127422332764, acc: 0.16129031777381897)
[2025-02-04 00:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:49][root][INFO] - Training Epoch: 2/2, step 19970/23838 completed (loss: 3.2814388275146484, acc: 0.44117647409439087)
[2025-02-04 00:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:49][root][INFO] - Training Epoch: 2/2, step 19971/23838 completed (loss: 3.7601332664489746, acc: 0.32258063554763794)
[2025-02-04 00:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:50][root][INFO] - Training Epoch: 2/2, step 19972/23838 completed (loss: 3.191624641418457, acc: 0.41025641560554504)
[2025-02-04 00:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:50][root][INFO] - Training Epoch: 2/2, step 19973/23838 completed (loss: 5.160785675048828, acc: 0.1666666716337204)
[2025-02-04 00:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:51][root][INFO] - Training Epoch: 2/2, step 19974/23838 completed (loss: 4.200261116027832, acc: 0.3255814015865326)
[2025-02-04 00:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:51][root][INFO] - Training Epoch: 2/2, step 19975/23838 completed (loss: 2.886500835418701, acc: 0.5)
[2025-02-04 00:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:51][root][INFO] - Training Epoch: 2/2, step 19976/23838 completed (loss: 3.3266117572784424, acc: 0.4615384638309479)
[2025-02-04 00:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:52][root][INFO] - Training Epoch: 2/2, step 19977/23838 completed (loss: 3.793353796005249, acc: 0.31578946113586426)
[2025-02-04 00:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:52][root][INFO] - Training Epoch: 2/2, step 19978/23838 completed (loss: 3.9532663822174072, acc: 0.2380952388048172)
[2025-02-04 00:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:53][root][INFO] - Training Epoch: 2/2, step 19979/23838 completed (loss: 2.7047061920166016, acc: 0.4285714328289032)
[2025-02-04 00:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:53][root][INFO] - Training Epoch: 2/2, step 19980/23838 completed (loss: 3.5535333156585693, acc: 0.3529411852359772)
[2025-02-04 00:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:54][root][INFO] - Training Epoch: 2/2, step 19981/23838 completed (loss: 3.35923171043396, acc: 0.31578946113586426)
[2025-02-04 00:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:54][root][INFO] - Training Epoch: 2/2, step 19982/23838 completed (loss: 3.5989632606506348, acc: 0.3333333432674408)
[2025-02-04 00:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:55][root][INFO] - Training Epoch: 2/2, step 19983/23838 completed (loss: 4.612475872039795, acc: 0.17391304671764374)
[2025-02-04 00:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:55][root][INFO] - Training Epoch: 2/2, step 19984/23838 completed (loss: 3.7898080348968506, acc: 0.3243243098258972)
[2025-02-04 00:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:56][root][INFO] - Training Epoch: 2/2, step 19985/23838 completed (loss: 3.846491813659668, acc: 0.27272728085517883)
[2025-02-04 00:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:56][root][INFO] - Training Epoch: 2/2, step 19986/23838 completed (loss: 3.1341936588287354, acc: 0.4516128897666931)
[2025-02-04 00:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:57][root][INFO] - Training Epoch: 2/2, step 19987/23838 completed (loss: 2.4925472736358643, acc: 0.625)
[2025-02-04 00:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:57][root][INFO] - Training Epoch: 2/2, step 19988/23838 completed (loss: 2.6119465827941895, acc: 0.48275861144065857)
[2025-02-04 00:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:58][root][INFO] - Training Epoch: 2/2, step 19989/23838 completed (loss: 4.136448383331299, acc: 0.3095238208770752)
[2025-02-04 00:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:58][root][INFO] - Training Epoch: 2/2, step 19990/23838 completed (loss: 2.9560604095458984, acc: 0.4146341383457184)
[2025-02-04 00:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:59][root][INFO] - Training Epoch: 2/2, step 19991/23838 completed (loss: 4.453952312469482, acc: 0.27659574151039124)
[2025-02-04 00:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:48:59][root][INFO] - Training Epoch: 2/2, step 19992/23838 completed (loss: 3.625372886657715, acc: 0.3513513505458832)
[2025-02-04 00:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:00][root][INFO] - Training Epoch: 2/2, step 19993/23838 completed (loss: 3.6202378273010254, acc: 0.3913043439388275)
[2025-02-04 00:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:00][root][INFO] - Training Epoch: 2/2, step 19994/23838 completed (loss: 2.2902615070343018, acc: 0.6086956262588501)
[2025-02-04 00:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:00][root][INFO] - Training Epoch: 2/2, step 19995/23838 completed (loss: 3.673783302307129, acc: 0.3799999952316284)
[2025-02-04 00:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:01][root][INFO] - Training Epoch: 2/2, step 19996/23838 completed (loss: 3.17128324508667, acc: 0.4375)
[2025-02-04 00:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:01][root][INFO] - Training Epoch: 2/2, step 19997/23838 completed (loss: 3.62007999420166, acc: 0.22727273404598236)
[2025-02-04 00:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:02][root][INFO] - Training Epoch: 2/2, step 19998/23838 completed (loss: 3.0829570293426514, acc: 0.3913043439388275)
[2025-02-04 00:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:02][root][INFO] - Training Epoch: 2/2, step 19999/23838 completed (loss: 2.946697473526001, acc: 0.40909090638160706)
[2025-02-04 00:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:03][root][INFO] - Training Epoch: 2/2, step 20000/23838 completed (loss: 3.7195234298706055, acc: 0.3333333432674408)
[2025-02-04 00:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:03][root][INFO] - Training Epoch: 2/2, step 20001/23838 completed (loss: 3.6322243213653564, acc: 0.3499999940395355)
[2025-02-04 00:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:03][root][INFO] - Training Epoch: 2/2, step 20002/23838 completed (loss: 2.0945675373077393, acc: 0.5333333611488342)
[2025-02-04 00:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:04][root][INFO] - Training Epoch: 2/2, step 20003/23838 completed (loss: 2.688117265701294, acc: 0.52173912525177)
[2025-02-04 00:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:04][root][INFO] - Training Epoch: 2/2, step 20004/23838 completed (loss: 0.5669037103652954, acc: 0.8636363744735718)
[2025-02-04 00:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:05][root][INFO] - Training Epoch: 2/2, step 20005/23838 completed (loss: 2.8769922256469727, acc: 0.42307692766189575)
[2025-02-04 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:05][root][INFO] - Training Epoch: 2/2, step 20006/23838 completed (loss: 2.193472146987915, acc: 0.6315789222717285)
[2025-02-04 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:06][root][INFO] - Training Epoch: 2/2, step 20007/23838 completed (loss: 4.2628583908081055, acc: 0.32499998807907104)
[2025-02-04 00:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:06][root][INFO] - Training Epoch: 2/2, step 20008/23838 completed (loss: 4.858768939971924, acc: 0.17073170840740204)
[2025-02-04 00:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:07][root][INFO] - Training Epoch: 2/2, step 20009/23838 completed (loss: 4.39927339553833, acc: 0.2857142984867096)
[2025-02-04 00:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:07][root][INFO] - Training Epoch: 2/2, step 20010/23838 completed (loss: 3.7774605751037598, acc: 0.3913043439388275)
[2025-02-04 00:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:08][root][INFO] - Training Epoch: 2/2, step 20011/23838 completed (loss: 2.7176895141601562, acc: 0.45945945382118225)
[2025-02-04 00:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:08][root][INFO] - Training Epoch: 2/2, step 20012/23838 completed (loss: 3.65338134765625, acc: 0.3636363744735718)
[2025-02-04 00:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:08][root][INFO] - Training Epoch: 2/2, step 20013/23838 completed (loss: 1.519503116607666, acc: 0.5600000023841858)
[2025-02-04 00:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:09][root][INFO] - Training Epoch: 2/2, step 20014/23838 completed (loss: 2.328500270843506, acc: 0.5319148898124695)
[2025-02-04 00:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:09][root][INFO] - Training Epoch: 2/2, step 20015/23838 completed (loss: 2.2158737182617188, acc: 0.5526315569877625)
[2025-02-04 00:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:10][root][INFO] - Training Epoch: 2/2, step 20016/23838 completed (loss: 3.220003604888916, acc: 0.3055555522441864)
[2025-02-04 00:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:10][root][INFO] - Training Epoch: 2/2, step 20017/23838 completed (loss: 3.020296812057495, acc: 0.3947368562221527)
[2025-02-04 00:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:10][root][INFO] - Training Epoch: 2/2, step 20018/23838 completed (loss: 3.2053375244140625, acc: 0.48076921701431274)
[2025-02-04 00:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:11][root][INFO] - Training Epoch: 2/2, step 20019/23838 completed (loss: 1.0467630624771118, acc: 0.7272727489471436)
[2025-02-04 00:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:11][root][INFO] - Training Epoch: 2/2, step 20020/23838 completed (loss: 1.5420863628387451, acc: 0.717391312122345)
[2025-02-04 00:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:12][root][INFO] - Training Epoch: 2/2, step 20021/23838 completed (loss: 1.8385858535766602, acc: 0.5483871102333069)
[2025-02-04 00:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:12][root][INFO] - Training Epoch: 2/2, step 20022/23838 completed (loss: 1.9659366607666016, acc: 0.5833333134651184)
[2025-02-04 00:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:13][root][INFO] - Training Epoch: 2/2, step 20023/23838 completed (loss: 3.477907657623291, acc: 0.5769230723381042)
[2025-02-04 00:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:13][root][INFO] - Training Epoch: 2/2, step 20024/23838 completed (loss: 3.9755194187164307, acc: 0.5555555820465088)
[2025-02-04 00:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:13][root][INFO] - Training Epoch: 2/2, step 20025/23838 completed (loss: 3.5846104621887207, acc: 0.5)
[2025-02-04 00:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:14][root][INFO] - Training Epoch: 2/2, step 20026/23838 completed (loss: 4.776686191558838, acc: 0.375)
[2025-02-04 00:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:14][root][INFO] - Training Epoch: 2/2, step 20027/23838 completed (loss: 4.978115558624268, acc: 0.4444444477558136)
[2025-02-04 00:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:15][root][INFO] - Training Epoch: 2/2, step 20028/23838 completed (loss: 4.98496675491333, acc: 0.4444444477558136)
[2025-02-04 00:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:15][root][INFO] - Training Epoch: 2/2, step 20029/23838 completed (loss: 4.116598129272461, acc: 0.4000000059604645)
[2025-02-04 00:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:16][root][INFO] - Training Epoch: 2/2, step 20030/23838 completed (loss: 4.194716930389404, acc: 0.3636363744735718)
[2025-02-04 00:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:16][root][INFO] - Training Epoch: 2/2, step 20031/23838 completed (loss: 4.299998760223389, acc: 0.4444444477558136)
[2025-02-04 00:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:16][root][INFO] - Training Epoch: 2/2, step 20032/23838 completed (loss: 5.174071311950684, acc: 0.4000000059604645)
[2025-02-04 00:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:17][root][INFO] - Training Epoch: 2/2, step 20033/23838 completed (loss: 3.83013653755188, acc: 0.4000000059604645)
[2025-02-04 00:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:17][root][INFO] - Training Epoch: 2/2, step 20034/23838 completed (loss: 5.349287509918213, acc: 0.4000000059604645)
[2025-02-04 00:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:18][root][INFO] - Training Epoch: 2/2, step 20035/23838 completed (loss: 4.054906845092773, acc: 0.4444444477558136)
[2025-02-04 00:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:18][root][INFO] - Training Epoch: 2/2, step 20036/23838 completed (loss: 4.68599271774292, acc: 0.4000000059604645)
[2025-02-04 00:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:18][root][INFO] - Training Epoch: 2/2, step 20037/23838 completed (loss: 4.742804050445557, acc: 0.4000000059604645)
[2025-02-04 00:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:19][root][INFO] - Training Epoch: 2/2, step 20038/23838 completed (loss: 4.570806503295898, acc: 0.5)
[2025-02-04 00:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:19][root][INFO] - Training Epoch: 2/2, step 20039/23838 completed (loss: 4.209671974182129, acc: 0.4444444477558136)
[2025-02-04 00:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:20][root][INFO] - Training Epoch: 2/2, step 20040/23838 completed (loss: 5.086008071899414, acc: 0.4444444477558136)
[2025-02-04 00:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:20][root][INFO] - Training Epoch: 2/2, step 20041/23838 completed (loss: 4.296032428741455, acc: 0.25925925374031067)
[2025-02-04 00:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:20][root][INFO] - Training Epoch: 2/2, step 20042/23838 completed (loss: 3.2784671783447266, acc: 0.2857142984867096)
[2025-02-04 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:21][root][INFO] - Training Epoch: 2/2, step 20043/23838 completed (loss: 4.676537990570068, acc: 0.18000000715255737)
[2025-02-04 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:21][root][INFO] - Training Epoch: 2/2, step 20044/23838 completed (loss: 4.398575782775879, acc: 0.28125)
[2025-02-04 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:22][root][INFO] - Training Epoch: 2/2, step 20045/23838 completed (loss: 3.809563159942627, acc: 0.3684210479259491)
[2025-02-04 00:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:22][root][INFO] - Training Epoch: 2/2, step 20046/23838 completed (loss: 3.1362075805664062, acc: 0.4000000059604645)
[2025-02-04 00:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:23][root][INFO] - Training Epoch: 2/2, step 20047/23838 completed (loss: 3.6265804767608643, acc: 0.2678571343421936)
[2025-02-04 00:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:23][root][INFO] - Training Epoch: 2/2, step 20048/23838 completed (loss: 3.05269193649292, acc: 0.39393940567970276)
[2025-02-04 00:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:23][root][INFO] - Training Epoch: 2/2, step 20049/23838 completed (loss: 2.30264949798584, acc: 0.5806451439857483)
[2025-02-04 00:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:24][root][INFO] - Training Epoch: 2/2, step 20050/23838 completed (loss: 3.8294498920440674, acc: 0.3333333432674408)
[2025-02-04 00:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:24][root][INFO] - Training Epoch: 2/2, step 20051/23838 completed (loss: 4.491155624389648, acc: 0.23728813230991364)
[2025-02-04 00:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:25][root][INFO] - Training Epoch: 2/2, step 20052/23838 completed (loss: 2.8415298461914062, acc: 0.3333333432674408)
[2025-02-04 00:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:25][root][INFO] - Training Epoch: 2/2, step 20053/23838 completed (loss: 3.272078514099121, acc: 0.4117647111415863)
[2025-02-04 00:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:25][root][INFO] - Training Epoch: 2/2, step 20054/23838 completed (loss: 3.6801323890686035, acc: 0.3617021143436432)
[2025-02-04 00:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:26][root][INFO] - Training Epoch: 2/2, step 20055/23838 completed (loss: 5.2593889236450195, acc: 0.2926829159259796)
[2025-02-04 00:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:26][root][INFO] - Training Epoch: 2/2, step 20056/23838 completed (loss: 3.426487684249878, acc: 0.3958333432674408)
[2025-02-04 00:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:27][root][INFO] - Training Epoch: 2/2, step 20057/23838 completed (loss: 3.32485294342041, acc: 0.4375)
[2025-02-04 00:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:27][root][INFO] - Training Epoch: 2/2, step 20058/23838 completed (loss: 3.887155294418335, acc: 0.36538460850715637)
[2025-02-04 00:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:28][root][INFO] - Training Epoch: 2/2, step 20059/23838 completed (loss: 3.5810487270355225, acc: 0.3333333432674408)
[2025-02-04 00:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:28][root][INFO] - Training Epoch: 2/2, step 20060/23838 completed (loss: 2.9950807094573975, acc: 0.34090909361839294)
[2025-02-04 00:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:28][root][INFO] - Training Epoch: 2/2, step 20061/23838 completed (loss: 2.8033671379089355, acc: 0.4615384638309479)
[2025-02-04 00:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:29][root][INFO] - Training Epoch: 2/2, step 20062/23838 completed (loss: 3.2773067951202393, acc: 0.4363636374473572)
[2025-02-04 00:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:29][root][INFO] - Training Epoch: 2/2, step 20063/23838 completed (loss: 3.0724945068359375, acc: 0.4444444477558136)
[2025-02-04 00:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:30][root][INFO] - Training Epoch: 2/2, step 20064/23838 completed (loss: 3.591810703277588, acc: 0.3488371968269348)
[2025-02-04 00:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:30][root][INFO] - Training Epoch: 2/2, step 20065/23838 completed (loss: 3.2213847637176514, acc: 0.40625)
[2025-02-04 00:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:30][root][INFO] - Training Epoch: 2/2, step 20066/23838 completed (loss: 3.6764254570007324, acc: 0.37254902720451355)
[2025-02-04 00:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:31][root][INFO] - Training Epoch: 2/2, step 20067/23838 completed (loss: 2.6732308864593506, acc: 0.4761904776096344)
[2025-02-04 00:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:31][root][INFO] - Training Epoch: 2/2, step 20068/23838 completed (loss: 2.6198012828826904, acc: 0.4444444477558136)
[2025-02-04 00:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:32][root][INFO] - Training Epoch: 2/2, step 20069/23838 completed (loss: 3.672377347946167, acc: 0.34285715222358704)
[2025-02-04 00:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:32][root][INFO] - Training Epoch: 2/2, step 20070/23838 completed (loss: 3.2369720935821533, acc: 0.4000000059604645)
[2025-02-04 00:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:32][root][INFO] - Training Epoch: 2/2, step 20071/23838 completed (loss: 2.818188428878784, acc: 0.4000000059604645)
[2025-02-04 00:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:33][root][INFO] - Training Epoch: 2/2, step 20072/23838 completed (loss: 2.7339766025543213, acc: 0.3684210479259491)
[2025-02-04 00:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:33][root][INFO] - Training Epoch: 2/2, step 20073/23838 completed (loss: 3.1355323791503906, acc: 0.3888888955116272)
[2025-02-04 00:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:34][root][INFO] - Training Epoch: 2/2, step 20074/23838 completed (loss: 3.8491573333740234, acc: 0.2777777910232544)
[2025-02-04 00:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:34][root][INFO] - Training Epoch: 2/2, step 20075/23838 completed (loss: 1.7971705198287964, acc: 0.5789473652839661)
[2025-02-04 00:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:35][root][INFO] - Training Epoch: 2/2, step 20076/23838 completed (loss: 4.415422439575195, acc: 0.2857142984867096)
[2025-02-04 00:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:35][root][INFO] - Training Epoch: 2/2, step 20077/23838 completed (loss: 2.2516350746154785, acc: 0.6499999761581421)
[2025-02-04 00:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:35][root][INFO] - Training Epoch: 2/2, step 20078/23838 completed (loss: 3.726080894470215, acc: 0.22641509771347046)
[2025-02-04 00:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:36][root][INFO] - Training Epoch: 2/2, step 20079/23838 completed (loss: 3.9217681884765625, acc: 0.30434781312942505)
[2025-02-04 00:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:36][root][INFO] - Training Epoch: 2/2, step 20080/23838 completed (loss: 3.516146421432495, acc: 0.25)
[2025-02-04 00:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:37][root][INFO] - Training Epoch: 2/2, step 20081/23838 completed (loss: 3.941272020339966, acc: 0.2199999988079071)
[2025-02-04 00:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:37][root][INFO] - Training Epoch: 2/2, step 20082/23838 completed (loss: 3.4254839420318604, acc: 0.29629629850387573)
[2025-02-04 00:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:37][root][INFO] - Training Epoch: 2/2, step 20083/23838 completed (loss: 3.7404158115386963, acc: 0.3913043439388275)
[2025-02-04 00:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:38][root][INFO] - Training Epoch: 2/2, step 20084/23838 completed (loss: 3.0958733558654785, acc: 0.4838709533214569)
[2025-02-04 00:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:38][root][INFO] - Training Epoch: 2/2, step 20085/23838 completed (loss: 3.5160815715789795, acc: 0.3333333432674408)
[2025-02-04 00:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:39][root][INFO] - Training Epoch: 2/2, step 20086/23838 completed (loss: 2.5260565280914307, acc: 0.4375)
[2025-02-04 00:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:39][root][INFO] - Training Epoch: 2/2, step 20087/23838 completed (loss: 4.253114700317383, acc: 0.2448979616165161)
[2025-02-04 00:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:40][root][INFO] - Training Epoch: 2/2, step 20088/23838 completed (loss: 3.9690380096435547, acc: 0.24074074625968933)
[2025-02-04 00:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:40][root][INFO] - Training Epoch: 2/2, step 20089/23838 completed (loss: 3.850966215133667, acc: 0.2978723347187042)
[2025-02-04 00:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:40][root][INFO] - Training Epoch: 2/2, step 20090/23838 completed (loss: 3.842533826828003, acc: 0.23404255509376526)
[2025-02-04 00:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:41][root][INFO] - Training Epoch: 2/2, step 20091/23838 completed (loss: 3.2045552730560303, acc: 0.31111112236976624)
[2025-02-04 00:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:41][root][INFO] - Training Epoch: 2/2, step 20092/23838 completed (loss: 3.691570281982422, acc: 0.25641027092933655)
[2025-02-04 00:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:42][root][INFO] - Training Epoch: 2/2, step 20093/23838 completed (loss: 3.6562678813934326, acc: 0.25925925374031067)
[2025-02-04 00:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:42][root][INFO] - Training Epoch: 2/2, step 20094/23838 completed (loss: 3.175990581512451, acc: 0.48571428656578064)
[2025-02-04 00:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:43][root][INFO] - Training Epoch: 2/2, step 20095/23838 completed (loss: 3.7278761863708496, acc: 0.380952388048172)
[2025-02-04 00:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:43][root][INFO] - Training Epoch: 2/2, step 20096/23838 completed (loss: 2.730052947998047, acc: 0.4324324429035187)
[2025-02-04 00:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:43][root][INFO] - Training Epoch: 2/2, step 20097/23838 completed (loss: 3.585467576980591, acc: 0.4000000059604645)
[2025-02-04 00:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:44][root][INFO] - Training Epoch: 2/2, step 20098/23838 completed (loss: 3.3706912994384766, acc: 0.3777777850627899)
[2025-02-04 00:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:44][root][INFO] - Training Epoch: 2/2, step 20099/23838 completed (loss: 3.620204448699951, acc: 0.38461539149284363)
[2025-02-04 00:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:45][root][INFO] - Training Epoch: 2/2, step 20100/23838 completed (loss: 3.825838327407837, acc: 0.3870967626571655)
[2025-02-04 00:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:45][root][INFO] - Training Epoch: 2/2, step 20101/23838 completed (loss: 3.338883399963379, acc: 0.3636363744735718)
[2025-02-04 00:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:45][root][INFO] - Training Epoch: 2/2, step 20102/23838 completed (loss: 2.026179075241089, acc: 0.6000000238418579)
[2025-02-04 00:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:46][root][INFO] - Training Epoch: 2/2, step 20103/23838 completed (loss: 3.6182568073272705, acc: 0.3958333432674408)
[2025-02-04 00:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:46][root][INFO] - Training Epoch: 2/2, step 20104/23838 completed (loss: 2.5601344108581543, acc: 0.3870967626571655)
[2025-02-04 00:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:47][root][INFO] - Training Epoch: 2/2, step 20105/23838 completed (loss: 3.3407816886901855, acc: 0.4285714328289032)
[2025-02-04 00:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:47][root][INFO] - Training Epoch: 2/2, step 20106/23838 completed (loss: 4.181906700134277, acc: 0.29411765933036804)
[2025-02-04 00:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:48][root][INFO] - Training Epoch: 2/2, step 20107/23838 completed (loss: 3.627549409866333, acc: 0.39024388790130615)
[2025-02-04 00:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:48][root][INFO] - Training Epoch: 2/2, step 20108/23838 completed (loss: 3.7342870235443115, acc: 0.4047619104385376)
[2025-02-04 00:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:49][root][INFO] - Training Epoch: 2/2, step 20109/23838 completed (loss: 3.3902599811553955, acc: 0.3142857253551483)
[2025-02-04 00:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:49][root][INFO] - Training Epoch: 2/2, step 20110/23838 completed (loss: 3.179769515991211, acc: 0.3469387888908386)
[2025-02-04 00:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:49][root][INFO] - Training Epoch: 2/2, step 20111/23838 completed (loss: 2.3783116340637207, acc: 0.4444444477558136)
[2025-02-04 00:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:50][root][INFO] - Training Epoch: 2/2, step 20112/23838 completed (loss: 3.1714065074920654, acc: 0.3684210479259491)
[2025-02-04 00:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:50][root][INFO] - Training Epoch: 2/2, step 20113/23838 completed (loss: 2.6312272548675537, acc: 0.5714285969734192)
[2025-02-04 00:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:51][root][INFO] - Training Epoch: 2/2, step 20114/23838 completed (loss: 3.153163194656372, acc: 0.3863636255264282)
[2025-02-04 00:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:51][root][INFO] - Training Epoch: 2/2, step 20115/23838 completed (loss: 3.6068739891052246, acc: 0.3255814015865326)
[2025-02-04 00:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:51][root][INFO] - Training Epoch: 2/2, step 20116/23838 completed (loss: 3.4144434928894043, acc: 0.3076923191547394)
[2025-02-04 00:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:52][root][INFO] - Training Epoch: 2/2, step 20117/23838 completed (loss: 2.5104899406433105, acc: 0.48275861144065857)
[2025-02-04 00:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:52][root][INFO] - Training Epoch: 2/2, step 20118/23838 completed (loss: 2.6572248935699463, acc: 0.5263158082962036)
[2025-02-04 00:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:53][root][INFO] - Training Epoch: 2/2, step 20119/23838 completed (loss: 3.502230167388916, acc: 0.3928571343421936)
[2025-02-04 00:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:53][root][INFO] - Training Epoch: 2/2, step 20120/23838 completed (loss: 3.7324981689453125, acc: 0.23999999463558197)
[2025-02-04 00:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:54][root][INFO] - Training Epoch: 2/2, step 20121/23838 completed (loss: 2.434635877609253, acc: 0.4444444477558136)
[2025-02-04 00:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:54][root][INFO] - Training Epoch: 2/2, step 20122/23838 completed (loss: 3.400675058364868, acc: 0.3333333432674408)
[2025-02-04 00:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:55][root][INFO] - Training Epoch: 2/2, step 20123/23838 completed (loss: 3.8170225620269775, acc: 0.3086419701576233)
[2025-02-04 00:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:55][root][INFO] - Training Epoch: 2/2, step 20124/23838 completed (loss: 3.58780837059021, acc: 0.32692307233810425)
[2025-02-04 00:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:55][root][INFO] - Training Epoch: 2/2, step 20125/23838 completed (loss: 2.969129800796509, acc: 0.4390243887901306)
[2025-02-04 00:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:56][root][INFO] - Training Epoch: 2/2, step 20126/23838 completed (loss: 3.7104909420013428, acc: 0.27272728085517883)
[2025-02-04 00:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:56][root][INFO] - Training Epoch: 2/2, step 20127/23838 completed (loss: 3.23469614982605, acc: 0.3400000035762787)
[2025-02-04 00:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:57][root][INFO] - Training Epoch: 2/2, step 20128/23838 completed (loss: 4.61824369430542, acc: 0.2461538463830948)
[2025-02-04 00:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:57][root][INFO] - Training Epoch: 2/2, step 20129/23838 completed (loss: 3.2777621746063232, acc: 0.39024388790130615)
[2025-02-04 00:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:58][root][INFO] - Training Epoch: 2/2, step 20130/23838 completed (loss: 2.8070642948150635, acc: 0.4193548262119293)
[2025-02-04 00:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:58][root][INFO] - Training Epoch: 2/2, step 20131/23838 completed (loss: 3.140846014022827, acc: 0.44999998807907104)
[2025-02-04 00:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:59][root][INFO] - Training Epoch: 2/2, step 20132/23838 completed (loss: 3.5043368339538574, acc: 0.3142857253551483)
[2025-02-04 00:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:49:59][root][INFO] - Training Epoch: 2/2, step 20133/23838 completed (loss: 3.244091033935547, acc: 0.4054054021835327)
[2025-02-04 00:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:00][root][INFO] - Training Epoch: 2/2, step 20134/23838 completed (loss: 3.8132264614105225, acc: 0.2857142984867096)
[2025-02-04 00:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:00][root][INFO] - Training Epoch: 2/2, step 20135/23838 completed (loss: 2.5590758323669434, acc: 0.4482758641242981)
[2025-02-04 00:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:01][root][INFO] - Training Epoch: 2/2, step 20136/23838 completed (loss: 3.977996349334717, acc: 0.2195121943950653)
[2025-02-04 00:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:01][root][INFO] - Training Epoch: 2/2, step 20137/23838 completed (loss: 3.5987093448638916, acc: 0.4333333373069763)
[2025-02-04 00:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:02][root][INFO] - Training Epoch: 2/2, step 20138/23838 completed (loss: 3.137132167816162, acc: 0.380952388048172)
[2025-02-04 00:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:02][root][INFO] - Training Epoch: 2/2, step 20139/23838 completed (loss: 2.5843465328216553, acc: 0.4054054021835327)
[2025-02-04 00:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:03][root][INFO] - Training Epoch: 2/2, step 20140/23838 completed (loss: 3.6025311946868896, acc: 0.375)
[2025-02-04 00:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:03][root][INFO] - Training Epoch: 2/2, step 20141/23838 completed (loss: 1.7136735916137695, acc: 0.5555555820465088)
[2025-02-04 00:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:03][root][INFO] - Training Epoch: 2/2, step 20142/23838 completed (loss: 2.054004430770874, acc: 0.5625)
[2025-02-04 00:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:04][root][INFO] - Training Epoch: 2/2, step 20143/23838 completed (loss: 2.629269599914551, acc: 0.6190476417541504)
[2025-02-04 00:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:04][root][INFO] - Training Epoch: 2/2, step 20144/23838 completed (loss: 3.3920860290527344, acc: 0.4615384638309479)
[2025-02-04 00:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:05][root][INFO] - Training Epoch: 2/2, step 20145/23838 completed (loss: 2.721304416656494, acc: 0.4285714328289032)
[2025-02-04 00:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:05][root][INFO] - Training Epoch: 2/2, step 20146/23838 completed (loss: 3.8484432697296143, acc: 0.30000001192092896)
[2025-02-04 00:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:06][root][INFO] - Training Epoch: 2/2, step 20147/23838 completed (loss: 3.084939956665039, acc: 0.5)
[2025-02-04 00:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:06][root][INFO] - Training Epoch: 2/2, step 20148/23838 completed (loss: 3.5952301025390625, acc: 0.3199999928474426)
[2025-02-04 00:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:07][root][INFO] - Training Epoch: 2/2, step 20149/23838 completed (loss: 3.6783053874969482, acc: 0.32258063554763794)
[2025-02-04 00:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:07][root][INFO] - Training Epoch: 2/2, step 20150/23838 completed (loss: 4.210157871246338, acc: 0.34375)
[2025-02-04 00:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:08][root][INFO] - Training Epoch: 2/2, step 20151/23838 completed (loss: 1.232381820678711, acc: 0.7058823704719543)
[2025-02-04 00:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:08][root][INFO] - Training Epoch: 2/2, step 20152/23838 completed (loss: 2.1579575538635254, acc: 0.42105263471603394)
[2025-02-04 00:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:08][root][INFO] - Training Epoch: 2/2, step 20153/23838 completed (loss: 2.0417706966400146, acc: 0.5384615659713745)
[2025-02-04 00:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:09][root][INFO] - Training Epoch: 2/2, step 20154/23838 completed (loss: 3.2247262001037598, acc: 0.47826087474823)
[2025-02-04 00:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:09][root][INFO] - Training Epoch: 2/2, step 20155/23838 completed (loss: 2.34074068069458, acc: 0.4838709533214569)
[2025-02-04 00:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:09][root][INFO] - Training Epoch: 2/2, step 20156/23838 completed (loss: 2.527324914932251, acc: 0.48275861144065857)
[2025-02-04 00:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:10][root][INFO] - Training Epoch: 2/2, step 20157/23838 completed (loss: 2.137343406677246, acc: 0.7058823704719543)
[2025-02-04 00:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:10][root][INFO] - Training Epoch: 2/2, step 20158/23838 completed (loss: 3.0589730739593506, acc: 0.47058823704719543)
[2025-02-04 00:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:11][root][INFO] - Training Epoch: 2/2, step 20159/23838 completed (loss: 2.3847765922546387, acc: 0.6551724076271057)
[2025-02-04 00:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:11][root][INFO] - Training Epoch: 2/2, step 20160/23838 completed (loss: 1.4439982175827026, acc: 0.6666666865348816)
[2025-02-04 00:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:12][root][INFO] - Training Epoch: 2/2, step 20161/23838 completed (loss: 1.8905932903289795, acc: 0.5600000023841858)
[2025-02-04 00:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:12][root][INFO] - Training Epoch: 2/2, step 20162/23838 completed (loss: 2.452164888381958, acc: 0.4736842215061188)
[2025-02-04 00:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:13][root][INFO] - Training Epoch: 2/2, step 20163/23838 completed (loss: 2.5171945095062256, acc: 0.48275861144065857)
[2025-02-04 00:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:13][root][INFO] - Training Epoch: 2/2, step 20164/23838 completed (loss: 0.25890660285949707, acc: 0.875)
[2025-02-04 00:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:13][root][INFO] - Training Epoch: 2/2, step 20165/23838 completed (loss: 0.6797454953193665, acc: 0.8999999761581421)
[2025-02-04 00:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:14][root][INFO] - Training Epoch: 2/2, step 20166/23838 completed (loss: 2.7408175468444824, acc: 0.47999998927116394)
[2025-02-04 00:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:14][root][INFO] - Training Epoch: 2/2, step 20167/23838 completed (loss: 1.593612551689148, acc: 0.75)
[2025-02-04 00:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:15][root][INFO] - Training Epoch: 2/2, step 20168/23838 completed (loss: 2.434135913848877, acc: 0.5483871102333069)
[2025-02-04 00:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:15][root][INFO] - Training Epoch: 2/2, step 20169/23838 completed (loss: 1.5064173936843872, acc: 0.6800000071525574)
[2025-02-04 00:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:16][root][INFO] - Training Epoch: 2/2, step 20170/23838 completed (loss: 0.8440669775009155, acc: 0.800000011920929)
[2025-02-04 00:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:16][root][INFO] - Training Epoch: 2/2, step 20171/23838 completed (loss: 2.662529230117798, acc: 0.44117647409439087)
[2025-02-04 00:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:16][root][INFO] - Training Epoch: 2/2, step 20172/23838 completed (loss: 3.0588064193725586, acc: 0.47999998927116394)
[2025-02-04 00:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:17][root][INFO] - Training Epoch: 2/2, step 20173/23838 completed (loss: 2.366960048675537, acc: 0.5769230723381042)
[2025-02-04 00:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:17][root][INFO] - Training Epoch: 2/2, step 20174/23838 completed (loss: 2.842498779296875, acc: 0.46341463923454285)
[2025-02-04 00:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:18][root][INFO] - Training Epoch: 2/2, step 20175/23838 completed (loss: 2.8546814918518066, acc: 0.4615384638309479)
[2025-02-04 00:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:18][root][INFO] - Training Epoch: 2/2, step 20176/23838 completed (loss: 3.4971582889556885, acc: 0.3199999928474426)
[2025-02-04 00:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:19][root][INFO] - Training Epoch: 2/2, step 20177/23838 completed (loss: 2.0257201194763184, acc: 0.5555555820465088)
[2025-02-04 00:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:19][root][INFO] - Training Epoch: 2/2, step 20178/23838 completed (loss: 1.5661486387252808, acc: 0.5384615659713745)
[2025-02-04 00:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:19][root][INFO] - Training Epoch: 2/2, step 20179/23838 completed (loss: 0.6699877977371216, acc: 0.875)
[2025-02-04 00:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:20][root][INFO] - Training Epoch: 2/2, step 20180/23838 completed (loss: 2.529428243637085, acc: 0.5789473652839661)
[2025-02-04 00:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:20][root][INFO] - Training Epoch: 2/2, step 20181/23838 completed (loss: 3.2256174087524414, acc: 0.5)
[2025-02-04 00:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:21][root][INFO] - Training Epoch: 2/2, step 20182/23838 completed (loss: 1.4765392541885376, acc: 0.7272727489471436)
[2025-02-04 00:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:21][root][INFO] - Training Epoch: 2/2, step 20183/23838 completed (loss: 1.9863712787628174, acc: 0.48275861144065857)
[2025-02-04 00:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:21][root][INFO] - Training Epoch: 2/2, step 20184/23838 completed (loss: 3.3227336406707764, acc: 0.3962264060974121)
[2025-02-04 00:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:22][root][INFO] - Training Epoch: 2/2, step 20185/23838 completed (loss: 3.6436121463775635, acc: 0.261904776096344)
[2025-02-04 00:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:22][root][INFO] - Training Epoch: 2/2, step 20186/23838 completed (loss: 3.8849287033081055, acc: 0.2916666567325592)
[2025-02-04 00:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:23][root][INFO] - Training Epoch: 2/2, step 20187/23838 completed (loss: 3.968484401702881, acc: 0.23529411852359772)
[2025-02-04 00:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:23][root][INFO] - Training Epoch: 2/2, step 20188/23838 completed (loss: 2.2977428436279297, acc: 0.5789473652839661)
[2025-02-04 00:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:24][root][INFO] - Training Epoch: 2/2, step 20189/23838 completed (loss: 3.1423168182373047, acc: 0.31578946113586426)
[2025-02-04 00:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:24][root][INFO] - Training Epoch: 2/2, step 20190/23838 completed (loss: 4.298081398010254, acc: 0.3529411852359772)
[2025-02-04 00:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:25][root][INFO] - Training Epoch: 2/2, step 20191/23838 completed (loss: 2.742885112762451, acc: 0.4516128897666931)
[2025-02-04 00:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:25][root][INFO] - Training Epoch: 2/2, step 20192/23838 completed (loss: 2.834812879562378, acc: 0.42424243688583374)
[2025-02-04 00:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:26][root][INFO] - Training Epoch: 2/2, step 20193/23838 completed (loss: 3.501736640930176, acc: 0.36734694242477417)
[2025-02-04 00:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:26][root][INFO] - Training Epoch: 2/2, step 20194/23838 completed (loss: 2.6169745922088623, acc: 0.4166666567325592)
[2025-02-04 00:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:26][root][INFO] - Training Epoch: 2/2, step 20195/23838 completed (loss: 2.010922908782959, acc: 0.6190476417541504)
[2025-02-04 00:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:27][root][INFO] - Training Epoch: 2/2, step 20196/23838 completed (loss: 3.33463454246521, acc: 0.4137931168079376)
[2025-02-04 00:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:28][root][INFO] - Training Epoch: 2/2, step 20197/23838 completed (loss: 4.104546070098877, acc: 0.26923078298568726)
[2025-02-04 00:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:28][root][INFO] - Training Epoch: 2/2, step 20198/23838 completed (loss: 2.548457622528076, acc: 0.517241358757019)
[2025-02-04 00:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:29][root][INFO] - Training Epoch: 2/2, step 20199/23838 completed (loss: 3.333641767501831, acc: 0.4107142984867096)
[2025-02-04 00:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:29][root][INFO] - Training Epoch: 2/2, step 20200/23838 completed (loss: 2.62721848487854, acc: 0.45945945382118225)
[2025-02-04 00:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:30][root][INFO] - Training Epoch: 2/2, step 20201/23838 completed (loss: 3.203829526901245, acc: 0.4000000059604645)
[2025-02-04 00:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:30][root][INFO] - Training Epoch: 2/2, step 20202/23838 completed (loss: 3.0257222652435303, acc: 0.5600000023841858)
[2025-02-04 00:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:31][root][INFO] - Training Epoch: 2/2, step 20203/23838 completed (loss: 2.610649585723877, acc: 0.5)
[2025-02-04 00:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:31][root][INFO] - Training Epoch: 2/2, step 20204/23838 completed (loss: 2.0280039310455322, acc: 0.6666666865348816)
[2025-02-04 00:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:31][root][INFO] - Training Epoch: 2/2, step 20205/23838 completed (loss: 3.8083558082580566, acc: 0.37931033968925476)
[2025-02-04 00:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:32][root][INFO] - Training Epoch: 2/2, step 20206/23838 completed (loss: 5.0524187088012695, acc: 0.22727273404598236)
[2025-02-04 00:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:32][root][INFO] - Training Epoch: 2/2, step 20207/23838 completed (loss: 3.448692798614502, acc: 0.3499999940395355)
[2025-02-04 00:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:33][root][INFO] - Training Epoch: 2/2, step 20208/23838 completed (loss: 2.7594516277313232, acc: 0.2380952388048172)
[2025-02-04 00:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:33][root][INFO] - Training Epoch: 2/2, step 20209/23838 completed (loss: 2.8684492111206055, acc: 0.4583333432674408)
[2025-02-04 00:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:34][root][INFO] - Training Epoch: 2/2, step 20210/23838 completed (loss: 3.273270845413208, acc: 0.375)
[2025-02-04 00:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:34][root][INFO] - Training Epoch: 2/2, step 20211/23838 completed (loss: 3.1519787311553955, acc: 0.4166666567325592)
[2025-02-04 00:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:35][root][INFO] - Training Epoch: 2/2, step 20212/23838 completed (loss: 2.4859793186187744, acc: 0.4166666567325592)
[2025-02-04 00:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:35][root][INFO] - Training Epoch: 2/2, step 20213/23838 completed (loss: 2.207688570022583, acc: 0.4444444477558136)
[2025-02-04 00:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:36][root][INFO] - Training Epoch: 2/2, step 20214/23838 completed (loss: 3.404391050338745, acc: 0.34375)
[2025-02-04 00:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:36][root][INFO] - Training Epoch: 2/2, step 20215/23838 completed (loss: 2.694319725036621, acc: 0.47826087474823)
[2025-02-04 00:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:37][root][INFO] - Training Epoch: 2/2, step 20216/23838 completed (loss: 2.911282539367676, acc: 0.46666666865348816)
[2025-02-04 00:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:37][root][INFO] - Training Epoch: 2/2, step 20217/23838 completed (loss: 2.257110357284546, acc: 0.4000000059604645)
[2025-02-04 00:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:37][root][INFO] - Training Epoch: 2/2, step 20218/23838 completed (loss: 3.14086651802063, acc: 0.42307692766189575)
[2025-02-04 00:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:38][root][INFO] - Training Epoch: 2/2, step 20219/23838 completed (loss: 2.632819652557373, acc: 0.44999998807907104)
[2025-02-04 00:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:38][root][INFO] - Training Epoch: 2/2, step 20220/23838 completed (loss: 3.473745822906494, acc: 0.38235294818878174)
[2025-02-04 00:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:39][root][INFO] - Training Epoch: 2/2, step 20221/23838 completed (loss: 2.004101037979126, acc: 0.2857142984867096)
[2025-02-04 00:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:39][root][INFO] - Training Epoch: 2/2, step 20222/23838 completed (loss: 2.9241082668304443, acc: 0.3513513505458832)
[2025-02-04 00:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:40][root][INFO] - Training Epoch: 2/2, step 20223/23838 completed (loss: 3.3311245441436768, acc: 0.3684210479259491)
[2025-02-04 00:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:40][root][INFO] - Training Epoch: 2/2, step 20224/23838 completed (loss: 3.260939598083496, acc: 0.2641509473323822)
[2025-02-04 00:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:41][root][INFO] - Training Epoch: 2/2, step 20225/23838 completed (loss: 2.426353931427002, acc: 0.6666666865348816)
[2025-02-04 00:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:41][root][INFO] - Training Epoch: 2/2, step 20226/23838 completed (loss: 3.212477922439575, acc: 0.39534884691238403)
[2025-02-04 00:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:41][root][INFO] - Training Epoch: 2/2, step 20227/23838 completed (loss: 2.6822710037231445, acc: 0.46341463923454285)
[2025-02-04 00:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:42][root][INFO] - Training Epoch: 2/2, step 20228/23838 completed (loss: 3.7001969814300537, acc: 0.2857142984867096)
[2025-02-04 00:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:42][root][INFO] - Training Epoch: 2/2, step 20229/23838 completed (loss: 3.9841737747192383, acc: 0.4000000059604645)
[2025-02-04 00:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:43][root][INFO] - Training Epoch: 2/2, step 20230/23838 completed (loss: 2.510582447052002, acc: 0.3333333432674408)
[2025-02-04 00:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:43][root][INFO] - Training Epoch: 2/2, step 20231/23838 completed (loss: 3.303506374359131, acc: 0.30434781312942505)
[2025-02-04 00:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:44][root][INFO] - Training Epoch: 2/2, step 20232/23838 completed (loss: 2.8170273303985596, acc: 0.4399999976158142)
[2025-02-04 00:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:44][root][INFO] - Training Epoch: 2/2, step 20233/23838 completed (loss: 3.532822847366333, acc: 0.26923078298568726)
[2025-02-04 00:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:45][root][INFO] - Training Epoch: 2/2, step 20234/23838 completed (loss: 3.6861367225646973, acc: 0.2857142984867096)
[2025-02-04 00:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:45][root][INFO] - Training Epoch: 2/2, step 20235/23838 completed (loss: 1.4421461820602417, acc: 0.7333333492279053)
[2025-02-04 00:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:46][root][INFO] - Training Epoch: 2/2, step 20236/23838 completed (loss: 1.630092740058899, acc: 0.4166666567325592)
[2025-02-04 00:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:46][root][INFO] - Training Epoch: 2/2, step 20237/23838 completed (loss: 1.7745041847229004, acc: 0.6153846383094788)
[2025-02-04 00:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:46][root][INFO] - Training Epoch: 2/2, step 20238/23838 completed (loss: 2.8712551593780518, acc: 0.40625)
[2025-02-04 00:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:47][root][INFO] - Training Epoch: 2/2, step 20239/23838 completed (loss: 4.1690826416015625, acc: 0.3333333432674408)
[2025-02-04 00:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:47][root][INFO] - Training Epoch: 2/2, step 20240/23838 completed (loss: 3.1058077812194824, acc: 0.40740740299224854)
[2025-02-04 00:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:48][root][INFO] - Training Epoch: 2/2, step 20241/23838 completed (loss: 3.6424667835235596, acc: 0.27906978130340576)
[2025-02-04 00:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:48][root][INFO] - Training Epoch: 2/2, step 20242/23838 completed (loss: 3.886082649230957, acc: 0.19512194395065308)
[2025-02-04 00:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:49][root][INFO] - Training Epoch: 2/2, step 20243/23838 completed (loss: 3.1611053943634033, acc: 0.40740740299224854)
[2025-02-04 00:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:49][root][INFO] - Training Epoch: 2/2, step 20244/23838 completed (loss: 2.8227155208587646, acc: 0.2631579041481018)
[2025-02-04 00:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:49][root][INFO] - Training Epoch: 2/2, step 20245/23838 completed (loss: 2.87911319732666, acc: 0.24137930572032928)
[2025-02-04 00:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:50][root][INFO] - Training Epoch: 2/2, step 20246/23838 completed (loss: 3.0865540504455566, acc: 0.4166666567325592)
[2025-02-04 00:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:50][root][INFO] - Training Epoch: 2/2, step 20247/23838 completed (loss: 2.969072103500366, acc: 0.36538460850715637)
[2025-02-04 00:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:51][root][INFO] - Training Epoch: 2/2, step 20248/23838 completed (loss: 3.3642921447753906, acc: 0.2954545319080353)
[2025-02-04 00:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:51][root][INFO] - Training Epoch: 2/2, step 20249/23838 completed (loss: 3.017319440841675, acc: 0.27272728085517883)
[2025-02-04 00:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:52][root][INFO] - Training Epoch: 2/2, step 20250/23838 completed (loss: 3.2041027545928955, acc: 0.2916666567325592)
[2025-02-04 00:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:52][root][INFO] - Training Epoch: 2/2, step 20251/23838 completed (loss: 3.075122117996216, acc: 0.27272728085517883)
[2025-02-04 00:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:52][root][INFO] - Training Epoch: 2/2, step 20252/23838 completed (loss: 1.9479305744171143, acc: 0.4838709533214569)
[2025-02-04 00:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:53][root][INFO] - Training Epoch: 2/2, step 20253/23838 completed (loss: 3.6071066856384277, acc: 0.3684210479259491)
[2025-02-04 00:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:53][root][INFO] - Training Epoch: 2/2, step 20254/23838 completed (loss: 2.168447256088257, acc: 0.550000011920929)
[2025-02-04 00:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:54][root][INFO] - Training Epoch: 2/2, step 20255/23838 completed (loss: 3.93769907951355, acc: 0.3333333432674408)
[2025-02-04 00:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:54][root][INFO] - Training Epoch: 2/2, step 20256/23838 completed (loss: 2.759592056274414, acc: 0.5)
[2025-02-04 00:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:55][root][INFO] - Training Epoch: 2/2, step 20257/23838 completed (loss: 2.296334743499756, acc: 0.529411792755127)
[2025-02-04 00:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:55][root][INFO] - Training Epoch: 2/2, step 20258/23838 completed (loss: 2.302577257156372, acc: 0.5833333134651184)
[2025-02-04 00:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:56][root][INFO] - Training Epoch: 2/2, step 20259/23838 completed (loss: 2.77481746673584, acc: 0.375)
[2025-02-04 00:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:56][root][INFO] - Training Epoch: 2/2, step 20260/23838 completed (loss: 3.089998245239258, acc: 0.3333333432674408)
[2025-02-04 00:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:57][root][INFO] - Training Epoch: 2/2, step 20261/23838 completed (loss: 2.397862434387207, acc: 0.6666666865348816)
[2025-02-04 00:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:57][root][INFO] - Training Epoch: 2/2, step 20262/23838 completed (loss: 2.6615655422210693, acc: 0.375)
[2025-02-04 00:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:58][root][INFO] - Training Epoch: 2/2, step 20263/23838 completed (loss: 2.784285068511963, acc: 0.3499999940395355)
[2025-02-04 00:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:58][root][INFO] - Training Epoch: 2/2, step 20264/23838 completed (loss: 2.258275032043457, acc: 0.6399999856948853)
[2025-02-04 00:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:58][root][INFO] - Training Epoch: 2/2, step 20265/23838 completed (loss: 2.025120973587036, acc: 0.5652173757553101)
[2025-02-04 00:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:59][root][INFO] - Training Epoch: 2/2, step 20266/23838 completed (loss: 2.468096971511841, acc: 0.4399999976158142)
[2025-02-04 00:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:50:59][root][INFO] - Training Epoch: 2/2, step 20267/23838 completed (loss: 3.1940431594848633, acc: 0.3684210479259491)
[2025-02-04 00:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:00][root][INFO] - Training Epoch: 2/2, step 20268/23838 completed (loss: 2.7347819805145264, acc: 0.4583333432674408)
[2025-02-04 00:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:00][root][INFO] - Training Epoch: 2/2, step 20269/23838 completed (loss: 3.617798328399658, acc: 0.5263158082962036)
[2025-02-04 00:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:00][root][INFO] - Training Epoch: 2/2, step 20270/23838 completed (loss: 4.063125133514404, acc: 0.3461538553237915)
[2025-02-04 00:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:01][root][INFO] - Training Epoch: 2/2, step 20271/23838 completed (loss: 3.702176809310913, acc: 0.3181818127632141)
[2025-02-04 00:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:01][root][INFO] - Training Epoch: 2/2, step 20272/23838 completed (loss: 4.0000834465026855, acc: 0.550000011920929)
[2025-02-04 00:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:02][root][INFO] - Training Epoch: 2/2, step 20273/23838 completed (loss: 3.4306066036224365, acc: 0.31578946113586426)
[2025-02-04 00:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:02][root][INFO] - Training Epoch: 2/2, step 20274/23838 completed (loss: 3.1189780235290527, acc: 0.4000000059604645)
[2025-02-04 00:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:03][root][INFO] - Training Epoch: 2/2, step 20275/23838 completed (loss: 4.142951965332031, acc: 0.37142857909202576)
[2025-02-04 00:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:03][root][INFO] - Training Epoch: 2/2, step 20276/23838 completed (loss: 2.916317939758301, acc: 0.5882353186607361)
[2025-02-04 00:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:04][root][INFO] - Training Epoch: 2/2, step 20277/23838 completed (loss: 2.016216516494751, acc: 0.5714285969734192)
[2025-02-04 00:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:04][root][INFO] - Training Epoch: 2/2, step 20278/23838 completed (loss: 2.779076099395752, acc: 0.529411792755127)
[2025-02-04 00:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:05][root][INFO] - Training Epoch: 2/2, step 20279/23838 completed (loss: 2.0477335453033447, acc: 0.6363636255264282)
[2025-02-04 00:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:05][root][INFO] - Training Epoch: 2/2, step 20280/23838 completed (loss: 2.163627862930298, acc: 0.6086956262588501)
[2025-02-04 00:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:05][root][INFO] - Training Epoch: 2/2, step 20281/23838 completed (loss: 1.5620073080062866, acc: 0.6666666865348816)
[2025-02-04 00:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:06][root][INFO] - Training Epoch: 2/2, step 20282/23838 completed (loss: 3.1010186672210693, acc: 0.4285714328289032)
[2025-02-04 00:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:06][root][INFO] - Training Epoch: 2/2, step 20283/23838 completed (loss: 3.072549343109131, acc: 0.43478259444236755)
[2025-02-04 00:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:07][root][INFO] - Training Epoch: 2/2, step 20284/23838 completed (loss: 3.6778924465179443, acc: 0.4166666567325592)
[2025-02-04 00:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:07][root][INFO] - Training Epoch: 2/2, step 20285/23838 completed (loss: 2.6154966354370117, acc: 0.529411792755127)
[2025-02-04 00:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:08][root][INFO] - Training Epoch: 2/2, step 20286/23838 completed (loss: 0.6479222774505615, acc: 0.800000011920929)
[2025-02-04 00:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:08][root][INFO] - Training Epoch: 2/2, step 20287/23838 completed (loss: 0.5189388990402222, acc: 0.8999999761581421)
[2025-02-04 00:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:09][root][INFO] - Training Epoch: 2/2, step 20288/23838 completed (loss: 1.771508812904358, acc: 0.7333333492279053)
[2025-02-04 00:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:09][root][INFO] - Training Epoch: 2/2, step 20289/23838 completed (loss: 0.17980195581912994, acc: 0.875)
[2025-02-04 00:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:09][root][INFO] - Training Epoch: 2/2, step 20290/23838 completed (loss: 1.6181280612945557, acc: 0.6774193644523621)
[2025-02-04 00:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:10][root][INFO] - Training Epoch: 2/2, step 20291/23838 completed (loss: 2.0911991596221924, acc: 0.6086956262588501)
[2025-02-04 00:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:10][root][INFO] - Training Epoch: 2/2, step 20292/23838 completed (loss: 2.665605306625366, acc: 0.523809552192688)
[2025-02-04 00:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:11][root][INFO] - Training Epoch: 2/2, step 20293/23838 completed (loss: 1.6259666681289673, acc: 0.5333333611488342)
[2025-02-04 00:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:11][root][INFO] - Training Epoch: 2/2, step 20294/23838 completed (loss: 1.7076841592788696, acc: 0.5789473652839661)
[2025-02-04 00:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:12][root][INFO] - Training Epoch: 2/2, step 20295/23838 completed (loss: 2.1802990436553955, acc: 0.5)
[2025-02-04 00:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:12][root][INFO] - Training Epoch: 2/2, step 20296/23838 completed (loss: 2.3415114879608154, acc: 0.47999998927116394)
[2025-02-04 00:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:13][root][INFO] - Training Epoch: 2/2, step 20297/23838 completed (loss: 1.9627102613449097, acc: 0.6666666865348816)
[2025-02-04 00:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:13][root][INFO] - Training Epoch: 2/2, step 20298/23838 completed (loss: 2.9056267738342285, acc: 0.625)
[2025-02-04 00:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:14][root][INFO] - Training Epoch: 2/2, step 20299/23838 completed (loss: 2.9900336265563965, acc: 0.5384615659713745)
[2025-02-04 00:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:14][root][INFO] - Training Epoch: 2/2, step 20300/23838 completed (loss: 4.209161758422852, acc: 0.4000000059604645)
[2025-02-04 00:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:15][root][INFO] - Training Epoch: 2/2, step 20301/23838 completed (loss: 3.928622007369995, acc: 0.3333333432674408)
[2025-02-04 00:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:15][root][INFO] - Training Epoch: 2/2, step 20302/23838 completed (loss: 4.412501811981201, acc: 0.3333333432674408)
[2025-02-04 00:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:15][root][INFO] - Training Epoch: 2/2, step 20303/23838 completed (loss: 2.9342119693756104, acc: 0.3888888955116272)
[2025-02-04 00:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:16][root][INFO] - Training Epoch: 2/2, step 20304/23838 completed (loss: 2.895078659057617, acc: 0.4285714328289032)
[2025-02-04 00:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:16][root][INFO] - Training Epoch: 2/2, step 20305/23838 completed (loss: 2.7241122722625732, acc: 0.375)
[2025-02-04 00:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:17][root][INFO] - Training Epoch: 2/2, step 20306/23838 completed (loss: 1.6820526123046875, acc: 0.7058823704719543)
[2025-02-04 00:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:17][root][INFO] - Training Epoch: 2/2, step 20307/23838 completed (loss: 2.1244077682495117, acc: 0.5)
[2025-02-04 00:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:18][root][INFO] - Training Epoch: 2/2, step 20308/23838 completed (loss: 2.076568841934204, acc: 0.5714285969734192)
[2025-02-04 00:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:18][root][INFO] - Training Epoch: 2/2, step 20309/23838 completed (loss: 3.2669756412506104, acc: 0.4117647111415863)
[2025-02-04 00:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:18][root][INFO] - Training Epoch: 2/2, step 20310/23838 completed (loss: 2.175426959991455, acc: 0.5789473652839661)
[2025-02-04 00:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:19][root][INFO] - Training Epoch: 2/2, step 20311/23838 completed (loss: 2.678804636001587, acc: 0.5333333611488342)
[2025-02-04 00:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:19][root][INFO] - Training Epoch: 2/2, step 20312/23838 completed (loss: 1.6098592281341553, acc: 0.6111111044883728)
[2025-02-04 00:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:20][root][INFO] - Training Epoch: 2/2, step 20313/23838 completed (loss: 3.67261004447937, acc: 0.3499999940395355)
[2025-02-04 00:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:20][root][INFO] - Training Epoch: 2/2, step 20314/23838 completed (loss: 1.9168343544006348, acc: 0.5555555820465088)
[2025-02-04 00:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:21][root][INFO] - Training Epoch: 2/2, step 20315/23838 completed (loss: 2.9299733638763428, acc: 0.4285714328289032)
[2025-02-04 00:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:21][root][INFO] - Training Epoch: 2/2, step 20316/23838 completed (loss: 2.928739547729492, acc: 0.4166666567325592)
[2025-02-04 00:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:21][root][INFO] - Training Epoch: 2/2, step 20317/23838 completed (loss: 3.392955780029297, acc: 0.5333333611488342)
[2025-02-04 00:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:22][root][INFO] - Training Epoch: 2/2, step 20318/23838 completed (loss: 3.3535537719726562, acc: 0.6000000238418579)
[2025-02-04 00:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:22][root][INFO] - Training Epoch: 2/2, step 20319/23838 completed (loss: 3.869905471801758, acc: 0.4000000059604645)
[2025-02-04 00:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:23][root][INFO] - Training Epoch: 2/2, step 20320/23838 completed (loss: 2.8823564052581787, acc: 0.43478259444236755)
[2025-02-04 00:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:23][root][INFO] - Training Epoch: 2/2, step 20321/23838 completed (loss: 1.6412982940673828, acc: 0.6470588445663452)
[2025-02-04 00:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:23][root][INFO] - Training Epoch: 2/2, step 20322/23838 completed (loss: 2.7178497314453125, acc: 0.4444444477558136)
[2025-02-04 00:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:24][root][INFO] - Training Epoch: 2/2, step 20323/23838 completed (loss: 2.1782727241516113, acc: 0.5)
[2025-02-04 00:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:25][root][INFO] - Training Epoch: 2/2, step 20324/23838 completed (loss: 3.506779432296753, acc: 0.3499999940395355)
[2025-02-04 00:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:25][root][INFO] - Training Epoch: 2/2, step 20325/23838 completed (loss: 2.8160433769226074, acc: 0.42424243688583374)
[2025-02-04 00:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:25][root][INFO] - Training Epoch: 2/2, step 20326/23838 completed (loss: 2.1851160526275635, acc: 0.6666666865348816)
[2025-02-04 00:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:26][root][INFO] - Training Epoch: 2/2, step 20327/23838 completed (loss: 2.950686454772949, acc: 0.47058823704719543)
[2025-02-04 00:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:26][root][INFO] - Training Epoch: 2/2, step 20328/23838 completed (loss: 2.7027957439422607, acc: 0.46666666865348816)
[2025-02-04 00:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:27][root][INFO] - Training Epoch: 2/2, step 20329/23838 completed (loss: 3.9518496990203857, acc: 0.3499999940395355)
[2025-02-04 00:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:27][root][INFO] - Training Epoch: 2/2, step 20330/23838 completed (loss: 1.8003034591674805, acc: 0.5)
[2025-02-04 00:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:28][root][INFO] - Training Epoch: 2/2, step 20331/23838 completed (loss: 4.178213596343994, acc: 0.3125)
[2025-02-04 00:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:28][root][INFO] - Training Epoch: 2/2, step 20332/23838 completed (loss: 3.1462900638580322, acc: 0.3571428656578064)
[2025-02-04 00:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:29][root][INFO] - Training Epoch: 2/2, step 20333/23838 completed (loss: 3.1096222400665283, acc: 0.3499999940395355)
[2025-02-04 00:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:29][root][INFO] - Training Epoch: 2/2, step 20334/23838 completed (loss: 3.3143043518066406, acc: 0.2926829159259796)
[2025-02-04 00:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:30][root][INFO] - Training Epoch: 2/2, step 20335/23838 completed (loss: 3.571070909500122, acc: 0.5600000023841858)
[2025-02-04 00:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:30][root][INFO] - Training Epoch: 2/2, step 20336/23838 completed (loss: 3.601123809814453, acc: 0.3333333432674408)
[2025-02-04 00:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:30][root][INFO] - Training Epoch: 2/2, step 20337/23838 completed (loss: 4.1793365478515625, acc: 0.4000000059604645)
[2025-02-04 00:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:31][root][INFO] - Training Epoch: 2/2, step 20338/23838 completed (loss: 3.240171194076538, acc: 0.4482758641242981)
[2025-02-04 00:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:31][root][INFO] - Training Epoch: 2/2, step 20339/23838 completed (loss: 4.272974491119385, acc: 0.28125)
[2025-02-04 00:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:32][root][INFO] - Training Epoch: 2/2, step 20340/23838 completed (loss: 4.938416957855225, acc: 0.25)
[2025-02-04 00:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:32][root][INFO] - Training Epoch: 2/2, step 20341/23838 completed (loss: 4.942321300506592, acc: 0.2857142984867096)
[2025-02-04 00:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:33][root][INFO] - Training Epoch: 2/2, step 20342/23838 completed (loss: 4.339327812194824, acc: 0.34210526943206787)
[2025-02-04 00:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:33][root][INFO] - Training Epoch: 2/2, step 20343/23838 completed (loss: 3.214996814727783, acc: 0.38461539149284363)
[2025-02-04 00:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:34][root][INFO] - Training Epoch: 2/2, step 20344/23838 completed (loss: 3.112130641937256, acc: 0.3461538553237915)
[2025-02-04 00:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:34][root][INFO] - Training Epoch: 2/2, step 20345/23838 completed (loss: 3.631509304046631, acc: 0.3928571343421936)
[2025-02-04 00:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:35][root][INFO] - Training Epoch: 2/2, step 20346/23838 completed (loss: 3.1236720085144043, acc: 0.42307692766189575)
[2025-02-04 00:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:35][root][INFO] - Training Epoch: 2/2, step 20347/23838 completed (loss: 4.545657157897949, acc: 0.3488371968269348)
[2025-02-04 00:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:36][root][INFO] - Training Epoch: 2/2, step 20348/23838 completed (loss: 4.224349498748779, acc: 0.3636363744735718)
[2025-02-04 00:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:36][root][INFO] - Training Epoch: 2/2, step 20349/23838 completed (loss: 2.586409091949463, acc: 0.5454545617103577)
[2025-02-04 00:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:37][root][INFO] - Training Epoch: 2/2, step 20350/23838 completed (loss: 2.9743075370788574, acc: 0.38461539149284363)
[2025-02-04 00:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:37][root][INFO] - Training Epoch: 2/2, step 20351/23838 completed (loss: 2.223775625228882, acc: 0.5483871102333069)
[2025-02-04 00:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:38][root][INFO] - Training Epoch: 2/2, step 20352/23838 completed (loss: 2.9512157440185547, acc: 0.44999998807907104)
[2025-02-04 00:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:38][root][INFO] - Training Epoch: 2/2, step 20353/23838 completed (loss: 3.9503676891326904, acc: 0.2631579041481018)
[2025-02-04 00:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:39][root][INFO] - Training Epoch: 2/2, step 20354/23838 completed (loss: 3.642899513244629, acc: 0.28125)
[2025-02-04 00:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:39][root][INFO] - Training Epoch: 2/2, step 20355/23838 completed (loss: 3.1953845024108887, acc: 0.29411765933036804)
[2025-02-04 00:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:40][root][INFO] - Training Epoch: 2/2, step 20356/23838 completed (loss: 2.4963033199310303, acc: 0.3571428656578064)
[2025-02-04 00:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:40][root][INFO] - Training Epoch: 2/2, step 20357/23838 completed (loss: 3.0927138328552246, acc: 0.4642857015132904)
[2025-02-04 00:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:41][root][INFO] - Training Epoch: 2/2, step 20358/23838 completed (loss: 3.5796425342559814, acc: 0.3870967626571655)
[2025-02-04 00:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:41][root][INFO] - Training Epoch: 2/2, step 20359/23838 completed (loss: 3.8222997188568115, acc: 0.4193548262119293)
[2025-02-04 00:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:42][root][INFO] - Training Epoch: 2/2, step 20360/23838 completed (loss: 3.8660976886749268, acc: 0.34285715222358704)
[2025-02-04 00:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:42][root][INFO] - Training Epoch: 2/2, step 20361/23838 completed (loss: 1.8161046504974365, acc: 0.6666666865348816)
[2025-02-04 00:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:42][root][INFO] - Training Epoch: 2/2, step 20362/23838 completed (loss: 3.871764659881592, acc: 0.3396226465702057)
[2025-02-04 00:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:43][root][INFO] - Training Epoch: 2/2, step 20363/23838 completed (loss: 4.0766119956970215, acc: 0.2857142984867096)
[2025-02-04 00:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:43][root][INFO] - Training Epoch: 2/2, step 20364/23838 completed (loss: 3.7787606716156006, acc: 0.4000000059604645)
[2025-02-04 00:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:44][root][INFO] - Training Epoch: 2/2, step 20365/23838 completed (loss: 3.6276628971099854, acc: 0.31707316637039185)
[2025-02-04 00:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:44][root][INFO] - Training Epoch: 2/2, step 20366/23838 completed (loss: 2.402822256088257, acc: 0.5)
[2025-02-04 00:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:45][root][INFO] - Training Epoch: 2/2, step 20367/23838 completed (loss: 3.1127912998199463, acc: 0.46666666865348816)
[2025-02-04 00:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:45][root][INFO] - Training Epoch: 2/2, step 20368/23838 completed (loss: 2.5514190196990967, acc: 0.6000000238418579)
[2025-02-04 00:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:46][root][INFO] - Training Epoch: 2/2, step 20369/23838 completed (loss: 4.08596134185791, acc: 0.380952388048172)
[2025-02-04 00:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:46][root][INFO] - Training Epoch: 2/2, step 20370/23838 completed (loss: 3.263509750366211, acc: 0.2631579041481018)
[2025-02-04 00:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:47][root][INFO] - Training Epoch: 2/2, step 20371/23838 completed (loss: 4.289522171020508, acc: 0.260869562625885)
[2025-02-04 00:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:47][root][INFO] - Training Epoch: 2/2, step 20372/23838 completed (loss: 2.648712158203125, acc: 0.5)
[2025-02-04 00:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:47][root][INFO] - Training Epoch: 2/2, step 20373/23838 completed (loss: 3.4293553829193115, acc: 0.4761904776096344)
[2025-02-04 00:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:48][root][INFO] - Training Epoch: 2/2, step 20374/23838 completed (loss: 3.469980001449585, acc: 0.29032257199287415)
[2025-02-04 00:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:48][root][INFO] - Training Epoch: 2/2, step 20375/23838 completed (loss: 2.7169315814971924, acc: 0.4848484992980957)
[2025-02-04 00:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:49][root][INFO] - Training Epoch: 2/2, step 20376/23838 completed (loss: 3.2599174976348877, acc: 0.40909090638160706)
[2025-02-04 00:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:49][root][INFO] - Training Epoch: 2/2, step 20377/23838 completed (loss: 2.923814535140991, acc: 0.5897436141967773)
[2025-02-04 00:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:50][root][INFO] - Training Epoch: 2/2, step 20378/23838 completed (loss: 2.839963912963867, acc: 0.5625)
[2025-02-04 00:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:50][root][INFO] - Training Epoch: 2/2, step 20379/23838 completed (loss: 3.0516245365142822, acc: 0.5)
[2025-02-04 00:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:51][root][INFO] - Training Epoch: 2/2, step 20380/23838 completed (loss: 3.652794122695923, acc: 0.3103448152542114)
[2025-02-04 00:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:51][root][INFO] - Training Epoch: 2/2, step 20381/23838 completed (loss: 2.7671337127685547, acc: 0.47826087474823)
[2025-02-04 00:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:51][root][INFO] - Training Epoch: 2/2, step 20382/23838 completed (loss: 3.1110353469848633, acc: 0.48571428656578064)
[2025-02-04 00:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:52][root][INFO] - Training Epoch: 2/2, step 20383/23838 completed (loss: 2.355278968811035, acc: 0.5517241358757019)
[2025-02-04 00:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:52][root][INFO] - Training Epoch: 2/2, step 20384/23838 completed (loss: 2.158144950866699, acc: 0.52173912525177)
[2025-02-04 00:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:53][root][INFO] - Training Epoch: 2/2, step 20385/23838 completed (loss: 3.772052526473999, acc: 0.18571428954601288)
[2025-02-04 00:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:53][root][INFO] - Training Epoch: 2/2, step 20386/23838 completed (loss: 4.727219581604004, acc: 0.24137930572032928)
[2025-02-04 00:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:54][root][INFO] - Training Epoch: 2/2, step 20387/23838 completed (loss: 4.163740634918213, acc: 0.26923078298568726)
[2025-02-04 00:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:54][root][INFO] - Training Epoch: 2/2, step 20388/23838 completed (loss: 4.564602375030518, acc: 0.3541666567325592)
[2025-02-04 00:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:54][root][INFO] - Training Epoch: 2/2, step 20389/23838 completed (loss: 3.442129611968994, acc: 0.375)
[2025-02-04 00:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:55][root][INFO] - Training Epoch: 2/2, step 20390/23838 completed (loss: 4.581998348236084, acc: 0.3255814015865326)
[2025-02-04 00:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:55][root][INFO] - Training Epoch: 2/2, step 20391/23838 completed (loss: 3.6412899494171143, acc: 0.21212121844291687)
[2025-02-04 00:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:56][root][INFO] - Training Epoch: 2/2, step 20392/23838 completed (loss: 3.9463303089141846, acc: 0.2954545319080353)
[2025-02-04 00:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:56][root][INFO] - Training Epoch: 2/2, step 20393/23838 completed (loss: 3.8521037101745605, acc: 0.30000001192092896)
[2025-02-04 00:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:57][root][INFO] - Training Epoch: 2/2, step 20394/23838 completed (loss: 3.840108871459961, acc: 0.3529411852359772)
[2025-02-04 00:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:57][root][INFO] - Training Epoch: 2/2, step 20395/23838 completed (loss: 3.876741647720337, acc: 0.3265306055545807)
[2025-02-04 00:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:57][root][INFO] - Training Epoch: 2/2, step 20396/23838 completed (loss: 3.442089319229126, acc: 0.3214285671710968)
[2025-02-04 00:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:58][root][INFO] - Training Epoch: 2/2, step 20397/23838 completed (loss: 4.95692777633667, acc: 0.21153846383094788)
[2025-02-04 00:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:59][root][INFO] - Training Epoch: 2/2, step 20398/23838 completed (loss: 3.302908420562744, acc: 0.3617021143436432)
[2025-02-04 00:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:51:59][root][INFO] - Training Epoch: 2/2, step 20399/23838 completed (loss: 2.811727285385132, acc: 0.4324324429035187)
[2025-02-04 00:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:00][root][INFO] - Training Epoch: 2/2, step 20400/23838 completed (loss: 3.6130707263946533, acc: 0.3469387888908386)
[2025-02-04 00:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:00][root][INFO] - Training Epoch: 2/2, step 20401/23838 completed (loss: 4.834320545196533, acc: 0.2777777910232544)
[2025-02-04 00:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:01][root][INFO] - Training Epoch: 2/2, step 20402/23838 completed (loss: 3.2039849758148193, acc: 0.2954545319080353)
[2025-02-04 00:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:01][root][INFO] - Training Epoch: 2/2, step 20403/23838 completed (loss: 4.095950603485107, acc: 0.30000001192092896)
[2025-02-04 00:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:02][root][INFO] - Training Epoch: 2/2, step 20404/23838 completed (loss: 3.394521951675415, acc: 0.2954545319080353)
[2025-02-04 00:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:02][root][INFO] - Training Epoch: 2/2, step 20405/23838 completed (loss: 2.690330982208252, acc: 0.5625)
[2025-02-04 00:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:03][root][INFO] - Training Epoch: 2/2, step 20406/23838 completed (loss: 2.081700325012207, acc: 0.5625)
[2025-02-04 00:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:03][root][INFO] - Training Epoch: 2/2, step 20407/23838 completed (loss: 4.317103862762451, acc: 0.3333333432674408)
[2025-02-04 00:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:04][root][INFO] - Training Epoch: 2/2, step 20408/23838 completed (loss: 3.0844457149505615, acc: 0.43478259444236755)
[2025-02-04 00:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:04][root][INFO] - Training Epoch: 2/2, step 20409/23838 completed (loss: 4.398224353790283, acc: 0.3888888955116272)
[2025-02-04 00:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:04][root][INFO] - Training Epoch: 2/2, step 20410/23838 completed (loss: 3.7751452922821045, acc: 0.4000000059604645)
[2025-02-04 00:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:05][root][INFO] - Training Epoch: 2/2, step 20411/23838 completed (loss: 2.9845504760742188, acc: 0.4318181872367859)
[2025-02-04 00:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:05][root][INFO] - Training Epoch: 2/2, step 20412/23838 completed (loss: 4.766541004180908, acc: 0.25)
[2025-02-04 00:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:06][root][INFO] - Training Epoch: 2/2, step 20413/23838 completed (loss: 3.122983455657959, acc: 0.4285714328289032)
[2025-02-04 00:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:06][root][INFO] - Training Epoch: 2/2, step 20414/23838 completed (loss: 2.268329381942749, acc: 0.6363636255264282)
[2025-02-04 00:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:07][root][INFO] - Training Epoch: 2/2, step 20415/23838 completed (loss: 3.7106921672821045, acc: 0.4193548262119293)
[2025-02-04 00:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:07][root][INFO] - Training Epoch: 2/2, step 20416/23838 completed (loss: 3.2493956089019775, acc: 0.529411792755127)
[2025-02-04 00:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:07][root][INFO] - Training Epoch: 2/2, step 20417/23838 completed (loss: 2.0465614795684814, acc: 0.6666666865348816)
[2025-02-04 00:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:08][root][INFO] - Training Epoch: 2/2, step 20418/23838 completed (loss: 2.8280017375946045, acc: 0.5454545617103577)
[2025-02-04 00:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:08][root][INFO] - Training Epoch: 2/2, step 20419/23838 completed (loss: 4.174037933349609, acc: 0.4761904776096344)
[2025-02-04 00:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:09][root][INFO] - Training Epoch: 2/2, step 20420/23838 completed (loss: 2.9434549808502197, acc: 0.4516128897666931)
[2025-02-04 00:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:09][root][INFO] - Training Epoch: 2/2, step 20421/23838 completed (loss: 3.725896120071411, acc: 0.2916666567325592)
[2025-02-04 00:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:10][root][INFO] - Training Epoch: 2/2, step 20422/23838 completed (loss: 3.5193850994110107, acc: 0.4736842215061188)
[2025-02-04 00:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:10][root][INFO] - Training Epoch: 2/2, step 20423/23838 completed (loss: 2.3855295181274414, acc: 0.529411792755127)
[2025-02-04 00:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:10][root][INFO] - Training Epoch: 2/2, step 20424/23838 completed (loss: 3.771871328353882, acc: 0.3181818127632141)
[2025-02-04 00:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:11][root][INFO] - Training Epoch: 2/2, step 20425/23838 completed (loss: 3.4657962322235107, acc: 0.3684210479259491)
[2025-02-04 00:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:11][root][INFO] - Training Epoch: 2/2, step 20426/23838 completed (loss: 2.983213424682617, acc: 0.42105263471603394)
[2025-02-04 00:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:12][root][INFO] - Training Epoch: 2/2, step 20427/23838 completed (loss: 3.5568087100982666, acc: 0.4166666567325592)
[2025-02-04 00:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:12][root][INFO] - Training Epoch: 2/2, step 20428/23838 completed (loss: 3.3427610397338867, acc: 0.3684210479259491)
[2025-02-04 00:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:12][root][INFO] - Training Epoch: 2/2, step 20429/23838 completed (loss: 3.328315496444702, acc: 0.4583333432674408)
[2025-02-04 00:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:13][root][INFO] - Training Epoch: 2/2, step 20430/23838 completed (loss: 3.5525765419006348, acc: 0.3870967626571655)
[2025-02-04 00:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:13][root][INFO] - Training Epoch: 2/2, step 20431/23838 completed (loss: 2.787383556365967, acc: 0.5862069129943848)
[2025-02-04 00:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:14][root][INFO] - Training Epoch: 2/2, step 20432/23838 completed (loss: 2.6846225261688232, acc: 0.529411792755127)
[2025-02-04 00:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:14][root][INFO] - Training Epoch: 2/2, step 20433/23838 completed (loss: 3.0828871726989746, acc: 0.4615384638309479)
[2025-02-04 00:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:15][root][INFO] - Training Epoch: 2/2, step 20434/23838 completed (loss: 4.5582122802734375, acc: 0.21875)
[2025-02-04 00:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:15][root][INFO] - Training Epoch: 2/2, step 20435/23838 completed (loss: 2.8962771892547607, acc: 0.5714285969734192)
[2025-02-04 00:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:16][root][INFO] - Training Epoch: 2/2, step 20436/23838 completed (loss: 3.4718799591064453, acc: 0.3448275923728943)
[2025-02-04 00:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:16][root][INFO] - Training Epoch: 2/2, step 20437/23838 completed (loss: 3.538555145263672, acc: 0.4615384638309479)
[2025-02-04 00:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:16][root][INFO] - Training Epoch: 2/2, step 20438/23838 completed (loss: 3.0362143516540527, acc: 0.4000000059604645)
[2025-02-04 00:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:17][root][INFO] - Training Epoch: 2/2, step 20439/23838 completed (loss: 4.780539035797119, acc: 0.22580644488334656)
[2025-02-04 00:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:17][root][INFO] - Training Epoch: 2/2, step 20440/23838 completed (loss: 2.095156192779541, acc: 0.4375)
[2025-02-04 00:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:18][root][INFO] - Training Epoch: 2/2, step 20441/23838 completed (loss: 3.559896945953369, acc: 0.40740740299224854)
[2025-02-04 00:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:18][root][INFO] - Training Epoch: 2/2, step 20442/23838 completed (loss: 3.3605406284332275, acc: 0.43478259444236755)
[2025-02-04 00:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:19][root][INFO] - Training Epoch: 2/2, step 20443/23838 completed (loss: 3.881991386413574, acc: 0.27272728085517883)
[2025-02-04 00:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:19][root][INFO] - Training Epoch: 2/2, step 20444/23838 completed (loss: 2.612837553024292, acc: 0.6470588445663452)
[2025-02-04 00:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:19][root][INFO] - Training Epoch: 2/2, step 20445/23838 completed (loss: 2.2013964653015137, acc: 0.6086956262588501)
[2025-02-04 00:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:20][root][INFO] - Training Epoch: 2/2, step 20446/23838 completed (loss: 2.8175055980682373, acc: 0.5357142686843872)
[2025-02-04 00:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:20][root][INFO] - Training Epoch: 2/2, step 20447/23838 completed (loss: 2.016430139541626, acc: 0.4166666567325592)
[2025-02-04 00:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:21][root][INFO] - Training Epoch: 2/2, step 20448/23838 completed (loss: 4.6674089431762695, acc: 0.22727273404598236)
[2025-02-04 00:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:21][root][INFO] - Training Epoch: 2/2, step 20449/23838 completed (loss: 4.5278191566467285, acc: 0.4285714328289032)
[2025-02-04 00:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:21][root][INFO] - Training Epoch: 2/2, step 20450/23838 completed (loss: 2.7430574893951416, acc: 0.5714285969734192)
[2025-02-04 00:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:22][root][INFO] - Training Epoch: 2/2, step 20451/23838 completed (loss: 3.632209539413452, acc: 0.4000000059604645)
[2025-02-04 00:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:22][root][INFO] - Training Epoch: 2/2, step 20452/23838 completed (loss: 3.8506600856781006, acc: 0.3243243098258972)
[2025-02-04 00:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:23][root][INFO] - Training Epoch: 2/2, step 20453/23838 completed (loss: 3.8346188068389893, acc: 0.3333333432674408)
[2025-02-04 00:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:23][root][INFO] - Training Epoch: 2/2, step 20454/23838 completed (loss: 3.970686912536621, acc: 0.260869562625885)
[2025-02-04 00:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:23][root][INFO] - Training Epoch: 2/2, step 20455/23838 completed (loss: 3.6177899837493896, acc: 0.2800000011920929)
[2025-02-04 00:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:24][root][INFO] - Training Epoch: 2/2, step 20456/23838 completed (loss: 3.8964526653289795, acc: 0.2857142984867096)
[2025-02-04 00:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:24][root][INFO] - Training Epoch: 2/2, step 20457/23838 completed (loss: 2.6109461784362793, acc: 0.47058823704719543)
[2025-02-04 00:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:25][root][INFO] - Training Epoch: 2/2, step 20458/23838 completed (loss: 3.693455934524536, acc: 0.3333333432674408)
[2025-02-04 00:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:25][root][INFO] - Training Epoch: 2/2, step 20459/23838 completed (loss: 2.6420297622680664, acc: 0.4444444477558136)
[2025-02-04 00:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:26][root][INFO] - Training Epoch: 2/2, step 20460/23838 completed (loss: 1.7612202167510986, acc: 0.692307710647583)
[2025-02-04 00:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:26][root][INFO] - Training Epoch: 2/2, step 20461/23838 completed (loss: 3.1271626949310303, acc: 0.3461538553237915)
[2025-02-04 00:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:27][root][INFO] - Training Epoch: 2/2, step 20462/23838 completed (loss: 2.452219247817993, acc: 0.4000000059604645)
[2025-02-04 00:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:27][root][INFO] - Training Epoch: 2/2, step 20463/23838 completed (loss: 1.4670330286026, acc: 0.6666666865348816)
[2025-02-04 00:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:27][root][INFO] - Training Epoch: 2/2, step 20464/23838 completed (loss: 2.818885326385498, acc: 0.3333333432674408)
[2025-02-04 00:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:28][root][INFO] - Training Epoch: 2/2, step 20465/23838 completed (loss: 3.5972275733947754, acc: 0.20000000298023224)
[2025-02-04 00:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:28][root][INFO] - Training Epoch: 2/2, step 20466/23838 completed (loss: 2.6767537593841553, acc: 0.3478260934352875)
[2025-02-04 00:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:29][root][INFO] - Training Epoch: 2/2, step 20467/23838 completed (loss: 3.0305683612823486, acc: 0.44999998807907104)
[2025-02-04 00:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:29][root][INFO] - Training Epoch: 2/2, step 20468/23838 completed (loss: 3.046644449234009, acc: 0.4375)
[2025-02-04 00:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:30][root][INFO] - Training Epoch: 2/2, step 20469/23838 completed (loss: 4.778927803039551, acc: 0.3125)
[2025-02-04 00:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:30][root][INFO] - Training Epoch: 2/2, step 20470/23838 completed (loss: 3.147056818008423, acc: 0.3461538553237915)
[2025-02-04 00:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:31][root][INFO] - Training Epoch: 2/2, step 20471/23838 completed (loss: 2.5668532848358154, acc: 0.5882353186607361)
[2025-02-04 00:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:31][root][INFO] - Training Epoch: 2/2, step 20472/23838 completed (loss: 2.744555711746216, acc: 0.4166666567325592)
[2025-02-04 00:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:32][root][INFO] - Training Epoch: 2/2, step 20473/23838 completed (loss: 2.622073173522949, acc: 0.4761904776096344)
[2025-02-04 00:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:32][root][INFO] - Training Epoch: 2/2, step 20474/23838 completed (loss: 2.48496675491333, acc: 0.3636363744735718)
[2025-02-04 00:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:32][root][INFO] - Training Epoch: 2/2, step 20475/23838 completed (loss: 3.31463885307312, acc: 0.4444444477558136)
[2025-02-04 00:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:33][root][INFO] - Training Epoch: 2/2, step 20476/23838 completed (loss: 2.007005214691162, acc: 0.550000011920929)
[2025-02-04 00:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:33][root][INFO] - Training Epoch: 2/2, step 20477/23838 completed (loss: 2.189136028289795, acc: 0.6153846383094788)
[2025-02-04 00:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:34][root][INFO] - Training Epoch: 2/2, step 20478/23838 completed (loss: 4.152904033660889, acc: 0.2857142984867096)
[2025-02-04 00:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:34][root][INFO] - Training Epoch: 2/2, step 20479/23838 completed (loss: 2.6747255325317383, acc: 0.3333333432674408)
[2025-02-04 00:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:35][root][INFO] - Training Epoch: 2/2, step 20480/23838 completed (loss: 2.9390499591827393, acc: 0.3333333432674408)
[2025-02-04 00:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:35][root][INFO] - Training Epoch: 2/2, step 20481/23838 completed (loss: 3.134531259536743, acc: 0.25)
[2025-02-04 00:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:35][root][INFO] - Training Epoch: 2/2, step 20482/23838 completed (loss: 1.8773998022079468, acc: 0.5833333134651184)
[2025-02-04 00:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:36][root][INFO] - Training Epoch: 2/2, step 20483/23838 completed (loss: 2.3844993114471436, acc: 0.5625)
[2025-02-04 00:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:36][root][INFO] - Training Epoch: 2/2, step 20484/23838 completed (loss: 4.0451202392578125, acc: 0.3499999940395355)
[2025-02-04 00:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:37][root][INFO] - Training Epoch: 2/2, step 20485/23838 completed (loss: 3.265216588973999, acc: 0.4285714328289032)
[2025-02-04 00:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:37][root][INFO] - Training Epoch: 2/2, step 20486/23838 completed (loss: 2.4858882427215576, acc: 0.5384615659713745)
[2025-02-04 00:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:38][root][INFO] - Training Epoch: 2/2, step 20487/23838 completed (loss: 3.947519540786743, acc: 0.3333333432674408)
[2025-02-04 00:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:38][root][INFO] - Training Epoch: 2/2, step 20488/23838 completed (loss: 3.86503005027771, acc: 0.3928571343421936)
[2025-02-04 00:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:38][root][INFO] - Training Epoch: 2/2, step 20489/23838 completed (loss: 2.7925522327423096, acc: 0.5416666865348816)
[2025-02-04 00:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:39][root][INFO] - Training Epoch: 2/2, step 20490/23838 completed (loss: 4.778383731842041, acc: 0.1111111119389534)
[2025-02-04 00:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:39][root][INFO] - Training Epoch: 2/2, step 20491/23838 completed (loss: 2.3358373641967773, acc: 0.5833333134651184)
[2025-02-04 00:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:40][root][INFO] - Training Epoch: 2/2, step 20492/23838 completed (loss: 3.0116264820098877, acc: 0.43478259444236755)
[2025-02-04 00:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:40][root][INFO] - Training Epoch: 2/2, step 20493/23838 completed (loss: 2.3803365230560303, acc: 0.5714285969734192)
[2025-02-04 00:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:40][root][INFO] - Training Epoch: 2/2, step 20494/23838 completed (loss: 3.7076029777526855, acc: 0.3529411852359772)
[2025-02-04 00:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:41][root][INFO] - Training Epoch: 2/2, step 20495/23838 completed (loss: 3.7259457111358643, acc: 0.3333333432674408)
[2025-02-04 00:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:41][root][INFO] - Training Epoch: 2/2, step 20496/23838 completed (loss: 4.019998550415039, acc: 0.24074074625968933)
[2025-02-04 00:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:42][root][INFO] - Training Epoch: 2/2, step 20497/23838 completed (loss: 3.3179714679718018, acc: 0.30000001192092896)
[2025-02-04 00:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:42][root][INFO] - Training Epoch: 2/2, step 20498/23838 completed (loss: 3.3713486194610596, acc: 0.38235294818878174)
[2025-02-04 00:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:42][root][INFO] - Training Epoch: 2/2, step 20499/23838 completed (loss: 3.611567258834839, acc: 0.2647058963775635)
[2025-02-04 00:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:43][root][INFO] - Training Epoch: 2/2, step 20500/23838 completed (loss: 3.5355165004730225, acc: 0.30434781312942505)
[2025-02-04 00:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:43][root][INFO] - Training Epoch: 2/2, step 20501/23838 completed (loss: 3.3201024532318115, acc: 0.27906978130340576)
[2025-02-04 00:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:44][root][INFO] - Training Epoch: 2/2, step 20502/23838 completed (loss: 2.898711919784546, acc: 0.36000001430511475)
[2025-02-04 00:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:44][root][INFO] - Training Epoch: 2/2, step 20503/23838 completed (loss: 3.4109320640563965, acc: 0.302325576543808)
[2025-02-04 00:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:45][root][INFO] - Training Epoch: 2/2, step 20504/23838 completed (loss: 3.5834038257598877, acc: 0.26829269528388977)
[2025-02-04 00:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:45][root][INFO] - Training Epoch: 2/2, step 20505/23838 completed (loss: 3.5629124641418457, acc: 0.38461539149284363)
[2025-02-04 00:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:45][root][INFO] - Training Epoch: 2/2, step 20506/23838 completed (loss: 3.5356521606445312, acc: 0.3333333432674408)
[2025-02-04 00:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:46][root][INFO] - Training Epoch: 2/2, step 20507/23838 completed (loss: 3.1893415451049805, acc: 0.3333333432674408)
[2025-02-04 00:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:46][root][INFO] - Training Epoch: 2/2, step 20508/23838 completed (loss: 3.2630226612091064, acc: 0.22580644488334656)
[2025-02-04 00:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:47][root][INFO] - Training Epoch: 2/2, step 20509/23838 completed (loss: 3.014796733856201, acc: 0.2857142984867096)
[2025-02-04 00:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:47][root][INFO] - Training Epoch: 2/2, step 20510/23838 completed (loss: 3.590073823928833, acc: 0.36000001430511475)
[2025-02-04 00:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:47][root][INFO] - Training Epoch: 2/2, step 20511/23838 completed (loss: 2.883469581604004, acc: 0.4399999976158142)
[2025-02-04 00:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:48][root][INFO] - Training Epoch: 2/2, step 20512/23838 completed (loss: 3.2297492027282715, acc: 0.30000001192092896)
[2025-02-04 00:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:48][root][INFO] - Training Epoch: 2/2, step 20513/23838 completed (loss: 4.1816911697387695, acc: 0.20000000298023224)
[2025-02-04 00:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:49][root][INFO] - Training Epoch: 2/2, step 20514/23838 completed (loss: 3.7276203632354736, acc: 0.2884615361690521)
[2025-02-04 00:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:49][root][INFO] - Training Epoch: 2/2, step 20515/23838 completed (loss: 3.58636212348938, acc: 0.3333333432674408)
[2025-02-04 00:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:49][root][INFO] - Training Epoch: 2/2, step 20516/23838 completed (loss: 3.5884084701538086, acc: 0.4000000059604645)
[2025-02-04 00:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:50][root][INFO] - Training Epoch: 2/2, step 20517/23838 completed (loss: 3.7707748413085938, acc: 0.47058823704719543)
[2025-02-04 00:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:50][root][INFO] - Training Epoch: 2/2, step 20518/23838 completed (loss: 3.842926502227783, acc: 0.2380952388048172)
[2025-02-04 00:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:51][root][INFO] - Training Epoch: 2/2, step 20519/23838 completed (loss: 3.6199374198913574, acc: 0.3333333432674408)
[2025-02-04 00:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:51][root][INFO] - Training Epoch: 2/2, step 20520/23838 completed (loss: 3.664541244506836, acc: 0.3142857253551483)
[2025-02-04 00:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:52][root][INFO] - Training Epoch: 2/2, step 20521/23838 completed (loss: 2.4533369541168213, acc: 0.5)
[2025-02-04 00:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:52][root][INFO] - Training Epoch: 2/2, step 20522/23838 completed (loss: 3.264310359954834, acc: 0.40425533056259155)
[2025-02-04 00:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:53][root][INFO] - Training Epoch: 2/2, step 20523/23838 completed (loss: 3.0739736557006836, acc: 0.29729729890823364)
[2025-02-04 00:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:53][root][INFO] - Training Epoch: 2/2, step 20524/23838 completed (loss: 3.4641032218933105, acc: 0.29032257199287415)
[2025-02-04 00:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:53][root][INFO] - Training Epoch: 2/2, step 20525/23838 completed (loss: 3.5959935188293457, acc: 0.3333333432674408)
[2025-02-04 00:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:54][root][INFO] - Training Epoch: 2/2, step 20526/23838 completed (loss: 3.1097700595855713, acc: 0.375)
[2025-02-04 00:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:54][root][INFO] - Training Epoch: 2/2, step 20527/23838 completed (loss: 3.814879894256592, acc: 0.25641027092933655)
[2025-02-04 00:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:55][root][INFO] - Training Epoch: 2/2, step 20528/23838 completed (loss: 4.299862384796143, acc: 0.17391304671764374)
[2025-02-04 00:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:55][root][INFO] - Training Epoch: 2/2, step 20529/23838 completed (loss: 3.6872761249542236, acc: 0.2432432472705841)
[2025-02-04 00:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:55][root][INFO] - Training Epoch: 2/2, step 20530/23838 completed (loss: 3.1444830894470215, acc: 0.3199999928474426)
[2025-02-04 00:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:56][root][INFO] - Training Epoch: 2/2, step 20531/23838 completed (loss: 3.2401392459869385, acc: 0.27272728085517883)
[2025-02-04 00:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:56][root][INFO] - Training Epoch: 2/2, step 20532/23838 completed (loss: 4.057850360870361, acc: 0.21052631735801697)
[2025-02-04 00:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:57][root][INFO] - Training Epoch: 2/2, step 20533/23838 completed (loss: 3.287778854370117, acc: 0.375)
[2025-02-04 00:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:57][root][INFO] - Training Epoch: 2/2, step 20534/23838 completed (loss: 3.0592052936553955, acc: 0.29629629850387573)
[2025-02-04 00:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:58][root][INFO] - Training Epoch: 2/2, step 20535/23838 completed (loss: 3.436708688735962, acc: 0.2448979616165161)
[2025-02-04 00:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:58][root][INFO] - Training Epoch: 2/2, step 20536/23838 completed (loss: 2.815976619720459, acc: 0.4838709533214569)
[2025-02-04 00:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:58][root][INFO] - Training Epoch: 2/2, step 20537/23838 completed (loss: 3.096203565597534, acc: 0.4137931168079376)
[2025-02-04 00:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:59][root][INFO] - Training Epoch: 2/2, step 20538/23838 completed (loss: 3.6542181968688965, acc: 0.4193548262119293)
[2025-02-04 00:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:52:59][root][INFO] - Training Epoch: 2/2, step 20539/23838 completed (loss: 3.5071799755096436, acc: 0.3958333432674408)
[2025-02-04 00:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:00][root][INFO] - Training Epoch: 2/2, step 20540/23838 completed (loss: 3.6127240657806396, acc: 0.3142857253551483)
[2025-02-04 00:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:00][root][INFO] - Training Epoch: 2/2, step 20541/23838 completed (loss: 3.6210427284240723, acc: 0.30000001192092896)
[2025-02-04 00:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:00][root][INFO] - Training Epoch: 2/2, step 20542/23838 completed (loss: 3.4264981746673584, acc: 0.4583333432674408)
[2025-02-04 00:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:01][root][INFO] - Training Epoch: 2/2, step 20543/23838 completed (loss: 3.9252769947052, acc: 0.2537313401699066)
[2025-02-04 00:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:01][root][INFO] - Training Epoch: 2/2, step 20544/23838 completed (loss: 3.288658857345581, acc: 0.3478260934352875)
[2025-02-04 00:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:01][root][INFO] - Training Epoch: 2/2, step 20545/23838 completed (loss: 3.0586776733398438, acc: 0.2631579041481018)
[2025-02-04 00:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:02][root][INFO] - Training Epoch: 2/2, step 20546/23838 completed (loss: 2.819056510925293, acc: 0.5)
[2025-02-04 00:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:02][root][INFO] - Training Epoch: 2/2, step 20547/23838 completed (loss: 3.0344619750976562, acc: 0.4047619104385376)
[2025-02-04 00:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:03][root][INFO] - Training Epoch: 2/2, step 20548/23838 completed (loss: 3.2077808380126953, acc: 0.4000000059604645)
[2025-02-04 00:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:03][root][INFO] - Training Epoch: 2/2, step 20549/23838 completed (loss: 3.679835081100464, acc: 0.39534884691238403)
[2025-02-04 00:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:04][root][INFO] - Training Epoch: 2/2, step 20550/23838 completed (loss: 3.5848841667175293, acc: 0.21621622145175934)
[2025-02-04 00:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:04][root][INFO] - Training Epoch: 2/2, step 20551/23838 completed (loss: 3.5860507488250732, acc: 0.2857142984867096)
[2025-02-04 00:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:04][root][INFO] - Training Epoch: 2/2, step 20552/23838 completed (loss: 3.654409646987915, acc: 0.2266666740179062)
[2025-02-04 00:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:05][root][INFO] - Training Epoch: 2/2, step 20553/23838 completed (loss: 3.406214952468872, acc: 0.2195121943950653)
[2025-02-04 00:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:05][root][INFO] - Training Epoch: 2/2, step 20554/23838 completed (loss: 3.9235119819641113, acc: 0.3103448152542114)
[2025-02-04 00:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:06][root][INFO] - Training Epoch: 2/2, step 20555/23838 completed (loss: 4.1134257316589355, acc: 0.2857142984867096)
[2025-02-04 00:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:06][root][INFO] - Training Epoch: 2/2, step 20556/23838 completed (loss: 2.8405251502990723, acc: 0.3513513505458832)
[2025-02-04 00:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:07][root][INFO] - Training Epoch: 2/2, step 20557/23838 completed (loss: 3.703122615814209, acc: 0.31578946113586426)
[2025-02-04 00:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:07][root][INFO] - Training Epoch: 2/2, step 20558/23838 completed (loss: 2.8285863399505615, acc: 0.47826087474823)
[2025-02-04 00:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:07][root][INFO] - Training Epoch: 2/2, step 20559/23838 completed (loss: 3.398238182067871, acc: 0.39534884691238403)
[2025-02-04 00:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:08][root][INFO] - Training Epoch: 2/2, step 20560/23838 completed (loss: 2.795531749725342, acc: 0.3571428656578064)
[2025-02-04 00:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:08][root][INFO] - Training Epoch: 2/2, step 20561/23838 completed (loss: 3.624202013015747, acc: 0.31578946113586426)
[2025-02-04 00:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:09][root][INFO] - Training Epoch: 2/2, step 20562/23838 completed (loss: 3.003615617752075, acc: 0.3947368562221527)
[2025-02-04 00:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:09][root][INFO] - Training Epoch: 2/2, step 20563/23838 completed (loss: 3.4664957523345947, acc: 0.2777777910232544)
[2025-02-04 00:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:10][root][INFO] - Training Epoch: 2/2, step 20564/23838 completed (loss: 3.1905510425567627, acc: 0.45945945382118225)
[2025-02-04 00:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:10][root][INFO] - Training Epoch: 2/2, step 20565/23838 completed (loss: 3.6869969367980957, acc: 0.2195121943950653)
[2025-02-04 00:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:10][root][INFO] - Training Epoch: 2/2, step 20566/23838 completed (loss: 2.7334887981414795, acc: 0.42424243688583374)
[2025-02-04 00:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:11][root][INFO] - Training Epoch: 2/2, step 20567/23838 completed (loss: 3.4002268314361572, acc: 0.2916666567325592)
[2025-02-04 00:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:11][root][INFO] - Training Epoch: 2/2, step 20568/23838 completed (loss: 3.6484169960021973, acc: 0.3414634168148041)
[2025-02-04 00:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:12][root][INFO] - Training Epoch: 2/2, step 20569/23838 completed (loss: 2.7089755535125732, acc: 0.48571428656578064)
[2025-02-04 00:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:12][root][INFO] - Training Epoch: 2/2, step 20570/23838 completed (loss: 3.5009703636169434, acc: 0.3636363744735718)
[2025-02-04 00:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:13][root][INFO] - Training Epoch: 2/2, step 20571/23838 completed (loss: 2.5887513160705566, acc: 0.3947368562221527)
[2025-02-04 00:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:13][root][INFO] - Training Epoch: 2/2, step 20572/23838 completed (loss: 3.2932538986206055, acc: 0.3636363744735718)
[2025-02-04 00:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:14][root][INFO] - Training Epoch: 2/2, step 20573/23838 completed (loss: 2.9725606441497803, acc: 0.3928571343421936)
[2025-02-04 00:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:14][root][INFO] - Training Epoch: 2/2, step 20574/23838 completed (loss: 1.7013680934906006, acc: 0.5925925970077515)
[2025-02-04 00:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:15][root][INFO] - Training Epoch: 2/2, step 20575/23838 completed (loss: 2.467719554901123, acc: 0.375)
[2025-02-04 00:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:15][root][INFO] - Training Epoch: 2/2, step 20576/23838 completed (loss: 3.0012571811676025, acc: 0.37931033968925476)
[2025-02-04 00:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:15][root][INFO] - Training Epoch: 2/2, step 20577/23838 completed (loss: 3.8521435260772705, acc: 0.34285715222358704)
[2025-02-04 00:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:16][root][INFO] - Training Epoch: 2/2, step 20578/23838 completed (loss: 3.5040431022644043, acc: 0.27272728085517883)
[2025-02-04 00:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:16][root][INFO] - Training Epoch: 2/2, step 20579/23838 completed (loss: 4.223039627075195, acc: 0.3255814015865326)
[2025-02-04 00:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:17][root][INFO] - Training Epoch: 2/2, step 20580/23838 completed (loss: 3.504786729812622, acc: 0.35483869910240173)
[2025-02-04 00:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:17][root][INFO] - Training Epoch: 2/2, step 20581/23838 completed (loss: 2.5307748317718506, acc: 0.4736842215061188)
[2025-02-04 00:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:18][root][INFO] - Training Epoch: 2/2, step 20582/23838 completed (loss: 3.3699469566345215, acc: 0.48275861144065857)
[2025-02-04 00:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:18][root][INFO] - Training Epoch: 2/2, step 20583/23838 completed (loss: 3.8964009284973145, acc: 0.2666666805744171)
[2025-02-04 00:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:19][root][INFO] - Training Epoch: 2/2, step 20584/23838 completed (loss: 2.546149492263794, acc: 0.4848484992980957)
[2025-02-04 00:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:19][root][INFO] - Training Epoch: 2/2, step 20585/23838 completed (loss: 2.9807450771331787, acc: 0.3589743673801422)
[2025-02-04 00:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:19][root][INFO] - Training Epoch: 2/2, step 20586/23838 completed (loss: 3.653075933456421, acc: 0.2978723347187042)
[2025-02-04 00:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:20][root][INFO] - Training Epoch: 2/2, step 20587/23838 completed (loss: 4.129230499267578, acc: 0.2800000011920929)
[2025-02-04 00:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:20][root][INFO] - Training Epoch: 2/2, step 20588/23838 completed (loss: 3.156808853149414, acc: 0.3571428656578064)
[2025-02-04 00:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:21][root][INFO] - Training Epoch: 2/2, step 20589/23838 completed (loss: 2.3856332302093506, acc: 0.5714285969734192)
[2025-02-04 00:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:21][root][INFO] - Training Epoch: 2/2, step 20590/23838 completed (loss: 3.74285626411438, acc: 0.3125)
[2025-02-04 00:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:21][root][INFO] - Training Epoch: 2/2, step 20591/23838 completed (loss: 2.999437093734741, acc: 0.3877550959587097)
[2025-02-04 00:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:22][root][INFO] - Training Epoch: 2/2, step 20592/23838 completed (loss: 3.0616953372955322, acc: 0.42307692766189575)
[2025-02-04 00:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:22][root][INFO] - Training Epoch: 2/2, step 20593/23838 completed (loss: 2.8834786415100098, acc: 0.34375)
[2025-02-04 00:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:23][root][INFO] - Training Epoch: 2/2, step 20594/23838 completed (loss: 3.2474701404571533, acc: 0.27906978130340576)
[2025-02-04 00:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:23][root][INFO] - Training Epoch: 2/2, step 20595/23838 completed (loss: 2.9051876068115234, acc: 0.2777777910232544)
[2025-02-04 00:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:23][root][INFO] - Training Epoch: 2/2, step 20596/23838 completed (loss: 3.2036850452423096, acc: 0.42500001192092896)
[2025-02-04 00:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:24][root][INFO] - Training Epoch: 2/2, step 20597/23838 completed (loss: 3.5630717277526855, acc: 0.3142857253551483)
[2025-02-04 00:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:24][root][INFO] - Training Epoch: 2/2, step 20598/23838 completed (loss: 3.5987815856933594, acc: 0.3055555522441864)
[2025-02-04 00:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:25][root][INFO] - Training Epoch: 2/2, step 20599/23838 completed (loss: 2.773115634918213, acc: 0.3255814015865326)
[2025-02-04 00:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:25][root][INFO] - Training Epoch: 2/2, step 20600/23838 completed (loss: 3.491628885269165, acc: 0.32608696818351746)
[2025-02-04 00:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:26][root][INFO] - Training Epoch: 2/2, step 20601/23838 completed (loss: 3.668551445007324, acc: 0.28787878155708313)
[2025-02-04 00:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:26][root][INFO] - Training Epoch: 2/2, step 20602/23838 completed (loss: 3.0045418739318848, acc: 0.41860464215278625)
[2025-02-04 00:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:26][root][INFO] - Training Epoch: 2/2, step 20603/23838 completed (loss: 3.5859014987945557, acc: 0.25)
[2025-02-04 00:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:27][root][INFO] - Training Epoch: 2/2, step 20604/23838 completed (loss: 2.9650745391845703, acc: 0.34375)
[2025-02-04 00:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:27][root][INFO] - Training Epoch: 2/2, step 20605/23838 completed (loss: 3.3908071517944336, acc: 0.27272728085517883)
[2025-02-04 00:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:28][root][INFO] - Training Epoch: 2/2, step 20606/23838 completed (loss: 2.290407657623291, acc: 0.5416666865348816)
[2025-02-04 00:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:28][root][INFO] - Training Epoch: 2/2, step 20607/23838 completed (loss: 4.231166362762451, acc: 0.2857142984867096)
[2025-02-04 00:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:29][root][INFO] - Training Epoch: 2/2, step 20608/23838 completed (loss: 3.5002222061157227, acc: 0.24390244483947754)
[2025-02-04 00:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:29][root][INFO] - Training Epoch: 2/2, step 20609/23838 completed (loss: 4.640867710113525, acc: 0.20000000298023224)
[2025-02-04 00:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:29][root][INFO] - Training Epoch: 2/2, step 20610/23838 completed (loss: 3.8054850101470947, acc: 0.3333333432674408)
[2025-02-04 00:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:30][root][INFO] - Training Epoch: 2/2, step 20611/23838 completed (loss: 4.086074352264404, acc: 0.2222222238779068)
[2025-02-04 00:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:30][root][INFO] - Training Epoch: 2/2, step 20612/23838 completed (loss: 5.426092624664307, acc: 0.1764705926179886)
[2025-02-04 00:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:31][root][INFO] - Training Epoch: 2/2, step 20613/23838 completed (loss: 4.239444255828857, acc: 0.3888888955116272)
[2025-02-04 00:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:31][root][INFO] - Training Epoch: 2/2, step 20614/23838 completed (loss: 4.153690814971924, acc: 0.13636364042758942)
[2025-02-04 00:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:32][root][INFO] - Training Epoch: 2/2, step 20615/23838 completed (loss: 4.739558219909668, acc: 0.1666666716337204)
[2025-02-04 00:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:32][root][INFO] - Training Epoch: 2/2, step 20616/23838 completed (loss: 3.5179405212402344, acc: 0.3181818127632141)
[2025-02-04 00:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:32][root][INFO] - Training Epoch: 2/2, step 20617/23838 completed (loss: 4.402456760406494, acc: 0.30434781312942505)
[2025-02-04 00:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:33][root][INFO] - Training Epoch: 2/2, step 20618/23838 completed (loss: 4.169336795806885, acc: 0.3199999928474426)
[2025-02-04 00:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:33][root][INFO] - Training Epoch: 2/2, step 20619/23838 completed (loss: 2.8407487869262695, acc: 0.3103448152542114)
[2025-02-04 00:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:34][root][INFO] - Training Epoch: 2/2, step 20620/23838 completed (loss: 4.857873439788818, acc: 0.3333333432674408)
[2025-02-04 00:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:34][root][INFO] - Training Epoch: 2/2, step 20621/23838 completed (loss: 3.9510128498077393, acc: 0.380952388048172)
[2025-02-04 00:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:35][root][INFO] - Training Epoch: 2/2, step 20622/23838 completed (loss: 4.022436618804932, acc: 0.2777777910232544)
[2025-02-04 00:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:35][root][INFO] - Training Epoch: 2/2, step 20623/23838 completed (loss: 3.682429552078247, acc: 0.5)
[2025-02-04 00:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:36][root][INFO] - Training Epoch: 2/2, step 20624/23838 completed (loss: 2.7469124794006348, acc: 0.5)
[2025-02-04 00:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:36][root][INFO] - Training Epoch: 2/2, step 20625/23838 completed (loss: 4.1746721267700195, acc: 0.2631579041481018)
[2025-02-04 00:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:36][root][INFO] - Training Epoch: 2/2, step 20626/23838 completed (loss: 4.021838665008545, acc: 0.29032257199287415)
[2025-02-04 00:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:37][root][INFO] - Training Epoch: 2/2, step 20627/23838 completed (loss: 3.6085855960845947, acc: 0.4642857015132904)
[2025-02-04 00:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:37][root][INFO] - Training Epoch: 2/2, step 20628/23838 completed (loss: 4.654886245727539, acc: 0.36000001430511475)
[2025-02-04 00:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:38][root][INFO] - Training Epoch: 2/2, step 20629/23838 completed (loss: 3.1150074005126953, acc: 0.3571428656578064)
[2025-02-04 00:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:38][root][INFO] - Training Epoch: 2/2, step 20630/23838 completed (loss: 3.168848991394043, acc: 0.4193548262119293)
[2025-02-04 00:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:38][root][INFO] - Training Epoch: 2/2, step 20631/23838 completed (loss: 2.747422695159912, acc: 0.5)
[2025-02-04 00:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:39][root][INFO] - Training Epoch: 2/2, step 20632/23838 completed (loss: 4.001931190490723, acc: 0.30000001192092896)
[2025-02-04 00:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:39][root][INFO] - Training Epoch: 2/2, step 20633/23838 completed (loss: 4.715091705322266, acc: 0.2068965584039688)
[2025-02-04 00:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:40][root][INFO] - Training Epoch: 2/2, step 20634/23838 completed (loss: 4.432643413543701, acc: 0.25)
[2025-02-04 00:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:40][root][INFO] - Training Epoch: 2/2, step 20635/23838 completed (loss: 4.12454080581665, acc: 0.3076923191547394)
[2025-02-04 00:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:40][root][INFO] - Training Epoch: 2/2, step 20636/23838 completed (loss: 4.103496074676514, acc: 0.375)
[2025-02-04 00:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:41][root][INFO] - Training Epoch: 2/2, step 20637/23838 completed (loss: 3.5946435928344727, acc: 0.34285715222358704)
[2025-02-04 00:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:41][root][INFO] - Training Epoch: 2/2, step 20638/23838 completed (loss: 3.118215322494507, acc: 0.40909090638160706)
[2025-02-04 00:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:42][root][INFO] - Training Epoch: 2/2, step 20639/23838 completed (loss: 3.2359530925750732, acc: 0.26923078298568726)
[2025-02-04 00:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:42][root][INFO] - Training Epoch: 2/2, step 20640/23838 completed (loss: 2.463806629180908, acc: 0.5)
[2025-02-04 00:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:43][root][INFO] - Training Epoch: 2/2, step 20641/23838 completed (loss: 3.418429136276245, acc: 0.3214285671710968)
[2025-02-04 00:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:43][root][INFO] - Training Epoch: 2/2, step 20642/23838 completed (loss: 5.044085502624512, acc: 0.2222222238779068)
[2025-02-04 00:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:44][root][INFO] - Training Epoch: 2/2, step 20643/23838 completed (loss: 4.101006984710693, acc: 0.3199999928474426)
[2025-02-04 00:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:44][root][INFO] - Training Epoch: 2/2, step 20644/23838 completed (loss: 2.1529722213745117, acc: 0.5)
[2025-02-04 00:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:45][root][INFO] - Training Epoch: 2/2, step 20645/23838 completed (loss: 2.978881597518921, acc: 0.4399999976158142)
[2025-02-04 00:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:45][root][INFO] - Training Epoch: 2/2, step 20646/23838 completed (loss: 3.4551403522491455, acc: 0.2857142984867096)
[2025-02-04 00:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:45][root][INFO] - Training Epoch: 2/2, step 20647/23838 completed (loss: 4.34116792678833, acc: 0.23076923191547394)
[2025-02-04 00:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:46][root][INFO] - Training Epoch: 2/2, step 20648/23838 completed (loss: 4.234230995178223, acc: 0.3181818127632141)
[2025-02-04 00:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:46][root][INFO] - Training Epoch: 2/2, step 20649/23838 completed (loss: 2.8658649921417236, acc: 0.4285714328289032)
[2025-02-04 00:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:47][root][INFO] - Training Epoch: 2/2, step 20650/23838 completed (loss: 3.524303674697876, acc: 0.4166666567325592)
[2025-02-04 00:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:47][root][INFO] - Training Epoch: 2/2, step 20651/23838 completed (loss: 3.799229145050049, acc: 0.3333333432674408)
[2025-02-04 00:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:48][root][INFO] - Training Epoch: 2/2, step 20652/23838 completed (loss: 4.003830909729004, acc: 0.24137930572032928)
[2025-02-04 00:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:49][root][INFO] - Training Epoch: 2/2, step 20653/23838 completed (loss: 4.521368503570557, acc: 0.38235294818878174)
[2025-02-04 00:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:49][root][INFO] - Training Epoch: 2/2, step 20654/23838 completed (loss: 4.231560230255127, acc: 0.3076923191547394)
[2025-02-04 00:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:49][root][INFO] - Training Epoch: 2/2, step 20655/23838 completed (loss: 2.7793331146240234, acc: 0.5)
[2025-02-04 00:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:50][root][INFO] - Training Epoch: 2/2, step 20656/23838 completed (loss: 2.692180633544922, acc: 0.3684210479259491)
[2025-02-04 00:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:50][root][INFO] - Training Epoch: 2/2, step 20657/23838 completed (loss: 3.9071483612060547, acc: 0.4166666567325592)
[2025-02-04 00:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:51][root][INFO] - Training Epoch: 2/2, step 20658/23838 completed (loss: 3.140434503555298, acc: 0.4000000059604645)
[2025-02-04 00:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:51][root][INFO] - Training Epoch: 2/2, step 20659/23838 completed (loss: 4.386852741241455, acc: 0.2083333283662796)
[2025-02-04 00:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:52][root][INFO] - Training Epoch: 2/2, step 20660/23838 completed (loss: 3.326458692550659, acc: 0.4583333432674408)
[2025-02-04 00:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:52][root][INFO] - Training Epoch: 2/2, step 20661/23838 completed (loss: 5.0936279296875, acc: 0.1785714328289032)
[2025-02-04 00:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:53][root][INFO] - Training Epoch: 2/2, step 20662/23838 completed (loss: 2.72926926612854, acc: 0.5909090638160706)
[2025-02-04 00:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:53][root][INFO] - Training Epoch: 2/2, step 20663/23838 completed (loss: 3.5737171173095703, acc: 0.4583333432674408)
[2025-02-04 00:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:54][root][INFO] - Training Epoch: 2/2, step 20664/23838 completed (loss: 3.597203016281128, acc: 0.4137931168079376)
[2025-02-04 00:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:54][root][INFO] - Training Epoch: 2/2, step 20665/23838 completed (loss: 2.9419829845428467, acc: 0.5)
[2025-02-04 00:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:55][root][INFO] - Training Epoch: 2/2, step 20666/23838 completed (loss: 4.407576560974121, acc: 0.23076923191547394)
[2025-02-04 00:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:55][root][INFO] - Training Epoch: 2/2, step 20667/23838 completed (loss: 3.7037007808685303, acc: 0.2631579041481018)
[2025-02-04 00:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:55][root][INFO] - Training Epoch: 2/2, step 20668/23838 completed (loss: 3.392188549041748, acc: 0.42307692766189575)
[2025-02-04 00:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:56][root][INFO] - Training Epoch: 2/2, step 20669/23838 completed (loss: 2.878417491912842, acc: 0.5)
[2025-02-04 00:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:56][root][INFO] - Training Epoch: 2/2, step 20670/23838 completed (loss: 2.9286694526672363, acc: 0.5)
[2025-02-04 00:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:57][root][INFO] - Training Epoch: 2/2, step 20671/23838 completed (loss: 3.1873559951782227, acc: 0.260869562625885)
[2025-02-04 00:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:57][root][INFO] - Training Epoch: 2/2, step 20672/23838 completed (loss: 4.358561992645264, acc: 0.261904776096344)
[2025-02-04 00:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:58][root][INFO] - Training Epoch: 2/2, step 20673/23838 completed (loss: 4.29178524017334, acc: 0.2711864411830902)
[2025-02-04 00:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:58][root][INFO] - Training Epoch: 2/2, step 20674/23838 completed (loss: 4.373485565185547, acc: 0.3469387888908386)
[2025-02-04 00:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:59][root][INFO] - Training Epoch: 2/2, step 20675/23838 completed (loss: 3.317701578140259, acc: 0.3478260934352875)
[2025-02-04 00:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:53:59][root][INFO] - Training Epoch: 2/2, step 20676/23838 completed (loss: 3.2772958278656006, acc: 0.3333333432674408)
[2025-02-04 00:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:00][root][INFO] - Training Epoch: 2/2, step 20677/23838 completed (loss: 3.655366897583008, acc: 0.32499998807907104)
[2025-02-04 00:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:00][root][INFO] - Training Epoch: 2/2, step 20678/23838 completed (loss: 3.128467559814453, acc: 0.34210526943206787)
[2025-02-04 00:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:00][root][INFO] - Training Epoch: 2/2, step 20679/23838 completed (loss: 3.2814338207244873, acc: 0.4000000059604645)
[2025-02-04 00:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:01][root][INFO] - Training Epoch: 2/2, step 20680/23838 completed (loss: 3.638740301132202, acc: 0.39393940567970276)
[2025-02-04 00:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:01][root][INFO] - Training Epoch: 2/2, step 20681/23838 completed (loss: 2.623037099838257, acc: 0.5)
[2025-02-04 00:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:02][root][INFO] - Training Epoch: 2/2, step 20682/23838 completed (loss: 2.9544551372528076, acc: 0.46666666865348816)
[2025-02-04 00:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:02][root][INFO] - Training Epoch: 2/2, step 20683/23838 completed (loss: 2.933690309524536, acc: 0.48275861144065857)
[2025-02-04 00:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:02][root][INFO] - Training Epoch: 2/2, step 20684/23838 completed (loss: 4.078097343444824, acc: 0.3611111044883728)
[2025-02-04 00:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:03][root][INFO] - Training Epoch: 2/2, step 20685/23838 completed (loss: 2.849799156188965, acc: 0.4651162922382355)
[2025-02-04 00:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:03][root][INFO] - Training Epoch: 2/2, step 20686/23838 completed (loss: 2.0459911823272705, acc: 0.6333333253860474)
[2025-02-04 00:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:04][root][INFO] - Training Epoch: 2/2, step 20687/23838 completed (loss: 3.7880589962005615, acc: 0.30158731341362)
[2025-02-04 00:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:04][root][INFO] - Training Epoch: 2/2, step 20688/23838 completed (loss: 3.7192230224609375, acc: 0.380952388048172)
[2025-02-04 00:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:04][root][INFO] - Training Epoch: 2/2, step 20689/23838 completed (loss: 3.207279920578003, acc: 0.3255814015865326)
[2025-02-04 00:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:05][root][INFO] - Training Epoch: 2/2, step 20690/23838 completed (loss: 3.114232301712036, acc: 0.39024388790130615)
[2025-02-04 00:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:05][root][INFO] - Training Epoch: 2/2, step 20691/23838 completed (loss: 3.6922032833099365, acc: 0.3799999952316284)
[2025-02-04 00:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:06][root][INFO] - Training Epoch: 2/2, step 20692/23838 completed (loss: 3.439953565597534, acc: 0.3333333432674408)
[2025-02-04 00:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:06][root][INFO] - Training Epoch: 2/2, step 20693/23838 completed (loss: 4.323710918426514, acc: 0.2954545319080353)
[2025-02-04 00:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:07][root][INFO] - Training Epoch: 2/2, step 20694/23838 completed (loss: 3.518035411834717, acc: 0.38181817531585693)
[2025-02-04 00:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:07][root][INFO] - Training Epoch: 2/2, step 20695/23838 completed (loss: 3.8102903366088867, acc: 0.27906978130340576)
[2025-02-04 00:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:08][root][INFO] - Training Epoch: 2/2, step 20696/23838 completed (loss: 4.011099338531494, acc: 0.37931033968925476)
[2025-02-04 00:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:08][root][INFO] - Training Epoch: 2/2, step 20697/23838 completed (loss: 4.235556125640869, acc: 0.3333333432674408)
[2025-02-04 00:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:09][root][INFO] - Training Epoch: 2/2, step 20698/23838 completed (loss: 3.8245792388916016, acc: 0.4000000059604645)
[2025-02-04 00:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:09][root][INFO] - Training Epoch: 2/2, step 20699/23838 completed (loss: 3.6937389373779297, acc: 0.3265306055545807)
[2025-02-04 00:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:10][root][INFO] - Training Epoch: 2/2, step 20700/23838 completed (loss: 4.158816814422607, acc: 0.3478260934352875)
[2025-02-04 00:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:10][root][INFO] - Training Epoch: 2/2, step 20701/23838 completed (loss: 3.7643628120422363, acc: 0.4838709533214569)
[2025-02-04 00:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:10][root][INFO] - Training Epoch: 2/2, step 20702/23838 completed (loss: 3.2542121410369873, acc: 0.2857142984867096)
[2025-02-04 00:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:11][root][INFO] - Training Epoch: 2/2, step 20703/23838 completed (loss: 4.139286041259766, acc: 0.375)
[2025-02-04 00:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:11][root][INFO] - Training Epoch: 2/2, step 20704/23838 completed (loss: 3.5439817905426025, acc: 0.3870967626571655)
[2025-02-04 00:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:12][root][INFO] - Training Epoch: 2/2, step 20705/23838 completed (loss: 3.823331832885742, acc: 0.3333333432674408)
[2025-02-04 00:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:12][root][INFO] - Training Epoch: 2/2, step 20706/23838 completed (loss: 3.1122193336486816, acc: 0.52173912525177)
[2025-02-04 00:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:13][root][INFO] - Training Epoch: 2/2, step 20707/23838 completed (loss: 2.6708757877349854, acc: 0.4583333432674408)
[2025-02-04 00:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:13][root][INFO] - Training Epoch: 2/2, step 20708/23838 completed (loss: 2.8811967372894287, acc: 0.5)
[2025-02-04 00:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:14][root][INFO] - Training Epoch: 2/2, step 20709/23838 completed (loss: 2.921987533569336, acc: 0.375)
[2025-02-04 00:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:14][root][INFO] - Training Epoch: 2/2, step 20710/23838 completed (loss: 3.5619096755981445, acc: 0.4146341383457184)
[2025-02-04 00:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:14][root][INFO] - Training Epoch: 2/2, step 20711/23838 completed (loss: 3.7049336433410645, acc: 0.27272728085517883)
[2025-02-04 00:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:15][root][INFO] - Training Epoch: 2/2, step 20712/23838 completed (loss: 2.3289263248443604, acc: 0.38461539149284363)
[2025-02-04 00:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:15][root][INFO] - Training Epoch: 2/2, step 20713/23838 completed (loss: 4.253258228302002, acc: 0.2857142984867096)
[2025-02-04 00:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:16][root][INFO] - Training Epoch: 2/2, step 20714/23838 completed (loss: 4.3604044914245605, acc: 0.25925925374031067)
[2025-02-04 00:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:16][root][INFO] - Training Epoch: 2/2, step 20715/23838 completed (loss: 4.426353931427002, acc: 0.3199999928474426)
[2025-02-04 00:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:17][root][INFO] - Training Epoch: 2/2, step 20716/23838 completed (loss: 2.4056434631347656, acc: 0.4545454680919647)
[2025-02-04 00:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:17][root][INFO] - Training Epoch: 2/2, step 20717/23838 completed (loss: 3.470759391784668, acc: 0.3448275923728943)
[2025-02-04 00:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:18][root][INFO] - Training Epoch: 2/2, step 20718/23838 completed (loss: 3.6586265563964844, acc: 0.2222222238779068)
[2025-02-04 00:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:18][root][INFO] - Training Epoch: 2/2, step 20719/23838 completed (loss: 2.9055724143981934, acc: 0.4324324429035187)
[2025-02-04 00:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:19][root][INFO] - Training Epoch: 2/2, step 20720/23838 completed (loss: 4.185672283172607, acc: 0.22033898532390594)
[2025-02-04 00:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:19][root][INFO] - Training Epoch: 2/2, step 20721/23838 completed (loss: 1.919517993927002, acc: 0.6111111044883728)
[2025-02-04 00:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:20][root][INFO] - Training Epoch: 2/2, step 20722/23838 completed (loss: 2.919152021408081, acc: 0.4146341383457184)
[2025-02-04 00:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:20][root][INFO] - Training Epoch: 2/2, step 20723/23838 completed (loss: 2.5363664627075195, acc: 0.41025641560554504)
[2025-02-04 00:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:20][root][INFO] - Training Epoch: 2/2, step 20724/23838 completed (loss: 2.5385189056396484, acc: 0.4838709533214569)
[2025-02-04 00:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:21][root][INFO] - Training Epoch: 2/2, step 20725/23838 completed (loss: 3.6355230808258057, acc: 0.3571428656578064)
[2025-02-04 00:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:21][root][INFO] - Training Epoch: 2/2, step 20726/23838 completed (loss: 1.6652634143829346, acc: 0.6000000238418579)
[2025-02-04 00:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:22][root][INFO] - Training Epoch: 2/2, step 20727/23838 completed (loss: 2.1176886558532715, acc: 0.6086956262588501)
[2025-02-04 00:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:22][root][INFO] - Training Epoch: 2/2, step 20728/23838 completed (loss: 2.250608205795288, acc: 0.5652173757553101)
[2025-02-04 00:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:22][root][INFO] - Training Epoch: 2/2, step 20729/23838 completed (loss: 2.373448371887207, acc: 0.4375)
[2025-02-04 00:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:23][root][INFO] - Training Epoch: 2/2, step 20730/23838 completed (loss: 2.8524515628814697, acc: 0.3243243098258972)
[2025-02-04 00:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:23][root][INFO] - Training Epoch: 2/2, step 20731/23838 completed (loss: 1.7915148735046387, acc: 0.6800000071525574)
[2025-02-04 00:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:24][root][INFO] - Training Epoch: 2/2, step 20732/23838 completed (loss: 1.9395689964294434, acc: 0.6842105388641357)
[2025-02-04 00:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:24][root][INFO] - Training Epoch: 2/2, step 20733/23838 completed (loss: 2.3863906860351562, acc: 0.4375)
[2025-02-04 00:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:24][root][INFO] - Training Epoch: 2/2, step 20734/23838 completed (loss: 1.8801088333129883, acc: 0.48275861144065857)
[2025-02-04 00:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:25][root][INFO] - Training Epoch: 2/2, step 20735/23838 completed (loss: 2.452369451522827, acc: 0.47457626461982727)
[2025-02-04 00:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:25][root][INFO] - Training Epoch: 2/2, step 20736/23838 completed (loss: 2.506166458129883, acc: 0.5555555820465088)
[2025-02-04 00:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:26][root][INFO] - Training Epoch: 2/2, step 20737/23838 completed (loss: 3.30643367767334, acc: 0.4146341383457184)
[2025-02-04 00:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:26][root][INFO] - Training Epoch: 2/2, step 20738/23838 completed (loss: 3.2485601902008057, acc: 0.375)
[2025-02-04 00:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:27][root][INFO] - Training Epoch: 2/2, step 20739/23838 completed (loss: 3.2218921184539795, acc: 0.39393940567970276)
[2025-02-04 00:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:27][root][INFO] - Training Epoch: 2/2, step 20740/23838 completed (loss: 2.753641128540039, acc: 0.43103447556495667)
[2025-02-04 00:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:28][root][INFO] - Training Epoch: 2/2, step 20741/23838 completed (loss: 3.338195323944092, acc: 0.3333333432674408)
[2025-02-04 00:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:28][root][INFO] - Training Epoch: 2/2, step 20742/23838 completed (loss: 3.209662914276123, acc: 0.4166666567325592)
[2025-02-04 00:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:28][root][INFO] - Training Epoch: 2/2, step 20743/23838 completed (loss: 3.7304673194885254, acc: 0.27906978130340576)
[2025-02-04 00:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:29][root][INFO] - Training Epoch: 2/2, step 20744/23838 completed (loss: 3.952023506164551, acc: 0.37837839126586914)
[2025-02-04 00:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:29][root][INFO] - Training Epoch: 2/2, step 20745/23838 completed (loss: 2.9518821239471436, acc: 0.3589743673801422)
[2025-02-04 00:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:30][root][INFO] - Training Epoch: 2/2, step 20746/23838 completed (loss: 3.0723090171813965, acc: 0.4000000059604645)
[2025-02-04 00:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:30][root][INFO] - Training Epoch: 2/2, step 20747/23838 completed (loss: 3.509974718093872, acc: 0.30263158679008484)
[2025-02-04 00:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:31][root][INFO] - Training Epoch: 2/2, step 20748/23838 completed (loss: 4.160930156707764, acc: 0.3243243098258972)
[2025-02-04 00:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:31][root][INFO] - Training Epoch: 2/2, step 20749/23838 completed (loss: 2.630150079727173, acc: 0.4444444477558136)
[2025-02-04 00:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:31][root][INFO] - Training Epoch: 2/2, step 20750/23838 completed (loss: 3.4188365936279297, acc: 0.4117647111415863)
[2025-02-04 00:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:32][root][INFO] - Training Epoch: 2/2, step 20751/23838 completed (loss: 2.8128879070281982, acc: 0.4000000059604645)
[2025-02-04 00:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:32][root][INFO] - Training Epoch: 2/2, step 20752/23838 completed (loss: 3.641942024230957, acc: 0.2195121943950653)
[2025-02-04 00:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:33][root][INFO] - Training Epoch: 2/2, step 20753/23838 completed (loss: 3.2586140632629395, acc: 0.44999998807907104)
[2025-02-04 00:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:33][root][INFO] - Training Epoch: 2/2, step 20754/23838 completed (loss: 2.2148303985595703, acc: 0.6071428656578064)
[2025-02-04 00:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:34][root][INFO] - Training Epoch: 2/2, step 20755/23838 completed (loss: 2.5730507373809814, acc: 0.4390243887901306)
[2025-02-04 00:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:34][root][INFO] - Training Epoch: 2/2, step 20756/23838 completed (loss: 3.99527907371521, acc: 0.29629629850387573)
[2025-02-04 00:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:35][root][INFO] - Training Epoch: 2/2, step 20757/23838 completed (loss: 2.7063233852386475, acc: 0.5208333134651184)
[2025-02-04 00:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:35][root][INFO] - Training Epoch: 2/2, step 20758/23838 completed (loss: 4.049026012420654, acc: 0.36666667461395264)
[2025-02-04 00:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:35][root][INFO] - Training Epoch: 2/2, step 20759/23838 completed (loss: 2.8377697467803955, acc: 0.5)
[2025-02-04 00:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:36][root][INFO] - Training Epoch: 2/2, step 20760/23838 completed (loss: 2.63421368598938, acc: 0.4318181872367859)
[2025-02-04 00:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:36][root][INFO] - Training Epoch: 2/2, step 20761/23838 completed (loss: 2.6849377155303955, acc: 0.30612245202064514)
[2025-02-04 00:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:37][root][INFO] - Training Epoch: 2/2, step 20762/23838 completed (loss: 2.9515910148620605, acc: 0.38297873735427856)
[2025-02-04 00:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:37][root][INFO] - Training Epoch: 2/2, step 20763/23838 completed (loss: 2.1871354579925537, acc: 0.550000011920929)
[2025-02-04 00:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:38][root][INFO] - Training Epoch: 2/2, step 20764/23838 completed (loss: 2.5885820388793945, acc: 0.46341463923454285)
[2025-02-04 00:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:38][root][INFO] - Training Epoch: 2/2, step 20765/23838 completed (loss: 3.6487643718719482, acc: 0.25925925374031067)
[2025-02-04 00:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:38][root][INFO] - Training Epoch: 2/2, step 20766/23838 completed (loss: 3.857018232345581, acc: 0.3191489279270172)
[2025-02-04 00:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:39][root][INFO] - Training Epoch: 2/2, step 20767/23838 completed (loss: 4.742680549621582, acc: 0.30000001192092896)
[2025-02-04 00:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:39][root][INFO] - Training Epoch: 2/2, step 20768/23838 completed (loss: 3.253603219985962, acc: 0.4146341383457184)
[2025-02-04 00:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:40][root][INFO] - Training Epoch: 2/2, step 20769/23838 completed (loss: 3.5475013256073, acc: 0.4642857015132904)
[2025-02-04 00:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:40][root][INFO] - Training Epoch: 2/2, step 20770/23838 completed (loss: 3.1580653190612793, acc: 0.4000000059604645)
[2025-02-04 00:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:41][root][INFO] - Training Epoch: 2/2, step 20771/23838 completed (loss: 3.5059778690338135, acc: 0.36000001430511475)
[2025-02-04 00:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:41][root][INFO] - Training Epoch: 2/2, step 20772/23838 completed (loss: 4.288003444671631, acc: 0.3529411852359772)
[2025-02-04 00:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:42][root][INFO] - Training Epoch: 2/2, step 20773/23838 completed (loss: 2.9895448684692383, acc: 0.5333333611488342)
[2025-02-04 00:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:42][root][INFO] - Training Epoch: 2/2, step 20774/23838 completed (loss: 3.7030420303344727, acc: 0.3658536672592163)
[2025-02-04 00:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:42][root][INFO] - Training Epoch: 2/2, step 20775/23838 completed (loss: 3.733996629714966, acc: 0.3478260934352875)
[2025-02-04 00:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:43][root][INFO] - Training Epoch: 2/2, step 20776/23838 completed (loss: 2.7083308696746826, acc: 0.5277777910232544)
[2025-02-04 00:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:43][root][INFO] - Training Epoch: 2/2, step 20777/23838 completed (loss: 4.1077704429626465, acc: 0.2549019753932953)
[2025-02-04 00:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:44][root][INFO] - Training Epoch: 2/2, step 20778/23838 completed (loss: 4.095032691955566, acc: 0.48148149251937866)
[2025-02-04 00:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:44][root][INFO] - Training Epoch: 2/2, step 20779/23838 completed (loss: 3.6843373775482178, acc: 0.31707316637039185)
[2025-02-04 00:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:45][root][INFO] - Training Epoch: 2/2, step 20780/23838 completed (loss: 3.6903414726257324, acc: 0.40740740299224854)
[2025-02-04 00:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:45][root][INFO] - Training Epoch: 2/2, step 20781/23838 completed (loss: 3.123114824295044, acc: 0.32258063554763794)
[2025-02-04 00:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:46][root][INFO] - Training Epoch: 2/2, step 20782/23838 completed (loss: 2.3662445545196533, acc: 0.4583333432674408)
[2025-02-04 00:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:46][root][INFO] - Training Epoch: 2/2, step 20783/23838 completed (loss: 3.8239598274230957, acc: 0.3333333432674408)
[2025-02-04 00:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:47][root][INFO] - Training Epoch: 2/2, step 20784/23838 completed (loss: 4.038549423217773, acc: 0.3333333432674408)
[2025-02-04 00:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:47][root][INFO] - Training Epoch: 2/2, step 20785/23838 completed (loss: 3.961183786392212, acc: 0.4285714328289032)
[2025-02-04 00:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:47][root][INFO] - Training Epoch: 2/2, step 20786/23838 completed (loss: 0.5633560419082642, acc: 0.875)
[2025-02-04 00:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:48][root][INFO] - Training Epoch: 2/2, step 20787/23838 completed (loss: 3.2905285358428955, acc: 0.3333333432674408)
[2025-02-04 00:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:48][root][INFO] - Training Epoch: 2/2, step 20788/23838 completed (loss: 2.9025254249572754, acc: 0.4166666567325592)
[2025-02-04 00:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:49][root][INFO] - Training Epoch: 2/2, step 20789/23838 completed (loss: 3.570430040359497, acc: 0.28947368264198303)
[2025-02-04 00:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:49][root][INFO] - Training Epoch: 2/2, step 20790/23838 completed (loss: 3.2535006999969482, acc: 0.37037035822868347)
[2025-02-04 00:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:49][root][INFO] - Training Epoch: 2/2, step 20791/23838 completed (loss: 2.4923503398895264, acc: 0.4516128897666931)
[2025-02-04 00:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:50][root][INFO] - Training Epoch: 2/2, step 20792/23838 completed (loss: 2.4045534133911133, acc: 0.4375)
[2025-02-04 00:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:50][root][INFO] - Training Epoch: 2/2, step 20793/23838 completed (loss: 2.8522613048553467, acc: 0.4482758641242981)
[2025-02-04 00:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:51][root][INFO] - Training Epoch: 2/2, step 20794/23838 completed (loss: 2.6123132705688477, acc: 0.3181818127632141)
[2025-02-04 00:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:51][root][INFO] - Training Epoch: 2/2, step 20795/23838 completed (loss: 2.288038492202759, acc: 0.4736842215061188)
[2025-02-04 00:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:52][root][INFO] - Training Epoch: 2/2, step 20796/23838 completed (loss: 2.8779265880584717, acc: 0.48148149251937866)
[2025-02-04 00:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:52][root][INFO] - Training Epoch: 2/2, step 20797/23838 completed (loss: 2.878502607345581, acc: 0.38297873735427856)
[2025-02-04 00:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:53][root][INFO] - Training Epoch: 2/2, step 20798/23838 completed (loss: 2.413332223892212, acc: 0.529411792755127)
[2025-02-04 00:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:53][root][INFO] - Training Epoch: 2/2, step 20799/23838 completed (loss: 2.9902780055999756, acc: 0.4583333432674408)
[2025-02-04 00:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:54][root][INFO] - Training Epoch: 2/2, step 20800/23838 completed (loss: 2.815972328186035, acc: 0.3333333432674408)
[2025-02-04 00:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:54][root][INFO] - Training Epoch: 2/2, step 20801/23838 completed (loss: 5.500598907470703, acc: 0.2857142984867096)
[2025-02-04 00:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:55][root][INFO] - Training Epoch: 2/2, step 20802/23838 completed (loss: 3.7275519371032715, acc: 0.3513513505458832)
[2025-02-04 00:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:55][root][INFO] - Training Epoch: 2/2, step 20803/23838 completed (loss: 3.2054474353790283, acc: 0.42500001192092896)
[2025-02-04 00:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:55][root][INFO] - Training Epoch: 2/2, step 20804/23838 completed (loss: 3.406339168548584, acc: 0.3928571343421936)
[2025-02-04 00:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:56][root][INFO] - Training Epoch: 2/2, step 20805/23838 completed (loss: 3.8778934478759766, acc: 0.25)
[2025-02-04 00:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:56][root][INFO] - Training Epoch: 2/2, step 20806/23838 completed (loss: 2.744645357131958, acc: 0.5416666865348816)
[2025-02-04 00:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:57][root][INFO] - Training Epoch: 2/2, step 20807/23838 completed (loss: 2.838752508163452, acc: 0.4285714328289032)
[2025-02-04 00:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:57][root][INFO] - Training Epoch: 2/2, step 20808/23838 completed (loss: 2.2244882583618164, acc: 0.7142857313156128)
[2025-02-04 00:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:58][root][INFO] - Training Epoch: 2/2, step 20809/23838 completed (loss: 3.4561681747436523, acc: 0.4444444477558136)
[2025-02-04 00:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:58][root][INFO] - Training Epoch: 2/2, step 20810/23838 completed (loss: 2.830671548843384, acc: 0.44186046719551086)
[2025-02-04 00:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:59][root][INFO] - Training Epoch: 2/2, step 20811/23838 completed (loss: 2.916544198989868, acc: 0.4390243887901306)
[2025-02-04 00:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:54:59][root][INFO] - Training Epoch: 2/2, step 20812/23838 completed (loss: 2.9965553283691406, acc: 0.4150943458080292)
[2025-02-04 00:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:00][root][INFO] - Training Epoch: 2/2, step 20813/23838 completed (loss: 3.6212217807769775, acc: 0.3414634168148041)
[2025-02-04 00:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:00][root][INFO] - Training Epoch: 2/2, step 20814/23838 completed (loss: 2.760695695877075, acc: 0.40425533056259155)
[2025-02-04 00:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:00][root][INFO] - Training Epoch: 2/2, step 20815/23838 completed (loss: 4.213871955871582, acc: 0.3461538553237915)
[2025-02-04 00:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:01][root][INFO] - Training Epoch: 2/2, step 20816/23838 completed (loss: 2.7443480491638184, acc: 0.44999998807907104)
[2025-02-04 00:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:01][root][INFO] - Training Epoch: 2/2, step 20817/23838 completed (loss: 1.8156206607818604, acc: 0.6000000238418579)
[2025-02-04 00:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:02][root][INFO] - Training Epoch: 2/2, step 20818/23838 completed (loss: 3.0091238021850586, acc: 0.4722222089767456)
[2025-02-04 00:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:02][root][INFO] - Training Epoch: 2/2, step 20819/23838 completed (loss: 1.8726059198379517, acc: 0.5416666865348816)
[2025-02-04 00:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:03][root][INFO] - Training Epoch: 2/2, step 20820/23838 completed (loss: 3.633249282836914, acc: 0.4117647111415863)
[2025-02-04 00:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:03][root][INFO] - Training Epoch: 2/2, step 20821/23838 completed (loss: 2.952986240386963, acc: 0.2777777910232544)
[2025-02-04 00:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:03][root][INFO] - Training Epoch: 2/2, step 20822/23838 completed (loss: 2.8218863010406494, acc: 0.4545454680919647)
[2025-02-04 00:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:04][root][INFO] - Training Epoch: 2/2, step 20823/23838 completed (loss: 2.3318963050842285, acc: 0.5)
[2025-02-04 00:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:04][root][INFO] - Training Epoch: 2/2, step 20824/23838 completed (loss: 2.164092779159546, acc: 0.48148149251937866)
[2025-02-04 00:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:05][root][INFO] - Training Epoch: 2/2, step 20825/23838 completed (loss: 3.7291853427886963, acc: 0.3333333432674408)
[2025-02-04 00:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:05][root][INFO] - Training Epoch: 2/2, step 20826/23838 completed (loss: 3.263018846511841, acc: 0.4000000059604645)
[2025-02-04 00:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:06][root][INFO] - Training Epoch: 2/2, step 20827/23838 completed (loss: 2.2554259300231934, acc: 0.5)
[2025-02-04 00:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:06][root][INFO] - Training Epoch: 2/2, step 20828/23838 completed (loss: 4.046720504760742, acc: 0.3461538553237915)
[2025-02-04 00:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:06][root][INFO] - Training Epoch: 2/2, step 20829/23838 completed (loss: 2.0158214569091797, acc: 0.529411792755127)
[2025-02-04 00:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:07][root][INFO] - Training Epoch: 2/2, step 20830/23838 completed (loss: 2.113417863845825, acc: 0.5555555820465088)
[2025-02-04 00:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:07][root][INFO] - Training Epoch: 2/2, step 20831/23838 completed (loss: 2.053950309753418, acc: 0.5)
[2025-02-04 00:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:08][root][INFO] - Training Epoch: 2/2, step 20832/23838 completed (loss: 2.1211254596710205, acc: 0.5357142686843872)
[2025-02-04 00:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:08][root][INFO] - Training Epoch: 2/2, step 20833/23838 completed (loss: 4.126056671142578, acc: 0.36000001430511475)
[2025-02-04 00:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:08][root][INFO] - Training Epoch: 2/2, step 20834/23838 completed (loss: 4.570005893707275, acc: 0.3695652186870575)
[2025-02-04 00:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:09][root][INFO] - Training Epoch: 2/2, step 20835/23838 completed (loss: 4.2165751457214355, acc: 0.375)
[2025-02-04 00:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:09][root][INFO] - Training Epoch: 2/2, step 20836/23838 completed (loss: 2.9433202743530273, acc: 0.5384615659713745)
[2025-02-04 00:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:10][root][INFO] - Training Epoch: 2/2, step 20837/23838 completed (loss: 2.9503684043884277, acc: 0.5)
[2025-02-04 00:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:10][root][INFO] - Training Epoch: 2/2, step 20838/23838 completed (loss: 3.9379184246063232, acc: 0.3333333432674408)
[2025-02-04 00:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:11][root][INFO] - Training Epoch: 2/2, step 20839/23838 completed (loss: 4.015359401702881, acc: 0.3333333432674408)
[2025-02-04 00:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:11][root][INFO] - Training Epoch: 2/2, step 20840/23838 completed (loss: 4.256042957305908, acc: 0.2222222238779068)
[2025-02-04 00:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:12][root][INFO] - Training Epoch: 2/2, step 20841/23838 completed (loss: 3.5052993297576904, acc: 0.3888888955116272)
[2025-02-04 00:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:12][root][INFO] - Training Epoch: 2/2, step 20842/23838 completed (loss: 0.5549405217170715, acc: 0.875)
[2025-02-04 00:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:13][root][INFO] - Training Epoch: 2/2, step 20843/23838 completed (loss: 4.088043689727783, acc: 0.30434781312942505)
[2025-02-04 00:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:13][root][INFO] - Training Epoch: 2/2, step 20844/23838 completed (loss: 5.501962184906006, acc: 0.3333333432674408)
[2025-02-04 00:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:14][root][INFO] - Training Epoch: 2/2, step 20845/23838 completed (loss: 4.781404495239258, acc: 0.2222222238779068)
[2025-02-04 00:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:14][root][INFO] - Training Epoch: 2/2, step 20846/23838 completed (loss: 3.8823940753936768, acc: 0.4166666567325592)
[2025-02-04 00:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:14][root][INFO] - Training Epoch: 2/2, step 20847/23838 completed (loss: 3.2501940727233887, acc: 0.44999998807907104)
[2025-02-04 00:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:15][root][INFO] - Training Epoch: 2/2, step 20848/23838 completed (loss: 4.428038120269775, acc: 0.3076923191547394)
[2025-02-04 00:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:15][root][INFO] - Training Epoch: 2/2, step 20849/23838 completed (loss: 1.9864128828048706, acc: 0.6315789222717285)
[2025-02-04 00:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:16][root][INFO] - Training Epoch: 2/2, step 20850/23838 completed (loss: 4.74221134185791, acc: 0.4117647111415863)
[2025-02-04 00:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:16][root][INFO] - Training Epoch: 2/2, step 20851/23838 completed (loss: 2.602977991104126, acc: 0.3125)
[2025-02-04 00:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:17][root][INFO] - Training Epoch: 2/2, step 20852/23838 completed (loss: 2.7446706295013428, acc: 0.5)
[2025-02-04 00:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:17][root][INFO] - Training Epoch: 2/2, step 20853/23838 completed (loss: 3.2854983806610107, acc: 0.46666666865348816)
[2025-02-04 00:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:18][root][INFO] - Training Epoch: 2/2, step 20854/23838 completed (loss: 1.8940012454986572, acc: 0.5714285969734192)
[2025-02-04 00:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:18][root][INFO] - Training Epoch: 2/2, step 20855/23838 completed (loss: 1.040956735610962, acc: 0.7857142686843872)
[2025-02-04 00:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:18][root][INFO] - Training Epoch: 2/2, step 20856/23838 completed (loss: 3.5697498321533203, acc: 0.4285714328289032)
[2025-02-04 00:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:19][root][INFO] - Training Epoch: 2/2, step 20857/23838 completed (loss: 4.871917724609375, acc: 0.47058823704719543)
[2025-02-04 00:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:19][root][INFO] - Training Epoch: 2/2, step 20858/23838 completed (loss: 4.592644691467285, acc: 0.30000001192092896)
[2025-02-04 00:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:20][root][INFO] - Training Epoch: 2/2, step 20859/23838 completed (loss: 3.1388609409332275, acc: 0.3888888955116272)
[2025-02-04 00:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:20][root][INFO] - Training Epoch: 2/2, step 20860/23838 completed (loss: 2.341304302215576, acc: 0.6666666865348816)
[2025-02-04 00:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:20][root][INFO] - Training Epoch: 2/2, step 20861/23838 completed (loss: 3.476985454559326, acc: 0.4399999976158142)
[2025-02-04 00:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:21][root][INFO] - Training Epoch: 2/2, step 20862/23838 completed (loss: 3.158328056335449, acc: 0.3636363744735718)
[2025-02-04 00:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:21][root][INFO] - Training Epoch: 2/2, step 20863/23838 completed (loss: 3.386662006378174, acc: 0.3448275923728943)
[2025-02-04 00:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:22][root][INFO] - Training Epoch: 2/2, step 20864/23838 completed (loss: 4.201415538787842, acc: 0.3030303120613098)
[2025-02-04 00:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:22][root][INFO] - Training Epoch: 2/2, step 20865/23838 completed (loss: 3.88442063331604, acc: 0.37931033968925476)
[2025-02-04 00:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:22][root][INFO] - Training Epoch: 2/2, step 20866/23838 completed (loss: 2.8048923015594482, acc: 0.40740740299224854)
[2025-02-04 00:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:23][root][INFO] - Training Epoch: 2/2, step 20867/23838 completed (loss: 1.9981950521469116, acc: 0.5)
[2025-02-04 00:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:23][root][INFO] - Training Epoch: 2/2, step 20868/23838 completed (loss: 3.8100621700286865, acc: 0.31578946113586426)
[2025-02-04 00:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:24][root][INFO] - Training Epoch: 2/2, step 20869/23838 completed (loss: 2.0282721519470215, acc: 0.5384615659713745)
[2025-02-04 00:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:24][root][INFO] - Training Epoch: 2/2, step 20870/23838 completed (loss: 2.1055614948272705, acc: 0.5)
[2025-02-04 00:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:24][root][INFO] - Training Epoch: 2/2, step 20871/23838 completed (loss: 1.3753293752670288, acc: 0.5714285969734192)
[2025-02-04 00:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:25][root][INFO] - Training Epoch: 2/2, step 20872/23838 completed (loss: 1.5073162317276, acc: 0.6666666865348816)
[2025-02-04 00:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:25][root][INFO] - Training Epoch: 2/2, step 20873/23838 completed (loss: 2.817359209060669, acc: 0.3636363744735718)
[2025-02-04 00:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:26][root][INFO] - Training Epoch: 2/2, step 20874/23838 completed (loss: 0.6086622476577759, acc: 0.8999999761581421)
[2025-02-04 00:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:26][root][INFO] - Training Epoch: 2/2, step 20875/23838 completed (loss: 2.0830910205841064, acc: 0.5833333134651184)
[2025-02-04 00:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:27][root][INFO] - Training Epoch: 2/2, step 20876/23838 completed (loss: 1.669924259185791, acc: 0.7333333492279053)
[2025-02-04 00:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:27][root][INFO] - Training Epoch: 2/2, step 20877/23838 completed (loss: 3.6617069244384766, acc: 0.6000000238418579)
[2025-02-04 00:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:28][root][INFO] - Training Epoch: 2/2, step 20878/23838 completed (loss: 2.126948356628418, acc: 0.5)
[2025-02-04 00:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:28][root][INFO] - Training Epoch: 2/2, step 20879/23838 completed (loss: 1.9777888059616089, acc: 0.6666666865348816)
[2025-02-04 00:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:28][root][INFO] - Training Epoch: 2/2, step 20880/23838 completed (loss: 1.2977741956710815, acc: 0.75)
[2025-02-04 00:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:29][root][INFO] - Training Epoch: 2/2, step 20881/23838 completed (loss: 3.16436505317688, acc: 0.5)
[2025-02-04 00:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:29][root][INFO] - Training Epoch: 2/2, step 20882/23838 completed (loss: 3.574374198913574, acc: 0.3076923191547394)
[2025-02-04 00:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:30][root][INFO] - Training Epoch: 2/2, step 20883/23838 completed (loss: 0.7877092361450195, acc: 0.7857142686843872)
[2025-02-04 00:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:30][root][INFO] - Training Epoch: 2/2, step 20884/23838 completed (loss: 2.987851619720459, acc: 0.5)
[2025-02-04 00:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:30][root][INFO] - Training Epoch: 2/2, step 20885/23838 completed (loss: 2.189316987991333, acc: 0.4000000059604645)
[2025-02-04 00:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:31][root][INFO] - Training Epoch: 2/2, step 20886/23838 completed (loss: 3.1599903106689453, acc: 0.4375)
[2025-02-04 00:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:31][root][INFO] - Training Epoch: 2/2, step 20887/23838 completed (loss: 1.0644670724868774, acc: 0.8461538553237915)
[2025-02-04 00:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:32][root][INFO] - Training Epoch: 2/2, step 20888/23838 completed (loss: 1.9243931770324707, acc: 0.6666666865348816)
[2025-02-04 00:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:32][root][INFO] - Training Epoch: 2/2, step 20889/23838 completed (loss: 2.964454174041748, acc: 0.7142857313156128)
[2025-02-04 00:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:33][root][INFO] - Training Epoch: 2/2, step 20890/23838 completed (loss: 0.5048635601997375, acc: 0.875)
[2025-02-04 00:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:33][root][INFO] - Training Epoch: 2/2, step 20891/23838 completed (loss: 2.169457197189331, acc: 0.3333333432674408)
[2025-02-04 00:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:33][root][INFO] - Training Epoch: 2/2, step 20892/23838 completed (loss: 3.40242075920105, acc: 0.5714285969734192)
[2025-02-04 00:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:34][root][INFO] - Training Epoch: 2/2, step 20893/23838 completed (loss: 2.3023879528045654, acc: 0.625)
[2025-02-04 00:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:34][root][INFO] - Training Epoch: 2/2, step 20894/23838 completed (loss: 1.983489990234375, acc: 0.3636363744735718)
[2025-02-04 00:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:35][root][INFO] - Training Epoch: 2/2, step 20895/23838 completed (loss: 1.8312326669692993, acc: 0.625)
[2025-02-04 00:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:35][root][INFO] - Training Epoch: 2/2, step 20896/23838 completed (loss: 1.26510488986969, acc: 0.5)
[2025-02-04 00:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:35][root][INFO] - Training Epoch: 2/2, step 20897/23838 completed (loss: 1.3012410402297974, acc: 0.800000011920929)
[2025-02-04 00:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:36][root][INFO] - Training Epoch: 2/2, step 20898/23838 completed (loss: 1.809346318244934, acc: 0.7272727489471436)
[2025-02-04 00:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:36][root][INFO] - Training Epoch: 2/2, step 20899/23838 completed (loss: 1.4067925214767456, acc: 0.5)
[2025-02-04 00:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:37][root][INFO] - Training Epoch: 2/2, step 20900/23838 completed (loss: 1.2083020210266113, acc: 0.6666666865348816)
[2025-02-04 00:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:37][root][INFO] - Training Epoch: 2/2, step 20901/23838 completed (loss: 1.8406423330307007, acc: 0.6666666865348816)
[2025-02-04 00:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:38][root][INFO] - Training Epoch: 2/2, step 20902/23838 completed (loss: 2.6868696212768555, acc: 0.6000000238418579)
[2025-02-04 00:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:38][root][INFO] - Training Epoch: 2/2, step 20903/23838 completed (loss: 2.5590102672576904, acc: 0.5)
[2025-02-04 00:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:38][root][INFO] - Training Epoch: 2/2, step 20904/23838 completed (loss: 2.5457417964935303, acc: 0.5)
[2025-02-04 00:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:39][root][INFO] - Training Epoch: 2/2, step 20905/23838 completed (loss: 3.6604416370391846, acc: 0.5)
[2025-02-04 00:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:39][root][INFO] - Training Epoch: 2/2, step 20906/23838 completed (loss: 2.8045833110809326, acc: 0.5)
[2025-02-04 00:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:40][root][INFO] - Training Epoch: 2/2, step 20907/23838 completed (loss: 4.603188514709473, acc: 0.46666666865348816)
[2025-02-04 00:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:40][root][INFO] - Training Epoch: 2/2, step 20908/23838 completed (loss: 2.770393133163452, acc: 0.4166666567325592)
[2025-02-04 00:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:41][root][INFO] - Training Epoch: 2/2, step 20909/23838 completed (loss: 2.195295810699463, acc: 0.6000000238418579)
[2025-02-04 00:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:41][root][INFO] - Training Epoch: 2/2, step 20910/23838 completed (loss: 3.1338467597961426, acc: 0.4285714328289032)
[2025-02-04 00:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:41][root][INFO] - Training Epoch: 2/2, step 20911/23838 completed (loss: 2.605335235595703, acc: 0.5454545617103577)
[2025-02-04 00:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:42][root][INFO] - Training Epoch: 2/2, step 20912/23838 completed (loss: 2.7230184078216553, acc: 0.5333333611488342)
[2025-02-04 00:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:42][root][INFO] - Training Epoch: 2/2, step 20913/23838 completed (loss: 0.9942548871040344, acc: 0.800000011920929)
[2025-02-04 00:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:43][root][INFO] - Training Epoch: 2/2, step 20914/23838 completed (loss: 2.291375160217285, acc: 0.4545454680919647)
[2025-02-04 00:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:43][root][INFO] - Training Epoch: 2/2, step 20915/23838 completed (loss: 3.523310422897339, acc: 0.38461539149284363)
[2025-02-04 00:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:44][root][INFO] - Training Epoch: 2/2, step 20916/23838 completed (loss: 1.7973577976226807, acc: 0.5)
[2025-02-04 00:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:44][root][INFO] - Training Epoch: 2/2, step 20917/23838 completed (loss: 1.8521254062652588, acc: 0.5)
[2025-02-04 00:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:44][root][INFO] - Training Epoch: 2/2, step 20918/23838 completed (loss: 3.6775004863739014, acc: 0.25)
[2025-02-04 00:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:45][root][INFO] - Training Epoch: 2/2, step 20919/23838 completed (loss: 2.145749092102051, acc: 0.5714285969734192)
[2025-02-04 00:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:45][root][INFO] - Training Epoch: 2/2, step 20920/23838 completed (loss: 1.3767989873886108, acc: 0.6428571343421936)
[2025-02-04 00:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:46][root][INFO] - Training Epoch: 2/2, step 20921/23838 completed (loss: 2.3423399925231934, acc: 0.4166666567325592)
[2025-02-04 00:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:46][root][INFO] - Training Epoch: 2/2, step 20922/23838 completed (loss: 2.3954710960388184, acc: 0.692307710647583)
[2025-02-04 00:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:47][root][INFO] - Training Epoch: 2/2, step 20923/23838 completed (loss: 3.7872183322906494, acc: 0.5)
[2025-02-04 00:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:47][root][INFO] - Training Epoch: 2/2, step 20924/23838 completed (loss: 4.455902576446533, acc: 0.4285714328289032)
[2025-02-04 00:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:48][root][INFO] - Training Epoch: 2/2, step 20925/23838 completed (loss: 4.090482234954834, acc: 0.4000000059604645)
[2025-02-04 00:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:48][root][INFO] - Training Epoch: 2/2, step 20926/23838 completed (loss: 3.15301251411438, acc: 0.5333333611488342)
[2025-02-04 00:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:49][root][INFO] - Training Epoch: 2/2, step 20927/23838 completed (loss: 2.2188174724578857, acc: 0.6153846383094788)
[2025-02-04 00:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:49][root][INFO] - Training Epoch: 2/2, step 20928/23838 completed (loss: 2.4810190200805664, acc: 0.5454545617103577)
[2025-02-04 00:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:50][root][INFO] - Training Epoch: 2/2, step 20929/23838 completed (loss: 3.6491754055023193, acc: 0.4545454680919647)
[2025-02-04 00:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:50][root][INFO] - Training Epoch: 2/2, step 20930/23838 completed (loss: 4.287547588348389, acc: 0.46666666865348816)
[2025-02-04 00:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:50][root][INFO] - Training Epoch: 2/2, step 20931/23838 completed (loss: 2.7814979553222656, acc: 0.5)
[2025-02-04 00:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:51][root][INFO] - Training Epoch: 2/2, step 20932/23838 completed (loss: 3.2782223224639893, acc: 0.4444444477558136)
[2025-02-04 00:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:51][root][INFO] - Training Epoch: 2/2, step 20933/23838 completed (loss: 3.372220516204834, acc: 0.3888888955116272)
[2025-02-04 00:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:52][root][INFO] - Training Epoch: 2/2, step 20934/23838 completed (loss: 5.245904922485352, acc: 0.3181818127632141)
[2025-02-04 00:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:52][root][INFO] - Training Epoch: 2/2, step 20935/23838 completed (loss: 1.7595165967941284, acc: 0.5714285969734192)
[2025-02-04 00:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:52][root][INFO] - Training Epoch: 2/2, step 20936/23838 completed (loss: 0.9233192205429077, acc: 0.7272727489471436)
[2025-02-04 00:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:53][root][INFO] - Training Epoch: 2/2, step 20937/23838 completed (loss: 4.271390914916992, acc: 0.27272728085517883)
[2025-02-04 00:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:53][root][INFO] - Training Epoch: 2/2, step 20938/23838 completed (loss: 3.39426851272583, acc: 0.3333333432674408)
[2025-02-04 00:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:54][root][INFO] - Training Epoch: 2/2, step 20939/23838 completed (loss: 1.3469629287719727, acc: 0.5714285969734192)
[2025-02-04 00:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:54][root][INFO] - Training Epoch: 2/2, step 20940/23838 completed (loss: 1.6118699312210083, acc: 0.46666666865348816)
[2025-02-04 00:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:55][root][INFO] - Training Epoch: 2/2, step 20941/23838 completed (loss: 3.2304136753082275, acc: 0.4000000059604645)
[2025-02-04 00:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:55][root][INFO] - Training Epoch: 2/2, step 20942/23838 completed (loss: 4.245175838470459, acc: 0.1428571492433548)
[2025-02-04 00:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:55][root][INFO] - Training Epoch: 2/2, step 20943/23838 completed (loss: 2.7748100757598877, acc: 0.5)
[2025-02-04 00:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:56][root][INFO] - Training Epoch: 2/2, step 20944/23838 completed (loss: 3.531581163406372, acc: 0.5)
[2025-02-04 00:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:56][root][INFO] - Training Epoch: 2/2, step 20945/23838 completed (loss: 3.1980702877044678, acc: 0.5384615659713745)
[2025-02-04 00:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:57][root][INFO] - Training Epoch: 2/2, step 20946/23838 completed (loss: 2.571650981903076, acc: 0.4000000059604645)
[2025-02-04 00:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:57][root][INFO] - Training Epoch: 2/2, step 20947/23838 completed (loss: 1.9907786846160889, acc: 0.6153846383094788)
[2025-02-04 00:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:57][root][INFO] - Training Epoch: 2/2, step 20948/23838 completed (loss: 3.715320348739624, acc: 0.5833333134651184)
[2025-02-04 00:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:58][root][INFO] - Training Epoch: 2/2, step 20949/23838 completed (loss: 2.475496530532837, acc: 0.6363636255264282)
[2025-02-04 00:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:58][root][INFO] - Training Epoch: 2/2, step 20950/23838 completed (loss: 4.693525314331055, acc: 0.2666666805744171)
[2025-02-04 00:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:59][root][INFO] - Training Epoch: 2/2, step 20951/23838 completed (loss: 3.3856759071350098, acc: 0.4545454680919647)
[2025-02-04 00:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:55:59][root][INFO] - Training Epoch: 2/2, step 20952/23838 completed (loss: 3.8684206008911133, acc: 0.4615384638309479)
[2025-02-04 00:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:00][root][INFO] - Training Epoch: 2/2, step 20953/23838 completed (loss: 3.0290729999542236, acc: 0.5)
[2025-02-04 00:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:00][root][INFO] - Training Epoch: 2/2, step 20954/23838 completed (loss: 1.0790659189224243, acc: 0.692307710647583)
[2025-02-04 00:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:01][root][INFO] - Training Epoch: 2/2, step 20955/23838 completed (loss: 1.0283794403076172, acc: 0.800000011920929)
[2025-02-04 00:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:01][root][INFO] - Training Epoch: 2/2, step 20956/23838 completed (loss: 2.566842794418335, acc: 0.5454545617103577)
[2025-02-04 00:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:01][root][INFO] - Training Epoch: 2/2, step 20957/23838 completed (loss: 1.4675918817520142, acc: 0.625)
[2025-02-04 00:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:02][root][INFO] - Training Epoch: 2/2, step 20958/23838 completed (loss: 2.4839816093444824, acc: 0.46666666865348816)
[2025-02-04 00:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:02][root][INFO] - Training Epoch: 2/2, step 20959/23838 completed (loss: 4.590568542480469, acc: 0.27272728085517883)
[2025-02-04 00:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:03][root][INFO] - Training Epoch: 2/2, step 20960/23838 completed (loss: 3.8209593296051025, acc: 0.375)
[2025-02-04 00:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:03][root][INFO] - Training Epoch: 2/2, step 20961/23838 completed (loss: 2.2778561115264893, acc: 0.692307710647583)
[2025-02-04 00:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:04][root][INFO] - Training Epoch: 2/2, step 20962/23838 completed (loss: 3.8630802631378174, acc: 0.3571428656578064)
[2025-02-04 00:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:04][root][INFO] - Training Epoch: 2/2, step 20963/23838 completed (loss: 2.275771379470825, acc: 0.6153846383094788)
[2025-02-04 00:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:04][root][INFO] - Training Epoch: 2/2, step 20964/23838 completed (loss: 4.1260085105896, acc: 0.27272728085517883)
[2025-02-04 00:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:05][root][INFO] - Training Epoch: 2/2, step 20965/23838 completed (loss: 3.470670700073242, acc: 0.4166666567325592)
[2025-02-04 00:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:05][root][INFO] - Training Epoch: 2/2, step 20966/23838 completed (loss: 1.6867529153823853, acc: 0.6190476417541504)
[2025-02-04 00:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:06][root][INFO] - Training Epoch: 2/2, step 20967/23838 completed (loss: 3.547513008117676, acc: 0.3684210479259491)
[2025-02-04 00:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:06][root][INFO] - Training Epoch: 2/2, step 20968/23838 completed (loss: 3.8390629291534424, acc: 0.19354838132858276)
[2025-02-04 00:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:07][root][INFO] - Training Epoch: 2/2, step 20969/23838 completed (loss: 3.4068193435668945, acc: 0.3888888955116272)
[2025-02-04 00:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:07][root][INFO] - Training Epoch: 2/2, step 20970/23838 completed (loss: 3.2307004928588867, acc: 0.3870967626571655)
[2025-02-04 00:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:07][root][INFO] - Training Epoch: 2/2, step 20971/23838 completed (loss: 1.9595247507095337, acc: 0.4000000059604645)
[2025-02-04 00:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:08][root][INFO] - Training Epoch: 2/2, step 20972/23838 completed (loss: 2.6183688640594482, acc: 0.5)
[2025-02-04 00:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:08][root][INFO] - Training Epoch: 2/2, step 20973/23838 completed (loss: 2.5206165313720703, acc: 0.5555555820465088)
[2025-02-04 00:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:09][root][INFO] - Training Epoch: 2/2, step 20974/23838 completed (loss: 2.4492244720458984, acc: 0.5)
[2025-02-04 00:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:09][root][INFO] - Training Epoch: 2/2, step 20975/23838 completed (loss: 2.202300548553467, acc: 0.5)
[2025-02-04 00:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:10][root][INFO] - Training Epoch: 2/2, step 20976/23838 completed (loss: 2.3346755504608154, acc: 0.5)
[2025-02-04 00:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:10][root][INFO] - Training Epoch: 2/2, step 20977/23838 completed (loss: 2.965425729751587, acc: 0.4193548262119293)
[2025-02-04 00:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:11][root][INFO] - Training Epoch: 2/2, step 20978/23838 completed (loss: 2.887213706970215, acc: 0.4545454680919647)
[2025-02-04 00:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:11][root][INFO] - Training Epoch: 2/2, step 20979/23838 completed (loss: 3.544679880142212, acc: 0.3513513505458832)
[2025-02-04 00:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:12][root][INFO] - Training Epoch: 2/2, step 20980/23838 completed (loss: 2.5272879600524902, acc: 0.6111111044883728)
[2025-02-04 00:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:12][root][INFO] - Training Epoch: 2/2, step 20981/23838 completed (loss: 2.295832395553589, acc: 0.4642857015132904)
[2025-02-04 00:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:13][root][INFO] - Training Epoch: 2/2, step 20982/23838 completed (loss: 3.327624559402466, acc: 0.3888888955116272)
[2025-02-04 00:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:13][root][INFO] - Training Epoch: 2/2, step 20983/23838 completed (loss: 2.9544169902801514, acc: 0.44999998807907104)
[2025-02-04 00:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:14][root][INFO] - Training Epoch: 2/2, step 20984/23838 completed (loss: 2.8415207862854004, acc: 0.4000000059604645)
[2025-02-04 00:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:14][root][INFO] - Training Epoch: 2/2, step 20985/23838 completed (loss: 2.9880483150482178, acc: 0.38235294818878174)
[2025-02-04 00:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:15][root][INFO] - Training Epoch: 2/2, step 20986/23838 completed (loss: 2.9441347122192383, acc: 0.4054054021835327)
[2025-02-04 00:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:15][root][INFO] - Training Epoch: 2/2, step 20987/23838 completed (loss: 1.9460200071334839, acc: 0.5)
[2025-02-04 00:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:16][root][INFO] - Training Epoch: 2/2, step 20988/23838 completed (loss: 2.8320653438568115, acc: 0.4642857015132904)
[2025-02-04 00:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:16][root][INFO] - Training Epoch: 2/2, step 20989/23838 completed (loss: 4.171271324157715, acc: 0.23333333432674408)
[2025-02-04 00:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:17][root][INFO] - Training Epoch: 2/2, step 20990/23838 completed (loss: 1.904568076133728, acc: 0.5714285969734192)
[2025-02-04 00:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:17][root][INFO] - Training Epoch: 2/2, step 20991/23838 completed (loss: 2.4251630306243896, acc: 0.4117647111415863)
[2025-02-04 00:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:18][root][INFO] - Training Epoch: 2/2, step 20992/23838 completed (loss: 2.9965598583221436, acc: 0.37931033968925476)
[2025-02-04 00:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:18][root][INFO] - Training Epoch: 2/2, step 20993/23838 completed (loss: 2.428936719894409, acc: 0.3333333432674408)
[2025-02-04 00:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:18][root][INFO] - Training Epoch: 2/2, step 20994/23838 completed (loss: 2.367340326309204, acc: 0.47058823704719543)
[2025-02-04 00:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:19][root][INFO] - Training Epoch: 2/2, step 20995/23838 completed (loss: 1.5339603424072266, acc: 0.6666666865348816)
[2025-02-04 00:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:19][root][INFO] - Training Epoch: 2/2, step 20996/23838 completed (loss: 2.7771928310394287, acc: 0.5)
[2025-02-04 00:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:20][root][INFO] - Training Epoch: 2/2, step 20997/23838 completed (loss: 3.224961280822754, acc: 0.2916666567325592)
[2025-02-04 00:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:20][root][INFO] - Training Epoch: 2/2, step 20998/23838 completed (loss: 4.360763072967529, acc: 0.30434781312942505)
[2025-02-04 00:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:21][root][INFO] - Training Epoch: 2/2, step 20999/23838 completed (loss: 2.4881370067596436, acc: 0.375)
[2025-02-04 00:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:21][root][INFO] - Training Epoch: 2/2, step 21000/23838 completed (loss: 3.2349321842193604, acc: 0.36666667461395264)
[2025-02-04 00:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:22][root][INFO] - Training Epoch: 2/2, step 21001/23838 completed (loss: 2.346341371536255, acc: 0.3333333432674408)
[2025-02-04 00:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:22][root][INFO] - Training Epoch: 2/2, step 21002/23838 completed (loss: 2.901646375656128, acc: 0.5)
[2025-02-04 00:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:23][root][INFO] - Training Epoch: 2/2, step 21003/23838 completed (loss: 2.529207706451416, acc: 0.47058823704719543)
[2025-02-04 00:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:23][root][INFO] - Training Epoch: 2/2, step 21004/23838 completed (loss: 2.5008037090301514, acc: 0.5454545617103577)
[2025-02-04 00:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:23][root][INFO] - Training Epoch: 2/2, step 21005/23838 completed (loss: 2.981405258178711, acc: 0.3870967626571655)
[2025-02-04 00:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:24][root][INFO] - Training Epoch: 2/2, step 21006/23838 completed (loss: 2.267442464828491, acc: 0.5454545617103577)
[2025-02-04 00:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:24][root][INFO] - Training Epoch: 2/2, step 21007/23838 completed (loss: 3.3307623863220215, acc: 0.4871794879436493)
[2025-02-04 00:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:25][root][INFO] - Training Epoch: 2/2, step 21008/23838 completed (loss: 2.917684555053711, acc: 0.4193548262119293)
[2025-02-04 00:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:25][root][INFO] - Training Epoch: 2/2, step 21009/23838 completed (loss: 2.0322558879852295, acc: 0.5925925970077515)
[2025-02-04 00:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:26][root][INFO] - Training Epoch: 2/2, step 21010/23838 completed (loss: 2.5451793670654297, acc: 0.5185185074806213)
[2025-02-04 00:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:26][root][INFO] - Training Epoch: 2/2, step 21011/23838 completed (loss: 1.4687657356262207, acc: 0.5789473652839661)
[2025-02-04 00:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:27][root][INFO] - Training Epoch: 2/2, step 21012/23838 completed (loss: 1.844788670539856, acc: 0.7857142686843872)
[2025-02-04 00:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:27][root][INFO] - Training Epoch: 2/2, step 21013/23838 completed (loss: 1.350744605064392, acc: 0.7894737124443054)
[2025-02-04 00:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:28][root][INFO] - Training Epoch: 2/2, step 21014/23838 completed (loss: 2.2418148517608643, acc: 0.3684210479259491)
[2025-02-04 00:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:28][root][INFO] - Training Epoch: 2/2, step 21015/23838 completed (loss: 2.271944999694824, acc: 0.5666666626930237)
[2025-02-04 00:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:28][root][INFO] - Training Epoch: 2/2, step 21016/23838 completed (loss: 1.6305774450302124, acc: 0.5454545617103577)
[2025-02-04 00:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:29][root][INFO] - Training Epoch: 2/2, step 21017/23838 completed (loss: 2.116323709487915, acc: 0.5416666865348816)
[2025-02-04 00:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:29][root][INFO] - Training Epoch: 2/2, step 21018/23838 completed (loss: 1.7033989429473877, acc: 0.699999988079071)
[2025-02-04 00:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:30][root][INFO] - Training Epoch: 2/2, step 21019/23838 completed (loss: 2.7938759326934814, acc: 0.3448275923728943)
[2025-02-04 00:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:30][root][INFO] - Training Epoch: 2/2, step 21020/23838 completed (loss: 2.385451078414917, acc: 0.5)
[2025-02-04 00:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:31][root][INFO] - Training Epoch: 2/2, step 21021/23838 completed (loss: 2.76237154006958, acc: 0.4651162922382355)
[2025-02-04 00:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:31][root][INFO] - Training Epoch: 2/2, step 21022/23838 completed (loss: 2.564589262008667, acc: 0.4000000059604645)
[2025-02-04 00:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:31][root][INFO] - Training Epoch: 2/2, step 21023/23838 completed (loss: 3.5240321159362793, acc: 0.4285714328289032)
[2025-02-04 00:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:32][root][INFO] - Training Epoch: 2/2, step 21024/23838 completed (loss: 3.3443939685821533, acc: 0.34210526943206787)
[2025-02-04 00:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:32][root][INFO] - Training Epoch: 2/2, step 21025/23838 completed (loss: 1.326877236366272, acc: 0.6875)
[2025-02-04 00:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:33][root][INFO] - Training Epoch: 2/2, step 21026/23838 completed (loss: 1.883840799331665, acc: 0.5517241358757019)
[2025-02-04 00:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:33][root][INFO] - Training Epoch: 2/2, step 21027/23838 completed (loss: 3.210550546646118, acc: 0.40740740299224854)
[2025-02-04 00:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:34][root][INFO] - Training Epoch: 2/2, step 21028/23838 completed (loss: 2.5158379077911377, acc: 0.4054054021835327)
[2025-02-04 00:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:34][root][INFO] - Training Epoch: 2/2, step 21029/23838 completed (loss: 3.356245517730713, acc: 0.4000000059604645)
[2025-02-04 00:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:35][root][INFO] - Training Epoch: 2/2, step 21030/23838 completed (loss: 2.0259885787963867, acc: 0.5833333134651184)
[2025-02-04 00:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:35][root][INFO] - Training Epoch: 2/2, step 21031/23838 completed (loss: 2.528357982635498, acc: 0.5)
[2025-02-04 00:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:35][root][INFO] - Training Epoch: 2/2, step 21032/23838 completed (loss: 3.2752389907836914, acc: 0.4615384638309479)
[2025-02-04 00:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:36][root][INFO] - Training Epoch: 2/2, step 21033/23838 completed (loss: 2.6025421619415283, acc: 0.5862069129943848)
[2025-02-04 00:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:36][root][INFO] - Training Epoch: 2/2, step 21034/23838 completed (loss: 1.820566177368164, acc: 0.6363636255264282)
[2025-02-04 00:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:37][root][INFO] - Training Epoch: 2/2, step 21035/23838 completed (loss: 1.9556405544281006, acc: 0.5806451439857483)
[2025-02-04 00:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:37][root][INFO] - Training Epoch: 2/2, step 21036/23838 completed (loss: 2.5909745693206787, acc: 0.4583333432674408)
[2025-02-04 00:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:37][root][INFO] - Training Epoch: 2/2, step 21037/23838 completed (loss: 3.462430953979492, acc: 0.37931033968925476)
[2025-02-04 00:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:38][root][INFO] - Training Epoch: 2/2, step 21038/23838 completed (loss: 3.125958204269409, acc: 0.4375)
[2025-02-04 00:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:38][root][INFO] - Training Epoch: 2/2, step 21039/23838 completed (loss: 4.010748863220215, acc: 0.37037035822868347)
[2025-02-04 00:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:39][root][INFO] - Training Epoch: 2/2, step 21040/23838 completed (loss: 2.7052054405212402, acc: 0.5789473652839661)
[2025-02-04 00:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:39][root][INFO] - Training Epoch: 2/2, step 21041/23838 completed (loss: 3.1048200130462646, acc: 0.46666666865348816)
[2025-02-04 00:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:40][root][INFO] - Training Epoch: 2/2, step 21042/23838 completed (loss: 2.357440710067749, acc: 0.5)
[2025-02-04 00:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:40][root][INFO] - Training Epoch: 2/2, step 21043/23838 completed (loss: 2.626913070678711, acc: 0.5)
[2025-02-04 00:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:40][root][INFO] - Training Epoch: 2/2, step 21044/23838 completed (loss: 2.6822128295898438, acc: 0.550000011920929)
[2025-02-04 00:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:41][root][INFO] - Training Epoch: 2/2, step 21045/23838 completed (loss: 5.268586158752441, acc: 0.3571428656578064)
[2025-02-04 00:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:41][root][INFO] - Training Epoch: 2/2, step 21046/23838 completed (loss: 3.7301530838012695, acc: 0.3636363744735718)
[2025-02-04 00:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:42][root][INFO] - Training Epoch: 2/2, step 21047/23838 completed (loss: 3.9894301891326904, acc: 0.25925925374031067)
[2025-02-04 00:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:42][root][INFO] - Training Epoch: 2/2, step 21048/23838 completed (loss: 2.2846014499664307, acc: 0.4000000059604645)
[2025-02-04 00:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:43][root][INFO] - Training Epoch: 2/2, step 21049/23838 completed (loss: 3.3504886627197266, acc: 0.4375)
[2025-02-04 00:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:43][root][INFO] - Training Epoch: 2/2, step 21050/23838 completed (loss: 4.444216728210449, acc: 0.3076923191547394)
[2025-02-04 00:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:44][root][INFO] - Training Epoch: 2/2, step 21051/23838 completed (loss: 3.6466991901397705, acc: 0.15000000596046448)
[2025-02-04 00:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:44][root][INFO] - Training Epoch: 2/2, step 21052/23838 completed (loss: 2.274198532104492, acc: 0.5263158082962036)
[2025-02-04 00:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:45][root][INFO] - Training Epoch: 2/2, step 21053/23838 completed (loss: 2.102778434753418, acc: 0.4848484992980957)
[2025-02-04 00:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:45][root][INFO] - Training Epoch: 2/2, step 21054/23838 completed (loss: 3.2324492931365967, acc: 0.4137931168079376)
[2025-02-04 00:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:45][root][INFO] - Training Epoch: 2/2, step 21055/23838 completed (loss: 2.647148847579956, acc: 0.47999998927116394)
[2025-02-04 00:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:46][root][INFO] - Training Epoch: 2/2, step 21056/23838 completed (loss: 2.578805685043335, acc: 0.4117647111415863)
[2025-02-04 00:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:46][root][INFO] - Training Epoch: 2/2, step 21057/23838 completed (loss: 1.5319515466690063, acc: 0.5757575631141663)
[2025-02-04 00:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:47][root][INFO] - Training Epoch: 2/2, step 21058/23838 completed (loss: 3.161590337753296, acc: 0.3529411852359772)
[2025-02-04 00:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:47][root][INFO] - Training Epoch: 2/2, step 21059/23838 completed (loss: 2.513023614883423, acc: 0.52173912525177)
[2025-02-04 00:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:48][root][INFO] - Training Epoch: 2/2, step 21060/23838 completed (loss: 2.546257495880127, acc: 0.5357142686843872)
[2025-02-04 00:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:48][root][INFO] - Training Epoch: 2/2, step 21061/23838 completed (loss: 3.138922929763794, acc: 0.4285714328289032)
[2025-02-04 00:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:49][root][INFO] - Training Epoch: 2/2, step 21062/23838 completed (loss: 2.843654155731201, acc: 0.42307692766189575)
[2025-02-04 00:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:49][root][INFO] - Training Epoch: 2/2, step 21063/23838 completed (loss: 2.6769444942474365, acc: 0.380952388048172)
[2025-02-04 00:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:50][root][INFO] - Training Epoch: 2/2, step 21064/23838 completed (loss: 1.8068249225616455, acc: 0.6451612710952759)
[2025-02-04 00:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:50][root][INFO] - Training Epoch: 2/2, step 21065/23838 completed (loss: 2.218027114868164, acc: 0.5)
[2025-02-04 00:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:51][root][INFO] - Training Epoch: 2/2, step 21066/23838 completed (loss: 1.3301299810409546, acc: 0.6666666865348816)
[2025-02-04 00:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:51][root][INFO] - Training Epoch: 2/2, step 21067/23838 completed (loss: 2.5482521057128906, acc: 0.23529411852359772)
[2025-02-04 00:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:51][root][INFO] - Training Epoch: 2/2, step 21068/23838 completed (loss: 1.9501850605010986, acc: 0.6000000238418579)
[2025-02-04 00:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:52][root][INFO] - Training Epoch: 2/2, step 21069/23838 completed (loss: 1.7414203882217407, acc: 0.625)
[2025-02-04 00:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:52][root][INFO] - Training Epoch: 2/2, step 21070/23838 completed (loss: 1.9126118421554565, acc: 0.6111111044883728)
[2025-02-04 00:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:53][root][INFO] - Training Epoch: 2/2, step 21071/23838 completed (loss: 2.343259572982788, acc: 0.5681818127632141)
[2025-02-04 00:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:53][root][INFO] - Training Epoch: 2/2, step 21072/23838 completed (loss: 3.572906255722046, acc: 0.3199999928474426)
[2025-02-04 00:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:54][root][INFO] - Training Epoch: 2/2, step 21073/23838 completed (loss: 1.1765046119689941, acc: 0.7272727489471436)
[2025-02-04 00:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:54][root][INFO] - Training Epoch: 2/2, step 21074/23838 completed (loss: 2.3420307636260986, acc: 0.6666666865348816)
[2025-02-04 00:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:55][root][INFO] - Training Epoch: 2/2, step 21075/23838 completed (loss: 3.62290096282959, acc: 0.3461538553237915)
[2025-02-04 00:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:55][root][INFO] - Training Epoch: 2/2, step 21076/23838 completed (loss: 1.9446337223052979, acc: 0.5483871102333069)
[2025-02-04 00:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:55][root][INFO] - Training Epoch: 2/2, step 21077/23838 completed (loss: 3.2456412315368652, acc: 0.4000000059604645)
[2025-02-04 00:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:56][root][INFO] - Training Epoch: 2/2, step 21078/23838 completed (loss: 3.1377358436584473, acc: 0.28125)
[2025-02-04 00:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:56][root][INFO] - Training Epoch: 2/2, step 21079/23838 completed (loss: 2.2765796184539795, acc: 0.46875)
[2025-02-04 00:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:57][root][INFO] - Training Epoch: 2/2, step 21080/23838 completed (loss: 2.5493807792663574, acc: 0.5588235259056091)
[2025-02-04 00:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:57][root][INFO] - Training Epoch: 2/2, step 21081/23838 completed (loss: 1.93219792842865, acc: 0.59375)
[2025-02-04 00:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:58][root][INFO] - Training Epoch: 2/2, step 21082/23838 completed (loss: 2.8394954204559326, acc: 0.47826087474823)
[2025-02-04 00:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:58][root][INFO] - Training Epoch: 2/2, step 21083/23838 completed (loss: 2.212442636489868, acc: 0.5)
[2025-02-04 00:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:59][root][INFO] - Training Epoch: 2/2, step 21084/23838 completed (loss: 2.457568883895874, acc: 0.5600000023841858)
[2025-02-04 00:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:59][root][INFO] - Training Epoch: 2/2, step 21085/23838 completed (loss: 2.754260540008545, acc: 0.4516128897666931)
[2025-02-04 00:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:56:59][root][INFO] - Training Epoch: 2/2, step 21086/23838 completed (loss: 2.7151570320129395, acc: 0.42307692766189575)
[2025-02-04 00:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:00][root][INFO] - Training Epoch: 2/2, step 21087/23838 completed (loss: 2.2677853107452393, acc: 0.42424243688583374)
[2025-02-04 00:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:00][root][INFO] - Training Epoch: 2/2, step 21088/23838 completed (loss: 2.857654094696045, acc: 0.4642857015132904)
[2025-02-04 00:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:01][root][INFO] - Training Epoch: 2/2, step 21089/23838 completed (loss: 2.6192142963409424, acc: 0.4482758641242981)
[2025-02-04 00:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:01][root][INFO] - Training Epoch: 2/2, step 21090/23838 completed (loss: 2.417231321334839, acc: 0.5142857432365417)
[2025-02-04 00:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:01][root][INFO] - Training Epoch: 2/2, step 21091/23838 completed (loss: 2.03720760345459, acc: 0.6000000238418579)
[2025-02-04 00:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:02][root][INFO] - Training Epoch: 2/2, step 21092/23838 completed (loss: 2.621588706970215, acc: 0.4399999976158142)
[2025-02-04 00:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:02][root][INFO] - Training Epoch: 2/2, step 21093/23838 completed (loss: 1.596906304359436, acc: 0.6875)
[2025-02-04 00:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:03][root][INFO] - Training Epoch: 2/2, step 21094/23838 completed (loss: 1.9928648471832275, acc: 0.44999998807907104)
[2025-02-04 00:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:03][root][INFO] - Training Epoch: 2/2, step 21095/23838 completed (loss: 3.1679069995880127, acc: 0.523809552192688)
[2025-02-04 00:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:04][root][INFO] - Training Epoch: 2/2, step 21096/23838 completed (loss: 4.361114025115967, acc: 0.4000000059604645)
[2025-02-04 00:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:04][root][INFO] - Training Epoch: 2/2, step 21097/23838 completed (loss: 2.5716795921325684, acc: 0.6000000238418579)
[2025-02-04 00:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:04][root][INFO] - Training Epoch: 2/2, step 21098/23838 completed (loss: 2.62884259223938, acc: 0.5)
[2025-02-04 00:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:05][root][INFO] - Training Epoch: 2/2, step 21099/23838 completed (loss: 3.58994722366333, acc: 0.3076923191547394)
[2025-02-04 00:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:05][root][INFO] - Training Epoch: 2/2, step 21100/23838 completed (loss: 1.7279407978057861, acc: 0.5384615659713745)
[2025-02-04 00:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:06][root][INFO] - Training Epoch: 2/2, step 21101/23838 completed (loss: 2.2991504669189453, acc: 0.6190476417541504)
[2025-02-04 00:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:06][root][INFO] - Training Epoch: 2/2, step 21102/23838 completed (loss: 1.6472218036651611, acc: 0.7857142686843872)
[2025-02-04 00:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:07][root][INFO] - Training Epoch: 2/2, step 21103/23838 completed (loss: 1.7514944076538086, acc: 0.625)
[2025-02-04 00:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:07][root][INFO] - Training Epoch: 2/2, step 21104/23838 completed (loss: 2.740062952041626, acc: 0.5384615659713745)
[2025-02-04 00:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:08][root][INFO] - Training Epoch: 2/2, step 21105/23838 completed (loss: 2.9449193477630615, acc: 0.5)
[2025-02-04 00:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:08][root][INFO] - Training Epoch: 2/2, step 21106/23838 completed (loss: 4.769396781921387, acc: 0.5)
[2025-02-04 00:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:09][root][INFO] - Training Epoch: 2/2, step 21107/23838 completed (loss: 2.3716201782226562, acc: 0.5454545617103577)
[2025-02-04 00:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:09][root][INFO] - Training Epoch: 2/2, step 21108/23838 completed (loss: 4.162083625793457, acc: 0.4000000059604645)
[2025-02-04 00:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:10][root][INFO] - Training Epoch: 2/2, step 21109/23838 completed (loss: 3.698943853378296, acc: 0.4615384638309479)
[2025-02-04 00:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:10][root][INFO] - Training Epoch: 2/2, step 21110/23838 completed (loss: 3.39337420463562, acc: 0.4615384638309479)
[2025-02-04 00:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:11][root][INFO] - Training Epoch: 2/2, step 21111/23838 completed (loss: 4.472782135009766, acc: 0.3076923191547394)
[2025-02-04 00:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:11][root][INFO] - Training Epoch: 2/2, step 21112/23838 completed (loss: 3.144273519515991, acc: 0.4000000059604645)
[2025-02-04 00:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:12][root][INFO] - Training Epoch: 2/2, step 21113/23838 completed (loss: 3.7020201683044434, acc: 0.3636363744735718)
[2025-02-04 00:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:12][root][INFO] - Training Epoch: 2/2, step 21114/23838 completed (loss: 4.348989009857178, acc: 0.25)
[2025-02-04 00:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:13][root][INFO] - Training Epoch: 2/2, step 21115/23838 completed (loss: 4.236575603485107, acc: 0.25)
[2025-02-04 00:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:13][root][INFO] - Training Epoch: 2/2, step 21116/23838 completed (loss: 4.867535591125488, acc: 0.2800000011920929)
[2025-02-04 00:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:13][root][INFO] - Training Epoch: 2/2, step 21117/23838 completed (loss: 3.137848138809204, acc: 0.375)
[2025-02-04 00:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:14][root][INFO] - Training Epoch: 2/2, step 21118/23838 completed (loss: 4.719664573669434, acc: 0.3076923191547394)
[2025-02-04 00:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:14][root][INFO] - Training Epoch: 2/2, step 21119/23838 completed (loss: 3.9821078777313232, acc: 0.3333333432674408)
[2025-02-04 00:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:15][root][INFO] - Training Epoch: 2/2, step 21120/23838 completed (loss: 3.137542963027954, acc: 0.47058823704719543)
[2025-02-04 00:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:15][root][INFO] - Training Epoch: 2/2, step 21121/23838 completed (loss: 4.109774589538574, acc: 0.1428571492433548)
[2025-02-04 00:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:16][root][INFO] - Training Epoch: 2/2, step 21122/23838 completed (loss: 3.8592958450317383, acc: 0.3888888955116272)
[2025-02-04 00:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:16][root][INFO] - Training Epoch: 2/2, step 21123/23838 completed (loss: 4.0036492347717285, acc: 0.31111112236976624)
[2025-02-04 00:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:16][root][INFO] - Training Epoch: 2/2, step 21124/23838 completed (loss: 4.163907527923584, acc: 0.25)
[2025-02-04 00:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:17][root][INFO] - Training Epoch: 2/2, step 21125/23838 completed (loss: 4.574672222137451, acc: 0.10256410390138626)
[2025-02-04 00:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:17][root][INFO] - Training Epoch: 2/2, step 21126/23838 completed (loss: 2.378729820251465, acc: 0.3888888955116272)
[2025-02-04 00:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:18][root][INFO] - Training Epoch: 2/2, step 21127/23838 completed (loss: 5.265535831451416, acc: 0.23529411852359772)
[2025-02-04 00:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:18][root][INFO] - Training Epoch: 2/2, step 21128/23838 completed (loss: 3.608930826187134, acc: 0.4545454680919647)
[2025-02-04 00:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:19][root][INFO] - Training Epoch: 2/2, step 21129/23838 completed (loss: 3.673001289367676, acc: 0.3333333432674408)
[2025-02-04 00:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:19][root][INFO] - Training Epoch: 2/2, step 21130/23838 completed (loss: 3.533139705657959, acc: 0.30434781312942505)
[2025-02-04 00:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:20][root][INFO] - Training Epoch: 2/2, step 21131/23838 completed (loss: 3.780107021331787, acc: 0.34375)
[2025-02-04 00:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:20][root][INFO] - Training Epoch: 2/2, step 21132/23838 completed (loss: 3.146458864212036, acc: 0.3333333432674408)
[2025-02-04 00:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:21][root][INFO] - Training Epoch: 2/2, step 21133/23838 completed (loss: 2.1109585762023926, acc: 0.5909090638160706)
[2025-02-04 00:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:21][root][INFO] - Training Epoch: 2/2, step 21134/23838 completed (loss: 3.3468148708343506, acc: 0.4545454680919647)
[2025-02-04 00:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:21][root][INFO] - Training Epoch: 2/2, step 21135/23838 completed (loss: 2.5796828269958496, acc: 0.44999998807907104)
[2025-02-04 00:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:22][root][INFO] - Training Epoch: 2/2, step 21136/23838 completed (loss: 4.5385518074035645, acc: 0.40909090638160706)
[2025-02-04 00:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:22][root][INFO] - Training Epoch: 2/2, step 21137/23838 completed (loss: 2.771146059036255, acc: 0.380952388048172)
[2025-02-04 00:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:23][root][INFO] - Training Epoch: 2/2, step 21138/23838 completed (loss: 4.6630964279174805, acc: 0.29629629850387573)
[2025-02-04 00:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:23][root][INFO] - Training Epoch: 2/2, step 21139/23838 completed (loss: 3.0787975788116455, acc: 0.4333333373069763)
[2025-02-04 00:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:24][root][INFO] - Training Epoch: 2/2, step 21140/23838 completed (loss: 3.064042568206787, acc: 0.5)
[2025-02-04 00:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:24][root][INFO] - Training Epoch: 2/2, step 21141/23838 completed (loss: 2.835113763809204, acc: 0.42307692766189575)
[2025-02-04 00:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:25][root][INFO] - Training Epoch: 2/2, step 21142/23838 completed (loss: 2.745577573776245, acc: 0.5416666865348816)
[2025-02-04 00:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:25][root][INFO] - Training Epoch: 2/2, step 21143/23838 completed (loss: 2.3760523796081543, acc: 0.4545454680919647)
[2025-02-04 00:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:26][root][INFO] - Training Epoch: 2/2, step 21144/23838 completed (loss: 3.7723586559295654, acc: 0.3333333432674408)
[2025-02-04 00:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:26][root][INFO] - Training Epoch: 2/2, step 21145/23838 completed (loss: 2.8881783485412598, acc: 0.5)
[2025-02-04 00:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:26][root][INFO] - Training Epoch: 2/2, step 21146/23838 completed (loss: 4.59628963470459, acc: 0.24242424964904785)
[2025-02-04 00:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:27][root][INFO] - Training Epoch: 2/2, step 21147/23838 completed (loss: 3.746230125427246, acc: 0.4583333432674408)
[2025-02-04 00:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:27][root][INFO] - Training Epoch: 2/2, step 21148/23838 completed (loss: 3.211540460586548, acc: 0.3513513505458832)
[2025-02-04 00:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:28][root][INFO] - Training Epoch: 2/2, step 21149/23838 completed (loss: 1.1609920263290405, acc: 0.739130437374115)
[2025-02-04 00:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:28][root][INFO] - Training Epoch: 2/2, step 21150/23838 completed (loss: 4.419333457946777, acc: 0.4333333373069763)
[2025-02-04 00:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:29][root][INFO] - Training Epoch: 2/2, step 21151/23838 completed (loss: 2.5858752727508545, acc: 0.523809552192688)
[2025-02-04 00:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:29][root][INFO] - Training Epoch: 2/2, step 21152/23838 completed (loss: 3.8451812267303467, acc: 0.3125)
[2025-02-04 00:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:30][root][INFO] - Training Epoch: 2/2, step 21153/23838 completed (loss: 4.285513877868652, acc: 0.23999999463558197)
[2025-02-04 00:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:30][root][INFO] - Training Epoch: 2/2, step 21154/23838 completed (loss: 3.0973989963531494, acc: 0.3870967626571655)
[2025-02-04 00:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:31][root][INFO] - Training Epoch: 2/2, step 21155/23838 completed (loss: 2.1928086280822754, acc: 0.5384615659713745)
[2025-02-04 00:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:31][root][INFO] - Training Epoch: 2/2, step 21156/23838 completed (loss: 2.3901429176330566, acc: 0.4761904776096344)
[2025-02-04 00:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:32][root][INFO] - Training Epoch: 2/2, step 21157/23838 completed (loss: 3.6846182346343994, acc: 0.40909090638160706)
[2025-02-04 00:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:32][root][INFO] - Training Epoch: 2/2, step 21158/23838 completed (loss: 3.013597249984741, acc: 0.375)
[2025-02-04 00:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:32][root][INFO] - Training Epoch: 2/2, step 21159/23838 completed (loss: 4.528689861297607, acc: 0.4642857015132904)
[2025-02-04 00:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:33][root][INFO] - Training Epoch: 2/2, step 21160/23838 completed (loss: 3.7417891025543213, acc: 0.4000000059604645)
[2025-02-04 00:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:33][root][INFO] - Training Epoch: 2/2, step 21161/23838 completed (loss: 3.1093878746032715, acc: 0.47999998927116394)
[2025-02-04 00:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:34][root][INFO] - Training Epoch: 2/2, step 21162/23838 completed (loss: 3.645266056060791, acc: 0.40625)
[2025-02-04 00:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:34][root][INFO] - Training Epoch: 2/2, step 21163/23838 completed (loss: 5.553332805633545, acc: 0.2666666805744171)
[2025-02-04 00:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:35][root][INFO] - Training Epoch: 2/2, step 21164/23838 completed (loss: 5.38918924331665, acc: 0.2777777910232544)
[2025-02-04 00:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:35][root][INFO] - Training Epoch: 2/2, step 21165/23838 completed (loss: 2.033437728881836, acc: 0.5555555820465088)
[2025-02-04 00:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:36][root][INFO] - Training Epoch: 2/2, step 21166/23838 completed (loss: 3.073024034500122, acc: 0.29629629850387573)
[2025-02-04 00:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:36][root][INFO] - Training Epoch: 2/2, step 21167/23838 completed (loss: 3.1552605628967285, acc: 0.4117647111415863)
[2025-02-04 00:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:37][root][INFO] - Training Epoch: 2/2, step 21168/23838 completed (loss: 3.056589365005493, acc: 0.42307692766189575)
[2025-02-04 00:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:37][root][INFO] - Training Epoch: 2/2, step 21169/23838 completed (loss: 3.2286908626556396, acc: 0.4117647111415863)
[2025-02-04 00:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:38][root][INFO] - Training Epoch: 2/2, step 21170/23838 completed (loss: 3.0104713439941406, acc: 0.47058823704719543)
[2025-02-04 00:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:38][root][INFO] - Training Epoch: 2/2, step 21171/23838 completed (loss: 2.996554136276245, acc: 0.40740740299224854)
[2025-02-04 00:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:39][root][INFO] - Training Epoch: 2/2, step 21172/23838 completed (loss: 0.6755883097648621, acc: 0.8888888955116272)
[2025-02-04 00:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:39][root][INFO] - Training Epoch: 2/2, step 21173/23838 completed (loss: 2.73221755027771, acc: 0.4545454680919647)
[2025-02-04 00:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:40][root][INFO] - Training Epoch: 2/2, step 21174/23838 completed (loss: 1.120181679725647, acc: 0.75)
[2025-02-04 00:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:40][root][INFO] - Training Epoch: 2/2, step 21175/23838 completed (loss: 1.431264042854309, acc: 0.6363636255264282)
[2025-02-04 00:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:41][root][INFO] - Training Epoch: 2/2, step 21176/23838 completed (loss: 1.6593189239501953, acc: 0.800000011920929)
[2025-02-04 00:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:41][root][INFO] - Training Epoch: 2/2, step 21177/23838 completed (loss: 3.480642795562744, acc: 0.40909090638160706)
[2025-02-04 00:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:42][root][INFO] - Training Epoch: 2/2, step 21178/23838 completed (loss: 0.800269365310669, acc: 0.75)
[2025-02-04 00:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:42][root][INFO] - Training Epoch: 2/2, step 21179/23838 completed (loss: 3.711116075515747, acc: 0.3499999940395355)
[2025-02-04 00:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:43][root][INFO] - Training Epoch: 2/2, step 21180/23838 completed (loss: 3.3801753520965576, acc: 0.30434781312942505)
[2025-02-04 00:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:43][root][INFO] - Training Epoch: 2/2, step 21181/23838 completed (loss: 1.0308897495269775, acc: 0.800000011920929)
[2025-02-04 00:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:44][root][INFO] - Training Epoch: 2/2, step 21182/23838 completed (loss: 3.221895933151245, acc: 0.25)
[2025-02-04 00:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:44][root][INFO] - Training Epoch: 2/2, step 21183/23838 completed (loss: 3.074782133102417, acc: 0.4000000059604645)
[2025-02-04 00:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:44][root][INFO] - Training Epoch: 2/2, step 21184/23838 completed (loss: 3.3650710582733154, acc: 0.5555555820465088)
[2025-02-04 00:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:45][root][INFO] - Training Epoch: 2/2, step 21185/23838 completed (loss: 2.3153858184814453, acc: 0.5625)
[2025-02-04 00:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:45][root][INFO] - Training Epoch: 2/2, step 21186/23838 completed (loss: 0.6030281782150269, acc: 0.8461538553237915)
[2025-02-04 00:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:46][root][INFO] - Training Epoch: 2/2, step 21187/23838 completed (loss: 2.5249247550964355, acc: 0.5)
[2025-02-04 00:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:46][root][INFO] - Training Epoch: 2/2, step 21188/23838 completed (loss: 0.9742167592048645, acc: 0.692307710647583)
[2025-02-04 00:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:46][root][INFO] - Training Epoch: 2/2, step 21189/23838 completed (loss: 2.7910079956054688, acc: 0.5)
[2025-02-04 00:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:47][root][INFO] - Training Epoch: 2/2, step 21190/23838 completed (loss: 2.9978137016296387, acc: 0.375)
[2025-02-04 00:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:47][root][INFO] - Training Epoch: 2/2, step 21191/23838 completed (loss: 3.160364866256714, acc: 0.44999998807907104)
[2025-02-04 00:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:48][root][INFO] - Training Epoch: 2/2, step 21192/23838 completed (loss: 3.1388018131256104, acc: 0.40740740299224854)
[2025-02-04 00:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:48][root][INFO] - Training Epoch: 2/2, step 21193/23838 completed (loss: 1.9712496995925903, acc: 0.4444444477558136)
[2025-02-04 00:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:49][root][INFO] - Training Epoch: 2/2, step 21194/23838 completed (loss: 0.9859471917152405, acc: 0.8888888955116272)
[2025-02-04 00:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:49][root][INFO] - Training Epoch: 2/2, step 21195/23838 completed (loss: 4.466545581817627, acc: 0.19354838132858276)
[2025-02-04 00:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:49][root][INFO] - Training Epoch: 2/2, step 21196/23838 completed (loss: 1.5084692239761353, acc: 0.625)
[2025-02-04 00:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:50][root][INFO] - Training Epoch: 2/2, step 21197/23838 completed (loss: 3.1976568698883057, acc: 0.3125)
[2025-02-04 00:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:50][root][INFO] - Training Epoch: 2/2, step 21198/23838 completed (loss: 1.4682772159576416, acc: 0.7647058963775635)
[2025-02-04 00:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:51][root][INFO] - Training Epoch: 2/2, step 21199/23838 completed (loss: 2.3441083431243896, acc: 0.5)
[2025-02-04 00:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:51][root][INFO] - Training Epoch: 2/2, step 21200/23838 completed (loss: 0.9059650897979736, acc: 0.807692289352417)
[2025-02-04 00:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:52][root][INFO] - Training Epoch: 2/2, step 21201/23838 completed (loss: 0.7236936092376709, acc: 0.9230769276618958)
[2025-02-04 00:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:52][root][INFO] - Training Epoch: 2/2, step 21202/23838 completed (loss: 1.4010484218597412, acc: 0.6428571343421936)
[2025-02-04 00:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:53][root][INFO] - Training Epoch: 2/2, step 21203/23838 completed (loss: 4.810924530029297, acc: 0.27272728085517883)
[2025-02-04 00:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:53][root][INFO] - Training Epoch: 2/2, step 21204/23838 completed (loss: 2.0415000915527344, acc: 0.6666666865348816)
[2025-02-04 00:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:54][root][INFO] - Training Epoch: 2/2, step 21205/23838 completed (loss: 2.964109420776367, acc: 0.523809552192688)
[2025-02-04 00:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:54][root][INFO] - Training Epoch: 2/2, step 21206/23838 completed (loss: 3.5289058685302734, acc: 0.4285714328289032)
[2025-02-04 00:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:55][root][INFO] - Training Epoch: 2/2, step 21207/23838 completed (loss: 1.429725170135498, acc: 0.7777777910232544)
[2025-02-04 00:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:55][root][INFO] - Training Epoch: 2/2, step 21208/23838 completed (loss: 0.7877733707427979, acc: 0.625)
[2025-02-04 00:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:56][root][INFO] - Training Epoch: 2/2, step 21209/23838 completed (loss: 4.455958366394043, acc: 0.42105263471603394)
[2025-02-04 00:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:56][root][INFO] - Training Epoch: 2/2, step 21210/23838 completed (loss: 1.6223756074905396, acc: 0.7142857313156128)
[2025-02-04 00:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:57][root][INFO] - Training Epoch: 2/2, step 21211/23838 completed (loss: 3.283210277557373, acc: 0.529411792755127)
[2025-02-04 00:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:57][root][INFO] - Training Epoch: 2/2, step 21212/23838 completed (loss: 2.5404391288757324, acc: 0.4285714328289032)
[2025-02-04 00:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:58][root][INFO] - Training Epoch: 2/2, step 21213/23838 completed (loss: 2.9850287437438965, acc: 0.46666666865348816)
[2025-02-04 00:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:58][root][INFO] - Training Epoch: 2/2, step 21214/23838 completed (loss: 0.9639434814453125, acc: 0.6666666865348816)
[2025-02-04 00:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:59][root][INFO] - Training Epoch: 2/2, step 21215/23838 completed (loss: 1.363277554512024, acc: 0.7142857313156128)
[2025-02-04 00:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:59][root][INFO] - Training Epoch: 2/2, step 21216/23838 completed (loss: 1.6302096843719482, acc: 0.75)
[2025-02-04 00:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:57:59][root][INFO] - Training Epoch: 2/2, step 21217/23838 completed (loss: 3.924304723739624, acc: 0.24074074625968933)
[2025-02-04 00:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:00][root][INFO] - Training Epoch: 2/2, step 21218/23838 completed (loss: 3.267302989959717, acc: 0.4571428596973419)
[2025-02-04 00:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:00][root][INFO] - Training Epoch: 2/2, step 21219/23838 completed (loss: 3.526231288909912, acc: 0.2954545319080353)
[2025-02-04 00:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:01][root][INFO] - Training Epoch: 2/2, step 21220/23838 completed (loss: 3.0318071842193604, acc: 0.31707316637039185)
[2025-02-04 00:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:01][root][INFO] - Training Epoch: 2/2, step 21221/23838 completed (loss: 3.56892991065979, acc: 0.41025641560554504)
[2025-02-04 00:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:02][root][INFO] - Training Epoch: 2/2, step 21222/23838 completed (loss: 3.3144915103912354, acc: 0.22727273404598236)
[2025-02-04 00:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:02][root][INFO] - Training Epoch: 2/2, step 21223/23838 completed (loss: 2.896263360977173, acc: 0.47826087474823)
[2025-02-04 00:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:02][root][INFO] - Training Epoch: 2/2, step 21224/23838 completed (loss: 2.724440813064575, acc: 0.4117647111415863)
[2025-02-04 00:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:03][root][INFO] - Training Epoch: 2/2, step 21225/23838 completed (loss: 4.5677409172058105, acc: 0.29729729890823364)
[2025-02-04 00:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:03][root][INFO] - Training Epoch: 2/2, step 21226/23838 completed (loss: 4.147891044616699, acc: 0.2702702581882477)
[2025-02-04 00:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:04][root][INFO] - Training Epoch: 2/2, step 21227/23838 completed (loss: 3.970867395401001, acc: 0.29487180709838867)
[2025-02-04 00:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:04][root][INFO] - Training Epoch: 2/2, step 21228/23838 completed (loss: 4.150088787078857, acc: 0.2571428716182709)
[2025-02-04 00:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:04][root][INFO] - Training Epoch: 2/2, step 21229/23838 completed (loss: 2.714386224746704, acc: 0.375)
[2025-02-04 00:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:05][root][INFO] - Training Epoch: 2/2, step 21230/23838 completed (loss: 2.6419572830200195, acc: 0.4047619104385376)
[2025-02-04 00:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:05][root][INFO] - Training Epoch: 2/2, step 21231/23838 completed (loss: 2.2295498847961426, acc: 0.4399999976158142)
[2025-02-04 00:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:06][root][INFO] - Training Epoch: 2/2, step 21232/23838 completed (loss: 2.4935436248779297, acc: 0.48148149251937866)
[2025-02-04 00:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:06][root][INFO] - Training Epoch: 2/2, step 21233/23838 completed (loss: 2.2498984336853027, acc: 0.5128205418586731)
[2025-02-04 00:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:07][root][INFO] - Training Epoch: 2/2, step 21234/23838 completed (loss: 2.1179287433624268, acc: 0.5106382966041565)
[2025-02-04 00:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:07][root][INFO] - Training Epoch: 2/2, step 21235/23838 completed (loss: 2.5880703926086426, acc: 0.32258063554763794)
[2025-02-04 00:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:07][root][INFO] - Training Epoch: 2/2, step 21236/23838 completed (loss: 3.067249298095703, acc: 0.4375)
[2025-02-04 00:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:08][root][INFO] - Training Epoch: 2/2, step 21237/23838 completed (loss: 3.6346466541290283, acc: 0.25)
[2025-02-04 00:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:08][root][INFO] - Training Epoch: 2/2, step 21238/23838 completed (loss: 2.5423758029937744, acc: 0.5121951103210449)
[2025-02-04 00:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:09][root][INFO] - Training Epoch: 2/2, step 21239/23838 completed (loss: 3.3738579750061035, acc: 0.40909090638160706)
[2025-02-04 00:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:09][root][INFO] - Training Epoch: 2/2, step 21240/23838 completed (loss: 2.6716742515563965, acc: 0.5)
[2025-02-04 00:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:09][root][INFO] - Training Epoch: 2/2, step 21241/23838 completed (loss: 3.540407180786133, acc: 0.3541666567325592)
[2025-02-04 00:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:10][root][INFO] - Training Epoch: 2/2, step 21242/23838 completed (loss: 3.527944803237915, acc: 0.31707316637039185)
[2025-02-04 00:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:10][root][INFO] - Training Epoch: 2/2, step 21243/23838 completed (loss: 2.5937342643737793, acc: 0.4571428596973419)
[2025-02-04 00:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:11][root][INFO] - Training Epoch: 2/2, step 21244/23838 completed (loss: 2.293562173843384, acc: 0.5625)
[2025-02-04 00:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:11][root][INFO] - Training Epoch: 2/2, step 21245/23838 completed (loss: 2.400573492050171, acc: 0.5416666865348816)
[2025-02-04 00:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:12][root][INFO] - Training Epoch: 2/2, step 21246/23838 completed (loss: 2.218097448348999, acc: 0.47826087474823)
[2025-02-04 00:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:12][root][INFO] - Training Epoch: 2/2, step 21247/23838 completed (loss: 2.463803768157959, acc: 0.42105263471603394)
[2025-02-04 00:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:13][root][INFO] - Training Epoch: 2/2, step 21248/23838 completed (loss: 2.707491397857666, acc: 0.375)
[2025-02-04 00:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:13][root][INFO] - Training Epoch: 2/2, step 21249/23838 completed (loss: 2.2089486122131348, acc: 0.4285714328289032)
[2025-02-04 00:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:14][root][INFO] - Training Epoch: 2/2, step 21250/23838 completed (loss: 3.61509370803833, acc: 0.29629629850387573)
[2025-02-04 00:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:14][root][INFO] - Training Epoch: 2/2, step 21251/23838 completed (loss: 2.5129032135009766, acc: 0.48275861144065857)
[2025-02-04 00:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:14][root][INFO] - Training Epoch: 2/2, step 21252/23838 completed (loss: 2.010347604751587, acc: 0.5)
[2025-02-04 00:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:15][root][INFO] - Training Epoch: 2/2, step 21253/23838 completed (loss: 1.8981765508651733, acc: 0.5925925970077515)
[2025-02-04 00:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:15][root][INFO] - Training Epoch: 2/2, step 21254/23838 completed (loss: 2.684155225753784, acc: 0.4047619104385376)
[2025-02-04 00:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:16][root][INFO] - Training Epoch: 2/2, step 21255/23838 completed (loss: 2.392732620239258, acc: 0.5769230723381042)
[2025-02-04 00:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:17][root][INFO] - Training Epoch: 2/2, step 21256/23838 completed (loss: 3.308779001235962, acc: 0.5675675868988037)
[2025-02-04 00:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:17][root][INFO] - Training Epoch: 2/2, step 21257/23838 completed (loss: 1.9044359922409058, acc: 0.625)
[2025-02-04 00:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:18][root][INFO] - Training Epoch: 2/2, step 21258/23838 completed (loss: 2.933837652206421, acc: 0.4193548262119293)
[2025-02-04 00:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:18][root][INFO] - Training Epoch: 2/2, step 21259/23838 completed (loss: 2.112861156463623, acc: 0.5652173757553101)
[2025-02-04 00:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:19][root][INFO] - Training Epoch: 2/2, step 21260/23838 completed (loss: 2.484276533126831, acc: 0.4871794879436493)
[2025-02-04 00:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:19][root][INFO] - Training Epoch: 2/2, step 21261/23838 completed (loss: 2.435107707977295, acc: 0.4285714328289032)
[2025-02-04 00:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:20][root][INFO] - Training Epoch: 2/2, step 21262/23838 completed (loss: 1.6480234861373901, acc: 0.5757575631141663)
[2025-02-04 00:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:20][root][INFO] - Training Epoch: 2/2, step 21263/23838 completed (loss: 2.6121816635131836, acc: 0.4444444477558136)
[2025-02-04 00:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:21][root][INFO] - Training Epoch: 2/2, step 21264/23838 completed (loss: 1.7001711130142212, acc: 0.6470588445663452)
[2025-02-04 00:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:21][root][INFO] - Training Epoch: 2/2, step 21265/23838 completed (loss: 2.4366371631622314, acc: 0.5625)
[2025-02-04 00:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:22][root][INFO] - Training Epoch: 2/2, step 21266/23838 completed (loss: 2.2186524868011475, acc: 0.5142857432365417)
[2025-02-04 00:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:22][root][INFO] - Training Epoch: 2/2, step 21267/23838 completed (loss: 3.0697526931762695, acc: 0.4137931168079376)
[2025-02-04 00:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:23][root][INFO] - Training Epoch: 2/2, step 21268/23838 completed (loss: 2.409467935562134, acc: 0.4390243887901306)
[2025-02-04 00:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:24][root][INFO] - Training Epoch: 2/2, step 21269/23838 completed (loss: 2.9796142578125, acc: 0.33898305892944336)
[2025-02-04 00:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:24][root][INFO] - Training Epoch: 2/2, step 21270/23838 completed (loss: 2.134812831878662, acc: 0.4736842215061188)
[2025-02-04 00:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:25][root][INFO] - Training Epoch: 2/2, step 21271/23838 completed (loss: 4.258330345153809, acc: 0.3636363744735718)
[2025-02-04 00:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:26][root][INFO] - Training Epoch: 2/2, step 21272/23838 completed (loss: 3.2330875396728516, acc: 0.5)
[2025-02-04 00:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:26][root][INFO] - Training Epoch: 2/2, step 21273/23838 completed (loss: 3.203348398208618, acc: 0.46666666865348816)
[2025-02-04 00:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:26][root][INFO] - Training Epoch: 2/2, step 21274/23838 completed (loss: 3.023653268814087, acc: 0.47999998927116394)
[2025-02-04 00:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:27][root][INFO] - Training Epoch: 2/2, step 21275/23838 completed (loss: 2.4065160751342773, acc: 0.5185185074806213)
[2025-02-04 00:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:27][root][INFO] - Training Epoch: 2/2, step 21276/23838 completed (loss: 2.1432178020477295, acc: 0.5714285969734192)
[2025-02-04 00:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:28][root][INFO] - Training Epoch: 2/2, step 21277/23838 completed (loss: 3.293114185333252, acc: 0.5)
[2025-02-04 00:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:28][root][INFO] - Training Epoch: 2/2, step 21278/23838 completed (loss: 3.0594301223754883, acc: 0.4615384638309479)
[2025-02-04 00:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:29][root][INFO] - Training Epoch: 2/2, step 21279/23838 completed (loss: 3.7134127616882324, acc: 0.23333333432674408)
[2025-02-04 00:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:29][root][INFO] - Training Epoch: 2/2, step 21280/23838 completed (loss: 1.6200028657913208, acc: 0.5384615659713745)
[2025-02-04 00:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:29][root][INFO] - Training Epoch: 2/2, step 21281/23838 completed (loss: 3.56131649017334, acc: 0.2777777910232544)
[2025-02-04 00:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:30][root][INFO] - Training Epoch: 2/2, step 21282/23838 completed (loss: 2.580684185028076, acc: 0.37037035822868347)
[2025-02-04 00:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:30][root][INFO] - Training Epoch: 2/2, step 21283/23838 completed (loss: 2.082413911819458, acc: 0.550000011920929)
[2025-02-04 00:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:31][root][INFO] - Training Epoch: 2/2, step 21284/23838 completed (loss: 1.0700992345809937, acc: 0.774193525314331)
[2025-02-04 00:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:31][root][INFO] - Training Epoch: 2/2, step 21285/23838 completed (loss: 1.1549880504608154, acc: 0.7250000238418579)
[2025-02-04 00:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:32][root][INFO] - Training Epoch: 2/2, step 21286/23838 completed (loss: 0.9069288372993469, acc: 0.7586206793785095)
[2025-02-04 00:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:32][root][INFO] - Training Epoch: 2/2, step 21287/23838 completed (loss: 1.3961538076400757, acc: 0.6388888955116272)
[2025-02-04 00:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:32][root][INFO] - Training Epoch: 2/2, step 21288/23838 completed (loss: 1.7562615871429443, acc: 0.6333333253860474)
[2025-02-04 00:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:33][root][INFO] - Training Epoch: 2/2, step 21289/23838 completed (loss: 1.6960588693618774, acc: 0.6521739363670349)
[2025-02-04 00:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:33][root][INFO] - Training Epoch: 2/2, step 21290/23838 completed (loss: 1.0807452201843262, acc: 0.800000011920929)
[2025-02-04 00:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:34][root][INFO] - Training Epoch: 2/2, step 21291/23838 completed (loss: 1.8187428712844849, acc: 0.6333333253860474)
[2025-02-04 00:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:34][root][INFO] - Training Epoch: 2/2, step 21292/23838 completed (loss: 2.2751126289367676, acc: 0.6896551847457886)
[2025-02-04 00:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:35][root][INFO] - Training Epoch: 2/2, step 21293/23838 completed (loss: 2.844717502593994, acc: 0.692307710647583)
[2025-02-04 00:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:35][root][INFO] - Training Epoch: 2/2, step 21294/23838 completed (loss: 0.9824120998382568, acc: 0.75)
[2025-02-04 00:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:36][root][INFO] - Training Epoch: 2/2, step 21295/23838 completed (loss: 2.6454296112060547, acc: 0.47826087474823)
[2025-02-04 00:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:36][root][INFO] - Training Epoch: 2/2, step 21296/23838 completed (loss: 2.706451416015625, acc: 0.4545454680919647)
[2025-02-04 00:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:37][root][INFO] - Training Epoch: 2/2, step 21297/23838 completed (loss: 1.990125298500061, acc: 0.5263158082962036)
[2025-02-04 00:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:37][root][INFO] - Training Epoch: 2/2, step 21298/23838 completed (loss: 4.218716144561768, acc: 0.2777777910232544)
[2025-02-04 00:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:38][root][INFO] - Training Epoch: 2/2, step 21299/23838 completed (loss: 2.212592840194702, acc: 0.5)
[2025-02-04 00:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:38][root][INFO] - Training Epoch: 2/2, step 21300/23838 completed (loss: 1.961564064025879, acc: 0.5)
[2025-02-04 00:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:38][root][INFO] - Training Epoch: 2/2, step 21301/23838 completed (loss: 2.335012435913086, acc: 0.4615384638309479)
[2025-02-04 00:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:39][root][INFO] - Training Epoch: 2/2, step 21302/23838 completed (loss: 2.123192548751831, acc: 0.6470588445663452)
[2025-02-04 00:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:39][root][INFO] - Training Epoch: 2/2, step 21303/23838 completed (loss: 2.806626796722412, acc: 0.4642857015132904)
[2025-02-04 00:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:40][root][INFO] - Training Epoch: 2/2, step 21304/23838 completed (loss: 3.4614086151123047, acc: 0.2800000011920929)
[2025-02-04 00:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:40][root][INFO] - Training Epoch: 2/2, step 21305/23838 completed (loss: 1.4276536703109741, acc: 0.7200000286102295)
[2025-02-04 00:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:41][root][INFO] - Training Epoch: 2/2, step 21306/23838 completed (loss: 2.415738105773926, acc: 0.5333333611488342)
[2025-02-04 00:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:41][root][INFO] - Training Epoch: 2/2, step 21307/23838 completed (loss: 3.8682944774627686, acc: 0.27586206793785095)
[2025-02-04 00:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:42][root][INFO] - Training Epoch: 2/2, step 21308/23838 completed (loss: 2.5676639080047607, acc: 0.3684210479259491)
[2025-02-04 00:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:42][root][INFO] - Training Epoch: 2/2, step 21309/23838 completed (loss: 2.263252019882202, acc: 0.5)
[2025-02-04 00:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:43][root][INFO] - Training Epoch: 2/2, step 21310/23838 completed (loss: 3.275721788406372, acc: 0.4848484992980957)
[2025-02-04 00:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:43][root][INFO] - Training Epoch: 2/2, step 21311/23838 completed (loss: 1.043959140777588, acc: 0.7333333492279053)
[2025-02-04 00:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:43][root][INFO] - Training Epoch: 2/2, step 21312/23838 completed (loss: 2.0304462909698486, acc: 0.4761904776096344)
[2025-02-04 00:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:44][root][INFO] - Training Epoch: 2/2, step 21313/23838 completed (loss: 1.5911998748779297, acc: 0.6000000238418579)
[2025-02-04 00:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:44][root][INFO] - Training Epoch: 2/2, step 21314/23838 completed (loss: 2.4074323177337646, acc: 0.48148149251937866)
[2025-02-04 00:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:44][root][INFO] - Training Epoch: 2/2, step 21315/23838 completed (loss: 2.3812782764434814, acc: 0.5263158082962036)
[2025-02-04 00:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:45][root][INFO] - Training Epoch: 2/2, step 21316/23838 completed (loss: 1.7553737163543701, acc: 0.4736842215061188)
[2025-02-04 00:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:45][root][INFO] - Training Epoch: 2/2, step 21317/23838 completed (loss: 3.0439889430999756, acc: 0.4166666567325592)
[2025-02-04 00:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:46][root][INFO] - Training Epoch: 2/2, step 21318/23838 completed (loss: 2.1697652339935303, acc: 0.47826087474823)
[2025-02-04 00:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:46][root][INFO] - Training Epoch: 2/2, step 21319/23838 completed (loss: 2.5866148471832275, acc: 0.4615384638309479)
[2025-02-04 00:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:46][root][INFO] - Training Epoch: 2/2, step 21320/23838 completed (loss: 2.784367799758911, acc: 0.3461538553237915)
[2025-02-04 00:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:47][root][INFO] - Training Epoch: 2/2, step 21321/23838 completed (loss: 2.687028646469116, acc: 0.4333333373069763)
[2025-02-04 00:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:47][root][INFO] - Training Epoch: 2/2, step 21322/23838 completed (loss: 2.4264440536499023, acc: 0.4166666567325592)
[2025-02-04 00:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:47][root][INFO] - Training Epoch: 2/2, step 21323/23838 completed (loss: 2.9049506187438965, acc: 0.4444444477558136)
[2025-02-04 00:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:48][root][INFO] - Training Epoch: 2/2, step 21324/23838 completed (loss: 2.4985766410827637, acc: 0.4516128897666931)
[2025-02-04 00:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:48][root][INFO] - Training Epoch: 2/2, step 21325/23838 completed (loss: 2.0133261680603027, acc: 0.6666666865348816)
[2025-02-04 00:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:49][root][INFO] - Training Epoch: 2/2, step 21326/23838 completed (loss: 1.0989187955856323, acc: 0.6875)
[2025-02-04 00:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:49][root][INFO] - Training Epoch: 2/2, step 21327/23838 completed (loss: 2.570747137069702, acc: 0.375)
[2025-02-04 00:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:50][root][INFO] - Training Epoch: 2/2, step 21328/23838 completed (loss: 1.421175479888916, acc: 0.6666666865348816)
[2025-02-04 00:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:50][root][INFO] - Training Epoch: 2/2, step 21329/23838 completed (loss: 2.396059036254883, acc: 0.5)
[2025-02-04 00:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:50][root][INFO] - Training Epoch: 2/2, step 21330/23838 completed (loss: 1.7944297790527344, acc: 0.5909090638160706)
[2025-02-04 00:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:51][root][INFO] - Training Epoch: 2/2, step 21331/23838 completed (loss: 1.9479851722717285, acc: 0.6129032373428345)
[2025-02-04 00:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:51][root][INFO] - Training Epoch: 2/2, step 21332/23838 completed (loss: 2.6203746795654297, acc: 0.5)
[2025-02-04 00:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:52][root][INFO] - Training Epoch: 2/2, step 21333/23838 completed (loss: 2.608001470565796, acc: 0.5)
[2025-02-04 00:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:52][root][INFO] - Training Epoch: 2/2, step 21334/23838 completed (loss: 1.8145817518234253, acc: 0.5)
[2025-02-04 00:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:53][root][INFO] - Training Epoch: 2/2, step 21335/23838 completed (loss: 1.3194618225097656, acc: 0.8333333134651184)
[2025-02-04 00:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:53][root][INFO] - Training Epoch: 2/2, step 21336/23838 completed (loss: 2.951127052307129, acc: 0.5)
[2025-02-04 00:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:53][root][INFO] - Training Epoch: 2/2, step 21337/23838 completed (loss: 4.206330299377441, acc: 0.29629629850387573)
[2025-02-04 00:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:54][root][INFO] - Training Epoch: 2/2, step 21338/23838 completed (loss: 4.007249355316162, acc: 0.30000001192092896)
[2025-02-04 00:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:54][root][INFO] - Training Epoch: 2/2, step 21339/23838 completed (loss: 3.144815444946289, acc: 0.47058823704719543)
[2025-02-04 00:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:55][root][INFO] - Training Epoch: 2/2, step 21340/23838 completed (loss: 3.609262228012085, acc: 0.38461539149284363)
[2025-02-04 00:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:55][root][INFO] - Training Epoch: 2/2, step 21341/23838 completed (loss: 3.00712513923645, acc: 0.6428571343421936)
[2025-02-04 00:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:56][root][INFO] - Training Epoch: 2/2, step 21342/23838 completed (loss: 3.791722059249878, acc: 0.37037035822868347)
[2025-02-04 00:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:56][root][INFO] - Training Epoch: 2/2, step 21343/23838 completed (loss: 3.6080162525177, acc: 0.3333333432674408)
[2025-02-04 00:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:57][root][INFO] - Training Epoch: 2/2, step 21344/23838 completed (loss: 4.869998931884766, acc: 0.20454545319080353)
[2025-02-04 00:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:57][root][INFO] - Training Epoch: 2/2, step 21345/23838 completed (loss: 2.9045002460479736, acc: 0.40909090638160706)
[2025-02-04 00:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:57][root][INFO] - Training Epoch: 2/2, step 21346/23838 completed (loss: 4.42021369934082, acc: 0.2777777910232544)
[2025-02-04 00:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:58][root][INFO] - Training Epoch: 2/2, step 21347/23838 completed (loss: 4.324049949645996, acc: 0.25)
[2025-02-04 00:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:58][root][INFO] - Training Epoch: 2/2, step 21348/23838 completed (loss: 4.012623310089111, acc: 0.375)
[2025-02-04 00:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:59][root][INFO] - Training Epoch: 2/2, step 21349/23838 completed (loss: 3.419624090194702, acc: 0.4193548262119293)
[2025-02-04 00:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:58:59][root][INFO] - Training Epoch: 2/2, step 21350/23838 completed (loss: 2.177037000656128, acc: 0.5714285969734192)
[2025-02-04 00:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:00][root][INFO] - Training Epoch: 2/2, step 21351/23838 completed (loss: 3.9607012271881104, acc: 0.4000000059604645)
[2025-02-04 00:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:00][root][INFO] - Training Epoch: 2/2, step 21352/23838 completed (loss: 3.2347207069396973, acc: 0.5384615659713745)
[2025-02-04 00:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:01][root][INFO] - Training Epoch: 2/2, step 21353/23838 completed (loss: 2.2298977375030518, acc: 0.5714285969734192)
[2025-02-04 00:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:01][root][INFO] - Training Epoch: 2/2, step 21354/23838 completed (loss: 4.257479190826416, acc: 0.1818181872367859)
[2025-02-04 00:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:01][root][INFO] - Training Epoch: 2/2, step 21355/23838 completed (loss: 3.0832903385162354, acc: 0.42307692766189575)
[2025-02-04 00:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:02][root][INFO] - Training Epoch: 2/2, step 21356/23838 completed (loss: 3.847195625305176, acc: 0.46666666865348816)
[2025-02-04 00:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:02][root][INFO] - Training Epoch: 2/2, step 21357/23838 completed (loss: 2.7525811195373535, acc: 0.3888888955116272)
[2025-02-04 00:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:03][root][INFO] - Training Epoch: 2/2, step 21358/23838 completed (loss: 0.47609612345695496, acc: 0.9090909361839294)
[2025-02-04 00:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:03][root][INFO] - Training Epoch: 2/2, step 21359/23838 completed (loss: 1.9938541650772095, acc: 0.4736842215061188)
[2025-02-04 00:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:03][root][INFO] - Training Epoch: 2/2, step 21360/23838 completed (loss: 1.9536007642745972, acc: 0.6000000238418579)
[2025-02-04 00:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:04][root][INFO] - Training Epoch: 2/2, step 21361/23838 completed (loss: 1.9693129062652588, acc: 0.5)
[2025-02-04 00:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:04][root][INFO] - Training Epoch: 2/2, step 21362/23838 completed (loss: 2.0531716346740723, acc: 0.4000000059604645)
[2025-02-04 00:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:05][root][INFO] - Training Epoch: 2/2, step 21363/23838 completed (loss: 2.9729127883911133, acc: 0.4545454680919647)
[2025-02-04 00:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:05][root][INFO] - Training Epoch: 2/2, step 21364/23838 completed (loss: 1.272812843322754, acc: 0.7894737124443054)
[2025-02-04 00:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:05][root][INFO] - Training Epoch: 2/2, step 21365/23838 completed (loss: 2.617724895477295, acc: 0.5882353186607361)
[2025-02-04 00:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:06][root][INFO] - Training Epoch: 2/2, step 21366/23838 completed (loss: 2.9633078575134277, acc: 0.47058823704719543)
[2025-02-04 00:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:06][root][INFO] - Training Epoch: 2/2, step 21367/23838 completed (loss: 2.7663674354553223, acc: 0.4545454680919647)
[2025-02-04 00:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:07][root][INFO] - Training Epoch: 2/2, step 21368/23838 completed (loss: 3.56962251663208, acc: 0.27586206793785095)
[2025-02-04 00:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:07][root][INFO] - Training Epoch: 2/2, step 21369/23838 completed (loss: 2.1233861446380615, acc: 0.529411792755127)
[2025-02-04 00:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:07][root][INFO] - Training Epoch: 2/2, step 21370/23838 completed (loss: 3.4942901134490967, acc: 0.4166666567325592)
[2025-02-04 00:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:08][root][INFO] - Training Epoch: 2/2, step 21371/23838 completed (loss: 2.9741411209106445, acc: 0.5333333611488342)
[2025-02-04 00:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:08][root][INFO] - Training Epoch: 2/2, step 21372/23838 completed (loss: 2.6899216175079346, acc: 0.5333333611488342)
[2025-02-04 00:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:09][root][INFO] - Training Epoch: 2/2, step 21373/23838 completed (loss: 2.4153385162353516, acc: 0.5625)
[2025-02-04 00:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:09][root][INFO] - Training Epoch: 2/2, step 21374/23838 completed (loss: 2.5859570503234863, acc: 0.5714285969734192)
[2025-02-04 00:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:10][root][INFO] - Training Epoch: 2/2, step 21375/23838 completed (loss: 2.728114604949951, acc: 0.6000000238418579)
[2025-02-04 00:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:10][root][INFO] - Training Epoch: 2/2, step 21376/23838 completed (loss: 1.8204649686813354, acc: 0.7857142686843872)
[2025-02-04 00:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:11][root][INFO] - Training Epoch: 2/2, step 21377/23838 completed (loss: 2.665006637573242, acc: 0.3333333432674408)
[2025-02-04 00:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:11][root][INFO] - Training Epoch: 2/2, step 21378/23838 completed (loss: 3.7671775817871094, acc: 0.27272728085517883)
[2025-02-04 00:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:11][root][INFO] - Training Epoch: 2/2, step 21379/23838 completed (loss: 2.01755952835083, acc: 0.6000000238418579)
[2025-02-04 00:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:12][root][INFO] - Training Epoch: 2/2, step 21380/23838 completed (loss: 1.8061105012893677, acc: 0.529411792755127)
[2025-02-04 00:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:12][root][INFO] - Training Epoch: 2/2, step 21381/23838 completed (loss: 1.8367351293563843, acc: 0.6086956262588501)
[2025-02-04 00:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:13][root][INFO] - Training Epoch: 2/2, step 21382/23838 completed (loss: 1.535000205039978, acc: 0.7647058963775635)
[2025-02-04 00:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:13][root][INFO] - Training Epoch: 2/2, step 21383/23838 completed (loss: 3.508391857147217, acc: 0.5555555820465088)
[2025-02-04 00:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:14][root][INFO] - Training Epoch: 2/2, step 21384/23838 completed (loss: 1.0201730728149414, acc: 0.692307710647583)
[2025-02-04 00:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:14][root][INFO] - Training Epoch: 2/2, step 21385/23838 completed (loss: 1.4817131757736206, acc: 0.7857142686843872)
[2025-02-04 00:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:14][root][INFO] - Training Epoch: 2/2, step 21386/23838 completed (loss: 0.9286034107208252, acc: 0.5833333134651184)
[2025-02-04 00:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:15][root][INFO] - Training Epoch: 2/2, step 21387/23838 completed (loss: 2.879626989364624, acc: 0.42105263471603394)
[2025-02-04 00:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:15][root][INFO] - Training Epoch: 2/2, step 21388/23838 completed (loss: 0.9886290431022644, acc: 0.7777777910232544)
[2025-02-04 00:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:16][root][INFO] - Training Epoch: 2/2, step 21389/23838 completed (loss: 1.7172818183898926, acc: 0.6111111044883728)
[2025-02-04 00:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:16][root][INFO] - Training Epoch: 2/2, step 21390/23838 completed (loss: 1.214430570602417, acc: 0.6315789222717285)
[2025-02-04 00:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:17][root][INFO] - Training Epoch: 2/2, step 21391/23838 completed (loss: 1.9071571826934814, acc: 0.625)
[2025-02-04 00:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:17][root][INFO] - Training Epoch: 2/2, step 21392/23838 completed (loss: 1.8813056945800781, acc: 0.550000011920929)
[2025-02-04 00:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:18][root][INFO] - Training Epoch: 2/2, step 21393/23838 completed (loss: 3.2616024017333984, acc: 0.375)
[2025-02-04 00:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:18][root][INFO] - Training Epoch: 2/2, step 21394/23838 completed (loss: 0.14706717431545258, acc: 1.0)
[2025-02-04 00:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:18][root][INFO] - Training Epoch: 2/2, step 21395/23838 completed (loss: 2.5185253620147705, acc: 0.52173912525177)
[2025-02-04 00:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:19][root][INFO] - Training Epoch: 2/2, step 21396/23838 completed (loss: 2.9499351978302, acc: 0.47058823704719543)
[2025-02-04 00:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:19][root][INFO] - Training Epoch: 2/2, step 21397/23838 completed (loss: 3.7007176876068115, acc: 0.3333333432674408)
[2025-02-04 00:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:20][root][INFO] - Training Epoch: 2/2, step 21398/23838 completed (loss: 2.6247260570526123, acc: 0.4285714328289032)
[2025-02-04 00:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:20][root][INFO] - Training Epoch: 2/2, step 21399/23838 completed (loss: 3.258314609527588, acc: 0.4117647111415863)
[2025-02-04 00:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:21][root][INFO] - Training Epoch: 2/2, step 21400/23838 completed (loss: 3.4991981983184814, acc: 0.3636363744735718)
[2025-02-04 00:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:21][root][INFO] - Training Epoch: 2/2, step 21401/23838 completed (loss: 2.5479838848114014, acc: 0.375)
[2025-02-04 00:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:21][root][INFO] - Training Epoch: 2/2, step 21402/23838 completed (loss: 1.2411011457443237, acc: 0.75)
[2025-02-04 00:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:22][root][INFO] - Training Epoch: 2/2, step 21403/23838 completed (loss: 3.082183837890625, acc: 0.5384615659713745)
[2025-02-04 00:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:22][root][INFO] - Training Epoch: 2/2, step 21404/23838 completed (loss: 4.0581955909729, acc: 0.3333333432674408)
[2025-02-04 00:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:23][root][INFO] - Training Epoch: 2/2, step 21405/23838 completed (loss: 4.478835105895996, acc: 0.2222222238779068)
[2025-02-04 00:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:23][root][INFO] - Training Epoch: 2/2, step 21406/23838 completed (loss: 1.5226552486419678, acc: 0.6153846383094788)
[2025-02-04 00:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:23][root][INFO] - Training Epoch: 2/2, step 21407/23838 completed (loss: 3.773566246032715, acc: 0.41025641560554504)
[2025-02-04 00:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:24][root][INFO] - Training Epoch: 2/2, step 21408/23838 completed (loss: 2.955169916152954, acc: 0.38461539149284363)
[2025-02-04 00:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:24][root][INFO] - Training Epoch: 2/2, step 21409/23838 completed (loss: 3.501342296600342, acc: 0.5)
[2025-02-04 00:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:25][root][INFO] - Training Epoch: 2/2, step 21410/23838 completed (loss: 3.2776541709899902, acc: 0.36000001430511475)
[2025-02-04 00:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:25][root][INFO] - Training Epoch: 2/2, step 21411/23838 completed (loss: 3.191863775253296, acc: 0.3333333432674408)
[2025-02-04 00:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:25][root][INFO] - Training Epoch: 2/2, step 21412/23838 completed (loss: 1.6329011917114258, acc: 0.7142857313156128)
[2025-02-04 00:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:26][root][INFO] - Training Epoch: 2/2, step 21413/23838 completed (loss: 4.469119548797607, acc: 0.21052631735801697)
[2025-02-04 00:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:26][root][INFO] - Training Epoch: 2/2, step 21414/23838 completed (loss: 2.3364205360412598, acc: 0.4444444477558136)
[2025-02-04 00:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:27][root][INFO] - Training Epoch: 2/2, step 21415/23838 completed (loss: 2.647488832473755, acc: 0.6499999761581421)
[2025-02-04 00:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:27][root][INFO] - Training Epoch: 2/2, step 21416/23838 completed (loss: 3.34978985786438, acc: 0.37037035822868347)
[2025-02-04 00:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:27][root][INFO] - Training Epoch: 2/2, step 21417/23838 completed (loss: 3.8733625411987305, acc: 0.3333333432674408)
[2025-02-04 00:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:28][root][INFO] - Training Epoch: 2/2, step 21418/23838 completed (loss: 3.746642827987671, acc: 0.302325576543808)
[2025-02-04 00:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:28][root][INFO] - Training Epoch: 2/2, step 21419/23838 completed (loss: 2.5407896041870117, acc: 0.6399999856948853)
[2025-02-04 00:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:29][root][INFO] - Training Epoch: 2/2, step 21420/23838 completed (loss: 1.8619052171707153, acc: 0.5)
[2025-02-04 00:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:29][root][INFO] - Training Epoch: 2/2, step 21421/23838 completed (loss: 1.9704749584197998, acc: 0.529411792755127)
[2025-02-04 00:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:29][root][INFO] - Training Epoch: 2/2, step 21422/23838 completed (loss: 3.165426015853882, acc: 0.5)
[2025-02-04 00:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:30][root][INFO] - Training Epoch: 2/2, step 21423/23838 completed (loss: 2.9378931522369385, acc: 0.550000011920929)
[2025-02-04 00:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:30][root][INFO] - Training Epoch: 2/2, step 21424/23838 completed (loss: 3.7399308681488037, acc: 0.5)
[2025-02-04 00:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:31][root][INFO] - Training Epoch: 2/2, step 21425/23838 completed (loss: 6.062963962554932, acc: 0.1428571492433548)
[2025-02-04 00:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:31][root][INFO] - Training Epoch: 2/2, step 21426/23838 completed (loss: 4.975731372833252, acc: 0.25)
[2025-02-04 00:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:32][root][INFO] - Training Epoch: 2/2, step 21427/23838 completed (loss: 6.136112213134766, acc: 0.190476194024086)
[2025-02-04 00:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:32][root][INFO] - Training Epoch: 2/2, step 21428/23838 completed (loss: 4.378478050231934, acc: 0.4000000059604645)
[2025-02-04 00:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:33][root][INFO] - Training Epoch: 2/2, step 21429/23838 completed (loss: 4.89076566696167, acc: 0.2800000011920929)
[2025-02-04 00:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:33][root][INFO] - Training Epoch: 2/2, step 21430/23838 completed (loss: 3.326744794845581, acc: 0.5)
[2025-02-04 00:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:34][root][INFO] - Training Epoch: 2/2, step 21431/23838 completed (loss: 3.6356453895568848, acc: 0.5)
[2025-02-04 00:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:34][root][INFO] - Training Epoch: 2/2, step 21432/23838 completed (loss: 3.598004102706909, acc: 0.4000000059604645)
[2025-02-04 00:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:34][root][INFO] - Training Epoch: 2/2, step 21433/23838 completed (loss: 5.371755123138428, acc: 0.260869562625885)
[2025-02-04 00:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:35][root][INFO] - Training Epoch: 2/2, step 21434/23838 completed (loss: 5.96741247177124, acc: 0.29411765933036804)
[2025-02-04 00:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:35][root][INFO] - Training Epoch: 2/2, step 21435/23838 completed (loss: 5.189183235168457, acc: 0.3499999940395355)
[2025-02-04 00:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:36][root][INFO] - Training Epoch: 2/2, step 21436/23838 completed (loss: 5.084700584411621, acc: 0.2631579041481018)
[2025-02-04 00:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:36][root][INFO] - Training Epoch: 2/2, step 21437/23838 completed (loss: 3.893458843231201, acc: 0.5)
[2025-02-04 00:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:37][root][INFO] - Training Epoch: 2/2, step 21438/23838 completed (loss: 1.974787950515747, acc: 0.6000000238418579)
[2025-02-04 00:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:37][root][INFO] - Training Epoch: 2/2, step 21439/23838 completed (loss: 4.080343246459961, acc: 0.5555555820465088)
[2025-02-04 00:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:38][root][INFO] - Training Epoch: 2/2, step 21440/23838 completed (loss: 4.081075668334961, acc: 0.4000000059604645)
[2025-02-04 00:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:38][root][INFO] - Training Epoch: 2/2, step 21441/23838 completed (loss: 3.946014165878296, acc: 0.2916666567325592)
[2025-02-04 00:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:39][root][INFO] - Training Epoch: 2/2, step 21442/23838 completed (loss: 4.409326076507568, acc: 0.3571428656578064)
[2025-02-04 00:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:39][root][INFO] - Training Epoch: 2/2, step 21443/23838 completed (loss: 3.799207925796509, acc: 0.3888888955116272)
[2025-02-04 00:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:39][root][INFO] - Training Epoch: 2/2, step 21444/23838 completed (loss: 2.98956561088562, acc: 0.4736842215061188)
[2025-02-04 00:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:40][root][INFO] - Training Epoch: 2/2, step 21445/23838 completed (loss: 3.426011085510254, acc: 0.5333333611488342)
[2025-02-04 00:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:40][root][INFO] - Training Epoch: 2/2, step 21446/23838 completed (loss: 2.8874008655548096, acc: 0.5909090638160706)
[2025-02-04 00:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:41][root][INFO] - Training Epoch: 2/2, step 21447/23838 completed (loss: 3.687114953994751, acc: 0.46666666865348816)
[2025-02-04 00:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:41][root][INFO] - Training Epoch: 2/2, step 21448/23838 completed (loss: 4.074938774108887, acc: 0.260869562625885)
[2025-02-04 00:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:42][root][INFO] - Training Epoch: 2/2, step 21449/23838 completed (loss: 2.55307936668396, acc: 0.5199999809265137)
[2025-02-04 00:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:42][root][INFO] - Training Epoch: 2/2, step 21450/23838 completed (loss: 3.4247162342071533, acc: 0.517241358757019)
[2025-02-04 00:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:42][root][INFO] - Training Epoch: 2/2, step 21451/23838 completed (loss: 3.4807848930358887, acc: 0.5)
[2025-02-04 00:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:43][root][INFO] - Training Epoch: 2/2, step 21452/23838 completed (loss: 3.3655686378479004, acc: 0.47826087474823)
[2025-02-04 00:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:43][root][INFO] - Training Epoch: 2/2, step 21453/23838 completed (loss: 3.124763250350952, acc: 0.5)
[2025-02-04 00:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:44][root][INFO] - Training Epoch: 2/2, step 21454/23838 completed (loss: 3.454387903213501, acc: 0.47999998927116394)
[2025-02-04 00:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:44][root][INFO] - Training Epoch: 2/2, step 21455/23838 completed (loss: 2.4137680530548096, acc: 0.6000000238418579)
[2025-02-04 00:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:45][root][INFO] - Training Epoch: 2/2, step 21456/23838 completed (loss: 3.1510112285614014, acc: 0.6666666865348816)
[2025-02-04 00:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:45][root][INFO] - Training Epoch: 2/2, step 21457/23838 completed (loss: 3.449888229370117, acc: 0.4444444477558136)
[2025-02-04 00:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:46][root][INFO] - Training Epoch: 2/2, step 21458/23838 completed (loss: 3.4997072219848633, acc: 0.4166666567325592)
[2025-02-04 00:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:46][root][INFO] - Training Epoch: 2/2, step 21459/23838 completed (loss: 3.452548027038574, acc: 0.39534884691238403)
[2025-02-04 00:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:46][root][INFO] - Training Epoch: 2/2, step 21460/23838 completed (loss: 3.761518955230713, acc: 0.3414634168148041)
[2025-02-04 00:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:47][root][INFO] - Training Epoch: 2/2, step 21461/23838 completed (loss: 3.0601017475128174, acc: 0.3958333432674408)
[2025-02-04 00:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:47][root][INFO] - Training Epoch: 2/2, step 21462/23838 completed (loss: 3.8825275897979736, acc: 0.28070175647735596)
[2025-02-04 00:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:48][root][INFO] - Training Epoch: 2/2, step 21463/23838 completed (loss: 2.8998961448669434, acc: 0.4333333373069763)
[2025-02-04 00:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:49][root][INFO] - Training Epoch: 2/2, step 21464/23838 completed (loss: 3.5226330757141113, acc: 0.3076923191547394)
[2025-02-04 00:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:49][root][INFO] - Training Epoch: 2/2, step 21465/23838 completed (loss: 3.753399133682251, acc: 0.3333333432674408)
[2025-02-04 00:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:49][root][INFO] - Training Epoch: 2/2, step 21466/23838 completed (loss: 3.2024052143096924, acc: 0.34375)
[2025-02-04 00:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:50][root][INFO] - Training Epoch: 2/2, step 21467/23838 completed (loss: 2.687385082244873, acc: 0.37037035822868347)
[2025-02-04 00:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:50][root][INFO] - Training Epoch: 2/2, step 21468/23838 completed (loss: 3.2878522872924805, acc: 0.3611111044883728)
[2025-02-04 00:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:50][root][INFO] - Training Epoch: 2/2, step 21469/23838 completed (loss: 3.539093017578125, acc: 0.30434781312942505)
[2025-02-04 00:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:51][root][INFO] - Training Epoch: 2/2, step 21470/23838 completed (loss: 4.181927680969238, acc: 0.29729729890823364)
[2025-02-04 00:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:51][root][INFO] - Training Epoch: 2/2, step 21471/23838 completed (loss: 2.627760410308838, acc: 0.30000001192092896)
[2025-02-04 00:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:52][root][INFO] - Training Epoch: 2/2, step 21472/23838 completed (loss: 3.191136598587036, acc: 0.34375)
[2025-02-04 00:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:52][root][INFO] - Training Epoch: 2/2, step 21473/23838 completed (loss: 2.301025152206421, acc: 0.5517241358757019)
[2025-02-04 00:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:53][root][INFO] - Training Epoch: 2/2, step 21474/23838 completed (loss: 2.501826286315918, acc: 0.4545454680919647)
[2025-02-04 00:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:53][root][INFO] - Training Epoch: 2/2, step 21475/23838 completed (loss: 2.0660223960876465, acc: 0.4000000059604645)
[2025-02-04 00:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:53][root][INFO] - Training Epoch: 2/2, step 21476/23838 completed (loss: 1.5758764743804932, acc: 0.6153846383094788)
[2025-02-04 00:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:54][root][INFO] - Training Epoch: 2/2, step 21477/23838 completed (loss: 2.50232195854187, acc: 0.3913043439388275)
[2025-02-04 00:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:54][root][INFO] - Training Epoch: 2/2, step 21478/23838 completed (loss: 2.4402267932891846, acc: 0.41999998688697815)
[2025-02-04 00:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:55][root][INFO] - Training Epoch: 2/2, step 21479/23838 completed (loss: 3.2668707370758057, acc: 0.3617021143436432)
[2025-02-04 00:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:55][root][INFO] - Training Epoch: 2/2, step 21480/23838 completed (loss: 4.015803813934326, acc: 0.39024388790130615)
[2025-02-04 00:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:56][root][INFO] - Training Epoch: 2/2, step 21481/23838 completed (loss: 2.769270181655884, acc: 0.42424243688583374)
[2025-02-04 00:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:56][root][INFO] - Training Epoch: 2/2, step 21482/23838 completed (loss: 1.9740957021713257, acc: 0.6969696879386902)
[2025-02-04 00:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:57][root][INFO] - Training Epoch: 2/2, step 21483/23838 completed (loss: 2.409100294113159, acc: 0.5454545617103577)
[2025-02-04 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:57][root][INFO] - Training Epoch: 2/2, step 21484/23838 completed (loss: 1.91250741481781, acc: 0.6333333253860474)
[2025-02-04 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:58][root][INFO] - Training Epoch: 2/2, step 21485/23838 completed (loss: 2.0219814777374268, acc: 0.5)
[2025-02-04 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:58][root][INFO] - Training Epoch: 2/2, step 21486/23838 completed (loss: 1.4797637462615967, acc: 0.5769230723381042)
[2025-02-04 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:58][root][INFO] - Training Epoch: 2/2, step 21487/23838 completed (loss: 3.2896840572357178, acc: 0.37037035822868347)
[2025-02-04 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:59][root][INFO] - Training Epoch: 2/2, step 21488/23838 completed (loss: 2.980795383453369, acc: 0.5)
[2025-02-04 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 00:59:59][root][INFO] - Training Epoch: 2/2, step 21489/23838 completed (loss: 3.939589738845825, acc: 0.2857142984867096)
[2025-02-04 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:00][root][INFO] - Training Epoch: 2/2, step 21490/23838 completed (loss: 3.4157376289367676, acc: 0.44999998807907104)
[2025-02-04 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:00][root][INFO] - Training Epoch: 2/2, step 21491/23838 completed (loss: 2.7357711791992188, acc: 0.6153846383094788)
[2025-02-04 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:00][root][INFO] - Training Epoch: 2/2, step 21492/23838 completed (loss: 1.2788660526275635, acc: 0.699999988079071)
[2025-02-04 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:01][root][INFO] - Training Epoch: 2/2, step 21493/23838 completed (loss: 3.4904468059539795, acc: 0.46666666865348816)
[2025-02-04 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:02][root][INFO] - Training Epoch: 2/2, step 21494/23838 completed (loss: 4.366823196411133, acc: 0.4054054021835327)
[2025-02-04 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:02][root][INFO] - Training Epoch: 2/2, step 21495/23838 completed (loss: 1.730120062828064, acc: 0.5833333134651184)
[2025-02-04 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:02][root][INFO] - Training Epoch: 2/2, step 21496/23838 completed (loss: 1.7931855916976929, acc: 0.5384615659713745)
[2025-02-04 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:03][root][INFO] - Training Epoch: 2/2, step 21497/23838 completed (loss: 1.513609766960144, acc: 0.75)
[2025-02-04 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:03][root][INFO] - Training Epoch: 2/2, step 21498/23838 completed (loss: 2.97363018989563, acc: 0.4000000059604645)
[2025-02-04 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:04][root][INFO] - Training Epoch: 2/2, step 21499/23838 completed (loss: 2.4882595539093018, acc: 0.529411792755127)
[2025-02-04 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:04][root][INFO] - Training Epoch: 2/2, step 21500/23838 completed (loss: 1.3112826347351074, acc: 0.699999988079071)
[2025-02-04 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:05][root][INFO] - Training Epoch: 2/2, step 21501/23838 completed (loss: 2.903247356414795, acc: 0.3333333432674408)
[2025-02-04 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:05][root][INFO] - Training Epoch: 2/2, step 21502/23838 completed (loss: 1.7125287055969238, acc: 0.6111111044883728)
[2025-02-04 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:05][root][INFO] - Training Epoch: 2/2, step 21503/23838 completed (loss: 2.7760465145111084, acc: 0.5555555820465088)
[2025-02-04 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:06][root][INFO] - Training Epoch: 2/2, step 21504/23838 completed (loss: 2.38143253326416, acc: 0.47058823704719543)
[2025-02-04 01:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:06][root][INFO] - Training Epoch: 2/2, step 21505/23838 completed (loss: 2.2447402477264404, acc: 0.6000000238418579)
[2025-02-04 01:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:07][root][INFO] - Training Epoch: 2/2, step 21506/23838 completed (loss: 1.597866177558899, acc: 0.5625)
[2025-02-04 01:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:07][root][INFO] - Training Epoch: 2/2, step 21507/23838 completed (loss: 0.9520736336708069, acc: 0.8125)
[2025-02-04 01:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:07][root][INFO] - Training Epoch: 2/2, step 21508/23838 completed (loss: 2.0141172409057617, acc: 0.75)
[2025-02-04 01:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:08][root][INFO] - Training Epoch: 2/2, step 21509/23838 completed (loss: 2.863462448120117, acc: 0.4482758641242981)
[2025-02-04 01:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:08][root][INFO] - Training Epoch: 2/2, step 21510/23838 completed (loss: 2.259105920791626, acc: 0.5)
[2025-02-04 01:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:09][root][INFO] - Training Epoch: 2/2, step 21511/23838 completed (loss: 2.0893633365631104, acc: 0.4285714328289032)
[2025-02-04 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:09][root][INFO] - Training Epoch: 2/2, step 21512/23838 completed (loss: 3.3066883087158203, acc: 0.550000011920929)
[2025-02-04 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:10][root][INFO] - Training Epoch: 2/2, step 21513/23838 completed (loss: 2.2307982444763184, acc: 0.6399999856948853)
[2025-02-04 01:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:10][root][INFO] - Training Epoch: 2/2, step 21514/23838 completed (loss: 0.9100127816200256, acc: 0.800000011920929)
[2025-02-04 01:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:11][root][INFO] - Training Epoch: 2/2, step 21515/23838 completed (loss: 1.82734215259552, acc: 0.5925925970077515)
[2025-02-04 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:11][root][INFO] - Training Epoch: 2/2, step 21516/23838 completed (loss: 3.2373859882354736, acc: 0.2631579041481018)
[2025-02-04 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:12][root][INFO] - Training Epoch: 2/2, step 21517/23838 completed (loss: 3.302186965942383, acc: 0.25)
[2025-02-04 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:12][root][INFO] - Training Epoch: 2/2, step 21518/23838 completed (loss: 3.52573561668396, acc: 0.5)
[2025-02-04 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:12][root][INFO] - Training Epoch: 2/2, step 21519/23838 completed (loss: 1.286195158958435, acc: 0.6666666865348816)
[2025-02-04 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:13][root][INFO] - Training Epoch: 2/2, step 21520/23838 completed (loss: 2.4014382362365723, acc: 0.550000011920929)
[2025-02-04 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:14][root][INFO] - Training Epoch: 2/2, step 21521/23838 completed (loss: 1.9283969402313232, acc: 0.5333333611488342)
[2025-02-04 01:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:14][root][INFO] - Training Epoch: 2/2, step 21522/23838 completed (loss: 2.7220051288604736, acc: 0.6190476417541504)
[2025-02-04 01:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:15][root][INFO] - Training Epoch: 2/2, step 21523/23838 completed (loss: 2.2676875591278076, acc: 0.5199999809265137)
[2025-02-04 01:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:15][root][INFO] - Training Epoch: 2/2, step 21524/23838 completed (loss: 1.0270458459854126, acc: 0.8571428656578064)
[2025-02-04 01:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:16][root][INFO] - Training Epoch: 2/2, step 21525/23838 completed (loss: 1.3629733324050903, acc: 0.699999988079071)
[2025-02-04 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:16][root][INFO] - Training Epoch: 2/2, step 21526/23838 completed (loss: 4.04103946685791, acc: 0.30000001192092896)
[2025-02-04 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:16][root][INFO] - Training Epoch: 2/2, step 21527/23838 completed (loss: 3.6903162002563477, acc: 0.4000000059604645)
[2025-02-04 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:17][root][INFO] - Training Epoch: 2/2, step 21528/23838 completed (loss: 2.6385538578033447, acc: 0.5384615659713745)
[2025-02-04 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:17][root][INFO] - Training Epoch: 2/2, step 21529/23838 completed (loss: 2.25719952583313, acc: 0.4375)
[2025-02-04 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:18][root][INFO] - Training Epoch: 2/2, step 21530/23838 completed (loss: 1.6067748069763184, acc: 0.8181818127632141)
[2025-02-04 01:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:18][root][INFO] - Training Epoch: 2/2, step 21531/23838 completed (loss: 2.9971251487731934, acc: 0.5333333611488342)
[2025-02-04 01:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:19][root][INFO] - Training Epoch: 2/2, step 21532/23838 completed (loss: 2.2840046882629395, acc: 0.47826087474823)
[2025-02-04 01:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:19][root][INFO] - Training Epoch: 2/2, step 21533/23838 completed (loss: 2.2165157794952393, acc: 0.6666666865348816)
[2025-02-04 01:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:19][root][INFO] - Training Epoch: 2/2, step 21534/23838 completed (loss: 2.8047373294830322, acc: 0.6666666865348816)
[2025-02-04 01:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:20][root][INFO] - Training Epoch: 2/2, step 21535/23838 completed (loss: 1.6912208795547485, acc: 0.8181818127632141)
[2025-02-04 01:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:20][root][INFO] - Training Epoch: 2/2, step 21536/23838 completed (loss: 2.2966089248657227, acc: 0.6363636255264282)
[2025-02-04 01:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:21][root][INFO] - Training Epoch: 2/2, step 21537/23838 completed (loss: 4.055505275726318, acc: 0.3571428656578064)
[2025-02-04 01:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:21][root][INFO] - Training Epoch: 2/2, step 21538/23838 completed (loss: 3.874918222427368, acc: 0.4285714328289032)
[2025-02-04 01:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:21][root][INFO] - Training Epoch: 2/2, step 21539/23838 completed (loss: 1.8069639205932617, acc: 0.5714285969734192)
[2025-02-04 01:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:22][root][INFO] - Training Epoch: 2/2, step 21540/23838 completed (loss: 3.4493279457092285, acc: 0.4000000059604645)
[2025-02-04 01:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:22][root][INFO] - Training Epoch: 2/2, step 21541/23838 completed (loss: 3.395250082015991, acc: 0.4444444477558136)
[2025-02-04 01:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:23][root][INFO] - Training Epoch: 2/2, step 21542/23838 completed (loss: 4.85869026184082, acc: 0.3529411852359772)
[2025-02-04 01:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:23][root][INFO] - Training Epoch: 2/2, step 21543/23838 completed (loss: 3.427830457687378, acc: 0.5)
[2025-02-04 01:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:23][root][INFO] - Training Epoch: 2/2, step 21544/23838 completed (loss: 3.2945024967193604, acc: 0.4545454680919647)
[2025-02-04 01:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:24][root][INFO] - Training Epoch: 2/2, step 21545/23838 completed (loss: 2.979982852935791, acc: 0.3636363744735718)
[2025-02-04 01:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:24][root][INFO] - Training Epoch: 2/2, step 21546/23838 completed (loss: 1.6768569946289062, acc: 0.6969696879386902)
[2025-02-04 01:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:25][root][INFO] - Training Epoch: 2/2, step 21547/23838 completed (loss: 2.538126230239868, acc: 0.40740740299224854)
[2025-02-04 01:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:25][root][INFO] - Training Epoch: 2/2, step 21548/23838 completed (loss: 2.5526392459869385, acc: 0.3461538553237915)
[2025-02-04 01:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:26][root][INFO] - Training Epoch: 2/2, step 21549/23838 completed (loss: 3.1014604568481445, acc: 0.3888888955116272)
[2025-02-04 01:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:26][root][INFO] - Training Epoch: 2/2, step 21550/23838 completed (loss: 2.153299570083618, acc: 0.6451612710952759)
[2025-02-04 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:27][root][INFO] - Training Epoch: 2/2, step 21551/23838 completed (loss: 1.3549892902374268, acc: 0.6470588445663452)
[2025-02-04 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:27][root][INFO] - Training Epoch: 2/2, step 21552/23838 completed (loss: 1.46467924118042, acc: 0.6111111044883728)
[2025-02-04 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:28][root][INFO] - Training Epoch: 2/2, step 21553/23838 completed (loss: 2.9397225379943848, acc: 0.47826087474823)
[2025-02-04 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:28][root][INFO] - Training Epoch: 2/2, step 21554/23838 completed (loss: 3.432878017425537, acc: 0.380952388048172)
[2025-02-04 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:29][root][INFO] - Training Epoch: 2/2, step 21555/23838 completed (loss: 3.161517381668091, acc: 0.42105263471603394)
[2025-02-04 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:29][root][INFO] - Training Epoch: 2/2, step 21556/23838 completed (loss: 2.793283224105835, acc: 0.44117647409439087)
[2025-02-04 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:30][root][INFO] - Training Epoch: 2/2, step 21557/23838 completed (loss: 2.0227136611938477, acc: 0.5185185074806213)
[2025-02-04 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:30][root][INFO] - Training Epoch: 2/2, step 21558/23838 completed (loss: 1.8111259937286377, acc: 0.761904776096344)
[2025-02-04 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:31][root][INFO] - Training Epoch: 2/2, step 21559/23838 completed (loss: 2.916248321533203, acc: 0.5625)
[2025-02-04 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:31][root][INFO] - Training Epoch: 2/2, step 21560/23838 completed (loss: 4.519577980041504, acc: 0.4000000059604645)
[2025-02-04 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:32][root][INFO] - Training Epoch: 2/2, step 21561/23838 completed (loss: 3.2887165546417236, acc: 0.5428571701049805)
[2025-02-04 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:32][root][INFO] - Training Epoch: 2/2, step 21562/23838 completed (loss: 2.119675636291504, acc: 0.6190476417541504)
[2025-02-04 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:33][root][INFO] - Training Epoch: 2/2, step 21563/23838 completed (loss: 3.027575969696045, acc: 0.6206896305084229)
[2025-02-04 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:33][root][INFO] - Training Epoch: 2/2, step 21564/23838 completed (loss: 1.2961148023605347, acc: 0.7272727489471436)
[2025-02-04 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:34][root][INFO] - Training Epoch: 2/2, step 21565/23838 completed (loss: 1.9691706895828247, acc: 0.6666666865348816)
[2025-02-04 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:34][root][INFO] - Training Epoch: 2/2, step 21566/23838 completed (loss: 2.2161946296691895, acc: 0.59375)
[2025-02-04 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:34][root][INFO] - Training Epoch: 2/2, step 21567/23838 completed (loss: 2.0494744777679443, acc: 0.6000000238418579)
[2025-02-04 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:35][root][INFO] - Training Epoch: 2/2, step 21568/23838 completed (loss: 2.935581922531128, acc: 0.4878048896789551)
[2025-02-04 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:35][root][INFO] - Training Epoch: 2/2, step 21569/23838 completed (loss: 1.4063968658447266, acc: 0.75)
[2025-02-04 01:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:36][root][INFO] - Training Epoch: 2/2, step 21570/23838 completed (loss: 2.1259560585021973, acc: 0.5)
[2025-02-04 01:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:36][root][INFO] - Training Epoch: 2/2, step 21571/23838 completed (loss: 1.8009063005447388, acc: 0.5909090638160706)
[2025-02-04 01:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:37][root][INFO] - Training Epoch: 2/2, step 21572/23838 completed (loss: 1.6202629804611206, acc: 0.7333333492279053)
[2025-02-04 01:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:38][root][INFO] - Training Epoch: 2/2, step 21573/23838 completed (loss: 2.715285539627075, acc: 0.5142857432365417)
[2025-02-04 01:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:38][root][INFO] - Training Epoch: 2/2, step 21574/23838 completed (loss: 1.5692389011383057, acc: 0.6538461446762085)
[2025-02-04 01:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:39][root][INFO] - Training Epoch: 2/2, step 21575/23838 completed (loss: 1.698692798614502, acc: 0.6363636255264282)
[2025-02-04 01:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:39][root][INFO] - Training Epoch: 2/2, step 21576/23838 completed (loss: 2.222310781478882, acc: 0.5476190447807312)
[2025-02-04 01:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:40][root][INFO] - Training Epoch: 2/2, step 21577/23838 completed (loss: 2.723482370376587, acc: 0.5185185074806213)
[2025-02-04 01:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:41][root][INFO] - Training Epoch: 2/2, step 21578/23838 completed (loss: 1.987969994544983, acc: 0.699999988079071)
[2025-02-04 01:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:41][root][INFO] - Training Epoch: 2/2, step 21579/23838 completed (loss: 3.2912070751190186, acc: 0.46666666865348816)
[2025-02-04 01:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:42][root][INFO] - Training Epoch: 2/2, step 21580/23838 completed (loss: 2.201256513595581, acc: 0.6086956262588501)
[2025-02-04 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:42][root][INFO] - Training Epoch: 2/2, step 21581/23838 completed (loss: 3.5562326908111572, acc: 0.523809552192688)
[2025-02-04 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:43][root][INFO] - Training Epoch: 2/2, step 21582/23838 completed (loss: 3.6259641647338867, acc: 0.25999999046325684)
[2025-02-04 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:43][root][INFO] - Training Epoch: 2/2, step 21583/23838 completed (loss: 3.0572354793548584, acc: 0.4193548262119293)
[2025-02-04 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:43][root][INFO] - Training Epoch: 2/2, step 21584/23838 completed (loss: 3.885891914367676, acc: 0.23404255509376526)
[2025-02-04 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:44][root][INFO] - Training Epoch: 2/2, step 21585/23838 completed (loss: 3.4513559341430664, acc: 0.3478260934352875)
[2025-02-04 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:44][root][INFO] - Training Epoch: 2/2, step 21586/23838 completed (loss: 3.4921963214874268, acc: 0.3076923191547394)
[2025-02-04 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:45][root][INFO] - Training Epoch: 2/2, step 21587/23838 completed (loss: 4.119685649871826, acc: 0.25)
[2025-02-04 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:45][root][INFO] - Training Epoch: 2/2, step 21588/23838 completed (loss: 4.356770992279053, acc: 0.30434781312942505)
[2025-02-04 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:46][root][INFO] - Training Epoch: 2/2, step 21589/23838 completed (loss: 3.997321128845215, acc: 0.2083333283662796)
[2025-02-04 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:46][root][INFO] - Training Epoch: 2/2, step 21590/23838 completed (loss: 2.8691089153289795, acc: 0.4761904776096344)
[2025-02-04 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:46][root][INFO] - Training Epoch: 2/2, step 21591/23838 completed (loss: 3.461216449737549, acc: 0.4444444477558136)
[2025-02-04 01:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:47][root][INFO] - Training Epoch: 2/2, step 21592/23838 completed (loss: 3.406165361404419, acc: 0.29032257199287415)
[2025-02-04 01:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:47][root][INFO] - Training Epoch: 2/2, step 21593/23838 completed (loss: 3.2933266162872314, acc: 0.3636363744735718)
[2025-02-04 01:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:48][root][INFO] - Training Epoch: 2/2, step 21594/23838 completed (loss: 2.0314977169036865, acc: 0.5)
[2025-02-04 01:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:48][root][INFO] - Training Epoch: 2/2, step 21595/23838 completed (loss: 4.442957878112793, acc: 0.3636363744735718)
[2025-02-04 01:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:48][root][INFO] - Training Epoch: 2/2, step 21596/23838 completed (loss: 3.0641634464263916, acc: 0.375)
[2025-02-04 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:49][root][INFO] - Training Epoch: 2/2, step 21597/23838 completed (loss: 4.3870463371276855, acc: 0.2777777910232544)
[2025-02-04 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:49][root][INFO] - Training Epoch: 2/2, step 21598/23838 completed (loss: 2.9332754611968994, acc: 0.48571428656578064)
[2025-02-04 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:50][root][INFO] - Training Epoch: 2/2, step 21599/23838 completed (loss: 3.984626054763794, acc: 0.5263158082962036)
[2025-02-04 01:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:50][root][INFO] - Training Epoch: 2/2, step 21600/23838 completed (loss: 2.3618502616882324, acc: 0.6499999761581421)
[2025-02-04 01:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:51][root][INFO] - Training Epoch: 2/2, step 21601/23838 completed (loss: 2.358889102935791, acc: 0.42105263471603394)
[2025-02-04 01:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:51][root][INFO] - Training Epoch: 2/2, step 21602/23838 completed (loss: 1.6706352233886719, acc: 0.5384615659713745)
[2025-02-04 01:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:51][root][INFO] - Training Epoch: 2/2, step 21603/23838 completed (loss: 3.038665294647217, acc: 0.5357142686843872)
[2025-02-04 01:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:52][root][INFO] - Training Epoch: 2/2, step 21604/23838 completed (loss: 2.6786322593688965, acc: 0.5333333611488342)
[2025-02-04 01:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:52][root][INFO] - Training Epoch: 2/2, step 21605/23838 completed (loss: 4.392882823944092, acc: 0.31578946113586426)
[2025-02-04 01:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:53][root][INFO] - Training Epoch: 2/2, step 21606/23838 completed (loss: 3.151869297027588, acc: 0.4482758641242981)
[2025-02-04 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:53][root][INFO] - Training Epoch: 2/2, step 21607/23838 completed (loss: 2.389133930206299, acc: 0.5600000023841858)
[2025-02-04 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:54][root][INFO] - Training Epoch: 2/2, step 21608/23838 completed (loss: 3.9224960803985596, acc: 0.3214285671710968)
[2025-02-04 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:54][root][INFO] - Training Epoch: 2/2, step 21609/23838 completed (loss: 3.5539023876190186, acc: 0.31578946113586426)
[2025-02-04 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:54][root][INFO] - Training Epoch: 2/2, step 21610/23838 completed (loss: 2.922706365585327, acc: 0.4000000059604645)
[2025-02-04 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:55][root][INFO] - Training Epoch: 2/2, step 21611/23838 completed (loss: 2.6984784603118896, acc: 0.42307692766189575)
[2025-02-04 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:55][root][INFO] - Training Epoch: 2/2, step 21612/23838 completed (loss: 3.2812135219573975, acc: 0.3333333432674408)
[2025-02-04 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:56][root][INFO] - Training Epoch: 2/2, step 21613/23838 completed (loss: 3.393423557281494, acc: 0.34375)
[2025-02-04 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:56][root][INFO] - Training Epoch: 2/2, step 21614/23838 completed (loss: 0.928260862827301, acc: 0.8333333134651184)
[2025-02-04 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:57][root][INFO] - Training Epoch: 2/2, step 21615/23838 completed (loss: 2.3133797645568848, acc: 0.550000011920929)
[2025-02-04 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:57][root][INFO] - Training Epoch: 2/2, step 21616/23838 completed (loss: 2.148299217224121, acc: 0.6086956262588501)
[2025-02-04 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:57][root][INFO] - Training Epoch: 2/2, step 21617/23838 completed (loss: 2.7135891914367676, acc: 0.3611111044883728)
[2025-02-04 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:58][root][INFO] - Training Epoch: 2/2, step 21618/23838 completed (loss: 3.360365629196167, acc: 0.3076923191547394)
[2025-02-04 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:58][root][INFO] - Training Epoch: 2/2, step 21619/23838 completed (loss: 3.2853004932403564, acc: 0.23529411852359772)
[2025-02-04 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:59][root][INFO] - Training Epoch: 2/2, step 21620/23838 completed (loss: 3.8372280597686768, acc: 0.29629629850387573)
[2025-02-04 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:59][root][INFO] - Training Epoch: 2/2, step 21621/23838 completed (loss: 2.508437395095825, acc: 0.4285714328289032)
[2025-02-04 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:00:59][root][INFO] - Training Epoch: 2/2, step 21622/23838 completed (loss: 3.4952290058135986, acc: 0.3333333432674408)
[2025-02-04 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:00][root][INFO] - Training Epoch: 2/2, step 21623/23838 completed (loss: 3.867008924484253, acc: 0.2666666805744171)
[2025-02-04 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:00][root][INFO] - Training Epoch: 2/2, step 21624/23838 completed (loss: 2.6369547843933105, acc: 0.5357142686843872)
[2025-02-04 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:01][root][INFO] - Training Epoch: 2/2, step 21625/23838 completed (loss: 3.462651252746582, acc: 0.34375)
[2025-02-04 01:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:01][root][INFO] - Training Epoch: 2/2, step 21626/23838 completed (loss: 2.051340341567993, acc: 0.6153846383094788)
[2025-02-04 01:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:02][root][INFO] - Training Epoch: 2/2, step 21627/23838 completed (loss: 1.345272421836853, acc: 0.6428571343421936)
[2025-02-04 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:02][root][INFO] - Training Epoch: 2/2, step 21628/23838 completed (loss: 2.034827470779419, acc: 0.4736842215061188)
[2025-02-04 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:02][root][INFO] - Training Epoch: 2/2, step 21629/23838 completed (loss: 3.2632670402526855, acc: 0.375)
[2025-02-04 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:03][root][INFO] - Training Epoch: 2/2, step 21630/23838 completed (loss: 3.0099880695343018, acc: 0.3333333432674408)
[2025-02-04 01:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:03][root][INFO] - Training Epoch: 2/2, step 21631/23838 completed (loss: 3.7521159648895264, acc: 0.30000001192092896)
[2025-02-04 01:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:04][root][INFO] - Training Epoch: 2/2, step 21632/23838 completed (loss: 2.858306884765625, acc: 0.2702702581882477)
[2025-02-04 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:04][root][INFO] - Training Epoch: 2/2, step 21633/23838 completed (loss: 2.6110503673553467, acc: 0.4333333373069763)
[2025-02-04 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:04][root][INFO] - Training Epoch: 2/2, step 21634/23838 completed (loss: 1.867367148399353, acc: 0.5652173757553101)
[2025-02-04 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:05][root][INFO] - Training Epoch: 2/2, step 21635/23838 completed (loss: 2.1778669357299805, acc: 0.4146341383457184)
[2025-02-04 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:05][root][INFO] - Training Epoch: 2/2, step 21636/23838 completed (loss: 3.3930962085723877, acc: 0.30000001192092896)
[2025-02-04 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:06][root][INFO] - Training Epoch: 2/2, step 21637/23838 completed (loss: 2.770015239715576, acc: 0.2380952388048172)
[2025-02-04 01:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:06][root][INFO] - Training Epoch: 2/2, step 21638/23838 completed (loss: 2.1796321868896484, acc: 0.6153846383094788)
[2025-02-04 01:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:07][root][INFO] - Training Epoch: 2/2, step 21639/23838 completed (loss: 1.8444879055023193, acc: 0.6129032373428345)
[2025-02-04 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:07][root][INFO] - Training Epoch: 2/2, step 21640/23838 completed (loss: 2.3761355876922607, acc: 0.5789473652839661)
[2025-02-04 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:08][root][INFO] - Training Epoch: 2/2, step 21641/23838 completed (loss: 3.169921875, acc: 0.4571428596973419)
[2025-02-04 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:08][root][INFO] - Training Epoch: 2/2, step 21642/23838 completed (loss: 2.403135299682617, acc: 0.5161290168762207)
[2025-02-04 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:08][root][INFO] - Training Epoch: 2/2, step 21643/23838 completed (loss: 3.2956650257110596, acc: 0.3181818127632141)
[2025-02-04 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:09][root][INFO] - Training Epoch: 2/2, step 21644/23838 completed (loss: 3.1640632152557373, acc: 0.3333333432674408)
[2025-02-04 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:09][root][INFO] - Training Epoch: 2/2, step 21645/23838 completed (loss: 2.3729264736175537, acc: 0.5882353186607361)
[2025-02-04 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:10][root][INFO] - Training Epoch: 2/2, step 21646/23838 completed (loss: 1.9557218551635742, acc: 0.6000000238418579)
[2025-02-04 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:10][root][INFO] - Training Epoch: 2/2, step 21647/23838 completed (loss: 3.3068652153015137, acc: 0.3333333432674408)
[2025-02-04 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:11][root][INFO] - Training Epoch: 2/2, step 21648/23838 completed (loss: 2.4659154415130615, acc: 0.46875)
[2025-02-04 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:11][root][INFO] - Training Epoch: 2/2, step 21649/23838 completed (loss: 2.3007524013519287, acc: 0.4000000059604645)
[2025-02-04 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:11][root][INFO] - Training Epoch: 2/2, step 21650/23838 completed (loss: 3.010993242263794, acc: 0.35483869910240173)
[2025-02-04 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:12][root][INFO] - Training Epoch: 2/2, step 21651/23838 completed (loss: 4.024110317230225, acc: 0.29411765933036804)
[2025-02-04 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:12][root][INFO] - Training Epoch: 2/2, step 21652/23838 completed (loss: 2.007594585418701, acc: 0.5)
[2025-02-04 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:13][root][INFO] - Training Epoch: 2/2, step 21653/23838 completed (loss: 3.9118549823760986, acc: 0.24242424964904785)
[2025-02-04 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:13][root][INFO] - Training Epoch: 2/2, step 21654/23838 completed (loss: 3.3571457862854004, acc: 0.3448275923728943)
[2025-02-04 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:14][root][INFO] - Training Epoch: 2/2, step 21655/23838 completed (loss: 2.744234800338745, acc: 0.3571428656578064)
[2025-02-04 01:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:14][root][INFO] - Training Epoch: 2/2, step 21656/23838 completed (loss: 3.198801040649414, acc: 0.3877550959587097)
[2025-02-04 01:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:15][root][INFO] - Training Epoch: 2/2, step 21657/23838 completed (loss: 3.6638407707214355, acc: 0.3333333432674408)
[2025-02-04 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:15][root][INFO] - Training Epoch: 2/2, step 21658/23838 completed (loss: 2.437397003173828, acc: 0.4864864945411682)
[2025-02-04 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:16][root][INFO] - Training Epoch: 2/2, step 21659/23838 completed (loss: 2.4528098106384277, acc: 0.47058823704719543)
[2025-02-04 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:16][root][INFO] - Training Epoch: 2/2, step 21660/23838 completed (loss: 2.572924852371216, acc: 0.625)
[2025-02-04 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:17][root][INFO] - Training Epoch: 2/2, step 21661/23838 completed (loss: 2.299114227294922, acc: 0.6086956262588501)
[2025-02-04 01:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:17][root][INFO] - Training Epoch: 2/2, step 21662/23838 completed (loss: 4.959813117980957, acc: 0.4000000059604645)
[2025-02-04 01:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:17][root][INFO] - Training Epoch: 2/2, step 21663/23838 completed (loss: 2.662137985229492, acc: 0.6000000238418579)
[2025-02-04 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:18][root][INFO] - Training Epoch: 2/2, step 21664/23838 completed (loss: 2.351935863494873, acc: 0.4285714328289032)
[2025-02-04 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:18][root][INFO] - Training Epoch: 2/2, step 21665/23838 completed (loss: 2.702122211456299, acc: 0.5)
[2025-02-04 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:19][root][INFO] - Training Epoch: 2/2, step 21666/23838 completed (loss: 1.7071549892425537, acc: 0.6000000238418579)
[2025-02-04 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:19][root][INFO] - Training Epoch: 2/2, step 21667/23838 completed (loss: 3.843430280685425, acc: 0.3658536672592163)
[2025-02-04 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:20][root][INFO] - Training Epoch: 2/2, step 21668/23838 completed (loss: 3.012613296508789, acc: 0.375)
[2025-02-04 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:20][root][INFO] - Training Epoch: 2/2, step 21669/23838 completed (loss: 1.1410664319992065, acc: 0.8333333134651184)
[2025-02-04 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:21][root][INFO] - Training Epoch: 2/2, step 21670/23838 completed (loss: 3.5305824279785156, acc: 0.3333333432674408)
[2025-02-04 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:21][root][INFO] - Training Epoch: 2/2, step 21671/23838 completed (loss: 4.569209098815918, acc: 0.3333333432674408)
[2025-02-04 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:21][root][INFO] - Training Epoch: 2/2, step 21672/23838 completed (loss: 2.8730430603027344, acc: 0.5333333611488342)
[2025-02-04 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:22][root][INFO] - Training Epoch: 2/2, step 21673/23838 completed (loss: 2.3745620250701904, acc: 0.5833333134651184)
[2025-02-04 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:22][root][INFO] - Training Epoch: 2/2, step 21674/23838 completed (loss: 3.9604666233062744, acc: 0.5)
[2025-02-04 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:23][root][INFO] - Training Epoch: 2/2, step 21675/23838 completed (loss: 1.8907935619354248, acc: 0.529411792755127)
[2025-02-04 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:23][root][INFO] - Training Epoch: 2/2, step 21676/23838 completed (loss: 2.9269468784332275, acc: 0.3529411852359772)
[2025-02-04 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:24][root][INFO] - Training Epoch: 2/2, step 21677/23838 completed (loss: 2.32879376411438, acc: 0.44999998807907104)
[2025-02-04 01:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:24][root][INFO] - Training Epoch: 2/2, step 21678/23838 completed (loss: 1.6760187149047852, acc: 0.6296296119689941)
[2025-02-04 01:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:25][root][INFO] - Training Epoch: 2/2, step 21679/23838 completed (loss: 2.7702856063842773, acc: 0.5)
[2025-02-04 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:25][root][INFO] - Training Epoch: 2/2, step 21680/23838 completed (loss: 4.224421501159668, acc: 0.2631579041481018)
[2025-02-04 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:25][root][INFO] - Training Epoch: 2/2, step 21681/23838 completed (loss: 3.196133613586426, acc: 0.5833333134651184)
[2025-02-04 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:26][root][INFO] - Training Epoch: 2/2, step 21682/23838 completed (loss: 1.4160971641540527, acc: 0.6111111044883728)
[2025-02-04 01:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:26][root][INFO] - Training Epoch: 2/2, step 21683/23838 completed (loss: 2.82965350151062, acc: 0.6000000238418579)
[2025-02-04 01:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:27][root][INFO] - Training Epoch: 2/2, step 21684/23838 completed (loss: 5.760161876678467, acc: 0.3636363744735718)
[2025-02-04 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:27][root][INFO] - Training Epoch: 2/2, step 21685/23838 completed (loss: 4.324092388153076, acc: 0.3461538553237915)
[2025-02-04 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:27][root][INFO] - Training Epoch: 2/2, step 21686/23838 completed (loss: 3.204925060272217, acc: 0.4761904776096344)
[2025-02-04 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:28][root][INFO] - Training Epoch: 2/2, step 21687/23838 completed (loss: 5.539748191833496, acc: 0.2666666805744171)
[2025-02-04 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:28][root][INFO] - Training Epoch: 2/2, step 21688/23838 completed (loss: 3.1255531311035156, acc: 0.4000000059604645)
[2025-02-04 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:29][root][INFO] - Training Epoch: 2/2, step 21689/23838 completed (loss: 3.0471291542053223, acc: 0.4444444477558136)
[2025-02-04 01:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:29][root][INFO] - Training Epoch: 2/2, step 21690/23838 completed (loss: 2.547941207885742, acc: 0.3913043439388275)
[2025-02-04 01:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:29][root][INFO] - Training Epoch: 2/2, step 21691/23838 completed (loss: 3.0521693229675293, acc: 0.42307692766189575)
[2025-02-04 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:30][root][INFO] - Training Epoch: 2/2, step 21692/23838 completed (loss: 2.3338375091552734, acc: 0.4736842215061188)
[2025-02-04 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:30][root][INFO] - Training Epoch: 2/2, step 21693/23838 completed (loss: 1.175041913986206, acc: 0.5882353186607361)
[2025-02-04 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:31][root][INFO] - Training Epoch: 2/2, step 21694/23838 completed (loss: 1.4462859630584717, acc: 0.699999988079071)
[2025-02-04 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:31][root][INFO] - Training Epoch: 2/2, step 21695/23838 completed (loss: 2.983797311782837, acc: 0.4583333432674408)
[2025-02-04 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:32][root][INFO] - Training Epoch: 2/2, step 21696/23838 completed (loss: 2.030090808868408, acc: 0.6666666865348816)
[2025-02-04 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:32][root][INFO] - Training Epoch: 2/2, step 21697/23838 completed (loss: 3.2061851024627686, acc: 0.529411792755127)
[2025-02-04 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:33][root][INFO] - Training Epoch: 2/2, step 21698/23838 completed (loss: 0.6654305458068848, acc: 0.75)
[2025-02-04 01:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:33][root][INFO] - Training Epoch: 2/2, step 21699/23838 completed (loss: 2.9447386264801025, acc: 0.37037035822868347)
[2025-02-04 01:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:33][root][INFO] - Training Epoch: 2/2, step 21700/23838 completed (loss: 3.0275778770446777, acc: 0.5)
[2025-02-04 01:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:34][root][INFO] - Training Epoch: 2/2, step 21701/23838 completed (loss: 2.589473247528076, acc: 0.5199999809265137)
[2025-02-04 01:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:34][root][INFO] - Training Epoch: 2/2, step 21702/23838 completed (loss: 3.2865660190582275, acc: 0.4736842215061188)
[2025-02-04 01:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:35][root][INFO] - Training Epoch: 2/2, step 21703/23838 completed (loss: 2.052091360092163, acc: 0.46666666865348816)
[2025-02-04 01:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:35][root][INFO] - Training Epoch: 2/2, step 21704/23838 completed (loss: 2.3474133014678955, acc: 0.6111111044883728)
[2025-02-04 01:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:36][root][INFO] - Training Epoch: 2/2, step 21705/23838 completed (loss: 4.094130516052246, acc: 0.43478259444236755)
[2025-02-04 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:36][root][INFO] - Training Epoch: 2/2, step 21706/23838 completed (loss: 2.9037024974823, acc: 0.5)
[2025-02-04 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:36][root][INFO] - Training Epoch: 2/2, step 21707/23838 completed (loss: 2.6622583866119385, acc: 0.46666666865348816)
[2025-02-04 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:37][root][INFO] - Training Epoch: 2/2, step 21708/23838 completed (loss: 2.308788299560547, acc: 0.6428571343421936)
[2025-02-04 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:37][root][INFO] - Training Epoch: 2/2, step 21709/23838 completed (loss: 2.360285758972168, acc: 0.46666666865348816)
[2025-02-04 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:38][root][INFO] - Training Epoch: 2/2, step 21710/23838 completed (loss: 2.5761821269989014, acc: 0.5625)
[2025-02-04 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:38][root][INFO] - Training Epoch: 2/2, step 21711/23838 completed (loss: 2.478632926940918, acc: 0.5333333611488342)
[2025-02-04 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:38][root][INFO] - Training Epoch: 2/2, step 21712/23838 completed (loss: 2.8908684253692627, acc: 0.3888888955116272)
[2025-02-04 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:39][root][INFO] - Training Epoch: 2/2, step 21713/23838 completed (loss: 2.9872591495513916, acc: 0.3913043439388275)
[2025-02-04 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:39][root][INFO] - Training Epoch: 2/2, step 21714/23838 completed (loss: 2.6707022190093994, acc: 0.40909090638160706)
[2025-02-04 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:40][root][INFO] - Training Epoch: 2/2, step 21715/23838 completed (loss: 2.4343338012695312, acc: 0.5555555820465088)
[2025-02-04 01:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:40][root][INFO] - Training Epoch: 2/2, step 21716/23838 completed (loss: 2.1315200328826904, acc: 0.40909090638160706)
[2025-02-04 01:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:41][root][INFO] - Training Epoch: 2/2, step 21717/23838 completed (loss: 4.18513298034668, acc: 0.3333333432674408)
[2025-02-04 01:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:41][root][INFO] - Training Epoch: 2/2, step 21718/23838 completed (loss: 2.957131862640381, acc: 0.5)
[2025-02-04 01:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:41][root][INFO] - Training Epoch: 2/2, step 21719/23838 completed (loss: 2.426685333251953, acc: 0.5625)
[2025-02-04 01:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:42][root][INFO] - Training Epoch: 2/2, step 21720/23838 completed (loss: 3.4650349617004395, acc: 0.3499999940395355)
[2025-02-04 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:42][root][INFO] - Training Epoch: 2/2, step 21721/23838 completed (loss: 2.361588478088379, acc: 0.38461539149284363)
[2025-02-04 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:43][root][INFO] - Training Epoch: 2/2, step 21722/23838 completed (loss: 3.4589319229125977, acc: 0.4375)
[2025-02-04 01:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:43][root][INFO] - Training Epoch: 2/2, step 21723/23838 completed (loss: 1.4569432735443115, acc: 0.6153846383094788)
[2025-02-04 01:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:43][root][INFO] - Training Epoch: 2/2, step 21724/23838 completed (loss: 1.4536648988723755, acc: 0.800000011920929)
[2025-02-04 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:44][root][INFO] - Training Epoch: 2/2, step 21725/23838 completed (loss: 1.714171290397644, acc: 0.5199999809265137)
[2025-02-04 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:44][root][INFO] - Training Epoch: 2/2, step 21726/23838 completed (loss: 2.6965878009796143, acc: 0.5)
[2025-02-04 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:45][root][INFO] - Training Epoch: 2/2, step 21727/23838 completed (loss: 2.406752109527588, acc: 0.5)
[2025-02-04 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:45][root][INFO] - Training Epoch: 2/2, step 21728/23838 completed (loss: 3.211750030517578, acc: 0.4444444477558136)
[2025-02-04 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:46][root][INFO] - Training Epoch: 2/2, step 21729/23838 completed (loss: 4.803818702697754, acc: 0.3611111044883728)
[2025-02-04 01:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:46][root][INFO] - Training Epoch: 2/2, step 21730/23838 completed (loss: 4.39827299118042, acc: 0.3913043439388275)
[2025-02-04 01:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:46][root][INFO] - Training Epoch: 2/2, step 21731/23838 completed (loss: 2.5794477462768555, acc: 0.517241358757019)
[2025-02-04 01:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:47][root][INFO] - Training Epoch: 2/2, step 21732/23838 completed (loss: 2.9765818119049072, acc: 0.4444444477558136)
[2025-02-04 01:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:47][root][INFO] - Training Epoch: 2/2, step 21733/23838 completed (loss: 3.657050371170044, acc: 0.37142857909202576)
[2025-02-04 01:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:48][root][INFO] - Training Epoch: 2/2, step 21734/23838 completed (loss: 4.4726738929748535, acc: 0.30434781312942505)
[2025-02-04 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:48][root][INFO] - Training Epoch: 2/2, step 21735/23838 completed (loss: 3.1944220066070557, acc: 0.3636363744735718)
[2025-02-04 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:49][root][INFO] - Training Epoch: 2/2, step 21736/23838 completed (loss: 2.707237482070923, acc: 0.4000000059604645)
[2025-02-04 01:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:49][root][INFO] - Training Epoch: 2/2, step 21737/23838 completed (loss: 2.4067699909210205, acc: 0.42424243688583374)
[2025-02-04 01:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:50][root][INFO] - Training Epoch: 2/2, step 21738/23838 completed (loss: 2.639012575149536, acc: 0.48275861144065857)
[2025-02-04 01:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:51][root][INFO] - Training Epoch: 2/2, step 21739/23838 completed (loss: 3.367605209350586, acc: 0.375)
[2025-02-04 01:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:51][root][INFO] - Training Epoch: 2/2, step 21740/23838 completed (loss: 3.041733741760254, acc: 0.42424243688583374)
[2025-02-04 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:52][root][INFO] - Training Epoch: 2/2, step 21741/23838 completed (loss: 3.195810079574585, acc: 0.3636363744735718)
[2025-02-04 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:52][root][INFO] - Training Epoch: 2/2, step 21742/23838 completed (loss: 2.5399258136749268, acc: 0.4285714328289032)
[2025-02-04 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:53][root][INFO] - Training Epoch: 2/2, step 21743/23838 completed (loss: 0.5684436559677124, acc: 0.7857142686843872)
[2025-02-04 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:53][root][INFO] - Training Epoch: 2/2, step 21744/23838 completed (loss: 2.2455732822418213, acc: 0.6153846383094788)
[2025-02-04 01:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:54][root][INFO] - Training Epoch: 2/2, step 21745/23838 completed (loss: 2.061330795288086, acc: 0.43478259444236755)
[2025-02-04 01:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:55][root][INFO] - Training Epoch: 2/2, step 21746/23838 completed (loss: 2.2498350143432617, acc: 0.5675675868988037)
[2025-02-04 01:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:55][root][INFO] - Training Epoch: 2/2, step 21747/23838 completed (loss: 3.0292487144470215, acc: 0.41860464215278625)
[2025-02-04 01:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:56][root][INFO] - Training Epoch: 2/2, step 21748/23838 completed (loss: 3.2419328689575195, acc: 0.3877550959587097)
[2025-02-04 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:56][root][INFO] - Training Epoch: 2/2, step 21749/23838 completed (loss: 2.8385300636291504, acc: 0.3870967626571655)
[2025-02-04 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:56][root][INFO] - Training Epoch: 2/2, step 21750/23838 completed (loss: 2.865086078643799, acc: 0.52173912525177)
[2025-02-04 01:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:57][root][INFO] - Training Epoch: 2/2, step 21751/23838 completed (loss: 3.247629165649414, acc: 0.37037035822868347)
[2025-02-04 01:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:57][root][INFO] - Training Epoch: 2/2, step 21752/23838 completed (loss: 2.986306667327881, acc: 0.3030303120613098)
[2025-02-04 01:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:58][root][INFO] - Training Epoch: 2/2, step 21753/23838 completed (loss: 3.765974760055542, acc: 0.3469387888908386)
[2025-02-04 01:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:58][root][INFO] - Training Epoch: 2/2, step 21754/23838 completed (loss: 3.1708619594573975, acc: 0.3448275923728943)
[2025-02-04 01:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:59][root][INFO] - Training Epoch: 2/2, step 21755/23838 completed (loss: 3.228139638900757, acc: 0.375)
[2025-02-04 01:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:59][root][INFO] - Training Epoch: 2/2, step 21756/23838 completed (loss: 2.1579411029815674, acc: 0.46666666865348816)
[2025-02-04 01:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:01:59][root][INFO] - Training Epoch: 2/2, step 21757/23838 completed (loss: 3.202116012573242, acc: 0.4137931168079376)
[2025-02-04 01:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:00][root][INFO] - Training Epoch: 2/2, step 21758/23838 completed (loss: 2.0411908626556396, acc: 0.47826087474823)
[2025-02-04 01:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:00][root][INFO] - Training Epoch: 2/2, step 21759/23838 completed (loss: 2.0348081588745117, acc: 0.5161290168762207)
[2025-02-04 01:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:01][root][INFO] - Training Epoch: 2/2, step 21760/23838 completed (loss: 1.5419557094573975, acc: 0.7142857313156128)
[2025-02-04 01:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:01][root][INFO] - Training Epoch: 2/2, step 21761/23838 completed (loss: 3.2377874851226807, acc: 0.3611111044883728)
[2025-02-04 01:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:02][root][INFO] - Training Epoch: 2/2, step 21762/23838 completed (loss: 3.6339492797851562, acc: 0.38461539149284363)
[2025-02-04 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:02][root][INFO] - Training Epoch: 2/2, step 21763/23838 completed (loss: 2.519277811050415, acc: 0.625)
[2025-02-04 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:02][root][INFO] - Training Epoch: 2/2, step 21764/23838 completed (loss: 2.4035682678222656, acc: 0.4545454680919647)
[2025-02-04 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:03][root][INFO] - Training Epoch: 2/2, step 21765/23838 completed (loss: 2.5839009284973145, acc: 0.4516128897666931)
[2025-02-04 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:03][root][INFO] - Training Epoch: 2/2, step 21766/23838 completed (loss: 3.431565999984741, acc: 0.31578946113586426)
[2025-02-04 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:04][root][INFO] - Training Epoch: 2/2, step 21767/23838 completed (loss: 3.24453067779541, acc: 0.36000001430511475)
[2025-02-04 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:04][root][INFO] - Training Epoch: 2/2, step 21768/23838 completed (loss: 3.199312210083008, acc: 0.3235294222831726)
[2025-02-04 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:04][root][INFO] - Training Epoch: 2/2, step 21769/23838 completed (loss: 2.9240827560424805, acc: 0.5)
[2025-02-04 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:05][root][INFO] - Training Epoch: 2/2, step 21770/23838 completed (loss: 3.8193788528442383, acc: 0.27272728085517883)
[2025-02-04 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:05][root][INFO] - Training Epoch: 2/2, step 21771/23838 completed (loss: 3.803569793701172, acc: 0.42424243688583374)
[2025-02-04 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:06][root][INFO] - Training Epoch: 2/2, step 21772/23838 completed (loss: 4.3568525314331055, acc: 0.20000000298023224)
[2025-02-04 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:06][root][INFO] - Training Epoch: 2/2, step 21773/23838 completed (loss: 3.5609147548675537, acc: 0.20930232107639313)
[2025-02-04 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:06][root][INFO] - Training Epoch: 2/2, step 21774/23838 completed (loss: 3.1062588691711426, acc: 0.380952388048172)
[2025-02-04 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:07][root][INFO] - Training Epoch: 2/2, step 21775/23838 completed (loss: 3.9342000484466553, acc: 0.18918919563293457)
[2025-02-04 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:07][root][INFO] - Training Epoch: 2/2, step 21776/23838 completed (loss: 2.927318811416626, acc: 0.4399999976158142)
[2025-02-04 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:08][root][INFO] - Training Epoch: 2/2, step 21777/23838 completed (loss: 2.436657190322876, acc: 0.41025641560554504)
[2025-02-04 01:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:08][root][INFO] - Training Epoch: 2/2, step 21778/23838 completed (loss: 3.045027732849121, acc: 0.3030303120613098)
[2025-02-04 01:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:08][root][INFO] - Training Epoch: 2/2, step 21779/23838 completed (loss: 4.214858531951904, acc: 0.3076923191547394)
[2025-02-04 01:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:09][root][INFO] - Training Epoch: 2/2, step 21780/23838 completed (loss: 4.232853412628174, acc: 0.261904776096344)
[2025-02-04 01:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:09][root][INFO] - Training Epoch: 2/2, step 21781/23838 completed (loss: 2.436753749847412, acc: 0.48275861144065857)
[2025-02-04 01:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:10][root][INFO] - Training Epoch: 2/2, step 21782/23838 completed (loss: 3.7373814582824707, acc: 0.12820513546466827)
[2025-02-04 01:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:10][root][INFO] - Training Epoch: 2/2, step 21783/23838 completed (loss: 2.6007139682769775, acc: 0.42424243688583374)
[2025-02-04 01:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:10][root][INFO] - Training Epoch: 2/2, step 21784/23838 completed (loss: 3.3293838500976562, acc: 0.3636363744735718)
[2025-02-04 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:11][root][INFO] - Training Epoch: 2/2, step 21785/23838 completed (loss: 4.084969997406006, acc: 0.21212121844291687)
[2025-02-04 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:11][root][INFO] - Training Epoch: 2/2, step 21786/23838 completed (loss: 1.88760507106781, acc: 0.5)
[2025-02-04 01:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:12][root][INFO] - Training Epoch: 2/2, step 21787/23838 completed (loss: 3.3412961959838867, acc: 0.36538460850715637)
[2025-02-04 01:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:12][root][INFO] - Training Epoch: 2/2, step 21788/23838 completed (loss: 4.160404682159424, acc: 0.2926829159259796)
[2025-02-04 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:13][root][INFO] - Training Epoch: 2/2, step 21789/23838 completed (loss: 1.9895116090774536, acc: 0.5454545617103577)
[2025-02-04 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:13][root][INFO] - Training Epoch: 2/2, step 21790/23838 completed (loss: 3.1364967823028564, acc: 0.4047619104385376)
[2025-02-04 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:14][root][INFO] - Training Epoch: 2/2, step 21791/23838 completed (loss: 2.891040086746216, acc: 0.37931033968925476)
[2025-02-04 01:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:14][root][INFO] - Training Epoch: 2/2, step 21792/23838 completed (loss: 2.926805019378662, acc: 0.4117647111415863)
[2025-02-04 01:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:15][root][INFO] - Training Epoch: 2/2, step 21793/23838 completed (loss: 3.1042346954345703, acc: 0.2777777910232544)
[2025-02-04 01:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:15][root][INFO] - Training Epoch: 2/2, step 21794/23838 completed (loss: 2.733927011489868, acc: 0.4482758641242981)
[2025-02-04 01:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:15][root][INFO] - Training Epoch: 2/2, step 21795/23838 completed (loss: 3.7136218547821045, acc: 0.22499999403953552)
[2025-02-04 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:16][root][INFO] - Training Epoch: 2/2, step 21796/23838 completed (loss: 3.4247114658355713, acc: 0.3333333432674408)
[2025-02-04 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:16][root][INFO] - Training Epoch: 2/2, step 21797/23838 completed (loss: 3.5832767486572266, acc: 0.20000000298023224)
[2025-02-04 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:17][root][INFO] - Training Epoch: 2/2, step 21798/23838 completed (loss: 3.262772560119629, acc: 0.375)
[2025-02-04 01:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:17][root][INFO] - Training Epoch: 2/2, step 21799/23838 completed (loss: 2.0468688011169434, acc: 0.48275861144065857)
[2025-02-04 01:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:18][root][INFO] - Training Epoch: 2/2, step 21800/23838 completed (loss: 3.723191499710083, acc: 0.3199999928474426)
[2025-02-04 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:18][root][INFO] - Training Epoch: 2/2, step 21801/23838 completed (loss: 4.262167453765869, acc: 0.2631579041481018)
[2025-02-04 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:19][root][INFO] - Training Epoch: 2/2, step 21802/23838 completed (loss: 3.4627060890197754, acc: 0.3199999928474426)
[2025-02-04 01:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:19][root][INFO] - Training Epoch: 2/2, step 21803/23838 completed (loss: 2.0306236743927, acc: 0.5714285969734192)
[2025-02-04 01:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:19][root][INFO] - Training Epoch: 2/2, step 21804/23838 completed (loss: 2.6522738933563232, acc: 0.3333333432674408)
[2025-02-04 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:20][root][INFO] - Training Epoch: 2/2, step 21805/23838 completed (loss: 3.648214340209961, acc: 0.25806450843811035)
[2025-02-04 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:20][root][INFO] - Training Epoch: 2/2, step 21806/23838 completed (loss: 1.3441436290740967, acc: 0.6666666865348816)
[2025-02-04 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:21][root][INFO] - Training Epoch: 2/2, step 21807/23838 completed (loss: 3.354475975036621, acc: 0.27586206793785095)
[2025-02-04 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:21][root][INFO] - Training Epoch: 2/2, step 21808/23838 completed (loss: 4.12976598739624, acc: 0.18518517911434174)
[2025-02-04 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:21][root][INFO] - Training Epoch: 2/2, step 21809/23838 completed (loss: 3.3520755767822266, acc: 0.25925925374031067)
[2025-02-04 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:22][root][INFO] - Training Epoch: 2/2, step 21810/23838 completed (loss: 3.1053073406219482, acc: 0.29032257199287415)
[2025-02-04 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:22][root][INFO] - Training Epoch: 2/2, step 21811/23838 completed (loss: 3.129894495010376, acc: 0.3181818127632141)
[2025-02-04 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:23][root][INFO] - Training Epoch: 2/2, step 21812/23838 completed (loss: 3.0603747367858887, acc: 0.3076923191547394)
[2025-02-04 01:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:23][root][INFO] - Training Epoch: 2/2, step 21813/23838 completed (loss: 1.393575668334961, acc: 0.75)
[2025-02-04 01:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:24][root][INFO] - Training Epoch: 2/2, step 21814/23838 completed (loss: 2.1449620723724365, acc: 0.5384615659713745)
[2025-02-04 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:24][root][INFO] - Training Epoch: 2/2, step 21815/23838 completed (loss: 2.9274072647094727, acc: 0.4000000059604645)
[2025-02-04 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:25][root][INFO] - Training Epoch: 2/2, step 21816/23838 completed (loss: 1.7580995559692383, acc: 0.5806451439857483)
[2025-02-04 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:25][root][INFO] - Training Epoch: 2/2, step 21817/23838 completed (loss: 3.6631531715393066, acc: 0.3333333432674408)
[2025-02-04 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:25][root][INFO] - Training Epoch: 2/2, step 21818/23838 completed (loss: 2.745939254760742, acc: 0.4583333432674408)
[2025-02-04 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:26][root][INFO] - Training Epoch: 2/2, step 21819/23838 completed (loss: 2.6710731983184814, acc: 0.35483869910240173)
[2025-02-04 01:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:26][root][INFO] - Training Epoch: 2/2, step 21820/23838 completed (loss: 2.957515001296997, acc: 0.37931033968925476)
[2025-02-04 01:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:27][root][INFO] - Training Epoch: 2/2, step 21821/23838 completed (loss: 3.4793996810913086, acc: 0.3636363744735718)
[2025-02-04 01:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:27][root][INFO] - Training Epoch: 2/2, step 21822/23838 completed (loss: 3.354102373123169, acc: 0.3913043439388275)
[2025-02-04 01:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:28][root][INFO] - Training Epoch: 2/2, step 21823/23838 completed (loss: 3.729538917541504, acc: 0.2777777910232544)
[2025-02-04 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:28][root][INFO] - Training Epoch: 2/2, step 21824/23838 completed (loss: 3.55413818359375, acc: 0.3571428656578064)
[2025-02-04 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:28][root][INFO] - Training Epoch: 2/2, step 21825/23838 completed (loss: 3.223557233810425, acc: 0.24390244483947754)
[2025-02-04 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:29][root][INFO] - Training Epoch: 2/2, step 21826/23838 completed (loss: 2.513444662094116, acc: 0.4736842215061188)
[2025-02-04 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:29][root][INFO] - Training Epoch: 2/2, step 21827/23838 completed (loss: 2.542370080947876, acc: 0.375)
[2025-02-04 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:30][root][INFO] - Training Epoch: 2/2, step 21828/23838 completed (loss: 1.9295310974121094, acc: 0.3529411852359772)
[2025-02-04 01:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:30][root][INFO] - Training Epoch: 2/2, step 21829/23838 completed (loss: 3.059777021408081, acc: 0.32499998807907104)
[2025-02-04 01:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:31][root][INFO] - Training Epoch: 2/2, step 21830/23838 completed (loss: 2.6506528854370117, acc: 0.46666666865348816)
[2025-02-04 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:31][root][INFO] - Training Epoch: 2/2, step 21831/23838 completed (loss: 2.9157397747039795, acc: 0.47058823704719543)
[2025-02-04 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:32][root][INFO] - Training Epoch: 2/2, step 21832/23838 completed (loss: 2.6625945568084717, acc: 0.42424243688583374)
[2025-02-04 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:32][root][INFO] - Training Epoch: 2/2, step 21833/23838 completed (loss: 2.2818825244903564, acc: 0.625)
[2025-02-04 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:32][root][INFO] - Training Epoch: 2/2, step 21834/23838 completed (loss: 3.884089708328247, acc: 0.30434781312942505)
[2025-02-04 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:33][root][INFO] - Training Epoch: 2/2, step 21835/23838 completed (loss: 2.479327440261841, acc: 0.4761904776096344)
[2025-02-04 01:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:33][root][INFO] - Training Epoch: 2/2, step 21836/23838 completed (loss: 3.456521987915039, acc: 0.20000000298023224)
[2025-02-04 01:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:34][root][INFO] - Training Epoch: 2/2, step 21837/23838 completed (loss: 2.7001547813415527, acc: 0.3529411852359772)
[2025-02-04 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:34][root][INFO] - Training Epoch: 2/2, step 21838/23838 completed (loss: 2.813462972640991, acc: 0.47826087474823)
[2025-02-04 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:34][root][INFO] - Training Epoch: 2/2, step 21839/23838 completed (loss: 3.4005954265594482, acc: 0.36666667461395264)
[2025-02-04 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:35][root][INFO] - Training Epoch: 2/2, step 21840/23838 completed (loss: 2.071110248565674, acc: 0.5652173757553101)
[2025-02-04 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:35][root][INFO] - Training Epoch: 2/2, step 21841/23838 completed (loss: 3.440945625305176, acc: 0.2777777910232544)
[2025-02-04 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:36][root][INFO] - Training Epoch: 2/2, step 21842/23838 completed (loss: 4.10677433013916, acc: 0.3571428656578064)
[2025-02-04 01:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:36][root][INFO] - Training Epoch: 2/2, step 21843/23838 completed (loss: 2.740966796875, acc: 0.5)
[2025-02-04 01:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:37][root][INFO] - Training Epoch: 2/2, step 21844/23838 completed (loss: 2.028878688812256, acc: 0.4615384638309479)
[2025-02-04 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:37][root][INFO] - Training Epoch: 2/2, step 21845/23838 completed (loss: 2.557831287384033, acc: 0.40740740299224854)
[2025-02-04 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:37][root][INFO] - Training Epoch: 2/2, step 21846/23838 completed (loss: 2.8171772956848145, acc: 0.44999998807907104)
[2025-02-04 01:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:38][root][INFO] - Training Epoch: 2/2, step 21847/23838 completed (loss: 2.769991397857666, acc: 0.3333333432674408)
[2025-02-04 01:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:38][root][INFO] - Training Epoch: 2/2, step 21848/23838 completed (loss: 3.346296787261963, acc: 0.3076923191547394)
[2025-02-04 01:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:39][root][INFO] - Training Epoch: 2/2, step 21849/23838 completed (loss: 2.70214581489563, acc: 0.5)
[2025-02-04 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:39][root][INFO] - Training Epoch: 2/2, step 21850/23838 completed (loss: 2.8650448322296143, acc: 0.4444444477558136)
[2025-02-04 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:39][root][INFO] - Training Epoch: 2/2, step 21851/23838 completed (loss: 2.4846677780151367, acc: 0.6153846383094788)
[2025-02-04 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:40][root][INFO] - Training Epoch: 2/2, step 21852/23838 completed (loss: 3.0418167114257812, acc: 0.5263158082962036)
[2025-02-04 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:40][root][INFO] - Training Epoch: 2/2, step 21853/23838 completed (loss: 1.7729932069778442, acc: 0.6428571343421936)
[2025-02-04 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:41][root][INFO] - Training Epoch: 2/2, step 21854/23838 completed (loss: 2.2295796871185303, acc: 0.4583333432674408)
[2025-02-04 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:41][root][INFO] - Training Epoch: 2/2, step 21855/23838 completed (loss: 3.1236557960510254, acc: 0.44999998807907104)
[2025-02-04 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:42][root][INFO] - Training Epoch: 2/2, step 21856/23838 completed (loss: 3.5296154022216797, acc: 0.4146341383457184)
[2025-02-04 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:42][root][INFO] - Training Epoch: 2/2, step 21857/23838 completed (loss: 2.736966371536255, acc: 0.2666666805744171)
[2025-02-04 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:42][root][INFO] - Training Epoch: 2/2, step 21858/23838 completed (loss: 2.720916509628296, acc: 0.3181818127632141)
[2025-02-04 01:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:43][root][INFO] - Training Epoch: 2/2, step 21859/23838 completed (loss: 3.2609879970550537, acc: 0.4375)
[2025-02-04 01:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:43][root][INFO] - Training Epoch: 2/2, step 21860/23838 completed (loss: 3.1671295166015625, acc: 0.3461538553237915)
[2025-02-04 01:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:44][root][INFO] - Training Epoch: 2/2, step 21861/23838 completed (loss: 3.230107307434082, acc: 0.5)
[2025-02-04 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:44][root][INFO] - Training Epoch: 2/2, step 21862/23838 completed (loss: 2.6921896934509277, acc: 0.4285714328289032)
[2025-02-04 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:44][root][INFO] - Training Epoch: 2/2, step 21863/23838 completed (loss: 2.98302960395813, acc: 0.47826087474823)
[2025-02-04 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:45][root][INFO] - Training Epoch: 2/2, step 21864/23838 completed (loss: 3.7170329093933105, acc: 0.36666667461395264)
[2025-02-04 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:45][root][INFO] - Training Epoch: 2/2, step 21865/23838 completed (loss: 2.810786247253418, acc: 0.37037035822868347)
[2025-02-04 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:46][root][INFO] - Training Epoch: 2/2, step 21866/23838 completed (loss: 3.580615758895874, acc: 0.3571428656578064)
[2025-02-04 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:46][root][INFO] - Training Epoch: 2/2, step 21867/23838 completed (loss: 3.7449843883514404, acc: 0.34210526943206787)
[2025-02-04 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:47][root][INFO] - Training Epoch: 2/2, step 21868/23838 completed (loss: 4.2484025955200195, acc: 0.2222222238779068)
[2025-02-04 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:47][root][INFO] - Training Epoch: 2/2, step 21869/23838 completed (loss: 3.2548184394836426, acc: 0.25925925374031067)
[2025-02-04 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:47][root][INFO] - Training Epoch: 2/2, step 21870/23838 completed (loss: 2.0236704349517822, acc: 0.6315789222717285)
[2025-02-04 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:48][root][INFO] - Training Epoch: 2/2, step 21871/23838 completed (loss: 3.0696921348571777, acc: 0.3888888955116272)
[2025-02-04 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:48][root][INFO] - Training Epoch: 2/2, step 21872/23838 completed (loss: 2.7877089977264404, acc: 0.4399999976158142)
[2025-02-04 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:49][root][INFO] - Training Epoch: 2/2, step 21873/23838 completed (loss: 2.3491146564483643, acc: 0.42105263471603394)
[2025-02-04 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:49][root][INFO] - Training Epoch: 2/2, step 21874/23838 completed (loss: 1.869315505027771, acc: 0.5789473652839661)
[2025-02-04 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:49][root][INFO] - Training Epoch: 2/2, step 21875/23838 completed (loss: 3.8347692489624023, acc: 0.3333333432674408)
[2025-02-04 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:50][root][INFO] - Training Epoch: 2/2, step 21876/23838 completed (loss: 3.5462589263916016, acc: 0.4166666567325592)
[2025-02-04 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:50][root][INFO] - Training Epoch: 2/2, step 21877/23838 completed (loss: 3.521604061126709, acc: 0.3928571343421936)
[2025-02-04 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:51][root][INFO] - Training Epoch: 2/2, step 21878/23838 completed (loss: 2.6006627082824707, acc: 0.4583333432674408)
[2025-02-04 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:51][root][INFO] - Training Epoch: 2/2, step 21879/23838 completed (loss: 2.882214307785034, acc: 0.2142857164144516)
[2025-02-04 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:51][root][INFO] - Training Epoch: 2/2, step 21880/23838 completed (loss: 3.724379777908325, acc: 0.30434781312942505)
[2025-02-04 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:52][root][INFO] - Training Epoch: 2/2, step 21881/23838 completed (loss: 2.8722946643829346, acc: 0.3478260934352875)
[2025-02-04 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:52][root][INFO] - Training Epoch: 2/2, step 21882/23838 completed (loss: 2.5103912353515625, acc: 0.3684210479259491)
[2025-02-04 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:53][root][INFO] - Training Epoch: 2/2, step 21883/23838 completed (loss: 2.9093356132507324, acc: 0.5714285969734192)
[2025-02-04 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:53][root][INFO] - Training Epoch: 2/2, step 21884/23838 completed (loss: 3.8153443336486816, acc: 0.3529411852359772)
[2025-02-04 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:53][root][INFO] - Training Epoch: 2/2, step 21885/23838 completed (loss: 3.4793365001678467, acc: 0.3181818127632141)
[2025-02-04 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:54][root][INFO] - Training Epoch: 2/2, step 21886/23838 completed (loss: 4.634361743927002, acc: 0.32258063554763794)
[2025-02-04 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:54][root][INFO] - Training Epoch: 2/2, step 21887/23838 completed (loss: 3.5708277225494385, acc: 0.3214285671710968)
[2025-02-04 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:55][root][INFO] - Training Epoch: 2/2, step 21888/23838 completed (loss: 2.8098926544189453, acc: 0.29411765933036804)
[2025-02-04 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:55][root][INFO] - Training Epoch: 2/2, step 21889/23838 completed (loss: 3.2749717235565186, acc: 0.380952388048172)
[2025-02-04 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:55][root][INFO] - Training Epoch: 2/2, step 21890/23838 completed (loss: 3.725152015686035, acc: 0.30434781312942505)
[2025-02-04 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:56][root][INFO] - Training Epoch: 2/2, step 21891/23838 completed (loss: 2.9127066135406494, acc: 0.44117647409439087)
[2025-02-04 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:56][root][INFO] - Training Epoch: 2/2, step 21892/23838 completed (loss: 2.91316819190979, acc: 0.5)
[2025-02-04 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:57][root][INFO] - Training Epoch: 2/2, step 21893/23838 completed (loss: 1.5497199296951294, acc: 0.6153846383094788)
[2025-02-04 01:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:57][root][INFO] - Training Epoch: 2/2, step 21894/23838 completed (loss: 3.0196337699890137, acc: 0.4000000059604645)
[2025-02-04 01:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:58][root][INFO] - Training Epoch: 2/2, step 21895/23838 completed (loss: 2.762017250061035, acc: 0.5)
[2025-02-04 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:58][root][INFO] - Training Epoch: 2/2, step 21896/23838 completed (loss: 2.4971110820770264, acc: 0.29411765933036804)
[2025-02-04 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:59][root][INFO] - Training Epoch: 2/2, step 21897/23838 completed (loss: 3.0958876609802246, acc: 0.3571428656578064)
[2025-02-04 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:59][root][INFO] - Training Epoch: 2/2, step 21898/23838 completed (loss: 2.300471782684326, acc: 0.529411792755127)
[2025-02-04 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:02:59][root][INFO] - Training Epoch: 2/2, step 21899/23838 completed (loss: 3.1801798343658447, acc: 0.48148149251937866)
[2025-02-04 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:00][root][INFO] - Training Epoch: 2/2, step 21900/23838 completed (loss: 2.0162618160247803, acc: 0.47826087474823)
[2025-02-04 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:00][root][INFO] - Training Epoch: 2/2, step 21901/23838 completed (loss: 2.901423931121826, acc: 0.4117647111415863)
[2025-02-04 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:01][root][INFO] - Training Epoch: 2/2, step 21902/23838 completed (loss: 2.811079502105713, acc: 0.42307692766189575)
[2025-02-04 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:01][root][INFO] - Training Epoch: 2/2, step 21903/23838 completed (loss: 1.9799553155899048, acc: 0.5652173757553101)
[2025-02-04 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:02][root][INFO] - Training Epoch: 2/2, step 21904/23838 completed (loss: 3.6003224849700928, acc: 0.42307692766189575)
[2025-02-04 01:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:02][root][INFO] - Training Epoch: 2/2, step 21905/23838 completed (loss: 3.5244455337524414, acc: 0.25)
[2025-02-04 01:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:02][root][INFO] - Training Epoch: 2/2, step 21906/23838 completed (loss: 2.5043468475341797, acc: 0.4871794879436493)
[2025-02-04 01:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:03][root][INFO] - Training Epoch: 2/2, step 21907/23838 completed (loss: 2.5262656211853027, acc: 0.44736841320991516)
[2025-02-04 01:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:03][root][INFO] - Training Epoch: 2/2, step 21908/23838 completed (loss: 2.441077709197998, acc: 0.4761904776096344)
[2025-02-04 01:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:04][root][INFO] - Training Epoch: 2/2, step 21909/23838 completed (loss: 2.474015951156616, acc: 0.5)
[2025-02-04 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:04][root][INFO] - Training Epoch: 2/2, step 21910/23838 completed (loss: 3.6876437664031982, acc: 0.3333333432674408)
[2025-02-04 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:05][root][INFO] - Training Epoch: 2/2, step 21911/23838 completed (loss: 3.4045355319976807, acc: 0.3333333432674408)
[2025-02-04 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:05][root][INFO] - Training Epoch: 2/2, step 21912/23838 completed (loss: 3.3926215171813965, acc: 0.3777777850627899)
[2025-02-04 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:06][root][INFO] - Training Epoch: 2/2, step 21913/23838 completed (loss: 1.3683003187179565, acc: 0.6153846383094788)
[2025-02-04 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:06][root][INFO] - Training Epoch: 2/2, step 21914/23838 completed (loss: 2.327998638153076, acc: 0.47058823704719543)
[2025-02-04 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:07][root][INFO] - Training Epoch: 2/2, step 21915/23838 completed (loss: 2.450286865234375, acc: 0.4166666567325592)
[2025-02-04 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:07][root][INFO] - Training Epoch: 2/2, step 21916/23838 completed (loss: 3.2484731674194336, acc: 0.38461539149284363)
[2025-02-04 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:08][root][INFO] - Training Epoch: 2/2, step 21917/23838 completed (loss: 3.401340961456299, acc: 0.4375)
[2025-02-04 01:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:08][root][INFO] - Training Epoch: 2/2, step 21918/23838 completed (loss: 3.460392951965332, acc: 0.37142857909202576)
[2025-02-04 01:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:09][root][INFO] - Training Epoch: 2/2, step 21919/23838 completed (loss: 2.8640174865722656, acc: 0.529411792755127)
[2025-02-04 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:09][root][INFO] - Training Epoch: 2/2, step 21920/23838 completed (loss: 2.4329702854156494, acc: 0.6499999761581421)
[2025-02-04 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:10][root][INFO] - Training Epoch: 2/2, step 21921/23838 completed (loss: 3.2376060485839844, acc: 0.4000000059604645)
[2025-02-04 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:10][root][INFO] - Training Epoch: 2/2, step 21922/23838 completed (loss: 0.9017321467399597, acc: 0.692307710647583)
[2025-02-04 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:11][root][INFO] - Training Epoch: 2/2, step 21923/23838 completed (loss: 2.7105228900909424, acc: 0.6428571343421936)
[2025-02-04 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:11][root][INFO] - Training Epoch: 2/2, step 21924/23838 completed (loss: 1.884637713432312, acc: 0.5833333134651184)
[2025-02-04 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:12][root][INFO] - Training Epoch: 2/2, step 21925/23838 completed (loss: 4.395489692687988, acc: 0.30000001192092896)
[2025-02-04 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:12][root][INFO] - Training Epoch: 2/2, step 21926/23838 completed (loss: 5.693338394165039, acc: 0.3461538553237915)
[2025-02-04 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:12][root][INFO] - Training Epoch: 2/2, step 21927/23838 completed (loss: 2.4939796924591064, acc: 0.5789473652839661)
[2025-02-04 01:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:13][root][INFO] - Training Epoch: 2/2, step 21928/23838 completed (loss: 3.871899366378784, acc: 0.4375)
[2025-02-04 01:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:13][root][INFO] - Training Epoch: 2/2, step 21929/23838 completed (loss: 2.1594460010528564, acc: 0.5882353186607361)
[2025-02-04 01:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:14][root][INFO] - Training Epoch: 2/2, step 21930/23838 completed (loss: 2.71050763130188, acc: 0.5833333134651184)
[2025-02-04 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:14][root][INFO] - Training Epoch: 2/2, step 21931/23838 completed (loss: 2.3930037021636963, acc: 0.5555555820465088)
[2025-02-04 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:15][root][INFO] - Training Epoch: 2/2, step 21932/23838 completed (loss: 1.1235638856887817, acc: 0.699999988079071)
[2025-02-04 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:15][root][INFO] - Training Epoch: 2/2, step 21933/23838 completed (loss: 1.6095632314682007, acc: 0.6428571343421936)
[2025-02-04 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:15][root][INFO] - Training Epoch: 2/2, step 21934/23838 completed (loss: 3.5318028926849365, acc: 0.4761904776096344)
[2025-02-04 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:16][root][INFO] - Training Epoch: 2/2, step 21935/23838 completed (loss: 3.6937475204467773, acc: 0.4117647111415863)
[2025-02-04 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:16][root][INFO] - Training Epoch: 2/2, step 21936/23838 completed (loss: 3.3774478435516357, acc: 0.5625)
[2025-02-04 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:17][root][INFO] - Training Epoch: 2/2, step 21937/23838 completed (loss: 2.4484901428222656, acc: 0.5789473652839661)
[2025-02-04 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:17][root][INFO] - Training Epoch: 2/2, step 21938/23838 completed (loss: 2.353431224822998, acc: 0.5333333611488342)
[2025-02-04 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:17][root][INFO] - Training Epoch: 2/2, step 21939/23838 completed (loss: 3.608210563659668, acc: 0.4761904776096344)
[2025-02-04 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:18][root][INFO] - Training Epoch: 2/2, step 21940/23838 completed (loss: 3.000063896179199, acc: 0.4000000059604645)
[2025-02-04 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:18][root][INFO] - Training Epoch: 2/2, step 21941/23838 completed (loss: 1.4588648080825806, acc: 0.692307710647583)
[2025-02-04 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:19][root][INFO] - Training Epoch: 2/2, step 21942/23838 completed (loss: 3.7048909664154053, acc: 0.5)
[2025-02-04 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:19][root][INFO] - Training Epoch: 2/2, step 21943/23838 completed (loss: 3.709918975830078, acc: 0.4444444477558136)
[2025-02-04 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:19][root][INFO] - Training Epoch: 2/2, step 21944/23838 completed (loss: 3.2446892261505127, acc: 0.5)
[2025-02-04 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:20][root][INFO] - Training Epoch: 2/2, step 21945/23838 completed (loss: 2.0095889568328857, acc: 0.6428571343421936)
[2025-02-04 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:20][root][INFO] - Training Epoch: 2/2, step 21946/23838 completed (loss: 1.435538411140442, acc: 0.6428571343421936)
[2025-02-04 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:21][root][INFO] - Training Epoch: 2/2, step 21947/23838 completed (loss: 2.631190538406372, acc: 0.42105263471603394)
[2025-02-04 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:21][root][INFO] - Training Epoch: 2/2, step 21948/23838 completed (loss: 4.021803379058838, acc: 0.4285714328289032)
[2025-02-04 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:21][root][INFO] - Training Epoch: 2/2, step 21949/23838 completed (loss: 2.585134506225586, acc: 0.5882353186607361)
[2025-02-04 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:22][root][INFO] - Training Epoch: 2/2, step 21950/23838 completed (loss: 3.5712602138519287, acc: 0.4117647111415863)
[2025-02-04 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:22][root][INFO] - Training Epoch: 2/2, step 21951/23838 completed (loss: 3.4145328998565674, acc: 0.3499999940395355)
[2025-02-04 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:23][root][INFO] - Training Epoch: 2/2, step 21952/23838 completed (loss: 2.337127685546875, acc: 0.5555555820465088)
[2025-02-04 01:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:23][root][INFO] - Training Epoch: 2/2, step 21953/23838 completed (loss: 2.9128031730651855, acc: 0.4285714328289032)
[2025-02-04 01:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:24][root][INFO] - Training Epoch: 2/2, step 21954/23838 completed (loss: 3.416759729385376, acc: 0.30434781312942505)
[2025-02-04 01:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:24][root][INFO] - Training Epoch: 2/2, step 21955/23838 completed (loss: 2.9696149826049805, acc: 0.5384615659713745)
[2025-02-04 01:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:25][root][INFO] - Training Epoch: 2/2, step 21956/23838 completed (loss: 2.936481237411499, acc: 0.5666666626930237)
[2025-02-04 01:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:25][root][INFO] - Training Epoch: 2/2, step 21957/23838 completed (loss: 1.9662048816680908, acc: 0.5714285969734192)
[2025-02-04 01:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:26][root][INFO] - Training Epoch: 2/2, step 21958/23838 completed (loss: 3.50768780708313, acc: 0.4285714328289032)
[2025-02-04 01:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:26][root][INFO] - Training Epoch: 2/2, step 21959/23838 completed (loss: 2.0503506660461426, acc: 0.5789473652839661)
[2025-02-04 01:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:26][root][INFO] - Training Epoch: 2/2, step 21960/23838 completed (loss: 2.2465786933898926, acc: 0.6000000238418579)
[2025-02-04 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:27][root][INFO] - Training Epoch: 2/2, step 21961/23838 completed (loss: 1.7963969707489014, acc: 0.5263158082962036)
[2025-02-04 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:27][root][INFO] - Training Epoch: 2/2, step 21962/23838 completed (loss: 2.065089464187622, acc: 0.5357142686843872)
[2025-02-04 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:28][root][INFO] - Training Epoch: 2/2, step 21963/23838 completed (loss: 2.7467246055603027, acc: 0.4761904776096344)
[2025-02-04 01:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:28][root][INFO] - Training Epoch: 2/2, step 21964/23838 completed (loss: 4.107112407684326, acc: 0.4761904776096344)
[2025-02-04 01:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:29][root][INFO] - Training Epoch: 2/2, step 21965/23838 completed (loss: 4.279253959655762, acc: 0.3571428656578064)
[2025-02-04 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:29][root][INFO] - Training Epoch: 2/2, step 21966/23838 completed (loss: 3.494453191757202, acc: 0.3333333432674408)
[2025-02-04 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:29][root][INFO] - Training Epoch: 2/2, step 21967/23838 completed (loss: 3.4417688846588135, acc: 0.529411792755127)
[2025-02-04 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:30][root][INFO] - Training Epoch: 2/2, step 21968/23838 completed (loss: 4.130645751953125, acc: 0.46341463923454285)
[2025-02-04 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:30][root][INFO] - Training Epoch: 2/2, step 21969/23838 completed (loss: 2.7721517086029053, acc: 0.42307692766189575)
[2025-02-04 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:31][root][INFO] - Training Epoch: 2/2, step 21970/23838 completed (loss: 2.6727914810180664, acc: 0.4000000059604645)
[2025-02-04 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:31][root][INFO] - Training Epoch: 2/2, step 21971/23838 completed (loss: 4.024133205413818, acc: 0.2857142984867096)
[2025-02-04 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:32][root][INFO] - Training Epoch: 2/2, step 21972/23838 completed (loss: 3.235785961151123, acc: 0.4285714328289032)
[2025-02-04 01:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:32][root][INFO] - Training Epoch: 2/2, step 21973/23838 completed (loss: 2.0146806240081787, acc: 0.6153846383094788)
[2025-02-04 01:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:33][root][INFO] - Training Epoch: 2/2, step 21974/23838 completed (loss: 2.2776167392730713, acc: 0.6000000238418579)
[2025-02-04 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:33][root][INFO] - Training Epoch: 2/2, step 21975/23838 completed (loss: 3.175179958343506, acc: 0.529411792755127)
[2025-02-04 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:33][root][INFO] - Training Epoch: 2/2, step 21976/23838 completed (loss: 3.1155359745025635, acc: 0.42105263471603394)
[2025-02-04 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:34][root][INFO] - Training Epoch: 2/2, step 21977/23838 completed (loss: 3.5145974159240723, acc: 0.4399999976158142)
[2025-02-04 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:34][root][INFO] - Training Epoch: 2/2, step 21978/23838 completed (loss: 4.361062526702881, acc: 0.2068965584039688)
[2025-02-04 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:35][root][INFO] - Training Epoch: 2/2, step 21979/23838 completed (loss: 2.572268009185791, acc: 0.5)
[2025-02-04 01:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:35][root][INFO] - Training Epoch: 2/2, step 21980/23838 completed (loss: 2.712709903717041, acc: 0.4642857015132904)
[2025-02-04 01:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:35][root][INFO] - Training Epoch: 2/2, step 21981/23838 completed (loss: 3.19108510017395, acc: 0.5)
[2025-02-04 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:36][root][INFO] - Training Epoch: 2/2, step 21982/23838 completed (loss: 1.877433180809021, acc: 0.7058823704719543)
[2025-02-04 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:36][root][INFO] - Training Epoch: 2/2, step 21983/23838 completed (loss: 2.9277918338775635, acc: 0.48571428656578064)
[2025-02-04 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:37][root][INFO] - Training Epoch: 2/2, step 21984/23838 completed (loss: 2.1781978607177734, acc: 0.5555555820465088)
[2025-02-04 01:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:37][root][INFO] - Training Epoch: 2/2, step 21985/23838 completed (loss: 1.9776148796081543, acc: 0.5625)
[2025-02-04 01:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:38][root][INFO] - Training Epoch: 2/2, step 21986/23838 completed (loss: 3.167699098587036, acc: 0.47058823704719543)
[2025-02-04 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:38][root][INFO] - Training Epoch: 2/2, step 21987/23838 completed (loss: 2.803438901901245, acc: 0.43589743971824646)
[2025-02-04 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:39][root][INFO] - Training Epoch: 2/2, step 21988/23838 completed (loss: 2.7280983924865723, acc: 0.44117647409439087)
[2025-02-04 01:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:39][root][INFO] - Training Epoch: 2/2, step 21989/23838 completed (loss: 3.3801891803741455, acc: 0.38461539149284363)
[2025-02-04 01:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:39][root][INFO] - Training Epoch: 2/2, step 21990/23838 completed (loss: 3.0815091133117676, acc: 0.52173912525177)
[2025-02-04 01:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:40][root][INFO] - Training Epoch: 2/2, step 21991/23838 completed (loss: 2.672281265258789, acc: 0.5517241358757019)
[2025-02-04 01:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:40][root][INFO] - Training Epoch: 2/2, step 21992/23838 completed (loss: 2.7818734645843506, acc: 0.5)
[2025-02-04 01:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:41][root][INFO] - Training Epoch: 2/2, step 21993/23838 completed (loss: 3.4086859226226807, acc: 0.3461538553237915)
[2025-02-04 01:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:41][root][INFO] - Training Epoch: 2/2, step 21994/23838 completed (loss: 2.938953161239624, acc: 0.5)
[2025-02-04 01:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:41][root][INFO] - Training Epoch: 2/2, step 21995/23838 completed (loss: 2.035090923309326, acc: 0.5714285969734192)
[2025-02-04 01:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:42][root][INFO] - Training Epoch: 2/2, step 21996/23838 completed (loss: 3.280121088027954, acc: 0.4516128897666931)
[2025-02-04 01:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:42][root][INFO] - Training Epoch: 2/2, step 21997/23838 completed (loss: 3.376735210418701, acc: 0.3777777850627899)
[2025-02-04 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:43][root][INFO] - Training Epoch: 2/2, step 21998/23838 completed (loss: 3.563011884689331, acc: 0.5)
[2025-02-04 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:43][root][INFO] - Training Epoch: 2/2, step 21999/23838 completed (loss: 2.4434525966644287, acc: 0.6470588445663452)
[2025-02-04 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:44][root][INFO] - Training Epoch: 2/2, step 22000/23838 completed (loss: 1.6566880941390991, acc: 0.6428571343421936)
[2025-02-04 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:44][root][INFO] - Training Epoch: 2/2, step 22001/23838 completed (loss: 2.8345139026641846, acc: 0.47826087474823)
[2025-02-04 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:44][root][INFO] - Training Epoch: 2/2, step 22002/23838 completed (loss: 3.519911766052246, acc: 0.40909090638160706)
[2025-02-04 01:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:45][root][INFO] - Training Epoch: 2/2, step 22003/23838 completed (loss: 2.040799379348755, acc: 0.5454545617103577)
[2025-02-04 01:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:45][root][INFO] - Training Epoch: 2/2, step 22004/23838 completed (loss: 3.5076067447662354, acc: 0.47826087474823)
[2025-02-04 01:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:46][root][INFO] - Training Epoch: 2/2, step 22005/23838 completed (loss: 1.1245722770690918, acc: 0.8500000238418579)
[2025-02-04 01:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:46][root][INFO] - Training Epoch: 2/2, step 22006/23838 completed (loss: 2.9411988258361816, acc: 0.5)
[2025-02-04 01:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:47][root][INFO] - Training Epoch: 2/2, step 22007/23838 completed (loss: 3.438927412033081, acc: 0.3636363744735718)
[2025-02-04 01:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:47][root][INFO] - Training Epoch: 2/2, step 22008/23838 completed (loss: 3.8768539428710938, acc: 0.36666667461395264)
[2025-02-04 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:48][root][INFO] - Training Epoch: 2/2, step 22009/23838 completed (loss: 2.3134591579437256, acc: 0.5)
[2025-02-04 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:48][root][INFO] - Training Epoch: 2/2, step 22010/23838 completed (loss: 2.2683351039886475, acc: 0.517241358757019)
[2025-02-04 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:49][root][INFO] - Training Epoch: 2/2, step 22011/23838 completed (loss: 3.899109363555908, acc: 0.3870967626571655)
[2025-02-04 01:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:49][root][INFO] - Training Epoch: 2/2, step 22012/23838 completed (loss: 2.2482361793518066, acc: 0.5714285969734192)
[2025-02-04 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:50][root][INFO] - Training Epoch: 2/2, step 22013/23838 completed (loss: 2.692148447036743, acc: 0.47999998927116394)
[2025-02-04 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:50][root][INFO] - Training Epoch: 2/2, step 22014/23838 completed (loss: 1.0767388343811035, acc: 0.7222222089767456)
[2025-02-04 01:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:51][root][INFO] - Training Epoch: 2/2, step 22015/23838 completed (loss: 2.2181198596954346, acc: 0.550000011920929)
[2025-02-04 01:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:51][root][INFO] - Training Epoch: 2/2, step 22016/23838 completed (loss: 3.8292083740234375, acc: 0.3333333432674408)
[2025-02-04 01:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:52][root][INFO] - Training Epoch: 2/2, step 22017/23838 completed (loss: 2.65116810798645, acc: 0.4516128897666931)
[2025-02-04 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:52][root][INFO] - Training Epoch: 2/2, step 22018/23838 completed (loss: 3.655510187149048, acc: 0.43478259444236755)
[2025-02-04 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:53][root][INFO] - Training Epoch: 2/2, step 22019/23838 completed (loss: 2.920966863632202, acc: 0.5)
[2025-02-04 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:53][root][INFO] - Training Epoch: 2/2, step 22020/23838 completed (loss: 2.265326976776123, acc: 0.6000000238418579)
[2025-02-04 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:54][root][INFO] - Training Epoch: 2/2, step 22021/23838 completed (loss: 1.956470251083374, acc: 0.6000000238418579)
[2025-02-04 01:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:54][root][INFO] - Training Epoch: 2/2, step 22022/23838 completed (loss: 3.2986347675323486, acc: 0.3636363744735718)
[2025-02-04 01:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:54][root][INFO] - Training Epoch: 2/2, step 22023/23838 completed (loss: 2.594855308532715, acc: 0.38461539149284363)
[2025-02-04 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:55][root][INFO] - Training Epoch: 2/2, step 22024/23838 completed (loss: 3.709059238433838, acc: 0.35483869910240173)
[2025-02-04 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:55][root][INFO] - Training Epoch: 2/2, step 22025/23838 completed (loss: 3.1668269634246826, acc: 0.4444444477558136)
[2025-02-04 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:56][root][INFO] - Training Epoch: 2/2, step 22026/23838 completed (loss: 3.3981878757476807, acc: 0.37837839126586914)
[2025-02-04 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:56][root][INFO] - Training Epoch: 2/2, step 22027/23838 completed (loss: 3.816955804824829, acc: 0.21276596188545227)
[2025-02-04 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:57][root][INFO] - Training Epoch: 2/2, step 22028/23838 completed (loss: 4.493165969848633, acc: 0.3333333432674408)
[2025-02-04 01:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:57][root][INFO] - Training Epoch: 2/2, step 22029/23838 completed (loss: 4.231729507446289, acc: 0.4117647111415863)
[2025-02-04 01:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:58][root][INFO] - Training Epoch: 2/2, step 22030/23838 completed (loss: 3.5062406063079834, acc: 0.42307692766189575)
[2025-02-04 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:58][root][INFO] - Training Epoch: 2/2, step 22031/23838 completed (loss: 4.616375923156738, acc: 0.30000001192092896)
[2025-02-04 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:59][root][INFO] - Training Epoch: 2/2, step 22032/23838 completed (loss: 3.896026134490967, acc: 0.3529411852359772)
[2025-02-04 01:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:59][root][INFO] - Training Epoch: 2/2, step 22033/23838 completed (loss: 3.2825241088867188, acc: 0.30000001192092896)
[2025-02-04 01:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:03:59][root][INFO] - Training Epoch: 2/2, step 22034/23838 completed (loss: 3.405444383621216, acc: 0.27586206793785095)
[2025-02-04 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:00][root][INFO] - Training Epoch: 2/2, step 22035/23838 completed (loss: 2.7567431926727295, acc: 0.3636363744735718)
[2025-02-04 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:00][root][INFO] - Training Epoch: 2/2, step 22036/23838 completed (loss: 3.8534581661224365, acc: 0.4642857015132904)
[2025-02-04 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:01][root][INFO] - Training Epoch: 2/2, step 22037/23838 completed (loss: 3.0911803245544434, acc: 0.4736842215061188)
[2025-02-04 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:01][root][INFO] - Training Epoch: 2/2, step 22038/23838 completed (loss: 3.714207410812378, acc: 0.28125)
[2025-02-04 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:02][root][INFO] - Training Epoch: 2/2, step 22039/23838 completed (loss: 4.293198108673096, acc: 0.2800000011920929)
[2025-02-04 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:02][root][INFO] - Training Epoch: 2/2, step 22040/23838 completed (loss: 2.689802646636963, acc: 0.4848484992980957)
[2025-02-04 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:02][root][INFO] - Training Epoch: 2/2, step 22041/23838 completed (loss: 3.8605971336364746, acc: 0.28125)
[2025-02-04 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:03][root][INFO] - Training Epoch: 2/2, step 22042/23838 completed (loss: 3.270718574523926, acc: 0.4375)
[2025-02-04 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:03][root][INFO] - Training Epoch: 2/2, step 22043/23838 completed (loss: 3.482706069946289, acc: 0.23529411852359772)
[2025-02-04 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:04][root][INFO] - Training Epoch: 2/2, step 22044/23838 completed (loss: 3.408695697784424, acc: 0.30000001192092896)
[2025-02-04 01:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:04][root][INFO] - Training Epoch: 2/2, step 22045/23838 completed (loss: 3.3553526401519775, acc: 0.3870967626571655)
[2025-02-04 01:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:05][root][INFO] - Training Epoch: 2/2, step 22046/23838 completed (loss: 3.135934591293335, acc: 0.38461539149284363)
[2025-02-04 01:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:05][root][INFO] - Training Epoch: 2/2, step 22047/23838 completed (loss: 2.4969003200531006, acc: 0.4137931168079376)
[2025-02-04 01:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:06][root][INFO] - Training Epoch: 2/2, step 22048/23838 completed (loss: 2.603065013885498, acc: 0.5806451439857483)
[2025-02-04 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:06][root][INFO] - Training Epoch: 2/2, step 22049/23838 completed (loss: 3.556323289871216, acc: 0.3333333432674408)
[2025-02-04 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:06][root][INFO] - Training Epoch: 2/2, step 22050/23838 completed (loss: 2.038550615310669, acc: 0.6875)
[2025-02-04 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:07][root][INFO] - Training Epoch: 2/2, step 22051/23838 completed (loss: 3.0417885780334473, acc: 0.43478259444236755)
[2025-02-04 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:07][root][INFO] - Training Epoch: 2/2, step 22052/23838 completed (loss: 2.7029380798339844, acc: 0.5416666865348816)
[2025-02-04 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:08][root][INFO] - Training Epoch: 2/2, step 22053/23838 completed (loss: 3.448478937149048, acc: 0.38235294818878174)
[2025-02-04 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:08][root][INFO] - Training Epoch: 2/2, step 22054/23838 completed (loss: 2.5494039058685303, acc: 0.47826087474823)
[2025-02-04 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:08][root][INFO] - Training Epoch: 2/2, step 22055/23838 completed (loss: 3.8191146850585938, acc: 0.2647058963775635)
[2025-02-04 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:09][root][INFO] - Training Epoch: 2/2, step 22056/23838 completed (loss: 3.7643685340881348, acc: 0.2142857164144516)
[2025-02-04 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:09][root][INFO] - Training Epoch: 2/2, step 22057/23838 completed (loss: 3.183039665222168, acc: 0.46666666865348816)
[2025-02-04 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:10][root][INFO] - Training Epoch: 2/2, step 22058/23838 completed (loss: 2.568112373352051, acc: 0.48275861144065857)
[2025-02-04 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:10][root][INFO] - Training Epoch: 2/2, step 22059/23838 completed (loss: 2.825399160385132, acc: 0.47999998927116394)
[2025-02-04 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:10][root][INFO] - Training Epoch: 2/2, step 22060/23838 completed (loss: 3.964322566986084, acc: 0.3333333432674408)
[2025-02-04 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:11][root][INFO] - Training Epoch: 2/2, step 22061/23838 completed (loss: 3.487779140472412, acc: 0.42500001192092896)
[2025-02-04 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:11][root][INFO] - Training Epoch: 2/2, step 22062/23838 completed (loss: 2.7803006172180176, acc: 0.4642857015132904)
[2025-02-04 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:12][root][INFO] - Training Epoch: 2/2, step 22063/23838 completed (loss: 3.3915481567382812, acc: 0.3199999928474426)
[2025-02-04 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:12][root][INFO] - Training Epoch: 2/2, step 22064/23838 completed (loss: 3.7776761054992676, acc: 0.32258063554763794)
[2025-02-04 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:13][root][INFO] - Training Epoch: 2/2, step 22065/23838 completed (loss: 3.6440067291259766, acc: 0.3571428656578064)
[2025-02-04 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:13][root][INFO] - Training Epoch: 2/2, step 22066/23838 completed (loss: 3.854250192642212, acc: 0.3461538553237915)
[2025-02-04 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:13][root][INFO] - Training Epoch: 2/2, step 22067/23838 completed (loss: 3.2051286697387695, acc: 0.42105263471603394)
[2025-02-04 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:14][root][INFO] - Training Epoch: 2/2, step 22068/23838 completed (loss: 3.427248477935791, acc: 0.3103448152542114)
[2025-02-04 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:14][root][INFO] - Training Epoch: 2/2, step 22069/23838 completed (loss: 3.351738691329956, acc: 0.5)
[2025-02-04 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:15][root][INFO] - Training Epoch: 2/2, step 22070/23838 completed (loss: 3.751434803009033, acc: 0.5)
[2025-02-04 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:15][root][INFO] - Training Epoch: 2/2, step 22071/23838 completed (loss: 2.570673704147339, acc: 0.48275861144065857)
[2025-02-04 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:15][root][INFO] - Training Epoch: 2/2, step 22072/23838 completed (loss: 3.9431278705596924, acc: 0.25)
[2025-02-04 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:16][root][INFO] - Training Epoch: 2/2, step 22073/23838 completed (loss: 3.2579894065856934, acc: 0.3199999928474426)
[2025-02-04 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:16][root][INFO] - Training Epoch: 2/2, step 22074/23838 completed (loss: 3.176976442337036, acc: 0.5)
[2025-02-04 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:17][root][INFO] - Training Epoch: 2/2, step 22075/23838 completed (loss: 3.3327746391296387, acc: 0.36000001430511475)
[2025-02-04 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:17][root][INFO] - Training Epoch: 2/2, step 22076/23838 completed (loss: 4.268889904022217, acc: 0.24137930572032928)
[2025-02-04 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:18][root][INFO] - Training Epoch: 2/2, step 22077/23838 completed (loss: 3.225270986557007, acc: 0.34210526943206787)
[2025-02-04 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:18][root][INFO] - Training Epoch: 2/2, step 22078/23838 completed (loss: 3.4279749393463135, acc: 0.3030303120613098)
[2025-02-04 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:18][root][INFO] - Training Epoch: 2/2, step 22079/23838 completed (loss: 4.4866838455200195, acc: 0.24137930572032928)
[2025-02-04 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:19][root][INFO] - Training Epoch: 2/2, step 22080/23838 completed (loss: 3.3827438354492188, acc: 0.37931033968925476)
[2025-02-04 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:19][root][INFO] - Training Epoch: 2/2, step 22081/23838 completed (loss: 4.147612571716309, acc: 0.3333333432674408)
[2025-02-04 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:20][root][INFO] - Training Epoch: 2/2, step 22082/23838 completed (loss: 4.235431671142578, acc: 0.31578946113586426)
[2025-02-04 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:20][root][INFO] - Training Epoch: 2/2, step 22083/23838 completed (loss: 2.9974026679992676, acc: 0.5)
[2025-02-04 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:21][root][INFO] - Training Epoch: 2/2, step 22084/23838 completed (loss: 3.449714422225952, acc: 0.29629629850387573)
[2025-02-04 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:21][root][INFO] - Training Epoch: 2/2, step 22085/23838 completed (loss: 3.096471071243286, acc: 0.4642857015132904)
[2025-02-04 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:22][root][INFO] - Training Epoch: 2/2, step 22086/23838 completed (loss: 2.8865151405334473, acc: 0.3333333432674408)
[2025-02-04 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:22][root][INFO] - Training Epoch: 2/2, step 22087/23838 completed (loss: 3.677440881729126, acc: 0.2222222238779068)
[2025-02-04 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:22][root][INFO] - Training Epoch: 2/2, step 22088/23838 completed (loss: 3.665606737136841, acc: 0.36000001430511475)
[2025-02-04 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:23][root][INFO] - Training Epoch: 2/2, step 22089/23838 completed (loss: 3.2065858840942383, acc: 0.2666666805744171)
[2025-02-04 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:23][root][INFO] - Training Epoch: 2/2, step 22090/23838 completed (loss: 3.8664798736572266, acc: 0.2368421107530594)
[2025-02-04 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:24][root][INFO] - Training Epoch: 2/2, step 22091/23838 completed (loss: 2.6766974925994873, acc: 0.32258063554763794)
[2025-02-04 01:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:24][root][INFO] - Training Epoch: 2/2, step 22092/23838 completed (loss: 3.2268359661102295, acc: 0.30000001192092896)
[2025-02-04 01:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:25][root][INFO] - Training Epoch: 2/2, step 22093/23838 completed (loss: 2.5595030784606934, acc: 0.4137931168079376)
[2025-02-04 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:25][root][INFO] - Training Epoch: 2/2, step 22094/23838 completed (loss: 3.083784341812134, acc: 0.4000000059604645)
[2025-02-04 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:26][root][INFO] - Training Epoch: 2/2, step 22095/23838 completed (loss: 2.9806056022644043, acc: 0.5)
[2025-02-04 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:26][root][INFO] - Training Epoch: 2/2, step 22096/23838 completed (loss: 3.8535165786743164, acc: 0.3529411852359772)
[2025-02-04 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:26][root][INFO] - Training Epoch: 2/2, step 22097/23838 completed (loss: 2.8715660572052, acc: 0.4054054021835327)
[2025-02-04 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:27][root][INFO] - Training Epoch: 2/2, step 22098/23838 completed (loss: 3.99137544631958, acc: 0.3571428656578064)
[2025-02-04 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:27][root][INFO] - Training Epoch: 2/2, step 22099/23838 completed (loss: 2.944185733795166, acc: 0.3888888955116272)
[2025-02-04 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:28][root][INFO] - Training Epoch: 2/2, step 22100/23838 completed (loss: 3.512816905975342, acc: 0.47058823704719543)
[2025-02-04 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:28][root][INFO] - Training Epoch: 2/2, step 22101/23838 completed (loss: 2.265432119369507, acc: 0.5416666865348816)
[2025-02-04 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:29][root][INFO] - Training Epoch: 2/2, step 22102/23838 completed (loss: 2.6737282276153564, acc: 0.5199999809265137)
[2025-02-04 01:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:29][root][INFO] - Training Epoch: 2/2, step 22103/23838 completed (loss: 1.604897141456604, acc: 0.6153846383094788)
[2025-02-04 01:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:29][root][INFO] - Training Epoch: 2/2, step 22104/23838 completed (loss: 2.9204261302948, acc: 0.43589743971824646)
[2025-02-04 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:30][root][INFO] - Training Epoch: 2/2, step 22105/23838 completed (loss: 2.632735013961792, acc: 0.380952388048172)
[2025-02-04 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:30][root][INFO] - Training Epoch: 2/2, step 22106/23838 completed (loss: 3.142781972885132, acc: 0.44680851697921753)
[2025-02-04 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:31][root][INFO] - Training Epoch: 2/2, step 22107/23838 completed (loss: 4.005050182342529, acc: 0.3166666626930237)
[2025-02-04 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:31][root][INFO] - Training Epoch: 2/2, step 22108/23838 completed (loss: 2.686065912246704, acc: 0.47826087474823)
[2025-02-04 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:32][root][INFO] - Training Epoch: 2/2, step 22109/23838 completed (loss: 3.2486298084259033, acc: 0.3333333432674408)
[2025-02-04 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:32][root][INFO] - Training Epoch: 2/2, step 22110/23838 completed (loss: 3.4737708568573, acc: 0.4545454680919647)
[2025-02-04 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:32][root][INFO] - Training Epoch: 2/2, step 22111/23838 completed (loss: 2.6258656978607178, acc: 0.4642857015132904)
[2025-02-04 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:33][root][INFO] - Training Epoch: 2/2, step 22112/23838 completed (loss: 3.080599069595337, acc: 0.3870967626571655)
[2025-02-04 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:33][root][INFO] - Training Epoch: 2/2, step 22113/23838 completed (loss: 2.8422200679779053, acc: 0.47999998927116394)
[2025-02-04 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:34][root][INFO] - Training Epoch: 2/2, step 22114/23838 completed (loss: 3.3386776447296143, acc: 0.37037035822868347)
[2025-02-04 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:34][root][INFO] - Training Epoch: 2/2, step 22115/23838 completed (loss: 2.94933819770813, acc: 0.3684210479259491)
[2025-02-04 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:35][root][INFO] - Training Epoch: 2/2, step 22116/23838 completed (loss: 3.505821466445923, acc: 0.4137931168079376)
[2025-02-04 01:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:35][root][INFO] - Training Epoch: 2/2, step 22117/23838 completed (loss: 2.859928846359253, acc: 0.45098039507865906)
[2025-02-04 01:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:36][root][INFO] - Training Epoch: 2/2, step 22118/23838 completed (loss: 3.2843594551086426, acc: 0.37142857909202576)
[2025-02-04 01:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:36][root][INFO] - Training Epoch: 2/2, step 22119/23838 completed (loss: 3.9987988471984863, acc: 0.36666667461395264)
[2025-02-04 01:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:37][root][INFO] - Training Epoch: 2/2, step 22120/23838 completed (loss: 2.6963541507720947, acc: 0.43478259444236755)
[2025-02-04 01:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:37][root][INFO] - Training Epoch: 2/2, step 22121/23838 completed (loss: 3.0607409477233887, acc: 0.37931033968925476)
[2025-02-04 01:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:38][root][INFO] - Training Epoch: 2/2, step 22122/23838 completed (loss: 3.2303855419158936, acc: 0.3142857253551483)
[2025-02-04 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:38][root][INFO] - Training Epoch: 2/2, step 22123/23838 completed (loss: 2.831265687942505, acc: 0.4571428596973419)
[2025-02-04 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:38][root][INFO] - Training Epoch: 2/2, step 22124/23838 completed (loss: 2.6189661026000977, acc: 0.4545454680919647)
[2025-02-04 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:39][root][INFO] - Training Epoch: 2/2, step 22125/23838 completed (loss: 3.5792229175567627, acc: 0.2368421107530594)
[2025-02-04 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:39][root][INFO] - Training Epoch: 2/2, step 22126/23838 completed (loss: 3.5933187007904053, acc: 0.20588235557079315)
[2025-02-04 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:40][root][INFO] - Training Epoch: 2/2, step 22127/23838 completed (loss: 2.827714443206787, acc: 0.4615384638309479)
[2025-02-04 01:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:40][root][INFO] - Training Epoch: 2/2, step 22128/23838 completed (loss: 2.895451068878174, acc: 0.43478259444236755)
[2025-02-04 01:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:41][root][INFO] - Training Epoch: 2/2, step 22129/23838 completed (loss: 3.137690305709839, acc: 0.4444444477558136)
[2025-02-04 01:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:41][root][INFO] - Training Epoch: 2/2, step 22130/23838 completed (loss: 2.838029623031616, acc: 0.5)
[2025-02-04 01:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:42][root][INFO] - Training Epoch: 2/2, step 22131/23838 completed (loss: 3.3743624687194824, acc: 0.3492063581943512)
[2025-02-04 01:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:42][root][INFO] - Training Epoch: 2/2, step 22132/23838 completed (loss: 3.4866883754730225, acc: 0.2711864411830902)
[2025-02-04 01:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:42][root][INFO] - Training Epoch: 2/2, step 22133/23838 completed (loss: 2.929769992828369, acc: 0.32499998807907104)
[2025-02-04 01:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:43][root][INFO] - Training Epoch: 2/2, step 22134/23838 completed (loss: 2.680262565612793, acc: 0.5333333611488342)
[2025-02-04 01:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:43][root][INFO] - Training Epoch: 2/2, step 22135/23838 completed (loss: 2.5173323154449463, acc: 0.43589743971824646)
[2025-02-04 01:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:44][root][INFO] - Training Epoch: 2/2, step 22136/23838 completed (loss: 2.39739727973938, acc: 0.4615384638309479)
[2025-02-04 01:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:45][root][INFO] - Training Epoch: 2/2, step 22137/23838 completed (loss: 2.759138345718384, acc: 0.37288135290145874)
[2025-02-04 01:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:45][root][INFO] - Training Epoch: 2/2, step 22138/23838 completed (loss: 3.1388447284698486, acc: 0.32203391194343567)
[2025-02-04 01:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:46][root][INFO] - Training Epoch: 2/2, step 22139/23838 completed (loss: 1.9078401327133179, acc: 0.5185185074806213)
[2025-02-04 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:46][root][INFO] - Training Epoch: 2/2, step 22140/23838 completed (loss: 3.4664125442504883, acc: 0.3947368562221527)
[2025-02-04 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:46][root][INFO] - Training Epoch: 2/2, step 22141/23838 completed (loss: 4.2320475578308105, acc: 0.1764705926179886)
[2025-02-04 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:47][root][INFO] - Training Epoch: 2/2, step 22142/23838 completed (loss: 4.098359107971191, acc: 0.3125)
[2025-02-04 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:47][root][INFO] - Training Epoch: 2/2, step 22143/23838 completed (loss: 4.02582311630249, acc: 0.3684210479259491)
[2025-02-04 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:48][root][INFO] - Training Epoch: 2/2, step 22144/23838 completed (loss: 1.8380556106567383, acc: 0.6000000238418579)
[2025-02-04 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:48][root][INFO] - Training Epoch: 2/2, step 22145/23838 completed (loss: 3.7197678089141846, acc: 0.4000000059604645)
[2025-02-04 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:49][root][INFO] - Training Epoch: 2/2, step 22146/23838 completed (loss: 3.4806113243103027, acc: 0.3499999940395355)
[2025-02-04 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:49][root][INFO] - Training Epoch: 2/2, step 22147/23838 completed (loss: 2.6192781925201416, acc: 0.375)
[2025-02-04 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:49][root][INFO] - Training Epoch: 2/2, step 22148/23838 completed (loss: 3.273359537124634, acc: 0.48275861144065857)
[2025-02-04 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:50][root][INFO] - Training Epoch: 2/2, step 22149/23838 completed (loss: 2.2989118099212646, acc: 0.5)
[2025-02-04 01:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:50][root][INFO] - Training Epoch: 2/2, step 22150/23838 completed (loss: 3.054581880569458, acc: 0.5)
[2025-02-04 01:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:51][root][INFO] - Training Epoch: 2/2, step 22151/23838 completed (loss: 2.9934847354888916, acc: 0.3571428656578064)
[2025-02-04 01:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:51][root][INFO] - Training Epoch: 2/2, step 22152/23838 completed (loss: 2.0839526653289795, acc: 0.5625)
[2025-02-04 01:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:51][root][INFO] - Training Epoch: 2/2, step 22153/23838 completed (loss: 2.4075679779052734, acc: 0.3499999940395355)
[2025-02-04 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:52][root][INFO] - Training Epoch: 2/2, step 22154/23838 completed (loss: 3.327120304107666, acc: 0.38461539149284363)
[2025-02-04 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:52][root][INFO] - Training Epoch: 2/2, step 22155/23838 completed (loss: 1.8446911573410034, acc: 0.5454545617103577)
[2025-02-04 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:53][root][INFO] - Training Epoch: 2/2, step 22156/23838 completed (loss: 1.8219729661941528, acc: 0.5454545617103577)
[2025-02-04 01:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:53][root][INFO] - Training Epoch: 2/2, step 22157/23838 completed (loss: 3.3948891162872314, acc: 0.2142857164144516)
[2025-02-04 01:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:53][root][INFO] - Training Epoch: 2/2, step 22158/23838 completed (loss: 3.520439624786377, acc: 0.34210526943206787)
[2025-02-04 01:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:54][root][INFO] - Training Epoch: 2/2, step 22159/23838 completed (loss: 3.299531936645508, acc: 0.4583333432674408)
[2025-02-04 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:54][root][INFO] - Training Epoch: 2/2, step 22160/23838 completed (loss: 3.276426315307617, acc: 0.3199999928474426)
[2025-02-04 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:55][root][INFO] - Training Epoch: 2/2, step 22161/23838 completed (loss: 3.562495470046997, acc: 0.29411765933036804)
[2025-02-04 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:55][root][INFO] - Training Epoch: 2/2, step 22162/23838 completed (loss: 3.643691301345825, acc: 0.4000000059604645)
[2025-02-04 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:55][root][INFO] - Training Epoch: 2/2, step 22163/23838 completed (loss: 1.9563144445419312, acc: 0.5833333134651184)
[2025-02-04 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:56][root][INFO] - Training Epoch: 2/2, step 22164/23838 completed (loss: 3.549151659011841, acc: 0.260869562625885)
[2025-02-04 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:56][root][INFO] - Training Epoch: 2/2, step 22165/23838 completed (loss: 3.62846302986145, acc: 0.2857142984867096)
[2025-02-04 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:57][root][INFO] - Training Epoch: 2/2, step 22166/23838 completed (loss: 2.839036226272583, acc: 0.375)
[2025-02-04 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:57][root][INFO] - Training Epoch: 2/2, step 22167/23838 completed (loss: 2.6754090785980225, acc: 0.3499999940395355)
[2025-02-04 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:57][root][INFO] - Training Epoch: 2/2, step 22168/23838 completed (loss: 3.041438102722168, acc: 0.3181818127632141)
[2025-02-04 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:58][root][INFO] - Training Epoch: 2/2, step 22169/23838 completed (loss: 2.571146249771118, acc: 0.53125)
[2025-02-04 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:58][root][INFO] - Training Epoch: 2/2, step 22170/23838 completed (loss: 4.529439449310303, acc: 0.2857142984867096)
[2025-02-04 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:59][root][INFO] - Training Epoch: 2/2, step 22171/23838 completed (loss: 3.124530076980591, acc: 0.48275861144065857)
[2025-02-04 01:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:59][root][INFO] - Training Epoch: 2/2, step 22172/23838 completed (loss: 2.9185070991516113, acc: 0.3636363744735718)
[2025-02-04 01:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:04:59][root][INFO] - Training Epoch: 2/2, step 22173/23838 completed (loss: 3.650050401687622, acc: 0.1818181872367859)
[2025-02-04 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:00][root][INFO] - Training Epoch: 2/2, step 22174/23838 completed (loss: 2.597047805786133, acc: 0.4848484992980957)
[2025-02-04 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:00][root][INFO] - Training Epoch: 2/2, step 22175/23838 completed (loss: 3.0184834003448486, acc: 0.3333333432674408)
[2025-02-04 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:01][root][INFO] - Training Epoch: 2/2, step 22176/23838 completed (loss: 2.157191753387451, acc: 0.3571428656578064)
[2025-02-04 01:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:01][root][INFO] - Training Epoch: 2/2, step 22177/23838 completed (loss: 4.104085922241211, acc: 0.27272728085517883)
[2025-02-04 01:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:01][root][INFO] - Training Epoch: 2/2, step 22178/23838 completed (loss: 4.1542439460754395, acc: 0.21621622145175934)
[2025-02-04 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:02][root][INFO] - Training Epoch: 2/2, step 22179/23838 completed (loss: 3.971283197402954, acc: 0.20000000298023224)
[2025-02-04 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:02][root][INFO] - Training Epoch: 2/2, step 22180/23838 completed (loss: 3.3924388885498047, acc: 0.34210526943206787)
[2025-02-04 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:03][root][INFO] - Training Epoch: 2/2, step 22181/23838 completed (loss: 5.328264236450195, acc: 0.23529411852359772)
[2025-02-04 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:03][root][INFO] - Training Epoch: 2/2, step 22182/23838 completed (loss: 4.048547267913818, acc: 0.4166666567325592)
[2025-02-04 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:04][root][INFO] - Training Epoch: 2/2, step 22183/23838 completed (loss: 2.2462520599365234, acc: 0.5555555820465088)
[2025-02-04 01:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:04][root][INFO] - Training Epoch: 2/2, step 22184/23838 completed (loss: 3.410494804382324, acc: 0.3461538553237915)
[2025-02-04 01:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:05][root][INFO] - Training Epoch: 2/2, step 22185/23838 completed (loss: 3.8577992916107178, acc: 0.36000001430511475)
[2025-02-04 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:05][root][INFO] - Training Epoch: 2/2, step 22186/23838 completed (loss: 2.7197413444519043, acc: 0.5555555820465088)
[2025-02-04 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:06][root][INFO] - Training Epoch: 2/2, step 22187/23838 completed (loss: 4.560626029968262, acc: 0.25)
[2025-02-04 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:06][root][INFO] - Training Epoch: 2/2, step 22188/23838 completed (loss: 3.0961880683898926, acc: 0.36666667461395264)
[2025-02-04 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:06][root][INFO] - Training Epoch: 2/2, step 22189/23838 completed (loss: 2.4079344272613525, acc: 0.6551724076271057)
[2025-02-04 01:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:07][root][INFO] - Training Epoch: 2/2, step 22190/23838 completed (loss: 3.316200017929077, acc: 0.4615384638309479)
[2025-02-04 01:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:07][root][INFO] - Training Epoch: 2/2, step 22191/23838 completed (loss: 2.6468453407287598, acc: 0.25)
[2025-02-04 01:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:08][root][INFO] - Training Epoch: 2/2, step 22192/23838 completed (loss: 2.2424862384796143, acc: 0.550000011920929)
[2025-02-04 01:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:08][root][INFO] - Training Epoch: 2/2, step 22193/23838 completed (loss: 1.8754143714904785, acc: 0.5625)
[2025-02-04 01:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:09][root][INFO] - Training Epoch: 2/2, step 22194/23838 completed (loss: 1.591705322265625, acc: 0.5789473652839661)
[2025-02-04 01:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:09][root][INFO] - Training Epoch: 2/2, step 22195/23838 completed (loss: 2.509098529815674, acc: 0.5384615659713745)
[2025-02-04 01:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:09][root][INFO] - Training Epoch: 2/2, step 22196/23838 completed (loss: 1.326846957206726, acc: 0.692307710647583)
[2025-02-04 01:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:10][root][INFO] - Training Epoch: 2/2, step 22197/23838 completed (loss: 3.163522958755493, acc: 0.375)
[2025-02-04 01:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:10][root][INFO] - Training Epoch: 2/2, step 22198/23838 completed (loss: 2.8128106594085693, acc: 0.3333333432674408)
[2025-02-04 01:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:11][root][INFO] - Training Epoch: 2/2, step 22199/23838 completed (loss: 2.058161497116089, acc: 0.6499999761581421)
[2025-02-04 01:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:11][root][INFO] - Training Epoch: 2/2, step 22200/23838 completed (loss: 4.726348400115967, acc: 0.3181818127632141)
[2025-02-04 01:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:12][root][INFO] - Training Epoch: 2/2, step 22201/23838 completed (loss: 3.6986260414123535, acc: 0.40909090638160706)
[2025-02-04 01:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:12][root][INFO] - Training Epoch: 2/2, step 22202/23838 completed (loss: 4.220614910125732, acc: 0.25)
[2025-02-04 01:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:12][root][INFO] - Training Epoch: 2/2, step 22203/23838 completed (loss: 2.634016513824463, acc: 0.5263158082962036)
[2025-02-04 01:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:13][root][INFO] - Training Epoch: 2/2, step 22204/23838 completed (loss: 3.3053953647613525, acc: 0.37931033968925476)
[2025-02-04 01:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:13][root][INFO] - Training Epoch: 2/2, step 22205/23838 completed (loss: 4.163549423217773, acc: 0.5263158082962036)
[2025-02-04 01:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:14][root][INFO] - Training Epoch: 2/2, step 22206/23838 completed (loss: 3.5467660427093506, acc: 0.3125)
[2025-02-04 01:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:14][root][INFO] - Training Epoch: 2/2, step 22207/23838 completed (loss: 2.661865234375, acc: 0.30000001192092896)
[2025-02-04 01:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:15][root][INFO] - Training Epoch: 2/2, step 22208/23838 completed (loss: 3.9564871788024902, acc: 0.38461539149284363)
[2025-02-04 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:15][root][INFO] - Training Epoch: 2/2, step 22209/23838 completed (loss: 4.566694736480713, acc: 0.529411792755127)
[2025-02-04 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:16][root][INFO] - Training Epoch: 2/2, step 22210/23838 completed (loss: 3.4099888801574707, acc: 0.4285714328289032)
[2025-02-04 01:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:16][root][INFO] - Training Epoch: 2/2, step 22211/23838 completed (loss: 3.3266122341156006, acc: 0.2666666805744171)
[2025-02-04 01:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:17][root][INFO] - Training Epoch: 2/2, step 22212/23838 completed (loss: 4.13853120803833, acc: 0.3499999940395355)
[2025-02-04 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:17][root][INFO] - Training Epoch: 2/2, step 22213/23838 completed (loss: 1.744346022605896, acc: 0.5)
[2025-02-04 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:17][root][INFO] - Training Epoch: 2/2, step 22214/23838 completed (loss: 5.081017017364502, acc: 0.2777777910232544)
[2025-02-04 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:18][root][INFO] - Training Epoch: 2/2, step 22215/23838 completed (loss: 2.6490249633789062, acc: 0.47999998927116394)
[2025-02-04 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:18][root][INFO] - Training Epoch: 2/2, step 22216/23838 completed (loss: 4.702825546264648, acc: 0.2222222238779068)
[2025-02-04 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:19][root][INFO] - Training Epoch: 2/2, step 22217/23838 completed (loss: 2.1248867511749268, acc: 0.42105263471603394)
[2025-02-04 01:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:19][root][INFO] - Training Epoch: 2/2, step 22218/23838 completed (loss: 2.110248327255249, acc: 0.3888888955116272)
[2025-02-04 01:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:19][root][INFO] - Training Epoch: 2/2, step 22219/23838 completed (loss: 2.055569648742676, acc: 0.4000000059604645)
[2025-02-04 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:20][root][INFO] - Training Epoch: 2/2, step 22220/23838 completed (loss: 2.738987684249878, acc: 0.5263158082962036)
[2025-02-04 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:20][root][INFO] - Training Epoch: 2/2, step 22221/23838 completed (loss: 4.371048450469971, acc: 0.37037035822868347)
[2025-02-04 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:21][root][INFO] - Training Epoch: 2/2, step 22222/23838 completed (loss: 3.7902495861053467, acc: 0.4482758641242981)
[2025-02-04 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:21][root][INFO] - Training Epoch: 2/2, step 22223/23838 completed (loss: 2.6989331245422363, acc: 0.523809552192688)
[2025-02-04 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:21][root][INFO] - Training Epoch: 2/2, step 22224/23838 completed (loss: 2.767106533050537, acc: 0.38461539149284363)
[2025-02-04 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:22][root][INFO] - Training Epoch: 2/2, step 22225/23838 completed (loss: 1.9185094833374023, acc: 0.5714285969734192)
[2025-02-04 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:22][root][INFO] - Training Epoch: 2/2, step 22226/23838 completed (loss: 1.4315145015716553, acc: 0.5714285969734192)
[2025-02-04 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:23][root][INFO] - Training Epoch: 2/2, step 22227/23838 completed (loss: 3.474334478378296, acc: 0.3181818127632141)
[2025-02-04 01:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:23][root][INFO] - Training Epoch: 2/2, step 22228/23838 completed (loss: 3.1517534255981445, acc: 0.3529411852359772)
[2025-02-04 01:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:24][root][INFO] - Training Epoch: 2/2, step 22229/23838 completed (loss: 2.2669882774353027, acc: 0.47058823704719543)
[2025-02-04 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:24][root][INFO] - Training Epoch: 2/2, step 22230/23838 completed (loss: 1.953633427619934, acc: 0.4545454680919647)
[2025-02-04 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:24][root][INFO] - Training Epoch: 2/2, step 22231/23838 completed (loss: 1.526687741279602, acc: 0.6666666865348816)
[2025-02-04 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:25][root][INFO] - Training Epoch: 2/2, step 22232/23838 completed (loss: 2.4423117637634277, acc: 0.5)
[2025-02-04 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:25][root][INFO] - Training Epoch: 2/2, step 22233/23838 completed (loss: 0.7988613843917847, acc: 0.800000011920929)
[2025-02-04 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:26][root][INFO] - Training Epoch: 2/2, step 22234/23838 completed (loss: 1.6581029891967773, acc: 0.5833333134651184)
[2025-02-04 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:26][root][INFO] - Training Epoch: 2/2, step 22235/23838 completed (loss: 0.24936476349830627, acc: 1.0)
[2025-02-04 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:27][root][INFO] - Training Epoch: 2/2, step 22236/23838 completed (loss: 2.039870262145996, acc: 0.5454545617103577)
[2025-02-04 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:27][root][INFO] - Training Epoch: 2/2, step 22237/23838 completed (loss: 1.161690354347229, acc: 0.800000011920929)
[2025-02-04 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:27][root][INFO] - Training Epoch: 2/2, step 22238/23838 completed (loss: 2.4559574127197266, acc: 0.5714285969734192)
[2025-02-04 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:28][root][INFO] - Training Epoch: 2/2, step 22239/23838 completed (loss: 2.592484951019287, acc: 0.5384615659713745)
[2025-02-04 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:28][root][INFO] - Training Epoch: 2/2, step 22240/23838 completed (loss: 3.77055287361145, acc: 0.3333333432674408)
[2025-02-04 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:29][root][INFO] - Training Epoch: 2/2, step 22241/23838 completed (loss: 4.470880031585693, acc: 0.38461539149284363)
[2025-02-04 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:29][root][INFO] - Training Epoch: 2/2, step 22242/23838 completed (loss: 3.6218626499176025, acc: 0.3076923191547394)
[2025-02-04 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:29][root][INFO] - Training Epoch: 2/2, step 22243/23838 completed (loss: 4.596391677856445, acc: 0.3333333432674408)
[2025-02-04 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:30][root][INFO] - Training Epoch: 2/2, step 22244/23838 completed (loss: 1.1175813674926758, acc: 0.5555555820465088)
[2025-02-04 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:30][root][INFO] - Training Epoch: 2/2, step 22245/23838 completed (loss: 2.281944513320923, acc: 0.4444444477558136)
[2025-02-04 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:31][root][INFO] - Training Epoch: 2/2, step 22246/23838 completed (loss: 2.245149850845337, acc: 0.5)
[2025-02-04 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:31][root][INFO] - Training Epoch: 2/2, step 22247/23838 completed (loss: 1.8931972980499268, acc: 0.6666666865348816)
[2025-02-04 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:32][root][INFO] - Training Epoch: 2/2, step 22248/23838 completed (loss: 2.3479044437408447, acc: 0.5)
[2025-02-04 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:32][root][INFO] - Training Epoch: 2/2, step 22249/23838 completed (loss: 3.086164712905884, acc: 0.5384615659713745)
[2025-02-04 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:32][root][INFO] - Training Epoch: 2/2, step 22250/23838 completed (loss: 2.351369857788086, acc: 0.5714285969734192)
[2025-02-04 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:33][root][INFO] - Training Epoch: 2/2, step 22251/23838 completed (loss: 1.6367703676223755, acc: 0.6666666865348816)
[2025-02-04 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:33][root][INFO] - Training Epoch: 2/2, step 22252/23838 completed (loss: 2.1493091583251953, acc: 0.6000000238418579)
[2025-02-04 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:34][root][INFO] - Training Epoch: 2/2, step 22253/23838 completed (loss: 2.503596305847168, acc: 0.5454545617103577)
[2025-02-04 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:34][root][INFO] - Training Epoch: 2/2, step 22254/23838 completed (loss: 2.6937930583953857, acc: 0.6153846383094788)
[2025-02-04 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:35][root][INFO] - Training Epoch: 2/2, step 22255/23838 completed (loss: 5.274576187133789, acc: 0.2666666805744171)
[2025-02-04 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:35][root][INFO] - Training Epoch: 2/2, step 22256/23838 completed (loss: 6.599691867828369, acc: 0.25)
[2025-02-04 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:36][root][INFO] - Training Epoch: 2/2, step 22257/23838 completed (loss: 3.126159191131592, acc: 0.5)
[2025-02-04 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:36][root][INFO] - Training Epoch: 2/2, step 22258/23838 completed (loss: 4.5334367752075195, acc: 0.3499999940395355)
[2025-02-04 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:36][root][INFO] - Training Epoch: 2/2, step 22259/23838 completed (loss: 2.4237818717956543, acc: 0.625)
[2025-02-04 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:37][root][INFO] - Training Epoch: 2/2, step 22260/23838 completed (loss: 2.788137197494507, acc: 0.5333333611488342)
[2025-02-04 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:37][root][INFO] - Training Epoch: 2/2, step 22261/23838 completed (loss: 4.058342456817627, acc: 0.5)
[2025-02-04 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:38][root][INFO] - Training Epoch: 2/2, step 22262/23838 completed (loss: 2.482205629348755, acc: 0.5833333134651184)
[2025-02-04 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:38][root][INFO] - Training Epoch: 2/2, step 22263/23838 completed (loss: 2.3632116317749023, acc: 0.692307710647583)
[2025-02-04 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:38][root][INFO] - Training Epoch: 2/2, step 22264/23838 completed (loss: 3.5605783462524414, acc: 0.46666666865348816)
[2025-02-04 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:39][root][INFO] - Training Epoch: 2/2, step 22265/23838 completed (loss: 3.5924692153930664, acc: 0.4000000059604645)
[2025-02-04 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:39][root][INFO] - Training Epoch: 2/2, step 22266/23838 completed (loss: 2.8850769996643066, acc: 0.4375)
[2025-02-04 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:40][root][INFO] - Training Epoch: 2/2, step 22267/23838 completed (loss: 3.387317419052124, acc: 0.4545454680919647)
[2025-02-04 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:40][root][INFO] - Training Epoch: 2/2, step 22268/23838 completed (loss: 1.355439305305481, acc: 0.6666666865348816)
[2025-02-04 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:41][root][INFO] - Training Epoch: 2/2, step 22269/23838 completed (loss: 1.7844208478927612, acc: 0.5833333134651184)
[2025-02-04 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:41][root][INFO] - Training Epoch: 2/2, step 22270/23838 completed (loss: 2.848036050796509, acc: 0.5833333134651184)
[2025-02-04 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:41][root][INFO] - Training Epoch: 2/2, step 22271/23838 completed (loss: 2.3550033569335938, acc: 0.6000000238418579)
[2025-02-04 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:42][root][INFO] - Training Epoch: 2/2, step 22272/23838 completed (loss: 3.5875954627990723, acc: 0.625)
[2025-02-04 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:42][root][INFO] - Training Epoch: 2/2, step 22273/23838 completed (loss: 0.5738088488578796, acc: 0.8333333134651184)
[2025-02-04 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:43][root][INFO] - Training Epoch: 2/2, step 22274/23838 completed (loss: 3.3410136699676514, acc: 0.3571428656578064)
[2025-02-04 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:43][root][INFO] - Training Epoch: 2/2, step 22275/23838 completed (loss: 3.758934736251831, acc: 0.4545454680919647)
[2025-02-04 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:43][root][INFO] - Training Epoch: 2/2, step 22276/23838 completed (loss: 2.8923614025115967, acc: 0.5333333611488342)
[2025-02-04 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:44][root][INFO] - Training Epoch: 2/2, step 22277/23838 completed (loss: 1.8153057098388672, acc: 0.46666666865348816)
[2025-02-04 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:44][root][INFO] - Training Epoch: 2/2, step 22278/23838 completed (loss: 2.738576889038086, acc: 0.5)
[2025-02-04 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:45][root][INFO] - Training Epoch: 2/2, step 22279/23838 completed (loss: 3.7369587421417236, acc: 0.25)
[2025-02-04 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:45][root][INFO] - Training Epoch: 2/2, step 22280/23838 completed (loss: 1.8402085304260254, acc: 0.6666666865348816)
[2025-02-04 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:46][root][INFO] - Training Epoch: 2/2, step 22281/23838 completed (loss: 3.4033825397491455, acc: 0.4736842215061188)
[2025-02-04 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:46][root][INFO] - Training Epoch: 2/2, step 22282/23838 completed (loss: 4.329062461853027, acc: 0.2857142984867096)
[2025-02-04 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:46][root][INFO] - Training Epoch: 2/2, step 22283/23838 completed (loss: 1.9887293577194214, acc: 0.5)
[2025-02-04 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:47][root][INFO] - Training Epoch: 2/2, step 22284/23838 completed (loss: 2.2837281227111816, acc: 0.5454545617103577)
[2025-02-04 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:47][root][INFO] - Training Epoch: 2/2, step 22285/23838 completed (loss: 1.1453346014022827, acc: 0.7692307829856873)
[2025-02-04 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:48][root][INFO] - Training Epoch: 2/2, step 22286/23838 completed (loss: 2.882643938064575, acc: 0.4000000059604645)
[2025-02-04 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:48][root][INFO] - Training Epoch: 2/2, step 22287/23838 completed (loss: 4.882778167724609, acc: 0.31578946113586426)
[2025-02-04 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:49][root][INFO] - Training Epoch: 2/2, step 22288/23838 completed (loss: 2.4342503547668457, acc: 0.5)
[2025-02-04 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:49][root][INFO] - Training Epoch: 2/2, step 22289/23838 completed (loss: 3.3752591609954834, acc: 0.4117647111415863)
[2025-02-04 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:49][root][INFO] - Training Epoch: 2/2, step 22290/23838 completed (loss: 3.834108591079712, acc: 0.47058823704719543)
[2025-02-04 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:50][root][INFO] - Training Epoch: 2/2, step 22291/23838 completed (loss: 5.131929397583008, acc: 0.2666666805744171)
[2025-02-04 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:50][root][INFO] - Training Epoch: 2/2, step 22292/23838 completed (loss: 4.119863033294678, acc: 0.2222222238779068)
[2025-02-04 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:51][root][INFO] - Training Epoch: 2/2, step 22293/23838 completed (loss: 3.4309258460998535, acc: 0.3636363744735718)
[2025-02-04 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:51][root][INFO] - Training Epoch: 2/2, step 22294/23838 completed (loss: 3.2059085369110107, acc: 0.47058823704719543)
[2025-02-04 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:51][root][INFO] - Training Epoch: 2/2, step 22295/23838 completed (loss: 6.244991779327393, acc: 0.3333333432674408)
[2025-02-04 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:52][root][INFO] - Training Epoch: 2/2, step 22296/23838 completed (loss: 2.6377248764038086, acc: 0.6111111044883728)
[2025-02-04 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:52][root][INFO] - Training Epoch: 2/2, step 22297/23838 completed (loss: 3.752811908721924, acc: 0.38461539149284363)
[2025-02-04 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:53][root][INFO] - Training Epoch: 2/2, step 22298/23838 completed (loss: 2.1312108039855957, acc: 0.5833333134651184)
[2025-02-04 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:53][root][INFO] - Training Epoch: 2/2, step 22299/23838 completed (loss: 2.6002275943756104, acc: 0.46666666865348816)
[2025-02-04 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:53][root][INFO] - Training Epoch: 2/2, step 22300/23838 completed (loss: 3.347555160522461, acc: 0.4117647111415863)
[2025-02-04 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:54][root][INFO] - Training Epoch: 2/2, step 22301/23838 completed (loss: 2.0825459957122803, acc: 0.6000000238418579)
[2025-02-04 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:54][root][INFO] - Training Epoch: 2/2, step 22302/23838 completed (loss: 4.541636943817139, acc: 0.38461539149284363)
[2025-02-04 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:55][root][INFO] - Training Epoch: 2/2, step 22303/23838 completed (loss: 3.1387319564819336, acc: 0.3076923191547394)
[2025-02-04 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:55][root][INFO] - Training Epoch: 2/2, step 22304/23838 completed (loss: 3.151545524597168, acc: 0.3636363744735718)
[2025-02-04 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:56][root][INFO] - Training Epoch: 2/2, step 22305/23838 completed (loss: 2.2317798137664795, acc: 0.625)
[2025-02-04 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:56][root][INFO] - Training Epoch: 2/2, step 22306/23838 completed (loss: 2.90251088142395, acc: 0.5263158082962036)
[2025-02-04 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:56][root][INFO] - Training Epoch: 2/2, step 22307/23838 completed (loss: 1.8170554637908936, acc: 0.6666666865348816)
[2025-02-04 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:57][root][INFO] - Training Epoch: 2/2, step 22308/23838 completed (loss: 4.000537872314453, acc: 0.2666666805744171)
[2025-02-04 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:57][root][INFO] - Training Epoch: 2/2, step 22309/23838 completed (loss: 2.434986114501953, acc: 0.550000011920929)
[2025-02-04 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:58][root][INFO] - Training Epoch: 2/2, step 22310/23838 completed (loss: 3.7740771770477295, acc: 0.21052631735801697)
[2025-02-04 01:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:58][root][INFO] - Training Epoch: 2/2, step 22311/23838 completed (loss: 3.6844706535339355, acc: 0.3469387888908386)
[2025-02-04 01:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:59][root][INFO] - Training Epoch: 2/2, step 22312/23838 completed (loss: 3.757415533065796, acc: 0.3695652186870575)
[2025-02-04 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:59][root][INFO] - Training Epoch: 2/2, step 22313/23838 completed (loss: 4.409506797790527, acc: 0.2549019753932953)
[2025-02-04 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:05:59][root][INFO] - Training Epoch: 2/2, step 22314/23838 completed (loss: 2.9120168685913086, acc: 0.3888888955116272)
[2025-02-04 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:00][root][INFO] - Training Epoch: 2/2, step 22315/23838 completed (loss: 3.0802929401397705, acc: 0.37037035822868347)
[2025-02-04 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:00][root][INFO] - Training Epoch: 2/2, step 22316/23838 completed (loss: 3.3224425315856934, acc: 0.3863636255264282)
[2025-02-04 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:01][root][INFO] - Training Epoch: 2/2, step 22317/23838 completed (loss: 3.363199472427368, acc: 0.32692307233810425)
[2025-02-04 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:01][root][INFO] - Training Epoch: 2/2, step 22318/23838 completed (loss: 3.176255226135254, acc: 0.3636363744735718)
[2025-02-04 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:02][root][INFO] - Training Epoch: 2/2, step 22319/23838 completed (loss: 3.3132386207580566, acc: 0.290909081697464)
[2025-02-04 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:02][root][INFO] - Training Epoch: 2/2, step 22320/23838 completed (loss: 3.0323524475097656, acc: 0.4318181872367859)
[2025-02-04 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:02][root][INFO] - Training Epoch: 2/2, step 22321/23838 completed (loss: 3.504042625427246, acc: 0.3636363744735718)
[2025-02-04 01:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:03][root][INFO] - Training Epoch: 2/2, step 22322/23838 completed (loss: 4.078735828399658, acc: 0.3255814015865326)
[2025-02-04 01:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:03][root][INFO] - Training Epoch: 2/2, step 22323/23838 completed (loss: 3.9519119262695312, acc: 0.3035714328289032)
[2025-02-04 01:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:04][root][INFO] - Training Epoch: 2/2, step 22324/23838 completed (loss: 3.8715288639068604, acc: 0.302325576543808)
[2025-02-04 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:04][root][INFO] - Training Epoch: 2/2, step 22325/23838 completed (loss: 3.377413749694824, acc: 0.4375)
[2025-02-04 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:04][root][INFO] - Training Epoch: 2/2, step 22326/23838 completed (loss: 3.3571367263793945, acc: 0.3589743673801422)
[2025-02-04 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:05][root][INFO] - Training Epoch: 2/2, step 22327/23838 completed (loss: 3.9533700942993164, acc: 0.3030303120613098)
[2025-02-04 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:05][root][INFO] - Training Epoch: 2/2, step 22328/23838 completed (loss: 3.5949599742889404, acc: 0.3400000035762787)
[2025-02-04 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:06][root][INFO] - Training Epoch: 2/2, step 22329/23838 completed (loss: 2.4709932804107666, acc: 0.5161290168762207)
[2025-02-04 01:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:06][root][INFO] - Training Epoch: 2/2, step 22330/23838 completed (loss: 4.2143425941467285, acc: 0.28125)
[2025-02-04 01:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:07][root][INFO] - Training Epoch: 2/2, step 22331/23838 completed (loss: 3.9544312953948975, acc: 0.23255814611911774)
[2025-02-04 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:07][root][INFO] - Training Epoch: 2/2, step 22332/23838 completed (loss: 2.86415433883667, acc: 0.43478259444236755)
[2025-02-04 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:08][root][INFO] - Training Epoch: 2/2, step 22333/23838 completed (loss: 3.286998987197876, acc: 0.2647058963775635)
[2025-02-04 01:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:08][root][INFO] - Training Epoch: 2/2, step 22334/23838 completed (loss: 4.19492244720459, acc: 0.3076923191547394)
[2025-02-04 01:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:09][root][INFO] - Training Epoch: 2/2, step 22335/23838 completed (loss: 3.086571216583252, acc: 0.4545454680919647)
[2025-02-04 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:09][root][INFO] - Training Epoch: 2/2, step 22336/23838 completed (loss: 2.760598659515381, acc: 0.31578946113586426)
[2025-02-04 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:09][root][INFO] - Training Epoch: 2/2, step 22337/23838 completed (loss: 3.3245532512664795, acc: 0.41025641560554504)
[2025-02-04 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:10][root][INFO] - Training Epoch: 2/2, step 22338/23838 completed (loss: 3.2981133460998535, acc: 0.47826087474823)
[2025-02-04 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:10][root][INFO] - Training Epoch: 2/2, step 22339/23838 completed (loss: 3.200828790664673, acc: 0.3076923191547394)
[2025-02-04 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:11][root][INFO] - Training Epoch: 2/2, step 22340/23838 completed (loss: 2.394365072250366, acc: 0.4864864945411682)
[2025-02-04 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:11][root][INFO] - Training Epoch: 2/2, step 22341/23838 completed (loss: 2.6893327236175537, acc: 0.41025641560554504)
[2025-02-04 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:12][root][INFO] - Training Epoch: 2/2, step 22342/23838 completed (loss: 3.0499706268310547, acc: 0.3513513505458832)
[2025-02-04 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:12][root][INFO] - Training Epoch: 2/2, step 22343/23838 completed (loss: 2.1402363777160645, acc: 0.5769230723381042)
[2025-02-04 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:12][root][INFO] - Training Epoch: 2/2, step 22344/23838 completed (loss: 3.5894675254821777, acc: 0.38461539149284363)
[2025-02-04 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:13][root][INFO] - Training Epoch: 2/2, step 22345/23838 completed (loss: 2.9240756034851074, acc: 0.4038461446762085)
[2025-02-04 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:13][root][INFO] - Training Epoch: 2/2, step 22346/23838 completed (loss: 3.1273648738861084, acc: 0.30158731341362)
[2025-02-04 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:14][root][INFO] - Training Epoch: 2/2, step 22347/23838 completed (loss: 2.353444814682007, acc: 0.4878048896789551)
[2025-02-04 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:14][root][INFO] - Training Epoch: 2/2, step 22348/23838 completed (loss: 2.713879108428955, acc: 0.4324324429035187)
[2025-02-04 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:15][root][INFO] - Training Epoch: 2/2, step 22349/23838 completed (loss: 3.4172189235687256, acc: 0.3142857253551483)
[2025-02-04 01:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:15][root][INFO] - Training Epoch: 2/2, step 22350/23838 completed (loss: 2.1645965576171875, acc: 0.48571428656578064)
[2025-02-04 01:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:15][root][INFO] - Training Epoch: 2/2, step 22351/23838 completed (loss: 3.5106964111328125, acc: 0.3404255211353302)
[2025-02-04 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:16][root][INFO] - Training Epoch: 2/2, step 22352/23838 completed (loss: 2.019291400909424, acc: 0.5416666865348816)
[2025-02-04 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:16][root][INFO] - Training Epoch: 2/2, step 22353/23838 completed (loss: 2.1344120502471924, acc: 0.5)
[2025-02-04 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:17][root][INFO] - Training Epoch: 2/2, step 22354/23838 completed (loss: 3.134007692337036, acc: 0.3478260934352875)
[2025-02-04 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:17][root][INFO] - Training Epoch: 2/2, step 22355/23838 completed (loss: 3.6725642681121826, acc: 0.3499999940395355)
[2025-02-04 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:18][root][INFO] - Training Epoch: 2/2, step 22356/23838 completed (loss: 3.464482069015503, acc: 0.35483869910240173)
[2025-02-04 01:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:18][root][INFO] - Training Epoch: 2/2, step 22357/23838 completed (loss: 3.493168592453003, acc: 0.375)
[2025-02-04 01:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:18][root][INFO] - Training Epoch: 2/2, step 22358/23838 completed (loss: 2.808461904525757, acc: 0.36000001430511475)
[2025-02-04 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:19][root][INFO] - Training Epoch: 2/2, step 22359/23838 completed (loss: 2.765960931777954, acc: 0.5263158082962036)
[2025-02-04 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:19][root][INFO] - Training Epoch: 2/2, step 22360/23838 completed (loss: 3.6513359546661377, acc: 0.2857142984867096)
[2025-02-04 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:20][root][INFO] - Training Epoch: 2/2, step 22361/23838 completed (loss: 4.394050121307373, acc: 0.3478260934352875)
[2025-02-04 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:20][root][INFO] - Training Epoch: 2/2, step 22362/23838 completed (loss: 2.763164520263672, acc: 0.29411765933036804)
[2025-02-04 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:21][root][INFO] - Training Epoch: 2/2, step 22363/23838 completed (loss: 2.313542127609253, acc: 0.4375)
[2025-02-04 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:21][root][INFO] - Training Epoch: 2/2, step 22364/23838 completed (loss: 3.0749526023864746, acc: 0.5)
[2025-02-04 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:22][root][INFO] - Training Epoch: 2/2, step 22365/23838 completed (loss: 3.7990946769714355, acc: 0.3461538553237915)
[2025-02-04 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:22][root][INFO] - Training Epoch: 2/2, step 22366/23838 completed (loss: 3.3540539741516113, acc: 0.4000000059604645)
[2025-02-04 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:23][root][INFO] - Training Epoch: 2/2, step 22367/23838 completed (loss: 2.5553417205810547, acc: 0.5789473652839661)
[2025-02-04 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:23][root][INFO] - Training Epoch: 2/2, step 22368/23838 completed (loss: 3.3325703144073486, acc: 0.4444444477558136)
[2025-02-04 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:23][root][INFO] - Training Epoch: 2/2, step 22369/23838 completed (loss: 3.2187845706939697, acc: 0.4761904776096344)
[2025-02-04 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:24][root][INFO] - Training Epoch: 2/2, step 22370/23838 completed (loss: 2.421950101852417, acc: 0.380952388048172)
[2025-02-04 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:24][root][INFO] - Training Epoch: 2/2, step 22371/23838 completed (loss: 1.2654529809951782, acc: 0.7333333492279053)
[2025-02-04 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:25][root][INFO] - Training Epoch: 2/2, step 22372/23838 completed (loss: 2.698700428009033, acc: 0.43478259444236755)
[2025-02-04 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:25][root][INFO] - Training Epoch: 2/2, step 22373/23838 completed (loss: 4.405811309814453, acc: 0.380952388048172)
[2025-02-04 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:26][root][INFO] - Training Epoch: 2/2, step 22374/23838 completed (loss: 3.197183847427368, acc: 0.380952388048172)
[2025-02-04 01:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:26][root][INFO] - Training Epoch: 2/2, step 22375/23838 completed (loss: 2.974043130874634, acc: 0.4444444477558136)
[2025-02-04 01:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:26][root][INFO] - Training Epoch: 2/2, step 22376/23838 completed (loss: 1.635456919670105, acc: 0.5)
[2025-02-04 01:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:27][root][INFO] - Training Epoch: 2/2, step 22377/23838 completed (loss: 2.5769593715667725, acc: 0.5)
[2025-02-04 01:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:27][root][INFO] - Training Epoch: 2/2, step 22378/23838 completed (loss: 3.1056277751922607, acc: 0.4838709533214569)
[2025-02-04 01:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:28][root][INFO] - Training Epoch: 2/2, step 22379/23838 completed (loss: 3.0192627906799316, acc: 0.30000001192092896)
[2025-02-04 01:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:28][root][INFO] - Training Epoch: 2/2, step 22380/23838 completed (loss: 3.210364818572998, acc: 0.4137931168079376)
[2025-02-04 01:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:29][root][INFO] - Training Epoch: 2/2, step 22381/23838 completed (loss: 3.8814969062805176, acc: 0.3125)
[2025-02-04 01:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:29][root][INFO] - Training Epoch: 2/2, step 22382/23838 completed (loss: 3.6038897037506104, acc: 0.2800000011920929)
[2025-02-04 01:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:29][root][INFO] - Training Epoch: 2/2, step 22383/23838 completed (loss: 3.6668787002563477, acc: 0.3333333432674408)
[2025-02-04 01:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:30][root][INFO] - Training Epoch: 2/2, step 22384/23838 completed (loss: 2.659198045730591, acc: 0.3928571343421936)
[2025-02-04 01:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:30][root][INFO] - Training Epoch: 2/2, step 22385/23838 completed (loss: 2.908684253692627, acc: 0.4615384638309479)
[2025-02-04 01:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:31][root][INFO] - Training Epoch: 2/2, step 22386/23838 completed (loss: 3.475170135498047, acc: 0.3928571343421936)
[2025-02-04 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:31][root][INFO] - Training Epoch: 2/2, step 22387/23838 completed (loss: 3.6693742275238037, acc: 0.3888888955116272)
[2025-02-04 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:31][root][INFO] - Training Epoch: 2/2, step 22388/23838 completed (loss: 2.7879037857055664, acc: 0.375)
[2025-02-04 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:32][root][INFO] - Training Epoch: 2/2, step 22389/23838 completed (loss: 1.9904530048370361, acc: 0.5714285969734192)
[2025-02-04 01:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:32][root][INFO] - Training Epoch: 2/2, step 22390/23838 completed (loss: 3.7149267196655273, acc: 0.25925925374031067)
[2025-02-04 01:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:33][root][INFO] - Training Epoch: 2/2, step 22391/23838 completed (loss: 2.775446653366089, acc: 0.3333333432674408)
[2025-02-04 01:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:33][root][INFO] - Training Epoch: 2/2, step 22392/23838 completed (loss: 2.5383431911468506, acc: 0.4736842215061188)
[2025-02-04 01:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:33][root][INFO] - Training Epoch: 2/2, step 22393/23838 completed (loss: 2.6155753135681152, acc: 0.5)
[2025-02-04 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:34][root][INFO] - Training Epoch: 2/2, step 22394/23838 completed (loss: 2.415776252746582, acc: 0.4736842215061188)
[2025-02-04 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:34][root][INFO] - Training Epoch: 2/2, step 22395/23838 completed (loss: 3.3898887634277344, acc: 0.19230769574642181)
[2025-02-04 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:35][root][INFO] - Training Epoch: 2/2, step 22396/23838 completed (loss: 3.3077783584594727, acc: 0.3333333432674408)
[2025-02-04 01:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:35][root][INFO] - Training Epoch: 2/2, step 22397/23838 completed (loss: 2.526939630508423, acc: 0.4285714328289032)
[2025-02-04 01:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:36][root][INFO] - Training Epoch: 2/2, step 22398/23838 completed (loss: 3.3888607025146484, acc: 0.3333333432674408)
[2025-02-04 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:36][root][INFO] - Training Epoch: 2/2, step 22399/23838 completed (loss: 2.7997591495513916, acc: 0.375)
[2025-02-04 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:37][root][INFO] - Training Epoch: 2/2, step 22400/23838 completed (loss: 3.4607343673706055, acc: 0.25925925374031067)
[2025-02-04 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:37][root][INFO] - Training Epoch: 2/2, step 22401/23838 completed (loss: 3.367647409439087, acc: 0.35483869910240173)
[2025-02-04 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:38][root][INFO] - Training Epoch: 2/2, step 22402/23838 completed (loss: 2.670069694519043, acc: 0.52173912525177)
[2025-02-04 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:38][root][INFO] - Training Epoch: 2/2, step 22403/23838 completed (loss: 2.3484692573547363, acc: 0.5)
[2025-02-04 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:38][root][INFO] - Training Epoch: 2/2, step 22404/23838 completed (loss: 3.296964406967163, acc: 0.3529411852359772)
[2025-02-04 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:39][root][INFO] - Training Epoch: 2/2, step 22405/23838 completed (loss: 2.704317092895508, acc: 0.4444444477558136)
[2025-02-04 01:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:39][root][INFO] - Training Epoch: 2/2, step 22406/23838 completed (loss: 3.1355438232421875, acc: 0.3478260934352875)
[2025-02-04 01:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:40][root][INFO] - Training Epoch: 2/2, step 22407/23838 completed (loss: 3.2904140949249268, acc: 0.3499999940395355)
[2025-02-04 01:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:40][root][INFO] - Training Epoch: 2/2, step 22408/23838 completed (loss: 2.123302936553955, acc: 0.5789473652839661)
[2025-02-04 01:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:40][root][INFO] - Training Epoch: 2/2, step 22409/23838 completed (loss: 2.4968273639678955, acc: 0.4285714328289032)
[2025-02-04 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:41][root][INFO] - Training Epoch: 2/2, step 22410/23838 completed (loss: 1.431484341621399, acc: 0.5714285969734192)
[2025-02-04 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:41][root][INFO] - Training Epoch: 2/2, step 22411/23838 completed (loss: 3.0794548988342285, acc: 0.375)
[2025-02-04 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:42][root][INFO] - Training Epoch: 2/2, step 22412/23838 completed (loss: 2.718005418777466, acc: 0.2800000011920929)
[2025-02-04 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:42][root][INFO] - Training Epoch: 2/2, step 22413/23838 completed (loss: 3.758894443511963, acc: 0.30434781312942505)
[2025-02-04 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:42][root][INFO] - Training Epoch: 2/2, step 22414/23838 completed (loss: 3.581238269805908, acc: 0.3076923191547394)
[2025-02-04 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:43][root][INFO] - Training Epoch: 2/2, step 22415/23838 completed (loss: 2.533336639404297, acc: 0.4444444477558136)
[2025-02-04 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:43][root][INFO] - Training Epoch: 2/2, step 22416/23838 completed (loss: 4.091407299041748, acc: 0.4000000059604645)
[2025-02-04 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:44][root][INFO] - Training Epoch: 2/2, step 22417/23838 completed (loss: 6.121712684631348, acc: 0.1666666716337204)
[2025-02-04 01:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:44][root][INFO] - Training Epoch: 2/2, step 22418/23838 completed (loss: 4.8217363357543945, acc: 0.23529411852359772)
[2025-02-04 01:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:45][root][INFO] - Training Epoch: 2/2, step 22419/23838 completed (loss: 4.293874740600586, acc: 0.25925925374031067)
[2025-02-04 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:45][root][INFO] - Training Epoch: 2/2, step 22420/23838 completed (loss: 5.99117374420166, acc: 0.23076923191547394)
[2025-02-04 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:45][root][INFO] - Training Epoch: 2/2, step 22421/23838 completed (loss: 4.129732608795166, acc: 0.23999999463558197)
[2025-02-04 01:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:46][root][INFO] - Training Epoch: 2/2, step 22422/23838 completed (loss: 4.306593894958496, acc: 0.20000000298023224)
[2025-02-04 01:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:46][root][INFO] - Training Epoch: 2/2, step 22423/23838 completed (loss: 4.395745277404785, acc: 0.260869562625885)
[2025-02-04 01:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:47][root][INFO] - Training Epoch: 2/2, step 22424/23838 completed (loss: 4.854101181030273, acc: 0.10526315867900848)
[2025-02-04 01:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:47][root][INFO] - Training Epoch: 2/2, step 22425/23838 completed (loss: 4.424092769622803, acc: 0.3529411852359772)
[2025-02-04 01:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:48][root][INFO] - Training Epoch: 2/2, step 22426/23838 completed (loss: 5.050360679626465, acc: 0.260869562625885)
[2025-02-04 01:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:48][root][INFO] - Training Epoch: 2/2, step 22427/23838 completed (loss: 5.5857133865356445, acc: 0.27272728085517883)
[2025-02-04 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:49][root][INFO] - Training Epoch: 2/2, step 22428/23838 completed (loss: 2.9634571075439453, acc: 0.3571428656578064)
[2025-02-04 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:49][root][INFO] - Training Epoch: 2/2, step 22429/23838 completed (loss: 5.82581901550293, acc: 0.19565217196941376)
[2025-02-04 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:50][root][INFO] - Training Epoch: 2/2, step 22430/23838 completed (loss: 5.430019855499268, acc: 0.1794871836900711)
[2025-02-04 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:50][root][INFO] - Training Epoch: 2/2, step 22431/23838 completed (loss: 4.230473041534424, acc: 0.2857142984867096)
[2025-02-04 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:51][root][INFO] - Training Epoch: 2/2, step 22432/23838 completed (loss: 3.926276922225952, acc: 0.3636363744735718)
[2025-02-04 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:51][root][INFO] - Training Epoch: 2/2, step 22433/23838 completed (loss: 1.4236218929290771, acc: 0.800000011920929)
[2025-02-04 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:51][root][INFO] - Training Epoch: 2/2, step 22434/23838 completed (loss: 1.9844528436660767, acc: 0.6315789222717285)
[2025-02-04 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:52][root][INFO] - Training Epoch: 2/2, step 22435/23838 completed (loss: 3.8985962867736816, acc: 0.40909090638160706)
[2025-02-04 01:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:52][root][INFO] - Training Epoch: 2/2, step 22436/23838 completed (loss: 3.8571882247924805, acc: 0.2916666567325592)
[2025-02-04 01:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:52][root][INFO] - Training Epoch: 2/2, step 22437/23838 completed (loss: 5.079324722290039, acc: 0.3333333432674408)
[2025-02-04 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:53][root][INFO] - Training Epoch: 2/2, step 22438/23838 completed (loss: 3.563594341278076, acc: 0.3888888955116272)
[2025-02-04 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:53][root][INFO] - Training Epoch: 2/2, step 22439/23838 completed (loss: 1.7877360582351685, acc: 0.7142857313156128)
[2025-02-04 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:54][root][INFO] - Training Epoch: 2/2, step 22440/23838 completed (loss: 4.24445915222168, acc: 0.3636363744735718)
[2025-02-04 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:54][root][INFO] - Training Epoch: 2/2, step 22441/23838 completed (loss: 5.2737016677856445, acc: 0.1666666716337204)
[2025-02-04 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:55][root][INFO] - Training Epoch: 2/2, step 22442/23838 completed (loss: 2.4761202335357666, acc: 0.7272727489471436)
[2025-02-04 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:55][root][INFO] - Training Epoch: 2/2, step 22443/23838 completed (loss: 4.334929466247559, acc: 0.3529411852359772)
[2025-02-04 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:55][root][INFO] - Training Epoch: 2/2, step 22444/23838 completed (loss: 3.2328615188598633, acc: 0.3333333432674408)
[2025-02-04 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:56][root][INFO] - Training Epoch: 2/2, step 22445/23838 completed (loss: 4.917349815368652, acc: 0.10000000149011612)
[2025-02-04 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:56][root][INFO] - Training Epoch: 2/2, step 22446/23838 completed (loss: 3.562737226486206, acc: 0.36000001430511475)
[2025-02-04 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:57][root][INFO] - Training Epoch: 2/2, step 22447/23838 completed (loss: 3.966352701187134, acc: 0.31578946113586426)
[2025-02-04 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:57][root][INFO] - Training Epoch: 2/2, step 22448/23838 completed (loss: 4.04672384262085, acc: 0.380952388048172)
[2025-02-04 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:57][root][INFO] - Training Epoch: 2/2, step 22449/23838 completed (loss: 2.460322380065918, acc: 0.5151515007019043)
[2025-02-04 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:58][root][INFO] - Training Epoch: 2/2, step 22450/23838 completed (loss: 3.4730064868927, acc: 0.3529411852359772)
[2025-02-04 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:58][root][INFO] - Training Epoch: 2/2, step 22451/23838 completed (loss: 2.447798013687134, acc: 0.6363636255264282)
[2025-02-04 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:59][root][INFO] - Training Epoch: 2/2, step 22452/23838 completed (loss: 2.545947551727295, acc: 0.523809552192688)
[2025-02-04 01:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:06:59][root][INFO] - Training Epoch: 2/2, step 22453/23838 completed (loss: 3.4479150772094727, acc: 0.36666667461395264)
[2025-02-04 01:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:00][root][INFO] - Training Epoch: 2/2, step 22454/23838 completed (loss: 5.424187660217285, acc: 0.3571428656578064)
[2025-02-04 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:00][root][INFO] - Training Epoch: 2/2, step 22455/23838 completed (loss: 3.217846393585205, acc: 0.52173912525177)
[2025-02-04 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:00][root][INFO] - Training Epoch: 2/2, step 22456/23838 completed (loss: 2.824768543243408, acc: 0.3181818127632141)
[2025-02-04 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:01][root][INFO] - Training Epoch: 2/2, step 22457/23838 completed (loss: 4.249726295471191, acc: 0.3199999928474426)
[2025-02-04 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:01][root][INFO] - Training Epoch: 2/2, step 22458/23838 completed (loss: 3.2461636066436768, acc: 0.30000001192092896)
[2025-02-04 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:02][root][INFO] - Training Epoch: 2/2, step 22459/23838 completed (loss: 2.5617287158966064, acc: 0.43478259444236755)
[2025-02-04 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:02][root][INFO] - Training Epoch: 2/2, step 22460/23838 completed (loss: 2.4619834423065186, acc: 0.4285714328289032)
[2025-02-04 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:02][root][INFO] - Training Epoch: 2/2, step 22461/23838 completed (loss: 3.7402868270874023, acc: 0.3333333432674408)
[2025-02-04 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:03][root][INFO] - Training Epoch: 2/2, step 22462/23838 completed (loss: 3.1292941570281982, acc: 0.4285714328289032)
[2025-02-04 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:03][root][INFO] - Training Epoch: 2/2, step 22463/23838 completed (loss: 3.2723233699798584, acc: 0.3529411852359772)
[2025-02-04 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:03][root][INFO] - Training Epoch: 2/2, step 22464/23838 completed (loss: 4.290217399597168, acc: 0.2666666805744171)
[2025-02-04 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:04][root][INFO] - Training Epoch: 2/2, step 22465/23838 completed (loss: 4.626724720001221, acc: 0.25)
[2025-02-04 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:04][root][INFO] - Training Epoch: 2/2, step 22466/23838 completed (loss: 3.3661062717437744, acc: 0.29411765933036804)
[2025-02-04 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:05][root][INFO] - Training Epoch: 2/2, step 22467/23838 completed (loss: 2.1146721839904785, acc: 0.5)
[2025-02-04 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:05][root][INFO] - Training Epoch: 2/2, step 22468/23838 completed (loss: 4.084599018096924, acc: 0.375)
[2025-02-04 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:06][root][INFO] - Training Epoch: 2/2, step 22469/23838 completed (loss: 4.027849197387695, acc: 0.29411765933036804)
[2025-02-04 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:06][root][INFO] - Training Epoch: 2/2, step 22470/23838 completed (loss: 5.125253677368164, acc: 0.04545454680919647)
[2025-02-04 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:06][root][INFO] - Training Epoch: 2/2, step 22471/23838 completed (loss: 3.834207057952881, acc: 0.190476194024086)
[2025-02-04 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:07][root][INFO] - Training Epoch: 2/2, step 22472/23838 completed (loss: 2.4362571239471436, acc: 0.5)
[2025-02-04 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:07][root][INFO] - Training Epoch: 2/2, step 22473/23838 completed (loss: 2.7004597187042236, acc: 0.38461539149284363)
[2025-02-04 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:08][root][INFO] - Training Epoch: 2/2, step 22474/23838 completed (loss: 4.08698034286499, acc: 0.3571428656578064)
[2025-02-04 01:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:08][root][INFO] - Training Epoch: 2/2, step 22475/23838 completed (loss: 2.445375680923462, acc: 0.6315789222717285)
[2025-02-04 01:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:08][root][INFO] - Training Epoch: 2/2, step 22476/23838 completed (loss: 2.3350088596343994, acc: 0.7142857313156128)
[2025-02-04 01:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:09][root][INFO] - Training Epoch: 2/2, step 22477/23838 completed (loss: 3.2106986045837402, acc: 0.5)
[2025-02-04 01:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:09][root][INFO] - Training Epoch: 2/2, step 22478/23838 completed (loss: 3.2697293758392334, acc: 0.42105263471603394)
[2025-02-04 01:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:09][root][INFO] - Training Epoch: 2/2, step 22479/23838 completed (loss: 2.8560850620269775, acc: 0.5625)
[2025-02-04 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:10][root][INFO] - Training Epoch: 2/2, step 22480/23838 completed (loss: 2.9514567852020264, acc: 0.4375)
[2025-02-04 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:10][root][INFO] - Training Epoch: 2/2, step 22481/23838 completed (loss: 3.3414788246154785, acc: 0.23529411852359772)
[2025-02-04 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:11][root][INFO] - Training Epoch: 2/2, step 22482/23838 completed (loss: 3.215862512588501, acc: 0.3571428656578064)
[2025-02-04 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:11][root][INFO] - Training Epoch: 2/2, step 22483/23838 completed (loss: 4.366697788238525, acc: 0.23999999463558197)
[2025-02-04 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:11][root][INFO] - Training Epoch: 2/2, step 22484/23838 completed (loss: 2.863539457321167, acc: 0.4444444477558136)
[2025-02-04 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:12][root][INFO] - Training Epoch: 2/2, step 22485/23838 completed (loss: 3.2445690631866455, acc: 0.3333333432674408)
[2025-02-04 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:12][root][INFO] - Training Epoch: 2/2, step 22486/23838 completed (loss: 3.5301711559295654, acc: 0.4000000059604645)
[2025-02-04 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:13][root][INFO] - Training Epoch: 2/2, step 22487/23838 completed (loss: 3.772367238998413, acc: 0.4285714328289032)
[2025-02-04 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:13][root][INFO] - Training Epoch: 2/2, step 22488/23838 completed (loss: 3.2118780612945557, acc: 0.5882353186607361)
[2025-02-04 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:14][root][INFO] - Training Epoch: 2/2, step 22489/23838 completed (loss: 3.826153039932251, acc: 0.43478259444236755)
[2025-02-04 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:14][root][INFO] - Training Epoch: 2/2, step 22490/23838 completed (loss: 3.795292615890503, acc: 0.3499999940395355)
[2025-02-04 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:15][root][INFO] - Training Epoch: 2/2, step 22491/23838 completed (loss: 2.481036424636841, acc: 0.4000000059604645)
[2025-02-04 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:15][root][INFO] - Training Epoch: 2/2, step 22492/23838 completed (loss: 3.114917278289795, acc: 0.47826087474823)
[2025-02-04 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:15][root][INFO] - Training Epoch: 2/2, step 22493/23838 completed (loss: 2.349310874938965, acc: 0.47058823704719543)
[2025-02-04 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:16][root][INFO] - Training Epoch: 2/2, step 22494/23838 completed (loss: 3.24599027633667, acc: 0.4000000059604645)
[2025-02-04 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:16][root][INFO] - Training Epoch: 2/2, step 22495/23838 completed (loss: 2.035674571990967, acc: 0.6428571343421936)
[2025-02-04 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:17][root][INFO] - Training Epoch: 2/2, step 22496/23838 completed (loss: 3.3699443340301514, acc: 0.30000001192092896)
[2025-02-04 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:17][root][INFO] - Training Epoch: 2/2, step 22497/23838 completed (loss: 3.577043294906616, acc: 0.38461539149284363)
[2025-02-04 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:17][root][INFO] - Training Epoch: 2/2, step 22498/23838 completed (loss: 3.127716302871704, acc: 0.4642857015132904)
[2025-02-04 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:18][root][INFO] - Training Epoch: 2/2, step 22499/23838 completed (loss: 2.432905673980713, acc: 0.4545454680919647)
[2025-02-04 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:18][root][INFO] - Training Epoch: 2/2, step 22500/23838 completed (loss: 3.000040054321289, acc: 0.4399999976158142)
[2025-02-04 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:19][root][INFO] - Training Epoch: 2/2, step 22501/23838 completed (loss: 4.076938629150391, acc: 0.260869562625885)
[2025-02-04 01:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:19][root][INFO] - Training Epoch: 2/2, step 22502/23838 completed (loss: 3.3575055599212646, acc: 0.2916666567325592)
[2025-02-04 01:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:20][root][INFO] - Training Epoch: 2/2, step 22503/23838 completed (loss: 3.4956178665161133, acc: 0.27586206793785095)
[2025-02-04 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:20][root][INFO] - Training Epoch: 2/2, step 22504/23838 completed (loss: 4.070346832275391, acc: 0.2800000011920929)
[2025-02-04 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:21][root][INFO] - Training Epoch: 2/2, step 22505/23838 completed (loss: 3.5424811840057373, acc: 0.3499999940395355)
[2025-02-04 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:21][root][INFO] - Training Epoch: 2/2, step 22506/23838 completed (loss: 2.5685911178588867, acc: 0.3333333432674408)
[2025-02-04 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:22][root][INFO] - Training Epoch: 2/2, step 22507/23838 completed (loss: 3.262542963027954, acc: 0.2800000011920929)
[2025-02-04 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:22][root][INFO] - Training Epoch: 2/2, step 22508/23838 completed (loss: 4.136918544769287, acc: 0.2777777910232544)
[2025-02-04 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:22][root][INFO] - Training Epoch: 2/2, step 22509/23838 completed (loss: 3.9791958332061768, acc: 0.38461539149284363)
[2025-02-04 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:23][root][INFO] - Training Epoch: 2/2, step 22510/23838 completed (loss: 3.979665517807007, acc: 0.3571428656578064)
[2025-02-04 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:23][root][INFO] - Training Epoch: 2/2, step 22511/23838 completed (loss: 4.403018474578857, acc: 0.21739129722118378)
[2025-02-04 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:24][root][INFO] - Training Epoch: 2/2, step 22512/23838 completed (loss: 2.5981483459472656, acc: 0.375)
[2025-02-04 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:24][root][INFO] - Training Epoch: 2/2, step 22513/23838 completed (loss: 4.644043922424316, acc: 0.22580644488334656)
[2025-02-04 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:25][root][INFO] - Training Epoch: 2/2, step 22514/23838 completed (loss: 4.525808334350586, acc: 0.27586206793785095)
[2025-02-04 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:25][root][INFO] - Training Epoch: 2/2, step 22515/23838 completed (loss: 5.629979133605957, acc: 0.38461539149284363)
[2025-02-04 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:25][root][INFO] - Training Epoch: 2/2, step 22516/23838 completed (loss: 4.215889930725098, acc: 0.46666666865348816)
[2025-02-04 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:26][root][INFO] - Training Epoch: 2/2, step 22517/23838 completed (loss: 4.369271278381348, acc: 0.4642857015132904)
[2025-02-04 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:26][root][INFO] - Training Epoch: 2/2, step 22518/23838 completed (loss: 5.838465213775635, acc: 0.2857142984867096)
[2025-02-04 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:27][root][INFO] - Training Epoch: 2/2, step 22519/23838 completed (loss: 6.119089126586914, acc: 0.1875)
[2025-02-04 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:27][root][INFO] - Training Epoch: 2/2, step 22520/23838 completed (loss: 3.9823436737060547, acc: 0.3030303120613098)
[2025-02-04 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:28][root][INFO] - Training Epoch: 2/2, step 22521/23838 completed (loss: 3.602099895477295, acc: 0.4615384638309479)
[2025-02-04 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:28][root][INFO] - Training Epoch: 2/2, step 22522/23838 completed (loss: 2.495997428894043, acc: 0.5)
[2025-02-04 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:29][root][INFO] - Training Epoch: 2/2, step 22523/23838 completed (loss: 4.492741107940674, acc: 0.25)
[2025-02-04 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:29][root][INFO] - Training Epoch: 2/2, step 22524/23838 completed (loss: 2.7119157314300537, acc: 0.529411792755127)
[2025-02-04 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:30][root][INFO] - Training Epoch: 2/2, step 22525/23838 completed (loss: 3.137601137161255, acc: 0.4444444477558136)
[2025-02-04 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:30][root][INFO] - Training Epoch: 2/2, step 22526/23838 completed (loss: 2.562770366668701, acc: 0.4545454680919647)
[2025-02-04 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:31][root][INFO] - Training Epoch: 2/2, step 22527/23838 completed (loss: 3.8260927200317383, acc: 0.2777777910232544)
[2025-02-04 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:31][root][INFO] - Training Epoch: 2/2, step 22528/23838 completed (loss: 2.4295601844787598, acc: 0.5249999761581421)
[2025-02-04 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:31][root][INFO] - Training Epoch: 2/2, step 22529/23838 completed (loss: 2.2779953479766846, acc: 0.5)
[2025-02-04 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:32][root][INFO] - Training Epoch: 2/2, step 22530/23838 completed (loss: 2.616119861602783, acc: 0.48275861144065857)
[2025-02-04 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:32][root][INFO] - Training Epoch: 2/2, step 22531/23838 completed (loss: 4.2530717849731445, acc: 0.25)
[2025-02-04 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:33][root][INFO] - Training Epoch: 2/2, step 22532/23838 completed (loss: 4.010872840881348, acc: 0.3333333432674408)
[2025-02-04 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:33][root][INFO] - Training Epoch: 2/2, step 22533/23838 completed (loss: 3.1303837299346924, acc: 0.4117647111415863)
[2025-02-04 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:33][root][INFO] - Training Epoch: 2/2, step 22534/23838 completed (loss: 2.2427427768707275, acc: 0.5555555820465088)
[2025-02-04 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:34][root][INFO] - Training Epoch: 2/2, step 22535/23838 completed (loss: 3.5578324794769287, acc: 0.20000000298023224)
[2025-02-04 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:34][root][INFO] - Training Epoch: 2/2, step 22536/23838 completed (loss: 2.3259358406066895, acc: 0.5714285969734192)
[2025-02-04 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:35][root][INFO] - Training Epoch: 2/2, step 22537/23838 completed (loss: 2.0866611003875732, acc: 0.5)
[2025-02-04 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:35][root][INFO] - Training Epoch: 2/2, step 22538/23838 completed (loss: 1.0541776418685913, acc: 0.7777777910232544)
[2025-02-04 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:36][root][INFO] - Training Epoch: 2/2, step 22539/23838 completed (loss: 3.325573444366455, acc: 0.2800000011920929)
[2025-02-04 01:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:36][root][INFO] - Training Epoch: 2/2, step 22540/23838 completed (loss: 4.490334510803223, acc: 0.29411765933036804)
[2025-02-04 01:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:36][root][INFO] - Training Epoch: 2/2, step 22541/23838 completed (loss: 3.742280960083008, acc: 0.3947368562221527)
[2025-02-04 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:37][root][INFO] - Training Epoch: 2/2, step 22542/23838 completed (loss: 5.45012903213501, acc: 0.3076923191547394)
[2025-02-04 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:37][root][INFO] - Training Epoch: 2/2, step 22543/23838 completed (loss: 6.3172383308410645, acc: 0.3076923191547394)
[2025-02-04 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:38][root][INFO] - Training Epoch: 2/2, step 22544/23838 completed (loss: 4.5999345779418945, acc: 0.375)
[2025-02-04 01:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:38][root][INFO] - Training Epoch: 2/2, step 22545/23838 completed (loss: 5.889545917510986, acc: 0.2432432472705841)
[2025-02-04 01:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:38][root][INFO] - Training Epoch: 2/2, step 22546/23838 completed (loss: 2.8649239540100098, acc: 0.34375)
[2025-02-04 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:39][root][INFO] - Training Epoch: 2/2, step 22547/23838 completed (loss: 2.2821731567382812, acc: 0.5365853905677795)
[2025-02-04 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:39][root][INFO] - Training Epoch: 2/2, step 22548/23838 completed (loss: 2.8979156017303467, acc: 0.3571428656578064)
[2025-02-04 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:40][root][INFO] - Training Epoch: 2/2, step 22549/23838 completed (loss: 3.1728951930999756, acc: 0.5142857432365417)
[2025-02-04 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:40][root][INFO] - Training Epoch: 2/2, step 22550/23838 completed (loss: 2.7117650508880615, acc: 0.4722222089767456)
[2025-02-04 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:41][root][INFO] - Training Epoch: 2/2, step 22551/23838 completed (loss: 2.6920130252838135, acc: 0.5)
[2025-02-04 01:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:41][root][INFO] - Training Epoch: 2/2, step 22552/23838 completed (loss: 2.6589012145996094, acc: 0.43589743971824646)
[2025-02-04 01:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:42][root][INFO] - Training Epoch: 2/2, step 22553/23838 completed (loss: 2.7968337535858154, acc: 0.4761904776096344)
[2025-02-04 01:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:42][root][INFO] - Training Epoch: 2/2, step 22554/23838 completed (loss: 3.100708484649658, acc: 0.3235294222831726)
[2025-02-04 01:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:43][root][INFO] - Training Epoch: 2/2, step 22555/23838 completed (loss: 1.7182637453079224, acc: 0.625)
[2025-02-04 01:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:43][root][INFO] - Training Epoch: 2/2, step 22556/23838 completed (loss: 3.645742654800415, acc: 0.375)
[2025-02-04 01:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:43][root][INFO] - Training Epoch: 2/2, step 22557/23838 completed (loss: 4.381562232971191, acc: 0.1764705926179886)
[2025-02-04 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:44][root][INFO] - Training Epoch: 2/2, step 22558/23838 completed (loss: 1.681745171546936, acc: 0.5714285969734192)
[2025-02-04 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:44][root][INFO] - Training Epoch: 2/2, step 22559/23838 completed (loss: 1.901944637298584, acc: 0.6666666865348816)
[2025-02-04 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:45][root][INFO] - Training Epoch: 2/2, step 22560/23838 completed (loss: 2.3364410400390625, acc: 0.5454545617103577)
[2025-02-04 01:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:45][root][INFO] - Training Epoch: 2/2, step 22561/23838 completed (loss: 4.538578510284424, acc: 0.3888888955116272)
[2025-02-04 01:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:45][root][INFO] - Training Epoch: 2/2, step 22562/23838 completed (loss: 0.8858883380889893, acc: 0.6153846383094788)
[2025-02-04 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:46][root][INFO] - Training Epoch: 2/2, step 22563/23838 completed (loss: 1.567378044128418, acc: 0.6666666865348816)
[2025-02-04 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:46][root][INFO] - Training Epoch: 2/2, step 22564/23838 completed (loss: 1.4697128534317017, acc: 0.625)
[2025-02-04 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:47][root][INFO] - Training Epoch: 2/2, step 22565/23838 completed (loss: 0.8646652102470398, acc: 0.625)
[2025-02-04 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:47][root][INFO] - Training Epoch: 2/2, step 22566/23838 completed (loss: 1.4713455438613892, acc: 0.75)
[2025-02-04 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:47][root][INFO] - Training Epoch: 2/2, step 22567/23838 completed (loss: 3.1705756187438965, acc: 0.4324324429035187)
[2025-02-04 01:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:48][root][INFO] - Training Epoch: 2/2, step 22568/23838 completed (loss: 3.631619691848755, acc: 0.4000000059604645)
[2025-02-04 01:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:48][root][INFO] - Training Epoch: 2/2, step 22569/23838 completed (loss: 2.643169403076172, acc: 0.38235294818878174)
[2025-02-04 01:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:49][root][INFO] - Training Epoch: 2/2, step 22570/23838 completed (loss: 2.6447296142578125, acc: 0.40740740299224854)
[2025-02-04 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:49][root][INFO] - Training Epoch: 2/2, step 22571/23838 completed (loss: 3.0670950412750244, acc: 0.46341463923454285)
[2025-02-04 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:50][root][INFO] - Training Epoch: 2/2, step 22572/23838 completed (loss: 2.5849344730377197, acc: 0.4054054021835327)
[2025-02-04 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:50][root][INFO] - Training Epoch: 2/2, step 22573/23838 completed (loss: 3.0613205432891846, acc: 0.3684210479259491)
[2025-02-04 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:50][root][INFO] - Training Epoch: 2/2, step 22574/23838 completed (loss: 4.399036884307861, acc: 0.34090909361839294)
[2025-02-04 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:51][root][INFO] - Training Epoch: 2/2, step 22575/23838 completed (loss: 3.9916634559631348, acc: 0.3076923191547394)
[2025-02-04 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:51][root][INFO] - Training Epoch: 2/2, step 22576/23838 completed (loss: 2.3838918209075928, acc: 0.625)
[2025-02-04 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:52][root][INFO] - Training Epoch: 2/2, step 22577/23838 completed (loss: 2.829291343688965, acc: 0.4117647111415863)
[2025-02-04 01:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:52][root][INFO] - Training Epoch: 2/2, step 22578/23838 completed (loss: 1.7970337867736816, acc: 0.6000000238418579)
[2025-02-04 01:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:53][root][INFO] - Training Epoch: 2/2, step 22579/23838 completed (loss: 2.3726296424865723, acc: 0.4722222089767456)
[2025-02-04 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:53][root][INFO] - Training Epoch: 2/2, step 22580/23838 completed (loss: 3.122035026550293, acc: 0.3414634168148041)
[2025-02-04 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:54][root][INFO] - Training Epoch: 2/2, step 22581/23838 completed (loss: 3.22772479057312, acc: 0.30000001192092896)
[2025-02-04 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:54][root][INFO] - Training Epoch: 2/2, step 22582/23838 completed (loss: 3.341413974761963, acc: 0.24242424964904785)
[2025-02-04 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:54][root][INFO] - Training Epoch: 2/2, step 22583/23838 completed (loss: 3.656592845916748, acc: 0.3333333432674408)
[2025-02-04 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:55][root][INFO] - Training Epoch: 2/2, step 22584/23838 completed (loss: 2.952533483505249, acc: 0.41025641560554504)
[2025-02-04 01:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:55][root][INFO] - Training Epoch: 2/2, step 22585/23838 completed (loss: 3.338811159133911, acc: 0.4444444477558136)
[2025-02-04 01:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:56][root][INFO] - Training Epoch: 2/2, step 22586/23838 completed (loss: 2.9589858055114746, acc: 0.4333333373069763)
[2025-02-04 01:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:56][root][INFO] - Training Epoch: 2/2, step 22587/23838 completed (loss: 1.9910736083984375, acc: 0.5652173757553101)
[2025-02-04 01:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:56][root][INFO] - Training Epoch: 2/2, step 22588/23838 completed (loss: 2.5365443229675293, acc: 0.4680851101875305)
[2025-02-04 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:57][root][INFO] - Training Epoch: 2/2, step 22589/23838 completed (loss: 2.3020503520965576, acc: 0.5555555820465088)
[2025-02-04 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:57][root][INFO] - Training Epoch: 2/2, step 22590/23838 completed (loss: 2.1844656467437744, acc: 0.6000000238418579)
[2025-02-04 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:58][root][INFO] - Training Epoch: 2/2, step 22591/23838 completed (loss: 3.0891854763031006, acc: 0.4285714328289032)
[2025-02-04 01:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:58][root][INFO] - Training Epoch: 2/2, step 22592/23838 completed (loss: 3.0390512943267822, acc: 0.4406779706478119)
[2025-02-04 01:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:59][root][INFO] - Training Epoch: 2/2, step 22593/23838 completed (loss: 3.4918065071105957, acc: 0.29230770468711853)
[2025-02-04 01:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:59][root][INFO] - Training Epoch: 2/2, step 22594/23838 completed (loss: 3.181307315826416, acc: 0.3499999940395355)
[2025-02-04 01:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:07:59][root][INFO] - Training Epoch: 2/2, step 22595/23838 completed (loss: 1.64353346824646, acc: 0.625)
[2025-02-04 01:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:00][root][INFO] - Training Epoch: 2/2, step 22596/23838 completed (loss: 1.6791714429855347, acc: 0.5625)
[2025-02-04 01:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:00][root][INFO] - Training Epoch: 2/2, step 22597/23838 completed (loss: 2.42425274848938, acc: 0.44680851697921753)
[2025-02-04 01:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:01][root][INFO] - Training Epoch: 2/2, step 22598/23838 completed (loss: 3.1660988330841064, acc: 0.3188405930995941)
[2025-02-04 01:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:01][root][INFO] - Training Epoch: 2/2, step 22599/23838 completed (loss: 2.3812780380249023, acc: 0.6000000238418579)
[2025-02-04 01:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:01][root][INFO] - Training Epoch: 2/2, step 22600/23838 completed (loss: 1.3786166906356812, acc: 0.6785714030265808)
[2025-02-04 01:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:02][root][INFO] - Training Epoch: 2/2, step 22601/23838 completed (loss: 3.438302755355835, acc: 0.40740740299224854)
[2025-02-04 01:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:02][root][INFO] - Training Epoch: 2/2, step 22602/23838 completed (loss: 2.1243538856506348, acc: 0.5517241358757019)
[2025-02-04 01:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:03][root][INFO] - Training Epoch: 2/2, step 22603/23838 completed (loss: 2.6360228061676025, acc: 0.4166666567325592)
[2025-02-04 01:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:03][root][INFO] - Training Epoch: 2/2, step 22604/23838 completed (loss: 2.8719959259033203, acc: 0.48148149251937866)
[2025-02-04 01:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:04][root][INFO] - Training Epoch: 2/2, step 22605/23838 completed (loss: 1.223905324935913, acc: 0.7777777910232544)
[2025-02-04 01:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:04][root][INFO] - Training Epoch: 2/2, step 22606/23838 completed (loss: 2.3741869926452637, acc: 0.5121951103210449)
[2025-02-04 01:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:05][root][INFO] - Training Epoch: 2/2, step 22607/23838 completed (loss: 2.8690834045410156, acc: 0.6000000238418579)
[2025-02-04 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:05][root][INFO] - Training Epoch: 2/2, step 22608/23838 completed (loss: 3.02044415473938, acc: 0.5)
[2025-02-04 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:05][root][INFO] - Training Epoch: 2/2, step 22609/23838 completed (loss: 1.9978196620941162, acc: 0.625)
[2025-02-04 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:06][root][INFO] - Training Epoch: 2/2, step 22610/23838 completed (loss: 1.3810938596725464, acc: 0.5555555820465088)
[2025-02-04 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:06][root][INFO] - Training Epoch: 2/2, step 22611/23838 completed (loss: 2.5198163986206055, acc: 0.6000000238418579)
[2025-02-04 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:07][root][INFO] - Training Epoch: 2/2, step 22612/23838 completed (loss: 2.7444405555725098, acc: 0.4285714328289032)
[2025-02-04 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:07][root][INFO] - Training Epoch: 2/2, step 22613/23838 completed (loss: 4.307320594787598, acc: 0.375)
[2025-02-04 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:07][root][INFO] - Training Epoch: 2/2, step 22614/23838 completed (loss: 4.779179573059082, acc: 0.25)
[2025-02-04 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:08][root][INFO] - Training Epoch: 2/2, step 22615/23838 completed (loss: 3.738161087036133, acc: 0.15789473056793213)
[2025-02-04 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:08][root][INFO] - Training Epoch: 2/2, step 22616/23838 completed (loss: 4.309765815734863, acc: 0.3076923191547394)
[2025-02-04 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:09][root][INFO] - Training Epoch: 2/2, step 22617/23838 completed (loss: 3.4815797805786133, acc: 0.4545454680919647)
[2025-02-04 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:09][root][INFO] - Training Epoch: 2/2, step 22618/23838 completed (loss: 5.051491737365723, acc: 0.23529411852359772)
[2025-02-04 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:10][root][INFO] - Training Epoch: 2/2, step 22619/23838 completed (loss: 3.2440080642700195, acc: 0.3888888955116272)
[2025-02-04 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:10][root][INFO] - Training Epoch: 2/2, step 22620/23838 completed (loss: 3.9123499393463135, acc: 0.4375)
[2025-02-04 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:11][root][INFO] - Training Epoch: 2/2, step 22621/23838 completed (loss: 3.0782110691070557, acc: 0.5)
[2025-02-04 01:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:11][root][INFO] - Training Epoch: 2/2, step 22622/23838 completed (loss: 3.2133278846740723, acc: 0.5)
[2025-02-04 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:12][root][INFO] - Training Epoch: 2/2, step 22623/23838 completed (loss: 2.4534387588500977, acc: 0.5555555820465088)
[2025-02-04 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:12][root][INFO] - Training Epoch: 2/2, step 22624/23838 completed (loss: 1.8870702981948853, acc: 0.5714285969734192)
[2025-02-04 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:13][root][INFO] - Training Epoch: 2/2, step 22625/23838 completed (loss: 2.0533699989318848, acc: 0.699999988079071)
[2025-02-04 01:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:13][root][INFO] - Training Epoch: 2/2, step 22626/23838 completed (loss: 4.42628812789917, acc: 0.29629629850387573)
[2025-02-04 01:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:14][root][INFO] - Training Epoch: 2/2, step 22627/23838 completed (loss: 2.9216761589050293, acc: 0.4878048896789551)
[2025-02-04 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:14][root][INFO] - Training Epoch: 2/2, step 22628/23838 completed (loss: 2.9693615436553955, acc: 0.38461539149284363)
[2025-02-04 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:14][root][INFO] - Training Epoch: 2/2, step 22629/23838 completed (loss: 2.549278974533081, acc: 0.5)
[2025-02-04 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:15][root][INFO] - Training Epoch: 2/2, step 22630/23838 completed (loss: 3.4362916946411133, acc: 0.4166666567325592)
[2025-02-04 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:15][root][INFO] - Training Epoch: 2/2, step 22631/23838 completed (loss: 3.267540693283081, acc: 0.3333333432674408)
[2025-02-04 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:16][root][INFO] - Training Epoch: 2/2, step 22632/23838 completed (loss: 3.3279221057891846, acc: 0.3913043439388275)
[2025-02-04 01:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:16][root][INFO] - Training Epoch: 2/2, step 22633/23838 completed (loss: 3.5645792484283447, acc: 0.3333333432674408)
[2025-02-04 01:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:17][root][INFO] - Training Epoch: 2/2, step 22634/23838 completed (loss: 3.3214213848114014, acc: 0.3720930218696594)
[2025-02-04 01:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:17][root][INFO] - Training Epoch: 2/2, step 22635/23838 completed (loss: 3.39410138130188, acc: 0.3333333432674408)
[2025-02-04 01:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:18][root][INFO] - Training Epoch: 2/2, step 22636/23838 completed (loss: 3.5932397842407227, acc: 0.27272728085517883)
[2025-02-04 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:18][root][INFO] - Training Epoch: 2/2, step 22637/23838 completed (loss: 4.4486775398254395, acc: 0.21052631735801697)
[2025-02-04 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:18][root][INFO] - Training Epoch: 2/2, step 22638/23838 completed (loss: 3.2195751667022705, acc: 0.30000001192092896)
[2025-02-04 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:19][root][INFO] - Training Epoch: 2/2, step 22639/23838 completed (loss: 3.9395480155944824, acc: 0.2142857164144516)
[2025-02-04 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:19][root][INFO] - Training Epoch: 2/2, step 22640/23838 completed (loss: 3.265230894088745, acc: 0.3333333432674408)
[2025-02-04 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:20][root][INFO] - Training Epoch: 2/2, step 22641/23838 completed (loss: 3.366093873977661, acc: 0.35483869910240173)
[2025-02-04 01:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:20][root][INFO] - Training Epoch: 2/2, step 22642/23838 completed (loss: 4.738091945648193, acc: 0.15909090638160706)
[2025-02-04 01:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:20][root][INFO] - Training Epoch: 2/2, step 22643/23838 completed (loss: 2.7671597003936768, acc: 0.4000000059604645)
[2025-02-04 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:21][root][INFO] - Training Epoch: 2/2, step 22644/23838 completed (loss: 3.3440635204315186, acc: 0.42553192377090454)
[2025-02-04 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:21][root][INFO] - Training Epoch: 2/2, step 22645/23838 completed (loss: 3.544860601425171, acc: 0.1875)
[2025-02-04 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:22][root][INFO] - Training Epoch: 2/2, step 22646/23838 completed (loss: 3.6882638931274414, acc: 0.27272728085517883)
[2025-02-04 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:22][root][INFO] - Training Epoch: 2/2, step 22647/23838 completed (loss: 3.7881784439086914, acc: 0.32258063554763794)
[2025-02-04 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:22][root][INFO] - Training Epoch: 2/2, step 22648/23838 completed (loss: 4.068736553192139, acc: 0.25)
[2025-02-04 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:23][root][INFO] - Training Epoch: 2/2, step 22649/23838 completed (loss: 4.26868200302124, acc: 0.29411765933036804)
[2025-02-04 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:23][root][INFO] - Training Epoch: 2/2, step 22650/23838 completed (loss: 3.571026086807251, acc: 0.28947368264198303)
[2025-02-04 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:24][root][INFO] - Training Epoch: 2/2, step 22651/23838 completed (loss: 4.126134872436523, acc: 0.3529411852359772)
[2025-02-04 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:24][root][INFO] - Training Epoch: 2/2, step 22652/23838 completed (loss: 3.4216935634613037, acc: 0.3333333432674408)
[2025-02-04 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:25][root][INFO] - Training Epoch: 2/2, step 22653/23838 completed (loss: 3.2988176345825195, acc: 0.29032257199287415)
[2025-02-04 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:25][root][INFO] - Training Epoch: 2/2, step 22654/23838 completed (loss: 3.9095752239227295, acc: 0.27659574151039124)
[2025-02-04 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:25][root][INFO] - Training Epoch: 2/2, step 22655/23838 completed (loss: 3.954103946685791, acc: 0.18867924809455872)
[2025-02-04 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:26][root][INFO] - Training Epoch: 2/2, step 22656/23838 completed (loss: 3.380221366882324, acc: 0.40909090638160706)
[2025-02-04 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:26][root][INFO] - Training Epoch: 2/2, step 22657/23838 completed (loss: 3.722308874130249, acc: 0.3870967626571655)
[2025-02-04 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:27][root][INFO] - Training Epoch: 2/2, step 22658/23838 completed (loss: 2.486393928527832, acc: 0.4444444477558136)
[2025-02-04 01:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:27][root][INFO] - Training Epoch: 2/2, step 22659/23838 completed (loss: 3.9250893592834473, acc: 0.24390244483947754)
[2025-02-04 01:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:27][root][INFO] - Training Epoch: 2/2, step 22660/23838 completed (loss: 4.099193572998047, acc: 0.2199999988079071)
[2025-02-04 01:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:28][root][INFO] - Training Epoch: 2/2, step 22661/23838 completed (loss: 3.299926280975342, acc: 0.29411765933036804)
[2025-02-04 01:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:28][root][INFO] - Training Epoch: 2/2, step 22662/23838 completed (loss: 2.9492392539978027, acc: 0.43589743971824646)
[2025-02-04 01:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:29][root][INFO] - Training Epoch: 2/2, step 22663/23838 completed (loss: 3.9360764026641846, acc: 0.37837839126586914)
[2025-02-04 01:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:29][root][INFO] - Training Epoch: 2/2, step 22664/23838 completed (loss: 3.302259922027588, acc: 0.4146341383457184)
[2025-02-04 01:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:30][root][INFO] - Training Epoch: 2/2, step 22665/23838 completed (loss: 3.4633190631866455, acc: 0.21568627655506134)
[2025-02-04 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:30][root][INFO] - Training Epoch: 2/2, step 22666/23838 completed (loss: 3.8145952224731445, acc: 0.4000000059604645)
[2025-02-04 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:30][root][INFO] - Training Epoch: 2/2, step 22667/23838 completed (loss: 2.747223377227783, acc: 0.375)
[2025-02-04 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:31][root][INFO] - Training Epoch: 2/2, step 22668/23838 completed (loss: 2.0447423458099365, acc: 0.4333333373069763)
[2025-02-04 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:31][root][INFO] - Training Epoch: 2/2, step 22669/23838 completed (loss: 2.339977264404297, acc: 0.5714285969734192)
[2025-02-04 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:32][root][INFO] - Training Epoch: 2/2, step 22670/23838 completed (loss: 3.5963077545166016, acc: 0.2916666567325592)
[2025-02-04 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:32][root][INFO] - Training Epoch: 2/2, step 22671/23838 completed (loss: 4.567197322845459, acc: 0.24074074625968933)
[2025-02-04 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:33][root][INFO] - Training Epoch: 2/2, step 22672/23838 completed (loss: 4.039523124694824, acc: 0.261904776096344)
[2025-02-04 01:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:33][root][INFO] - Training Epoch: 2/2, step 22673/23838 completed (loss: 3.7525076866149902, acc: 0.4285714328289032)
[2025-02-04 01:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:34][root][INFO] - Training Epoch: 2/2, step 22674/23838 completed (loss: 3.710207462310791, acc: 0.375)
[2025-02-04 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:34][root][INFO] - Training Epoch: 2/2, step 22675/23838 completed (loss: 2.788510799407959, acc: 0.37837839126586914)
[2025-02-04 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:34][root][INFO] - Training Epoch: 2/2, step 22676/23838 completed (loss: 2.2545676231384277, acc: 0.5714285969734192)
[2025-02-04 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:35][root][INFO] - Training Epoch: 2/2, step 22677/23838 completed (loss: 2.780583620071411, acc: 0.37931033968925476)
[2025-02-04 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:35][root][INFO] - Training Epoch: 2/2, step 22678/23838 completed (loss: 2.6705219745635986, acc: 0.3636363744735718)
[2025-02-04 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:36][root][INFO] - Training Epoch: 2/2, step 22679/23838 completed (loss: 3.49288010597229, acc: 0.2985074520111084)
[2025-02-04 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:36][root][INFO] - Training Epoch: 2/2, step 22680/23838 completed (loss: 3.2970526218414307, acc: 0.4313725531101227)
[2025-02-04 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:37][root][INFO] - Training Epoch: 2/2, step 22681/23838 completed (loss: 3.6116790771484375, acc: 0.36538460850715637)
[2025-02-04 01:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:37][root][INFO] - Training Epoch: 2/2, step 22682/23838 completed (loss: 2.1592109203338623, acc: 0.5714285969734192)
[2025-02-04 01:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:37][root][INFO] - Training Epoch: 2/2, step 22683/23838 completed (loss: 3.8149561882019043, acc: 0.18918919563293457)
[2025-02-04 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:38][root][INFO] - Training Epoch: 2/2, step 22684/23838 completed (loss: 3.2417681217193604, acc: 0.3137255012989044)
[2025-02-04 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:38][root][INFO] - Training Epoch: 2/2, step 22685/23838 completed (loss: 2.4937970638275146, acc: 0.45945945382118225)
[2025-02-04 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:39][root][INFO] - Training Epoch: 2/2, step 22686/23838 completed (loss: 3.4897866249084473, acc: 0.39393940567970276)
[2025-02-04 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:39][root][INFO] - Training Epoch: 2/2, step 22687/23838 completed (loss: 3.3192195892333984, acc: 0.4749999940395355)
[2025-02-04 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:40][root][INFO] - Training Epoch: 2/2, step 22688/23838 completed (loss: 2.9091849327087402, acc: 0.3214285671710968)
[2025-02-04 01:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:40][root][INFO] - Training Epoch: 2/2, step 22689/23838 completed (loss: 2.3994224071502686, acc: 0.5)
[2025-02-04 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:41][root][INFO] - Training Epoch: 2/2, step 22690/23838 completed (loss: 5.2007341384887695, acc: 0.19718310236930847)
[2025-02-04 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:41][root][INFO] - Training Epoch: 2/2, step 22691/23838 completed (loss: 2.6815052032470703, acc: 0.4363636374473572)
[2025-02-04 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:42][root][INFO] - Training Epoch: 2/2, step 22692/23838 completed (loss: 2.961918830871582, acc: 0.5454545617103577)
[2025-02-04 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:42][root][INFO] - Training Epoch: 2/2, step 22693/23838 completed (loss: 3.1058566570281982, acc: 0.35087719559669495)
[2025-02-04 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:43][root][INFO] - Training Epoch: 2/2, step 22694/23838 completed (loss: 2.9855141639709473, acc: 0.3571428656578064)
[2025-02-04 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:43][root][INFO] - Training Epoch: 2/2, step 22695/23838 completed (loss: 3.3738813400268555, acc: 0.3333333432674408)
[2025-02-04 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:44][root][INFO] - Training Epoch: 2/2, step 22696/23838 completed (loss: 3.281212091445923, acc: 0.3636363744735718)
[2025-02-04 01:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:44][root][INFO] - Training Epoch: 2/2, step 22697/23838 completed (loss: 1.6022289991378784, acc: 0.5757575631141663)
[2025-02-04 01:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:45][root][INFO] - Training Epoch: 2/2, step 22698/23838 completed (loss: 1.9890481233596802, acc: 0.5833333134651184)
[2025-02-04 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:45][root][INFO] - Training Epoch: 2/2, step 22699/23838 completed (loss: 3.0640883445739746, acc: 0.3448275923728943)
[2025-02-04 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:45][root][INFO] - Training Epoch: 2/2, step 22700/23838 completed (loss: 2.936292886734009, acc: 0.30000001192092896)
[2025-02-04 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:46][root][INFO] - Training Epoch: 2/2, step 22701/23838 completed (loss: 2.483336925506592, acc: 0.6470588445663452)
[2025-02-04 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:46][root][INFO] - Training Epoch: 2/2, step 22702/23838 completed (loss: 2.087167978286743, acc: 0.5263158082962036)
[2025-02-04 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:46][root][INFO] - Training Epoch: 2/2, step 22703/23838 completed (loss: 3.5616588592529297, acc: 0.42105263471603394)
[2025-02-04 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:47][root][INFO] - Training Epoch: 2/2, step 22704/23838 completed (loss: 2.792579412460327, acc: 0.5416666865348816)
[2025-02-04 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:47][root][INFO] - Training Epoch: 2/2, step 22705/23838 completed (loss: 2.7610621452331543, acc: 0.38461539149284363)
[2025-02-04 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:48][root][INFO] - Training Epoch: 2/2, step 22706/23838 completed (loss: 2.88378643989563, acc: 0.5384615659713745)
[2025-02-04 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:48][root][INFO] - Training Epoch: 2/2, step 22707/23838 completed (loss: 2.917407751083374, acc: 0.4375)
[2025-02-04 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:49][root][INFO] - Training Epoch: 2/2, step 22708/23838 completed (loss: 2.549198865890503, acc: 0.3333333432674408)
[2025-02-04 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:49][root][INFO] - Training Epoch: 2/2, step 22709/23838 completed (loss: 3.848278284072876, acc: 0.3333333432674408)
[2025-02-04 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:49][root][INFO] - Training Epoch: 2/2, step 22710/23838 completed (loss: 2.6106834411621094, acc: 0.3571428656578064)
[2025-02-04 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:50][root][INFO] - Training Epoch: 2/2, step 22711/23838 completed (loss: 2.1993930339813232, acc: 0.5625)
[2025-02-04 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:50][root][INFO] - Training Epoch: 2/2, step 22712/23838 completed (loss: 2.6646318435668945, acc: 0.4615384638309479)
[2025-02-04 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:51][root][INFO] - Training Epoch: 2/2, step 22713/23838 completed (loss: 3.1699025630950928, acc: 0.3199999928474426)
[2025-02-04 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:51][root][INFO] - Training Epoch: 2/2, step 22714/23838 completed (loss: 3.561927080154419, acc: 0.36000001430511475)
[2025-02-04 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:51][root][INFO] - Training Epoch: 2/2, step 22715/23838 completed (loss: 4.424867630004883, acc: 0.2368421107530594)
[2025-02-04 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:52][root][INFO] - Training Epoch: 2/2, step 22716/23838 completed (loss: 3.7845301628112793, acc: 0.380952388048172)
[2025-02-04 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:52][root][INFO] - Training Epoch: 2/2, step 22717/23838 completed (loss: 2.6467230319976807, acc: 0.550000011920929)
[2025-02-04 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:53][root][INFO] - Training Epoch: 2/2, step 22718/23838 completed (loss: 2.152376174926758, acc: 0.6538461446762085)
[2025-02-04 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:53][root][INFO] - Training Epoch: 2/2, step 22719/23838 completed (loss: 2.3790884017944336, acc: 0.5789473652839661)
[2025-02-04 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:54][root][INFO] - Training Epoch: 2/2, step 22720/23838 completed (loss: 1.7856614589691162, acc: 0.5714285969734192)
[2025-02-04 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:54][root][INFO] - Training Epoch: 2/2, step 22721/23838 completed (loss: 1.2100114822387695, acc: 0.7647058963775635)
[2025-02-04 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:54][root][INFO] - Training Epoch: 2/2, step 22722/23838 completed (loss: 2.657421588897705, acc: 0.4545454680919647)
[2025-02-04 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:55][root][INFO] - Training Epoch: 2/2, step 22723/23838 completed (loss: 3.0000345706939697, acc: 0.4444444477558136)
[2025-02-04 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:55][root][INFO] - Training Epoch: 2/2, step 22724/23838 completed (loss: 3.5598504543304443, acc: 0.3333333432674408)
[2025-02-04 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:56][root][INFO] - Training Epoch: 2/2, step 22725/23838 completed (loss: 2.2026255130767822, acc: 0.5)
[2025-02-04 01:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:56][root][INFO] - Training Epoch: 2/2, step 22726/23838 completed (loss: 1.9398211240768433, acc: 0.6470588445663452)
[2025-02-04 01:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:56][root][INFO] - Training Epoch: 2/2, step 22727/23838 completed (loss: 2.3194212913513184, acc: 0.7692307829856873)
[2025-02-04 01:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:57][root][INFO] - Training Epoch: 2/2, step 22728/23838 completed (loss: 2.03611159324646, acc: 0.6666666865348816)
[2025-02-04 01:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:57][root][INFO] - Training Epoch: 2/2, step 22729/23838 completed (loss: 2.697216033935547, acc: 0.625)
[2025-02-04 01:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:58][root][INFO] - Training Epoch: 2/2, step 22730/23838 completed (loss: 2.831880569458008, acc: 0.5925925970077515)
[2025-02-04 01:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:58][root][INFO] - Training Epoch: 2/2, step 22731/23838 completed (loss: 3.079624652862549, acc: 0.3888888955116272)
[2025-02-04 01:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:58][root][INFO] - Training Epoch: 2/2, step 22732/23838 completed (loss: 1.398271083831787, acc: 0.7692307829856873)
[2025-02-04 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:59][root][INFO] - Training Epoch: 2/2, step 22733/23838 completed (loss: 1.879625678062439, acc: 0.7142857313156128)
[2025-02-04 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:08:59][root][INFO] - Training Epoch: 2/2, step 22734/23838 completed (loss: 2.3769781589508057, acc: 0.5199999809265137)
[2025-02-04 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:00][root][INFO] - Training Epoch: 2/2, step 22735/23838 completed (loss: 4.315168857574463, acc: 0.5)
[2025-02-04 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:00][root][INFO] - Training Epoch: 2/2, step 22736/23838 completed (loss: 2.7854228019714355, acc: 0.5454545617103577)
[2025-02-04 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:01][root][INFO] - Training Epoch: 2/2, step 22737/23838 completed (loss: 3.458112955093384, acc: 0.47058823704719543)
[2025-02-04 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:01][root][INFO] - Training Epoch: 2/2, step 22738/23838 completed (loss: 1.531359314918518, acc: 0.625)
[2025-02-04 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:02][root][INFO] - Training Epoch: 2/2, step 22739/23838 completed (loss: 2.9631764888763428, acc: 0.4000000059604645)
[2025-02-04 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:02][root][INFO] - Training Epoch: 2/2, step 22740/23838 completed (loss: 3.203282117843628, acc: 0.375)
[2025-02-04 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:02][root][INFO] - Training Epoch: 2/2, step 22741/23838 completed (loss: 3.11808705329895, acc: 0.27272728085517883)
[2025-02-04 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:03][root][INFO] - Training Epoch: 2/2, step 22742/23838 completed (loss: 2.515761375427246, acc: 0.5625)
[2025-02-04 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:03][root][INFO] - Training Epoch: 2/2, step 22743/23838 completed (loss: 4.3698320388793945, acc: 0.3636363744735718)
[2025-02-04 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:04][root][INFO] - Training Epoch: 2/2, step 22744/23838 completed (loss: 3.637024402618408, acc: 0.4000000059604645)
[2025-02-04 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:04][root][INFO] - Training Epoch: 2/2, step 22745/23838 completed (loss: 3.5310778617858887, acc: 0.2857142984867096)
[2025-02-04 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:05][root][INFO] - Training Epoch: 2/2, step 22746/23838 completed (loss: 2.698723316192627, acc: 0.5)
[2025-02-04 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:05][root][INFO] - Training Epoch: 2/2, step 22747/23838 completed (loss: 1.8932088613510132, acc: 0.5833333134651184)
[2025-02-04 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:06][root][INFO] - Training Epoch: 2/2, step 22748/23838 completed (loss: 3.7190074920654297, acc: 0.4000000059604645)
[2025-02-04 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:06][root][INFO] - Training Epoch: 2/2, step 22749/23838 completed (loss: 2.279256582260132, acc: 0.5789473652839661)
[2025-02-04 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:06][root][INFO] - Training Epoch: 2/2, step 22750/23838 completed (loss: 1.9577487707138062, acc: 0.5909090638160706)
[2025-02-04 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:07][root][INFO] - Training Epoch: 2/2, step 22751/23838 completed (loss: 4.052550792694092, acc: 0.3076923191547394)
[2025-02-04 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:07][root][INFO] - Training Epoch: 2/2, step 22752/23838 completed (loss: 2.3930487632751465, acc: 0.6875)
[2025-02-04 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:08][root][INFO] - Training Epoch: 2/2, step 22753/23838 completed (loss: 3.3134613037109375, acc: 0.4761904776096344)
[2025-02-04 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:08][root][INFO] - Training Epoch: 2/2, step 22754/23838 completed (loss: 2.3136775493621826, acc: 0.5454545617103577)
[2025-02-04 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:09][root][INFO] - Training Epoch: 2/2, step 22755/23838 completed (loss: 2.825201988220215, acc: 0.47826087474823)
[2025-02-04 01:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:09][root][INFO] - Training Epoch: 2/2, step 22756/23838 completed (loss: 3.303504228591919, acc: 0.3333333432674408)
[2025-02-04 01:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:09][root][INFO] - Training Epoch: 2/2, step 22757/23838 completed (loss: 2.047379493713379, acc: 0.5625)
[2025-02-04 01:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:10][root][INFO] - Training Epoch: 2/2, step 22758/23838 completed (loss: 1.9434552192687988, acc: 0.47058823704719543)
[2025-02-04 01:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:10][root][INFO] - Training Epoch: 2/2, step 22759/23838 completed (loss: 1.9370367527008057, acc: 0.6000000238418579)
[2025-02-04 01:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:11][root][INFO] - Training Epoch: 2/2, step 22760/23838 completed (loss: 2.3104469776153564, acc: 0.4375)
[2025-02-04 01:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:11][root][INFO] - Training Epoch: 2/2, step 22761/23838 completed (loss: 3.4755468368530273, acc: 0.3333333432674408)
[2025-02-04 01:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:12][root][INFO] - Training Epoch: 2/2, step 22762/23838 completed (loss: 2.9902989864349365, acc: 0.5625)
[2025-02-04 01:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:12][root][INFO] - Training Epoch: 2/2, step 22763/23838 completed (loss: 3.321805953979492, acc: 0.4375)
[2025-02-04 01:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:13][root][INFO] - Training Epoch: 2/2, step 22764/23838 completed (loss: 4.148794651031494, acc: 0.32786884903907776)
[2025-02-04 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:13][root][INFO] - Training Epoch: 2/2, step 22765/23838 completed (loss: 3.6724483966827393, acc: 0.3095238208770752)
[2025-02-04 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:14][root][INFO] - Training Epoch: 2/2, step 22766/23838 completed (loss: 2.793395519256592, acc: 0.4761904776096344)
[2025-02-04 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:14][root][INFO] - Training Epoch: 2/2, step 22767/23838 completed (loss: 4.205758571624756, acc: 0.4375)
[2025-02-04 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:14][root][INFO] - Training Epoch: 2/2, step 22768/23838 completed (loss: 1.7192529439926147, acc: 0.5625)
[2025-02-04 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:15][root][INFO] - Training Epoch: 2/2, step 22769/23838 completed (loss: 2.340611696243286, acc: 0.523809552192688)
[2025-02-04 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:15][root][INFO] - Training Epoch: 2/2, step 22770/23838 completed (loss: 4.578768253326416, acc: 0.3571428656578064)
[2025-02-04 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:16][root][INFO] - Training Epoch: 2/2, step 22771/23838 completed (loss: 2.663701057434082, acc: 0.4482758641242981)
[2025-02-04 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:16][root][INFO] - Training Epoch: 2/2, step 22772/23838 completed (loss: 3.9295051097869873, acc: 0.25)
[2025-02-04 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:16][root][INFO] - Training Epoch: 2/2, step 22773/23838 completed (loss: 3.201122999191284, acc: 0.3448275923728943)
[2025-02-04 01:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:17][root][INFO] - Training Epoch: 2/2, step 22774/23838 completed (loss: 3.6323750019073486, acc: 0.20000000298023224)
[2025-02-04 01:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:18][root][INFO] - Training Epoch: 2/2, step 22775/23838 completed (loss: 3.2677533626556396, acc: 0.375)
[2025-02-04 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:18][root][INFO] - Training Epoch: 2/2, step 22776/23838 completed (loss: 2.257854700088501, acc: 0.5)
[2025-02-04 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:18][root][INFO] - Training Epoch: 2/2, step 22777/23838 completed (loss: 3.0068929195404053, acc: 0.3125)
[2025-02-04 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:19][root][INFO] - Training Epoch: 2/2, step 22778/23838 completed (loss: 3.1929433345794678, acc: 0.4000000059604645)
[2025-02-04 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:19][root][INFO] - Training Epoch: 2/2, step 22779/23838 completed (loss: 2.4009804725646973, acc: 0.4615384638309479)
[2025-02-04 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:20][root][INFO] - Training Epoch: 2/2, step 22780/23838 completed (loss: 2.8847815990448, acc: 0.3095238208770752)
[2025-02-04 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:20][root][INFO] - Training Epoch: 2/2, step 22781/23838 completed (loss: 3.1019961833953857, acc: 0.38461539149284363)
[2025-02-04 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:21][root][INFO] - Training Epoch: 2/2, step 22782/23838 completed (loss: 3.783879518508911, acc: 0.3243243098258972)
[2025-02-04 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:21][root][INFO] - Training Epoch: 2/2, step 22783/23838 completed (loss: 3.639533281326294, acc: 0.3400000035762787)
[2025-02-04 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:21][root][INFO] - Training Epoch: 2/2, step 22784/23838 completed (loss: 3.6403284072875977, acc: 0.27272728085517883)
[2025-02-04 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:22][root][INFO] - Training Epoch: 2/2, step 22785/23838 completed (loss: 2.9447286128997803, acc: 0.3030303120613098)
[2025-02-04 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:22][root][INFO] - Training Epoch: 2/2, step 22786/23838 completed (loss: 2.505939245223999, acc: 0.4166666567325592)
[2025-02-04 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:22][root][INFO] - Training Epoch: 2/2, step 22787/23838 completed (loss: 2.132826089859009, acc: 0.5789473652839661)
[2025-02-04 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:23][root][INFO] - Training Epoch: 2/2, step 22788/23838 completed (loss: 3.488064765930176, acc: 0.2777777910232544)
[2025-02-04 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:23][root][INFO] - Training Epoch: 2/2, step 22789/23838 completed (loss: 4.2882585525512695, acc: 0.3333333432674408)
[2025-02-04 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:24][root][INFO] - Training Epoch: 2/2, step 22790/23838 completed (loss: 2.7808070182800293, acc: 0.4054054021835327)
[2025-02-04 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:24][root][INFO] - Training Epoch: 2/2, step 22791/23838 completed (loss: 3.7545385360717773, acc: 0.3448275923728943)
[2025-02-04 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:24][root][INFO] - Training Epoch: 2/2, step 22792/23838 completed (loss: 3.1484365463256836, acc: 0.4285714328289032)
[2025-02-04 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:25][root][INFO] - Training Epoch: 2/2, step 22793/23838 completed (loss: 3.233955144882202, acc: 0.3448275923728943)
[2025-02-04 01:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:25][root][INFO] - Training Epoch: 2/2, step 22794/23838 completed (loss: 2.834256410598755, acc: 0.35483869910240173)
[2025-02-04 01:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:25][root][INFO] - Training Epoch: 2/2, step 22795/23838 completed (loss: 3.037360191345215, acc: 0.3333333432674408)
[2025-02-04 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:26][root][INFO] - Training Epoch: 2/2, step 22796/23838 completed (loss: 2.7198915481567383, acc: 0.4545454680919647)
[2025-02-04 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:26][root][INFO] - Training Epoch: 2/2, step 22797/23838 completed (loss: 3.1302988529205322, acc: 0.3658536672592163)
[2025-02-04 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:27][root][INFO] - Training Epoch: 2/2, step 22798/23838 completed (loss: 3.4314239025115967, acc: 0.3448275923728943)
[2025-02-04 01:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:27][root][INFO] - Training Epoch: 2/2, step 22799/23838 completed (loss: 2.924020290374756, acc: 0.37837839126586914)
[2025-02-04 01:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:27][root][INFO] - Training Epoch: 2/2, step 22800/23838 completed (loss: 3.150829553604126, acc: 0.3333333432674408)
[2025-02-04 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:28][root][INFO] - Training Epoch: 2/2, step 22801/23838 completed (loss: 3.560218334197998, acc: 0.25641027092933655)
[2025-02-04 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:28][root][INFO] - Training Epoch: 2/2, step 22802/23838 completed (loss: 2.7501163482666016, acc: 0.47058823704719543)
[2025-02-04 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:29][root][INFO] - Training Epoch: 2/2, step 22803/23838 completed (loss: 2.4550278186798096, acc: 0.4000000059604645)
[2025-02-04 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:29][root][INFO] - Training Epoch: 2/2, step 22804/23838 completed (loss: 3.452716827392578, acc: 0.3207547068595886)
[2025-02-04 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:29][root][INFO] - Training Epoch: 2/2, step 22805/23838 completed (loss: 3.4261794090270996, acc: 0.39534884691238403)
[2025-02-04 01:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:30][root][INFO] - Training Epoch: 2/2, step 22806/23838 completed (loss: 2.940335988998413, acc: 0.45945945382118225)
[2025-02-04 01:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:30][root][INFO] - Training Epoch: 2/2, step 22807/23838 completed (loss: 2.5884954929351807, acc: 0.4166666567325592)
[2025-02-04 01:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:31][root][INFO] - Training Epoch: 2/2, step 22808/23838 completed (loss: 3.3106138706207275, acc: 0.3125)
[2025-02-04 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:31][root][INFO] - Training Epoch: 2/2, step 22809/23838 completed (loss: 3.389477252960205, acc: 0.3076923191547394)
[2025-02-04 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:32][root][INFO] - Training Epoch: 2/2, step 22810/23838 completed (loss: 2.869333028793335, acc: 0.3636363744735718)
[2025-02-04 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:32][root][INFO] - Training Epoch: 2/2, step 22811/23838 completed (loss: 2.6670515537261963, acc: 0.37931033968925476)
[2025-02-04 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:33][root][INFO] - Training Epoch: 2/2, step 22812/23838 completed (loss: 3.304948329925537, acc: 0.40740740299224854)
[2025-02-04 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:33][root][INFO] - Training Epoch: 2/2, step 22813/23838 completed (loss: 3.2412192821502686, acc: 0.34375)
[2025-02-04 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:33][root][INFO] - Training Epoch: 2/2, step 22814/23838 completed (loss: 3.0577831268310547, acc: 0.42105263471603394)
[2025-02-04 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:34][root][INFO] - Training Epoch: 2/2, step 22815/23838 completed (loss: 2.994527816772461, acc: 0.2857142984867096)
[2025-02-04 01:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:34][root][INFO] - Training Epoch: 2/2, step 22816/23838 completed (loss: 3.4779131412506104, acc: 0.3461538553237915)
[2025-02-04 01:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:35][root][INFO] - Training Epoch: 2/2, step 22817/23838 completed (loss: 3.026111125946045, acc: 0.4444444477558136)
[2025-02-04 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:35][root][INFO] - Training Epoch: 2/2, step 22818/23838 completed (loss: 2.132028341293335, acc: 0.5333333611488342)
[2025-02-04 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:35][root][INFO] - Training Epoch: 2/2, step 22819/23838 completed (loss: 3.0083022117614746, acc: 0.3863636255264282)
[2025-02-04 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:36][root][INFO] - Training Epoch: 2/2, step 22820/23838 completed (loss: 2.642354965209961, acc: 0.31578946113586426)
[2025-02-04 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:36][root][INFO] - Training Epoch: 2/2, step 22821/23838 completed (loss: 3.3434619903564453, acc: 0.35483869910240173)
[2025-02-04 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:37][root][INFO] - Training Epoch: 2/2, step 22822/23838 completed (loss: 2.8023757934570312, acc: 0.30434781312942505)
[2025-02-04 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:37][root][INFO] - Training Epoch: 2/2, step 22823/23838 completed (loss: 2.6411850452423096, acc: 0.4285714328289032)
[2025-02-04 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:38][root][INFO] - Training Epoch: 2/2, step 22824/23838 completed (loss: 2.03412127494812, acc: 0.5)
[2025-02-04 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:38][root][INFO] - Training Epoch: 2/2, step 22825/23838 completed (loss: 1.5981841087341309, acc: 0.5714285969734192)
[2025-02-04 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:39][root][INFO] - Training Epoch: 2/2, step 22826/23838 completed (loss: 3.0858776569366455, acc: 0.3333333432674408)
[2025-02-04 01:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:39][root][INFO] - Training Epoch: 2/2, step 22827/23838 completed (loss: 4.726490497589111, acc: 0.25999999046325684)
[2025-02-04 01:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:40][root][INFO] - Training Epoch: 2/2, step 22828/23838 completed (loss: 4.195668697357178, acc: 0.23255814611911774)
[2025-02-04 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:40][root][INFO] - Training Epoch: 2/2, step 22829/23838 completed (loss: 3.4891252517700195, acc: 0.24242424964904785)
[2025-02-04 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:40][root][INFO] - Training Epoch: 2/2, step 22830/23838 completed (loss: 3.242307186126709, acc: 0.4117647111415863)
[2025-02-04 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:41][root][INFO] - Training Epoch: 2/2, step 22831/23838 completed (loss: 4.000226020812988, acc: 0.40909090638160706)
[2025-02-04 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:41][root][INFO] - Training Epoch: 2/2, step 22832/23838 completed (loss: 2.3002610206604004, acc: 0.550000011920929)
[2025-02-04 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:42][root][INFO] - Training Epoch: 2/2, step 22833/23838 completed (loss: 3.438308000564575, acc: 0.4137931168079376)
[2025-02-04 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:42][root][INFO] - Training Epoch: 2/2, step 22834/23838 completed (loss: 4.4963860511779785, acc: 0.28125)
[2025-02-04 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:42][root][INFO] - Training Epoch: 2/2, step 22835/23838 completed (loss: 3.288539409637451, acc: 0.3636363744735718)
[2025-02-04 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:43][root][INFO] - Training Epoch: 2/2, step 22836/23838 completed (loss: 4.552955627441406, acc: 0.28260868787765503)
[2025-02-04 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:43][root][INFO] - Training Epoch: 2/2, step 22837/23838 completed (loss: 3.674680709838867, acc: 0.30000001192092896)
[2025-02-04 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:44][root][INFO] - Training Epoch: 2/2, step 22838/23838 completed (loss: 3.5445713996887207, acc: 0.3611111044883728)
[2025-02-04 01:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:44][root][INFO] - Training Epoch: 2/2, step 22839/23838 completed (loss: 3.081392288208008, acc: 0.3333333432674408)
[2025-02-04 01:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:45][root][INFO] - Training Epoch: 2/2, step 22840/23838 completed (loss: 3.9938008785247803, acc: 0.24390244483947754)
[2025-02-04 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:45][root][INFO] - Training Epoch: 2/2, step 22841/23838 completed (loss: 2.878038167953491, acc: 0.4444444477558136)
[2025-02-04 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:45][root][INFO] - Training Epoch: 2/2, step 22842/23838 completed (loss: 3.7466888427734375, acc: 0.3333333432674408)
[2025-02-04 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:46][root][INFO] - Training Epoch: 2/2, step 22843/23838 completed (loss: 3.2441518306732178, acc: 0.4333333373069763)
[2025-02-04 01:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:46][root][INFO] - Training Epoch: 2/2, step 22844/23838 completed (loss: 3.5880980491638184, acc: 0.3035714328289032)
[2025-02-04 01:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:47][root][INFO] - Training Epoch: 2/2, step 22845/23838 completed (loss: 3.2366292476654053, acc: 0.3333333432674408)
[2025-02-04 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:47][root][INFO] - Training Epoch: 2/2, step 22846/23838 completed (loss: 2.890733480453491, acc: 0.42105263471603394)
[2025-02-04 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:48][root][INFO] - Training Epoch: 2/2, step 22847/23838 completed (loss: 2.399243116378784, acc: 0.5714285969734192)
[2025-02-04 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:48][root][INFO] - Training Epoch: 2/2, step 22848/23838 completed (loss: 2.715744733810425, acc: 0.3636363744735718)
[2025-02-04 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:48][root][INFO] - Training Epoch: 2/2, step 22849/23838 completed (loss: 2.6677472591400146, acc: 0.4545454680919647)
[2025-02-04 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:49][root][INFO] - Training Epoch: 2/2, step 22850/23838 completed (loss: 2.6827783584594727, acc: 0.46000000834465027)
[2025-02-04 01:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:49][root][INFO] - Training Epoch: 2/2, step 22851/23838 completed (loss: 3.180910587310791, acc: 0.3265306055545807)
[2025-02-04 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:50][root][INFO] - Training Epoch: 2/2, step 22852/23838 completed (loss: 3.1022322177886963, acc: 0.35555556416511536)
[2025-02-04 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:50][root][INFO] - Training Epoch: 2/2, step 22853/23838 completed (loss: 3.2156617641448975, acc: 0.3076923191547394)
[2025-02-04 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:51][root][INFO] - Training Epoch: 2/2, step 22854/23838 completed (loss: 3.4562888145446777, acc: 0.3658536672592163)
[2025-02-04 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:51][root][INFO] - Training Epoch: 2/2, step 22855/23838 completed (loss: 2.307819366455078, acc: 0.4000000059604645)
[2025-02-04 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:52][root][INFO] - Training Epoch: 2/2, step 22856/23838 completed (loss: 2.735569953918457, acc: 0.5263158082962036)
[2025-02-04 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:52][root][INFO] - Training Epoch: 2/2, step 22857/23838 completed (loss: 2.7209911346435547, acc: 0.5416666865348816)
[2025-02-04 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:52][root][INFO] - Training Epoch: 2/2, step 22858/23838 completed (loss: 3.3289430141448975, acc: 0.4285714328289032)
[2025-02-04 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:53][root][INFO] - Training Epoch: 2/2, step 22859/23838 completed (loss: 3.1502625942230225, acc: 0.44186046719551086)
[2025-02-04 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:53][root][INFO] - Training Epoch: 2/2, step 22860/23838 completed (loss: 2.913814067840576, acc: 0.5)
[2025-02-04 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:54][root][INFO] - Training Epoch: 2/2, step 22861/23838 completed (loss: 3.0727365016937256, acc: 0.43589743971824646)
[2025-02-04 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:54][root][INFO] - Training Epoch: 2/2, step 22862/23838 completed (loss: 2.721234083175659, acc: 0.4878048896789551)
[2025-02-04 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:54][root][INFO] - Training Epoch: 2/2, step 22863/23838 completed (loss: 3.8719327449798584, acc: 0.2666666805744171)
[2025-02-04 01:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:55][root][INFO] - Training Epoch: 2/2, step 22864/23838 completed (loss: 2.7798140048980713, acc: 0.4054054021835327)
[2025-02-04 01:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:55][root][INFO] - Training Epoch: 2/2, step 22865/23838 completed (loss: 3.118039608001709, acc: 0.3684210479259491)
[2025-02-04 01:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:56][root][INFO] - Training Epoch: 2/2, step 22866/23838 completed (loss: 3.117823839187622, acc: 0.3928571343421936)
[2025-02-04 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:56][root][INFO] - Training Epoch: 2/2, step 22867/23838 completed (loss: 2.58730149269104, acc: 0.529411792755127)
[2025-02-04 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:57][root][INFO] - Training Epoch: 2/2, step 22868/23838 completed (loss: 2.850797414779663, acc: 0.4545454680919647)
[2025-02-04 01:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:57][root][INFO] - Training Epoch: 2/2, step 22869/23838 completed (loss: 3.7209925651550293, acc: 0.3333333432674408)
[2025-02-04 01:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:57][root][INFO] - Training Epoch: 2/2, step 22870/23838 completed (loss: 3.673128128051758, acc: 0.35483869910240173)
[2025-02-04 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:58][root][INFO] - Training Epoch: 2/2, step 22871/23838 completed (loss: 1.5596131086349487, acc: 0.5769230723381042)
[2025-02-04 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:58][root][INFO] - Training Epoch: 2/2, step 22872/23838 completed (loss: 2.54902720451355, acc: 0.5714285969734192)
[2025-02-04 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:59][root][INFO] - Training Epoch: 2/2, step 22873/23838 completed (loss: 2.848390817642212, acc: 0.4333333373069763)
[2025-02-04 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:09:59][root][INFO] - Training Epoch: 2/2, step 22874/23838 completed (loss: 3.5184030532836914, acc: 0.3636363744735718)
[2025-02-04 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:00][root][INFO] - Training Epoch: 2/2, step 22875/23838 completed (loss: 3.832341432571411, acc: 0.3888888955116272)
[2025-02-04 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:00][root][INFO] - Training Epoch: 2/2, step 22876/23838 completed (loss: 3.1574792861938477, acc: 0.3243243098258972)
[2025-02-04 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:00][root][INFO] - Training Epoch: 2/2, step 22877/23838 completed (loss: 5.68655252456665, acc: 0.21739129722118378)
[2025-02-04 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:01][root][INFO] - Training Epoch: 2/2, step 22878/23838 completed (loss: 4.201906204223633, acc: 0.29629629850387573)
[2025-02-04 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:01][root][INFO] - Training Epoch: 2/2, step 22879/23838 completed (loss: 2.289714813232422, acc: 0.4761904776096344)
[2025-02-04 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:02][root][INFO] - Training Epoch: 2/2, step 22880/23838 completed (loss: 2.067232131958008, acc: 0.6296296119689941)
[2025-02-04 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:02][root][INFO] - Training Epoch: 2/2, step 22881/23838 completed (loss: 3.023573875427246, acc: 0.37037035822868347)
[2025-02-04 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:03][root][INFO] - Training Epoch: 2/2, step 22882/23838 completed (loss: 2.931718349456787, acc: 0.4516128897666931)
[2025-02-04 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:03][root][INFO] - Training Epoch: 2/2, step 22883/23838 completed (loss: 3.411165237426758, acc: 0.43478259444236755)
[2025-02-04 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:04][root][INFO] - Training Epoch: 2/2, step 22884/23838 completed (loss: 4.39951753616333, acc: 0.3142857253551483)
[2025-02-04 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:04][root][INFO] - Training Epoch: 2/2, step 22885/23838 completed (loss: 2.7792558670043945, acc: 0.5555555820465088)
[2025-02-04 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:04][root][INFO] - Training Epoch: 2/2, step 22886/23838 completed (loss: 4.043375015258789, acc: 0.27272728085517883)
[2025-02-04 01:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:05][root][INFO] - Training Epoch: 2/2, step 22887/23838 completed (loss: 3.978239059448242, acc: 0.25)
[2025-02-04 01:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:05][root][INFO] - Training Epoch: 2/2, step 22888/23838 completed (loss: 2.7356038093566895, acc: 0.5333333611488342)
[2025-02-04 01:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:06][root][INFO] - Training Epoch: 2/2, step 22889/23838 completed (loss: 3.1261887550354004, acc: 0.2857142984867096)
[2025-02-04 01:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:06][root][INFO] - Training Epoch: 2/2, step 22890/23838 completed (loss: 2.398625135421753, acc: 0.5454545617103577)
[2025-02-04 01:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:07][root][INFO] - Training Epoch: 2/2, step 22891/23838 completed (loss: 3.208749771118164, acc: 0.3913043439388275)
[2025-02-04 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:07][root][INFO] - Training Epoch: 2/2, step 22892/23838 completed (loss: 1.5976061820983887, acc: 0.5384615659713745)
[2025-02-04 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:08][root][INFO] - Training Epoch: 2/2, step 22893/23838 completed (loss: 1.7162686586380005, acc: 0.5714285969734192)
[2025-02-04 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:08][root][INFO] - Training Epoch: 2/2, step 22894/23838 completed (loss: 2.918684482574463, acc: 0.39024388790130615)
[2025-02-04 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:09][root][INFO] - Training Epoch: 2/2, step 22895/23838 completed (loss: 3.1105377674102783, acc: 0.3947368562221527)
[2025-02-04 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:09][root][INFO] - Training Epoch: 2/2, step 22896/23838 completed (loss: 2.7771520614624023, acc: 0.44186046719551086)
[2025-02-04 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:09][root][INFO] - Training Epoch: 2/2, step 22897/23838 completed (loss: 3.5456347465515137, acc: 0.3877550959587097)
[2025-02-04 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:10][root][INFO] - Training Epoch: 2/2, step 22898/23838 completed (loss: 2.9881861209869385, acc: 0.4047619104385376)
[2025-02-04 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:10][root][INFO] - Training Epoch: 2/2, step 22899/23838 completed (loss: 2.8810176849365234, acc: 0.45098039507865906)
[2025-02-04 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:11][root][INFO] - Training Epoch: 2/2, step 22900/23838 completed (loss: 3.113736391067505, acc: 0.4000000059604645)
[2025-02-04 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:11][root][INFO] - Training Epoch: 2/2, step 22901/23838 completed (loss: 1.897451639175415, acc: 0.5357142686843872)
[2025-02-04 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:12][root][INFO] - Training Epoch: 2/2, step 22902/23838 completed (loss: 4.138638496398926, acc: 0.4722222089767456)
[2025-02-04 01:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:12][root][INFO] - Training Epoch: 2/2, step 22903/23838 completed (loss: 3.206989049911499, acc: 0.3947368562221527)
[2025-02-04 01:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:12][root][INFO] - Training Epoch: 2/2, step 22904/23838 completed (loss: 2.812364101409912, acc: 0.4642857015132904)
[2025-02-04 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:13][root][INFO] - Training Epoch: 2/2, step 22905/23838 completed (loss: 2.212038516998291, acc: 0.5)
[2025-02-04 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:13][root][INFO] - Training Epoch: 2/2, step 22906/23838 completed (loss: 3.05985951423645, acc: 0.37931033968925476)
[2025-02-04 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:14][root][INFO] - Training Epoch: 2/2, step 22907/23838 completed (loss: 2.499966859817505, acc: 0.5)
[2025-02-04 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:14][root][INFO] - Training Epoch: 2/2, step 22908/23838 completed (loss: 3.3367836475372314, acc: 0.37142857909202576)
[2025-02-04 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:14][root][INFO] - Training Epoch: 2/2, step 22909/23838 completed (loss: 3.2356624603271484, acc: 0.3541666567325592)
[2025-02-04 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:15][root][INFO] - Training Epoch: 2/2, step 22910/23838 completed (loss: 2.3982672691345215, acc: 0.4324324429035187)
[2025-02-04 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:15][root][INFO] - Training Epoch: 2/2, step 22911/23838 completed (loss: 2.8466944694519043, acc: 0.4038461446762085)
[2025-02-04 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:16][root][INFO] - Training Epoch: 2/2, step 22912/23838 completed (loss: 2.852389335632324, acc: 0.4761904776096344)
[2025-02-04 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:16][root][INFO] - Training Epoch: 2/2, step 22913/23838 completed (loss: 2.933159351348877, acc: 0.3488371968269348)
[2025-02-04 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:17][root][INFO] - Training Epoch: 2/2, step 22914/23838 completed (loss: 2.6555473804473877, acc: 0.4838709533214569)
[2025-02-04 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:17][root][INFO] - Training Epoch: 2/2, step 22915/23838 completed (loss: 1.8078030347824097, acc: 0.6666666865348816)
[2025-02-04 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:17][root][INFO] - Training Epoch: 2/2, step 22916/23838 completed (loss: 2.711341619491577, acc: 0.43589743971824646)
[2025-02-04 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:18][root][INFO] - Training Epoch: 2/2, step 22917/23838 completed (loss: 2.1088688373565674, acc: 0.6470588445663452)
[2025-02-04 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:18][root][INFO] - Training Epoch: 2/2, step 22918/23838 completed (loss: 2.8465704917907715, acc: 0.46875)
[2025-02-04 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:19][root][INFO] - Training Epoch: 2/2, step 22919/23838 completed (loss: 2.773125410079956, acc: 0.4615384638309479)
[2025-02-04 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:19][root][INFO] - Training Epoch: 2/2, step 22920/23838 completed (loss: 2.433321237564087, acc: 0.6060606241226196)
[2025-02-04 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:20][root][INFO] - Training Epoch: 2/2, step 22921/23838 completed (loss: 2.515063524246216, acc: 0.5199999809265137)
[2025-02-04 01:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:20][root][INFO] - Training Epoch: 2/2, step 22922/23838 completed (loss: 1.82009756565094, acc: 0.625)
[2025-02-04 01:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:21][root][INFO] - Training Epoch: 2/2, step 22923/23838 completed (loss: 2.022108554840088, acc: 0.5483871102333069)
[2025-02-04 01:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:21][root][INFO] - Training Epoch: 2/2, step 22924/23838 completed (loss: 3.0860607624053955, acc: 0.4000000059604645)
[2025-02-04 01:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:21][root][INFO] - Training Epoch: 2/2, step 22925/23838 completed (loss: 1.8335579633712769, acc: 0.5333333611488342)
[2025-02-04 01:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:22][root][INFO] - Training Epoch: 2/2, step 22926/23838 completed (loss: 2.0105721950531006, acc: 0.5625)
[2025-02-04 01:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:22][root][INFO] - Training Epoch: 2/2, step 22927/23838 completed (loss: 2.024670362472534, acc: 0.5357142686843872)
[2025-02-04 01:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:23][root][INFO] - Training Epoch: 2/2, step 22928/23838 completed (loss: 1.588714599609375, acc: 0.7222222089767456)
[2025-02-04 01:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:23][root][INFO] - Training Epoch: 2/2, step 22929/23838 completed (loss: 2.9609694480895996, acc: 0.3928571343421936)
[2025-02-04 01:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:23][root][INFO] - Training Epoch: 2/2, step 22930/23838 completed (loss: 2.3838701248168945, acc: 0.5833333134651184)
[2025-02-04 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:24][root][INFO] - Training Epoch: 2/2, step 22931/23838 completed (loss: 2.3963239192962646, acc: 0.7222222089767456)
[2025-02-04 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:24][root][INFO] - Training Epoch: 2/2, step 22932/23838 completed (loss: 1.1381902694702148, acc: 0.7058823704719543)
[2025-02-04 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:25][root][INFO] - Training Epoch: 2/2, step 22933/23838 completed (loss: 1.7664246559143066, acc: 0.6666666865348816)
[2025-02-04 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:25][root][INFO] - Training Epoch: 2/2, step 22934/23838 completed (loss: 3.6792514324188232, acc: 0.4000000059604645)
[2025-02-04 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:26][root][INFO] - Training Epoch: 2/2, step 22935/23838 completed (loss: 2.9334239959716797, acc: 0.3913043439388275)
[2025-02-04 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:26][root][INFO] - Training Epoch: 2/2, step 22936/23838 completed (loss: 2.858133554458618, acc: 0.5333333611488342)
[2025-02-04 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:26][root][INFO] - Training Epoch: 2/2, step 22937/23838 completed (loss: 2.3142688274383545, acc: 0.4878048896789551)
[2025-02-04 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:27][root][INFO] - Training Epoch: 2/2, step 22938/23838 completed (loss: 2.920806646347046, acc: 0.3913043439388275)
[2025-02-04 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:27][root][INFO] - Training Epoch: 2/2, step 22939/23838 completed (loss: 3.217806577682495, acc: 0.3207547068595886)
[2025-02-04 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:28][root][INFO] - Training Epoch: 2/2, step 22940/23838 completed (loss: 2.2805473804473877, acc: 0.44117647409439087)
[2025-02-04 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:28][root][INFO] - Training Epoch: 2/2, step 22941/23838 completed (loss: 2.7015810012817383, acc: 0.5185185074806213)
[2025-02-04 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:29][root][INFO] - Training Epoch: 2/2, step 22942/23838 completed (loss: 2.381519079208374, acc: 0.517241358757019)
[2025-02-04 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:29][root][INFO] - Training Epoch: 2/2, step 22943/23838 completed (loss: 2.223569869995117, acc: 0.4000000059604645)
[2025-02-04 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:29][root][INFO] - Training Epoch: 2/2, step 22944/23838 completed (loss: 2.1556501388549805, acc: 0.6551724076271057)
[2025-02-04 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:30][root][INFO] - Training Epoch: 2/2, step 22945/23838 completed (loss: 2.1584248542785645, acc: 0.5)
[2025-02-04 01:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:30][root][INFO] - Training Epoch: 2/2, step 22946/23838 completed (loss: 3.416712522506714, acc: 0.5277777910232544)
[2025-02-04 01:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:31][root][INFO] - Training Epoch: 2/2, step 22947/23838 completed (loss: 1.9245280027389526, acc: 0.71875)
[2025-02-04 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:31][root][INFO] - Training Epoch: 2/2, step 22948/23838 completed (loss: 2.4740850925445557, acc: 0.44680851697921753)
[2025-02-04 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:31][root][INFO] - Training Epoch: 2/2, step 22949/23838 completed (loss: 2.666378974914551, acc: 0.4871794879436493)
[2025-02-04 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:32][root][INFO] - Training Epoch: 2/2, step 22950/23838 completed (loss: 3.079972267150879, acc: 0.3333333432674408)
[2025-02-04 01:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:32][root][INFO] - Training Epoch: 2/2, step 22951/23838 completed (loss: 3.354006767272949, acc: 0.3243243098258972)
[2025-02-04 01:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:33][root][INFO] - Training Epoch: 2/2, step 22952/23838 completed (loss: 1.4108420610427856, acc: 0.7777777910232544)
[2025-02-04 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:33][root][INFO] - Training Epoch: 2/2, step 22953/23838 completed (loss: 2.042076587677002, acc: 0.5862069129943848)
[2025-02-04 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:33][root][INFO] - Training Epoch: 2/2, step 22954/23838 completed (loss: 1.6826791763305664, acc: 0.6000000238418579)
[2025-02-04 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:34][root][INFO] - Training Epoch: 2/2, step 22955/23838 completed (loss: 3.1031768321990967, acc: 0.3414634168148041)
[2025-02-04 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:34][root][INFO] - Training Epoch: 2/2, step 22956/23838 completed (loss: 3.334973096847534, acc: 0.31111112236976624)
[2025-02-04 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:35][root][INFO] - Training Epoch: 2/2, step 22957/23838 completed (loss: 3.1559066772460938, acc: 0.4137931168079376)
[2025-02-04 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:35][root][INFO] - Training Epoch: 2/2, step 22958/23838 completed (loss: 3.734457492828369, acc: 0.36666667461395264)
[2025-02-04 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:36][root][INFO] - Training Epoch: 2/2, step 22959/23838 completed (loss: 2.4806580543518066, acc: 0.4878048896789551)
[2025-02-04 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:36][root][INFO] - Training Epoch: 2/2, step 22960/23838 completed (loss: 2.3389720916748047, acc: 0.4285714328289032)
[2025-02-04 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:36][root][INFO] - Training Epoch: 2/2, step 22961/23838 completed (loss: 3.7461581230163574, acc: 0.3529411852359772)
[2025-02-04 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:37][root][INFO] - Training Epoch: 2/2, step 22962/23838 completed (loss: 3.337765693664551, acc: 0.3513513505458832)
[2025-02-04 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:37][root][INFO] - Training Epoch: 2/2, step 22963/23838 completed (loss: 3.599266529083252, acc: 0.42105263471603394)
[2025-02-04 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:38][root][INFO] - Training Epoch: 2/2, step 22964/23838 completed (loss: 3.6945667266845703, acc: 0.3684210479259491)
[2025-02-04 01:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:38][root][INFO] - Training Epoch: 2/2, step 22965/23838 completed (loss: 3.848173141479492, acc: 0.3333333432674408)
[2025-02-04 01:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:39][root][INFO] - Training Epoch: 2/2, step 22966/23838 completed (loss: 3.537475347518921, acc: 0.3142857253551483)
[2025-02-04 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:39][root][INFO] - Training Epoch: 2/2, step 22967/23838 completed (loss: 2.5255579948425293, acc: 0.4000000059604645)
[2025-02-04 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:40][root][INFO] - Training Epoch: 2/2, step 22968/23838 completed (loss: 4.662881851196289, acc: 0.3333333432674408)
[2025-02-04 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:40][root][INFO] - Training Epoch: 2/2, step 22969/23838 completed (loss: 5.254471778869629, acc: 0.24242424964904785)
[2025-02-04 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:41][root][INFO] - Training Epoch: 2/2, step 22970/23838 completed (loss: 3.463972806930542, acc: 0.3214285671710968)
[2025-02-04 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:41][root][INFO] - Training Epoch: 2/2, step 22971/23838 completed (loss: 3.5318455696105957, acc: 0.2432432472705841)
[2025-02-04 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:41][root][INFO] - Training Epoch: 2/2, step 22972/23838 completed (loss: 3.2094979286193848, acc: 0.52173912525177)
[2025-02-04 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:42][root][INFO] - Training Epoch: 2/2, step 22973/23838 completed (loss: 2.8010518550872803, acc: 0.4444444477558136)
[2025-02-04 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:42][root][INFO] - Training Epoch: 2/2, step 22974/23838 completed (loss: 3.6784427165985107, acc: 0.3333333432674408)
[2025-02-04 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:43][root][INFO] - Training Epoch: 2/2, step 22975/23838 completed (loss: 2.286137819290161, acc: 0.5714285969734192)
[2025-02-04 01:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:43][root][INFO] - Training Epoch: 2/2, step 22976/23838 completed (loss: 3.414412498474121, acc: 0.3478260934352875)
[2025-02-04 01:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:43][root][INFO] - Training Epoch: 2/2, step 22977/23838 completed (loss: 3.391791343688965, acc: 0.30434781312942505)
[2025-02-04 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:44][root][INFO] - Training Epoch: 2/2, step 22978/23838 completed (loss: 3.0964460372924805, acc: 0.36666667461395264)
[2025-02-04 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:45][root][INFO] - Training Epoch: 2/2, step 22979/23838 completed (loss: 3.681159734725952, acc: 0.29032257199287415)
[2025-02-04 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:45][root][INFO] - Training Epoch: 2/2, step 22980/23838 completed (loss: 4.157913684844971, acc: 0.1794871836900711)
[2025-02-04 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:45][root][INFO] - Training Epoch: 2/2, step 22981/23838 completed (loss: 3.705378293991089, acc: 0.28205129504203796)
[2025-02-04 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:46][root][INFO] - Training Epoch: 2/2, step 22982/23838 completed (loss: 1.765358805656433, acc: 0.6363636255264282)
[2025-02-04 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:46][root][INFO] - Training Epoch: 2/2, step 22983/23838 completed (loss: 3.367233991622925, acc: 0.375)
[2025-02-04 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:47][root][INFO] - Training Epoch: 2/2, step 22984/23838 completed (loss: 2.846405029296875, acc: 0.36000001430511475)
[2025-02-04 01:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:47][root][INFO] - Training Epoch: 2/2, step 22985/23838 completed (loss: 2.9567008018493652, acc: 0.42424243688583374)
[2025-02-04 01:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:48][root][INFO] - Training Epoch: 2/2, step 22986/23838 completed (loss: 3.087397336959839, acc: 0.375)
[2025-02-04 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:48][root][INFO] - Training Epoch: 2/2, step 22987/23838 completed (loss: 2.913320779800415, acc: 0.32258063554763794)
[2025-02-04 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:48][root][INFO] - Training Epoch: 2/2, step 22988/23838 completed (loss: 2.22110652923584, acc: 0.5789473652839661)
[2025-02-04 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:49][root][INFO] - Training Epoch: 2/2, step 22989/23838 completed (loss: 3.369885206222534, acc: 0.31111112236976624)
[2025-02-04 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:49][root][INFO] - Training Epoch: 2/2, step 22990/23838 completed (loss: 2.92734956741333, acc: 0.37837839126586914)
[2025-02-04 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:50][root][INFO] - Training Epoch: 2/2, step 22991/23838 completed (loss: 2.982316732406616, acc: 0.5652173757553101)
[2025-02-04 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:50][root][INFO] - Training Epoch: 2/2, step 22992/23838 completed (loss: 2.7942442893981934, acc: 0.40740740299224854)
[2025-02-04 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:51][root][INFO] - Training Epoch: 2/2, step 22993/23838 completed (loss: 3.0532596111297607, acc: 0.3571428656578064)
[2025-02-04 01:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:51][root][INFO] - Training Epoch: 2/2, step 22994/23838 completed (loss: 4.964953422546387, acc: 0.17142857611179352)
[2025-02-04 01:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:52][root][INFO] - Training Epoch: 2/2, step 22995/23838 completed (loss: 4.586002826690674, acc: 0.3030303120613098)
[2025-02-04 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:52][root][INFO] - Training Epoch: 2/2, step 22996/23838 completed (loss: 2.794660806655884, acc: 0.48571428656578064)
[2025-02-04 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:52][root][INFO] - Training Epoch: 2/2, step 22997/23838 completed (loss: 3.5837347507476807, acc: 0.35555556416511536)
[2025-02-04 01:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:53][root][INFO] - Training Epoch: 2/2, step 22998/23838 completed (loss: 3.5043718814849854, acc: 0.32692307233810425)
[2025-02-04 01:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:54][root][INFO] - Training Epoch: 2/2, step 22999/23838 completed (loss: 4.510909080505371, acc: 0.2857142984867096)
[2025-02-04 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:54][root][INFO] - Training Epoch: 2/2, step 23000/23838 completed (loss: 3.4488863945007324, acc: 0.39024388790130615)
[2025-02-04 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:55][root][INFO] - Training Epoch: 2/2, step 23001/23838 completed (loss: 3.5030832290649414, acc: 0.3255814015865326)
[2025-02-04 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:55][root][INFO] - Training Epoch: 2/2, step 23002/23838 completed (loss: 2.1308529376983643, acc: 0.6153846383094788)
[2025-02-04 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:56][root][INFO] - Training Epoch: 2/2, step 23003/23838 completed (loss: 2.956282615661621, acc: 0.4545454680919647)
[2025-02-04 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:56][root][INFO] - Training Epoch: 2/2, step 23004/23838 completed (loss: 3.5666632652282715, acc: 0.43589743971824646)
[2025-02-04 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:57][root][INFO] - Training Epoch: 2/2, step 23005/23838 completed (loss: 2.90539288520813, acc: 0.3636363744735718)
[2025-02-04 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:57][root][INFO] - Training Epoch: 2/2, step 23006/23838 completed (loss: 2.683717966079712, acc: 0.47999998927116394)
[2025-02-04 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:58][root][INFO] - Training Epoch: 2/2, step 23007/23838 completed (loss: 2.812631130218506, acc: 0.44736841320991516)
[2025-02-04 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:58][root][INFO] - Training Epoch: 2/2, step 23008/23838 completed (loss: 2.099337577819824, acc: 0.5588235259056091)
[2025-02-04 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:59][root][INFO] - Training Epoch: 2/2, step 23009/23838 completed (loss: 2.482036828994751, acc: 0.4000000059604645)
[2025-02-04 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:10:59][root][INFO] - Training Epoch: 2/2, step 23010/23838 completed (loss: 3.148221492767334, acc: 0.3499999940395355)
[2025-02-04 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:00][root][INFO] - Training Epoch: 2/2, step 23011/23838 completed (loss: 1.8640223741531372, acc: 0.6190476417541504)
[2025-02-04 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:00][root][INFO] - Training Epoch: 2/2, step 23012/23838 completed (loss: 1.088443636894226, acc: 0.6499999761581421)
[2025-02-04 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:01][root][INFO] - Training Epoch: 2/2, step 23013/23838 completed (loss: 2.764828681945801, acc: 0.38461539149284363)
[2025-02-04 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:01][root][INFO] - Training Epoch: 2/2, step 23014/23838 completed (loss: 3.177252769470215, acc: 0.3529411852359772)
[2025-02-04 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:02][root][INFO] - Training Epoch: 2/2, step 23015/23838 completed (loss: 2.753777503967285, acc: 0.44999998807907104)
[2025-02-04 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:03][root][INFO] - Training Epoch: 2/2, step 23016/23838 completed (loss: 3.0782222747802734, acc: 0.48571428656578064)
[2025-02-04 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:03][root][INFO] - Training Epoch: 2/2, step 23017/23838 completed (loss: 2.799151659011841, acc: 0.4285714328289032)
[2025-02-04 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:04][root][INFO] - Training Epoch: 2/2, step 23018/23838 completed (loss: 2.82112193107605, acc: 0.375)
[2025-02-04 01:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:04][root][INFO] - Training Epoch: 2/2, step 23019/23838 completed (loss: 2.700934648513794, acc: 0.5)
[2025-02-04 01:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:05][root][INFO] - Training Epoch: 2/2, step 23020/23838 completed (loss: 3.1753482818603516, acc: 0.3870967626571655)
[2025-02-04 01:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:05][root][INFO] - Training Epoch: 2/2, step 23021/23838 completed (loss: 2.4035964012145996, acc: 0.4482758641242981)
[2025-02-04 01:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:05][root][INFO] - Training Epoch: 2/2, step 23022/23838 completed (loss: 2.352029800415039, acc: 0.6086956262588501)
[2025-02-04 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:06][root][INFO] - Training Epoch: 2/2, step 23023/23838 completed (loss: 3.304882287979126, acc: 0.3636363744735718)
[2025-02-04 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:06][root][INFO] - Training Epoch: 2/2, step 23024/23838 completed (loss: 2.6203086376190186, acc: 0.52173912525177)
[2025-02-04 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:07][root][INFO] - Training Epoch: 2/2, step 23025/23838 completed (loss: 2.3576416969299316, acc: 0.48275861144065857)
[2025-02-04 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:07][root][INFO] - Training Epoch: 2/2, step 23026/23838 completed (loss: 2.916879892349243, acc: 0.34375)
[2025-02-04 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:08][root][INFO] - Training Epoch: 2/2, step 23027/23838 completed (loss: 2.9182803630828857, acc: 0.4193548262119293)
[2025-02-04 01:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:08][root][INFO] - Training Epoch: 2/2, step 23028/23838 completed (loss: 3.2112319469451904, acc: 0.36666667461395264)
[2025-02-04 01:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:08][root][INFO] - Training Epoch: 2/2, step 23029/23838 completed (loss: 2.0698800086975098, acc: 0.625)
[2025-02-04 01:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:09][root][INFO] - Training Epoch: 2/2, step 23030/23838 completed (loss: 3.582995653152466, acc: 0.3684210479259491)
[2025-02-04 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:09][root][INFO] - Training Epoch: 2/2, step 23031/23838 completed (loss: 2.91062593460083, acc: 0.4736842215061188)
[2025-02-04 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:10][root][INFO] - Training Epoch: 2/2, step 23032/23838 completed (loss: 2.452723264694214, acc: 0.5333333611488342)
[2025-02-04 01:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:10][root][INFO] - Training Epoch: 2/2, step 23033/23838 completed (loss: 1.9017633199691772, acc: 0.6000000238418579)
[2025-02-04 01:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:11][root][INFO] - Training Epoch: 2/2, step 23034/23838 completed (loss: 2.7173407077789307, acc: 0.5)
[2025-02-04 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:11][root][INFO] - Training Epoch: 2/2, step 23035/23838 completed (loss: 1.8672727346420288, acc: 0.5)
[2025-02-04 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:11][root][INFO] - Training Epoch: 2/2, step 23036/23838 completed (loss: 2.8336164951324463, acc: 0.5)
[2025-02-04 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:12][root][INFO] - Training Epoch: 2/2, step 23037/23838 completed (loss: 1.6290390491485596, acc: 0.5454545617103577)
[2025-02-04 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:12][root][INFO] - Training Epoch: 2/2, step 23038/23838 completed (loss: 2.8127849102020264, acc: 0.40740740299224854)
[2025-02-04 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:13][root][INFO] - Training Epoch: 2/2, step 23039/23838 completed (loss: 2.7033934593200684, acc: 0.550000011920929)
[2025-02-04 01:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:13][root][INFO] - Training Epoch: 2/2, step 23040/23838 completed (loss: 1.608100175857544, acc: 0.6499999761581421)
[2025-02-04 01:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:14][root][INFO] - Training Epoch: 2/2, step 23041/23838 completed (loss: 1.911360502243042, acc: 0.5517241358757019)
[2025-02-04 01:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:14][root][INFO] - Training Epoch: 2/2, step 23042/23838 completed (loss: 1.8517403602600098, acc: 0.5909090638160706)
[2025-02-04 01:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:14][root][INFO] - Training Epoch: 2/2, step 23043/23838 completed (loss: 1.8114384412765503, acc: 0.6428571343421936)
[2025-02-04 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:15][root][INFO] - Training Epoch: 2/2, step 23044/23838 completed (loss: 1.3700016736984253, acc: 0.7857142686843872)
[2025-02-04 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:15][root][INFO] - Training Epoch: 2/2, step 23045/23838 completed (loss: 3.9039430618286133, acc: 0.3461538553237915)
[2025-02-04 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:16][root][INFO] - Training Epoch: 2/2, step 23046/23838 completed (loss: 2.5262537002563477, acc: 0.5769230723381042)
[2025-02-04 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:16][root][INFO] - Training Epoch: 2/2, step 23047/23838 completed (loss: 2.281230926513672, acc: 0.529411792755127)
[2025-02-04 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:16][root][INFO] - Training Epoch: 2/2, step 23048/23838 completed (loss: 1.329877495765686, acc: 0.75)
[2025-02-04 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:17][root][INFO] - Training Epoch: 2/2, step 23049/23838 completed (loss: 3.102475881576538, acc: 0.3333333432674408)
[2025-02-04 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:17][root][INFO] - Training Epoch: 2/2, step 23050/23838 completed (loss: 1.7760800123214722, acc: 0.5454545617103577)
[2025-02-04 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:18][root][INFO] - Training Epoch: 2/2, step 23051/23838 completed (loss: 3.437891721725464, acc: 0.3461538553237915)
[2025-02-04 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:18][root][INFO] - Training Epoch: 2/2, step 23052/23838 completed (loss: 1.787266492843628, acc: 0.529411792755127)
[2025-02-04 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:19][root][INFO] - Training Epoch: 2/2, step 23053/23838 completed (loss: 2.9301717281341553, acc: 0.3870967626571655)
[2025-02-04 01:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:19][root][INFO] - Training Epoch: 2/2, step 23054/23838 completed (loss: 2.2671144008636475, acc: 0.5652173757553101)
[2025-02-04 01:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:20][root][INFO] - Training Epoch: 2/2, step 23055/23838 completed (loss: 2.577126979827881, acc: 0.4285714328289032)
[2025-02-04 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:20][root][INFO] - Training Epoch: 2/2, step 23056/23838 completed (loss: 3.59260892868042, acc: 0.3333333432674408)
[2025-02-04 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:20][root][INFO] - Training Epoch: 2/2, step 23057/23838 completed (loss: 2.364985942840576, acc: 0.6666666865348816)
[2025-02-04 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:21][root][INFO] - Training Epoch: 2/2, step 23058/23838 completed (loss: 3.7955410480499268, acc: 0.4375)
[2025-02-04 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:21][root][INFO] - Training Epoch: 2/2, step 23059/23838 completed (loss: 3.111663579940796, acc: 0.4166666567325592)
[2025-02-04 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:22][root][INFO] - Training Epoch: 2/2, step 23060/23838 completed (loss: 2.8946895599365234, acc: 0.4000000059604645)
[2025-02-04 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:22][root][INFO] - Training Epoch: 2/2, step 23061/23838 completed (loss: 2.0555951595306396, acc: 0.625)
[2025-02-04 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:23][root][INFO] - Training Epoch: 2/2, step 23062/23838 completed (loss: 2.5985519886016846, acc: 0.4333333373069763)
[2025-02-04 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:23][root][INFO] - Training Epoch: 2/2, step 23063/23838 completed (loss: 2.266624927520752, acc: 0.6190476417541504)
[2025-02-04 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:24][root][INFO] - Training Epoch: 2/2, step 23064/23838 completed (loss: 3.054527997970581, acc: 0.3636363744735718)
[2025-02-04 01:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:24][root][INFO] - Training Epoch: 2/2, step 23065/23838 completed (loss: 3.6045448780059814, acc: 0.2916666567325592)
[2025-02-04 01:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:25][root][INFO] - Training Epoch: 2/2, step 23066/23838 completed (loss: 3.146312952041626, acc: 0.380952388048172)
[2025-02-04 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:25][root][INFO] - Training Epoch: 2/2, step 23067/23838 completed (loss: 3.1472952365875244, acc: 0.4166666567325592)
[2025-02-04 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:25][root][INFO] - Training Epoch: 2/2, step 23068/23838 completed (loss: 2.249305486679077, acc: 0.4117647111415863)
[2025-02-04 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:26][root][INFO] - Training Epoch: 2/2, step 23069/23838 completed (loss: 3.764857292175293, acc: 0.3333333432674408)
[2025-02-04 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:26][root][INFO] - Training Epoch: 2/2, step 23070/23838 completed (loss: 2.8939106464385986, acc: 0.37837839126586914)
[2025-02-04 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:27][root][INFO] - Training Epoch: 2/2, step 23071/23838 completed (loss: 2.862661123275757, acc: 0.5)
[2025-02-04 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:27][root][INFO] - Training Epoch: 2/2, step 23072/23838 completed (loss: 3.3125228881835938, acc: 0.47999998927116394)
[2025-02-04 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:28][root][INFO] - Training Epoch: 2/2, step 23073/23838 completed (loss: 2.426605224609375, acc: 0.5333333611488342)
[2025-02-04 01:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:28][root][INFO] - Training Epoch: 2/2, step 23074/23838 completed (loss: 1.5644073486328125, acc: 0.6875)
[2025-02-04 01:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:29][root][INFO] - Training Epoch: 2/2, step 23075/23838 completed (loss: 2.2362687587738037, acc: 0.5277777910232544)
[2025-02-04 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:29][root][INFO] - Training Epoch: 2/2, step 23076/23838 completed (loss: 2.366398572921753, acc: 0.4516128897666931)
[2025-02-04 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:30][root][INFO] - Training Epoch: 2/2, step 23077/23838 completed (loss: 2.734525203704834, acc: 0.4871794879436493)
[2025-02-04 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:30][root][INFO] - Training Epoch: 2/2, step 23078/23838 completed (loss: 2.8299779891967773, acc: 0.2777777910232544)
[2025-02-04 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:30][root][INFO] - Training Epoch: 2/2, step 23079/23838 completed (loss: 3.4607787132263184, acc: 0.4761904776096344)
[2025-02-04 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:31][root][INFO] - Training Epoch: 2/2, step 23080/23838 completed (loss: 1.162732720375061, acc: 0.695652186870575)
[2025-02-04 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:31][root][INFO] - Training Epoch: 2/2, step 23081/23838 completed (loss: 1.8462297916412354, acc: 0.5714285969734192)
[2025-02-04 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:32][root][INFO] - Training Epoch: 2/2, step 23082/23838 completed (loss: 4.906423568725586, acc: 0.3404255211353302)
[2025-02-04 01:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:32][root][INFO] - Training Epoch: 2/2, step 23083/23838 completed (loss: 2.3060951232910156, acc: 0.6428571343421936)
[2025-02-04 01:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:33][root][INFO] - Training Epoch: 2/2, step 23084/23838 completed (loss: 3.097500801086426, acc: 0.25)
[2025-02-04 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:33][root][INFO] - Training Epoch: 2/2, step 23085/23838 completed (loss: 3.867532968521118, acc: 0.25)
[2025-02-04 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:33][root][INFO] - Training Epoch: 2/2, step 23086/23838 completed (loss: 2.775130033493042, acc: 0.5199999809265137)
[2025-02-04 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:34][root][INFO] - Training Epoch: 2/2, step 23087/23838 completed (loss: 2.5371875762939453, acc: 0.47999998927116394)
[2025-02-04 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:35][root][INFO] - Training Epoch: 2/2, step 23088/23838 completed (loss: 3.2648751735687256, acc: 0.4000000059604645)
[2025-02-04 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:35][root][INFO] - Training Epoch: 2/2, step 23089/23838 completed (loss: 2.95569109916687, acc: 0.3888888955116272)
[2025-02-04 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:36][root][INFO] - Training Epoch: 2/2, step 23090/23838 completed (loss: 3.2367615699768066, acc: 0.40909090638160706)
[2025-02-04 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:36][root][INFO] - Training Epoch: 2/2, step 23091/23838 completed (loss: 4.214829921722412, acc: 0.3448275923728943)
[2025-02-04 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:37][root][INFO] - Training Epoch: 2/2, step 23092/23838 completed (loss: 3.2872836589813232, acc: 0.5555555820465088)
[2025-02-04 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:37][root][INFO] - Training Epoch: 2/2, step 23093/23838 completed (loss: 3.678758144378662, acc: 0.380952388048172)
[2025-02-04 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:37][root][INFO] - Training Epoch: 2/2, step 23094/23838 completed (loss: 3.2821311950683594, acc: 0.40909090638160706)
[2025-02-04 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:38][root][INFO] - Training Epoch: 2/2, step 23095/23838 completed (loss: 3.0609612464904785, acc: 0.4642857015132904)
[2025-02-04 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:38][root][INFO] - Training Epoch: 2/2, step 23096/23838 completed (loss: 2.710747480392456, acc: 0.4117647111415863)
[2025-02-04 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:39][root][INFO] - Training Epoch: 2/2, step 23097/23838 completed (loss: 2.252955913543701, acc: 0.375)
[2025-02-04 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:39][root][INFO] - Training Epoch: 2/2, step 23098/23838 completed (loss: 2.8935697078704834, acc: 0.4615384638309479)
[2025-02-04 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:40][root][INFO] - Training Epoch: 2/2, step 23099/23838 completed (loss: 2.834977626800537, acc: 0.4000000059604645)
[2025-02-04 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:40][root][INFO] - Training Epoch: 2/2, step 23100/23838 completed (loss: 2.691119432449341, acc: 0.36000001430511475)
[2025-02-04 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:41][root][INFO] - Training Epoch: 2/2, step 23101/23838 completed (loss: 3.369544267654419, acc: 0.3636363744735718)
[2025-02-04 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:41][root][INFO] - Training Epoch: 2/2, step 23102/23838 completed (loss: 2.6972877979278564, acc: 0.4399999976158142)
[2025-02-04 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:42][root][INFO] - Training Epoch: 2/2, step 23103/23838 completed (loss: 3.910304069519043, acc: 0.31578946113586426)
[2025-02-04 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:42][root][INFO] - Training Epoch: 2/2, step 23104/23838 completed (loss: 2.901937484741211, acc: 0.3571428656578064)
[2025-02-04 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:42][root][INFO] - Training Epoch: 2/2, step 23105/23838 completed (loss: 3.0960230827331543, acc: 0.4444444477558136)
[2025-02-04 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:43][root][INFO] - Training Epoch: 2/2, step 23106/23838 completed (loss: 2.5989990234375, acc: 0.5)
[2025-02-04 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:43][root][INFO] - Training Epoch: 2/2, step 23107/23838 completed (loss: 3.5338854789733887, acc: 0.44999998807907104)
[2025-02-04 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:44][root][INFO] - Training Epoch: 2/2, step 23108/23838 completed (loss: 3.3659987449645996, acc: 0.38235294818878174)
[2025-02-04 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:44][root][INFO] - Training Epoch: 2/2, step 23109/23838 completed (loss: 2.1501708030700684, acc: 0.4736842215061188)
[2025-02-04 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:45][root][INFO] - Training Epoch: 2/2, step 23110/23838 completed (loss: 3.4312052726745605, acc: 0.3499999940395355)
[2025-02-04 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:45][root][INFO] - Training Epoch: 2/2, step 23111/23838 completed (loss: 3.8937089443206787, acc: 0.2777777910232544)
[2025-02-04 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:46][root][INFO] - Training Epoch: 2/2, step 23112/23838 completed (loss: 3.73817777633667, acc: 0.3125)
[2025-02-04 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:46][root][INFO] - Training Epoch: 2/2, step 23113/23838 completed (loss: 2.871488332748413, acc: 0.3636363744735718)
[2025-02-04 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:47][root][INFO] - Training Epoch: 2/2, step 23114/23838 completed (loss: 3.0964701175689697, acc: 0.42105263471603394)
[2025-02-04 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:47][root][INFO] - Training Epoch: 2/2, step 23115/23838 completed (loss: 2.998795509338379, acc: 0.3499999940395355)
[2025-02-04 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:47][root][INFO] - Training Epoch: 2/2, step 23116/23838 completed (loss: 2.9130282402038574, acc: 0.4000000059604645)
[2025-02-04 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:48][root][INFO] - Training Epoch: 2/2, step 23117/23838 completed (loss: 1.9847297668457031, acc: 0.5833333134651184)
[2025-02-04 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:48][root][INFO] - Training Epoch: 2/2, step 23118/23838 completed (loss: 2.446406126022339, acc: 0.5416666865348816)
[2025-02-04 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:49][root][INFO] - Training Epoch: 2/2, step 23119/23838 completed (loss: 1.4129745960235596, acc: 0.6875)
[2025-02-04 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:49][root][INFO] - Training Epoch: 2/2, step 23120/23838 completed (loss: 1.7442978620529175, acc: 0.5)
[2025-02-04 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:50][root][INFO] - Training Epoch: 2/2, step 23121/23838 completed (loss: 2.5071425437927246, acc: 0.5)
[2025-02-04 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:50][root][INFO] - Training Epoch: 2/2, step 23122/23838 completed (loss: 3.058413505554199, acc: 0.5263158082962036)
[2025-02-04 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:51][root][INFO] - Training Epoch: 2/2, step 23123/23838 completed (loss: 2.6582090854644775, acc: 0.4000000059604645)
[2025-02-04 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:51][root][INFO] - Training Epoch: 2/2, step 23124/23838 completed (loss: 2.921628475189209, acc: 0.4375)
[2025-02-04 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:52][root][INFO] - Training Epoch: 2/2, step 23125/23838 completed (loss: 3.4249629974365234, acc: 0.3333333432674408)
[2025-02-04 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:52][root][INFO] - Training Epoch: 2/2, step 23126/23838 completed (loss: 3.727306365966797, acc: 0.3499999940395355)
[2025-02-04 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:52][root][INFO] - Training Epoch: 2/2, step 23127/23838 completed (loss: 3.0048584938049316, acc: 0.3243243098258972)
[2025-02-04 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:53][root][INFO] - Training Epoch: 2/2, step 23128/23838 completed (loss: 2.9630346298217773, acc: 0.3076923191547394)
[2025-02-04 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:53][root][INFO] - Training Epoch: 2/2, step 23129/23838 completed (loss: 3.453983783721924, acc: 0.32692307233810425)
[2025-02-04 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:54][root][INFO] - Training Epoch: 2/2, step 23130/23838 completed (loss: 4.571849822998047, acc: 0.2800000011920929)
[2025-02-04 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:54][root][INFO] - Training Epoch: 2/2, step 23131/23838 completed (loss: 3.713796615600586, acc: 0.2916666567325592)
[2025-02-04 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:55][root][INFO] - Training Epoch: 2/2, step 23132/23838 completed (loss: 3.2617907524108887, acc: 0.3513513505458832)
[2025-02-04 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:55][root][INFO] - Training Epoch: 2/2, step 23133/23838 completed (loss: 4.740362644195557, acc: 0.23255814611911774)
[2025-02-04 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:56][root][INFO] - Training Epoch: 2/2, step 23134/23838 completed (loss: 3.4512979984283447, acc: 0.19230769574642181)
[2025-02-04 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:56][root][INFO] - Training Epoch: 2/2, step 23135/23838 completed (loss: 2.8536200523376465, acc: 0.4516128897666931)
[2025-02-04 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:56][root][INFO] - Training Epoch: 2/2, step 23136/23838 completed (loss: 3.4204068183898926, acc: 0.3333333432674408)
[2025-02-04 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:57][root][INFO] - Training Epoch: 2/2, step 23137/23838 completed (loss: 2.746948003768921, acc: 0.3636363744735718)
[2025-02-04 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:57][root][INFO] - Training Epoch: 2/2, step 23138/23838 completed (loss: 3.5450353622436523, acc: 0.2888889014720917)
[2025-02-04 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:58][root][INFO] - Training Epoch: 2/2, step 23139/23838 completed (loss: 2.785776138305664, acc: 0.3928571343421936)
[2025-02-04 01:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:58][root][INFO] - Training Epoch: 2/2, step 23140/23838 completed (loss: 3.217027187347412, acc: 0.29032257199287415)
[2025-02-04 01:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:59][root][INFO] - Training Epoch: 2/2, step 23141/23838 completed (loss: 3.114438056945801, acc: 0.30000001192092896)
[2025-02-04 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:59][root][INFO] - Training Epoch: 2/2, step 23142/23838 completed (loss: 2.999221086502075, acc: 0.37931033968925476)
[2025-02-04 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:11:59][root][INFO] - Training Epoch: 2/2, step 23143/23838 completed (loss: 2.546632766723633, acc: 0.4761904776096344)
[2025-02-04 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:00][root][INFO] - Training Epoch: 2/2, step 23144/23838 completed (loss: 2.572638511657715, acc: 0.3333333432674408)
[2025-02-04 01:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:00][root][INFO] - Training Epoch: 2/2, step 23145/23838 completed (loss: 2.7237484455108643, acc: 0.5)
[2025-02-04 01:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:01][root][INFO] - Training Epoch: 2/2, step 23146/23838 completed (loss: 3.089796543121338, acc: 0.3529411852359772)
[2025-02-04 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:01][root][INFO] - Training Epoch: 2/2, step 23147/23838 completed (loss: 3.448863983154297, acc: 0.3333333432674408)
[2025-02-04 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:01][root][INFO] - Training Epoch: 2/2, step 23148/23838 completed (loss: 2.573657989501953, acc: 0.38235294818878174)
[2025-02-04 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:02][root][INFO] - Training Epoch: 2/2, step 23149/23838 completed (loss: 3.8007521629333496, acc: 0.19354838132858276)
[2025-02-04 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:02][root][INFO] - Training Epoch: 2/2, step 23150/23838 completed (loss: 4.090873718261719, acc: 0.24390244483947754)
[2025-02-04 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:03][root][INFO] - Training Epoch: 2/2, step 23151/23838 completed (loss: 3.1700894832611084, acc: 0.35483869910240173)
[2025-02-04 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:03][root][INFO] - Training Epoch: 2/2, step 23152/23838 completed (loss: 4.409811973571777, acc: 0.27272728085517883)
[2025-02-04 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:04][root][INFO] - Training Epoch: 2/2, step 23153/23838 completed (loss: 2.8715760707855225, acc: 0.44186046719551086)
[2025-02-04 01:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:04][root][INFO] - Training Epoch: 2/2, step 23154/23838 completed (loss: 2.887242078781128, acc: 0.4285714328289032)
[2025-02-04 01:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:05][root][INFO] - Training Epoch: 2/2, step 23155/23838 completed (loss: 3.109408378601074, acc: 0.3333333432674408)
[2025-02-04 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:05][root][INFO] - Training Epoch: 2/2, step 23156/23838 completed (loss: 2.9920806884765625, acc: 0.3529411852359772)
[2025-02-04 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:05][root][INFO] - Training Epoch: 2/2, step 23157/23838 completed (loss: 3.4062018394470215, acc: 0.21212121844291687)
[2025-02-04 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:06][root][INFO] - Training Epoch: 2/2, step 23158/23838 completed (loss: 2.7924537658691406, acc: 0.38461539149284363)
[2025-02-04 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:06][root][INFO] - Training Epoch: 2/2, step 23159/23838 completed (loss: 3.4949538707733154, acc: 0.3414634168148041)
[2025-02-04 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:07][root][INFO] - Training Epoch: 2/2, step 23160/23838 completed (loss: 2.057748794555664, acc: 0.5)
[2025-02-04 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:07][root][INFO] - Training Epoch: 2/2, step 23161/23838 completed (loss: 2.486506223678589, acc: 0.5357142686843872)
[2025-02-04 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:08][root][INFO] - Training Epoch: 2/2, step 23162/23838 completed (loss: 3.643754243850708, acc: 0.34210526943206787)
[2025-02-04 01:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:08][root][INFO] - Training Epoch: 2/2, step 23163/23838 completed (loss: 1.6829750537872314, acc: 0.52173912525177)
[2025-02-04 01:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:09][root][INFO] - Training Epoch: 2/2, step 23164/23838 completed (loss: 3.34464955329895, acc: 0.40909090638160706)
[2025-02-04 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:09][root][INFO] - Training Epoch: 2/2, step 23165/23838 completed (loss: 2.6948153972625732, acc: 0.4285714328289032)
[2025-02-04 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:10][root][INFO] - Training Epoch: 2/2, step 23166/23838 completed (loss: 3.0697383880615234, acc: 0.4444444477558136)
[2025-02-04 01:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:10][root][INFO] - Training Epoch: 2/2, step 23167/23838 completed (loss: 3.741072654724121, acc: 0.3125)
[2025-02-04 01:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:10][root][INFO] - Training Epoch: 2/2, step 23168/23838 completed (loss: 1.9913169145584106, acc: 0.5263158082962036)
[2025-02-04 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:11][root][INFO] - Training Epoch: 2/2, step 23169/23838 completed (loss: 2.916888475418091, acc: 0.4545454680919647)
[2025-02-04 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:11][root][INFO] - Training Epoch: 2/2, step 23170/23838 completed (loss: 2.0356173515319824, acc: 0.5862069129943848)
[2025-02-04 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:12][root][INFO] - Training Epoch: 2/2, step 23171/23838 completed (loss: 2.112504720687866, acc: 0.48275861144065857)
[2025-02-04 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:12][root][INFO] - Training Epoch: 2/2, step 23172/23838 completed (loss: 2.8233563899993896, acc: 0.3478260934352875)
[2025-02-04 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:13][root][INFO] - Training Epoch: 2/2, step 23173/23838 completed (loss: 2.7810611724853516, acc: 0.3928571343421936)
[2025-02-04 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:13][root][INFO] - Training Epoch: 2/2, step 23174/23838 completed (loss: 1.90789794921875, acc: 0.5142857432365417)
[2025-02-04 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:13][root][INFO] - Training Epoch: 2/2, step 23175/23838 completed (loss: 2.6368680000305176, acc: 0.48148149251937866)
[2025-02-04 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:14][root][INFO] - Training Epoch: 2/2, step 23176/23838 completed (loss: 2.593911647796631, acc: 0.4324324429035187)
[2025-02-04 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:14][root][INFO] - Training Epoch: 2/2, step 23177/23838 completed (loss: 2.6282358169555664, acc: 0.42307692766189575)
[2025-02-04 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:15][root][INFO] - Training Epoch: 2/2, step 23178/23838 completed (loss: 2.914396047592163, acc: 0.38461539149284363)
[2025-02-04 01:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:15][root][INFO] - Training Epoch: 2/2, step 23179/23838 completed (loss: 3.1787593364715576, acc: 0.380952388048172)
[2025-02-04 01:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:15][root][INFO] - Training Epoch: 2/2, step 23180/23838 completed (loss: 1.115955114364624, acc: 0.7058823704719543)
[2025-02-04 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:16][root][INFO] - Training Epoch: 2/2, step 23181/23838 completed (loss: 2.6347413063049316, acc: 0.4444444477558136)
[2025-02-04 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:16][root][INFO] - Training Epoch: 2/2, step 23182/23838 completed (loss: 3.9679441452026367, acc: 0.23999999463558197)
[2025-02-04 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:17][root][INFO] - Training Epoch: 2/2, step 23183/23838 completed (loss: 3.0528979301452637, acc: 0.3947368562221527)
[2025-02-04 01:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:17][root][INFO] - Training Epoch: 2/2, step 23184/23838 completed (loss: 3.14278507232666, acc: 0.35555556416511536)
[2025-02-04 01:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:18][root][INFO] - Training Epoch: 2/2, step 23185/23838 completed (loss: 4.285155773162842, acc: 0.2321428507566452)
[2025-02-04 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:18][root][INFO] - Training Epoch: 2/2, step 23186/23838 completed (loss: 3.2836804389953613, acc: 0.2888889014720917)
[2025-02-04 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:19][root][INFO] - Training Epoch: 2/2, step 23187/23838 completed (loss: 4.677438259124756, acc: 0.17073170840740204)
[2025-02-04 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:19][root][INFO] - Training Epoch: 2/2, step 23188/23838 completed (loss: 3.5827174186706543, acc: 0.29729729890823364)
[2025-02-04 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:20][root][INFO] - Training Epoch: 2/2, step 23189/23838 completed (loss: 1.8512102365493774, acc: 0.5862069129943848)
[2025-02-04 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:20][root][INFO] - Training Epoch: 2/2, step 23190/23838 completed (loss: 3.0709123611450195, acc: 0.37037035822868347)
[2025-02-04 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:20][root][INFO] - Training Epoch: 2/2, step 23191/23838 completed (loss: 3.0780930519104004, acc: 0.29032257199287415)
[2025-02-04 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:21][root][INFO] - Training Epoch: 2/2, step 23192/23838 completed (loss: 3.326016426086426, acc: 0.34375)
[2025-02-04 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:21][root][INFO] - Training Epoch: 2/2, step 23193/23838 completed (loss: 3.237215518951416, acc: 0.4193548262119293)
[2025-02-04 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:22][root][INFO] - Training Epoch: 2/2, step 23194/23838 completed (loss: 2.8521506786346436, acc: 0.3243243098258972)
[2025-02-04 01:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:22][root][INFO] - Training Epoch: 2/2, step 23195/23838 completed (loss: 2.411134958267212, acc: 0.3448275923728943)
[2025-02-04 01:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:22][root][INFO] - Training Epoch: 2/2, step 23196/23838 completed (loss: 3.056879758834839, acc: 0.3571428656578064)
[2025-02-04 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:23][root][INFO] - Training Epoch: 2/2, step 23197/23838 completed (loss: 2.4233107566833496, acc: 0.5)
[2025-02-04 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:23][root][INFO] - Training Epoch: 2/2, step 23198/23838 completed (loss: 3.5777413845062256, acc: 0.4117647111415863)
[2025-02-04 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:24][root][INFO] - Training Epoch: 2/2, step 23199/23838 completed (loss: 3.1529479026794434, acc: 0.5483871102333069)
[2025-02-04 01:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:24][root][INFO] - Training Epoch: 2/2, step 23200/23838 completed (loss: 3.335012435913086, acc: 0.23255814611911774)
[2025-02-04 01:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:25][root][INFO] - Training Epoch: 2/2, step 23201/23838 completed (loss: 3.011690616607666, acc: 0.2142857164144516)
[2025-02-04 01:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:25][root][INFO] - Training Epoch: 2/2, step 23202/23838 completed (loss: 2.8251285552978516, acc: 0.4375)
[2025-02-04 01:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:25][root][INFO] - Training Epoch: 2/2, step 23203/23838 completed (loss: 3.033609390258789, acc: 0.260869562625885)
[2025-02-04 01:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:26][root][INFO] - Training Epoch: 2/2, step 23204/23838 completed (loss: 2.692868232727051, acc: 0.4838709533214569)
[2025-02-04 01:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:26][root][INFO] - Training Epoch: 2/2, step 23205/23838 completed (loss: 2.6993415355682373, acc: 0.47826087474823)
[2025-02-04 01:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:27][root][INFO] - Training Epoch: 2/2, step 23206/23838 completed (loss: 3.0033977031707764, acc: 0.4285714328289032)
[2025-02-04 01:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:27][root][INFO] - Training Epoch: 2/2, step 23207/23838 completed (loss: 2.490837574005127, acc: 0.3199999928474426)
[2025-02-04 01:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:28][root][INFO] - Training Epoch: 2/2, step 23208/23838 completed (loss: 3.260793447494507, acc: 0.3870967626571655)
[2025-02-04 01:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:28][root][INFO] - Training Epoch: 2/2, step 23209/23838 completed (loss: 2.2327616214752197, acc: 0.7272727489471436)
[2025-02-04 01:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:28][root][INFO] - Training Epoch: 2/2, step 23210/23838 completed (loss: 2.7347359657287598, acc: 0.375)
[2025-02-04 01:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:29][root][INFO] - Training Epoch: 2/2, step 23211/23838 completed (loss: 3.5588741302490234, acc: 0.32258063554763794)
[2025-02-04 01:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:29][root][INFO] - Training Epoch: 2/2, step 23212/23838 completed (loss: 3.410233497619629, acc: 0.3103448152542114)
[2025-02-04 01:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:30][root][INFO] - Training Epoch: 2/2, step 23213/23838 completed (loss: 3.608541965484619, acc: 0.3870967626571655)
[2025-02-04 01:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:30][root][INFO] - Training Epoch: 2/2, step 23214/23838 completed (loss: 2.857607126235962, acc: 0.5)
[2025-02-04 01:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:31][root][INFO] - Training Epoch: 2/2, step 23215/23838 completed (loss: 3.5705487728118896, acc: 0.25)
[2025-02-04 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:31][root][INFO] - Training Epoch: 2/2, step 23216/23838 completed (loss: 3.2243354320526123, acc: 0.3333333432674408)
[2025-02-04 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:31][root][INFO] - Training Epoch: 2/2, step 23217/23838 completed (loss: 2.7504875659942627, acc: 0.44736841320991516)
[2025-02-04 01:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:32][root][INFO] - Training Epoch: 2/2, step 23218/23838 completed (loss: 3.0816121101379395, acc: 0.39534884691238403)
[2025-02-04 01:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:32][root][INFO] - Training Epoch: 2/2, step 23219/23838 completed (loss: 3.789187431335449, acc: 0.3636363744735718)
[2025-02-04 01:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:33][root][INFO] - Training Epoch: 2/2, step 23220/23838 completed (loss: 3.110785961151123, acc: 0.3255814015865326)
[2025-02-04 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:33][root][INFO] - Training Epoch: 2/2, step 23221/23838 completed (loss: 3.1850221157073975, acc: 0.4285714328289032)
[2025-02-04 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:34][root][INFO] - Training Epoch: 2/2, step 23222/23838 completed (loss: 3.0000226497650146, acc: 0.4054054021835327)
[2025-02-04 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:34][root][INFO] - Training Epoch: 2/2, step 23223/23838 completed (loss: 2.293835163116455, acc: 0.4583333432674408)
[2025-02-04 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:34][root][INFO] - Training Epoch: 2/2, step 23224/23838 completed (loss: 2.947225332260132, acc: 0.37288135290145874)
[2025-02-04 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:35][root][INFO] - Training Epoch: 2/2, step 23225/23838 completed (loss: 3.497605562210083, acc: 0.25641027092933655)
[2025-02-04 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:35][root][INFO] - Training Epoch: 2/2, step 23226/23838 completed (loss: 3.786036491394043, acc: 0.1875)
[2025-02-04 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:36][root][INFO] - Training Epoch: 2/2, step 23227/23838 completed (loss: 3.3231122493743896, acc: 0.1794871836900711)
[2025-02-04 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:36][root][INFO] - Training Epoch: 2/2, step 23228/23838 completed (loss: 2.8490612506866455, acc: 0.375)
[2025-02-04 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:37][root][INFO] - Training Epoch: 2/2, step 23229/23838 completed (loss: 2.7669055461883545, acc: 0.40625)
[2025-02-04 01:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:37][root][INFO] - Training Epoch: 2/2, step 23230/23838 completed (loss: 3.360511541366577, acc: 0.3181818127632141)
[2025-02-04 01:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:37][root][INFO] - Training Epoch: 2/2, step 23231/23838 completed (loss: 3.8137431144714355, acc: 0.2142857164144516)
[2025-02-04 01:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:38][root][INFO] - Training Epoch: 2/2, step 23232/23838 completed (loss: 3.1699087619781494, acc: 0.3199999928474426)
[2025-02-04 01:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:38][root][INFO] - Training Epoch: 2/2, step 23233/23838 completed (loss: 2.60962176322937, acc: 0.4000000059604645)
[2025-02-04 01:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:39][root][INFO] - Training Epoch: 2/2, step 23234/23838 completed (loss: 2.8519093990325928, acc: 0.34210526943206787)
[2025-02-04 01:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:39][root][INFO] - Training Epoch: 2/2, step 23235/23838 completed (loss: 2.1465938091278076, acc: 0.5)
[2025-02-04 01:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:40][root][INFO] - Training Epoch: 2/2, step 23236/23838 completed (loss: 3.5996668338775635, acc: 0.3448275923728943)
[2025-02-04 01:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:40][root][INFO] - Training Epoch: 2/2, step 23237/23838 completed (loss: 3.2630090713500977, acc: 0.41025641560554504)
[2025-02-04 01:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:40][root][INFO] - Training Epoch: 2/2, step 23238/23838 completed (loss: 4.359048843383789, acc: 0.2571428716182709)
[2025-02-04 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:41][root][INFO] - Training Epoch: 2/2, step 23239/23838 completed (loss: 3.09023118019104, acc: 0.4615384638309479)
[2025-02-04 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:41][root][INFO] - Training Epoch: 2/2, step 23240/23838 completed (loss: 3.514039993286133, acc: 0.3333333432674408)
[2025-02-04 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:42][root][INFO] - Training Epoch: 2/2, step 23241/23838 completed (loss: 2.646980047225952, acc: 0.44736841320991516)
[2025-02-04 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:42][root][INFO] - Training Epoch: 2/2, step 23242/23838 completed (loss: 3.788686513900757, acc: 0.25)
[2025-02-04 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:43][root][INFO] - Training Epoch: 2/2, step 23243/23838 completed (loss: 3.313749074935913, acc: 0.3243243098258972)
[2025-02-04 01:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:43][root][INFO] - Training Epoch: 2/2, step 23244/23838 completed (loss: 2.8786697387695312, acc: 0.4285714328289032)
[2025-02-04 01:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:43][root][INFO] - Training Epoch: 2/2, step 23245/23838 completed (loss: 3.5845324993133545, acc: 0.2881355881690979)
[2025-02-04 01:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:44][root][INFO] - Training Epoch: 2/2, step 23246/23838 completed (loss: 2.9076128005981445, acc: 0.37254902720451355)
[2025-02-04 01:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:44][root][INFO] - Training Epoch: 2/2, step 23247/23838 completed (loss: 2.9934678077697754, acc: 0.41860464215278625)
[2025-02-04 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:45][root][INFO] - Training Epoch: 2/2, step 23248/23838 completed (loss: 3.573793411254883, acc: 0.20000000298023224)
[2025-02-04 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:45][root][INFO] - Training Epoch: 2/2, step 23249/23838 completed (loss: 3.0999083518981934, acc: 0.3333333432674408)
[2025-02-04 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:46][root][INFO] - Training Epoch: 2/2, step 23250/23838 completed (loss: 2.753898859024048, acc: 0.3611111044883728)
[2025-02-04 01:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:46][root][INFO] - Training Epoch: 2/2, step 23251/23838 completed (loss: 3.01141095161438, acc: 0.39393940567970276)
[2025-02-04 01:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:47][root][INFO] - Training Epoch: 2/2, step 23252/23838 completed (loss: 3.4957964420318604, acc: 0.21621622145175934)
[2025-02-04 01:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:47][root][INFO] - Training Epoch: 2/2, step 23253/23838 completed (loss: 2.7736823558807373, acc: 0.3142857253551483)
[2025-02-04 01:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:48][root][INFO] - Training Epoch: 2/2, step 23254/23838 completed (loss: 3.1493947505950928, acc: 0.36000001430511475)
[2025-02-04 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:48][root][INFO] - Training Epoch: 2/2, step 23255/23838 completed (loss: 2.7881505489349365, acc: 0.4545454680919647)
[2025-02-04 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:48][root][INFO] - Training Epoch: 2/2, step 23256/23838 completed (loss: 3.4753780364990234, acc: 0.3913043439388275)
[2025-02-04 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:49][root][INFO] - Training Epoch: 2/2, step 23257/23838 completed (loss: 4.377646446228027, acc: 0.4736842215061188)
[2025-02-04 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:49][root][INFO] - Training Epoch: 2/2, step 23258/23838 completed (loss: 3.4587297439575195, acc: 0.25806450843811035)
[2025-02-04 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:50][root][INFO] - Training Epoch: 2/2, step 23259/23838 completed (loss: 1.8515515327453613, acc: 0.5555555820465088)
[2025-02-04 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:50][root][INFO] - Training Epoch: 2/2, step 23260/23838 completed (loss: 2.577383279800415, acc: 0.3333333432674408)
[2025-02-04 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:50][root][INFO] - Training Epoch: 2/2, step 23261/23838 completed (loss: 2.8544230461120605, acc: 0.3055555522441864)
[2025-02-04 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:51][root][INFO] - Training Epoch: 2/2, step 23262/23838 completed (loss: 3.605637311935425, acc: 0.2666666805744171)
[2025-02-04 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:51][root][INFO] - Training Epoch: 2/2, step 23263/23838 completed (loss: 4.664504528045654, acc: 0.22857142984867096)
[2025-02-04 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:52][root][INFO] - Training Epoch: 2/2, step 23264/23838 completed (loss: 2.6820387840270996, acc: 0.40740740299224854)
[2025-02-04 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:52][root][INFO] - Training Epoch: 2/2, step 23265/23838 completed (loss: 2.7591328620910645, acc: 0.3333333432674408)
[2025-02-04 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:53][root][INFO] - Training Epoch: 2/2, step 23266/23838 completed (loss: 2.7953803539276123, acc: 0.4000000059604645)
[2025-02-04 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:53][root][INFO] - Training Epoch: 2/2, step 23267/23838 completed (loss: 3.3979034423828125, acc: 0.20588235557079315)
[2025-02-04 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:54][root][INFO] - Training Epoch: 2/2, step 23268/23838 completed (loss: 3.090768337249756, acc: 0.4375)
[2025-02-04 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:54][root][INFO] - Training Epoch: 2/2, step 23269/23838 completed (loss: 2.9285531044006348, acc: 0.3571428656578064)
[2025-02-04 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:54][root][INFO] - Training Epoch: 2/2, step 23270/23838 completed (loss: 4.181095123291016, acc: 0.2647058963775635)
[2025-02-04 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:55][root][INFO] - Training Epoch: 2/2, step 23271/23838 completed (loss: 2.5909061431884766, acc: 0.4516128897666931)
[2025-02-04 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:55][root][INFO] - Training Epoch: 2/2, step 23272/23838 completed (loss: 3.317072868347168, acc: 0.4000000059604645)
[2025-02-04 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:56][root][INFO] - Training Epoch: 2/2, step 23273/23838 completed (loss: 2.6372756958007812, acc: 0.4000000059604645)
[2025-02-04 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:56][root][INFO] - Training Epoch: 2/2, step 23274/23838 completed (loss: 2.977473258972168, acc: 0.40909090638160706)
[2025-02-04 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:57][root][INFO] - Training Epoch: 2/2, step 23275/23838 completed (loss: 2.797414541244507, acc: 0.47058823704719543)
[2025-02-04 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:57][root][INFO] - Training Epoch: 2/2, step 23276/23838 completed (loss: 3.5714263916015625, acc: 0.4000000059604645)
[2025-02-04 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:57][root][INFO] - Training Epoch: 2/2, step 23277/23838 completed (loss: 3.601069688796997, acc: 0.2857142984867096)
[2025-02-04 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:58][root][INFO] - Training Epoch: 2/2, step 23278/23838 completed (loss: 2.9876456260681152, acc: 0.3199999928474426)
[2025-02-04 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:58][root][INFO] - Training Epoch: 2/2, step 23279/23838 completed (loss: 3.32148814201355, acc: 0.4166666567325592)
[2025-02-04 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:59][root][INFO] - Training Epoch: 2/2, step 23280/23838 completed (loss: 2.610776662826538, acc: 0.37037035822868347)
[2025-02-04 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:12:59][root][INFO] - Training Epoch: 2/2, step 23281/23838 completed (loss: 3.0883357524871826, acc: 0.3448275923728943)
[2025-02-04 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:00][root][INFO] - Training Epoch: 2/2, step 23282/23838 completed (loss: 2.8911049365997314, acc: 0.34285715222358704)
[2025-02-04 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:00][root][INFO] - Training Epoch: 2/2, step 23283/23838 completed (loss: 2.9806900024414062, acc: 0.39393940567970276)
[2025-02-04 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:00][root][INFO] - Training Epoch: 2/2, step 23284/23838 completed (loss: 3.6449830532073975, acc: 0.26829269528388977)
[2025-02-04 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:01][root][INFO] - Training Epoch: 2/2, step 23285/23838 completed (loss: 2.7583963871002197, acc: 0.29729729890823364)
[2025-02-04 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:01][root][INFO] - Training Epoch: 2/2, step 23286/23838 completed (loss: 3.184155225753784, acc: 0.27586206793785095)
[2025-02-04 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:02][root][INFO] - Training Epoch: 2/2, step 23287/23838 completed (loss: 2.58551025390625, acc: 0.4000000059604645)
[2025-02-04 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:02][root][INFO] - Training Epoch: 2/2, step 23288/23838 completed (loss: 2.6169514656066895, acc: 0.5306122303009033)
[2025-02-04 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:03][root][INFO] - Training Epoch: 2/2, step 23289/23838 completed (loss: 2.514770269393921, acc: 0.5)
[2025-02-04 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:03][root][INFO] - Training Epoch: 2/2, step 23290/23838 completed (loss: 2.4694526195526123, acc: 0.5625)
[2025-02-04 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:03][root][INFO] - Training Epoch: 2/2, step 23291/23838 completed (loss: 2.177393913269043, acc: 0.6470588445663452)
[2025-02-04 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:04][root][INFO] - Training Epoch: 2/2, step 23292/23838 completed (loss: 2.407073497772217, acc: 0.4137931168079376)
[2025-02-04 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:04][root][INFO] - Training Epoch: 2/2, step 23293/23838 completed (loss: 3.1543233394622803, acc: 0.3333333432674408)
[2025-02-04 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:05][root][INFO] - Training Epoch: 2/2, step 23294/23838 completed (loss: 3.409536361694336, acc: 0.3529411852359772)
[2025-02-04 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:05][root][INFO] - Training Epoch: 2/2, step 23295/23838 completed (loss: 3.5197689533233643, acc: 0.3214285671710968)
[2025-02-04 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:06][root][INFO] - Training Epoch: 2/2, step 23296/23838 completed (loss: 4.25115442276001, acc: 0.23076923191547394)
[2025-02-04 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:06][root][INFO] - Training Epoch: 2/2, step 23297/23838 completed (loss: 2.949740409851074, acc: 0.4545454680919647)
[2025-02-04 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:06][root][INFO] - Training Epoch: 2/2, step 23298/23838 completed (loss: 3.9616525173187256, acc: 0.34090909361839294)
[2025-02-04 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:07][root][INFO] - Training Epoch: 2/2, step 23299/23838 completed (loss: 2.997725248336792, acc: 0.3888888955116272)
[2025-02-04 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:07][root][INFO] - Training Epoch: 2/2, step 23300/23838 completed (loss: 3.0717453956604004, acc: 0.38461539149284363)
[2025-02-04 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:08][root][INFO] - Training Epoch: 2/2, step 23301/23838 completed (loss: 2.9979326725006104, acc: 0.3488371968269348)
[2025-02-04 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:08][root][INFO] - Training Epoch: 2/2, step 23302/23838 completed (loss: 3.4984240531921387, acc: 0.25)
[2025-02-04 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:08][root][INFO] - Training Epoch: 2/2, step 23303/23838 completed (loss: 3.536458969116211, acc: 0.29411765933036804)
[2025-02-04 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:09][root][INFO] - Training Epoch: 2/2, step 23304/23838 completed (loss: 3.3389077186584473, acc: 0.2800000011920929)
[2025-02-04 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:09][root][INFO] - Training Epoch: 2/2, step 23305/23838 completed (loss: 3.710926055908203, acc: 0.3333333432674408)
[2025-02-04 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:10][root][INFO] - Training Epoch: 2/2, step 23306/23838 completed (loss: 3.279447078704834, acc: 0.32258063554763794)
[2025-02-04 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:10][root][INFO] - Training Epoch: 2/2, step 23307/23838 completed (loss: 3.1922905445098877, acc: 0.3333333432674408)
[2025-02-04 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:11][root][INFO] - Training Epoch: 2/2, step 23308/23838 completed (loss: 3.186581611633301, acc: 0.3611111044883728)
[2025-02-04 01:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:11][root][INFO] - Training Epoch: 2/2, step 23309/23838 completed (loss: 2.0816195011138916, acc: 0.5454545617103577)
[2025-02-04 01:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:12][root][INFO] - Training Epoch: 2/2, step 23310/23838 completed (loss: 3.387782335281372, acc: 0.3404255211353302)
[2025-02-04 01:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:12][root][INFO] - Training Epoch: 2/2, step 23311/23838 completed (loss: 2.5953481197357178, acc: 0.3913043439388275)
[2025-02-04 01:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:13][root][INFO] - Training Epoch: 2/2, step 23312/23838 completed (loss: 3.5089449882507324, acc: 0.3235294222831726)
[2025-02-04 01:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:13][root][INFO] - Training Epoch: 2/2, step 23313/23838 completed (loss: 3.1580355167388916, acc: 0.38461539149284363)
[2025-02-04 01:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:13][root][INFO] - Training Epoch: 2/2, step 23314/23838 completed (loss: 2.45902419090271, acc: 0.47727271914482117)
[2025-02-04 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:14][root][INFO] - Training Epoch: 2/2, step 23315/23838 completed (loss: 2.7827308177948, acc: 0.44999998807907104)
[2025-02-04 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:14][root][INFO] - Training Epoch: 2/2, step 23316/23838 completed (loss: 3.288975477218628, acc: 0.3333333432674408)
[2025-02-04 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:15][root][INFO] - Training Epoch: 2/2, step 23317/23838 completed (loss: 3.6926729679107666, acc: 0.375)
[2025-02-04 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:15][root][INFO] - Training Epoch: 2/2, step 23318/23838 completed (loss: 3.76208233833313, acc: 0.31707316637039185)
[2025-02-04 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:16][root][INFO] - Training Epoch: 2/2, step 23319/23838 completed (loss: 2.6135027408599854, acc: 0.5652173757553101)
[2025-02-04 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:16][root][INFO] - Training Epoch: 2/2, step 23320/23838 completed (loss: 2.8872933387756348, acc: 0.4285714328289032)
[2025-02-04 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:17][root][INFO] - Training Epoch: 2/2, step 23321/23838 completed (loss: 3.179931163787842, acc: 0.3333333432674408)
[2025-02-04 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:17][root][INFO] - Training Epoch: 2/2, step 23322/23838 completed (loss: 2.6397883892059326, acc: 0.40740740299224854)
[2025-02-04 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:17][root][INFO] - Training Epoch: 2/2, step 23323/23838 completed (loss: 2.705343723297119, acc: 0.42105263471603394)
[2025-02-04 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:18][root][INFO] - Training Epoch: 2/2, step 23324/23838 completed (loss: 2.553182363510132, acc: 0.517241358757019)
[2025-02-04 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:18][root][INFO] - Training Epoch: 2/2, step 23325/23838 completed (loss: 2.2249276638031006, acc: 0.4444444477558136)
[2025-02-04 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:19][root][INFO] - Training Epoch: 2/2, step 23326/23838 completed (loss: 3.2096030712127686, acc: 0.47999998927116394)
[2025-02-04 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:19][root][INFO] - Training Epoch: 2/2, step 23327/23838 completed (loss: 2.423363208770752, acc: 0.2857142984867096)
[2025-02-04 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:19][root][INFO] - Training Epoch: 2/2, step 23328/23838 completed (loss: 2.6467576026916504, acc: 0.3513513505458832)
[2025-02-04 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:20][root][INFO] - Training Epoch: 2/2, step 23329/23838 completed (loss: 2.9654557704925537, acc: 0.36666667461395264)
[2025-02-04 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:20][root][INFO] - Training Epoch: 2/2, step 23330/23838 completed (loss: 3.07401180267334, acc: 0.35483869910240173)
[2025-02-04 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:21][root][INFO] - Training Epoch: 2/2, step 23331/23838 completed (loss: 3.0006520748138428, acc: 0.35849055647850037)
[2025-02-04 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:21][root][INFO] - Training Epoch: 2/2, step 23332/23838 completed (loss: 2.7031798362731934, acc: 0.47058823704719543)
[2025-02-04 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:22][root][INFO] - Training Epoch: 2/2, step 23333/23838 completed (loss: 3.496551036834717, acc: 0.3611111044883728)
[2025-02-04 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:22][root][INFO] - Training Epoch: 2/2, step 23334/23838 completed (loss: 2.7946584224700928, acc: 0.42307692766189575)
[2025-02-04 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:22][root][INFO] - Training Epoch: 2/2, step 23335/23838 completed (loss: 2.3168554306030273, acc: 0.5277777910232544)
[2025-02-04 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:23][root][INFO] - Training Epoch: 2/2, step 23336/23838 completed (loss: 3.803638458251953, acc: 0.3333333432674408)
[2025-02-04 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:23][root][INFO] - Training Epoch: 2/2, step 23337/23838 completed (loss: 3.8763396739959717, acc: 0.28947368264198303)
[2025-02-04 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:24][root][INFO] - Training Epoch: 2/2, step 23338/23838 completed (loss: 3.7511980533599854, acc: 0.29629629850387573)
[2025-02-04 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:24][root][INFO] - Training Epoch: 2/2, step 23339/23838 completed (loss: 1.605777621269226, acc: 0.5199999809265137)
[2025-02-04 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:25][root][INFO] - Training Epoch: 2/2, step 23340/23838 completed (loss: 2.3232028484344482, acc: 0.5151515007019043)
[2025-02-04 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:25][root][INFO] - Training Epoch: 2/2, step 23341/23838 completed (loss: 2.869281530380249, acc: 0.40909090638160706)
[2025-02-04 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:25][root][INFO] - Training Epoch: 2/2, step 23342/23838 completed (loss: 3.8097097873687744, acc: 0.25925925374031067)
[2025-02-04 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:26][root][INFO] - Training Epoch: 2/2, step 23343/23838 completed (loss: 3.0187628269195557, acc: 0.4399999976158142)
[2025-02-04 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:26][root][INFO] - Training Epoch: 2/2, step 23344/23838 completed (loss: 3.3400492668151855, acc: 0.375)
[2025-02-04 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:27][root][INFO] - Training Epoch: 2/2, step 23345/23838 completed (loss: 3.156702756881714, acc: 0.2857142984867096)
[2025-02-04 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:27][root][INFO] - Training Epoch: 2/2, step 23346/23838 completed (loss: 3.1756227016448975, acc: 0.2857142984867096)
[2025-02-04 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:27][root][INFO] - Training Epoch: 2/2, step 23347/23838 completed (loss: 2.6598587036132812, acc: 0.4444444477558136)
[2025-02-04 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:28][root][INFO] - Training Epoch: 2/2, step 23348/23838 completed (loss: 2.8712329864501953, acc: 0.39534884691238403)
[2025-02-04 01:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:28][root][INFO] - Training Epoch: 2/2, step 23349/23838 completed (loss: 4.007787704467773, acc: 0.3571428656578064)
[2025-02-04 01:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:29][root][INFO] - Training Epoch: 2/2, step 23350/23838 completed (loss: 3.9674980640411377, acc: 0.31707316637039185)
[2025-02-04 01:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:29][root][INFO] - Training Epoch: 2/2, step 23351/23838 completed (loss: 2.992161750793457, acc: 0.45945945382118225)
[2025-02-04 01:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:29][root][INFO] - Training Epoch: 2/2, step 23352/23838 completed (loss: 3.9352974891662598, acc: 0.2926829159259796)
[2025-02-04 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:30][root][INFO] - Training Epoch: 2/2, step 23353/23838 completed (loss: 4.163926601409912, acc: 0.25641027092933655)
[2025-02-04 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:30][root][INFO] - Training Epoch: 2/2, step 23354/23838 completed (loss: 3.444096326828003, acc: 0.3529411852359772)
[2025-02-04 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:31][root][INFO] - Training Epoch: 2/2, step 23355/23838 completed (loss: 2.4522531032562256, acc: 0.5)
[2025-02-04 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:31][root][INFO] - Training Epoch: 2/2, step 23356/23838 completed (loss: 2.3074958324432373, acc: 0.5675675868988037)
[2025-02-04 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:32][root][INFO] - Training Epoch: 2/2, step 23357/23838 completed (loss: 3.8006839752197266, acc: 0.25925925374031067)
[2025-02-04 01:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:32][root][INFO] - Training Epoch: 2/2, step 23358/23838 completed (loss: 2.779956102371216, acc: 0.42105263471603394)
[2025-02-04 01:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:32][root][INFO] - Training Epoch: 2/2, step 23359/23838 completed (loss: 2.916491985321045, acc: 0.3799999952316284)
[2025-02-04 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:33][root][INFO] - Training Epoch: 2/2, step 23360/23838 completed (loss: 3.6530983448028564, acc: 0.4399999976158142)
[2025-02-04 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:33][root][INFO] - Training Epoch: 2/2, step 23361/23838 completed (loss: 3.3021187782287598, acc: 0.32758620381355286)
[2025-02-04 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:34][root][INFO] - Training Epoch: 2/2, step 23362/23838 completed (loss: 1.9708106517791748, acc: 0.5666666626930237)
[2025-02-04 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:34][root][INFO] - Training Epoch: 2/2, step 23363/23838 completed (loss: 3.683173656463623, acc: 0.3658536672592163)
[2025-02-04 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:34][root][INFO] - Training Epoch: 2/2, step 23364/23838 completed (loss: 2.4850196838378906, acc: 0.5)
[2025-02-04 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:35][root][INFO] - Training Epoch: 2/2, step 23365/23838 completed (loss: 3.347529411315918, acc: 0.26229506731033325)
[2025-02-04 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:35][root][INFO] - Training Epoch: 2/2, step 23366/23838 completed (loss: 3.6804487705230713, acc: 0.3142857253551483)
[2025-02-04 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:36][root][INFO] - Training Epoch: 2/2, step 23367/23838 completed (loss: 3.2129294872283936, acc: 0.3695652186870575)
[2025-02-04 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:36][root][INFO] - Training Epoch: 2/2, step 23368/23838 completed (loss: 2.3990819454193115, acc: 0.5384615659713745)
[2025-02-04 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:37][root][INFO] - Training Epoch: 2/2, step 23369/23838 completed (loss: 3.3227884769439697, acc: 0.37142857909202576)
[2025-02-04 01:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:37][root][INFO] - Training Epoch: 2/2, step 23370/23838 completed (loss: 2.869013786315918, acc: 0.3199999928474426)
[2025-02-04 01:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:37][root][INFO] - Training Epoch: 2/2, step 23371/23838 completed (loss: 2.461210250854492, acc: 0.4399999976158142)
[2025-02-04 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:38][root][INFO] - Training Epoch: 2/2, step 23372/23838 completed (loss: 2.6836416721343994, acc: 0.44117647409439087)
[2025-02-04 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:38][root][INFO] - Training Epoch: 2/2, step 23373/23838 completed (loss: 1.737659215927124, acc: 0.6000000238418579)
[2025-02-04 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:39][root][INFO] - Training Epoch: 2/2, step 23374/23838 completed (loss: 2.885763168334961, acc: 0.3799999952316284)
[2025-02-04 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:39][root][INFO] - Training Epoch: 2/2, step 23375/23838 completed (loss: 2.6262896060943604, acc: 0.4000000059604645)
[2025-02-04 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:40][root][INFO] - Training Epoch: 2/2, step 23376/23838 completed (loss: 3.034943103790283, acc: 0.3947368562221527)
[2025-02-04 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:40][root][INFO] - Training Epoch: 2/2, step 23377/23838 completed (loss: 2.4015469551086426, acc: 0.4000000059604645)
[2025-02-04 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:41][root][INFO] - Training Epoch: 2/2, step 23378/23838 completed (loss: 2.56600022315979, acc: 0.375)
[2025-02-04 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:41][root][INFO] - Training Epoch: 2/2, step 23379/23838 completed (loss: 2.978912591934204, acc: 0.36538460850715637)
[2025-02-04 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:41][root][INFO] - Training Epoch: 2/2, step 23380/23838 completed (loss: 3.0079524517059326, acc: 0.31111112236976624)
[2025-02-04 01:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:42][root][INFO] - Training Epoch: 2/2, step 23381/23838 completed (loss: 2.689641237258911, acc: 0.41025641560554504)
[2025-02-04 01:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:43][root][INFO] - Training Epoch: 2/2, step 23382/23838 completed (loss: 3.1785523891448975, acc: 0.28260868787765503)
[2025-02-04 01:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:43][root][INFO] - Training Epoch: 2/2, step 23383/23838 completed (loss: 2.639785051345825, acc: 0.4642857015132904)
[2025-02-04 01:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:43][root][INFO] - Training Epoch: 2/2, step 23384/23838 completed (loss: 3.197279930114746, acc: 0.3684210479259491)
[2025-02-04 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:44][root][INFO] - Training Epoch: 2/2, step 23385/23838 completed (loss: 3.2621917724609375, acc: 0.3529411852359772)
[2025-02-04 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:44][root][INFO] - Training Epoch: 2/2, step 23386/23838 completed (loss: 3.53670072555542, acc: 0.2448979616165161)
[2025-02-04 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:45][root][INFO] - Training Epoch: 2/2, step 23387/23838 completed (loss: 2.431467294692993, acc: 0.4722222089767456)
[2025-02-04 01:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:45][root][INFO] - Training Epoch: 2/2, step 23388/23838 completed (loss: 3.3254597187042236, acc: 0.3589743673801422)
[2025-02-04 01:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:46][root][INFO] - Training Epoch: 2/2, step 23389/23838 completed (loss: 2.2984540462493896, acc: 0.4642857015132904)
[2025-02-04 01:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:46][root][INFO] - Training Epoch: 2/2, step 23390/23838 completed (loss: 2.5798492431640625, acc: 0.40740740299224854)
[2025-02-04 01:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:46][root][INFO] - Training Epoch: 2/2, step 23391/23838 completed (loss: 2.1166293621063232, acc: 0.5333333611488342)
[2025-02-04 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:47][root][INFO] - Training Epoch: 2/2, step 23392/23838 completed (loss: 3.098172187805176, acc: 0.3333333432674408)
[2025-02-04 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:47][root][INFO] - Training Epoch: 2/2, step 23393/23838 completed (loss: 3.768261194229126, acc: 0.239130437374115)
[2025-02-04 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:48][root][INFO] - Training Epoch: 2/2, step 23394/23838 completed (loss: 3.1265294551849365, acc: 0.30000001192092896)
[2025-02-04 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:48][root][INFO] - Training Epoch: 2/2, step 23395/23838 completed (loss: 2.4512929916381836, acc: 0.5365853905677795)
[2025-02-04 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:49][root][INFO] - Training Epoch: 2/2, step 23396/23838 completed (loss: 3.552462577819824, acc: 0.3235294222831726)
[2025-02-04 01:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:49][root][INFO] - Training Epoch: 2/2, step 23397/23838 completed (loss: 2.9215946197509766, acc: 0.3888888955116272)
[2025-02-04 01:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:50][root][INFO] - Training Epoch: 2/2, step 23398/23838 completed (loss: 2.6165690422058105, acc: 0.2857142984867096)
[2025-02-04 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:50][root][INFO] - Training Epoch: 2/2, step 23399/23838 completed (loss: 2.2678415775299072, acc: 0.36000001430511475)
[2025-02-04 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:50][root][INFO] - Training Epoch: 2/2, step 23400/23838 completed (loss: 2.6153903007507324, acc: 0.4749999940395355)
[2025-02-04 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:51][root][INFO] - Training Epoch: 2/2, step 23401/23838 completed (loss: 2.47509503364563, acc: 0.5476190447807312)
[2025-02-04 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:51][root][INFO] - Training Epoch: 2/2, step 23402/23838 completed (loss: 3.0945475101470947, acc: 0.42424243688583374)
[2025-02-04 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:52][root][INFO] - Training Epoch: 2/2, step 23403/23838 completed (loss: 1.9789042472839355, acc: 0.5)
[2025-02-04 01:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:52][root][INFO] - Training Epoch: 2/2, step 23404/23838 completed (loss: 2.952415704727173, acc: 0.31578946113586426)
[2025-02-04 01:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:52][root][INFO] - Training Epoch: 2/2, step 23405/23838 completed (loss: 1.8617775440216064, acc: 0.47826087474823)
[2025-02-04 01:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:53][root][INFO] - Training Epoch: 2/2, step 23406/23838 completed (loss: 3.108335494995117, acc: 0.261904776096344)
[2025-02-04 01:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:53][root][INFO] - Training Epoch: 2/2, step 23407/23838 completed (loss: 3.6921515464782715, acc: 0.2666666805744171)
[2025-02-04 01:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:54][root][INFO] - Training Epoch: 2/2, step 23408/23838 completed (loss: 2.9524753093719482, acc: 0.4516128897666931)
[2025-02-04 01:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:54][root][INFO] - Training Epoch: 2/2, step 23409/23838 completed (loss: 2.574559211730957, acc: 0.47826087474823)
[2025-02-04 01:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:55][root][INFO] - Training Epoch: 2/2, step 23410/23838 completed (loss: 2.0734026432037354, acc: 0.4000000059604645)
[2025-02-04 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:55][root][INFO] - Training Epoch: 2/2, step 23411/23838 completed (loss: 2.717447280883789, acc: 0.44999998807907104)
[2025-02-04 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:55][root][INFO] - Training Epoch: 2/2, step 23412/23838 completed (loss: 2.7611324787139893, acc: 0.4571428596973419)
[2025-02-04 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:56][root][INFO] - Training Epoch: 2/2, step 23413/23838 completed (loss: 3.4714715480804443, acc: 0.3333333432674408)
[2025-02-04 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:56][root][INFO] - Training Epoch: 2/2, step 23414/23838 completed (loss: 2.516186237335205, acc: 0.5)
[2025-02-04 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:57][root][INFO] - Training Epoch: 2/2, step 23415/23838 completed (loss: 2.5649681091308594, acc: 0.4651162922382355)
[2025-02-04 01:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:57][root][INFO] - Training Epoch: 2/2, step 23416/23838 completed (loss: 2.6557533740997314, acc: 0.35483869910240173)
[2025-02-04 01:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:58][root][INFO] - Training Epoch: 2/2, step 23417/23838 completed (loss: 2.6326615810394287, acc: 0.37037035822868347)
[2025-02-04 01:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:58][root][INFO] - Training Epoch: 2/2, step 23418/23838 completed (loss: 2.6367056369781494, acc: 0.3333333432674408)
[2025-02-04 01:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:59][root][INFO] - Training Epoch: 2/2, step 23419/23838 completed (loss: 3.0405473709106445, acc: 0.4482758641242981)
[2025-02-04 01:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:59][root][INFO] - Training Epoch: 2/2, step 23420/23838 completed (loss: 2.138239622116089, acc: 0.6153846383094788)
[2025-02-04 01:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:13:59][root][INFO] - Training Epoch: 2/2, step 23421/23838 completed (loss: 1.8954461812973022, acc: 0.625)
[2025-02-04 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:00][root][INFO] - Training Epoch: 2/2, step 23422/23838 completed (loss: 3.395970344543457, acc: 0.3333333432674408)
[2025-02-04 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:00][root][INFO] - Training Epoch: 2/2, step 23423/23838 completed (loss: 2.6334805488586426, acc: 0.38235294818878174)
[2025-02-04 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:01][root][INFO] - Training Epoch: 2/2, step 23424/23838 completed (loss: 2.746168375015259, acc: 0.4883720874786377)
[2025-02-04 01:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:01][root][INFO] - Training Epoch: 2/2, step 23425/23838 completed (loss: 3.0002119541168213, acc: 0.3392857015132904)
[2025-02-04 01:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:01][root][INFO] - Training Epoch: 2/2, step 23426/23838 completed (loss: 2.9634783267974854, acc: 0.5)
[2025-02-04 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:02][root][INFO] - Training Epoch: 2/2, step 23427/23838 completed (loss: 2.5097496509552, acc: 0.42424243688583374)
[2025-02-04 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:02][root][INFO] - Training Epoch: 2/2, step 23428/23838 completed (loss: 3.9077508449554443, acc: 0.24390244483947754)
[2025-02-04 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:03][root][INFO] - Training Epoch: 2/2, step 23429/23838 completed (loss: 2.278611898422241, acc: 0.5333333611488342)
[2025-02-04 01:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:03][root][INFO] - Training Epoch: 2/2, step 23430/23838 completed (loss: 2.013598680496216, acc: 0.5)
[2025-02-04 01:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:04][root][INFO] - Training Epoch: 2/2, step 23431/23838 completed (loss: 4.425124645233154, acc: 0.23636363446712494)
[2025-02-04 01:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:04][root][INFO] - Training Epoch: 2/2, step 23432/23838 completed (loss: 2.9987542629241943, acc: 0.3928571343421936)
[2025-02-04 01:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:04][root][INFO] - Training Epoch: 2/2, step 23433/23838 completed (loss: 3.3747339248657227, acc: 0.4117647111415863)
[2025-02-04 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:05][root][INFO] - Training Epoch: 2/2, step 23434/23838 completed (loss: 3.8819053173065186, acc: 0.27272728085517883)
[2025-02-04 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:05][root][INFO] - Training Epoch: 2/2, step 23435/23838 completed (loss: 2.891111135482788, acc: 0.46875)
[2025-02-04 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:06][root][INFO] - Training Epoch: 2/2, step 23436/23838 completed (loss: 2.598653793334961, acc: 0.46875)
[2025-02-04 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:06][root][INFO] - Training Epoch: 2/2, step 23437/23838 completed (loss: 1.9455138444900513, acc: 0.5)
[2025-02-04 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:07][root][INFO] - Training Epoch: 2/2, step 23438/23838 completed (loss: 2.764918565750122, acc: 0.5)
[2025-02-04 01:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:07][root][INFO] - Training Epoch: 2/2, step 23439/23838 completed (loss: 3.0261332988739014, acc: 0.42424243688583374)
[2025-02-04 01:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:07][root][INFO] - Training Epoch: 2/2, step 23440/23838 completed (loss: 2.6002485752105713, acc: 0.40740740299224854)
[2025-02-04 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:08][root][INFO] - Training Epoch: 2/2, step 23441/23838 completed (loss: 3.239983558654785, acc: 0.42105263471603394)
[2025-02-04 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:08][root][INFO] - Training Epoch: 2/2, step 23442/23838 completed (loss: 3.667372703552246, acc: 0.2647058963775635)
[2025-02-04 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:09][root][INFO] - Training Epoch: 2/2, step 23443/23838 completed (loss: 2.4994544982910156, acc: 0.375)
[2025-02-04 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:09][root][INFO] - Training Epoch: 2/2, step 23444/23838 completed (loss: 2.8486886024475098, acc: 0.4166666567325592)
[2025-02-04 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:09][root][INFO] - Training Epoch: 2/2, step 23445/23838 completed (loss: 2.7940282821655273, acc: 0.30000001192092896)
[2025-02-04 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:10][root][INFO] - Training Epoch: 2/2, step 23446/23838 completed (loss: 2.2838447093963623, acc: 0.5)
[2025-02-04 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:10][root][INFO] - Training Epoch: 2/2, step 23447/23838 completed (loss: 3.6607954502105713, acc: 0.25)
[2025-02-04 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:11][root][INFO] - Training Epoch: 2/2, step 23448/23838 completed (loss: 2.9155499935150146, acc: 0.5185185074806213)
[2025-02-04 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:11][root][INFO] - Training Epoch: 2/2, step 23449/23838 completed (loss: 3.5436906814575195, acc: 0.28947368264198303)
[2025-02-04 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:12][root][INFO] - Training Epoch: 2/2, step 23450/23838 completed (loss: 2.7873682975769043, acc: 0.43589743971824646)
[2025-02-04 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:12][root][INFO] - Training Epoch: 2/2, step 23451/23838 completed (loss: 2.3284752368927, acc: 0.550000011920929)
[2025-02-04 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:13][root][INFO] - Training Epoch: 2/2, step 23452/23838 completed (loss: 2.645688772201538, acc: 0.47999998927116394)
[2025-02-04 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:13][root][INFO] - Training Epoch: 2/2, step 23453/23838 completed (loss: 3.2480733394622803, acc: 0.3055555522441864)
[2025-02-04 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:13][root][INFO] - Training Epoch: 2/2, step 23454/23838 completed (loss: 3.7402119636535645, acc: 0.30000001192092896)
[2025-02-04 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:14][root][INFO] - Training Epoch: 2/2, step 23455/23838 completed (loss: 2.83536696434021, acc: 0.3928571343421936)
[2025-02-04 01:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:14][root][INFO] - Training Epoch: 2/2, step 23456/23838 completed (loss: 3.189150333404541, acc: 0.38235294818878174)
[2025-02-04 01:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:15][root][INFO] - Training Epoch: 2/2, step 23457/23838 completed (loss: 3.0502030849456787, acc: 0.37837839126586914)
[2025-02-04 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:15][root][INFO] - Training Epoch: 2/2, step 23458/23838 completed (loss: 3.6544671058654785, acc: 0.2857142984867096)
[2025-02-04 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:15][root][INFO] - Training Epoch: 2/2, step 23459/23838 completed (loss: 2.828810453414917, acc: 0.3684210479259491)
[2025-02-04 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:16][root][INFO] - Training Epoch: 2/2, step 23460/23838 completed (loss: 2.989914894104004, acc: 0.4038461446762085)
[2025-02-04 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:16][root][INFO] - Training Epoch: 2/2, step 23461/23838 completed (loss: 3.7387099266052246, acc: 0.27906978130340576)
[2025-02-04 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:17][root][INFO] - Training Epoch: 2/2, step 23462/23838 completed (loss: 3.171267509460449, acc: 0.36538460850715637)
[2025-02-04 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:17][root][INFO] - Training Epoch: 2/2, step 23463/23838 completed (loss: 2.21040415763855, acc: 0.517241358757019)
[2025-02-04 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:17][root][INFO] - Training Epoch: 2/2, step 23464/23838 completed (loss: 3.0845983028411865, acc: 0.22580644488334656)
[2025-02-04 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:18][root][INFO] - Training Epoch: 2/2, step 23465/23838 completed (loss: 2.212351083755493, acc: 0.42307692766189575)
[2025-02-04 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:18][root][INFO] - Training Epoch: 2/2, step 23466/23838 completed (loss: 2.0759387016296387, acc: 0.5)
[2025-02-04 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:19][root][INFO] - Training Epoch: 2/2, step 23467/23838 completed (loss: 3.4488766193389893, acc: 0.3142857253551483)
[2025-02-04 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:19][root][INFO] - Training Epoch: 2/2, step 23468/23838 completed (loss: 3.11309814453125, acc: 0.4000000059604645)
[2025-02-04 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:19][root][INFO] - Training Epoch: 2/2, step 23469/23838 completed (loss: 3.4663453102111816, acc: 0.3571428656578064)
[2025-02-04 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:20][root][INFO] - Training Epoch: 2/2, step 23470/23838 completed (loss: 3.0294787883758545, acc: 0.4444444477558136)
[2025-02-04 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:20][root][INFO] - Training Epoch: 2/2, step 23471/23838 completed (loss: 2.4647066593170166, acc: 0.47826087474823)
[2025-02-04 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:21][root][INFO] - Training Epoch: 2/2, step 23472/23838 completed (loss: 2.9059293270111084, acc: 0.4482758641242981)
[2025-02-04 01:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:21][root][INFO] - Training Epoch: 2/2, step 23473/23838 completed (loss: 2.2954602241516113, acc: 0.5384615659713745)
[2025-02-04 01:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:21][root][INFO] - Training Epoch: 2/2, step 23474/23838 completed (loss: 2.892080068588257, acc: 0.4117647111415863)
[2025-02-04 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:22][root][INFO] - Training Epoch: 2/2, step 23475/23838 completed (loss: 3.257451295852661, acc: 0.31481480598449707)
[2025-02-04 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:22][root][INFO] - Training Epoch: 2/2, step 23476/23838 completed (loss: 2.9784164428710938, acc: 0.25)
[2025-02-04 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:23][root][INFO] - Training Epoch: 2/2, step 23477/23838 completed (loss: 3.205848455429077, acc: 0.29411765933036804)
[2025-02-04 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:23][root][INFO] - Training Epoch: 2/2, step 23478/23838 completed (loss: 2.7609682083129883, acc: 0.375)
[2025-02-04 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:24][root][INFO] - Training Epoch: 2/2, step 23479/23838 completed (loss: 2.089780569076538, acc: 0.625)
[2025-02-04 01:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:24][root][INFO] - Training Epoch: 2/2, step 23480/23838 completed (loss: 3.051210403442383, acc: 0.3243243098258972)
[2025-02-04 01:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:24][root][INFO] - Training Epoch: 2/2, step 23481/23838 completed (loss: 3.3457558155059814, acc: 0.3611111044883728)
[2025-02-04 01:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:25][root][INFO] - Training Epoch: 2/2, step 23482/23838 completed (loss: 2.6519370079040527, acc: 0.38235294818878174)
[2025-02-04 01:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:25][root][INFO] - Training Epoch: 2/2, step 23483/23838 completed (loss: 3.2030811309814453, acc: 0.37142857909202576)
[2025-02-04 01:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:26][root][INFO] - Training Epoch: 2/2, step 23484/23838 completed (loss: 3.6407604217529297, acc: 0.26829269528388977)
[2025-02-04 01:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:26][root][INFO] - Training Epoch: 2/2, step 23485/23838 completed (loss: 3.0647404193878174, acc: 0.30434781312942505)
[2025-02-04 01:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:26][root][INFO] - Training Epoch: 2/2, step 23486/23838 completed (loss: 2.651092767715454, acc: 0.4285714328289032)
[2025-02-04 01:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:27][root][INFO] - Training Epoch: 2/2, step 23487/23838 completed (loss: 3.076730728149414, acc: 0.4375)
[2025-02-04 01:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:27][root][INFO] - Training Epoch: 2/2, step 23488/23838 completed (loss: 2.7263143062591553, acc: 0.3877550959587097)
[2025-02-04 01:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:28][root][INFO] - Training Epoch: 2/2, step 23489/23838 completed (loss: 3.1283204555511475, acc: 0.375)
[2025-02-04 01:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:28][root][INFO] - Training Epoch: 2/2, step 23490/23838 completed (loss: 3.024632692337036, acc: 0.4444444477558136)
[2025-02-04 01:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:29][root][INFO] - Training Epoch: 2/2, step 23491/23838 completed (loss: 3.387441396713257, acc: 0.46666666865348816)
[2025-02-04 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:29][root][INFO] - Training Epoch: 2/2, step 23492/23838 completed (loss: 3.178236722946167, acc: 0.25)
[2025-02-04 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:29][root][INFO] - Training Epoch: 2/2, step 23493/23838 completed (loss: 1.700704574584961, acc: 0.6000000238418579)
[2025-02-04 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:30][root][INFO] - Training Epoch: 2/2, step 23494/23838 completed (loss: 2.3595480918884277, acc: 0.4482758641242981)
[2025-02-04 01:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:30][root][INFO] - Training Epoch: 2/2, step 23495/23838 completed (loss: 2.6121225357055664, acc: 0.3333333432674408)
[2025-02-04 01:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:31][root][INFO] - Training Epoch: 2/2, step 23496/23838 completed (loss: 2.362680435180664, acc: 0.5517241358757019)
[2025-02-04 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:31][root][INFO] - Training Epoch: 2/2, step 23497/23838 completed (loss: 4.227643013000488, acc: 0.3636363744735718)
[2025-02-04 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:31][root][INFO] - Training Epoch: 2/2, step 23498/23838 completed (loss: 2.25254225730896, acc: 0.4583333432674408)
[2025-02-04 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:32][root][INFO] - Training Epoch: 2/2, step 23499/23838 completed (loss: 2.840942859649658, acc: 0.42424243688583374)
[2025-02-04 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:32][root][INFO] - Training Epoch: 2/2, step 23500/23838 completed (loss: 2.880746364593506, acc: 0.47826087474823)
[2025-02-04 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:33][root][INFO] - Training Epoch: 2/2, step 23501/23838 completed (loss: 3.006382703781128, acc: 0.3235294222831726)
[2025-02-04 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:33][root][INFO] - Training Epoch: 2/2, step 23502/23838 completed (loss: 3.275280237197876, acc: 0.29411765933036804)
[2025-02-04 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:34][root][INFO] - Training Epoch: 2/2, step 23503/23838 completed (loss: 2.016947031021118, acc: 0.5714285969734192)
[2025-02-04 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:34][root][INFO] - Training Epoch: 2/2, step 23504/23838 completed (loss: 3.521016836166382, acc: 0.26829269528388977)
[2025-02-04 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:34][root][INFO] - Training Epoch: 2/2, step 23505/23838 completed (loss: 3.5706562995910645, acc: 0.4000000059604645)
[2025-02-04 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:35][root][INFO] - Training Epoch: 2/2, step 23506/23838 completed (loss: 3.519423723220825, acc: 0.3235294222831726)
[2025-02-04 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:35][root][INFO] - Training Epoch: 2/2, step 23507/23838 completed (loss: 2.368313789367676, acc: 0.31578946113586426)
[2025-02-04 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:36][root][INFO] - Training Epoch: 2/2, step 23508/23838 completed (loss: 2.63008189201355, acc: 0.46666666865348816)
[2025-02-04 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:36][root][INFO] - Training Epoch: 2/2, step 23509/23838 completed (loss: 2.749852418899536, acc: 0.375)
[2025-02-04 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:37][root][INFO] - Training Epoch: 2/2, step 23510/23838 completed (loss: 3.0232207775115967, acc: 0.30434781312942505)
[2025-02-04 01:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:37][root][INFO] - Training Epoch: 2/2, step 23511/23838 completed (loss: 2.2748827934265137, acc: 0.4761904776096344)
[2025-02-04 01:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:37][root][INFO] - Training Epoch: 2/2, step 23512/23838 completed (loss: 2.798140525817871, acc: 0.4390243887901306)
[2025-02-04 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:38][root][INFO] - Training Epoch: 2/2, step 23513/23838 completed (loss: 2.963029623031616, acc: 0.3571428656578064)
[2025-02-04 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:38][root][INFO] - Training Epoch: 2/2, step 23514/23838 completed (loss: 3.0171544551849365, acc: 0.4615384638309479)
[2025-02-04 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:39][root][INFO] - Training Epoch: 2/2, step 23515/23838 completed (loss: 3.4059722423553467, acc: 0.3499999940395355)
[2025-02-04 01:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:39][root][INFO] - Training Epoch: 2/2, step 23516/23838 completed (loss: 3.6418793201446533, acc: 0.3235294222831726)
[2025-02-04 01:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:40][root][INFO] - Training Epoch: 2/2, step 23517/23838 completed (loss: 2.805436611175537, acc: 0.36000001430511475)
[2025-02-04 01:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:40][root][INFO] - Training Epoch: 2/2, step 23518/23838 completed (loss: 3.9594411849975586, acc: 0.22033898532390594)
[2025-02-04 01:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:41][root][INFO] - Training Epoch: 2/2, step 23519/23838 completed (loss: 3.1379597187042236, acc: 0.46875)
[2025-02-04 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:41][root][INFO] - Training Epoch: 2/2, step 23520/23838 completed (loss: 4.548755645751953, acc: 0.2954545319080353)
[2025-02-04 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:41][root][INFO] - Training Epoch: 2/2, step 23521/23838 completed (loss: 3.554319381713867, acc: 0.2142857164144516)
[2025-02-04 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:42][root][INFO] - Training Epoch: 2/2, step 23522/23838 completed (loss: 3.2597599029541016, acc: 0.29629629850387573)
[2025-02-04 01:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:42][root][INFO] - Training Epoch: 2/2, step 23523/23838 completed (loss: 3.158735990524292, acc: 0.4285714328289032)
[2025-02-04 01:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:43][root][INFO] - Training Epoch: 2/2, step 23524/23838 completed (loss: 4.126161575317383, acc: 0.3125)
[2025-02-04 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:43][root][INFO] - Training Epoch: 2/2, step 23525/23838 completed (loss: 3.4844765663146973, acc: 0.34090909361839294)
[2025-02-04 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:43][root][INFO] - Training Epoch: 2/2, step 23526/23838 completed (loss: 2.6986186504364014, acc: 0.4038461446762085)
[2025-02-04 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:44][root][INFO] - Training Epoch: 2/2, step 23527/23838 completed (loss: 3.7174365520477295, acc: 0.3333333432674408)
[2025-02-04 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:44][root][INFO] - Training Epoch: 2/2, step 23528/23838 completed (loss: 3.135434865951538, acc: 0.3333333432674408)
[2025-02-04 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:45][root][INFO] - Training Epoch: 2/2, step 23529/23838 completed (loss: 2.6683199405670166, acc: 0.4333333373069763)
[2025-02-04 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:45][root][INFO] - Training Epoch: 2/2, step 23530/23838 completed (loss: 2.6573498249053955, acc: 0.37037035822868347)
[2025-02-04 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:46][root][INFO] - Training Epoch: 2/2, step 23531/23838 completed (loss: 2.674692153930664, acc: 0.3928571343421936)
[2025-02-04 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:46][root][INFO] - Training Epoch: 2/2, step 23532/23838 completed (loss: 3.7127296924591064, acc: 0.3448275923728943)
[2025-02-04 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:46][root][INFO] - Training Epoch: 2/2, step 23533/23838 completed (loss: 3.0959396362304688, acc: 0.4838709533214569)
[2025-02-04 01:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:47][root][INFO] - Training Epoch: 2/2, step 23534/23838 completed (loss: 3.200352191925049, acc: 0.3928571343421936)
[2025-02-04 01:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:47][root][INFO] - Training Epoch: 2/2, step 23535/23838 completed (loss: 2.5810658931732178, acc: 0.3448275923728943)
[2025-02-04 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:48][root][INFO] - Training Epoch: 2/2, step 23536/23838 completed (loss: 3.1048121452331543, acc: 0.3103448152542114)
[2025-02-04 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:48][root][INFO] - Training Epoch: 2/2, step 23537/23838 completed (loss: 1.7901263236999512, acc: 0.6875)
[2025-02-04 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:49][root][INFO] - Training Epoch: 2/2, step 23538/23838 completed (loss: 1.927994728088379, acc: 0.375)
[2025-02-04 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:49][root][INFO] - Training Epoch: 2/2, step 23539/23838 completed (loss: 1.9149727821350098, acc: 0.5151515007019043)
[2025-02-04 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:50][root][INFO] - Training Epoch: 2/2, step 23540/23838 completed (loss: 3.410182237625122, acc: 0.3214285671710968)
[2025-02-04 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:50][root][INFO] - Training Epoch: 2/2, step 23541/23838 completed (loss: 3.1618897914886475, acc: 0.2368421107530594)
[2025-02-04 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:51][root][INFO] - Training Epoch: 2/2, step 23542/23838 completed (loss: 2.470154047012329, acc: 0.42105263471603394)
[2025-02-04 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:51][root][INFO] - Training Epoch: 2/2, step 23543/23838 completed (loss: 2.3534109592437744, acc: 0.5)
[2025-02-04 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:51][root][INFO] - Training Epoch: 2/2, step 23544/23838 completed (loss: 2.440351724624634, acc: 0.40740740299224854)
[2025-02-04 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:52][root][INFO] - Training Epoch: 2/2, step 23545/23838 completed (loss: 3.066521644592285, acc: 0.4615384638309479)
[2025-02-04 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:52][root][INFO] - Training Epoch: 2/2, step 23546/23838 completed (loss: 2.570256233215332, acc: 0.3333333432674408)
[2025-02-04 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:53][root][INFO] - Training Epoch: 2/2, step 23547/23838 completed (loss: 2.5542349815368652, acc: 0.5)
[2025-02-04 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:53][root][INFO] - Training Epoch: 2/2, step 23548/23838 completed (loss: 2.6783430576324463, acc: 0.42424243688583374)
[2025-02-04 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:53][root][INFO] - Training Epoch: 2/2, step 23549/23838 completed (loss: 2.9490485191345215, acc: 0.4166666567325592)
[2025-02-04 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:54][root][INFO] - Training Epoch: 2/2, step 23550/23838 completed (loss: 2.6347618103027344, acc: 0.4583333432674408)
[2025-02-04 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:54][root][INFO] - Training Epoch: 2/2, step 23551/23838 completed (loss: 3.257206916809082, acc: 0.4166666567325592)
[2025-02-04 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:55][root][INFO] - Training Epoch: 2/2, step 23552/23838 completed (loss: 3.444312572479248, acc: 0.3191489279270172)
[2025-02-04 01:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:55][root][INFO] - Training Epoch: 2/2, step 23553/23838 completed (loss: 3.122659206390381, acc: 0.4399999976158142)
[2025-02-04 01:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:56][root][INFO] - Training Epoch: 2/2, step 23554/23838 completed (loss: 2.612863302230835, acc: 0.42307692766189575)
[2025-02-04 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:56][root][INFO] - Training Epoch: 2/2, step 23555/23838 completed (loss: 3.8822181224823, acc: 0.3125)
[2025-02-04 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:56][root][INFO] - Training Epoch: 2/2, step 23556/23838 completed (loss: 2.59753680229187, acc: 0.4583333432674408)
[2025-02-04 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:57][root][INFO] - Training Epoch: 2/2, step 23557/23838 completed (loss: 2.641470432281494, acc: 0.37837839126586914)
[2025-02-04 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:57][root][INFO] - Training Epoch: 2/2, step 23558/23838 completed (loss: 2.9429683685302734, acc: 0.38461539149284363)
[2025-02-04 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:58][root][INFO] - Training Epoch: 2/2, step 23559/23838 completed (loss: 3.247580051422119, acc: 0.3035714328289032)
[2025-02-04 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:58][root][INFO] - Training Epoch: 2/2, step 23560/23838 completed (loss: 3.2251505851745605, acc: 0.47058823704719543)
[2025-02-04 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:59][root][INFO] - Training Epoch: 2/2, step 23561/23838 completed (loss: 3.118605613708496, acc: 0.3913043439388275)
[2025-02-04 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:59][root][INFO] - Training Epoch: 2/2, step 23562/23838 completed (loss: 3.0993692874908447, acc: 0.5333333611488342)
[2025-02-04 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:14:59][root][INFO] - Training Epoch: 2/2, step 23563/23838 completed (loss: 2.1222786903381348, acc: 0.5357142686843872)
[2025-02-04 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:00][root][INFO] - Training Epoch: 2/2, step 23564/23838 completed (loss: 2.260580539703369, acc: 0.5555555820465088)
[2025-02-04 01:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:00][root][INFO] - Training Epoch: 2/2, step 23565/23838 completed (loss: 2.270291805267334, acc: 0.4000000059604645)
[2025-02-04 01:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:01][root][INFO] - Training Epoch: 2/2, step 23566/23838 completed (loss: 3.1813278198242188, acc: 0.380952388048172)
[2025-02-04 01:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:01][root][INFO] - Training Epoch: 2/2, step 23567/23838 completed (loss: 3.3921549320220947, acc: 0.3928571343421936)
[2025-02-04 01:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:01][root][INFO] - Training Epoch: 2/2, step 23568/23838 completed (loss: 2.6947288513183594, acc: 0.34375)
[2025-02-04 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:02][root][INFO] - Training Epoch: 2/2, step 23569/23838 completed (loss: 2.1610069274902344, acc: 0.52173912525177)
[2025-02-04 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:02][root][INFO] - Training Epoch: 2/2, step 23570/23838 completed (loss: 3.7184767723083496, acc: 0.24137930572032928)
[2025-02-04 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:03][root][INFO] - Training Epoch: 2/2, step 23571/23838 completed (loss: 3.415966272354126, acc: 0.40740740299224854)
[2025-02-04 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:03][root][INFO] - Training Epoch: 2/2, step 23572/23838 completed (loss: 2.8007442951202393, acc: 0.4000000059604645)
[2025-02-04 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:03][root][INFO] - Training Epoch: 2/2, step 23573/23838 completed (loss: 3.134361982345581, acc: 0.36000001430511475)
[2025-02-04 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:04][root][INFO] - Training Epoch: 2/2, step 23574/23838 completed (loss: 3.713737726211548, acc: 0.260869562625885)
[2025-02-04 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:04][root][INFO] - Training Epoch: 2/2, step 23575/23838 completed (loss: 2.410932779312134, acc: 0.4324324429035187)
[2025-02-04 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:05][root][INFO] - Training Epoch: 2/2, step 23576/23838 completed (loss: 3.8676226139068604, acc: 0.3571428656578064)
[2025-02-04 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:05][root][INFO] - Training Epoch: 2/2, step 23577/23838 completed (loss: 1.6191110610961914, acc: 0.6315789222717285)
[2025-02-04 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:05][root][INFO] - Training Epoch: 2/2, step 23578/23838 completed (loss: 2.8795700073242188, acc: 0.46341463923454285)
[2025-02-04 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:06][root][INFO] - Training Epoch: 2/2, step 23579/23838 completed (loss: 3.3632590770721436, acc: 0.30188679695129395)
[2025-02-04 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:06][root][INFO] - Training Epoch: 2/2, step 23580/23838 completed (loss: 2.6502082347869873, acc: 0.43478259444236755)
[2025-02-04 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:07][root][INFO] - Training Epoch: 2/2, step 23581/23838 completed (loss: 3.3020119667053223, acc: 0.40625)
[2025-02-04 01:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:07][root][INFO] - Training Epoch: 2/2, step 23582/23838 completed (loss: 3.598560094833374, acc: 0.36000001430511475)
[2025-02-04 01:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:08][root][INFO] - Training Epoch: 2/2, step 23583/23838 completed (loss: 3.7399933338165283, acc: 0.21052631735801697)
[2025-02-04 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:08][root][INFO] - Training Epoch: 2/2, step 23584/23838 completed (loss: 2.3085436820983887, acc: 0.3478260934352875)
[2025-02-04 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:08][root][INFO] - Training Epoch: 2/2, step 23585/23838 completed (loss: 2.824113607406616, acc: 0.517241358757019)
[2025-02-04 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:09][root][INFO] - Training Epoch: 2/2, step 23586/23838 completed (loss: 2.313263416290283, acc: 0.5)
[2025-02-04 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:09][root][INFO] - Training Epoch: 2/2, step 23587/23838 completed (loss: 3.72843861579895, acc: 0.380952388048172)
[2025-02-04 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:10][root][INFO] - Training Epoch: 2/2, step 23588/23838 completed (loss: 2.417989730834961, acc: 0.5)
[2025-02-04 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:10][root][INFO] - Training Epoch: 2/2, step 23589/23838 completed (loss: 2.282470464706421, acc: 0.5)
[2025-02-04 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:11][root][INFO] - Training Epoch: 2/2, step 23590/23838 completed (loss: 3.1856167316436768, acc: 0.3611111044883728)
[2025-02-04 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:11][root][INFO] - Training Epoch: 2/2, step 23591/23838 completed (loss: 4.277352333068848, acc: 0.2142857164144516)
[2025-02-04 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:12][root][INFO] - Training Epoch: 2/2, step 23592/23838 completed (loss: 3.1040165424346924, acc: 0.4193548262119293)
[2025-02-04 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:12][root][INFO] - Training Epoch: 2/2, step 23593/23838 completed (loss: 2.262085199356079, acc: 0.4117647111415863)
[2025-02-04 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:12][root][INFO] - Training Epoch: 2/2, step 23594/23838 completed (loss: 2.6300578117370605, acc: 0.4000000059604645)
[2025-02-04 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:13][root][INFO] - Training Epoch: 2/2, step 23595/23838 completed (loss: 2.0303757190704346, acc: 0.42105263471603394)
[2025-02-04 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:13][root][INFO] - Training Epoch: 2/2, step 23596/23838 completed (loss: 3.404326915740967, acc: 0.3333333432674408)
[2025-02-04 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:14][root][INFO] - Training Epoch: 2/2, step 23597/23838 completed (loss: 3.4242682456970215, acc: 0.2777777910232544)
[2025-02-04 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:14][root][INFO] - Training Epoch: 2/2, step 23598/23838 completed (loss: 3.42630672454834, acc: 0.3333333432674408)
[2025-02-04 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:15][root][INFO] - Training Epoch: 2/2, step 23599/23838 completed (loss: 2.4622159004211426, acc: 0.4285714328289032)
[2025-02-04 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:15][root][INFO] - Training Epoch: 2/2, step 23600/23838 completed (loss: 2.781799077987671, acc: 0.3877550959587097)
[2025-02-04 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:15][root][INFO] - Training Epoch: 2/2, step 23601/23838 completed (loss: 3.400270938873291, acc: 0.43589743971824646)
[2025-02-04 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:16][root][INFO] - Training Epoch: 2/2, step 23602/23838 completed (loss: 3.87898850440979, acc: 0.28947368264198303)
[2025-02-04 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:16][root][INFO] - Training Epoch: 2/2, step 23603/23838 completed (loss: 3.779550313949585, acc: 0.32258063554763794)
[2025-02-04 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:17][root][INFO] - Training Epoch: 2/2, step 23604/23838 completed (loss: 1.9698399305343628, acc: 0.5909090638160706)
[2025-02-04 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:17][root][INFO] - Training Epoch: 2/2, step 23605/23838 completed (loss: 3.1502397060394287, acc: 0.1818181872367859)
[2025-02-04 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:18][root][INFO] - Training Epoch: 2/2, step 23606/23838 completed (loss: 4.607818126678467, acc: 0.2926829159259796)
[2025-02-04 01:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:18][root][INFO] - Training Epoch: 2/2, step 23607/23838 completed (loss: 2.9531993865966797, acc: 0.3030303120613098)
[2025-02-04 01:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:19][root][INFO] - Training Epoch: 2/2, step 23608/23838 completed (loss: 1.914892554283142, acc: 0.5454545617103577)
[2025-02-04 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:19][root][INFO] - Training Epoch: 2/2, step 23609/23838 completed (loss: 2.0706124305725098, acc: 0.529411792755127)
[2025-02-04 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:19][root][INFO] - Training Epoch: 2/2, step 23610/23838 completed (loss: 2.504866361618042, acc: 0.5185185074806213)
[2025-02-04 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:20][root][INFO] - Training Epoch: 2/2, step 23611/23838 completed (loss: 3.9148483276367188, acc: 0.28947368264198303)
[2025-02-04 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:20][root][INFO] - Training Epoch: 2/2, step 23612/23838 completed (loss: 3.570152759552002, acc: 0.38461539149284363)
[2025-02-04 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:21][root][INFO] - Training Epoch: 2/2, step 23613/23838 completed (loss: 3.1072444915771484, acc: 0.3636363744735718)
[2025-02-04 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:21][root][INFO] - Training Epoch: 2/2, step 23614/23838 completed (loss: 2.420254707336426, acc: 0.5416666865348816)
[2025-02-04 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:21][root][INFO] - Training Epoch: 2/2, step 23615/23838 completed (loss: 2.9191951751708984, acc: 0.46875)
[2025-02-04 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:22][root][INFO] - Training Epoch: 2/2, step 23616/23838 completed (loss: 1.690908670425415, acc: 0.5666666626930237)
[2025-02-04 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:22][root][INFO] - Training Epoch: 2/2, step 23617/23838 completed (loss: 4.042414665222168, acc: 0.3571428656578064)
[2025-02-04 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:23][root][INFO] - Training Epoch: 2/2, step 23618/23838 completed (loss: 3.620063304901123, acc: 0.29032257199287415)
[2025-02-04 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:23][root][INFO] - Training Epoch: 2/2, step 23619/23838 completed (loss: 2.468031883239746, acc: 0.37037035822868347)
[2025-02-04 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:23][root][INFO] - Training Epoch: 2/2, step 23620/23838 completed (loss: 2.802467107772827, acc: 0.5199999809265137)
[2025-02-04 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:24][root][INFO] - Training Epoch: 2/2, step 23621/23838 completed (loss: 3.267569065093994, acc: 0.4000000059604645)
[2025-02-04 01:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:24][root][INFO] - Training Epoch: 2/2, step 23622/23838 completed (loss: 2.2494170665740967, acc: 0.5833333134651184)
[2025-02-04 01:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:24][root][INFO] - Training Epoch: 2/2, step 23623/23838 completed (loss: 2.753497838973999, acc: 0.3214285671710968)
[2025-02-04 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:25][root][INFO] - Training Epoch: 2/2, step 23624/23838 completed (loss: 2.426187515258789, acc: 0.4166666567325592)
[2025-02-04 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:25][root][INFO] - Training Epoch: 2/2, step 23625/23838 completed (loss: 2.673491954803467, acc: 0.3947368562221527)
[2025-02-04 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:26][root][INFO] - Training Epoch: 2/2, step 23626/23838 completed (loss: 3.774158239364624, acc: 0.2647058963775635)
[2025-02-04 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:26][root][INFO] - Training Epoch: 2/2, step 23627/23838 completed (loss: 2.7886741161346436, acc: 0.34210526943206787)
[2025-02-04 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:27][root][INFO] - Training Epoch: 2/2, step 23628/23838 completed (loss: 2.74466609954834, acc: 0.4399999976158142)
[2025-02-04 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:27][root][INFO] - Training Epoch: 2/2, step 23629/23838 completed (loss: 1.916390299797058, acc: 0.5185185074806213)
[2025-02-04 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:28][root][INFO] - Training Epoch: 2/2, step 23630/23838 completed (loss: 2.1021697521209717, acc: 0.47826087474823)
[2025-02-04 01:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:28][root][INFO] - Training Epoch: 2/2, step 23631/23838 completed (loss: 2.4107608795166016, acc: 0.5555555820465088)
[2025-02-04 01:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:29][root][INFO] - Training Epoch: 2/2, step 23632/23838 completed (loss: 3.5393471717834473, acc: 0.3396226465702057)
[2025-02-04 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:29][root][INFO] - Training Epoch: 2/2, step 23633/23838 completed (loss: 3.0485148429870605, acc: 0.3928571343421936)
[2025-02-04 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:29][root][INFO] - Training Epoch: 2/2, step 23634/23838 completed (loss: 2.9514951705932617, acc: 0.4000000059604645)
[2025-02-04 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:30][root][INFO] - Training Epoch: 2/2, step 23635/23838 completed (loss: 2.8388609886169434, acc: 0.42307692766189575)
[2025-02-04 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:30][root][INFO] - Training Epoch: 2/2, step 23636/23838 completed (loss: 2.66676926612854, acc: 0.42307692766189575)
[2025-02-04 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:30][root][INFO] - Training Epoch: 2/2, step 23637/23838 completed (loss: 3.0709471702575684, acc: 0.3414634168148041)
[2025-02-04 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:31][root][INFO] - Training Epoch: 2/2, step 23638/23838 completed (loss: 3.0536389350891113, acc: 0.4333333373069763)
[2025-02-04 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:31][root][INFO] - Training Epoch: 2/2, step 23639/23838 completed (loss: 2.237668991088867, acc: 0.4117647111415863)
[2025-02-04 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:32][root][INFO] - Training Epoch: 2/2, step 23640/23838 completed (loss: 2.7356483936309814, acc: 0.36000001430511475)
[2025-02-04 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:32][root][INFO] - Training Epoch: 2/2, step 23641/23838 completed (loss: 2.4100027084350586, acc: 0.47058823704719543)
[2025-02-04 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:33][root][INFO] - Training Epoch: 2/2, step 23642/23838 completed (loss: 2.5467910766601562, acc: 0.4642857015132904)
[2025-02-04 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:33][root][INFO] - Training Epoch: 2/2, step 23643/23838 completed (loss: 2.013965129852295, acc: 0.5384615659713745)
[2025-02-04 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:33][root][INFO] - Training Epoch: 2/2, step 23644/23838 completed (loss: 4.059846878051758, acc: 0.23999999463558197)
[2025-02-04 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:34][root][INFO] - Training Epoch: 2/2, step 23645/23838 completed (loss: 2.2440874576568604, acc: 0.47826087474823)
[2025-02-04 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:34][root][INFO] - Training Epoch: 2/2, step 23646/23838 completed (loss: 3.440411329269409, acc: 0.3611111044883728)
[2025-02-04 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:35][root][INFO] - Training Epoch: 2/2, step 23647/23838 completed (loss: 3.278057098388672, acc: 0.3404255211353302)
[2025-02-04 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:35][root][INFO] - Training Epoch: 2/2, step 23648/23838 completed (loss: 3.263848066329956, acc: 0.27586206793785095)
[2025-02-04 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:36][root][INFO] - Training Epoch: 2/2, step 23649/23838 completed (loss: 3.2497947216033936, acc: 0.42424243688583374)
[2025-02-04 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:36][root][INFO] - Training Epoch: 2/2, step 23650/23838 completed (loss: 3.2855777740478516, acc: 0.4000000059604645)
[2025-02-04 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:37][root][INFO] - Training Epoch: 2/2, step 23651/23838 completed (loss: 2.683079719543457, acc: 0.3684210479259491)
[2025-02-04 01:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:37][root][INFO] - Training Epoch: 2/2, step 23652/23838 completed (loss: 2.443014621734619, acc: 0.4838709533214569)
[2025-02-04 01:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:37][root][INFO] - Training Epoch: 2/2, step 23653/23838 completed (loss: 3.362042188644409, acc: 0.35483869910240173)
[2025-02-04 01:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:38][root][INFO] - Training Epoch: 2/2, step 23654/23838 completed (loss: 3.4213998317718506, acc: 0.3030303120613098)
[2025-02-04 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:38][root][INFO] - Training Epoch: 2/2, step 23655/23838 completed (loss: 2.6928133964538574, acc: 0.4523809552192688)
[2025-02-04 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:39][root][INFO] - Training Epoch: 2/2, step 23656/23838 completed (loss: 3.5328400135040283, acc: 0.42222222685813904)
[2025-02-04 01:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:39][root][INFO] - Training Epoch: 2/2, step 23657/23838 completed (loss: 3.744596004486084, acc: 0.21052631735801697)
[2025-02-04 01:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:40][root][INFO] - Training Epoch: 2/2, step 23658/23838 completed (loss: 2.5194973945617676, acc: 0.4000000059604645)
[2025-02-04 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:40][root][INFO] - Training Epoch: 2/2, step 23659/23838 completed (loss: 2.6335971355438232, acc: 0.4399999976158142)
[2025-02-04 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:40][root][INFO] - Training Epoch: 2/2, step 23660/23838 completed (loss: 2.540692090988159, acc: 0.4399999976158142)
[2025-02-04 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:41][root][INFO] - Training Epoch: 2/2, step 23661/23838 completed (loss: 2.4814212322235107, acc: 0.5)
[2025-02-04 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:41][root][INFO] - Training Epoch: 2/2, step 23662/23838 completed (loss: 3.5200560092926025, acc: 0.4000000059604645)
[2025-02-04 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:42][root][INFO] - Training Epoch: 2/2, step 23663/23838 completed (loss: 3.724870204925537, acc: 0.25925925374031067)
[2025-02-04 01:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:42][root][INFO] - Training Epoch: 2/2, step 23664/23838 completed (loss: 3.034014940261841, acc: 0.3571428656578064)
[2025-02-04 01:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:43][root][INFO] - Training Epoch: 2/2, step 23665/23838 completed (loss: 2.8206706047058105, acc: 0.42500001192092896)
[2025-02-04 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:43][root][INFO] - Training Epoch: 2/2, step 23666/23838 completed (loss: 2.525050401687622, acc: 0.3799999952316284)
[2025-02-04 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:43][root][INFO] - Training Epoch: 2/2, step 23667/23838 completed (loss: 3.348771810531616, acc: 0.3777777850627899)
[2025-02-04 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:44][root][INFO] - Training Epoch: 2/2, step 23668/23838 completed (loss: 2.634526252746582, acc: 0.4722222089767456)
[2025-02-04 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:44][root][INFO] - Training Epoch: 2/2, step 23669/23838 completed (loss: 3.1063954830169678, acc: 0.37735849618911743)
[2025-02-04 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:45][root][INFO] - Training Epoch: 2/2, step 23670/23838 completed (loss: 3.8015663623809814, acc: 0.2857142984867096)
[2025-02-04 01:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:45][root][INFO] - Training Epoch: 2/2, step 23671/23838 completed (loss: 4.226076126098633, acc: 0.25925925374031067)
[2025-02-04 01:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:46][root][INFO] - Training Epoch: 2/2, step 23672/23838 completed (loss: 2.9955124855041504, acc: 0.2857142984867096)
[2025-02-04 01:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:46][root][INFO] - Training Epoch: 2/2, step 23673/23838 completed (loss: 2.9977688789367676, acc: 0.4722222089767456)
[2025-02-04 01:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:46][root][INFO] - Training Epoch: 2/2, step 23674/23838 completed (loss: 2.5037410259246826, acc: 0.34375)
[2025-02-04 01:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:47][root][INFO] - Training Epoch: 2/2, step 23675/23838 completed (loss: 3.905513286590576, acc: 0.2142857164144516)
[2025-02-04 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:47][root][INFO] - Training Epoch: 2/2, step 23676/23838 completed (loss: 4.1831793785095215, acc: 0.2926829159259796)
[2025-02-04 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:47][root][INFO] - Training Epoch: 2/2, step 23677/23838 completed (loss: 2.7540838718414307, acc: 0.3617021143436432)
[2025-02-04 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:48][root][INFO] - Training Epoch: 2/2, step 23678/23838 completed (loss: 2.987734317779541, acc: 0.4545454680919647)
[2025-02-04 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:48][root][INFO] - Training Epoch: 2/2, step 23679/23838 completed (loss: 1.7728261947631836, acc: 0.7647058963775635)
[2025-02-04 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:49][root][INFO] - Training Epoch: 2/2, step 23680/23838 completed (loss: 2.6503243446350098, acc: 0.523809552192688)
[2025-02-04 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:49][root][INFO] - Training Epoch: 2/2, step 23681/23838 completed (loss: 2.895052671432495, acc: 0.4615384638309479)
[2025-02-04 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:50][root][INFO] - Training Epoch: 2/2, step 23682/23838 completed (loss: 3.4770286083221436, acc: 0.4545454680919647)
[2025-02-04 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:50][root][INFO] - Training Epoch: 2/2, step 23683/23838 completed (loss: 2.369292974472046, acc: 0.4516128897666931)
[2025-02-04 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:50][root][INFO] - Training Epoch: 2/2, step 23684/23838 completed (loss: 2.8369460105895996, acc: 0.5185185074806213)
[2025-02-04 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:51][root][INFO] - Training Epoch: 2/2, step 23685/23838 completed (loss: 3.0071840286254883, acc: 0.3877550959587097)
[2025-02-04 01:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:51][root][INFO] - Training Epoch: 2/2, step 23686/23838 completed (loss: 2.7454137802124023, acc: 0.3777777850627899)
[2025-02-04 01:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:51][root][INFO] - Training Epoch: 2/2, step 23687/23838 completed (loss: 2.190446615219116, acc: 0.4444444477558136)
[2025-02-04 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:52][root][INFO] - Training Epoch: 2/2, step 23688/23838 completed (loss: 1.676247477531433, acc: 0.52173912525177)
[2025-02-04 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:52][root][INFO] - Training Epoch: 2/2, step 23689/23838 completed (loss: 3.048919916152954, acc: 0.34210526943206787)
[2025-02-04 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:53][root][INFO] - Training Epoch: 2/2, step 23690/23838 completed (loss: 3.2423958778381348, acc: 0.25)
[2025-02-04 01:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:53][root][INFO] - Training Epoch: 2/2, step 23691/23838 completed (loss: 2.050170421600342, acc: 0.5263158082962036)
[2025-02-04 01:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:54][root][INFO] - Training Epoch: 2/2, step 23692/23838 completed (loss: 2.539731740951538, acc: 0.4375)
[2025-02-04 01:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:54][root][INFO] - Training Epoch: 2/2, step 23693/23838 completed (loss: 3.2423393726348877, acc: 0.3658536672592163)
[2025-02-04 01:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:54][root][INFO] - Training Epoch: 2/2, step 23694/23838 completed (loss: 3.344172239303589, acc: 0.3636363744735718)
[2025-02-04 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:55][root][INFO] - Training Epoch: 2/2, step 23695/23838 completed (loss: 2.696072816848755, acc: 0.4390243887901306)
[2025-02-04 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:55][root][INFO] - Training Epoch: 2/2, step 23696/23838 completed (loss: 2.5328917503356934, acc: 0.4516128897666931)
[2025-02-04 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:56][root][INFO] - Training Epoch: 2/2, step 23697/23838 completed (loss: 2.9832069873809814, acc: 0.4137931168079376)
[2025-02-04 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:56][root][INFO] - Training Epoch: 2/2, step 23698/23838 completed (loss: 3.1174581050872803, acc: 0.3333333432674408)
[2025-02-04 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:57][root][INFO] - Training Epoch: 2/2, step 23699/23838 completed (loss: 3.668229579925537, acc: 0.3499999940395355)
[2025-02-04 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:57][root][INFO] - Training Epoch: 2/2, step 23700/23838 completed (loss: 2.1914336681365967, acc: 0.5)
[2025-02-04 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:58][root][INFO] - Training Epoch: 2/2, step 23701/23838 completed (loss: 1.2423514127731323, acc: 0.7857142686843872)
[2025-02-04 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:58][root][INFO] - Training Epoch: 2/2, step 23702/23838 completed (loss: 0.945303201675415, acc: 0.6666666865348816)
[2025-02-04 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:58][root][INFO] - Training Epoch: 2/2, step 23703/23838 completed (loss: 1.7646750211715698, acc: 0.6363636255264282)
[2025-02-04 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:59][root][INFO] - Training Epoch: 2/2, step 23704/23838 completed (loss: 3.637899875640869, acc: 0.5)
[2025-02-04 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:15:59][root][INFO] - Training Epoch: 2/2, step 23705/23838 completed (loss: 1.917777419090271, acc: 0.75)
[2025-02-04 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:00][root][INFO] - Training Epoch: 2/2, step 23706/23838 completed (loss: 2.008824348449707, acc: 0.5454545617103577)
[2025-02-04 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:00][root][INFO] - Training Epoch: 2/2, step 23707/23838 completed (loss: 2.7049355506896973, acc: 0.5)
[2025-02-04 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:01][root][INFO] - Training Epoch: 2/2, step 23708/23838 completed (loss: 2.935865879058838, acc: 0.5263158082962036)
[2025-02-04 01:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:01][root][INFO] - Training Epoch: 2/2, step 23709/23838 completed (loss: 2.3902206420898438, acc: 0.5)
[2025-02-04 01:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:01][root][INFO] - Training Epoch: 2/2, step 23710/23838 completed (loss: 2.35447359085083, acc: 0.6190476417541504)
[2025-02-04 01:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:02][root][INFO] - Training Epoch: 2/2, step 23711/23838 completed (loss: 2.427001953125, acc: 0.5333333611488342)
[2025-02-04 01:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:02][root][INFO] - Training Epoch: 2/2, step 23712/23838 completed (loss: 2.120103597640991, acc: 0.6153846383094788)
[2025-02-04 01:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:03][root][INFO] - Training Epoch: 2/2, step 23713/23838 completed (loss: 2.71199631690979, acc: 0.4000000059604645)
[2025-02-04 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:03][root][INFO] - Training Epoch: 2/2, step 23714/23838 completed (loss: 1.127241849899292, acc: 0.5)
[2025-02-04 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:03][root][INFO] - Training Epoch: 2/2, step 23715/23838 completed (loss: 2.9206173419952393, acc: 0.4583333432674408)
[2025-02-04 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:04][root][INFO] - Training Epoch: 2/2, step 23716/23838 completed (loss: 3.6096367835998535, acc: 0.4000000059604645)
[2025-02-04 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:04][root][INFO] - Training Epoch: 2/2, step 23717/23838 completed (loss: 3.620220899581909, acc: 0.3913043439388275)
[2025-02-04 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:05][root][INFO] - Training Epoch: 2/2, step 23718/23838 completed (loss: 2.0908350944519043, acc: 0.6153846383094788)
[2025-02-04 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:05][root][INFO] - Training Epoch: 2/2, step 23719/23838 completed (loss: 1.77113676071167, acc: 0.692307710647583)
[2025-02-04 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:06][root][INFO] - Training Epoch: 2/2, step 23720/23838 completed (loss: 3.2102599143981934, acc: 0.27272728085517883)
[2025-02-04 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:06][root][INFO] - Training Epoch: 2/2, step 23721/23838 completed (loss: 2.1148173809051514, acc: 0.5454545617103577)
[2025-02-04 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:07][root][INFO] - Training Epoch: 2/2, step 23722/23838 completed (loss: 2.4595232009887695, acc: 0.4390243887901306)
[2025-02-04 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:07][root][INFO] - Training Epoch: 2/2, step 23723/23838 completed (loss: 1.161191463470459, acc: 0.7777777910232544)
[2025-02-04 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:08][root][INFO] - Training Epoch: 2/2, step 23724/23838 completed (loss: 2.441016912460327, acc: 0.5454545617103577)
[2025-02-04 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:08][root][INFO] - Training Epoch: 2/2, step 23725/23838 completed (loss: 2.298496723175049, acc: 0.529411792755127)
[2025-02-04 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:08][root][INFO] - Training Epoch: 2/2, step 23726/23838 completed (loss: 3.945772886276245, acc: 0.6000000238418579)
[2025-02-04 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:09][root][INFO] - Training Epoch: 2/2, step 23727/23838 completed (loss: 4.42791223526001, acc: 0.3529411852359772)
[2025-02-04 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:09][root][INFO] - Training Epoch: 2/2, step 23728/23838 completed (loss: 2.3354036808013916, acc: 0.5)
[2025-02-04 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:10][root][INFO] - Training Epoch: 2/2, step 23729/23838 completed (loss: 1.9870952367782593, acc: 0.5714285969734192)
[2025-02-04 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:10][root][INFO] - Training Epoch: 2/2, step 23730/23838 completed (loss: 2.6495704650878906, acc: 0.4000000059604645)
[2025-02-04 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:11][root][INFO] - Training Epoch: 2/2, step 23731/23838 completed (loss: 2.2170751094818115, acc: 0.6153846383094788)
[2025-02-04 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:11][root][INFO] - Training Epoch: 2/2, step 23732/23838 completed (loss: 0.9583755731582642, acc: 0.7272727489471436)
[2025-02-04 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:11][root][INFO] - Training Epoch: 2/2, step 23733/23838 completed (loss: 2.1434593200683594, acc: 0.4166666567325592)
[2025-02-04 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:12][root][INFO] - Training Epoch: 2/2, step 23734/23838 completed (loss: 4.41912317276001, acc: 0.3333333432674408)
[2025-02-04 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:12][root][INFO] - Training Epoch: 2/2, step 23735/23838 completed (loss: 2.694247245788574, acc: 0.5454545617103577)
[2025-02-04 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:13][root][INFO] - Training Epoch: 2/2, step 23736/23838 completed (loss: 2.2138168811798096, acc: 0.5)
[2025-02-04 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:13][root][INFO] - Training Epoch: 2/2, step 23737/23838 completed (loss: 2.5894336700439453, acc: 0.47826087474823)
[2025-02-04 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:14][root][INFO] - Training Epoch: 2/2, step 23738/23838 completed (loss: 2.2123217582702637, acc: 0.5263158082962036)
[2025-02-04 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:14][root][INFO] - Training Epoch: 2/2, step 23739/23838 completed (loss: 2.517582654953003, acc: 0.5555555820465088)
[2025-02-04 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:15][root][INFO] - Training Epoch: 2/2, step 23740/23838 completed (loss: 1.3886207342147827, acc: 0.7272727489471436)
[2025-02-04 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:15][root][INFO] - Training Epoch: 2/2, step 23741/23838 completed (loss: 3.054550886154175, acc: 0.4000000059604645)
[2025-02-04 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:15][root][INFO] - Training Epoch: 2/2, step 23742/23838 completed (loss: 3.206920623779297, acc: 0.42105263471603394)
[2025-02-04 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:16][root][INFO] - Training Epoch: 2/2, step 23743/23838 completed (loss: 3.7266855239868164, acc: 0.4375)
[2025-02-04 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:16][root][INFO] - Training Epoch: 2/2, step 23744/23838 completed (loss: 2.0249674320220947, acc: 0.48571428656578064)
[2025-02-04 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:17][root][INFO] - Training Epoch: 2/2, step 23745/23838 completed (loss: 2.3341736793518066, acc: 0.5454545617103577)
[2025-02-04 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:17][root][INFO] - Training Epoch: 2/2, step 23746/23838 completed (loss: 3.0113437175750732, acc: 0.4000000059604645)
[2025-02-04 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:18][root][INFO] - Training Epoch: 2/2, step 23747/23838 completed (loss: 2.9466257095336914, acc: 0.5625)
[2025-02-04 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:18][root][INFO] - Training Epoch: 2/2, step 23748/23838 completed (loss: 2.669269323348999, acc: 0.4117647111415863)
[2025-02-04 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:18][root][INFO] - Training Epoch: 2/2, step 23749/23838 completed (loss: 2.171182870864868, acc: 0.3888888955116272)
[2025-02-04 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:19][root][INFO] - Training Epoch: 2/2, step 23750/23838 completed (loss: 1.0991636514663696, acc: 0.6666666865348816)
[2025-02-04 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:19][root][INFO] - Training Epoch: 2/2, step 23751/23838 completed (loss: 3.892951250076294, acc: 0.3684210479259491)
[2025-02-04 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:20][root][INFO] - Training Epoch: 2/2, step 23752/23838 completed (loss: 3.166255474090576, acc: 0.6000000238418579)
[2025-02-04 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:20][root][INFO] - Training Epoch: 2/2, step 23753/23838 completed (loss: 0.5234713554382324, acc: 0.9166666865348816)
[2025-02-04 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:21][root][INFO] - Training Epoch: 2/2, step 23754/23838 completed (loss: 0.7214728593826294, acc: 0.8571428656578064)
[2025-02-04 01:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:21][root][INFO] - Training Epoch: 2/2, step 23755/23838 completed (loss: 3.1427526473999023, acc: 0.3913043439388275)
[2025-02-04 01:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:22][root][INFO] - Training Epoch: 2/2, step 23756/23838 completed (loss: 2.820310115814209, acc: 0.6000000238418579)
[2025-02-04 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:22][root][INFO] - Training Epoch: 2/2, step 23757/23838 completed (loss: 3.2051825523376465, acc: 0.5384615659713745)
[2025-02-04 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:22][root][INFO] - Training Epoch: 2/2, step 23758/23838 completed (loss: 3.15055775642395, acc: 0.5625)
[2025-02-04 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:23][root][INFO] - Training Epoch: 2/2, step 23759/23838 completed (loss: 0.6420680284500122, acc: 0.8461538553237915)
[2025-02-04 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:23][root][INFO] - Training Epoch: 2/2, step 23760/23838 completed (loss: 4.960648536682129, acc: 0.2666666805744171)
[2025-02-04 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:24][root][INFO] - Training Epoch: 2/2, step 23761/23838 completed (loss: 1.619184970855713, acc: 0.5)
[2025-02-04 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:24][root][INFO] - Training Epoch: 2/2, step 23762/23838 completed (loss: 3.3077383041381836, acc: 0.3636363744735718)
[2025-02-04 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:25][root][INFO] - Training Epoch: 2/2, step 23763/23838 completed (loss: 3.3041679859161377, acc: 0.34210526943206787)
[2025-02-04 01:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:25][root][INFO] - Training Epoch: 2/2, step 23764/23838 completed (loss: 3.24702525138855, acc: 0.3913043439388275)
[2025-02-04 01:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:26][root][INFO] - Training Epoch: 2/2, step 23765/23838 completed (loss: 3.397911787033081, acc: 0.32499998807907104)
[2025-02-04 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:26][root][INFO] - Training Epoch: 2/2, step 23766/23838 completed (loss: 1.299108862876892, acc: 0.6666666865348816)
[2025-02-04 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:27][root][INFO] - Training Epoch: 2/2, step 23767/23838 completed (loss: 2.4002797603607178, acc: 0.5263158082962036)
[2025-02-04 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:27][root][INFO] - Training Epoch: 2/2, step 23768/23838 completed (loss: 1.3131247758865356, acc: 0.7777777910232544)
[2025-02-04 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:28][root][INFO] - Training Epoch: 2/2, step 23769/23838 completed (loss: 2.5524513721466064, acc: 0.5625)
[2025-02-04 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:28][root][INFO] - Training Epoch: 2/2, step 23770/23838 completed (loss: 2.233541250228882, acc: 0.5416666865348816)
[2025-02-04 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:29][root][INFO] - Training Epoch: 2/2, step 23771/23838 completed (loss: 1.7323259115219116, acc: 0.5625)
[2025-02-04 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:29][root][INFO] - Training Epoch: 2/2, step 23772/23838 completed (loss: 1.527066946029663, acc: 0.6206896305084229)
[2025-02-04 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:30][root][INFO] - Training Epoch: 2/2, step 23773/23838 completed (loss: 1.0105211734771729, acc: 0.75)
[2025-02-04 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:30][root][INFO] - Training Epoch: 2/2, step 23774/23838 completed (loss: 2.055119752883911, acc: 0.523809552192688)
[2025-02-04 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:31][root][INFO] - Training Epoch: 2/2, step 23775/23838 completed (loss: 1.1967008113861084, acc: 0.692307710647583)
[2025-02-04 01:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:31][root][INFO] - Training Epoch: 2/2, step 23776/23838 completed (loss: 2.2247838973999023, acc: 0.6538461446762085)
[2025-02-04 01:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:32][root][INFO] - Training Epoch: 2/2, step 23777/23838 completed (loss: 2.093189001083374, acc: 0.6153846383094788)
[2025-02-04 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:32][root][INFO] - Training Epoch: 2/2, step 23778/23838 completed (loss: 2.14186954498291, acc: 0.5277777910232544)
[2025-02-04 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:33][root][INFO] - Training Epoch: 2/2, step 23779/23838 completed (loss: 1.506041169166565, acc: 0.75)
[2025-02-04 01:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:33][root][INFO] - Training Epoch: 2/2, step 23780/23838 completed (loss: 2.4210495948791504, acc: 0.5714285969734192)
[2025-02-04 01:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:34][root][INFO] - Training Epoch: 2/2, step 23781/23838 completed (loss: 0.9293221831321716, acc: 0.7333333492279053)
[2025-02-04 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:34][root][INFO] - Training Epoch: 2/2, step 23782/23838 completed (loss: 1.4424927234649658, acc: 0.6666666865348816)
[2025-02-04 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:34][root][INFO] - Training Epoch: 2/2, step 23783/23838 completed (loss: 2.826646327972412, acc: 0.5555555820465088)
[2025-02-04 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:35][root][INFO] - Training Epoch: 2/2, step 23784/23838 completed (loss: 1.371696949005127, acc: 0.699999988079071)
[2025-02-04 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:35][root][INFO] - Training Epoch: 2/2, step 23785/23838 completed (loss: 1.83207106590271, acc: 0.6666666865348816)
[2025-02-04 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:36][root][INFO] - Training Epoch: 2/2, step 23786/23838 completed (loss: 1.7044525146484375, acc: 0.5263158082962036)
[2025-02-04 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:36][root][INFO] - Training Epoch: 2/2, step 23787/23838 completed (loss: 2.423956871032715, acc: 0.5454545617103577)
[2025-02-04 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:37][root][INFO] - Training Epoch: 2/2, step 23788/23838 completed (loss: 1.7610678672790527, acc: 0.5555555820465088)
[2025-02-04 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:37][root][INFO] - Training Epoch: 2/2, step 23789/23838 completed (loss: 2.1818501949310303, acc: 0.529411792755127)
[2025-02-04 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:38][root][INFO] - Training Epoch: 2/2, step 23790/23838 completed (loss: 0.8533327579498291, acc: 0.7777777910232544)
[2025-02-04 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:38][root][INFO] - Training Epoch: 2/2, step 23791/23838 completed (loss: 0.9088591933250427, acc: 0.7692307829856873)
[2025-02-04 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:38][root][INFO] - Training Epoch: 2/2, step 23792/23838 completed (loss: 1.298388123512268, acc: 0.8125)
[2025-02-04 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:39][root][INFO] - Training Epoch: 2/2, step 23793/23838 completed (loss: 3.988084077835083, acc: 0.4583333432674408)
[2025-02-04 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:39][root][INFO] - Training Epoch: 2/2, step 23794/23838 completed (loss: 2.4133293628692627, acc: 0.5357142686843872)
[2025-02-04 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:40][root][INFO] - Training Epoch: 2/2, step 23795/23838 completed (loss: 1.3967769145965576, acc: 0.7241379022598267)
[2025-02-04 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:40][root][INFO] - Training Epoch: 2/2, step 23796/23838 completed (loss: 1.5893428325653076, acc: 0.6470588445663452)
[2025-02-04 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:41][root][INFO] - Training Epoch: 2/2, step 23797/23838 completed (loss: 0.420319527387619, acc: 0.9285714030265808)
[2025-02-04 01:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:41][root][INFO] - Training Epoch: 2/2, step 23798/23838 completed (loss: 1.1404974460601807, acc: 0.800000011920929)
[2025-02-04 01:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:41][root][INFO] - Training Epoch: 2/2, step 23799/23838 completed (loss: 2.8742122650146484, acc: 0.529411792755127)
[2025-02-04 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:42][root][INFO] - Training Epoch: 2/2, step 23800/23838 completed (loss: 2.2693700790405273, acc: 0.5)
[2025-02-04 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:42][root][INFO] - Training Epoch: 2/2, step 23801/23838 completed (loss: 1.021461844444275, acc: 0.75)
[2025-02-04 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:43][root][INFO] - Training Epoch: 2/2, step 23802/23838 completed (loss: 4.606151580810547, acc: 0.25)
[2025-02-04 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:43][root][INFO] - Training Epoch: 2/2, step 23803/23838 completed (loss: 2.802600860595703, acc: 0.5625)
[2025-02-04 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:43][root][INFO] - Training Epoch: 2/2, step 23804/23838 completed (loss: 2.1944689750671387, acc: 0.47058823704719543)
[2025-02-04 01:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:44][root][INFO] - Training Epoch: 2/2, step 23805/23838 completed (loss: 2.6741559505462646, acc: 0.48148149251937866)
[2025-02-04 01:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:44][root][INFO] - Training Epoch: 2/2, step 23806/23838 completed (loss: 1.8344694375991821, acc: 0.5277777910232544)
[2025-02-04 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:45][root][INFO] - Training Epoch: 2/2, step 23807/23838 completed (loss: 1.9075419902801514, acc: 0.5714285969734192)
[2025-02-04 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:46][root][INFO] - Training Epoch: 2/2, step 23808/23838 completed (loss: 3.1091339588165283, acc: 0.5)
[2025-02-04 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:46][root][INFO] - Training Epoch: 2/2, step 23809/23838 completed (loss: 1.8639887571334839, acc: 0.5416666865348816)
[2025-02-04 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:47][root][INFO] - Training Epoch: 2/2, step 23810/23838 completed (loss: 2.703964948654175, acc: 0.5600000023841858)
[2025-02-04 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:47][root][INFO] - Training Epoch: 2/2, step 23811/23838 completed (loss: 2.560356616973877, acc: 0.625)
[2025-02-04 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:48][root][INFO] - Training Epoch: 2/2, step 23812/23838 completed (loss: 1.831254005432129, acc: 0.7857142686843872)
[2025-02-04 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:48][root][INFO] - Training Epoch: 2/2, step 23813/23838 completed (loss: 2.130728006362915, acc: 0.7142857313156128)
[2025-02-04 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:49][root][INFO] - Training Epoch: 2/2, step 23814/23838 completed (loss: 2.279024600982666, acc: 0.6060606241226196)
[2025-02-04 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:49][root][INFO] - Training Epoch: 2/2, step 23815/23838 completed (loss: 2.2425343990325928, acc: 0.5862069129943848)
[2025-02-04 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:50][root][INFO] - Training Epoch: 2/2, step 23816/23838 completed (loss: 4.12617826461792, acc: 0.4000000059604645)
[2025-02-04 01:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:50][root][INFO] - Training Epoch: 2/2, step 23817/23838 completed (loss: 2.2852842807769775, acc: 0.5416666865348816)
[2025-02-04 01:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:51][root][INFO] - Training Epoch: 2/2, step 23818/23838 completed (loss: 4.076322555541992, acc: 0.25641027092933655)
[2025-02-04 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:51][root][INFO] - Training Epoch: 2/2, step 23819/23838 completed (loss: 3.895092487335205, acc: 0.45945945382118225)
[2025-02-04 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:51][root][INFO] - Training Epoch: 2/2, step 23820/23838 completed (loss: 4.482293128967285, acc: 0.25)
[2025-02-04 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:52][root][INFO] - Training Epoch: 2/2, step 23821/23838 completed (loss: 3.418736457824707, acc: 0.4193548262119293)
[2025-02-04 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:52][root][INFO] - Training Epoch: 2/2, step 23822/23838 completed (loss: 2.9637222290039062, acc: 0.4545454680919647)
[2025-02-04 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:53][root][INFO] - Training Epoch: 2/2, step 23823/23838 completed (loss: 3.7425506114959717, acc: 0.2800000011920929)
[2025-02-04 01:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:53][root][INFO] - Training Epoch: 2/2, step 23824/23838 completed (loss: 4.498381614685059, acc: 0.23529411852359772)
[2025-02-04 01:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:54][root][INFO] - Training Epoch: 2/2, step 23825/23838 completed (loss: 3.170241355895996, acc: 0.3333333432674408)
[2025-02-04 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:54][root][INFO] - Training Epoch: 2/2, step 23826/23838 completed (loss: 3.3726632595062256, acc: 0.4761904776096344)
[2025-02-04 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:54][root][INFO] - Training Epoch: 2/2, step 23827/23838 completed (loss: 3.73762845993042, acc: 0.25)
[2025-02-04 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:55][root][INFO] - Training Epoch: 2/2, step 23828/23838 completed (loss: 3.5274105072021484, acc: 0.30000001192092896)
[2025-02-04 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:55][root][INFO] - Training Epoch: 2/2, step 23829/23838 completed (loss: 3.231722354888916, acc: 0.4000000059604645)
[2025-02-04 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:56][root][INFO] - Training Epoch: 2/2, step 23830/23838 completed (loss: 2.869290351867676, acc: 0.37037035822868347)
[2025-02-04 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:56][root][INFO] - Training Epoch: 2/2, step 23831/23838 completed (loss: 3.015199899673462, acc: 0.25925925374031067)
[2025-02-04 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:57][root][INFO] - Training Epoch: 2/2, step 23832/23838 completed (loss: 3.020112991333008, acc: 0.3529411852359772)
[2025-02-04 01:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:57][root][INFO] - Training Epoch: 2/2, step 23833/23838 completed (loss: 3.4056396484375, acc: 0.34375)
[2025-02-04 01:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:26][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(33.2427, device='cuda:0') eval_epoch_loss=tensor(3.5038, device='cuda:0') eval_epoch_acc=tensor(0.3780, device='cuda:0')
[2025-02-04 01:38:26][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-04 01:38:26][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-04 01:38:27][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_23834_loss_3.503835439682007/model.pt
[2025-02-04 01:38:27][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft directory
[2025-02-04 01:38:27][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 3.503835439682007
[2025-02-04 01:38:27][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.3780093789100647
[2025-02-04 01:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:28][root][INFO] - Training Epoch: 2/2, step 23834/23838 completed (loss: 4.213386058807373, acc: 0.2291666716337204)
[2025-02-04 01:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:28][root][INFO] - Training Epoch: 2/2, step 23835/23838 completed (loss: 1.9555729627609253, acc: 0.5625)
[2025-02-04 01:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:29][root][INFO] - Training Epoch: 2/2, step 23836/23838 completed (loss: 2.882960319519043, acc: 0.3947368562221527)
[2025-02-04 01:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:29][root][INFO] - Training Epoch: 2/2, step 23837/23838 completed (loss: 3.6709282398223877, acc: 0.3529411852359772)
[2025-02-04 01:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:38:30][root][INFO] - Training Epoch: 2/2, step 23838/23838 completed (loss: 2.968832492828369, acc: 0.2647058963775635)
[2025-02-04 01:38:31][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=2.1129, train_epoch_loss=0.7481, epoch time 3922.6177120916545s
[2025-02-04 01:38:31][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 15 GB
[2025-02-04 01:38:31][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 20 GB
[2025-02-04 01:38:31][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 15 GB
[2025-02-04 01:38:31][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-04 01:38:31][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-04 01:38:31][root][INFO] - Key: avg_train_prep, Value: 2.1129260063171387
[2025-02-04 01:38:31][root][INFO] - Key: avg_train_loss, Value: 0.7480736970901489
[2025-02-04 01:38:31][root][INFO] - Key: avg_train_acc, Value: 0.11014064401388168
[2025-02-04 01:38:31][root][INFO] - Key: avg_eval_prep, Value: 33.24271011352539
[2025-02-04 01:38:31][root][INFO] - Key: avg_eval_loss, Value: 3.503835439682007
[2025-02-04 01:38:31][root][INFO] - Key: avg_eval_acc, Value: 0.3780093789100647
[2025-02-04 01:38:31][root][INFO] - Key: avg_epoch_time, Value: 3922.6177120916545
[2025-02-04 01:38:31][root][INFO] - Key: avg_checkpoint_time, Value: 0.9530314244329929
Selected lowest loss checkpoint: asr_epoch_2_step_17875_loss_3.3814234733581543
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_3.3814234733581543/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_3.3814234733581543
[2025-02-04 01:39:09][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-04 01:39:09][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-04 01:39:09][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'q-former', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_wavlm_llama32_1b_q-former_peft'}
[2025-02-04 01:39:11][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-04 01:39:17][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-04 01:39:17][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-04 01:39:17][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-04 01:39:17][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-04 01:39:23][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-04 01:39:23][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-04 01:39:23][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-04 01:39:23][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-04 01:39:23][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-04 01:39:24][slam_llm.utils.train_utils][INFO] - --> Module q-former
[2025-02-04 01:39:24][slam_llm.utils.train_utils][INFO] - --> q-former has 69.361152 Million params

[2025-02-04 01:39:24][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_3.3814234733581543/model.pt
[2025-02-04 01:39:24][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-04 01:39:24][slam_llm.utils.train_utils][INFO] - --> asr has 74.997248 Million params

[2025-02-04 01:39:28][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-04 01:39:29][root][INFO] - --> Training Set Length = 12232
[2025-02-04 01:39:29][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/test_config.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-02-04 01:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 01:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-04 02:25:13][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/decode_test_beam4_pred_20250204_013929
[2025-02-04 02:25:13][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/decode_test_beam4_gt_20250204_013929
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/decode_test_beam4_gt_20250204_013929
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_q-former_peft/decode_test_beam4_pred_20250204_013929
Combined WER: 0.9091950593710842

Filtering repeated words...

Found 30 repeated lines in total.
Repeated lines are:
- one shoe shoe shoe shoe
- the the the the the the
- many many many many
- and they got the the the the
- one shoe shoe shoe shoe shoe
- oh no no no no no
- well she's going to the ball and a fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy
- the the the the the the the
- we we we we we we we
- prince prince prince prince
- they they they they they they
- oh no no no no
- shoe shoe shoe shoe shoe shoe shoe shoe
- it was the first stroke stroke stroke stroke stroke stroke
- it was very good because it was a stroke stroke stroke stroke stroke stroke stroke stroke stroke stroke stroke stroke stroke stroke
- so she went to a fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy
- and her fairy fairy fairy fairy fairy fairy fairy
- so we we we we we we we we we we we we
- and her fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy
- there is a fairy fairy fairy fairy fairy fairy fairy fairy fairy
- and there is a shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe
- and there's a fairy fairy fairy fairy fairy fairy fairy
- one shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe shoe
- and cinderella and fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy
- and fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy fairy
- but it was a long time ago the stroke stroke stroke stroke stroke stroke stroke
- the shoe shoe shoe shoe
- many many many many many
- your shoe shoe shoe shoe
- one shoe shoe shoe shoe shoe shoe
Filtered Combined WER: 0.8942295042755486
