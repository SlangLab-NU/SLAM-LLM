/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
train_data_folder: psst_phoneme
test_data_folder: psst_phoneme
use_peft: true
seed: 
debug: 
Is test_run? 
freeze_encoder: true
Is save_embedding? false
projector_transfer_learning: true
transfer_data_folder: librispeech-100_phoneme
llm_inference_config: repetition_penalty
eval_ckpt: best
----------
----------
Final identifier: psst_phoneme_wavlm_llama32_1b_linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231



----- Transfer Learning Information -----
Resume Epoch: 1
Resume Step: 0
Train Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/train.jsonl
Validation Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/validation.jsonl
Test Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test.jsonl
Identifier: psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme
Output Directory: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme
----------------------------------------
----------------------------------------
Resume epoch: 1
Resume step: 0
[2025-02-13 02:23:34][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-13 02:23:34][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 02:23:34][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme'}
[2025-02-13 02:23:34][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-13_02-23-34.txt', 'log_interval': 5}
[2025-02-13 02:23:54][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 02:24:00][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 02:24:00][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 02:24:00][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 02:24:00][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 02:24:05][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 02:24:05][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 02:24:05][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 02:24:05][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 02:24:05][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 02:24:05][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 02:24:05][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 02:24:05][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_10_step_554_loss_1.0206900835037231/model.pt
[2025-02-13 02:24:05][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 02:24:05][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 02:24:08][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 02:24:09][root][INFO] - --> Training Set Length = 28539
[2025-02-13 02:24:09][root][INFO] - --> Validation Set Length = 2703
[2025-02-13 02:24:09][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 02:24:09][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 02:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:11][root][INFO] - Training Epoch: 1/2, step 0/7134 completed (loss: 2.260176420211792, acc: 0.5557206273078918)
[2025-02-13 02:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:12][root][INFO] - Training Epoch: 1/2, step 1/7134 completed (loss: 2.431884288787842, acc: 0.509065568447113)
[2025-02-13 02:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:12][root][INFO] - Training Epoch: 1/2, step 2/7134 completed (loss: 2.3166098594665527, acc: 0.4939759075641632)
[2025-02-13 02:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:13][root][INFO] - Training Epoch: 1/2, step 3/7134 completed (loss: 2.176783800125122, acc: 0.5211267471313477)
[2025-02-13 02:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:13][root][INFO] - Training Epoch: 1/2, step 4/7134 completed (loss: 2.3043711185455322, acc: 0.48567530512809753)
[2025-02-13 02:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:14][root][INFO] - Training Epoch: 1/2, step 5/7134 completed (loss: 2.358380079269409, acc: 0.5038071274757385)
[2025-02-13 02:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:14][root][INFO] - Training Epoch: 1/2, step 6/7134 completed (loss: 2.2279136180877686, acc: 0.5072231292724609)
[2025-02-13 02:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:15][root][INFO] - Training Epoch: 1/2, step 7/7134 completed (loss: 2.4676787853240967, acc: 0.4769614040851593)
[2025-02-13 02:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:15][root][INFO] - Training Epoch: 1/2, step 8/7134 completed (loss: 2.348905086517334, acc: 0.47867950797080994)
[2025-02-13 02:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:16][root][INFO] - Training Epoch: 1/2, step 9/7134 completed (loss: 2.397805690765381, acc: 0.5240309834480286)
[2025-02-13 02:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:16][root][INFO] - Training Epoch: 1/2, step 10/7134 completed (loss: 2.597505807876587, acc: 0.4509246051311493)
[2025-02-13 02:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:17][root][INFO] - Training Epoch: 1/2, step 11/7134 completed (loss: 2.2379262447357178, acc: 0.5365853905677795)
[2025-02-13 02:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:17][root][INFO] - Training Epoch: 1/2, step 12/7134 completed (loss: 2.201561212539673, acc: 0.5006729364395142)
[2025-02-13 02:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:18][root][INFO] - Training Epoch: 1/2, step 13/7134 completed (loss: 2.154956102371216, acc: 0.5158184170722961)
[2025-02-13 02:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:18][root][INFO] - Training Epoch: 1/2, step 14/7134 completed (loss: 2.2271029949188232, acc: 0.5512605309486389)
[2025-02-13 02:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:18][root][INFO] - Training Epoch: 1/2, step 15/7134 completed (loss: 2.1439809799194336, acc: 0.5611015558242798)
[2025-02-13 02:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:19][root][INFO] - Training Epoch: 1/2, step 16/7134 completed (loss: 2.1645631790161133, acc: 0.5528455376625061)
[2025-02-13 02:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:19][root][INFO] - Training Epoch: 1/2, step 17/7134 completed (loss: 2.3043055534362793, acc: 0.5540540814399719)
[2025-02-13 02:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:20][root][INFO] - Training Epoch: 1/2, step 18/7134 completed (loss: 2.2217345237731934, acc: 0.5255681872367859)
[2025-02-13 02:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:20][root][INFO] - Training Epoch: 1/2, step 19/7134 completed (loss: 2.293184280395508, acc: 0.5075861811637878)
[2025-02-13 02:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:21][root][INFO] - Training Epoch: 1/2, step 20/7134 completed (loss: 2.0943543910980225, acc: 0.5263158082962036)
[2025-02-13 02:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:21][root][INFO] - Training Epoch: 1/2, step 21/7134 completed (loss: 2.1166024208068848, acc: 0.5025773048400879)
[2025-02-13 02:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:21][root][INFO] - Training Epoch: 1/2, step 22/7134 completed (loss: 2.0580027103424072, acc: 0.5403458476066589)
[2025-02-13 02:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:22][root][INFO] - Training Epoch: 1/2, step 23/7134 completed (loss: 1.9904992580413818, acc: 0.55027174949646)
[2025-02-13 02:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:22][root][INFO] - Training Epoch: 1/2, step 24/7134 completed (loss: 1.9730256795883179, acc: 0.5529248118400574)
[2025-02-13 02:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:23][root][INFO] - Training Epoch: 1/2, step 25/7134 completed (loss: 1.9693783521652222, acc: 0.5340909361839294)
[2025-02-13 02:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:23][root][INFO] - Training Epoch: 1/2, step 26/7134 completed (loss: 1.8654817342758179, acc: 0.5648967623710632)
[2025-02-13 02:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:24][root][INFO] - Training Epoch: 1/2, step 27/7134 completed (loss: 1.7373212575912476, acc: 0.6129985451698303)
[2025-02-13 02:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:24][root][INFO] - Training Epoch: 1/2, step 28/7134 completed (loss: 2.1121156215667725, acc: 0.5325301289558411)
[2025-02-13 02:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:25][root][INFO] - Training Epoch: 1/2, step 29/7134 completed (loss: 2.1882026195526123, acc: 0.5099149942398071)
[2025-02-13 02:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:25][root][INFO] - Training Epoch: 1/2, step 30/7134 completed (loss: 2.04516863822937, acc: 0.5220338702201843)
[2025-02-13 02:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:26][root][INFO] - Training Epoch: 1/2, step 31/7134 completed (loss: 2.0604817867279053, acc: 0.4925847351551056)
[2025-02-13 02:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:26][root][INFO] - Training Epoch: 1/2, step 32/7134 completed (loss: 2.1946587562561035, acc: 0.459037721157074)
[2025-02-13 02:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:27][root][INFO] - Training Epoch: 1/2, step 33/7134 completed (loss: 2.1200785636901855, acc: 0.5358565449714661)
[2025-02-13 02:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:27][root][INFO] - Training Epoch: 1/2, step 34/7134 completed (loss: 2.0987069606781006, acc: 0.5310668349266052)
[2025-02-13 02:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:28][root][INFO] - Training Epoch: 1/2, step 35/7134 completed (loss: 1.9443883895874023, acc: 0.5117813348770142)
[2025-02-13 02:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:28][root][INFO] - Training Epoch: 1/2, step 36/7134 completed (loss: 1.9567351341247559, acc: 0.5407165884971619)
[2025-02-13 02:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:28][root][INFO] - Training Epoch: 1/2, step 37/7134 completed (loss: 1.7448383569717407, acc: 0.5544430613517761)
[2025-02-13 02:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:29][root][INFO] - Training Epoch: 1/2, step 38/7134 completed (loss: 1.817940354347229, acc: 0.5565476417541504)
[2025-02-13 02:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:29][root][INFO] - Training Epoch: 1/2, step 39/7134 completed (loss: 1.6201969385147095, acc: 0.5872800946235657)
[2025-02-13 02:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:30][root][INFO] - Training Epoch: 1/2, step 40/7134 completed (loss: 1.824886679649353, acc: 0.5896860957145691)
[2025-02-13 02:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:30][root][INFO] - Training Epoch: 1/2, step 41/7134 completed (loss: 1.7734125852584839, acc: 0.582524299621582)
[2025-02-13 02:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:31][root][INFO] - Training Epoch: 1/2, step 42/7134 completed (loss: 1.6469498872756958, acc: 0.5722300410270691)
[2025-02-13 02:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:31][root][INFO] - Training Epoch: 1/2, step 43/7134 completed (loss: 1.4634170532226562, acc: 0.6180645227432251)
[2025-02-13 02:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:32][root][INFO] - Training Epoch: 1/2, step 44/7134 completed (loss: 1.5499396324157715, acc: 0.6203821897506714)
[2025-02-13 02:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:32][root][INFO] - Training Epoch: 1/2, step 45/7134 completed (loss: 1.5852160453796387, acc: 0.5893824696540833)
[2025-02-13 02:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:32][root][INFO] - Training Epoch: 1/2, step 46/7134 completed (loss: 1.5565123558044434, acc: 0.6359999775886536)
[2025-02-13 02:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:33][root][INFO] - Training Epoch: 1/2, step 47/7134 completed (loss: 1.6031157970428467, acc: 0.6421356201171875)
[2025-02-13 02:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:33][root][INFO] - Training Epoch: 1/2, step 48/7134 completed (loss: 1.439233422279358, acc: 0.6235954761505127)
[2025-02-13 02:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:34][root][INFO] - Training Epoch: 1/2, step 49/7134 completed (loss: 1.3644226789474487, acc: 0.6241426467895508)
[2025-02-13 02:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:34][root][INFO] - Training Epoch: 1/2, step 50/7134 completed (loss: 1.4969511032104492, acc: 0.6136101484298706)
[2025-02-13 02:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:35][root][INFO] - Training Epoch: 1/2, step 51/7134 completed (loss: 1.3278170824050903, acc: 0.6713197827339172)
[2025-02-13 02:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:35][root][INFO] - Training Epoch: 1/2, step 52/7134 completed (loss: 1.3975400924682617, acc: 0.6520324945449829)
[2025-02-13 02:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:36][root][INFO] - Training Epoch: 1/2, step 53/7134 completed (loss: 1.3571503162384033, acc: 0.6325459480285645)
[2025-02-13 02:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:36][root][INFO] - Training Epoch: 1/2, step 54/7134 completed (loss: 1.2623183727264404, acc: 0.6651323437690735)
[2025-02-13 02:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:37][root][INFO] - Training Epoch: 1/2, step 55/7134 completed (loss: 1.2769818305969238, acc: 0.6598984599113464)
[2025-02-13 02:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:37][root][INFO] - Training Epoch: 1/2, step 56/7134 completed (loss: 1.25783109664917, acc: 0.6660714149475098)
[2025-02-13 02:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:37][root][INFO] - Training Epoch: 1/2, step 57/7134 completed (loss: 1.3061635494232178, acc: 0.6583850979804993)
[2025-02-13 02:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:38][root][INFO] - Training Epoch: 1/2, step 58/7134 completed (loss: 1.3548699617385864, acc: 0.6338028311729431)
[2025-02-13 02:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:38][root][INFO] - Training Epoch: 1/2, step 59/7134 completed (loss: 1.159551978111267, acc: 0.6843971610069275)
[2025-02-13 02:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:39][root][INFO] - Training Epoch: 1/2, step 60/7134 completed (loss: 1.3833616971969604, acc: 0.6292613744735718)
[2025-02-13 02:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:39][root][INFO] - Training Epoch: 1/2, step 61/7134 completed (loss: 1.2818470001220703, acc: 0.6413708925247192)
[2025-02-13 02:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:40][root][INFO] - Training Epoch: 1/2, step 62/7134 completed (loss: 1.1979732513427734, acc: 0.6721727848052979)
[2025-02-13 02:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:40][root][INFO] - Training Epoch: 1/2, step 63/7134 completed (loss: 1.3369488716125488, acc: 0.6525529026985168)
[2025-02-13 02:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:41][root][INFO] - Training Epoch: 1/2, step 64/7134 completed (loss: 1.3561406135559082, acc: 0.647849440574646)
[2025-02-13 02:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:41][root][INFO] - Training Epoch: 1/2, step 65/7134 completed (loss: 1.4270232915878296, acc: 0.601965606212616)
[2025-02-13 02:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:42][root][INFO] - Training Epoch: 1/2, step 66/7134 completed (loss: 1.3063743114471436, acc: 0.6559139490127563)
[2025-02-13 02:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:42][root][INFO] - Training Epoch: 1/2, step 67/7134 completed (loss: 1.203667163848877, acc: 0.671895444393158)
[2025-02-13 02:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:43][root][INFO] - Training Epoch: 1/2, step 68/7134 completed (loss: 1.3231706619262695, acc: 0.6567742228507996)
[2025-02-13 02:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:43][root][INFO] - Training Epoch: 1/2, step 69/7134 completed (loss: 1.2546907663345337, acc: 0.665835440158844)
[2025-02-13 02:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:44][root][INFO] - Training Epoch: 1/2, step 70/7134 completed (loss: 1.2133756875991821, acc: 0.6701754331588745)
[2025-02-13 02:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:44][root][INFO] - Training Epoch: 1/2, step 71/7134 completed (loss: 1.3619149923324585, acc: 0.6554622054100037)
[2025-02-13 02:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:44][root][INFO] - Training Epoch: 1/2, step 72/7134 completed (loss: 1.3391122817993164, acc: 0.6496163606643677)
[2025-02-13 02:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:45][root][INFO] - Training Epoch: 1/2, step 73/7134 completed (loss: 1.326568603515625, acc: 0.6521739363670349)
[2025-02-13 02:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:45][root][INFO] - Training Epoch: 1/2, step 74/7134 completed (loss: 1.3764866590499878, acc: 0.6601671576499939)
[2025-02-13 02:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:46][root][INFO] - Training Epoch: 1/2, step 75/7134 completed (loss: 1.42121422290802, acc: 0.6517189741134644)
[2025-02-13 02:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:46][root][INFO] - Training Epoch: 1/2, step 76/7134 completed (loss: 1.1978793144226074, acc: 0.6561712622642517)
[2025-02-13 02:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:47][root][INFO] - Training Epoch: 1/2, step 77/7134 completed (loss: 1.2558016777038574, acc: 0.6842105388641357)
[2025-02-13 02:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:47][root][INFO] - Training Epoch: 1/2, step 78/7134 completed (loss: 1.1323366165161133, acc: 0.704402506351471)
[2025-02-13 02:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:48][root][INFO] - Training Epoch: 1/2, step 79/7134 completed (loss: 1.2131632566452026, acc: 0.6604938507080078)
[2025-02-13 02:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:48][root][INFO] - Training Epoch: 1/2, step 80/7134 completed (loss: 1.0577462911605835, acc: 0.7122395634651184)
[2025-02-13 02:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:49][root][INFO] - Training Epoch: 1/2, step 81/7134 completed (loss: 1.0598846673965454, acc: 0.7146596908569336)
[2025-02-13 02:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:49][root][INFO] - Training Epoch: 1/2, step 82/7134 completed (loss: 1.0642740726470947, acc: 0.7213578224182129)
[2025-02-13 02:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:49][root][INFO] - Training Epoch: 1/2, step 83/7134 completed (loss: 1.154072880744934, acc: 0.6748071908950806)
[2025-02-13 02:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:50][root][INFO] - Training Epoch: 1/2, step 84/7134 completed (loss: 1.1912363767623901, acc: 0.6804835796356201)
[2025-02-13 02:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:50][root][INFO] - Training Epoch: 1/2, step 85/7134 completed (loss: 1.2738648653030396, acc: 0.6515513062477112)
[2025-02-13 02:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:51][root][INFO] - Training Epoch: 1/2, step 86/7134 completed (loss: 1.3349672555923462, acc: 0.6559633016586304)
[2025-02-13 02:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:51][root][INFO] - Training Epoch: 1/2, step 87/7134 completed (loss: 1.1146950721740723, acc: 0.685756266117096)
[2025-02-13 02:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:52][root][INFO] - Training Epoch: 1/2, step 88/7134 completed (loss: 1.056388020515442, acc: 0.7012448310852051)
[2025-02-13 02:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:52][root][INFO] - Training Epoch: 1/2, step 89/7134 completed (loss: 1.0593510866165161, acc: 0.7047497034072876)
[2025-02-13 02:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:53][root][INFO] - Training Epoch: 1/2, step 90/7134 completed (loss: 1.1565274000167847, acc: 0.6713286638259888)
[2025-02-13 02:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:53][root][INFO] - Training Epoch: 1/2, step 91/7134 completed (loss: 1.0066355466842651, acc: 0.7081632614135742)
[2025-02-13 02:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:53][root][INFO] - Training Epoch: 1/2, step 92/7134 completed (loss: 1.0631955862045288, acc: 0.6998597383499146)
[2025-02-13 02:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:54][root][INFO] - Training Epoch: 1/2, step 93/7134 completed (loss: 1.0093629360198975, acc: 0.7286501526832581)
[2025-02-13 02:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:54][root][INFO] - Training Epoch: 1/2, step 94/7134 completed (loss: 1.214046597480774, acc: 0.6728538274765015)
[2025-02-13 02:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:55][root][INFO] - Training Epoch: 1/2, step 95/7134 completed (loss: 1.0314124822616577, acc: 0.7097989916801453)
[2025-02-13 02:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:55][root][INFO] - Training Epoch: 1/2, step 96/7134 completed (loss: 1.0069177150726318, acc: 0.7355679869651794)
[2025-02-13 02:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:55][root][INFO] - Training Epoch: 1/2, step 97/7134 completed (loss: 1.752522349357605, acc: 0.5903614163398743)
[2025-02-13 02:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:56][root][INFO] - Training Epoch: 1/2, step 98/7134 completed (loss: 1.3345762491226196, acc: 0.6407766938209534)
[2025-02-13 02:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:56][root][INFO] - Training Epoch: 1/2, step 99/7134 completed (loss: 0.9784998297691345, acc: 0.7394495606422424)
[2025-02-13 02:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:57][root][INFO] - Training Epoch: 1/2, step 100/7134 completed (loss: 0.9534887075424194, acc: 0.7294871807098389)
[2025-02-13 02:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:57][root][INFO] - Training Epoch: 1/2, step 101/7134 completed (loss: 1.0240806341171265, acc: 0.7253289222717285)
[2025-02-13 02:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:58][root][INFO] - Training Epoch: 1/2, step 102/7134 completed (loss: 1.1067792177200317, acc: 0.6862027049064636)
[2025-02-13 02:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:58][root][INFO] - Training Epoch: 1/2, step 103/7134 completed (loss: 0.9186646342277527, acc: 0.7416666746139526)
[2025-02-13 02:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:58][root][INFO] - Training Epoch: 1/2, step 104/7134 completed (loss: 0.9423202872276306, acc: 0.7326284050941467)
[2025-02-13 02:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:59][root][INFO] - Training Epoch: 1/2, step 105/7134 completed (loss: 0.90606290102005, acc: 0.7145187854766846)
[2025-02-13 02:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:59][root][INFO] - Training Epoch: 1/2, step 106/7134 completed (loss: 0.9546143412590027, acc: 0.7289837002754211)
[2025-02-13 02:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:00][root][INFO] - Training Epoch: 1/2, step 107/7134 completed (loss: 1.1084234714508057, acc: 0.6848137378692627)
[2025-02-13 02:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:00][root][INFO] - Training Epoch: 1/2, step 108/7134 completed (loss: 0.8506556153297424, acc: 0.7494004964828491)
[2025-02-13 02:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:01][root][INFO] - Training Epoch: 1/2, step 109/7134 completed (loss: 0.9602189064025879, acc: 0.7352941036224365)
[2025-02-13 02:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:01][root][INFO] - Training Epoch: 1/2, step 110/7134 completed (loss: 0.8516778945922852, acc: 0.7544065713882446)
[2025-02-13 02:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:02][root][INFO] - Training Epoch: 1/2, step 111/7134 completed (loss: 0.9066793918609619, acc: 0.7386759519577026)
[2025-02-13 02:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:02][root][INFO] - Training Epoch: 1/2, step 112/7134 completed (loss: 0.8246921896934509, acc: 0.7411401867866516)
[2025-02-13 02:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:02][root][INFO] - Training Epoch: 1/2, step 113/7134 completed (loss: 0.8616845011711121, acc: 0.755696177482605)
[2025-02-13 02:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:03][root][INFO] - Training Epoch: 1/2, step 114/7134 completed (loss: 0.9495483040809631, acc: 0.7506631016731262)
[2025-02-13 02:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:03][root][INFO] - Training Epoch: 1/2, step 115/7134 completed (loss: 1.02203369140625, acc: 0.7139107584953308)
[2025-02-13 02:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:04][root][INFO] - Training Epoch: 1/2, step 116/7134 completed (loss: 0.8444240689277649, acc: 0.7455830574035645)
[2025-02-13 02:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:04][root][INFO] - Training Epoch: 1/2, step 117/7134 completed (loss: 0.8395592570304871, acc: 0.7631579041481018)
[2025-02-13 02:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:05][root][INFO] - Training Epoch: 1/2, step 118/7134 completed (loss: 0.792143702507019, acc: 0.7767857313156128)
[2025-02-13 02:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:05][root][INFO] - Training Epoch: 1/2, step 119/7134 completed (loss: 0.8335777521133423, acc: 0.7639225125312805)
[2025-02-13 02:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:06][root][INFO] - Training Epoch: 1/2, step 120/7134 completed (loss: 0.7510051131248474, acc: 0.7785059213638306)
[2025-02-13 02:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:06][root][INFO] - Training Epoch: 1/2, step 121/7134 completed (loss: 0.7541232705116272, acc: 0.7833553552627563)
[2025-02-13 02:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:06][root][INFO] - Training Epoch: 1/2, step 122/7134 completed (loss: 0.7462262511253357, acc: 0.800000011920929)
[2025-02-13 02:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:07][root][INFO] - Training Epoch: 1/2, step 123/7134 completed (loss: 0.7005439400672913, acc: 0.8033033013343811)
[2025-02-13 02:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:07][root][INFO] - Training Epoch: 1/2, step 124/7134 completed (loss: 0.8289502263069153, acc: 0.7706896662712097)
[2025-02-13 02:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:08][root][INFO] - Training Epoch: 1/2, step 125/7134 completed (loss: 0.8726658821105957, acc: 0.758849561214447)
[2025-02-13 02:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:08][root][INFO] - Training Epoch: 1/2, step 126/7134 completed (loss: 0.7603351473808289, acc: 0.7799999713897705)
[2025-02-13 02:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:09][root][INFO] - Training Epoch: 1/2, step 127/7134 completed (loss: 0.7296231985092163, acc: 0.790123462677002)
[2025-02-13 02:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:09][root][INFO] - Training Epoch: 1/2, step 128/7134 completed (loss: 0.9211060404777527, acc: 0.7416378259658813)
[2025-02-13 02:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:10][root][INFO] - Training Epoch: 1/2, step 129/7134 completed (loss: 0.8055824637413025, acc: 0.7683823704719543)
[2025-02-13 02:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:10][root][INFO] - Training Epoch: 1/2, step 130/7134 completed (loss: 0.759868323802948, acc: 0.7710526585578918)
[2025-02-13 02:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:10][root][INFO] - Training Epoch: 1/2, step 131/7134 completed (loss: 0.7313912510871887, acc: 0.7986842393875122)
[2025-02-13 02:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:11][root][INFO] - Training Epoch: 1/2, step 132/7134 completed (loss: 0.6555737257003784, acc: 0.8171262741088867)
[2025-02-13 02:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:11][root][INFO] - Training Epoch: 1/2, step 133/7134 completed (loss: 0.6725260019302368, acc: 0.8136439323425293)
[2025-02-13 02:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:12][root][INFO] - Training Epoch: 1/2, step 134/7134 completed (loss: 0.6160427331924438, acc: 0.8070796728134155)
[2025-02-13 02:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:12][root][INFO] - Training Epoch: 1/2, step 135/7134 completed (loss: 0.7412505149841309, acc: 0.7811111211776733)
[2025-02-13 02:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:13][root][INFO] - Training Epoch: 1/2, step 136/7134 completed (loss: 0.6337384581565857, acc: 0.8183990716934204)
[2025-02-13 02:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:13][root][INFO] - Training Epoch: 1/2, step 137/7134 completed (loss: 0.7565264105796814, acc: 0.7724477052688599)
[2025-02-13 02:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:14][root][INFO] - Training Epoch: 1/2, step 138/7134 completed (loss: 0.7792699337005615, acc: 0.7850821614265442)
[2025-02-13 02:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:14][root][INFO] - Training Epoch: 1/2, step 139/7134 completed (loss: 0.7245054244995117, acc: 0.8017902970314026)
[2025-02-13 02:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:15][root][INFO] - Training Epoch: 1/2, step 140/7134 completed (loss: 0.9140297174453735, acc: 0.7567164301872253)
[2025-02-13 02:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:15][root][INFO] - Training Epoch: 1/2, step 141/7134 completed (loss: 0.6953299641609192, acc: 0.8136792182922363)
[2025-02-13 02:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:15][root][INFO] - Training Epoch: 1/2, step 142/7134 completed (loss: 0.7532284259796143, acc: 0.7885952591896057)
[2025-02-13 02:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:16][root][INFO] - Training Epoch: 1/2, step 143/7134 completed (loss: 0.5945221781730652, acc: 0.8200782537460327)
[2025-02-13 02:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:16][root][INFO] - Training Epoch: 1/2, step 144/7134 completed (loss: 0.7742368578910828, acc: 0.7913188934326172)
[2025-02-13 02:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:17][root][INFO] - Training Epoch: 1/2, step 145/7134 completed (loss: 1.0167514085769653, acc: 0.7422535419464111)
[2025-02-13 02:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:17][root][INFO] - Training Epoch: 1/2, step 146/7134 completed (loss: 0.8402395844459534, acc: 0.7706552743911743)
[2025-02-13 02:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:18][root][INFO] - Training Epoch: 1/2, step 147/7134 completed (loss: 0.8711159825325012, acc: 0.7712305188179016)
[2025-02-13 02:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:18][root][INFO] - Training Epoch: 1/2, step 148/7134 completed (loss: 0.9031467437744141, acc: 0.7610208988189697)
[2025-02-13 02:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:19][root][INFO] - Training Epoch: 1/2, step 149/7134 completed (loss: 0.7988348007202148, acc: 0.7896774411201477)
[2025-02-13 02:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:19][root][INFO] - Training Epoch: 1/2, step 150/7134 completed (loss: 0.8684566020965576, acc: 0.7620320916175842)
[2025-02-13 02:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:19][root][INFO] - Training Epoch: 1/2, step 151/7134 completed (loss: 0.8362155556678772, acc: 0.776849627494812)
[2025-02-13 02:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:20][root][INFO] - Training Epoch: 1/2, step 152/7134 completed (loss: 0.9103264808654785, acc: 0.7494305372238159)
[2025-02-13 02:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:20][root][INFO] - Training Epoch: 1/2, step 153/7134 completed (loss: 0.94076007604599, acc: 0.7402234673500061)
[2025-02-13 02:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:21][root][INFO] - Training Epoch: 1/2, step 154/7134 completed (loss: 0.8940346240997314, acc: 0.7537993788719177)
[2025-02-13 02:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:21][root][INFO] - Training Epoch: 1/2, step 155/7134 completed (loss: 0.7993848323822021, acc: 0.78311687707901)
[2025-02-13 02:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:22][root][INFO] - Training Epoch: 1/2, step 156/7134 completed (loss: 0.8227267265319824, acc: 0.7759626507759094)
[2025-02-13 02:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:22][root][INFO] - Training Epoch: 1/2, step 157/7134 completed (loss: 0.8973734378814697, acc: 0.7722891569137573)
[2025-02-13 02:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:23][root][INFO] - Training Epoch: 1/2, step 158/7134 completed (loss: 0.7359259724617004, acc: 0.7912814021110535)
[2025-02-13 02:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:23][root][INFO] - Training Epoch: 1/2, step 159/7134 completed (loss: 0.71055006980896, acc: 0.789321780204773)
[2025-02-13 02:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:24][root][INFO] - Training Epoch: 1/2, step 160/7134 completed (loss: 0.9085018634796143, acc: 0.7550607323646545)
[2025-02-13 02:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:24][root][INFO] - Training Epoch: 1/2, step 161/7134 completed (loss: 0.7796516418457031, acc: 0.770348846912384)
[2025-02-13 02:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:24][root][INFO] - Training Epoch: 1/2, step 162/7134 completed (loss: 0.8000171184539795, acc: 0.7647058963775635)
[2025-02-13 02:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:25][root][INFO] - Training Epoch: 1/2, step 163/7134 completed (loss: 0.7588279843330383, acc: 0.7799113988876343)
[2025-02-13 02:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:25][root][INFO] - Training Epoch: 1/2, step 164/7134 completed (loss: 0.7531555891036987, acc: 0.7659574747085571)
[2025-02-13 02:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:25][root][INFO] - Training Epoch: 1/2, step 165/7134 completed (loss: 0.9105162024497986, acc: 0.7918660044670105)
[2025-02-13 02:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:26][root][INFO] - Training Epoch: 1/2, step 166/7134 completed (loss: 0.75174880027771, acc: 0.7641791105270386)
[2025-02-13 02:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:26][root][INFO] - Training Epoch: 1/2, step 167/7134 completed (loss: 1.1834332942962646, acc: 0.70944744348526)
[2025-02-13 02:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:26][root][INFO] - Training Epoch: 1/2, step 168/7134 completed (loss: 2.2191436290740967, acc: 0.4941176474094391)
[2025-02-13 02:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:27][root][INFO] - Training Epoch: 1/2, step 169/7134 completed (loss: 1.8000181913375854, acc: 0.6162465214729309)
[2025-02-13 02:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:27][root][INFO] - Training Epoch: 1/2, step 170/7134 completed (loss: 1.1717528104782104, acc: 0.6982921957969666)
[2025-02-13 02:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:28][root][INFO] - Training Epoch: 1/2, step 171/7134 completed (loss: 1.1694703102111816, acc: 0.7207792401313782)
[2025-02-13 02:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:28][root][INFO] - Training Epoch: 1/2, step 172/7134 completed (loss: 1.0007877349853516, acc: 0.7726161479949951)
[2025-02-13 02:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:28][root][INFO] - Training Epoch: 1/2, step 173/7134 completed (loss: 0.8059053421020508, acc: 0.778761088848114)
[2025-02-13 02:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:29][root][INFO] - Training Epoch: 1/2, step 174/7134 completed (loss: 0.803268313407898, acc: 0.76408451795578)
[2025-02-13 02:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:29][root][INFO] - Training Epoch: 1/2, step 175/7134 completed (loss: 0.8351854085922241, acc: 0.7927272915840149)
[2025-02-13 02:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:30][root][INFO] - Training Epoch: 1/2, step 176/7134 completed (loss: 0.8509477376937866, acc: 0.7753623127937317)
[2025-02-13 02:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:30][root][INFO] - Training Epoch: 1/2, step 177/7134 completed (loss: 0.811028242111206, acc: 0.7722222208976746)
[2025-02-13 02:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:30][root][INFO] - Training Epoch: 1/2, step 178/7134 completed (loss: 0.849933385848999, acc: 0.748062014579773)
[2025-02-13 02:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:31][root][INFO] - Training Epoch: 1/2, step 179/7134 completed (loss: 0.7554312348365784, acc: 0.8046421408653259)
[2025-02-13 02:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:31][root][INFO] - Training Epoch: 1/2, step 180/7134 completed (loss: 0.6292449831962585, acc: 0.8084833025932312)
[2025-02-13 02:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:32][root][INFO] - Training Epoch: 1/2, step 181/7134 completed (loss: 0.5519052743911743, acc: 0.8247694373130798)
[2025-02-13 02:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:32][root][INFO] - Training Epoch: 1/2, step 182/7134 completed (loss: 0.6160352230072021, acc: 0.8083028197288513)
[2025-02-13 02:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:33][root][INFO] - Training Epoch: 1/2, step 183/7134 completed (loss: 0.6050841212272644, acc: 0.8201634883880615)
[2025-02-13 02:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:33][root][INFO] - Training Epoch: 1/2, step 184/7134 completed (loss: 0.4945494830608368, acc: 0.8525345325469971)
[2025-02-13 02:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:33][root][INFO] - Training Epoch: 1/2, step 185/7134 completed (loss: 0.4603114426136017, acc: 0.8526466488838196)
[2025-02-13 02:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:34][root][INFO] - Training Epoch: 1/2, step 186/7134 completed (loss: 0.5518144369125366, acc: 0.8461538553237915)
[2025-02-13 02:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:34][root][INFO] - Training Epoch: 1/2, step 187/7134 completed (loss: 0.56703120470047, acc: 0.8379888534545898)
[2025-02-13 02:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:35][root][INFO] - Training Epoch: 1/2, step 188/7134 completed (loss: 0.5599114298820496, acc: 0.8333333134651184)
[2025-02-13 02:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:35][root][INFO] - Training Epoch: 1/2, step 189/7134 completed (loss: 0.67573481798172, acc: 0.8230769038200378)
[2025-02-13 02:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36][root][INFO] - Training Epoch: 1/2, step 190/7134 completed (loss: 0.5945736169815063, acc: 0.8315132856369019)
[2025-02-13 02:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36][root][INFO] - Training Epoch: 1/2, step 191/7134 completed (loss: 0.5203662514686584, acc: 0.8611111044883728)
[2025-02-13 02:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36][root][INFO] - Training Epoch: 1/2, step 192/7134 completed (loss: 0.5180495381355286, acc: 0.838487982749939)
[2025-02-13 02:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:37][root][INFO] - Training Epoch: 1/2, step 193/7134 completed (loss: 0.49155962467193604, acc: 0.8429530262947083)
[2025-02-13 02:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:37][root][INFO] - Training Epoch: 1/2, step 194/7134 completed (loss: 0.5640105605125427, acc: 0.8378766179084778)
[2025-02-13 02:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:38][root][INFO] - Training Epoch: 1/2, step 195/7134 completed (loss: 0.5775476098060608, acc: 0.848739504814148)
[2025-02-13 02:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:38][root][INFO] - Training Epoch: 1/2, step 196/7134 completed (loss: 0.4494188725948334, acc: 0.874331533908844)
[2025-02-13 02:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:39][root][INFO] - Training Epoch: 1/2, step 197/7134 completed (loss: 0.5646904110908508, acc: 0.8447293639183044)
[2025-02-13 02:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:39][root][INFO] - Training Epoch: 1/2, step 198/7134 completed (loss: 0.4969663619995117, acc: 0.8565940856933594)
[2025-02-13 02:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40][root][INFO] - Training Epoch: 1/2, step 199/7134 completed (loss: 0.5292457342147827, acc: 0.8390804529190063)
[2025-02-13 02:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40][root][INFO] - Training Epoch: 1/2, step 200/7134 completed (loss: 0.480578750371933, acc: 0.8590604066848755)
[2025-02-13 02:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40][root][INFO] - Training Epoch: 1/2, step 201/7134 completed (loss: 0.4825073480606079, acc: 0.8525280952453613)
[2025-02-13 02:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:41][root][INFO] - Training Epoch: 1/2, step 202/7134 completed (loss: 0.44395962357521057, acc: 0.860689640045166)
[2025-02-13 02:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:41][root][INFO] - Training Epoch: 1/2, step 203/7134 completed (loss: 0.44647619128227234, acc: 0.8856749534606934)
[2025-02-13 02:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:42][root][INFO] - Training Epoch: 1/2, step 204/7134 completed (loss: 0.4732474088668823, acc: 0.8784194588661194)
[2025-02-13 02:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:42][root][INFO] - Training Epoch: 1/2, step 205/7134 completed (loss: 0.5657241940498352, acc: 0.8473479747772217)
[2025-02-13 02:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:43][root][INFO] - Training Epoch: 1/2, step 206/7134 completed (loss: 0.5484351515769958, acc: 0.8523274660110474)
[2025-02-13 02:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:43][root][INFO] - Training Epoch: 1/2, step 207/7134 completed (loss: 0.5469732880592346, acc: 0.849829375743866)
[2025-02-13 02:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44][root][INFO] - Training Epoch: 1/2, step 208/7134 completed (loss: 0.720470130443573, acc: 0.8235294222831726)
[2025-02-13 02:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44][root][INFO] - Training Epoch: 1/2, step 209/7134 completed (loss: 0.65802001953125, acc: 0.8212435245513916)
[2025-02-13 02:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44][root][INFO] - Training Epoch: 1/2, step 210/7134 completed (loss: 0.6694484949111938, acc: 0.8211284279823303)
[2025-02-13 02:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:45][root][INFO] - Training Epoch: 1/2, step 211/7134 completed (loss: 0.746973991394043, acc: 0.8202614188194275)
[2025-02-13 02:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:45][root][INFO] - Training Epoch: 1/2, step 212/7134 completed (loss: 0.5742572546005249, acc: 0.8595041036605835)
[2025-02-13 02:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46][root][INFO] - Training Epoch: 1/2, step 213/7134 completed (loss: 0.4707237780094147, acc: 0.8705501556396484)
[2025-02-13 02:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46][root][INFO] - Training Epoch: 1/2, step 214/7134 completed (loss: 0.4813534915447235, acc: 0.8803418874740601)
[2025-02-13 02:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46][root][INFO] - Training Epoch: 1/2, step 215/7134 completed (loss: 0.5476793646812439, acc: 0.8472222089767456)
[2025-02-13 02:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:47][root][INFO] - Training Epoch: 1/2, step 216/7134 completed (loss: 0.5004048943519592, acc: 0.8680555820465088)
[2025-02-13 02:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:47][root][INFO] - Training Epoch: 1/2, step 217/7134 completed (loss: 0.5026820302009583, acc: 0.8703030347824097)
[2025-02-13 02:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:48][root][INFO] - Training Epoch: 1/2, step 218/7134 completed (loss: 0.3370971381664276, acc: 0.8969465494155884)
[2025-02-13 02:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:48][root][INFO] - Training Epoch: 1/2, step 219/7134 completed (loss: 0.4497421383857727, acc: 0.8791422843933105)
[2025-02-13 02:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:49][root][INFO] - Training Epoch: 1/2, step 220/7134 completed (loss: 0.44430527091026306, acc: 0.8828451633453369)
[2025-02-13 02:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:49][root][INFO] - Training Epoch: 1/2, step 221/7134 completed (loss: 0.4535428285598755, acc: 0.8805969953536987)
[2025-02-13 02:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50][root][INFO] - Training Epoch: 1/2, step 222/7134 completed (loss: 0.43005290627479553, acc: 0.8874345421791077)
[2025-02-13 02:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50][root][INFO] - Training Epoch: 1/2, step 223/7134 completed (loss: 0.48491114377975464, acc: 0.8613607287406921)
[2025-02-13 02:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50][root][INFO] - Training Epoch: 1/2, step 224/7134 completed (loss: 0.5981045365333557, acc: 0.8352788686752319)
[2025-02-13 02:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:51][root][INFO] - Training Epoch: 1/2, step 225/7134 completed (loss: 0.5580864548683167, acc: 0.8433889746665955)
[2025-02-13 02:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:51][root][INFO] - Training Epoch: 1/2, step 226/7134 completed (loss: 0.5468762516975403, acc: 0.8606356978416443)
[2025-02-13 02:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:52][root][INFO] - Training Epoch: 1/2, step 227/7134 completed (loss: 0.6949070692062378, acc: 0.8325061798095703)
[2025-02-13 02:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:52][root][INFO] - Training Epoch: 1/2, step 228/7134 completed (loss: 0.5679065585136414, acc: 0.8523878455162048)
[2025-02-13 02:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:53][root][INFO] - Training Epoch: 1/2, step 229/7134 completed (loss: 0.47936350107192993, acc: 0.8607594966888428)
[2025-02-13 02:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:53][root][INFO] - Training Epoch: 1/2, step 230/7134 completed (loss: 0.4101102948188782, acc: 0.8783783912658691)
[2025-02-13 02:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:54][root][INFO] - Training Epoch: 1/2, step 231/7134 completed (loss: 0.387101948261261, acc: 0.8813559412956238)
[2025-02-13 02:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:54][root][INFO] - Training Epoch: 1/2, step 232/7134 completed (loss: 0.4314437508583069, acc: 0.8731465935707092)
[2025-02-13 02:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55][root][INFO] - Training Epoch: 1/2, step 233/7134 completed (loss: 0.3593800663948059, acc: 0.9017432928085327)
[2025-02-13 02:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55][root][INFO] - Training Epoch: 1/2, step 234/7134 completed (loss: 0.4459789991378784, acc: 0.8817567825317383)
[2025-02-13 02:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55][root][INFO] - Training Epoch: 1/2, step 235/7134 completed (loss: 0.4185672402381897, acc: 0.8917378783226013)
[2025-02-13 02:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:56][root][INFO] - Training Epoch: 1/2, step 236/7134 completed (loss: 0.36614423990249634, acc: 0.8999999761581421)
[2025-02-13 02:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:56][root][INFO] - Training Epoch: 1/2, step 237/7134 completed (loss: 0.4466039836406708, acc: 0.876304030418396)
[2025-02-13 02:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57][root][INFO] - Training Epoch: 1/2, step 238/7134 completed (loss: 0.4237233102321625, acc: 0.8936567306518555)
[2025-02-13 02:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57][root][INFO] - Training Epoch: 1/2, step 239/7134 completed (loss: 0.30502375960350037, acc: 0.8970588445663452)
[2025-02-13 02:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57][root][INFO] - Training Epoch: 1/2, step 240/7134 completed (loss: 0.41813769936561584, acc: 0.883474588394165)
[2025-02-13 02:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:58][root][INFO] - Training Epoch: 1/2, step 241/7134 completed (loss: 0.4344514310359955, acc: 0.8888888955116272)
[2025-02-13 02:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:58][root][INFO] - Training Epoch: 1/2, step 242/7134 completed (loss: 0.4795067310333252, acc: 0.8821490406990051)
[2025-02-13 02:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:59][root][INFO] - Training Epoch: 1/2, step 243/7134 completed (loss: 0.5937274694442749, acc: 0.853741466999054)
[2025-02-13 02:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:59][root][INFO] - Training Epoch: 1/2, step 244/7134 completed (loss: 0.3244030177593231, acc: 0.9095022678375244)
[2025-02-13 02:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00][root][INFO] - Training Epoch: 1/2, step 245/7134 completed (loss: 0.4131011664867401, acc: 0.889667272567749)
[2025-02-13 02:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00][root][INFO] - Training Epoch: 1/2, step 246/7134 completed (loss: 0.4108200669288635, acc: 0.8832807540893555)
[2025-02-13 02:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00][root][INFO] - Training Epoch: 1/2, step 247/7134 completed (loss: 0.3886130154132843, acc: 0.8983666300773621)
[2025-02-13 02:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:01][root][INFO] - Training Epoch: 1/2, step 248/7134 completed (loss: 0.35999488830566406, acc: 0.8972868323326111)
[2025-02-13 02:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:01][root][INFO] - Training Epoch: 1/2, step 249/7134 completed (loss: 0.546938419342041, acc: 0.8713136911392212)
[2025-02-13 02:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02][root][INFO] - Training Epoch: 1/2, step 250/7134 completed (loss: 0.47871336340904236, acc: 0.8734177350997925)
[2025-02-13 02:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02][root][INFO] - Training Epoch: 1/2, step 251/7134 completed (loss: 0.3747141361236572, acc: 0.8940171003341675)
[2025-02-13 02:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02][root][INFO] - Training Epoch: 1/2, step 252/7134 completed (loss: 0.34897294640541077, acc: 0.8934426307678223)
[2025-02-13 02:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:03][root][INFO] - Training Epoch: 1/2, step 253/7134 completed (loss: 0.40786048769950867, acc: 0.8770053386688232)
[2025-02-13 02:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:03][root][INFO] - Training Epoch: 1/2, step 254/7134 completed (loss: 0.38793236017227173, acc: 0.8999999761581421)
[2025-02-13 02:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:04][root][INFO] - Training Epoch: 1/2, step 255/7134 completed (loss: 0.38663390278816223, acc: 0.8932748436927795)
[2025-02-13 02:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:04][root][INFO] - Training Epoch: 1/2, step 256/7134 completed (loss: 0.45939525961875916, acc: 0.8823529481887817)
[2025-02-13 02:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05][root][INFO] - Training Epoch: 1/2, step 257/7134 completed (loss: 0.35830405354499817, acc: 0.8926380276679993)
[2025-02-13 02:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05][root][INFO] - Training Epoch: 1/2, step 258/7134 completed (loss: 0.40464189648628235, acc: 0.8876032829284668)
[2025-02-13 02:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05][root][INFO] - Training Epoch: 1/2, step 259/7134 completed (loss: 0.36039459705352783, acc: 0.9043760299682617)
[2025-02-13 02:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:06][root][INFO] - Training Epoch: 1/2, step 260/7134 completed (loss: 0.3977467715740204, acc: 0.9001691937446594)
[2025-02-13 02:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:06][root][INFO] - Training Epoch: 1/2, step 261/7134 completed (loss: 0.35493627190589905, acc: 0.8997954726219177)
[2025-02-13 02:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:07][root][INFO] - Training Epoch: 1/2, step 262/7134 completed (loss: 0.33902645111083984, acc: 0.8985024690628052)
[2025-02-13 02:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:07][root][INFO] - Training Epoch: 1/2, step 263/7134 completed (loss: 0.3810350298881531, acc: 0.8850574493408203)
[2025-02-13 02:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08][root][INFO] - Training Epoch: 1/2, step 264/7134 completed (loss: 0.43599480390548706, acc: 0.8738317489624023)
[2025-02-13 02:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08][root][INFO] - Training Epoch: 1/2, step 265/7134 completed (loss: 0.38525253534317017, acc: 0.8891752362251282)
[2025-02-13 02:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08][root][INFO] - Training Epoch: 1/2, step 266/7134 completed (loss: 0.42395803332328796, acc: 0.8807339668273926)
[2025-02-13 02:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:09][root][INFO] - Training Epoch: 1/2, step 267/7134 completed (loss: 0.45383498072624207, acc: 0.8643678426742554)
[2025-02-13 02:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:09][root][INFO] - Training Epoch: 1/2, step 268/7134 completed (loss: 0.4538377821445465, acc: 0.8547341227531433)
[2025-02-13 02:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:10][root][INFO] - Training Epoch: 1/2, step 269/7134 completed (loss: 0.39861807227134705, acc: 0.8845618963241577)
[2025-02-13 02:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:10][root][INFO] - Training Epoch: 1/2, step 270/7134 completed (loss: 0.27459716796875, acc: 0.9268656969070435)
[2025-02-13 02:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:11][root][INFO] - Training Epoch: 1/2, step 271/7134 completed (loss: 0.3744392991065979, acc: 0.8902900218963623)
[2025-02-13 02:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:11][root][INFO] - Training Epoch: 1/2, step 272/7134 completed (loss: 0.3471226692199707, acc: 0.910941481590271)
[2025-02-13 02:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:12][root][INFO] - Training Epoch: 1/2, step 273/7134 completed (loss: 0.31576359272003174, acc: 0.9077844023704529)
[2025-02-13 02:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:12][root][INFO] - Training Epoch: 1/2, step 274/7134 completed (loss: 0.2859543561935425, acc: 0.9202200770378113)
[2025-02-13 02:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13][root][INFO] - Training Epoch: 1/2, step 275/7134 completed (loss: 0.4537599980831146, acc: 0.8805104494094849)
[2025-02-13 02:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13][root][INFO] - Training Epoch: 1/2, step 276/7134 completed (loss: 0.3144763708114624, acc: 0.9220463037490845)
[2025-02-13 02:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13][root][INFO] - Training Epoch: 1/2, step 277/7134 completed (loss: 0.40198805928230286, acc: 0.8880308866500854)
[2025-02-13 02:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:14][root][INFO] - Training Epoch: 1/2, step 278/7134 completed (loss: 0.30934715270996094, acc: 0.9042145609855652)
[2025-02-13 02:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:14][root][INFO] - Training Epoch: 1/2, step 279/7134 completed (loss: 0.3089154064655304, acc: 0.913385808467865)
[2025-02-13 02:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:15][root][INFO] - Training Epoch: 1/2, step 280/7134 completed (loss: 0.38235536217689514, acc: 0.8942652344703674)
[2025-02-13 02:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:15][root][INFO] - Training Epoch: 1/2, step 281/7134 completed (loss: 0.3044505715370178, acc: 0.9020000100135803)
[2025-02-13 02:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16][root][INFO] - Training Epoch: 1/2, step 282/7134 completed (loss: 0.4698634147644043, acc: 0.8824427723884583)
[2025-02-13 02:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16][root][INFO] - Training Epoch: 1/2, step 283/7134 completed (loss: 0.44345423579216003, acc: 0.8777589201927185)
[2025-02-13 02:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16][root][INFO] - Training Epoch: 1/2, step 284/7134 completed (loss: 0.4552900195121765, acc: 0.8741610646247864)
[2025-02-13 02:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:17][root][INFO] - Training Epoch: 1/2, step 285/7134 completed (loss: 0.40400293469429016, acc: 0.8888888955116272)
[2025-02-13 02:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:17][root][INFO] - Training Epoch: 1/2, step 286/7134 completed (loss: 0.298051655292511, acc: 0.9150442481040955)
[2025-02-13 02:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:18][root][INFO] - Training Epoch: 1/2, step 287/7134 completed (loss: 0.45127901434898376, acc: 0.8764045238494873)
[2025-02-13 02:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:18][root][INFO] - Training Epoch: 1/2, step 288/7134 completed (loss: 0.3766275942325592, acc: 0.8858194947242737)
[2025-02-13 02:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19][root][INFO] - Training Epoch: 1/2, step 289/7134 completed (loss: 0.37284529209136963, acc: 0.8915008902549744)
[2025-02-13 02:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19][root][INFO] - Training Epoch: 1/2, step 290/7134 completed (loss: 0.4025338590145111, acc: 0.8860182166099548)
[2025-02-13 02:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19][root][INFO] - Training Epoch: 1/2, step 291/7134 completed (loss: 0.3007246255874634, acc: 0.9226973652839661)
[2025-02-13 02:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:20][root][INFO] - Training Epoch: 1/2, step 292/7134 completed (loss: 0.43355345726013184, acc: 0.8773006200790405)
[2025-02-13 02:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:20][root][INFO] - Training Epoch: 1/2, step 293/7134 completed (loss: 0.29222163558006287, acc: 0.9196786880493164)
[2025-02-13 02:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:21][root][INFO] - Training Epoch: 1/2, step 294/7134 completed (loss: 0.3414256274700165, acc: 0.9005848169326782)
[2025-02-13 02:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:21][root][INFO] - Training Epoch: 1/2, step 295/7134 completed (loss: 0.33210310339927673, acc: 0.9095563292503357)
[2025-02-13 02:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:22][root][INFO] - Training Epoch: 1/2, step 296/7134 completed (loss: 0.274298757314682, acc: 0.9224806427955627)
[2025-02-13 02:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:22][root][INFO] - Training Epoch: 1/2, step 297/7134 completed (loss: 0.26571568846702576, acc: 0.9289520382881165)
[2025-02-13 02:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23][root][INFO] - Training Epoch: 1/2, step 298/7134 completed (loss: 0.2875135838985443, acc: 0.9247999787330627)
[2025-02-13 02:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23][root][INFO] - Training Epoch: 1/2, step 299/7134 completed (loss: 0.2819659113883972, acc: 0.927819550037384)
[2025-02-13 02:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23][root][INFO] - Training Epoch: 1/2, step 300/7134 completed (loss: 0.2482994645833969, acc: 0.9255663156509399)
[2025-02-13 02:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:24][root][INFO] - Training Epoch: 1/2, step 301/7134 completed (loss: 0.27562034130096436, acc: 0.9236209392547607)
[2025-02-13 02:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:24][root][INFO] - Training Epoch: 1/2, step 302/7134 completed (loss: 0.405356764793396, acc: 0.9005848169326782)
[2025-02-13 02:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25][root][INFO] - Training Epoch: 1/2, step 303/7134 completed (loss: 0.2775948643684387, acc: 0.9281553626060486)
[2025-02-13 02:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25][root][INFO] - Training Epoch: 1/2, step 304/7134 completed (loss: 0.22342464327812195, acc: 0.9277777671813965)
[2025-02-13 02:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25][root][INFO] - Training Epoch: 1/2, step 305/7134 completed (loss: 0.20463259518146515, acc: 0.9364069700241089)
[2025-02-13 02:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:26][root][INFO] - Training Epoch: 1/2, step 306/7134 completed (loss: 0.24864283204078674, acc: 0.9148550629615784)
[2025-02-13 02:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:26][root][INFO] - Training Epoch: 1/2, step 307/7134 completed (loss: 0.2731100916862488, acc: 0.9234875440597534)
[2025-02-13 02:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:27][root][INFO] - Training Epoch: 1/2, step 308/7134 completed (loss: 0.25395914912223816, acc: 0.9243420958518982)
[2025-02-13 02:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:27][root][INFO] - Training Epoch: 1/2, step 309/7134 completed (loss: 0.27354103326797485, acc: 0.9224137663841248)
[2025-02-13 02:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28][root][INFO] - Training Epoch: 1/2, step 310/7134 completed (loss: 0.237062469124794, acc: 0.932748556137085)
[2025-02-13 02:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28][root][INFO] - Training Epoch: 1/2, step 311/7134 completed (loss: 0.2693691849708557, acc: 0.923794686794281)
[2025-02-13 02:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28][root][INFO] - Training Epoch: 1/2, step 312/7134 completed (loss: 0.23838025331497192, acc: 0.9306759238243103)
[2025-02-13 02:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:29][root][INFO] - Training Epoch: 1/2, step 313/7134 completed (loss: 0.21592384576797485, acc: 0.9269565343856812)
[2025-02-13 02:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:29][root][INFO] - Training Epoch: 1/2, step 314/7134 completed (loss: 0.30252981185913086, acc: 0.909375011920929)
[2025-02-13 02:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:30][root][INFO] - Training Epoch: 1/2, step 315/7134 completed (loss: 0.2332119345664978, acc: 0.9163934588432312)
[2025-02-13 02:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:30][root][INFO] - Training Epoch: 1/2, step 316/7134 completed (loss: 0.2873712480068207, acc: 0.911552369594574)
[2025-02-13 02:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31][root][INFO] - Training Epoch: 1/2, step 317/7134 completed (loss: 0.2186964750289917, acc: 0.9377537369728088)
[2025-02-13 02:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31][root][INFO] - Training Epoch: 1/2, step 318/7134 completed (loss: 0.37540197372436523, acc: 0.9023199081420898)
[2025-02-13 02:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31][root][INFO] - Training Epoch: 1/2, step 319/7134 completed (loss: 0.36892974376678467, acc: 0.8934081196784973)
[2025-02-13 02:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:32][root][INFO] - Training Epoch: 1/2, step 320/7134 completed (loss: 0.3304632008075714, acc: 0.9144079685211182)
[2025-02-13 02:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:32][root][INFO] - Training Epoch: 1/2, step 321/7134 completed (loss: 0.3721316456794739, acc: 0.9042145609855652)
[2025-02-13 02:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:33][root][INFO] - Training Epoch: 1/2, step 322/7134 completed (loss: 0.3878288269042969, acc: 0.8888888955116272)
[2025-02-13 02:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:33][root][INFO] - Training Epoch: 1/2, step 323/7134 completed (loss: 0.43139490485191345, acc: 0.8844672441482544)
[2025-02-13 02:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:34][root][INFO] - Training Epoch: 1/2, step 324/7134 completed (loss: 0.37832126021385193, acc: 0.9010356664657593)
[2025-02-13 02:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:34][root][INFO] - Training Epoch: 1/2, step 325/7134 completed (loss: 0.2528364360332489, acc: 0.9241645336151123)
[2025-02-13 02:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:35][root][INFO] - Training Epoch: 1/2, step 326/7134 completed (loss: 0.29686540365219116, acc: 0.9086480140686035)
[2025-02-13 02:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:35][root][INFO] - Training Epoch: 1/2, step 327/7134 completed (loss: 0.3449731171131134, acc: 0.9020270109176636)
[2025-02-13 02:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:36][root][INFO] - Training Epoch: 1/2, step 328/7134 completed (loss: 0.3336453139781952, acc: 0.9007407426834106)
[2025-02-13 02:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:36][root][INFO] - Training Epoch: 1/2, step 329/7134 completed (loss: 0.27690020203590393, acc: 0.9177438020706177)
[2025-02-13 02:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37][root][INFO] - Training Epoch: 1/2, step 330/7134 completed (loss: 0.4017413258552551, acc: 0.8992950916290283)
[2025-02-13 02:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37][root][INFO] - Training Epoch: 1/2, step 331/7134 completed (loss: 0.281694620847702, acc: 0.9167616963386536)
[2025-02-13 02:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37][root][INFO] - Training Epoch: 1/2, step 332/7134 completed (loss: 0.2753671109676361, acc: 0.9185360074043274)
[2025-02-13 02:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:38][root][INFO] - Training Epoch: 1/2, step 333/7134 completed (loss: 0.354947954416275, acc: 0.9063260555267334)
[2025-02-13 02:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:38][root][INFO] - Training Epoch: 1/2, step 334/7134 completed (loss: 0.2759164273738861, acc: 0.9252218008041382)
[2025-02-13 02:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:39][root][INFO] - Training Epoch: 1/2, step 335/7134 completed (loss: 0.25209230184555054, acc: 0.9374090433120728)
[2025-02-13 02:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:39][root][INFO] - Training Epoch: 1/2, step 336/7134 completed (loss: 0.3104094862937927, acc: 0.9197604656219482)
[2025-02-13 02:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:40][root][INFO] - Training Epoch: 1/2, step 337/7134 completed (loss: 0.25661274790763855, acc: 0.9325153231620789)
[2025-02-13 02:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:40][root][INFO] - Training Epoch: 1/2, step 338/7134 completed (loss: 0.2728104591369629, acc: 0.9219143390655518)
[2025-02-13 02:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41][root][INFO] - Training Epoch: 1/2, step 339/7134 completed (loss: 0.22070658206939697, acc: 0.9399999976158142)
[2025-02-13 02:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41][root][INFO] - Training Epoch: 1/2, step 340/7134 completed (loss: 0.27505454421043396, acc: 0.9211195707321167)
[2025-02-13 02:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41][root][INFO] - Training Epoch: 1/2, step 341/7134 completed (loss: 0.2360978126525879, acc: 0.9306431412696838)
[2025-02-13 02:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:42][root][INFO] - Training Epoch: 1/2, step 342/7134 completed (loss: 0.2184537947177887, acc: 0.9439393877983093)
[2025-02-13 02:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:42][root][INFO] - Training Epoch: 1/2, step 343/7134 completed (loss: 0.25124073028564453, acc: 0.9281663298606873)
[2025-02-13 02:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:43][root][INFO] - Training Epoch: 1/2, step 344/7134 completed (loss: 0.20457319915294647, acc: 0.9470803141593933)
[2025-02-13 02:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:43][root][INFO] - Training Epoch: 1/2, step 345/7134 completed (loss: 0.18753695487976074, acc: 0.9482142925262451)
[2025-02-13 02:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44][root][INFO] - Training Epoch: 1/2, step 346/7134 completed (loss: 0.2074313461780548, acc: 0.9443561434745789)
[2025-02-13 02:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44][root][INFO] - Training Epoch: 1/2, step 347/7134 completed (loss: 0.22695158421993256, acc: 0.946704089641571)
[2025-02-13 02:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44][root][INFO] - Training Epoch: 1/2, step 348/7134 completed (loss: 0.17898178100585938, acc: 0.9552238583564758)
[2025-02-13 02:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:45][root][INFO] - Training Epoch: 1/2, step 349/7134 completed (loss: 0.19421155750751495, acc: 0.9409396052360535)
[2025-02-13 02:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:45][root][INFO] - Training Epoch: 1/2, step 350/7134 completed (loss: 0.17901961505413055, acc: 0.9482551217079163)
[2025-02-13 02:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:46][root][INFO] - Training Epoch: 1/2, step 351/7134 completed (loss: 0.3149796426296234, acc: 0.9300254583358765)
[2025-02-13 02:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:46][root][INFO] - Training Epoch: 1/2, step 352/7134 completed (loss: 0.19051551818847656, acc: 0.94972825050354)
[2025-02-13 02:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:47][root][INFO] - Training Epoch: 1/2, step 353/7134 completed (loss: 0.21102631092071533, acc: 0.9308510422706604)
[2025-02-13 02:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:47][root][INFO] - Training Epoch: 1/2, step 354/7134 completed (loss: 0.22090919315814972, acc: 0.9429569244384766)
[2025-02-13 02:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:48][root][INFO] - Training Epoch: 1/2, step 355/7134 completed (loss: 0.29639390110969543, acc: 0.9273743033409119)
[2025-02-13 02:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:48][root][INFO] - Training Epoch: 1/2, step 356/7134 completed (loss: 0.22691579163074493, acc: 0.9351851940155029)
[2025-02-13 02:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:49][root][INFO] - Training Epoch: 1/2, step 357/7134 completed (loss: 0.21972908079624176, acc: 0.9402035474777222)
[2025-02-13 02:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:49][root][INFO] - Training Epoch: 1/2, step 358/7134 completed (loss: 0.2308143675327301, acc: 0.944065511226654)
[2025-02-13 02:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50][root][INFO] - Training Epoch: 1/2, step 359/7134 completed (loss: 0.18227297067642212, acc: 0.9364864826202393)
[2025-02-13 02:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50][root][INFO] - Training Epoch: 1/2, step 360/7134 completed (loss: 0.20932167768478394, acc: 0.9442771077156067)
[2025-02-13 02:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50][root][INFO] - Training Epoch: 1/2, step 361/7134 completed (loss: 0.19690796732902527, acc: 0.9500734210014343)
[2025-02-13 02:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:51][root][INFO] - Training Epoch: 1/2, step 362/7134 completed (loss: 0.22569631040096283, acc: 0.946107804775238)
[2025-02-13 02:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:51][root][INFO] - Training Epoch: 1/2, step 363/7134 completed (loss: 0.2597355842590332, acc: 0.9296875)
[2025-02-13 02:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:52][root][INFO] - Training Epoch: 1/2, step 364/7134 completed (loss: 0.22070430219173431, acc: 0.9322429895401001)
[2025-02-13 02:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:52][root][INFO] - Training Epoch: 1/2, step 365/7134 completed (loss: 0.20399004220962524, acc: 0.9462810158729553)
[2025-02-13 02:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53][root][INFO] - Training Epoch: 1/2, step 366/7134 completed (loss: 0.3639390766620636, acc: 0.9059560894966125)
[2025-02-13 02:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53][root][INFO] - Training Epoch: 1/2, step 367/7134 completed (loss: 0.22894908487796783, acc: 0.9287499785423279)
[2025-02-13 02:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53][root][INFO] - Training Epoch: 1/2, step 368/7134 completed (loss: 0.18990767002105713, acc: 0.9405646324157715)
[2025-02-13 02:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:54][root][INFO] - Training Epoch: 1/2, step 369/7134 completed (loss: 0.30944743752479553, acc: 0.9264931082725525)
[2025-02-13 02:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:54][root][INFO] - Training Epoch: 1/2, step 370/7134 completed (loss: 0.27365395426750183, acc: 0.9301310181617737)
[2025-02-13 02:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:55][root][INFO] - Training Epoch: 1/2, step 371/7134 completed (loss: 0.22563129663467407, acc: 0.9503759145736694)
[2025-02-13 02:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:55][root][INFO] - Training Epoch: 1/2, step 372/7134 completed (loss: 0.2039763480424881, acc: 0.9390787482261658)
[2025-02-13 02:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56][root][INFO] - Training Epoch: 1/2, step 373/7134 completed (loss: 0.16918587684631348, acc: 0.9551569223403931)
[2025-02-13 02:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56][root][INFO] - Training Epoch: 1/2, step 374/7134 completed (loss: 0.3003382086753845, acc: 0.9212218523025513)
[2025-02-13 02:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56][root][INFO] - Training Epoch: 1/2, step 375/7134 completed (loss: 0.30858251452445984, acc: 0.9223560690879822)
[2025-02-13 02:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:57][root][INFO] - Training Epoch: 1/2, step 376/7134 completed (loss: 0.4368995428085327, acc: 0.8951724171638489)
[2025-02-13 02:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:57][root][INFO] - Training Epoch: 1/2, step 377/7134 completed (loss: 0.320189505815506, acc: 0.9188445806503296)
[2025-02-13 02:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:58][root][INFO] - Training Epoch: 1/2, step 378/7134 completed (loss: 0.3122116029262543, acc: 0.919618546962738)
[2025-02-13 02:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:58][root][INFO] - Training Epoch: 1/2, step 379/7134 completed (loss: 0.3047528862953186, acc: 0.9195979833602905)
[2025-02-13 02:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:59][root][INFO] - Training Epoch: 1/2, step 380/7134 completed (loss: 0.39455917477607727, acc: 0.8961892127990723)
[2025-02-13 02:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:59][root][INFO] - Training Epoch: 1/2, step 381/7134 completed (loss: 0.33069777488708496, acc: 0.9147287011146545)
[2025-02-13 02:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00][root][INFO] - Training Epoch: 1/2, step 382/7134 completed (loss: 0.3607659935951233, acc: 0.9062901139259338)
[2025-02-13 02:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00][root][INFO] - Training Epoch: 1/2, step 383/7134 completed (loss: 0.31748172640800476, acc: 0.9262759685516357)
[2025-02-13 02:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00][root][INFO] - Training Epoch: 1/2, step 384/7134 completed (loss: 0.36058124899864197, acc: 0.9105473756790161)
[2025-02-13 02:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:01][root][INFO] - Training Epoch: 1/2, step 385/7134 completed (loss: 0.22289444506168365, acc: 0.9434850811958313)
[2025-02-13 02:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:01][root][INFO] - Training Epoch: 1/2, step 386/7134 completed (loss: 0.24258890748023987, acc: 0.9345602989196777)
[2025-02-13 02:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02][root][INFO] - Training Epoch: 1/2, step 387/7134 completed (loss: 0.3914370834827423, acc: 0.8862974047660828)
[2025-02-13 02:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02][root][INFO] - Training Epoch: 1/2, step 388/7134 completed (loss: 0.2371263951063156, acc: 0.9460093975067139)
[2025-02-13 02:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03][root][INFO] - Training Epoch: 1/2, step 389/7134 completed (loss: 0.3468922972679138, acc: 0.9063360691070557)
[2025-02-13 02:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03][root][INFO] - Training Epoch: 1/2, step 390/7134 completed (loss: 0.3049200475215912, acc: 0.9304206967353821)
[2025-02-13 02:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03][root][INFO] - Training Epoch: 1/2, step 391/7134 completed (loss: 0.2820627689361572, acc: 0.930272102355957)
[2025-02-13 02:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:04][root][INFO] - Training Epoch: 1/2, step 392/7134 completed (loss: 0.3440741002559662, acc: 0.897777795791626)
[2025-02-13 02:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:04][root][INFO] - Training Epoch: 1/2, step 393/7134 completed (loss: 0.3573863208293915, acc: 0.9083333611488342)
[2025-02-13 02:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:05][root][INFO] - Training Epoch: 1/2, step 394/7134 completed (loss: 0.44311001896858215, acc: 0.887159526348114)
[2025-02-13 02:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:05][root][INFO] - Training Epoch: 1/2, step 395/7134 completed (loss: 0.324541300535202, acc: 0.9218106865882874)
[2025-02-13 02:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06][root][INFO] - Training Epoch: 1/2, step 396/7134 completed (loss: 0.2599591612815857, acc: 0.9397849440574646)
[2025-02-13 02:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06][root][INFO] - Training Epoch: 1/2, step 397/7134 completed (loss: 0.32365214824676514, acc: 0.9007936716079712)
[2025-02-13 02:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06][root][INFO] - Training Epoch: 1/2, step 398/7134 completed (loss: 0.293443500995636, acc: 0.9252577424049377)
[2025-02-13 02:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:07][root][INFO] - Training Epoch: 1/2, step 399/7134 completed (loss: 0.3160209357738495, acc: 0.9033280611038208)
[2025-02-13 02:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:07][root][INFO] - Training Epoch: 1/2, step 400/7134 completed (loss: 0.3847815692424774, acc: 0.9029850959777832)
[2025-02-13 02:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:08][root][INFO] - Training Epoch: 1/2, step 401/7134 completed (loss: 0.2735460102558136, acc: 0.9129902124404907)
[2025-02-13 02:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:08][root][INFO] - Training Epoch: 1/2, step 402/7134 completed (loss: 0.297296941280365, acc: 0.9125475287437439)
[2025-02-13 02:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:09][root][INFO] - Training Epoch: 1/2, step 403/7134 completed (loss: 0.27485525608062744, acc: 0.9244060516357422)
[2025-02-13 02:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:09][root][INFO] - Training Epoch: 1/2, step 404/7134 completed (loss: 0.2778698801994324, acc: 0.9333333373069763)
[2025-02-13 02:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10][root][INFO] - Training Epoch: 1/2, step 405/7134 completed (loss: 0.3287464380264282, acc: 0.9190031290054321)
[2025-02-13 02:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10][root][INFO] - Training Epoch: 1/2, step 406/7134 completed (loss: 0.3146585524082184, acc: 0.9163058996200562)
[2025-02-13 02:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10][root][INFO] - Training Epoch: 1/2, step 407/7134 completed (loss: 0.4211103022098541, acc: 0.8966101408004761)
[2025-02-13 02:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:11][root][INFO] - Training Epoch: 1/2, step 408/7134 completed (loss: 0.34745967388153076, acc: 0.9135802388191223)
[2025-02-13 02:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:11][root][INFO] - Training Epoch: 1/2, step 409/7134 completed (loss: 0.2624901533126831, acc: 0.9320755004882812)
[2025-02-13 02:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:12][root][INFO] - Training Epoch: 1/2, step 410/7134 completed (loss: 0.30421993136405945, acc: 0.924332320690155)
[2025-02-13 02:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:12][root][INFO] - Training Epoch: 1/2, step 411/7134 completed (loss: 0.31621915102005005, acc: 0.9148446321487427)
[2025-02-13 02:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:13][root][INFO] - Training Epoch: 1/2, step 412/7134 completed (loss: 0.3314942717552185, acc: 0.9130434989929199)
[2025-02-13 02:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:13][root][INFO] - Training Epoch: 1/2, step 413/7134 completed (loss: 0.2942539155483246, acc: 0.9225634336471558)
[2025-02-13 02:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:14][root][INFO] - Training Epoch: 1/2, step 414/7134 completed (loss: 0.2670258581638336, acc: 0.921364963054657)
[2025-02-13 02:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:14][root][INFO] - Training Epoch: 1/2, step 415/7134 completed (loss: 0.22180336713790894, acc: 0.9351851940155029)
[2025-02-13 02:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:15][root][INFO] - Training Epoch: 1/2, step 416/7134 completed (loss: 0.3294484317302704, acc: 0.9220778942108154)
[2025-02-13 02:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:15][root][INFO] - Training Epoch: 1/2, step 417/7134 completed (loss: 0.2757999002933502, acc: 0.9225543737411499)
[2025-02-13 02:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16][root][INFO] - Training Epoch: 1/2, step 418/7134 completed (loss: 0.22983138263225555, acc: 0.9380022883415222)
[2025-02-13 02:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16][root][INFO] - Training Epoch: 1/2, step 419/7134 completed (loss: 0.2340698093175888, acc: 0.9292804002761841)
[2025-02-13 02:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16][root][INFO] - Training Epoch: 1/2, step 420/7134 completed (loss: 0.2417231798171997, acc: 0.9323180913925171)
[2025-02-13 02:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:17][root][INFO] - Training Epoch: 1/2, step 421/7134 completed (loss: 0.24159865081310272, acc: 0.9378980994224548)
[2025-02-13 02:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:17][root][INFO] - Training Epoch: 1/2, step 422/7134 completed (loss: 0.14489175379276276, acc: 0.9647058844566345)
[2025-02-13 02:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:18][root][INFO] - Training Epoch: 1/2, step 423/7134 completed (loss: 0.18595093488693237, acc: 0.9514285922050476)
[2025-02-13 02:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:18][root][INFO] - Training Epoch: 1/2, step 424/7134 completed (loss: 0.21938619017601013, acc: 0.9351266026496887)
[2025-02-13 02:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19][root][INFO] - Training Epoch: 1/2, step 425/7134 completed (loss: 0.198316752910614, acc: 0.9476743936538696)
[2025-02-13 02:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19][root][INFO] - Training Epoch: 1/2, step 426/7134 completed (loss: 0.15366369485855103, acc: 0.961002767086029)
[2025-02-13 02:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19][root][INFO] - Training Epoch: 1/2, step 427/7134 completed (loss: 0.16920027136802673, acc: 0.9559939503669739)
[2025-02-13 02:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:20][root][INFO] - Training Epoch: 1/2, step 428/7134 completed (loss: 0.23466593027114868, acc: 0.9382529854774475)
[2025-02-13 02:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:20][root][INFO] - Training Epoch: 1/2, step 429/7134 completed (loss: 0.23819413781166077, acc: 0.9348171949386597)
[2025-02-13 02:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:21][root][INFO] - Training Epoch: 1/2, step 430/7134 completed (loss: 0.22279809415340424, acc: 0.9397417306900024)
[2025-02-13 02:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:21][root][INFO] - Training Epoch: 1/2, step 431/7134 completed (loss: 0.19498872756958008, acc: 0.9495677351951599)
[2025-02-13 02:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22][root][INFO] - Training Epoch: 1/2, step 432/7134 completed (loss: 0.13220423460006714, acc: 0.9634369015693665)
[2025-02-13 02:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22][root][INFO] - Training Epoch: 1/2, step 433/7134 completed (loss: 0.13037312030792236, acc: 0.9646739363670349)
[2025-02-13 02:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22][root][INFO] - Training Epoch: 1/2, step 434/7134 completed (loss: 0.15570473670959473, acc: 0.9633967876434326)
[2025-02-13 02:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:23][root][INFO] - Training Epoch: 1/2, step 435/7134 completed (loss: 0.16452744603157043, acc: 0.9517730474472046)
[2025-02-13 02:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:23][root][INFO] - Training Epoch: 1/2, step 436/7134 completed (loss: 0.2087867707014084, acc: 0.9468504190444946)
[2025-02-13 02:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:24][root][INFO] - Training Epoch: 1/2, step 437/7134 completed (loss: 0.26206842064857483, acc: 0.9381625652313232)
[2025-02-13 02:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:24][root][INFO] - Training Epoch: 1/2, step 438/7134 completed (loss: 0.22158518433570862, acc: 0.9461538195610046)
[2025-02-13 02:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25][root][INFO] - Training Epoch: 1/2, step 439/7134 completed (loss: 0.17402419447898865, acc: 0.948952853679657)
[2025-02-13 02:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25][root][INFO] - Training Epoch: 1/2, step 440/7134 completed (loss: 0.2374785840511322, acc: 0.9376083016395569)
[2025-02-13 02:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25][root][INFO] - Training Epoch: 1/2, step 441/7134 completed (loss: 0.1416441649198532, acc: 0.9505247473716736)
[2025-02-13 02:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:26][root][INFO] - Training Epoch: 1/2, step 442/7134 completed (loss: 0.13106384873390198, acc: 0.9626865386962891)
[2025-02-13 02:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:26][root][INFO] - Training Epoch: 1/2, step 443/7134 completed (loss: 0.172029510140419, acc: 0.9503875970840454)
[2025-02-13 02:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:27][root][INFO] - Training Epoch: 1/2, step 444/7134 completed (loss: 0.13473045825958252, acc: 0.9583333134651184)
[2025-02-13 02:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:27][root][INFO] - Training Epoch: 1/2, step 445/7134 completed (loss: 0.19680236279964447, acc: 0.9489796161651611)
[2025-02-13 02:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28][root][INFO] - Training Epoch: 1/2, step 446/7134 completed (loss: 0.11979224532842636, acc: 0.961002767086029)
[2025-02-13 02:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28][root][INFO] - Training Epoch: 1/2, step 447/7134 completed (loss: 0.13393057882785797, acc: 0.9639389514923096)
[2025-02-13 02:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28][root][INFO] - Training Epoch: 1/2, step 448/7134 completed (loss: 0.28916412591934204, acc: 0.9207161068916321)
[2025-02-13 02:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:29][root][INFO] - Training Epoch: 1/2, step 449/7134 completed (loss: 0.27565997838974, acc: 0.9293193817138672)
[2025-02-13 02:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:29][root][INFO] - Training Epoch: 1/2, step 450/7134 completed (loss: 0.3620569705963135, acc: 0.9065817594528198)
[2025-02-13 02:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:30][root][INFO] - Training Epoch: 1/2, step 451/7134 completed (loss: 0.40711769461631775, acc: 0.8929845690727234)
[2025-02-13 02:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:30][root][INFO] - Training Epoch: 1/2, step 452/7134 completed (loss: 0.43680649995803833, acc: 0.8767123222351074)
[2025-02-13 02:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:31][root][INFO] - Training Epoch: 1/2, step 453/7134 completed (loss: 0.3076126277446747, acc: 0.9174311757087708)
[2025-02-13 02:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:31][root][INFO] - Training Epoch: 1/2, step 454/7134 completed (loss: 0.36074933409690857, acc: 0.9063509106636047)
[2025-02-13 02:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:32][root][INFO] - Training Epoch: 1/2, step 455/7134 completed (loss: 0.3457704484462738, acc: 0.9045751690864563)
[2025-02-13 02:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:32][root][INFO] - Training Epoch: 1/2, step 456/7134 completed (loss: 0.2655658423900604, acc: 0.9235127568244934)
[2025-02-13 02:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33][root][INFO] - Training Epoch: 1/2, step 457/7134 completed (loss: 0.2811172604560852, acc: 0.9177852272987366)
[2025-02-13 02:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33][root][INFO] - Training Epoch: 1/2, step 458/7134 completed (loss: 0.22246210277080536, acc: 0.9304174780845642)
[2025-02-13 02:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33][root][INFO] - Training Epoch: 1/2, step 459/7134 completed (loss: 0.38620153069496155, acc: 0.9081481695175171)
[2025-02-13 02:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:34][root][INFO] - Training Epoch: 1/2, step 460/7134 completed (loss: 0.2142046093940735, acc: 0.9448819160461426)
[2025-02-13 02:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:34][root][INFO] - Training Epoch: 1/2, step 461/7134 completed (loss: 0.3075437545776367, acc: 0.9200561046600342)
[2025-02-13 02:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:35][root][INFO] - Training Epoch: 1/2, step 462/7134 completed (loss: 0.34468311071395874, acc: 0.9018691778182983)
[2025-02-13 02:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:35][root][INFO] - Training Epoch: 1/2, step 463/7134 completed (loss: 0.3026137053966522, acc: 0.9184441566467285)
[2025-02-13 02:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:36][root][INFO] - Training Epoch: 1/2, step 464/7134 completed (loss: 0.24554352462291718, acc: 0.9334277510643005)
[2025-02-13 02:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:36][root][INFO] - Training Epoch: 1/2, step 465/7134 completed (loss: 0.24843941628932953, acc: 0.9281984567642212)
[2025-02-13 02:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37][root][INFO] - Training Epoch: 1/2, step 466/7134 completed (loss: 0.3686943054199219, acc: 0.9086229205131531)
[2025-02-13 02:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37][root][INFO] - Training Epoch: 1/2, step 467/7134 completed (loss: 0.32707974314689636, acc: 0.9139297604560852)
[2025-02-13 02:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37][root][INFO] - Training Epoch: 1/2, step 468/7134 completed (loss: 0.35792186856269836, acc: 0.9078404307365417)
[2025-02-13 02:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:38][root][INFO] - Training Epoch: 1/2, step 469/7134 completed (loss: 0.2355642169713974, acc: 0.9337176084518433)
[2025-02-13 02:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:38][root][INFO] - Training Epoch: 1/2, step 470/7134 completed (loss: 0.3485656678676605, acc: 0.8897958993911743)
[2025-02-13 02:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:39][root][INFO] - Training Epoch: 1/2, step 471/7134 completed (loss: 0.26878538727760315, acc: 0.9218934774398804)
[2025-02-13 02:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:39][root][INFO] - Training Epoch: 1/2, step 472/7134 completed (loss: 0.2934868037700653, acc: 0.9208211302757263)
[2025-02-13 02:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40][root][INFO] - Training Epoch: 1/2, step 473/7134 completed (loss: 0.24401424825191498, acc: 0.9259259104728699)
[2025-02-13 02:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40][root][INFO] - Training Epoch: 1/2, step 474/7134 completed (loss: 0.21198633313179016, acc: 0.9341238737106323)
[2025-02-13 02:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40][root][INFO] - Training Epoch: 1/2, step 475/7134 completed (loss: 0.379740446805954, acc: 0.9014706015586853)
[2025-02-13 02:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:41][root][INFO] - Training Epoch: 1/2, step 476/7134 completed (loss: 0.322034627199173, acc: 0.9083094596862793)
[2025-02-13 02:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:41][root][INFO] - Training Epoch: 1/2, step 477/7134 completed (loss: 0.2419692575931549, acc: 0.932762861251831)
[2025-02-13 02:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:42][root][INFO] - Training Epoch: 1/2, step 478/7134 completed (loss: 0.3537032902240753, acc: 0.9112050533294678)
[2025-02-13 02:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:42][root][INFO] - Training Epoch: 1/2, step 479/7134 completed (loss: 0.16788215935230255, acc: 0.952654242515564)
[2025-02-13 02:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43][root][INFO] - Training Epoch: 1/2, step 480/7134 completed (loss: 0.16859376430511475, acc: 0.9518248438835144)
[2025-02-13 02:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43][root][INFO] - Training Epoch: 1/2, step 481/7134 completed (loss: 0.22029566764831543, acc: 0.9427083134651184)
[2025-02-13 02:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43][root][INFO] - Training Epoch: 1/2, step 482/7134 completed (loss: 0.15125837922096252, acc: 0.9628571271896362)
[2025-02-13 02:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:44][root][INFO] - Training Epoch: 1/2, step 483/7134 completed (loss: 0.17952391505241394, acc: 0.9508599638938904)
[2025-02-13 02:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:44][root][INFO] - Training Epoch: 1/2, step 484/7134 completed (loss: 0.22232499718666077, acc: 0.9371657967567444)
[2025-02-13 02:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:45][root][INFO] - Training Epoch: 1/2, step 485/7134 completed (loss: 0.2557569146156311, acc: 0.9364598989486694)
[2025-02-13 02:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:45][root][INFO] - Training Epoch: 1/2, step 486/7134 completed (loss: 0.3411679267883301, acc: 0.9292762875556946)
[2025-02-13 02:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46][root][INFO] - Training Epoch: 1/2, step 487/7134 completed (loss: 0.18640179932117462, acc: 0.9456681609153748)
[2025-02-13 02:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46][root][INFO] - Training Epoch: 1/2, step 488/7134 completed (loss: 0.2838001549243927, acc: 0.9202898740768433)
[2025-02-13 02:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46][root][INFO] - Training Epoch: 1/2, step 489/7134 completed (loss: 0.18764656782150269, acc: 0.9546218514442444)
[2025-02-13 02:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:47][root][INFO] - Training Epoch: 1/2, step 490/7134 completed (loss: 0.16546639800071716, acc: 0.956462562084198)
[2025-02-13 02:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:47][root][INFO] - Training Epoch: 1/2, step 491/7134 completed (loss: 0.1453867405653, acc: 0.9605262875556946)
[2025-02-13 02:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:48][root][INFO] - Training Epoch: 1/2, step 492/7134 completed (loss: 0.1234467551112175, acc: 0.9606656432151794)
[2025-02-13 02:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:48][root][INFO] - Training Epoch: 1/2, step 493/7134 completed (loss: 0.25613611936569214, acc: 0.9240687489509583)
[2025-02-13 02:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49][root][INFO] - Training Epoch: 1/2, step 494/7134 completed (loss: 0.16923245787620544, acc: 0.9593750238418579)
[2025-02-13 02:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49][root][INFO] - Training Epoch: 1/2, step 495/7134 completed (loss: 0.15592196583747864, acc: 0.9666666388511658)
[2025-02-13 02:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49][root][INFO] - Training Epoch: 1/2, step 496/7134 completed (loss: 0.1842360496520996, acc: 0.9457994699478149)
[2025-02-13 02:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:50][root][INFO] - Training Epoch: 1/2, step 497/7134 completed (loss: 0.1696721762418747, acc: 0.9554263353347778)
[2025-02-13 02:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:50][root][INFO] - Training Epoch: 1/2, step 498/7134 completed (loss: 0.1674698442220688, acc: 0.9583333134651184)
[2025-02-13 02:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:51][root][INFO] - Training Epoch: 1/2, step 499/7134 completed (loss: 0.09606675803661346, acc: 0.9749652147293091)
[2025-02-13 02:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:51][root][INFO] - Training Epoch: 1/2, step 500/7134 completed (loss: 0.15664911270141602, acc: 0.9626666903495789)
[2025-02-13 02:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52][root][INFO] - Training Epoch: 1/2, step 501/7134 completed (loss: 0.10682723671197891, acc: 0.9639889001846313)
[2025-02-13 02:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52][root][INFO] - Training Epoch: 1/2, step 502/7134 completed (loss: 0.08532429486513138, acc: 0.9785932898521423)
[2025-02-13 02:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52][root][INFO] - Training Epoch: 1/2, step 503/7134 completed (loss: 0.162911519408226, acc: 0.9557046890258789)
[2025-02-13 02:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:53][root][INFO] - Training Epoch: 1/2, step 504/7134 completed (loss: 0.10161270946264267, acc: 0.9678770899772644)
[2025-02-13 02:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:53][root][INFO] - Training Epoch: 1/2, step 505/7134 completed (loss: 0.06587622314691544, acc: 0.981675386428833)
[2025-02-13 02:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:54][root][INFO] - Training Epoch: 1/2, step 506/7134 completed (loss: 0.1656903475522995, acc: 0.9477611780166626)
[2025-02-13 02:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:54][root][INFO] - Training Epoch: 1/2, step 507/7134 completed (loss: 0.12779274582862854, acc: 0.9684210419654846)
[2025-02-13 02:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:55][root][INFO] - Training Epoch: 1/2, step 508/7134 completed (loss: 0.1781958043575287, acc: 0.9539105892181396)
[2025-02-13 02:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:55][root][INFO] - Training Epoch: 1/2, step 509/7134 completed (loss: 0.22212272882461548, acc: 0.9397217631340027)
[2025-02-13 02:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56][root][INFO] - Training Epoch: 1/2, step 510/7134 completed (loss: 0.3449840247631073, acc: 0.9168398976325989)
[2025-02-13 02:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56][root][INFO] - Training Epoch: 1/2, step 511/7134 completed (loss: 0.3264060318470001, acc: 0.9168704152107239)
[2025-02-13 02:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56][root][INFO] - Training Epoch: 1/2, step 512/7134 completed (loss: 0.2948702573776245, acc: 0.9298596978187561)
[2025-02-13 02:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57][root][INFO] - Training Epoch: 1/2, step 513/7134 completed (loss: 0.2379172146320343, acc: 0.9242684841156006)
[2025-02-13 02:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57][root][INFO] - Training Epoch: 1/2, step 514/7134 completed (loss: 0.3632795214653015, acc: 0.9120603203773499)
[2025-02-13 02:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58][root][INFO] - Training Epoch: 1/2, step 515/7134 completed (loss: 0.20788176357746124, acc: 0.9462517499923706)
[2025-02-13 02:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58][root][INFO] - Training Epoch: 1/2, step 516/7134 completed (loss: 0.16259144246578217, acc: 0.95652174949646)
[2025-02-13 02:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58][root][INFO] - Training Epoch: 1/2, step 517/7134 completed (loss: 0.27728837728500366, acc: 0.9354838728904724)
[2025-02-13 02:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:59][root][INFO] - Training Epoch: 1/2, step 518/7134 completed (loss: 0.1403169184923172, acc: 0.9610195159912109)
[2025-02-13 02:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:59][root][INFO] - Training Epoch: 1/2, step 519/7134 completed (loss: 0.19038709998130798, acc: 0.9422680139541626)
[2025-02-13 02:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00][root][INFO] - Training Epoch: 1/2, step 520/7134 completed (loss: 0.18125653266906738, acc: 0.9462025165557861)
[2025-02-13 02:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00][root][INFO] - Training Epoch: 1/2, step 521/7134 completed (loss: 0.21181683242321014, acc: 0.9412550330162048)
[2025-02-13 02:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00][root][INFO] - Training Epoch: 1/2, step 522/7134 completed (loss: 0.18992799520492554, acc: 0.9557640552520752)
[2025-02-13 02:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:01][root][INFO] - Training Epoch: 1/2, step 523/7134 completed (loss: 0.1920412927865982, acc: 0.9473684430122375)
[2025-02-13 02:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:01][root][INFO] - Training Epoch: 1/2, step 524/7134 completed (loss: 0.15033391118049622, acc: 0.9610214829444885)
[2025-02-13 02:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02][root][INFO] - Training Epoch: 1/2, step 525/7134 completed (loss: 0.2517574727535248, acc: 0.9304482340812683)
[2025-02-13 02:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02][root][INFO] - Training Epoch: 1/2, step 526/7134 completed (loss: 0.16596472263336182, acc: 0.9589040875434875)
[2025-02-13 02:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:03][root][INFO] - Training Epoch: 1/2, step 527/7134 completed (loss: 0.19092130661010742, acc: 0.9477611780166626)
[2025-02-13 02:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:03][root][INFO] - Training Epoch: 1/2, step 528/7134 completed (loss: 0.0887971818447113, acc: 0.9818913340568542)
[2025-02-13 02:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04][root][INFO] - Training Epoch: 1/2, step 529/7134 completed (loss: 0.2345631867647171, acc: 0.9317460060119629)
[2025-02-13 02:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04][root][INFO] - Training Epoch: 1/2, step 530/7134 completed (loss: 0.179239884018898, acc: 0.9451115131378174)
[2025-02-13 02:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04][root][INFO] - Training Epoch: 1/2, step 531/7134 completed (loss: 0.19793806970119476, acc: 0.952136754989624)
[2025-02-13 02:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:05][root][INFO] - Training Epoch: 1/2, step 532/7134 completed (loss: 0.1034039780497551, acc: 0.9729729890823364)
[2025-02-13 02:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:05][root][INFO] - Training Epoch: 1/2, step 533/7134 completed (loss: 0.16760146617889404, acc: 0.9450171589851379)
[2025-02-13 02:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:06][root][INFO] - Training Epoch: 1/2, step 534/7134 completed (loss: 0.12385625392198563, acc: 0.9615384340286255)
[2025-02-13 02:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:06][root][INFO] - Training Epoch: 1/2, step 535/7134 completed (loss: 0.15188556909561157, acc: 0.95782071352005)
[2025-02-13 02:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07][root][INFO] - Training Epoch: 1/2, step 536/7134 completed (loss: 0.20399042963981628, acc: 0.9547445178031921)
[2025-02-13 02:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07][root][INFO] - Training Epoch: 1/2, step 537/7134 completed (loss: 0.19532175362110138, acc: 0.9528718590736389)
[2025-02-13 02:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07][root][INFO] - Training Epoch: 1/2, step 538/7134 completed (loss: 0.17417290806770325, acc: 0.9554896354675293)
[2025-02-13 02:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:08][root][INFO] - Training Epoch: 1/2, step 539/7134 completed (loss: 0.16705597937107086, acc: 0.9584774971008301)
[2025-02-13 02:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:08][root][INFO] - Training Epoch: 1/2, step 540/7134 completed (loss: 0.17875684797763824, acc: 0.9575757384300232)
[2025-02-13 02:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:09][root][INFO] - Training Epoch: 1/2, step 541/7134 completed (loss: 0.2632414698600769, acc: 0.9308510422706604)
[2025-02-13 02:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:09][root][INFO] - Training Epoch: 1/2, step 542/7134 completed (loss: 0.2755109965801239, acc: 0.9178403615951538)
[2025-02-13 02:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:10][root][INFO] - Training Epoch: 1/2, step 543/7134 completed (loss: 0.26789093017578125, acc: 0.9240506291389465)
[2025-02-13 02:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:10][root][INFO] - Training Epoch: 1/2, step 544/7134 completed (loss: 0.30539989471435547, acc: 0.9132491946220398)
[2025-02-13 02:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:10][root][INFO] - Training Epoch: 1/2, step 545/7134 completed (loss: 0.2827964425086975, acc: 0.9275362491607666)
[2025-02-13 02:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:11][root][INFO] - Training Epoch: 1/2, step 546/7134 completed (loss: 0.2681274116039276, acc: 0.9243499040603638)
[2025-02-13 02:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:11][root][INFO] - Training Epoch: 1/2, step 547/7134 completed (loss: 0.2088097482919693, acc: 0.944847583770752)
[2025-02-13 02:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:12][root][INFO] - Training Epoch: 1/2, step 548/7134 completed (loss: 0.13293714821338654, acc: 0.9595828056335449)
[2025-02-13 02:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:12][root][INFO] - Training Epoch: 1/2, step 549/7134 completed (loss: 0.23777088522911072, acc: 0.9316770434379578)
[2025-02-13 02:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:13][root][INFO] - Training Epoch: 1/2, step 550/7134 completed (loss: 0.1541302353143692, acc: 0.9624573588371277)
[2025-02-13 02:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:13][root][INFO] - Training Epoch: 1/2, step 551/7134 completed (loss: 0.22417381405830383, acc: 0.9373849034309387)
[2025-02-13 02:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:13][root][INFO] - Training Epoch: 1/2, step 552/7134 completed (loss: 0.18438223004341125, acc: 0.9558404684066772)
[2025-02-13 02:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:14][root][INFO] - Training Epoch: 1/2, step 553/7134 completed (loss: 0.16651864349842072, acc: 0.9583333134651184)
[2025-02-13 02:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:14][root][INFO] - Training Epoch: 1/2, step 554/7134 completed (loss: 0.1672097146511078, acc: 0.9614835977554321)
[2025-02-13 02:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15][root][INFO] - Training Epoch: 1/2, step 555/7134 completed (loss: 0.2048666924238205, acc: 0.9478672742843628)
[2025-02-13 02:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15][root][INFO] - Training Epoch: 1/2, step 556/7134 completed (loss: 0.124444380402565, acc: 0.9702970385551453)
[2025-02-13 02:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15][root][INFO] - Training Epoch: 1/2, step 557/7134 completed (loss: 0.18538640439510345, acc: 0.9475465416908264)
[2025-02-13 02:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:16][root][INFO] - Training Epoch: 1/2, step 558/7134 completed (loss: 0.16142840683460236, acc: 0.9560283422470093)
[2025-02-13 02:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:16][root][INFO] - Training Epoch: 1/2, step 559/7134 completed (loss: 0.22849197685718536, acc: 0.9316239356994629)
[2025-02-13 02:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:17][root][INFO] - Training Epoch: 1/2, step 560/7134 completed (loss: 0.15850766003131866, acc: 0.9525483250617981)
[2025-02-13 02:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:17][root][INFO] - Training Epoch: 1/2, step 561/7134 completed (loss: 0.20069383084774017, acc: 0.9399744868278503)
[2025-02-13 02:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:18][root][INFO] - Training Epoch: 1/2, step 562/7134 completed (loss: 0.14976036548614502, acc: 0.9549669027328491)
[2025-02-13 02:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:18][root][INFO] - Training Epoch: 1/2, step 563/7134 completed (loss: 0.2737240791320801, acc: 0.922468364238739)
[2025-02-13 02:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19][root][INFO] - Training Epoch: 1/2, step 564/7134 completed (loss: 0.1767401248216629, acc: 0.9493201375007629)
[2025-02-13 02:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19][root][INFO] - Training Epoch: 1/2, step 565/7134 completed (loss: 0.1745561957359314, acc: 0.9564489126205444)
[2025-02-13 02:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19][root][INFO] - Training Epoch: 1/2, step 566/7134 completed (loss: 0.23862458765506744, acc: 0.9466357231140137)
[2025-02-13 02:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:20][root][INFO] - Training Epoch: 1/2, step 567/7134 completed (loss: 0.17803826928138733, acc: 0.9607293009757996)
[2025-02-13 02:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:20][root][INFO] - Training Epoch: 1/2, step 568/7134 completed (loss: 0.18401886522769928, acc: 0.9464052319526672)
[2025-02-13 02:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:21][root][INFO] - Training Epoch: 1/2, step 569/7134 completed (loss: 0.1598767340183258, acc: 0.9555555582046509)
[2025-02-13 02:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:21][root][INFO] - Training Epoch: 1/2, step 570/7134 completed (loss: 0.19269618391990662, acc: 0.9388954043388367)
[2025-02-13 02:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:22][root][INFO] - Training Epoch: 1/2, step 571/7134 completed (loss: 0.1635274589061737, acc: 0.9559193849563599)
[2025-02-13 02:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:22][root][INFO] - Training Epoch: 1/2, step 572/7134 completed (loss: 0.11138032376766205, acc: 0.9626769423484802)
[2025-02-13 02:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23][root][INFO] - Training Epoch: 1/2, step 573/7134 completed (loss: 0.12899711728096008, acc: 0.9629155993461609)
[2025-02-13 02:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23][root][INFO] - Training Epoch: 1/2, step 574/7134 completed (loss: 0.1372065246105194, acc: 0.9683794379234314)
[2025-02-13 02:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23][root][INFO] - Training Epoch: 1/2, step 575/7134 completed (loss: 0.14730539917945862, acc: 0.9627249240875244)
[2025-02-13 02:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:24][root][INFO] - Training Epoch: 1/2, step 576/7134 completed (loss: 0.12218122184276581, acc: 0.9678217768669128)
[2025-02-13 02:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:24][root][INFO] - Training Epoch: 1/2, step 577/7134 completed (loss: 0.15777915716171265, acc: 0.9552041888237)
[2025-02-13 02:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:25][root][INFO] - Training Epoch: 1/2, step 578/7134 completed (loss: 0.18783821165561676, acc: 0.9373549818992615)
[2025-02-13 02:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:25][root][INFO] - Training Epoch: 1/2, step 579/7134 completed (loss: 0.2701566219329834, acc: 0.9229781627655029)
[2025-02-13 02:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26][root][INFO] - Training Epoch: 1/2, step 580/7134 completed (loss: 0.2058863788843155, acc: 0.9401820302009583)
[2025-02-13 02:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26][root][INFO] - Training Epoch: 1/2, step 581/7134 completed (loss: 0.14108087122440338, acc: 0.9595537185668945)
[2025-02-13 02:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26][root][INFO] - Training Epoch: 1/2, step 582/7134 completed (loss: 0.1686624437570572, acc: 0.9580838084220886)
[2025-02-13 02:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:27][root][INFO] - Training Epoch: 1/2, step 583/7134 completed (loss: 0.12759821116924286, acc: 0.9623376727104187)
[2025-02-13 02:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:27][root][INFO] - Training Epoch: 1/2, step 584/7134 completed (loss: 0.12452694028615952, acc: 0.9665071964263916)
[2025-02-13 02:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:28][root][INFO] - Training Epoch: 1/2, step 585/7134 completed (loss: 0.16954141855239868, acc: 0.9528985619544983)
[2025-02-13 02:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:28][root][INFO] - Training Epoch: 1/2, step 586/7134 completed (loss: 0.16834937036037445, acc: 0.9543676376342773)
[2025-02-13 02:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:29][root][INFO] - Training Epoch: 1/2, step 587/7134 completed (loss: 0.14479215443134308, acc: 0.9644218683242798)
[2025-02-13 02:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:29][root][INFO] - Training Epoch: 1/2, step 588/7134 completed (loss: 0.18824614584445953, acc: 0.9544740915298462)
[2025-02-13 02:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30][root][INFO] - Training Epoch: 1/2, step 589/7134 completed (loss: 0.1547766774892807, acc: 0.9632892608642578)
[2025-02-13 02:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30][root][INFO] - Training Epoch: 1/2, step 590/7134 completed (loss: 0.06903763860464096, acc: 0.9729323387145996)
[2025-02-13 02:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30][root][INFO] - Training Epoch: 1/2, step 591/7134 completed (loss: 0.14499208331108093, acc: 0.9673123359680176)
[2025-02-13 02:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:31][root][INFO] - Training Epoch: 1/2, step 592/7134 completed (loss: 0.1397918313741684, acc: 0.9593908786773682)
[2025-02-13 02:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:31][root][INFO] - Training Epoch: 1/2, step 593/7134 completed (loss: 0.126477912068367, acc: 0.9566929340362549)
[2025-02-13 02:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:32][root][INFO] - Training Epoch: 1/2, step 594/7134 completed (loss: 0.15192359685897827, acc: 0.952201247215271)
[2025-02-13 02:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:32][root][INFO] - Training Epoch: 1/2, step 595/7134 completed (loss: 0.183546781539917, acc: 0.9532237648963928)
[2025-02-13 02:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:33][root][INFO] - Training Epoch: 1/2, step 596/7134 completed (loss: 0.10251060128211975, acc: 0.967783510684967)
[2025-02-13 02:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:33][root][INFO] - Training Epoch: 1/2, step 597/7134 completed (loss: 0.2941897213459015, acc: 0.9196310639381409)
[2025-02-13 02:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:34][root][INFO] - Training Epoch: 1/2, step 598/7134 completed (loss: 0.23835813999176025, acc: 0.9364005327224731)
[2025-02-13 02:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:34][root][INFO] - Training Epoch: 1/2, step 599/7134 completed (loss: 0.1767025887966156, acc: 0.9473684430122375)
[2025-02-13 02:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35][root][INFO] - Training Epoch: 1/2, step 600/7134 completed (loss: 0.1687031239271164, acc: 0.9554753303527832)
[2025-02-13 02:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35][root][INFO] - Training Epoch: 1/2, step 601/7134 completed (loss: 0.16695208847522736, acc: 0.954356849193573)
[2025-02-13 02:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35][root][INFO] - Training Epoch: 1/2, step 602/7134 completed (loss: 0.19395411014556885, acc: 0.9489558935165405)
[2025-02-13 02:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:36][root][INFO] - Training Epoch: 1/2, step 603/7134 completed (loss: 0.20807263255119324, acc: 0.9460154175758362)
[2025-02-13 02:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:36][root][INFO] - Training Epoch: 1/2, step 604/7134 completed (loss: 0.19552665948867798, acc: 0.9355783462524414)
[2025-02-13 02:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:37][root][INFO] - Training Epoch: 1/2, step 605/7134 completed (loss: 0.2068033367395401, acc: 0.9443005323410034)
[2025-02-13 02:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:37][root][INFO] - Training Epoch: 1/2, step 606/7134 completed (loss: 0.2929333448410034, acc: 0.9344870448112488)
[2025-02-13 02:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:38][root][INFO] - Training Epoch: 1/2, step 607/7134 completed (loss: 0.1917106956243515, acc: 0.9479305744171143)
[2025-02-13 02:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:38][root][INFO] - Training Epoch: 1/2, step 608/7134 completed (loss: 0.15064287185668945, acc: 0.9598445892333984)
[2025-02-13 02:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39][root][INFO] - Training Epoch: 1/2, step 609/7134 completed (loss: 0.22188059985637665, acc: 0.9332552552223206)
[2025-02-13 02:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39][root][INFO] - Training Epoch: 1/2, step 610/7134 completed (loss: 0.21639131009578705, acc: 0.9355246424674988)
[2025-02-13 02:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39][root][INFO] - Training Epoch: 1/2, step 611/7134 completed (loss: 0.18018752336502075, acc: 0.9512194991111755)
[2025-02-13 02:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:40][root][INFO] - Training Epoch: 1/2, step 612/7134 completed (loss: 0.16229093074798584, acc: 0.9459102749824524)
[2025-02-13 02:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:40][root][INFO] - Training Epoch: 1/2, step 613/7134 completed (loss: 0.20981323719024658, acc: 0.9378663301467896)
[2025-02-13 02:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:41][root][INFO] - Training Epoch: 1/2, step 614/7134 completed (loss: 0.15546640753746033, acc: 0.9580973982810974)
[2025-02-13 02:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:41][root][INFO] - Training Epoch: 1/2, step 615/7134 completed (loss: 0.18949252367019653, acc: 0.9496932625770569)
[2025-02-13 02:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:42][root][INFO] - Training Epoch: 1/2, step 616/7134 completed (loss: 0.17833825945854187, acc: 0.9511363506317139)
[2025-02-13 02:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:42][root][INFO] - Training Epoch: 1/2, step 617/7134 completed (loss: 0.16792775690555573, acc: 0.9589977264404297)
[2025-02-13 02:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43][root][INFO] - Training Epoch: 1/2, step 618/7134 completed (loss: 0.18444491922855377, acc: 0.9491978883743286)
[2025-02-13 02:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43][root][INFO] - Training Epoch: 1/2, step 619/7134 completed (loss: 0.22124451398849487, acc: 0.9453441500663757)
[2025-02-13 02:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43][root][INFO] - Training Epoch: 1/2, step 620/7134 completed (loss: 0.2576168477535248, acc: 0.9442675113677979)
[2025-02-13 02:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:44][root][INFO] - Training Epoch: 1/2, step 621/7134 completed (loss: 0.15215946733951569, acc: 0.9543378949165344)
[2025-02-13 02:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:44][root][INFO] - Training Epoch: 1/2, step 622/7134 completed (loss: 0.16780687868595123, acc: 0.9512194991111755)
[2025-02-13 02:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:45][root][INFO] - Training Epoch: 1/2, step 623/7134 completed (loss: 0.21891288459300995, acc: 0.9518950581550598)
[2025-02-13 02:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:45][root][INFO] - Training Epoch: 1/2, step 624/7134 completed (loss: 0.19493865966796875, acc: 0.9527027010917664)
[2025-02-13 02:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46][root][INFO] - Training Epoch: 1/2, step 625/7134 completed (loss: 0.18033641576766968, acc: 0.9445910453796387)
[2025-02-13 02:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46][root][INFO] - Training Epoch: 1/2, step 626/7134 completed (loss: 0.1714807003736496, acc: 0.9553264379501343)
[2025-02-13 02:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46][root][INFO] - Training Epoch: 1/2, step 627/7134 completed (loss: 0.166581392288208, acc: 0.9454277157783508)
[2025-02-13 02:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:47][root][INFO] - Training Epoch: 1/2, step 628/7134 completed (loss: 0.1401783525943756, acc: 0.957004189491272)
[2025-02-13 02:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:47][root][INFO] - Training Epoch: 1/2, step 629/7134 completed (loss: 0.1653779149055481, acc: 0.9570815563201904)
[2025-02-13 02:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:48][root][INFO] - Training Epoch: 1/2, step 630/7134 completed (loss: 0.16161295771598816, acc: 0.9559585452079773)
[2025-02-13 02:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:48][root][INFO] - Training Epoch: 1/2, step 631/7134 completed (loss: 0.13085904717445374, acc: 0.9605262875556946)
[2025-02-13 02:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49][root][INFO] - Training Epoch: 1/2, step 632/7134 completed (loss: 0.14592626690864563, acc: 0.9611650705337524)
[2025-02-13 02:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49][root][INFO] - Training Epoch: 1/2, step 633/7134 completed (loss: 0.16104631125926971, acc: 0.9535558819770813)
[2025-02-13 02:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49][root][INFO] - Training Epoch: 1/2, step 634/7134 completed (loss: 0.1401321142911911, acc: 0.9588607549667358)
[2025-02-13 02:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:50][root][INFO] - Training Epoch: 1/2, step 635/7134 completed (loss: 0.17894689738750458, acc: 0.9450381398200989)
[2025-02-13 02:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:50][root][INFO] - Training Epoch: 1/2, step 636/7134 completed (loss: 0.1762671321630478, acc: 0.9572413563728333)
[2025-02-13 02:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:51][root][INFO] - Training Epoch: 1/2, step 637/7134 completed (loss: 0.13566331565380096, acc: 0.9702048301696777)
[2025-02-13 02:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:51][root][INFO] - Training Epoch: 1/2, step 638/7134 completed (loss: 0.2134999781847, acc: 0.9516380429267883)
[2025-02-13 02:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52][root][INFO] - Training Epoch: 1/2, step 639/7134 completed (loss: 0.17783202230930328, acc: 0.9466666579246521)
[2025-02-13 02:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52][root][INFO] - Training Epoch: 1/2, step 640/7134 completed (loss: 0.19105027616024017, acc: 0.947826087474823)
[2025-02-13 02:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52][root][INFO] - Training Epoch: 1/2, step 641/7134 completed (loss: 0.11028873175382614, acc: 0.9705401062965393)
[2025-02-13 02:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:53][root][INFO] - Training Epoch: 1/2, step 642/7134 completed (loss: 0.12361599504947662, acc: 0.9625850319862366)
[2025-02-13 02:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:53][root][INFO] - Training Epoch: 1/2, step 643/7134 completed (loss: 0.09847338497638702, acc: 0.9640564918518066)
[2025-02-13 02:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54][root][INFO] - Training Epoch: 1/2, step 644/7134 completed (loss: 0.13501203060150146, acc: 0.9555984735488892)
[2025-02-13 02:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54][root][INFO] - Training Epoch: 1/2, step 645/7134 completed (loss: 0.17877952754497528, acc: 0.9528619647026062)
[2025-02-13 02:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54][root][INFO] - Training Epoch: 1/2, step 646/7134 completed (loss: 0.19581739604473114, acc: 0.9403669834136963)
[2025-02-13 02:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:55][root][INFO] - Training Epoch: 1/2, step 647/7134 completed (loss: 0.13214249908924103, acc: 0.9678511023521423)
[2025-02-13 02:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:55][root][INFO] - Training Epoch: 1/2, step 648/7134 completed (loss: 0.3025844991207123, acc: 0.9284467697143555)
[2025-02-13 02:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:56][root][INFO] - Training Epoch: 1/2, step 649/7134 completed (loss: 0.1711151897907257, acc: 0.9487179517745972)
[2025-02-13 02:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:56][root][INFO] - Training Epoch: 1/2, step 650/7134 completed (loss: 0.21704037487506866, acc: 0.948194682598114)
[2025-02-13 02:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57][root][INFO] - Training Epoch: 1/2, step 651/7134 completed (loss: 0.41102054715156555, acc: 0.9021406769752502)
[2025-02-13 02:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57][root][INFO] - Training Epoch: 1/2, step 652/7134 completed (loss: 0.22138062119483948, acc: 0.9347078800201416)
[2025-02-13 02:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57][root][INFO] - Training Epoch: 1/2, step 653/7134 completed (loss: 0.20496919751167297, acc: 0.937588632106781)
[2025-02-13 02:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58][root][INFO] - Training Epoch: 1/2, step 654/7134 completed (loss: 0.3534645736217499, acc: 0.9071274399757385)
[2025-02-13 02:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58][root][INFO] - Training Epoch: 1/2, step 655/7134 completed (loss: 0.22857415676116943, acc: 0.9395161271095276)
[2025-02-13 02:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:59][root][INFO] - Training Epoch: 1/2, step 656/7134 completed (loss: 0.26049768924713135, acc: 0.934036910533905)
[2025-02-13 02:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:59][root][INFO] - Training Epoch: 1/2, step 657/7134 completed (loss: 0.22712725400924683, acc: 0.9309791326522827)
[2025-02-13 02:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:00][root][INFO] - Training Epoch: 1/2, step 658/7134 completed (loss: 0.21271651983261108, acc: 0.9499341249465942)
[2025-02-13 02:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:00][root][INFO] - Training Epoch: 1/2, step 659/7134 completed (loss: 0.2169514149427414, acc: 0.9281525015830994)
[2025-02-13 02:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:01][root][INFO] - Training Epoch: 1/2, step 660/7134 completed (loss: 0.22642171382904053, acc: 0.9459064602851868)
[2025-02-13 02:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:01][root][INFO] - Training Epoch: 1/2, step 661/7134 completed (loss: 0.2545134425163269, acc: 0.939330518245697)
[2025-02-13 02:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02][root][INFO] - Training Epoch: 1/2, step 662/7134 completed (loss: 0.166092187166214, acc: 0.9517426490783691)
[2025-02-13 02:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02][root][INFO] - Training Epoch: 1/2, step 663/7134 completed (loss: 0.21555203199386597, acc: 0.9416385889053345)
[2025-02-13 02:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02][root][INFO] - Training Epoch: 1/2, step 664/7134 completed (loss: 0.16541650891304016, acc: 0.9583333134651184)
[2025-02-13 02:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03][root][INFO] - Training Epoch: 1/2, step 665/7134 completed (loss: 0.18853460252285004, acc: 0.9502018690109253)
[2025-02-13 02:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03][root][INFO] - Training Epoch: 1/2, step 666/7134 completed (loss: 0.25234341621398926, acc: 0.9313187003135681)
[2025-02-13 02:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:04][root][INFO] - Training Epoch: 1/2, step 667/7134 completed (loss: 0.21986667811870575, acc: 0.9440353512763977)
[2025-02-13 02:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:04][root][INFO] - Training Epoch: 1/2, step 668/7134 completed (loss: 0.12808173894882202, acc: 0.9670469164848328)
[2025-02-13 02:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:05][root][INFO] - Training Epoch: 1/2, step 669/7134 completed (loss: 0.15752267837524414, acc: 0.9604365825653076)
[2025-02-13 02:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:05][root][INFO] - Training Epoch: 1/2, step 670/7134 completed (loss: 0.2138551026582718, acc: 0.9445585012435913)
[2025-02-13 02:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:06][root][INFO] - Training Epoch: 1/2, step 671/7134 completed (loss: 0.19853341579437256, acc: 0.9421221613883972)
[2025-02-13 02:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:06][root][INFO] - Training Epoch: 1/2, step 672/7134 completed (loss: 0.1715976744890213, acc: 0.9552906155586243)
[2025-02-13 02:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:07][root][INFO] - Training Epoch: 1/2, step 673/7134 completed (loss: 0.1961270123720169, acc: 0.952088475227356)
[2025-02-13 02:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:07][root][INFO] - Training Epoch: 1/2, step 674/7134 completed (loss: 0.23332522809505463, acc: 0.9396681785583496)
[2025-02-13 02:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08][root][INFO] - Training Epoch: 1/2, step 675/7134 completed (loss: 0.19191208481788635, acc: 0.9530639052391052)
[2025-02-13 02:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08][root][INFO] - Training Epoch: 1/2, step 676/7134 completed (loss: 0.16361463069915771, acc: 0.9597632884979248)
[2025-02-13 02:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08][root][INFO] - Training Epoch: 1/2, step 677/7134 completed (loss: 0.11877705901861191, acc: 0.9642416834831238)
[2025-02-13 02:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:09][root][INFO] - Training Epoch: 1/2, step 678/7134 completed (loss: 0.21725215017795563, acc: 0.9367429614067078)
[2025-02-13 02:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:09][root][INFO] - Training Epoch: 1/2, step 679/7134 completed (loss: 0.09651636332273483, acc: 0.9754253029823303)
[2025-02-13 02:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:10][root][INFO] - Training Epoch: 1/2, step 680/7134 completed (loss: 0.2467256337404251, acc: 0.9353099465370178)
[2025-02-13 02:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:10][root][INFO] - Training Epoch: 1/2, step 681/7134 completed (loss: 0.13830094039440155, acc: 0.9582822322845459)
[2025-02-13 02:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:11][root][INFO] - Training Epoch: 1/2, step 682/7134 completed (loss: 0.1264866292476654, acc: 0.9711815714836121)
[2025-02-13 02:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:11][root][INFO] - Training Epoch: 1/2, step 683/7134 completed (loss: 0.14857375621795654, acc: 0.9753086566925049)
[2025-02-13 02:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12][root][INFO] - Training Epoch: 1/2, step 684/7134 completed (loss: 0.16795188188552856, acc: 0.9612188339233398)
[2025-02-13 02:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12][root][INFO] - Training Epoch: 1/2, step 685/7134 completed (loss: 0.17468424141407013, acc: 0.9513715505599976)
[2025-02-13 02:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12][root][INFO] - Training Epoch: 1/2, step 686/7134 completed (loss: 0.10484246164560318, acc: 0.9696168899536133)
[2025-02-13 02:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:13][root][INFO] - Training Epoch: 1/2, step 687/7134 completed (loss: 0.18028457462787628, acc: 0.9477756023406982)
[2025-02-13 02:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:13][root][INFO] - Training Epoch: 1/2, step 688/7134 completed (loss: 0.07721526175737381, acc: 0.9778106212615967)
[2025-02-13 02:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:14][root][INFO] - Training Epoch: 1/2, step 689/7134 completed (loss: 0.08306704461574554, acc: 0.9709401726722717)
[2025-02-13 02:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:14][root][INFO] - Training Epoch: 1/2, step 690/7134 completed (loss: 0.17454107105731964, acc: 0.9570747017860413)
[2025-02-13 02:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:15][root][INFO] - Training Epoch: 1/2, step 691/7134 completed (loss: 0.1408659666776657, acc: 0.9640804529190063)
[2025-02-13 02:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:15][root][INFO] - Training Epoch: 1/2, step 692/7134 completed (loss: 0.16889409720897675, acc: 0.9506666660308838)
[2025-02-13 02:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16][root][INFO] - Training Epoch: 1/2, step 693/7134 completed (loss: 0.15065020322799683, acc: 0.9563812613487244)
[2025-02-13 02:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16][root][INFO] - Training Epoch: 1/2, step 694/7134 completed (loss: 0.1379646360874176, acc: 0.962043821811676)
[2025-02-13 02:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16][root][INFO] - Training Epoch: 1/2, step 695/7134 completed (loss: 0.16125614941120148, acc: 0.9621342420578003)
[2025-02-13 02:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:17][root][INFO] - Training Epoch: 1/2, step 696/7134 completed (loss: 0.22493615746498108, acc: 0.9417475461959839)
[2025-02-13 02:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:17][root][INFO] - Training Epoch: 1/2, step 697/7134 completed (loss: 0.12717774510383606, acc: 0.9702233076095581)
[2025-02-13 02:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:18][root][INFO] - Training Epoch: 1/2, step 698/7134 completed (loss: 0.12230758368968964, acc: 0.9641693830490112)
[2025-02-13 02:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:18][root][INFO] - Training Epoch: 1/2, step 699/7134 completed (loss: 0.0777633860707283, acc: 0.9781931638717651)
[2025-02-13 02:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19][root][INFO] - Training Epoch: 1/2, step 700/7134 completed (loss: 0.1532142460346222, acc: 0.9626865386962891)
[2025-02-13 02:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19][root][INFO] - Training Epoch: 1/2, step 701/7134 completed (loss: 0.13633884489536285, acc: 0.9592198729515076)
[2025-02-13 02:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19][root][INFO] - Training Epoch: 1/2, step 702/7134 completed (loss: 0.1179543063044548, acc: 0.9642857313156128)
[2025-02-13 02:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:20][root][INFO] - Training Epoch: 1/2, step 703/7134 completed (loss: 0.11984960734844208, acc: 0.9725363254547119)
[2025-02-13 02:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:20][root][INFO] - Training Epoch: 1/2, step 704/7134 completed (loss: 0.14661088585853577, acc: 0.9589665532112122)
[2025-02-13 02:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:21][root][INFO] - Training Epoch: 1/2, step 705/7134 completed (loss: 0.1950605809688568, acc: 0.9463686943054199)
[2025-02-13 02:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:21][root][INFO] - Training Epoch: 1/2, step 706/7134 completed (loss: 0.1447761356830597, acc: 0.9529540538787842)
[2025-02-13 02:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:22][root][INFO] - Training Epoch: 1/2, step 707/7134 completed (loss: 0.17390386760234833, acc: 0.9568345546722412)
[2025-02-13 02:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:22][root][INFO] - Training Epoch: 1/2, step 708/7134 completed (loss: 0.11856238543987274, acc: 0.9578686356544495)
[2025-02-13 02:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23][root][INFO] - Training Epoch: 1/2, step 709/7134 completed (loss: 0.13281990587711334, acc: 0.9606205224990845)
[2025-02-13 02:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23][root][INFO] - Training Epoch: 1/2, step 710/7134 completed (loss: 0.14590424299240112, acc: 0.962469756603241)
[2025-02-13 02:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23][root][INFO] - Training Epoch: 1/2, step 711/7134 completed (loss: 0.14904344081878662, acc: 0.9622195959091187)
[2025-02-13 02:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:24][root][INFO] - Training Epoch: 1/2, step 712/7134 completed (loss: 0.10162688791751862, acc: 0.9694749712944031)
[2025-02-13 02:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:24][root][INFO] - Training Epoch: 1/2, step 713/7134 completed (loss: 0.10132142901420593, acc: 0.9723183512687683)
[2025-02-13 02:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:25][root][INFO] - Training Epoch: 1/2, step 714/7134 completed (loss: 0.10149835050106049, acc: 0.97826087474823)
[2025-02-13 02:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:25][root][INFO] - Training Epoch: 1/2, step 715/7134 completed (loss: 0.11060740798711777, acc: 0.96875)
[2025-02-13 02:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:26][root][INFO] - Training Epoch: 1/2, step 716/7134 completed (loss: 0.1208280473947525, acc: 0.9635949730873108)
[2025-02-13 02:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:26][root][INFO] - Training Epoch: 1/2, step 717/7134 completed (loss: 0.12519949674606323, acc: 0.9633375406265259)
[2025-02-13 02:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27][root][INFO] - Training Epoch: 1/2, step 718/7134 completed (loss: 0.13739164173603058, acc: 0.9561403393745422)
[2025-02-13 02:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27][root][INFO] - Training Epoch: 1/2, step 719/7134 completed (loss: 0.07643646746873856, acc: 0.9741379022598267)
[2025-02-13 02:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27][root][INFO] - Training Epoch: 1/2, step 720/7134 completed (loss: 0.11023246496915817, acc: 0.9736841917037964)
[2025-02-13 02:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:28][root][INFO] - Training Epoch: 1/2, step 721/7134 completed (loss: 0.15956953167915344, acc: 0.9551856517791748)
[2025-02-13 02:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:28][root][INFO] - Training Epoch: 1/2, step 722/7134 completed (loss: 0.13152927160263062, acc: 0.9614936113357544)
[2025-02-13 02:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:29][root][INFO] - Training Epoch: 1/2, step 723/7134 completed (loss: 0.07898669689893723, acc: 0.9782082438468933)
[2025-02-13 02:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:29][root][INFO] - Training Epoch: 1/2, step 724/7134 completed (loss: 0.10639068484306335, acc: 0.9642346501350403)
[2025-02-13 02:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:30][root][INFO] - Training Epoch: 1/2, step 725/7134 completed (loss: 0.08782825618982315, acc: 0.9753979444503784)
[2025-02-13 02:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:30][root][INFO] - Training Epoch: 1/2, step 726/7134 completed (loss: 0.10432200133800507, acc: 0.9748954176902771)
[2025-02-13 02:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31][root][INFO] - Training Epoch: 1/2, step 727/7134 completed (loss: 0.11239630728960037, acc: 0.9696969985961914)
[2025-02-13 02:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31][root][INFO] - Training Epoch: 1/2, step 728/7134 completed (loss: 0.08111035078763962, acc: 0.9724518060684204)
[2025-02-13 02:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31][root][INFO] - Training Epoch: 1/2, step 729/7134 completed (loss: 0.10718607157468796, acc: 0.9652295112609863)
[2025-02-13 02:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:32][root][INFO] - Training Epoch: 1/2, step 730/7134 completed (loss: 0.11199803650379181, acc: 0.9675745964050293)
[2025-02-13 02:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:32][root][INFO] - Training Epoch: 1/2, step 731/7134 completed (loss: 0.08331437408924103, acc: 0.9866666793823242)
[2025-02-13 02:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:33][root][INFO] - Training Epoch: 1/2, step 732/7134 completed (loss: 0.22013737261295319, acc: 0.9452829957008362)
[2025-02-13 02:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:33][root][INFO] - Training Epoch: 1/2, step 733/7134 completed (loss: 0.1273488849401474, acc: 0.962699830532074)
[2025-02-13 02:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34][root][INFO] - Training Epoch: 1/2, step 734/7134 completed (loss: 0.11130916327238083, acc: 0.9723502397537231)
[2025-02-13 02:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34][root][INFO] - Training Epoch: 1/2, step 735/7134 completed (loss: 0.12290451675653458, acc: 0.9646302461624146)
[2025-02-13 02:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34][root][INFO] - Training Epoch: 1/2, step 736/7134 completed (loss: 0.18651288747787476, acc: 0.9472527503967285)
[2025-02-13 02:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:35][root][INFO] - Training Epoch: 1/2, step 737/7134 completed (loss: 0.25803765654563904, acc: 0.9350912570953369)
[2025-02-13 02:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:35][root][INFO] - Training Epoch: 1/2, step 738/7134 completed (loss: 0.07791446894407272, acc: 0.9750000238418579)
[2025-02-13 02:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36][root][INFO] - Training Epoch: 1/2, step 739/7134 completed (loss: 0.14844000339508057, acc: 0.9522935748100281)
[2025-02-13 02:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36][root][INFO] - Training Epoch: 1/2, step 740/7134 completed (loss: 0.12772250175476074, acc: 0.9539682269096375)
[2025-02-13 02:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36][root][INFO] - Training Epoch: 1/2, step 741/7134 completed (loss: 0.1618538349866867, acc: 0.9574132561683655)
[2025-02-13 02:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:37][root][INFO] - Training Epoch: 1/2, step 742/7134 completed (loss: 0.06661137193441391, acc: 0.9769392013549805)
[2025-02-13 02:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:37][root][INFO] - Training Epoch: 1/2, step 743/7134 completed (loss: 0.1504373848438263, acc: 0.9623494148254395)
[2025-02-13 02:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38][root][INFO] - Training Epoch: 1/2, step 744/7134 completed (loss: 0.16416007280349731, acc: 0.9563491940498352)
[2025-02-13 02:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38][root][INFO] - Training Epoch: 1/2, step 745/7134 completed (loss: 0.104719378054142, acc: 0.9682779312133789)
[2025-02-13 02:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38][root][INFO] - Training Epoch: 1/2, step 746/7134 completed (loss: 0.15200279653072357, acc: 0.9556650519371033)
[2025-02-13 02:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:39][root][INFO] - Training Epoch: 1/2, step 747/7134 completed (loss: 0.1890370100736618, acc: 0.945182740688324)
[2025-02-13 02:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:39][root][INFO] - Training Epoch: 1/2, step 748/7134 completed (loss: 0.1285121589899063, acc: 0.9563909769058228)
[2025-02-13 02:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:40][root][INFO] - Training Epoch: 1/2, step 749/7134 completed (loss: 0.08191518485546112, acc: 0.9755043387413025)
[2025-02-13 02:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:40][root][INFO] - Training Epoch: 1/2, step 750/7134 completed (loss: 0.12598419189453125, acc: 0.9747747778892517)
[2025-02-13 02:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41][root][INFO] - Training Epoch: 1/2, step 751/7134 completed (loss: 0.1265607476234436, acc: 0.9642248749732971)
[2025-02-13 02:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41][root][INFO] - Training Epoch: 1/2, step 752/7134 completed (loss: 0.1012648269534111, acc: 0.9819819927215576)
[2025-02-13 02:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41][root][INFO] - Training Epoch: 1/2, step 753/7134 completed (loss: 0.1287556141614914, acc: 0.9645270109176636)
[2025-02-13 02:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:42][root][INFO] - Training Epoch: 1/2, step 754/7134 completed (loss: 0.10050725191831589, acc: 0.9713423848152161)
[2025-02-13 02:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:42][root][INFO] - Training Epoch: 1/2, step 755/7134 completed (loss: 0.1789141744375229, acc: 0.9570956826210022)
[2025-02-13 02:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43][root][INFO] - Training Epoch: 1/2, step 756/7134 completed (loss: 0.12628495693206787, acc: 0.9700934290885925)
[2025-02-13 02:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43][root][INFO] - Training Epoch: 1/2, step 757/7134 completed (loss: 0.10716409236192703, acc: 0.975095808506012)
[2025-02-13 02:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43][root][INFO] - Training Epoch: 1/2, step 758/7134 completed (loss: 0.15563957393169403, acc: 0.9586918950080872)
[2025-02-13 02:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:44][root][INFO] - Training Epoch: 1/2, step 759/7134 completed (loss: 0.13686950504779816, acc: 0.961240291595459)
[2025-02-13 02:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:44][root][INFO] - Training Epoch: 1/2, step 760/7134 completed (loss: 0.0759148895740509, acc: 0.9792027473449707)
[2025-02-13 02:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45][root][INFO] - Training Epoch: 1/2, step 761/7134 completed (loss: 0.09170505404472351, acc: 0.9749608635902405)
[2025-02-13 02:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45][root][INFO] - Training Epoch: 1/2, step 762/7134 completed (loss: 0.0827174112200737, acc: 0.9820788502693176)
[2025-02-13 02:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45][root][INFO] - Training Epoch: 1/2, step 763/7134 completed (loss: 0.11781379580497742, acc: 0.9706840515136719)
[2025-02-13 02:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:46][root][INFO] - Training Epoch: 1/2, step 764/7134 completed (loss: 0.06288761645555496, acc: 0.9799138903617859)
[2025-02-13 02:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:46][root][INFO] - Training Epoch: 1/2, step 765/7134 completed (loss: 0.10351467877626419, acc: 0.9677891731262207)
[2025-02-13 02:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:47][root][INFO] - Training Epoch: 1/2, step 766/7134 completed (loss: 0.08072856068611145, acc: 0.9777448177337646)
[2025-02-13 02:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:47][root][INFO] - Training Epoch: 1/2, step 767/7134 completed (loss: 0.08726229518651962, acc: 0.9692533016204834)
[2025-02-13 02:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48][root][INFO] - Training Epoch: 1/2, step 768/7134 completed (loss: 0.13185438513755798, acc: 0.9688385128974915)
[2025-02-13 02:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48][root][INFO] - Training Epoch: 1/2, step 769/7134 completed (loss: 0.0787578597664833, acc: 0.984240710735321)
[2025-02-13 02:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48][root][INFO] - Training Epoch: 1/2, step 770/7134 completed (loss: 0.11348044127225876, acc: 0.972434937953949)
[2025-02-13 02:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:49][root][INFO] - Training Epoch: 1/2, step 771/7134 completed (loss: 0.10397060960531235, acc: 0.9762375950813293)
[2025-02-13 02:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:49][root][INFO] - Training Epoch: 1/2, step 772/7134 completed (loss: 0.0762827917933464, acc: 0.9765493869781494)
[2025-02-13 02:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50][root][INFO] - Training Epoch: 1/2, step 773/7134 completed (loss: 0.11674454063177109, acc: 0.9678663015365601)
[2025-02-13 02:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50][root][INFO] - Training Epoch: 1/2, step 774/7134 completed (loss: 0.06304585188627243, acc: 0.985049843788147)
[2025-02-13 02:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50][root][INFO] - Training Epoch: 1/2, step 775/7134 completed (loss: 0.08389075845479965, acc: 0.9800266027450562)
[2025-02-13 02:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:51][root][INFO] - Training Epoch: 1/2, step 776/7134 completed (loss: 0.07194717973470688, acc: 0.9778106212615967)
[2025-02-13 02:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:51][root][INFO] - Training Epoch: 1/2, step 777/7134 completed (loss: 0.07156296819448471, acc: 0.9855538010597229)
[2025-02-13 02:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:52][root][INFO] - Training Epoch: 1/2, step 778/7134 completed (loss: 0.11310698091983795, acc: 0.9711538553237915)
[2025-02-13 02:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:52][root][INFO] - Training Epoch: 1/2, step 779/7134 completed (loss: 0.17514602839946747, acc: 0.9532846808433533)
[2025-02-13 02:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:53][root][INFO] - Training Epoch: 1/2, step 780/7134 completed (loss: 0.20306526124477386, acc: 0.9505703449249268)
[2025-02-13 02:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:53][root][INFO] - Training Epoch: 1/2, step 781/7134 completed (loss: 0.1909898966550827, acc: 0.9389499425888062)
[2025-02-13 02:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54][root][INFO] - Training Epoch: 1/2, step 782/7134 completed (loss: 0.19065943360328674, acc: 0.9477847814559937)
[2025-02-13 02:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54][root][INFO] - Training Epoch: 1/2, step 783/7134 completed (loss: 0.2583535611629486, acc: 0.9273885488510132)
[2025-02-13 02:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54][root][INFO] - Training Epoch: 1/2, step 784/7134 completed (loss: 0.17723602056503296, acc: 0.9476248621940613)
[2025-02-13 02:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:55][root][INFO] - Training Epoch: 1/2, step 785/7134 completed (loss: 0.16549423336982727, acc: 0.953177273273468)
[2025-02-13 02:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:55][root][INFO] - Training Epoch: 1/2, step 786/7134 completed (loss: 0.1212063580751419, acc: 0.9702602028846741)
[2025-02-13 02:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56][root][INFO] - Training Epoch: 1/2, step 787/7134 completed (loss: 0.11887949705123901, acc: 0.9689781069755554)
[2025-02-13 02:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56][root][INFO] - Training Epoch: 1/2, step 788/7134 completed (loss: 0.18242427706718445, acc: 0.9515418410301208)
[2025-02-13 02:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56][root][INFO] - Training Epoch: 1/2, step 789/7134 completed (loss: 0.11940784007310867, acc: 0.9679389595985413)
[2025-02-13 02:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:57][root][INFO] - Training Epoch: 1/2, step 790/7134 completed (loss: 0.19012588262557983, acc: 0.9452662467956543)
[2025-02-13 02:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:57][root][INFO] - Training Epoch: 1/2, step 791/7134 completed (loss: 0.11040020734071732, acc: 0.97826087474823)
[2025-02-13 02:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:58][root][INFO] - Training Epoch: 1/2, step 792/7134 completed (loss: 0.10808549076318741, acc: 0.9721792936325073)
[2025-02-13 02:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:58][root][INFO] - Training Epoch: 1/2, step 793/7134 completed (loss: 0.07846115529537201, acc: 0.9792899489402771)
[2025-02-13 02:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59][root][INFO] - Training Epoch: 1/2, step 794/7134 completed (loss: 0.081670843064785, acc: 0.9813519716262817)
[2025-02-13 02:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59][root][INFO] - Training Epoch: 1/2, step 795/7134 completed (loss: 0.16834400594234467, acc: 0.9595015645027161)
[2025-02-13 02:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59][root][INFO] - Training Epoch: 1/2, step 796/7134 completed (loss: 0.18758787214756012, acc: 0.9520202279090881)
[2025-02-13 02:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00][root][INFO] - Training Epoch: 1/2, step 797/7134 completed (loss: 0.13188254833221436, acc: 0.9738956093788147)
[2025-02-13 02:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00][root][INFO] - Training Epoch: 1/2, step 798/7134 completed (loss: 0.0679364800453186, acc: 0.9867724776268005)
[2025-02-13 02:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00][root][INFO] - Training Epoch: 1/2, step 799/7134 completed (loss: 0.10260210931301117, acc: 0.9741935729980469)
[2025-02-13 02:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:01][root][INFO] - Training Epoch: 1/2, step 800/7134 completed (loss: 0.15012586116790771, acc: 0.9590316414833069)
[2025-02-13 02:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:01][root][INFO] - Training Epoch: 1/2, step 801/7134 completed (loss: 0.17178967595100403, acc: 0.9567999839782715)
[2025-02-13 02:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02][root][INFO] - Training Epoch: 1/2, step 802/7134 completed (loss: 0.09883374720811844, acc: 0.967793881893158)
[2025-02-13 02:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02][root][INFO] - Training Epoch: 1/2, step 803/7134 completed (loss: 0.12544035911560059, acc: 0.9624999761581421)
[2025-02-13 02:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02][root][INFO] - Training Epoch: 1/2, step 804/7134 completed (loss: 0.14904430508613586, acc: 0.951120138168335)
[2025-02-13 02:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:03][root][INFO] - Training Epoch: 1/2, step 805/7134 completed (loss: 0.1386815905570984, acc: 0.9531835317611694)
[2025-02-13 02:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:03][root][INFO] - Training Epoch: 1/2, step 806/7134 completed (loss: 0.11659277975559235, acc: 0.9664804339408875)
[2025-02-13 02:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:04][root][INFO] - Training Epoch: 1/2, step 807/7134 completed (loss: 0.15405818819999695, acc: 0.9622871279716492)
[2025-02-13 02:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:04][root][INFO] - Training Epoch: 1/2, step 808/7134 completed (loss: 0.09449470043182373, acc: 0.9712575078010559)
[2025-02-13 02:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05][root][INFO] - Training Epoch: 1/2, step 809/7134 completed (loss: 0.09728958457708359, acc: 0.9717682003974915)
[2025-02-13 02:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05][root][INFO] - Training Epoch: 1/2, step 810/7134 completed (loss: 0.11803476512432098, acc: 0.973009467124939)
[2025-02-13 02:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05][root][INFO] - Training Epoch: 1/2, step 811/7134 completed (loss: 0.1559457778930664, acc: 0.9666081070899963)
[2025-02-13 02:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:06][root][INFO] - Training Epoch: 1/2, step 812/7134 completed (loss: 0.13056546449661255, acc: 0.9672130942344666)
[2025-02-13 02:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:06][root][INFO] - Training Epoch: 1/2, step 813/7134 completed (loss: 0.1383989155292511, acc: 0.9666182994842529)
[2025-02-13 02:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:07][root][INFO] - Training Epoch: 1/2, step 814/7134 completed (loss: 0.12804727256298065, acc: 0.9651612639427185)
[2025-02-13 02:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:07][root][INFO] - Training Epoch: 1/2, step 815/7134 completed (loss: 0.1302523910999298, acc: 0.9659969210624695)
[2025-02-13 02:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:08][root][INFO] - Training Epoch: 1/2, step 816/7134 completed (loss: 0.14227284491062164, acc: 0.9610169529914856)
[2025-02-13 02:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:08][root][INFO] - Training Epoch: 1/2, step 817/7134 completed (loss: 0.1332591027021408, acc: 0.9634369015693665)
[2025-02-13 02:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09][root][INFO] - Training Epoch: 1/2, step 818/7134 completed (loss: 0.19679555296897888, acc: 0.9432989954948425)
[2025-02-13 02:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09][root][INFO] - Training Epoch: 1/2, step 819/7134 completed (loss: 0.11356227844953537, acc: 0.9760000109672546)
[2025-02-13 02:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09][root][INFO] - Training Epoch: 1/2, step 820/7134 completed (loss: 0.10292010754346848, acc: 0.9675425291061401)
[2025-02-13 02:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:10][root][INFO] - Training Epoch: 1/2, step 821/7134 completed (loss: 0.08249888569116592, acc: 0.9772117733955383)
[2025-02-13 02:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:10][root][INFO] - Training Epoch: 1/2, step 822/7134 completed (loss: 0.0969175472855568, acc: 0.9701727032661438)
[2025-02-13 02:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:11][root][INFO] - Training Epoch: 1/2, step 823/7134 completed (loss: 0.08367960900068283, acc: 0.9793621301651001)
[2025-02-13 02:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:11][root][INFO] - Training Epoch: 1/2, step 824/7134 completed (loss: 0.1234937459230423, acc: 0.962897539138794)
[2025-02-13 02:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:12][root][INFO] - Training Epoch: 1/2, step 825/7134 completed (loss: 0.11745449155569077, acc: 0.9670619368553162)
[2025-02-13 02:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:12][root][INFO] - Training Epoch: 1/2, step 826/7134 completed (loss: 0.10995505005121231, acc: 0.968664824962616)
[2025-02-13 02:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13][root][INFO] - Training Epoch: 1/2, step 827/7134 completed (loss: 0.0971132218837738, acc: 0.9711285829544067)
[2025-02-13 02:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13][root][INFO] - Training Epoch: 1/2, step 828/7134 completed (loss: 0.09087312966585159, acc: 0.9735614061355591)
[2025-02-13 02:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13][root][INFO] - Training Epoch: 1/2, step 829/7134 completed (loss: 0.1235969215631485, acc: 0.9611650705337524)
[2025-02-13 02:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:14][root][INFO] - Training Epoch: 1/2, step 830/7134 completed (loss: 0.10367273539304733, acc: 0.9717608094215393)
[2025-02-13 02:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:14][root][INFO] - Training Epoch: 1/2, step 831/7134 completed (loss: 0.07471539825201035, acc: 0.9739726185798645)
[2025-02-13 02:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:15][root][INFO] - Training Epoch: 1/2, step 832/7134 completed (loss: 0.14531220495700836, acc: 0.9692832827568054)
[2025-02-13 02:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:15][root][INFO] - Training Epoch: 1/2, step 833/7134 completed (loss: 0.11418469250202179, acc: 0.970588207244873)
[2025-02-13 02:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16][root][INFO] - Training Epoch: 1/2, step 834/7134 completed (loss: 0.05666124448180199, acc: 0.988811194896698)
[2025-02-13 02:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16][root][INFO] - Training Epoch: 1/2, step 835/7134 completed (loss: 0.11056654900312424, acc: 0.9706293940544128)
[2025-02-13 02:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16][root][INFO] - Training Epoch: 1/2, step 836/7134 completed (loss: 0.11505588889122009, acc: 0.9675173759460449)
[2025-02-13 02:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:17][root][INFO] - Training Epoch: 1/2, step 837/7134 completed (loss: 0.09151285886764526, acc: 0.9768392443656921)
[2025-02-13 02:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:17][root][INFO] - Training Epoch: 1/2, step 838/7134 completed (loss: 0.09462286531925201, acc: 0.9745628237724304)
[2025-02-13 02:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:18][root][INFO] - Training Epoch: 1/2, step 839/7134 completed (loss: 0.10218462347984314, acc: 0.9688995480537415)
[2025-02-13 02:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:18][root][INFO] - Training Epoch: 1/2, step 840/7134 completed (loss: 0.0791681557893753, acc: 0.9776847958564758)
[2025-02-13 02:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:19][root][INFO] - Training Epoch: 1/2, step 841/7134 completed (loss: 0.18451161682605743, acc: 0.9612724781036377)
[2025-02-13 02:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:19][root][INFO] - Training Epoch: 1/2, step 842/7134 completed (loss: 0.1194210946559906, acc: 0.9690027236938477)
[2025-02-13 02:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20][root][INFO] - Training Epoch: 1/2, step 843/7134 completed (loss: 0.09048774838447571, acc: 0.9735743999481201)
[2025-02-13 02:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20][root][INFO] - Training Epoch: 1/2, step 844/7134 completed (loss: 0.08193913102149963, acc: 0.9747235178947449)
[2025-02-13 02:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20][root][INFO] - Training Epoch: 1/2, step 845/7134 completed (loss: 0.14527764916419983, acc: 0.9646017551422119)
[2025-02-13 02:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:21][root][INFO] - Training Epoch: 1/2, step 846/7134 completed (loss: 0.1334753930568695, acc: 0.9688427448272705)
[2025-02-13 02:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:21][root][INFO] - Training Epoch: 1/2, step 847/7134 completed (loss: 0.11040452867746353, acc: 0.9590268731117249)
[2025-02-13 02:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:22][root][INFO] - Training Epoch: 1/2, step 848/7134 completed (loss: 0.15482231974601746, acc: 0.955500602722168)
[2025-02-13 02:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:22][root][INFO] - Training Epoch: 1/2, step 849/7134 completed (loss: 0.10827169567346573, acc: 0.9673518538475037)
[2025-02-13 02:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23][root][INFO] - Training Epoch: 1/2, step 850/7134 completed (loss: 0.10078917443752289, acc: 0.9664429426193237)
[2025-02-13 02:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23][root][INFO] - Training Epoch: 1/2, step 851/7134 completed (loss: 0.080259770154953, acc: 0.980289101600647)
[2025-02-13 02:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24][root][INFO] - Training Epoch: 1/2, step 852/7134 completed (loss: 0.09902027249336243, acc: 0.9694749712944031)
[2025-02-13 02:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24][root][INFO] - Training Epoch: 1/2, step 853/7134 completed (loss: 0.08997122198343277, acc: 0.965859055519104)
[2025-02-13 02:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24][root][INFO] - Training Epoch: 1/2, step 854/7134 completed (loss: 0.14775146543979645, acc: 0.9577114582061768)
[2025-02-13 02:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:25][root][INFO] - Training Epoch: 1/2, step 855/7134 completed (loss: 0.155244380235672, acc: 0.9613583087921143)
[2025-02-13 02:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:25][root][INFO] - Training Epoch: 1/2, step 856/7134 completed (loss: 0.11619456857442856, acc: 0.9685230255126953)
[2025-02-13 02:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:26][root][INFO] - Training Epoch: 1/2, step 857/7134 completed (loss: 0.1355660855770111, acc: 0.9647576808929443)
[2025-02-13 02:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:26][root][INFO] - Training Epoch: 1/2, step 858/7134 completed (loss: 0.1186976283788681, acc: 0.9676616787910461)
[2025-02-13 02:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:27][root][INFO] - Training Epoch: 1/2, step 859/7134 completed (loss: 0.19684374332427979, acc: 0.9383886456489563)
[2025-02-13 02:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:27][root][INFO] - Training Epoch: 1/2, step 860/7134 completed (loss: 0.15689484775066376, acc: 0.9494311213493347)
[2025-02-13 02:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:28][root][INFO] - Training Epoch: 1/2, step 861/7134 completed (loss: 0.15904037654399872, acc: 0.9577465057373047)
[2025-02-13 02:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:28][root][INFO] - Training Epoch: 1/2, step 862/7134 completed (loss: 0.17406794428825378, acc: 0.950661838054657)
[2025-02-13 02:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29][root][INFO] - Training Epoch: 1/2, step 863/7134 completed (loss: 0.15866641700267792, acc: 0.9534342288970947)
[2025-02-13 02:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29][root][INFO] - Training Epoch: 1/2, step 864/7134 completed (loss: 0.15958364307880402, acc: 0.9539822936058044)
[2025-02-13 02:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29][root][INFO] - Training Epoch: 1/2, step 865/7134 completed (loss: 0.18379178643226624, acc: 0.956204354763031)
[2025-02-13 02:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:30][root][INFO] - Training Epoch: 1/2, step 866/7134 completed (loss: 0.20201852917671204, acc: 0.9407407641410828)
[2025-02-13 02:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:30][root][INFO] - Training Epoch: 1/2, step 867/7134 completed (loss: 0.21939460933208466, acc: 0.9474343061447144)
[2025-02-13 02:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:31][root][INFO] - Training Epoch: 1/2, step 868/7134 completed (loss: 0.18008233606815338, acc: 0.9502617716789246)
[2025-02-13 02:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:31][root][INFO] - Training Epoch: 1/2, step 869/7134 completed (loss: 0.2807022035121918, acc: 0.9299362897872925)
[2025-02-13 02:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32][root][INFO] - Training Epoch: 1/2, step 870/7134 completed (loss: 0.1253785938024521, acc: 0.9676550030708313)
[2025-02-13 02:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32][root][INFO] - Training Epoch: 1/2, step 871/7134 completed (loss: 0.17133736610412598, acc: 0.9592834115028381)
[2025-02-13 02:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32][root][INFO] - Training Epoch: 1/2, step 872/7134 completed (loss: 0.09903990477323532, acc: 0.9733123779296875)
[2025-02-13 02:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:33][root][INFO] - Training Epoch: 1/2, step 873/7134 completed (loss: 0.16617292165756226, acc: 0.9535211324691772)
[2025-02-13 02:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:33][root][INFO] - Training Epoch: 1/2, step 874/7134 completed (loss: 0.1747477799654007, acc: 0.9522342085838318)
[2025-02-13 02:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:34][root][INFO] - Training Epoch: 1/2, step 875/7134 completed (loss: 0.18007564544677734, acc: 0.9502196311950684)
[2025-02-13 02:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:34][root][INFO] - Training Epoch: 1/2, step 876/7134 completed (loss: 0.14285415410995483, acc: 0.9590643048286438)
[2025-02-13 02:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35][root][INFO] - Training Epoch: 1/2, step 877/7134 completed (loss: 0.2161562442779541, acc: 0.9443708658218384)
[2025-02-13 02:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35][root][INFO] - Training Epoch: 1/2, step 878/7134 completed (loss: 0.17552591860294342, acc: 0.9522546529769897)
[2025-02-13 02:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35][root][INFO] - Training Epoch: 1/2, step 879/7134 completed (loss: 0.14024990797042847, acc: 0.9636098742485046)
[2025-02-13 02:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:36][root][INFO] - Training Epoch: 1/2, step 880/7134 completed (loss: 0.06283474713563919, acc: 0.9883551597595215)
[2025-02-13 02:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:36][root][INFO] - Training Epoch: 1/2, step 881/7134 completed (loss: 0.07120407372713089, acc: 0.9799599051475525)
[2025-02-13 02:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:37][root][INFO] - Training Epoch: 1/2, step 882/7134 completed (loss: 0.12257207930088043, acc: 0.9694244861602783)
[2025-02-13 02:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:37][root][INFO] - Training Epoch: 1/2, step 883/7134 completed (loss: 0.2111474573612213, acc: 0.9403669834136963)
[2025-02-13 02:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38][root][INFO] - Training Epoch: 1/2, step 884/7134 completed (loss: 0.12913647294044495, acc: 0.9557640552520752)
[2025-02-13 02:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38][root][INFO] - Training Epoch: 1/2, step 885/7134 completed (loss: 0.10602988302707672, acc: 0.966292142868042)
[2025-02-13 02:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38][root][INFO] - Training Epoch: 1/2, step 886/7134 completed (loss: 0.16003452241420746, acc: 0.9415204524993896)
[2025-02-13 02:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:39][root][INFO] - Training Epoch: 1/2, step 887/7134 completed (loss: 0.11591088026762009, acc: 0.9628318548202515)
[2025-02-13 02:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:39][root][INFO] - Training Epoch: 1/2, step 888/7134 completed (loss: 0.16108644008636475, acc: 0.9575113654136658)
[2025-02-13 02:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40][root][INFO] - Training Epoch: 1/2, step 889/7134 completed (loss: 0.0794018805027008, acc: 0.9733333587646484)
[2025-02-13 02:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40][root][INFO] - Training Epoch: 1/2, step 890/7134 completed (loss: 0.10710105299949646, acc: 0.971137523651123)
[2025-02-13 02:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40][root][INFO] - Training Epoch: 1/2, step 891/7134 completed (loss: 0.11698678135871887, acc: 0.9805285334587097)
[2025-02-13 02:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:41][root][INFO] - Training Epoch: 1/2, step 892/7134 completed (loss: 0.11040998995304108, acc: 0.9738371968269348)
[2025-02-13 02:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:41][root][INFO] - Training Epoch: 1/2, step 893/7134 completed (loss: 0.08657483011484146, acc: 0.9767801761627197)
[2025-02-13 02:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:42][root][INFO] - Training Epoch: 1/2, step 894/7134 completed (loss: 0.15028469264507294, acc: 0.9622905254364014)
[2025-02-13 02:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:42][root][INFO] - Training Epoch: 1/2, step 895/7134 completed (loss: 0.08317650854587555, acc: 0.9811965823173523)
[2025-02-13 02:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43][root][INFO] - Training Epoch: 1/2, step 896/7134 completed (loss: 0.12156613171100616, acc: 0.9660000205039978)
[2025-02-13 02:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43][root][INFO] - Training Epoch: 1/2, step 897/7134 completed (loss: 0.14781831204891205, acc: 0.9596100449562073)
[2025-02-13 02:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43][root][INFO] - Training Epoch: 1/2, step 898/7134 completed (loss: 0.08831627666950226, acc: 0.9701120853424072)
[2025-02-13 02:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:44][root][INFO] - Training Epoch: 1/2, step 899/7134 completed (loss: 0.11081955581903458, acc: 0.9738805890083313)
[2025-02-13 02:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:44][root][INFO] - Training Epoch: 1/2, step 900/7134 completed (loss: 0.0955699235200882, acc: 0.9690576791763306)
[2025-02-13 02:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:45][root][INFO] - Training Epoch: 1/2, step 901/7134 completed (loss: 0.11496283113956451, acc: 0.9679999947547913)
[2025-02-13 02:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:45][root][INFO] - Training Epoch: 1/2, step 902/7134 completed (loss: 0.05475340783596039, acc: 0.9813486337661743)
[2025-02-13 02:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:46][root][INFO] - Training Epoch: 1/2, step 903/7134 completed (loss: 0.05760524794459343, acc: 0.9775429368019104)
[2025-02-13 02:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:46][root][INFO] - Training Epoch: 1/2, step 904/7134 completed (loss: 0.0868559256196022, acc: 0.9778645634651184)
[2025-02-13 02:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47][root][INFO] - Training Epoch: 1/2, step 905/7134 completed (loss: 0.07657809555530548, acc: 0.9770889282226562)
[2025-02-13 02:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47][root][INFO] - Training Epoch: 1/2, step 906/7134 completed (loss: 0.09209534525871277, acc: 0.9772117733955383)
[2025-02-13 02:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47][root][INFO] - Training Epoch: 1/2, step 907/7134 completed (loss: 0.0769738182425499, acc: 0.9789473414421082)
[2025-02-13 02:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:48][root][INFO] - Training Epoch: 1/2, step 908/7134 completed (loss: 0.07796357572078705, acc: 0.9739921689033508)
[2025-02-13 02:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:48][root][INFO] - Training Epoch: 1/2, step 909/7134 completed (loss: 0.09243188053369522, acc: 0.9714714884757996)
[2025-02-13 02:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:49][root][INFO] - Training Epoch: 1/2, step 910/7134 completed (loss: 0.11030620336532593, acc: 0.9639519453048706)
[2025-02-13 02:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:49][root][INFO] - Training Epoch: 1/2, step 911/7134 completed (loss: 0.11007259041070938, acc: 0.9692307710647583)
[2025-02-13 02:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:50][root][INFO] - Training Epoch: 1/2, step 912/7134 completed (loss: 0.1127709150314331, acc: 0.9659520983695984)
[2025-02-13 02:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:50][root][INFO] - Training Epoch: 1/2, step 913/7134 completed (loss: 0.06730371713638306, acc: 0.983668327331543)
[2025-02-13 02:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51][root][INFO] - Training Epoch: 1/2, step 914/7134 completed (loss: 0.0764317512512207, acc: 0.980867326259613)
[2025-02-13 02:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51][root][INFO] - Training Epoch: 1/2, step 915/7134 completed (loss: 0.16573460400104523, acc: 0.9579287767410278)
[2025-02-13 02:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51][root][INFO] - Training Epoch: 1/2, step 916/7134 completed (loss: 0.11168576776981354, acc: 0.9650959968566895)
[2025-02-13 02:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:52][root][INFO] - Training Epoch: 1/2, step 917/7134 completed (loss: 0.11938349902629852, acc: 0.9698340892791748)
[2025-02-13 02:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:52][root][INFO] - Training Epoch: 1/2, step 918/7134 completed (loss: 0.05449919030070305, acc: 0.9770290851593018)
[2025-02-13 02:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53][root][INFO] - Training Epoch: 1/2, step 919/7134 completed (loss: 0.05847703665494919, acc: 0.9797979593276978)
[2025-02-13 02:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53][root][INFO] - Training Epoch: 1/2, step 920/7134 completed (loss: 0.10171390324831009, acc: 0.9677870869636536)
[2025-02-13 02:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53][root][INFO] - Training Epoch: 1/2, step 921/7134 completed (loss: 0.08630027621984482, acc: 0.9742547273635864)
[2025-02-13 02:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:54][root][INFO] - Training Epoch: 1/2, step 922/7134 completed (loss: 0.13267794251441956, acc: 0.9672130942344666)
[2025-02-13 02:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:54][root][INFO] - Training Epoch: 1/2, step 923/7134 completed (loss: 0.10050061345100403, acc: 0.9656084775924683)
[2025-02-13 02:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:55][root][INFO] - Training Epoch: 1/2, step 924/7134 completed (loss: 0.07110611349344254, acc: 0.979468584060669)
[2025-02-13 02:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:55][root][INFO] - Training Epoch: 1/2, step 925/7134 completed (loss: 0.11587096750736237, acc: 0.9720998406410217)
[2025-02-13 02:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:56][root][INFO] - Training Epoch: 1/2, step 926/7134 completed (loss: 0.10484693199396133, acc: 0.9812734127044678)
[2025-02-13 02:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:56][root][INFO] - Training Epoch: 1/2, step 927/7134 completed (loss: 0.1053459420800209, acc: 0.9771241545677185)
[2025-02-13 02:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57][root][INFO] - Training Epoch: 1/2, step 928/7134 completed (loss: 0.12181907147169113, acc: 0.9723865985870361)
[2025-02-13 02:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57][root][INFO] - Training Epoch: 1/2, step 929/7134 completed (loss: 0.052820004522800446, acc: 0.9905213117599487)
[2025-02-13 02:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57][root][INFO] - Training Epoch: 1/2, step 930/7134 completed (loss: 0.11937203258275986, acc: 0.9657257795333862)
[2025-02-13 02:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:58][root][INFO] - Training Epoch: 1/2, step 931/7134 completed (loss: 0.10914116352796555, acc: 0.971781313419342)
[2025-02-13 02:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:58][root][INFO] - Training Epoch: 1/2, step 932/7134 completed (loss: 0.08264728635549545, acc: 0.972779393196106)
[2025-02-13 02:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59][root][INFO] - Training Epoch: 1/2, step 933/7134 completed (loss: 0.12302377074956894, acc: 0.9619181752204895)
[2025-02-13 02:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59][root][INFO] - Training Epoch: 1/2, step 934/7134 completed (loss: 0.07023852318525314, acc: 0.9803664684295654)
[2025-02-13 02:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59][root][INFO] - Training Epoch: 1/2, step 935/7134 completed (loss: 0.07096944004297256, acc: 0.9766277074813843)
[2025-02-13 02:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:00][root][INFO] - Training Epoch: 1/2, step 936/7134 completed (loss: 0.12191425263881683, acc: 0.9701230525970459)
[2025-02-13 02:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:00][root][INFO] - Training Epoch: 1/2, step 937/7134 completed (loss: 0.0973881259560585, acc: 0.9683098793029785)
[2025-02-13 02:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:01][root][INFO] - Training Epoch: 1/2, step 938/7134 completed (loss: 0.09103730320930481, acc: 0.9756447076797485)
[2025-02-13 02:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:01][root][INFO] - Training Epoch: 1/2, step 939/7134 completed (loss: 0.1079067811369896, acc: 0.9704049825668335)
[2025-02-13 02:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:02][root][INFO] - Training Epoch: 1/2, step 940/7134 completed (loss: 0.11367971450090408, acc: 0.9656652212142944)
[2025-02-13 02:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:02][root][INFO] - Training Epoch: 1/2, step 941/7134 completed (loss: 0.09359337389469147, acc: 0.9724137783050537)
[2025-02-13 02:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03][root][INFO] - Training Epoch: 1/2, step 942/7134 completed (loss: 0.09822767972946167, acc: 0.9782293438911438)
[2025-02-13 02:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03][root][INFO] - Training Epoch: 1/2, step 943/7134 completed (loss: 0.0700119212269783, acc: 0.9802131056785583)
[2025-02-13 02:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03][root][INFO] - Training Epoch: 1/2, step 944/7134 completed (loss: 0.08818406611680984, acc: 0.9760119915008545)
[2025-02-13 02:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:04][root][INFO] - Training Epoch: 1/2, step 945/7134 completed (loss: 0.07774241268634796, acc: 0.988811194896698)
[2025-02-13 02:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:04][root][INFO] - Training Epoch: 1/2, step 946/7134 completed (loss: 0.05193105712532997, acc: 0.9871520400047302)
[2025-02-13 02:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05][root][INFO] - Training Epoch: 1/2, step 947/7134 completed (loss: 0.08022872358560562, acc: 0.9810671210289001)
[2025-02-13 02:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05][root][INFO] - Training Epoch: 1/2, step 948/7134 completed (loss: 0.0994715467095375, acc: 0.9735743999481201)
[2025-02-13 02:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05][root][INFO] - Training Epoch: 1/2, step 949/7134 completed (loss: 0.07777318358421326, acc: 0.9817671775817871)
[2025-02-13 02:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:06][root][INFO] - Training Epoch: 1/2, step 950/7134 completed (loss: 0.08631376177072525, acc: 0.979938268661499)
[2025-02-13 02:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:06][root][INFO] - Training Epoch: 1/2, step 951/7134 completed (loss: 0.08714901655912399, acc: 0.9788732528686523)
[2025-02-13 02:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:07][root][INFO] - Training Epoch: 1/2, step 952/7134 completed (loss: 0.12603379786014557, acc: 0.960066556930542)
[2025-02-13 02:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:07][root][INFO] - Training Epoch: 1/2, step 953/7134 completed (loss: 0.17672249674797058, acc: 0.9509202241897583)
[2025-02-13 02:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08][root][INFO] - Training Epoch: 1/2, step 954/7134 completed (loss: 0.1078190952539444, acc: 0.9676945805549622)
[2025-02-13 02:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08][root][INFO] - Training Epoch: 1/2, step 955/7134 completed (loss: 0.09324607253074646, acc: 0.9772422909736633)
[2025-02-13 02:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09][root][INFO] - Training Epoch: 1/2, step 956/7134 completed (loss: 0.09764984995126724, acc: 0.9670731425285339)
[2025-02-13 02:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09][root][INFO] - Training Epoch: 1/2, step 957/7134 completed (loss: 0.07688336819410324, acc: 0.977246880531311)
[2025-02-13 02:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09][root][INFO] - Training Epoch: 1/2, step 958/7134 completed (loss: 0.188791885972023, acc: 0.9503676295280457)
[2025-02-13 02:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:10][root][INFO] - Training Epoch: 1/2, step 959/7134 completed (loss: 0.21921297907829285, acc: 0.9465240836143494)
[2025-02-13 02:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:10][root][INFO] - Training Epoch: 1/2, step 960/7134 completed (loss: 0.2053368240594864, acc: 0.9470699429512024)
[2025-02-13 02:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:11][root][INFO] - Training Epoch: 1/2, step 961/7134 completed (loss: 0.1403406262397766, acc: 0.9636118412017822)
[2025-02-13 02:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:11][root][INFO] - Training Epoch: 1/2, step 962/7134 completed (loss: 0.20842352509498596, acc: 0.9558620452880859)
[2025-02-13 02:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12][root][INFO] - Training Epoch: 1/2, step 963/7134 completed (loss: 0.15088219940662384, acc: 0.9634146094322205)
[2025-02-13 02:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12][root][INFO] - Training Epoch: 1/2, step 964/7134 completed (loss: 0.24936170876026154, acc: 0.9460269808769226)
[2025-02-13 02:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12][root][INFO] - Training Epoch: 1/2, step 965/7134 completed (loss: 0.20287497341632843, acc: 0.9569584131240845)
[2025-02-13 02:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:13][root][INFO] - Training Epoch: 1/2, step 966/7134 completed (loss: 0.11155959218740463, acc: 0.965573787689209)
[2025-02-13 02:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:13][root][INFO] - Training Epoch: 1/2, step 967/7134 completed (loss: 0.14823195338249207, acc: 0.9590722918510437)
[2025-02-13 02:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14][root][INFO] - Training Epoch: 1/2, step 968/7134 completed (loss: 0.23085615038871765, acc: 0.9443339705467224)
[2025-02-13 02:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14][root][INFO] - Training Epoch: 1/2, step 969/7134 completed (loss: 0.14381703734397888, acc: 0.965309202671051)
[2025-02-13 02:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14][root][INFO] - Training Epoch: 1/2, step 970/7134 completed (loss: 0.2045671045780182, acc: 0.9389312863349915)
[2025-02-13 02:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:15][root][INFO] - Training Epoch: 1/2, step 971/7134 completed (loss: 0.20260007679462433, acc: 0.9496932625770569)
[2025-02-13 02:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:15][root][INFO] - Training Epoch: 1/2, step 972/7134 completed (loss: 0.1522868573665619, acc: 0.9551681280136108)
[2025-02-13 02:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:16][root][INFO] - Training Epoch: 1/2, step 973/7134 completed (loss: 0.16937550902366638, acc: 0.9589743614196777)
[2025-02-13 02:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:16][root][INFO] - Training Epoch: 1/2, step 974/7134 completed (loss: 0.15530365705490112, acc: 0.9646522402763367)
[2025-02-13 02:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:17][root][INFO] - Training Epoch: 1/2, step 975/7134 completed (loss: 0.15766793489456177, acc: 0.9509569406509399)
[2025-02-13 02:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:17][root][INFO] - Training Epoch: 1/2, step 976/7134 completed (loss: 0.14109569787979126, acc: 0.9579439163208008)
[2025-02-13 02:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:18][root][INFO] - Training Epoch: 1/2, step 977/7134 completed (loss: 0.16223108768463135, acc: 0.9545454382896423)
[2025-02-13 02:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:18][root][INFO] - Training Epoch: 1/2, step 978/7134 completed (loss: 0.08073865622282028, acc: 0.9783393740653992)
[2025-02-13 02:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19][root][INFO] - Training Epoch: 1/2, step 979/7134 completed (loss: 0.1391316056251526, acc: 0.970588207244873)
[2025-02-13 02:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19][root][INFO] - Training Epoch: 1/2, step 980/7134 completed (loss: 0.12210536748170853, acc: 0.9649999737739563)
[2025-02-13 02:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19][root][INFO] - Training Epoch: 1/2, step 981/7134 completed (loss: 0.21439862251281738, acc: 0.9485530257225037)
[2025-02-13 02:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:20][root][INFO] - Training Epoch: 1/2, step 982/7134 completed (loss: 0.10377736389636993, acc: 0.9775280952453613)
[2025-02-13 02:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:20][root][INFO] - Training Epoch: 1/2, step 983/7134 completed (loss: 0.12393069267272949, acc: 0.9708904027938843)
[2025-02-13 02:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21][root][INFO] - Training Epoch: 1/2, step 984/7134 completed (loss: 0.09658229351043701, acc: 0.9724264740943909)
[2025-02-13 02:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21][root][INFO] - Training Epoch: 1/2, step 985/7134 completed (loss: 0.10351316630840302, acc: 0.9681122303009033)
[2025-02-13 02:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21][root][INFO] - Training Epoch: 1/2, step 986/7134 completed (loss: 0.11295320838689804, acc: 0.9702549576759338)
[2025-02-13 02:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:22][root][INFO] - Training Epoch: 1/2, step 987/7134 completed (loss: 0.07212354242801666, acc: 0.9774919748306274)
[2025-02-13 02:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:22][root][INFO] - Training Epoch: 1/2, step 988/7134 completed (loss: 0.17886760830879211, acc: 0.9597780704498291)
[2025-02-13 02:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:23][root][INFO] - Training Epoch: 1/2, step 989/7134 completed (loss: 0.14008116722106934, acc: 0.9610570073127747)
[2025-02-13 02:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:23][root][INFO] - Training Epoch: 1/2, step 990/7134 completed (loss: 0.18903957307338715, acc: 0.9494311213493347)
[2025-02-13 02:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24][root][INFO] - Training Epoch: 1/2, step 991/7134 completed (loss: 0.16384842991828918, acc: 0.956181526184082)
[2025-02-13 02:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24][root][INFO] - Training Epoch: 1/2, step 992/7134 completed (loss: 0.14433518052101135, acc: 0.9513157606124878)
[2025-02-13 02:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24][root][INFO] - Training Epoch: 1/2, step 993/7134 completed (loss: 0.11363767087459564, acc: 0.972176730632782)
[2025-02-13 02:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:25][root][INFO] - Training Epoch: 1/2, step 994/7134 completed (loss: 0.11834532022476196, acc: 0.9673423171043396)
[2025-02-13 02:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:25][root][INFO] - Training Epoch: 1/2, step 995/7134 completed (loss: 0.1619621366262436, acc: 0.9629629850387573)
[2025-02-13 02:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:26][root][INFO] - Training Epoch: 1/2, step 996/7134 completed (loss: 0.12746918201446533, acc: 0.9678770899772644)
[2025-02-13 02:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:26][root][INFO] - Training Epoch: 1/2, step 997/7134 completed (loss: 0.13693678379058838, acc: 0.9584527015686035)
[2025-02-13 02:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:27][root][INFO] - Training Epoch: 1/2, step 998/7134 completed (loss: 0.13704019784927368, acc: 0.9591549038887024)
[2025-02-13 02:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:27][root][INFO] - Training Epoch: 1/2, step 999/7134 completed (loss: 0.09650568664073944, acc: 0.9731183052062988)
[2025-02-13 02:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28][root][INFO] - Training Epoch: 1/2, step 1000/7134 completed (loss: 0.10846716910600662, acc: 0.9760273694992065)
[2025-02-13 02:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28][root][INFO] - Training Epoch: 1/2, step 1001/7134 completed (loss: 0.12100950628519058, acc: 0.9595959782600403)
[2025-02-13 02:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28][root][INFO] - Training Epoch: 1/2, step 1002/7134 completed (loss: 0.1286163330078125, acc: 0.9657632112503052)
[2025-02-13 02:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:29][root][INFO] - Training Epoch: 1/2, step 1003/7134 completed (loss: 0.1358112245798111, acc: 0.9673105478286743)
[2025-02-13 02:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:29][root][INFO] - Training Epoch: 1/2, step 1004/7134 completed (loss: 0.1576738953590393, acc: 0.9670103192329407)
[2025-02-13 02:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30][root][INFO] - Training Epoch: 1/2, step 1005/7134 completed (loss: 0.17073580622673035, acc: 0.9519867300987244)
[2025-02-13 02:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30][root][INFO] - Training Epoch: 1/2, step 1006/7134 completed (loss: 0.12602785229682922, acc: 0.9637883305549622)
[2025-02-13 02:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30][root][INFO] - Training Epoch: 1/2, step 1007/7134 completed (loss: 0.1511133313179016, acc: 0.9591836929321289)
[2025-02-13 02:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:31][root][INFO] - Training Epoch: 1/2, step 1008/7134 completed (loss: 0.13304823637008667, acc: 0.9602836966514587)
[2025-02-13 02:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:31][root][INFO] - Training Epoch: 1/2, step 1009/7134 completed (loss: 0.1351301074028015, acc: 0.9733570218086243)
[2025-02-13 02:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:32][root][INFO] - Training Epoch: 1/2, step 1010/7134 completed (loss: 0.09742289781570435, acc: 0.9664031863212585)
[2025-02-13 02:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:32][root][INFO] - Training Epoch: 1/2, step 1011/7134 completed (loss: 0.14723309874534607, acc: 0.9654218554496765)
[2025-02-13 02:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33][root][INFO] - Training Epoch: 1/2, step 1012/7134 completed (loss: 0.12180541455745697, acc: 0.9688385128974915)
[2025-02-13 02:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33][root][INFO] - Training Epoch: 1/2, step 1013/7134 completed (loss: 0.11500011384487152, acc: 0.9699140191078186)
[2025-02-13 02:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33][root][INFO] - Training Epoch: 1/2, step 1014/7134 completed (loss: 0.1060202345252037, acc: 0.9655172228813171)
[2025-02-13 02:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:34][root][INFO] - Training Epoch: 1/2, step 1015/7134 completed (loss: 0.07584526389837265, acc: 0.9789156913757324)
[2025-02-13 02:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:34][root][INFO] - Training Epoch: 1/2, step 1016/7134 completed (loss: 0.11569744348526001, acc: 0.9751434326171875)
[2025-02-13 02:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:35][root][INFO] - Training Epoch: 1/2, step 1017/7134 completed (loss: 0.056953154504299164, acc: 0.9865771532058716)
[2025-02-13 02:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:35][root][INFO] - Training Epoch: 1/2, step 1018/7134 completed (loss: 0.11405618488788605, acc: 0.9751098155975342)
[2025-02-13 02:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:36][root][INFO] - Training Epoch: 1/2, step 1019/7134 completed (loss: 0.12364362180233002, acc: 0.9647436141967773)
[2025-02-13 02:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:36][root][INFO] - Training Epoch: 1/2, step 1020/7134 completed (loss: 0.10951614379882812, acc: 0.967164158821106)
[2025-02-13 02:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37][root][INFO] - Training Epoch: 1/2, step 1021/7134 completed (loss: 0.06562381982803345, acc: 0.9808917045593262)
[2025-02-13 02:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37][root][INFO] - Training Epoch: 1/2, step 1022/7134 completed (loss: 0.06542974710464478, acc: 0.9815546870231628)
[2025-02-13 02:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37][root][INFO] - Training Epoch: 1/2, step 1023/7134 completed (loss: 0.08938528597354889, acc: 0.9806950092315674)
[2025-02-13 02:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38][root][INFO] - Training Epoch: 1/2, step 1024/7134 completed (loss: 0.12890882790088654, acc: 0.9694793820381165)
[2025-02-13 02:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38][root][INFO] - Training Epoch: 1/2, step 1025/7134 completed (loss: 0.12065233290195465, acc: 0.9768683314323425)
[2025-02-13 02:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:39][root][INFO] - Training Epoch: 1/2, step 1026/7134 completed (loss: 0.12763556838035583, acc: 0.9684210419654846)
[2025-02-13 02:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:39][root][INFO] - Training Epoch: 1/2, step 1027/7134 completed (loss: 0.06597007066011429, acc: 0.9797022938728333)
[2025-02-13 02:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40][root][INFO] - Training Epoch: 1/2, step 1028/7134 completed (loss: 0.08318062126636505, acc: 0.9865471124649048)
[2025-02-13 02:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40][root][INFO] - Training Epoch: 1/2, step 1029/7134 completed (loss: 0.07605478912591934, acc: 0.9732540845870972)
[2025-02-13 02:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40][root][INFO] - Training Epoch: 1/2, step 1030/7134 completed (loss: 0.0362909771502018, acc: 0.9892183542251587)
[2025-02-13 02:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:41][root][INFO] - Training Epoch: 1/2, step 1031/7134 completed (loss: 0.1206742599606514, acc: 0.9664633870124817)
[2025-02-13 02:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:41][root][INFO] - Training Epoch: 1/2, step 1032/7134 completed (loss: 0.11783955246210098, acc: 0.9723502397537231)
[2025-02-13 02:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:42][root][INFO] - Training Epoch: 1/2, step 1033/7134 completed (loss: 0.08051152527332306, acc: 0.9756097793579102)
[2025-02-13 02:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:42][root][INFO] - Training Epoch: 1/2, step 1034/7134 completed (loss: 0.11305166035890579, acc: 0.9692058563232422)
[2025-02-13 02:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43][root][INFO] - Training Epoch: 1/2, step 1035/7134 completed (loss: 0.29439714550971985, acc: 0.9456140398979187)
[2025-02-13 02:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43][root][INFO] - Training Epoch: 1/2, step 1036/7134 completed (loss: 0.26381468772888184, acc: 0.9399999976158142)
[2025-02-13 02:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43][root][INFO] - Training Epoch: 1/2, step 1037/7134 completed (loss: 0.06640356034040451, acc: 0.976190447807312)
[2025-02-13 02:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:44][root][INFO] - Training Epoch: 1/2, step 1038/7134 completed (loss: 0.08594711869955063, acc: 0.9731543660163879)
[2025-02-13 02:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:44][root][INFO] - Training Epoch: 1/2, step 1039/7134 completed (loss: 0.09865584969520569, acc: 0.977225661277771)
[2025-02-13 02:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45][root][INFO] - Training Epoch: 1/2, step 1040/7134 completed (loss: 0.05192507058382034, acc: 0.9863013625144958)
[2025-02-13 02:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45][root][INFO] - Training Epoch: 1/2, step 1041/7134 completed (loss: 0.07612910121679306, acc: 0.9813084006309509)
[2025-02-13 02:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45][root][INFO] - Training Epoch: 1/2, step 1042/7134 completed (loss: 0.10482224822044373, acc: 0.9679487347602844)
[2025-02-13 02:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:46][root][INFO] - Training Epoch: 1/2, step 1043/7134 completed (loss: 0.08652424067258835, acc: 0.9711191058158875)
[2025-02-13 02:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:46][root][INFO] - Training Epoch: 1/2, step 1044/7134 completed (loss: 0.10060148686170578, acc: 0.9794520735740662)
[2025-02-13 02:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:47][root][INFO] - Training Epoch: 1/2, step 1045/7134 completed (loss: 0.15681906044483185, acc: 0.9660742878913879)
[2025-02-13 02:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:47][root][INFO] - Training Epoch: 1/2, step 1046/7134 completed (loss: 0.08373116701841354, acc: 0.9768339991569519)
[2025-02-13 02:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:48][root][INFO] - Training Epoch: 1/2, step 1047/7134 completed (loss: 0.16147112846374512, acc: 0.9585185050964355)
[2025-02-13 02:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:48][root][INFO] - Training Epoch: 1/2, step 1048/7134 completed (loss: 0.06367498636245728, acc: 0.9838709831237793)
[2025-02-13 02:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49][root][INFO] - Training Epoch: 1/2, step 1049/7134 completed (loss: 0.12557163834571838, acc: 0.9677891731262207)
[2025-02-13 02:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49][root][INFO] - Training Epoch: 1/2, step 1050/7134 completed (loss: 0.1021270751953125, acc: 0.9685792326927185)
[2025-02-13 02:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49][root][INFO] - Training Epoch: 1/2, step 1051/7134 completed (loss: 0.08010999858379364, acc: 0.9794520735740662)
[2025-02-13 02:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:50][root][INFO] - Training Epoch: 1/2, step 1052/7134 completed (loss: 0.15705405175685883, acc: 0.9612277746200562)
[2025-02-13 02:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:50][root][INFO] - Training Epoch: 1/2, step 1053/7134 completed (loss: 0.08447408676147461, acc: 0.9760563373565674)
[2025-02-13 02:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:51][root][INFO] - Training Epoch: 1/2, step 1054/7134 completed (loss: 0.0999903604388237, acc: 0.9682119488716125)
[2025-02-13 02:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:51][root][INFO] - Training Epoch: 1/2, step 1055/7134 completed (loss: 0.15220651030540466, acc: 0.9578713774681091)
[2025-02-13 02:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52][root][INFO] - Training Epoch: 1/2, step 1056/7134 completed (loss: 0.16250668466091156, acc: 0.9597989916801453)
[2025-02-13 02:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52][root][INFO] - Training Epoch: 1/2, step 1057/7134 completed (loss: 0.17110422253608704, acc: 0.9552023410797119)
[2025-02-13 02:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52][root][INFO] - Training Epoch: 1/2, step 1058/7134 completed (loss: 0.13024239242076874, acc: 0.9666666388511658)
[2025-02-13 02:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:53][root][INFO] - Training Epoch: 1/2, step 1059/7134 completed (loss: 0.11968853324651718, acc: 0.9690322875976562)
[2025-02-13 02:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:53][root][INFO] - Training Epoch: 1/2, step 1060/7134 completed (loss: 0.1330859661102295, acc: 0.9668769836425781)
[2025-02-13 02:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:54][root][INFO] - Training Epoch: 1/2, step 1061/7134 completed (loss: 0.17102722823619843, acc: 0.9622092843055725)
[2025-02-13 02:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:54][root][INFO] - Training Epoch: 1/2, step 1062/7134 completed (loss: 0.13070079684257507, acc: 0.9559321999549866)
[2025-02-13 02:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:55][root][INFO] - Training Epoch: 1/2, step 1063/7134 completed (loss: 0.08033964037895203, acc: 0.9789302945137024)
[2025-02-13 02:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:55][root][INFO] - Training Epoch: 1/2, step 1064/7134 completed (loss: 0.12915697693824768, acc: 0.9629629850387573)
[2025-02-13 02:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56][root][INFO] - Training Epoch: 1/2, step 1065/7134 completed (loss: 0.0713687539100647, acc: 0.9783197641372681)
[2025-02-13 02:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56][root][INFO] - Training Epoch: 1/2, step 1066/7134 completed (loss: 0.10604646801948547, acc: 0.9748322367668152)
[2025-02-13 02:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56][root][INFO] - Training Epoch: 1/2, step 1067/7134 completed (loss: 0.06835195422172546, acc: 0.9753466844558716)
[2025-02-13 02:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:57][root][INFO] - Training Epoch: 1/2, step 1068/7134 completed (loss: 0.09651612490415573, acc: 0.9699812531471252)
[2025-02-13 02:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:57][root][INFO] - Training Epoch: 1/2, step 1069/7134 completed (loss: 0.12210965901613235, acc: 0.9666110277175903)
[2025-02-13 02:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58][root][INFO] - Training Epoch: 1/2, step 1070/7134 completed (loss: 0.12469838559627533, acc: 0.9737274050712585)
[2025-02-13 02:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58][root][INFO] - Training Epoch: 1/2, step 1071/7134 completed (loss: 0.06720099598169327, acc: 0.9852744340896606)
[2025-02-13 02:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58][root][INFO] - Training Epoch: 1/2, step 1072/7134 completed (loss: 0.040396708995103836, acc: 0.9878214001655579)
[2025-02-13 02:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59][root][INFO] - Training Epoch: 1/2, step 1073/7134 completed (loss: 0.04522594437003136, acc: 0.9870316982269287)
[2025-02-13 02:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59][root][INFO] - Training Epoch: 1/2, step 1074/7134 completed (loss: 0.13367260992527008, acc: 0.9689119458198547)
[2025-02-13 02:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:00][root][INFO] - Training Epoch: 1/2, step 1075/7134 completed (loss: 0.22009789943695068, acc: 0.9558011293411255)
[2025-02-13 02:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:00][root][INFO] - Training Epoch: 1/2, step 1076/7134 completed (loss: 0.23376278579235077, acc: 0.9428191781044006)
[2025-02-13 02:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01][root][INFO] - Training Epoch: 1/2, step 1077/7134 completed (loss: 0.2237807810306549, acc: 0.9436936974525452)
[2025-02-13 02:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01][root][INFO] - Training Epoch: 1/2, step 1078/7134 completed (loss: 0.2815975546836853, acc: 0.9369951486587524)
[2025-02-13 02:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01][root][INFO] - Training Epoch: 1/2, step 1079/7134 completed (loss: 0.12132541090250015, acc: 0.967704713344574)
[2025-02-13 02:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:02][root][INFO] - Training Epoch: 1/2, step 1080/7134 completed (loss: 0.12188491225242615, acc: 0.9718044996261597)
[2025-02-13 02:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:02][root][INFO] - Training Epoch: 1/2, step 1081/7134 completed (loss: 0.10883935540914536, acc: 0.9719495177268982)
[2025-02-13 02:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:03][root][INFO] - Training Epoch: 1/2, step 1082/7134 completed (loss: 0.07139506191015244, acc: 0.9773895144462585)
[2025-02-13 02:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:03][root][INFO] - Training Epoch: 1/2, step 1083/7134 completed (loss: 0.08475882560014725, acc: 0.9751412272453308)
[2025-02-13 02:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:04][root][INFO] - Training Epoch: 1/2, step 1084/7134 completed (loss: 0.09020631015300751, acc: 0.9776951670646667)
[2025-02-13 02:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:04][root][INFO] - Training Epoch: 1/2, step 1085/7134 completed (loss: 0.07793385535478592, acc: 0.9785810112953186)
[2025-02-13 02:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05][root][INFO] - Training Epoch: 1/2, step 1086/7134 completed (loss: 0.13939762115478516, acc: 0.964083194732666)
[2025-02-13 02:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05][root][INFO] - Training Epoch: 1/2, step 1087/7134 completed (loss: 0.09257391840219498, acc: 0.9743902683258057)
[2025-02-13 02:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05][root][INFO] - Training Epoch: 1/2, step 1088/7134 completed (loss: 0.07374988496303558, acc: 0.9776536226272583)
[2025-02-13 02:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:06][root][INFO] - Training Epoch: 1/2, step 1089/7134 completed (loss: 0.09520865976810455, acc: 0.9701937437057495)
[2025-02-13 02:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:06][root][INFO] - Training Epoch: 1/2, step 1090/7134 completed (loss: 0.10616294294595718, acc: 0.9687987565994263)
[2025-02-13 02:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:07][root][INFO] - Training Epoch: 1/2, step 1091/7134 completed (loss: 0.0928250178694725, acc: 0.9710806608200073)
[2025-02-13 02:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:07][root][INFO] - Training Epoch: 1/2, step 1092/7134 completed (loss: 0.057352110743522644, acc: 0.9812138676643372)
[2025-02-13 02:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08][root][INFO] - Training Epoch: 1/2, step 1093/7134 completed (loss: 0.1318172663450241, acc: 0.9717868566513062)
[2025-02-13 02:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08][root][INFO] - Training Epoch: 1/2, step 1094/7134 completed (loss: 0.14494043588638306, acc: 0.9609665274620056)
[2025-02-13 02:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08][root][INFO] - Training Epoch: 1/2, step 1095/7134 completed (loss: 0.1210700273513794, acc: 0.9634591937065125)
[2025-02-13 02:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:09][root][INFO] - Training Epoch: 1/2, step 1096/7134 completed (loss: 0.1106351912021637, acc: 0.9683544039726257)
[2025-02-13 02:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:09][root][INFO] - Training Epoch: 1/2, step 1097/7134 completed (loss: 0.14060157537460327, acc: 0.959269642829895)
[2025-02-13 02:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:10][root][INFO] - Training Epoch: 1/2, step 1098/7134 completed (loss: 0.08513010293245316, acc: 0.9752781391143799)
[2025-02-13 02:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:10][root][INFO] - Training Epoch: 1/2, step 1099/7134 completed (loss: 0.100015789270401, acc: 0.9756410121917725)
[2025-02-13 02:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:11][root][INFO] - Training Epoch: 1/2, step 1100/7134 completed (loss: 0.10967656970024109, acc: 0.967391312122345)
[2025-02-13 02:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:11][root][INFO] - Training Epoch: 1/2, step 1101/7134 completed (loss: 0.0949607640504837, acc: 0.9701298475265503)
[2025-02-13 02:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12][root][INFO] - Training Epoch: 1/2, step 1102/7134 completed (loss: 0.13010773062705994, acc: 0.9608610272407532)
[2025-02-13 02:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12][root][INFO] - Training Epoch: 1/2, step 1103/7134 completed (loss: 0.13280099630355835, acc: 0.9666666388511658)
[2025-02-13 02:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12][root][INFO] - Training Epoch: 1/2, step 1104/7134 completed (loss: 0.10471991449594498, acc: 0.9783861637115479)
[2025-02-13 02:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:13][root][INFO] - Training Epoch: 1/2, step 1105/7134 completed (loss: 0.12193088233470917, acc: 0.9663072824478149)
[2025-02-13 02:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:13][root][INFO] - Training Epoch: 1/2, step 1106/7134 completed (loss: 0.15167169272899628, acc: 0.9614325165748596)
[2025-02-13 02:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:14][root][INFO] - Training Epoch: 1/2, step 1107/7134 completed (loss: 0.09704088419675827, acc: 0.969519317150116)
[2025-02-13 02:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:14][root][INFO] - Training Epoch: 1/2, step 1108/7134 completed (loss: 0.07793648540973663, acc: 0.9768041372299194)
[2025-02-13 02:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:15][root][INFO] - Training Epoch: 1/2, step 1109/7134 completed (loss: 0.1541372537612915, acc: 0.9497206807136536)
[2025-02-13 02:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:15][root][INFO] - Training Epoch: 1/2, step 1110/7134 completed (loss: 0.14806057512760162, acc: 0.9580602645874023)
[2025-02-13 02:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16][root][INFO] - Training Epoch: 1/2, step 1111/7134 completed (loss: 0.09113430976867676, acc: 0.980663001537323)
[2025-02-13 02:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16][root][INFO] - Training Epoch: 1/2, step 1112/7134 completed (loss: 0.14308872818946838, acc: 0.9558823704719543)
[2025-02-13 02:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16][root][INFO] - Training Epoch: 1/2, step 1113/7134 completed (loss: 0.0980084016919136, acc: 0.9678249955177307)
[2025-02-13 02:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:17][root][INFO] - Training Epoch: 1/2, step 1114/7134 completed (loss: 0.10573960840702057, acc: 0.9756986498832703)
[2025-02-13 02:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:17][root][INFO] - Training Epoch: 1/2, step 1115/7134 completed (loss: 0.1285342574119568, acc: 0.9622641801834106)
[2025-02-13 02:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18][root][INFO] - Training Epoch: 1/2, step 1116/7134 completed (loss: 0.059037432074546814, acc: 0.983208954334259)
[2025-02-13 02:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18][root][INFO] - Training Epoch: 1/2, step 1117/7134 completed (loss: 0.09574568271636963, acc: 0.9708588719367981)
[2025-02-13 02:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18][root][INFO] - Training Epoch: 1/2, step 1118/7134 completed (loss: 0.11364976316690445, acc: 0.9655172228813171)
[2025-02-13 02:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:19][root][INFO] - Training Epoch: 1/2, step 1119/7134 completed (loss: 0.11961212009191513, acc: 0.9636650681495667)
[2025-02-13 02:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:19][root][INFO] - Training Epoch: 1/2, step 1120/7134 completed (loss: 0.1600501388311386, acc: 0.955974817276001)
[2025-02-13 02:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:20][root][INFO] - Training Epoch: 1/2, step 1121/7134 completed (loss: 0.14618751406669617, acc: 0.9586410522460938)
[2025-02-13 02:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:20][root][INFO] - Training Epoch: 1/2, step 1122/7134 completed (loss: 0.07755716145038605, acc: 0.9825242757797241)
[2025-02-13 02:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21][root][INFO] - Training Epoch: 1/2, step 1123/7134 completed (loss: 0.07890767604112625, acc: 0.9791332483291626)
[2025-02-13 02:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21][root][INFO] - Training Epoch: 1/2, step 1124/7134 completed (loss: 0.06902550905942917, acc: 0.9829721450805664)
[2025-02-13 02:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21][root][INFO] - Training Epoch: 1/2, step 1125/7134 completed (loss: 0.058594271540641785, acc: 0.9824086427688599)
[2025-02-13 02:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:22][root][INFO] - Training Epoch: 1/2, step 1126/7134 completed (loss: 0.126235231757164, acc: 0.969613254070282)
[2025-02-13 02:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:22][root][INFO] - Training Epoch: 1/2, step 1127/7134 completed (loss: 0.09942415356636047, acc: 0.9648000001907349)
[2025-02-13 02:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23][root][INFO] - Training Epoch: 1/2, step 1128/7134 completed (loss: 0.08601026237010956, acc: 0.96875)
[2025-02-13 02:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23][root][INFO] - Training Epoch: 1/2, step 1129/7134 completed (loss: 0.09406592696905136, acc: 0.9730941653251648)
[2025-02-13 02:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23][root][INFO] - Training Epoch: 1/2, step 1130/7134 completed (loss: 0.08955217152833939, acc: 0.9773828983306885)
[2025-02-13 02:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:24][root][INFO] - Training Epoch: 1/2, step 1131/7134 completed (loss: 0.13011892139911652, acc: 0.963443398475647)
[2025-02-13 02:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:24][root][INFO] - Training Epoch: 1/2, step 1132/7134 completed (loss: 0.2149815857410431, acc: 0.9540581703186035)
[2025-02-13 02:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:25][root][INFO] - Training Epoch: 1/2, step 1133/7134 completed (loss: 0.17944540083408356, acc: 0.9543509483337402)
[2025-02-13 02:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:25][root][INFO] - Training Epoch: 1/2, step 1134/7134 completed (loss: 0.12326968461275101, acc: 0.971321702003479)
[2025-02-13 02:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26][root][INFO] - Training Epoch: 1/2, step 1135/7134 completed (loss: 0.11448980867862701, acc: 0.9760000109672546)
[2025-02-13 02:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26][root][INFO] - Training Epoch: 1/2, step 1136/7134 completed (loss: 0.17411591112613678, acc: 0.9551020264625549)
[2025-02-13 02:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26][root][INFO] - Training Epoch: 1/2, step 1137/7134 completed (loss: 0.1280898153781891, acc: 0.9636363387107849)
[2025-02-13 02:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:27][root][INFO] - Training Epoch: 1/2, step 1138/7134 completed (loss: 0.13225668668746948, acc: 0.9576502442359924)
[2025-02-13 02:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:27][root][INFO] - Training Epoch: 1/2, step 1139/7134 completed (loss: 0.1709594428539276, acc: 0.9494097828865051)
[2025-02-13 02:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:28][root][INFO] - Training Epoch: 1/2, step 1140/7134 completed (loss: 0.1079457625746727, acc: 0.9774436354637146)
[2025-02-13 02:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:28][root][INFO] - Training Epoch: 1/2, step 1141/7134 completed (loss: 0.09188124537467957, acc: 0.9672130942344666)
[2025-02-13 02:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:29][root][INFO] - Training Epoch: 1/2, step 1142/7134 completed (loss: 0.1060541495680809, acc: 0.9778645634651184)
[2025-02-13 02:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:29][root][INFO] - Training Epoch: 1/2, step 1143/7134 completed (loss: 0.15845713019371033, acc: 0.9554937481880188)
[2025-02-13 02:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30][root][INFO] - Training Epoch: 1/2, step 1144/7134 completed (loss: 0.12913087010383606, acc: 0.9674999713897705)
[2025-02-13 02:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30][root][INFO] - Training Epoch: 1/2, step 1145/7134 completed (loss: 0.0787515789270401, acc: 0.9808153510093689)
[2025-02-13 02:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30][root][INFO] - Training Epoch: 1/2, step 1146/7134 completed (loss: 0.09209388494491577, acc: 0.9800724387168884)
[2025-02-13 02:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:31][root][INFO] - Training Epoch: 1/2, step 1147/7134 completed (loss: 0.0805794894695282, acc: 0.971321702003479)
[2025-02-13 02:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:31][root][INFO] - Training Epoch: 1/2, step 1148/7134 completed (loss: 0.10806199908256531, acc: 0.9726962447166443)
[2025-02-13 02:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:32][root][INFO] - Training Epoch: 1/2, step 1149/7134 completed (loss: 0.21937468647956848, acc: 0.9384615421295166)
[2025-02-13 02:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:32][root][INFO] - Training Epoch: 1/2, step 1150/7134 completed (loss: 0.09367309510707855, acc: 0.9782945513725281)
[2025-02-13 02:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33][root][INFO] - Training Epoch: 1/2, step 1151/7134 completed (loss: 0.04325070232152939, acc: 0.9918864369392395)
[2025-02-13 02:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33][root][INFO] - Training Epoch: 1/2, step 1152/7134 completed (loss: 0.06021071970462799, acc: 0.9811965823173523)
[2025-02-13 02:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33][root][INFO] - Training Epoch: 1/2, step 1153/7134 completed (loss: 0.13720905780792236, acc: 0.9673024415969849)
[2025-02-13 02:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:34][root][INFO] - Training Epoch: 1/2, step 1154/7134 completed (loss: 0.13888293504714966, acc: 0.969911515712738)
[2025-02-13 02:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:34][root][INFO] - Training Epoch: 1/2, step 1155/7134 completed (loss: 0.1311265081167221, acc: 0.9627193212509155)
[2025-02-13 02:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:35][root][INFO] - Training Epoch: 1/2, step 1156/7134 completed (loss: 0.04919380322098732, acc: 0.9846389889717102)
[2025-02-13 02:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:35][root][INFO] - Training Epoch: 1/2, step 1157/7134 completed (loss: 0.11412164568901062, acc: 0.9762309193611145)
[2025-02-13 02:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36][root][INFO] - Training Epoch: 1/2, step 1158/7134 completed (loss: 0.12679968774318695, acc: 0.9684418439865112)
[2025-02-13 02:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36][root][INFO] - Training Epoch: 1/2, step 1159/7134 completed (loss: 0.11932067573070526, acc: 0.9673758745193481)
[2025-02-13 02:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36][root][INFO] - Training Epoch: 1/2, step 1160/7134 completed (loss: 0.07975698262453079, acc: 0.976190447807312)
[2025-02-13 02:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:37][root][INFO] - Training Epoch: 1/2, step 1161/7134 completed (loss: 0.15424826741218567, acc: 0.9706361889839172)
[2025-02-13 02:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:37][root][INFO] - Training Epoch: 1/2, step 1162/7134 completed (loss: 0.07499480992555618, acc: 0.9768595099449158)
[2025-02-13 02:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:38][root][INFO] - Training Epoch: 1/2, step 1163/7134 completed (loss: 0.07999706268310547, acc: 0.9812286496162415)
[2025-02-13 02:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:38][root][INFO] - Training Epoch: 1/2, step 1164/7134 completed (loss: 0.11118001490831375, acc: 0.9735743999481201)
[2025-02-13 02:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39][root][INFO] - Training Epoch: 1/2, step 1165/7134 completed (loss: 0.060686808079481125, acc: 0.9837177991867065)
[2025-02-13 02:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39][root][INFO] - Training Epoch: 1/2, step 1166/7134 completed (loss: 0.06688611954450607, acc: 0.9835391044616699)
[2025-02-13 02:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39][root][INFO] - Training Epoch: 1/2, step 1167/7134 completed (loss: 0.05054186284542084, acc: 0.9882121682167053)
[2025-02-13 02:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:40][root][INFO] - Training Epoch: 1/2, step 1168/7134 completed (loss: 0.04559580981731415, acc: 0.9868420958518982)
[2025-02-13 02:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:40][root][INFO] - Training Epoch: 1/2, step 1169/7134 completed (loss: 0.09609896689653397, acc: 0.9743243455886841)
[2025-02-13 02:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:41][root][INFO] - Training Epoch: 1/2, step 1170/7134 completed (loss: 0.05405386909842491, acc: 0.9873595237731934)
[2025-02-13 02:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:41][root][INFO] - Training Epoch: 1/2, step 1171/7134 completed (loss: 0.0512666329741478, acc: 0.9866443872451782)
[2025-02-13 02:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42][root][INFO] - Training Epoch: 1/2, step 1172/7134 completed (loss: 0.09327548742294312, acc: 0.9773070812225342)
[2025-02-13 02:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42][root][INFO] - Training Epoch: 1/2, step 1173/7134 completed (loss: 0.09142237901687622, acc: 0.9768115878105164)
[2025-02-13 02:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42][root][INFO] - Training Epoch: 1/2, step 1174/7134 completed (loss: 0.04872608557343483, acc: 0.9879879951477051)
[2025-02-13 02:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:43][root][INFO] - Training Epoch: 1/2, step 1175/7134 completed (loss: 0.07915420830249786, acc: 0.9739130139350891)
[2025-02-13 02:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:43][root][INFO] - Training Epoch: 1/2, step 1176/7134 completed (loss: 0.03417641296982765, acc: 0.9880136847496033)
[2025-02-13 02:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44][root][INFO] - Training Epoch: 1/2, step 1177/7134 completed (loss: 0.03178715333342552, acc: 0.9882352948188782)
[2025-02-13 02:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44][root][INFO] - Training Epoch: 1/2, step 1178/7134 completed (loss: 0.05101226270198822, acc: 0.9836448431015015)
[2025-02-13 02:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44][root][INFO] - Training Epoch: 1/2, step 1179/7134 completed (loss: 0.05935181304812431, acc: 0.980879545211792)
[2025-02-13 02:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45][root][INFO] - Training Epoch: 1/2, step 1180/7134 completed (loss: 0.06268524378538132, acc: 0.985401451587677)
[2025-02-13 02:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45][root][INFO] - Training Epoch: 1/2, step 1181/7134 completed (loss: 0.0767962634563446, acc: 0.9827337861061096)
[2025-02-13 02:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:46][root][INFO] - Training Epoch: 1/2, step 1182/7134 completed (loss: 0.045182134956121445, acc: 0.9863247871398926)
[2025-02-13 02:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:46][root][INFO] - Training Epoch: 1/2, step 1183/7134 completed (loss: 0.04513395577669144, acc: 0.98531574010849)
[2025-02-13 02:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47][root][INFO] - Training Epoch: 1/2, step 1184/7134 completed (loss: 0.1007508784532547, acc: 0.9785407781600952)
[2025-02-13 02:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47][root][INFO] - Training Epoch: 1/2, step 1185/7134 completed (loss: 0.030292149633169174, acc: 0.9894737005233765)
[2025-02-13 02:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47][root][INFO] - Training Epoch: 1/2, step 1186/7134 completed (loss: 0.042032916098833084, acc: 0.993514895439148)
[2025-02-13 02:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:48][root][INFO] - Training Epoch: 1/2, step 1187/7134 completed (loss: 0.06747884303331375, acc: 0.9806094169616699)
[2025-02-13 02:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:48][root][INFO] - Training Epoch: 1/2, step 1188/7134 completed (loss: 0.02472830004990101, acc: 0.9920844435691833)
[2025-02-13 02:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:49][root][INFO] - Training Epoch: 1/2, step 1189/7134 completed (loss: 0.05736825615167618, acc: 0.986994206905365)
[2025-02-13 02:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:49][root][INFO] - Training Epoch: 1/2, step 1190/7134 completed (loss: 0.04575251787900925, acc: 0.9872773289680481)
[2025-02-13 02:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50][root][INFO] - Training Epoch: 1/2, step 1191/7134 completed (loss: 0.10387091338634491, acc: 0.9689807891845703)
[2025-02-13 02:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50][root][INFO] - Training Epoch: 1/2, step 1192/7134 completed (loss: 0.06913569569587708, acc: 0.9737876653671265)
[2025-02-13 02:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50][root][INFO] - Training Epoch: 1/2, step 1193/7134 completed (loss: 0.11427845060825348, acc: 0.9696969985961914)
[2025-02-13 02:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:51][root][INFO] - Training Epoch: 1/2, step 1194/7134 completed (loss: 0.08156891912221909, acc: 0.9821428656578064)
[2025-02-13 02:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:51][root][INFO] - Training Epoch: 1/2, step 1195/7134 completed (loss: 0.10076556354761124, acc: 0.9754902124404907)
[2025-02-13 02:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:52][root][INFO] - Training Epoch: 1/2, step 1196/7134 completed (loss: 0.11654524505138397, acc: 0.9704918265342712)
[2025-02-13 02:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:52][root][INFO] - Training Epoch: 1/2, step 1197/7134 completed (loss: 0.08063548058271408, acc: 0.9809384346008301)
[2025-02-13 02:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:53][root][INFO] - Training Epoch: 1/2, step 1198/7134 completed (loss: 0.0678652822971344, acc: 0.9769784212112427)
[2025-02-13 02:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:53][root][INFO] - Training Epoch: 1/2, step 1199/7134 completed (loss: 0.05095489323139191, acc: 0.9867452383041382)
[2025-02-13 02:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54][root][INFO] - Training Epoch: 1/2, step 1200/7134 completed (loss: 0.12002868205308914, acc: 0.975359320640564)
[2025-02-13 02:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54][root][INFO] - Training Epoch: 1/2, step 1201/7134 completed (loss: 0.0814189612865448, acc: 0.9793814420700073)
[2025-02-13 02:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54][root][INFO] - Training Epoch: 1/2, step 1202/7134 completed (loss: 0.08061631768941879, acc: 0.9771198034286499)
[2025-02-13 02:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:55][root][INFO] - Training Epoch: 1/2, step 1203/7134 completed (loss: 0.03051431104540825, acc: 0.9924127459526062)
[2025-02-13 02:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:55][root][INFO] - Training Epoch: 1/2, step 1204/7134 completed (loss: 0.07761123776435852, acc: 0.97947758436203)
[2025-02-13 02:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:56][root][INFO] - Training Epoch: 1/2, step 1205/7134 completed (loss: 0.10325039178133011, acc: 0.9704225063323975)
[2025-02-13 02:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:56][root][INFO] - Training Epoch: 1/2, step 1206/7134 completed (loss: 0.08212539553642273, acc: 0.9789081811904907)
[2025-02-13 02:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57][root][INFO] - Training Epoch: 1/2, step 1207/7134 completed (loss: 0.07623588293790817, acc: 0.9861809015274048)
[2025-02-13 02:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57][root][INFO] - Training Epoch: 1/2, step 1208/7134 completed (loss: 0.0912327989935875, acc: 0.9757462739944458)
[2025-02-13 02:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57][root][INFO] - Training Epoch: 1/2, step 1209/7134 completed (loss: 0.15371209383010864, acc: 0.9616087675094604)
[2025-02-13 02:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:58][root][INFO] - Training Epoch: 1/2, step 1210/7134 completed (loss: 0.10307841002941132, acc: 0.9744816422462463)
[2025-02-13 02:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:58][root][INFO] - Training Epoch: 1/2, step 1211/7134 completed (loss: 0.18882378935813904, acc: 0.9518304467201233)
[2025-02-13 02:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:59][root][INFO] - Training Epoch: 1/2, step 1212/7134 completed (loss: 0.13781332969665527, acc: 0.9710366129875183)
[2025-02-13 02:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:59][root][INFO] - Training Epoch: 1/2, step 1213/7134 completed (loss: 0.08295831829309464, acc: 0.9688826203346252)
[2025-02-13 02:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00][root][INFO] - Training Epoch: 1/2, step 1214/7134 completed (loss: 0.1496112048625946, acc: 0.9615384340286255)
[2025-02-13 02:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00][root][INFO] - Training Epoch: 1/2, step 1215/7134 completed (loss: 0.1368524432182312, acc: 0.9641379117965698)
[2025-02-13 02:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00][root][INFO] - Training Epoch: 1/2, step 1216/7134 completed (loss: 0.07652075588703156, acc: 0.9751824736595154)
[2025-02-13 02:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:01][root][INFO] - Training Epoch: 1/2, step 1217/7134 completed (loss: 0.0996190756559372, acc: 0.975649356842041)
[2025-02-13 02:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:01][root][INFO] - Training Epoch: 1/2, step 1218/7134 completed (loss: 0.18133802711963654, acc: 0.9611940383911133)
[2025-02-13 02:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:02][root][INFO] - Training Epoch: 1/2, step 1219/7134 completed (loss: 0.22330273687839508, acc: 0.9507908821105957)
[2025-02-13 02:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:02][root][INFO] - Training Epoch: 1/2, step 1220/7134 completed (loss: 0.17006108164787292, acc: 0.9581218361854553)
[2025-02-13 02:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:03][root][INFO] - Training Epoch: 1/2, step 1221/7134 completed (loss: 0.1706673949956894, acc: 0.9550438523292542)
[2025-02-13 02:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:03][root][INFO] - Training Epoch: 1/2, step 1222/7134 completed (loss: 0.13782504200935364, acc: 0.9642431735992432)
[2025-02-13 02:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:04][root][INFO] - Training Epoch: 1/2, step 1223/7134 completed (loss: 0.2505141794681549, acc: 0.9451302886009216)
[2025-02-13 02:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:04][root][INFO] - Training Epoch: 1/2, step 1224/7134 completed (loss: 0.1273220330476761, acc: 0.9713024497032166)
[2025-02-13 02:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:05][root][INFO] - Training Epoch: 1/2, step 1225/7134 completed (loss: 0.1509818136692047, acc: 0.9545970559120178)
[2025-02-13 02:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:05][root][INFO] - Training Epoch: 1/2, step 1226/7134 completed (loss: 0.21807126700878143, acc: 0.9456740617752075)
[2025-02-13 02:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06][root][INFO] - Training Epoch: 1/2, step 1227/7134 completed (loss: 0.1893523931503296, acc: 0.9436997175216675)
[2025-02-13 02:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06][root][INFO] - Training Epoch: 1/2, step 1228/7134 completed (loss: 0.16167078912258148, acc: 0.9599427580833435)
[2025-02-13 02:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06][root][INFO] - Training Epoch: 1/2, step 1229/7134 completed (loss: 0.1261899173259735, acc: 0.9639856219291687)
[2025-02-13 02:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:07][root][INFO] - Training Epoch: 1/2, step 1230/7134 completed (loss: 0.14604218304157257, acc: 0.9604519605636597)
[2025-02-13 02:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:07][root][INFO] - Training Epoch: 1/2, step 1231/7134 completed (loss: 0.11968906223773956, acc: 0.9648396968841553)
[2025-02-13 02:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:08][root][INFO] - Training Epoch: 1/2, step 1232/7134 completed (loss: 0.12309218943119049, acc: 0.9701810479164124)
[2025-02-13 02:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:08][root][INFO] - Training Epoch: 1/2, step 1233/7134 completed (loss: 0.19677011668682098, acc: 0.9468207955360413)
[2025-02-13 02:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:09][root][INFO] - Training Epoch: 1/2, step 1234/7134 completed (loss: 0.10048073530197144, acc: 0.9767080545425415)
[2025-02-13 02:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:09][root][INFO] - Training Epoch: 1/2, step 1235/7134 completed (loss: 0.14436781406402588, acc: 0.9688249230384827)
[2025-02-13 02:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:10][root][INFO] - Training Epoch: 1/2, step 1236/7134 completed (loss: 0.24987688660621643, acc: 0.9311859607696533)
[2025-02-13 02:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:10][root][INFO] - Training Epoch: 1/2, step 1237/7134 completed (loss: 0.16132232546806335, acc: 0.9627329111099243)
[2025-02-13 02:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11][root][INFO] - Training Epoch: 1/2, step 1238/7134 completed (loss: 0.25321564078330994, acc: 0.9170305728912354)
[2025-02-13 02:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11][root][INFO] - Training Epoch: 1/2, step 1239/7134 completed (loss: 0.20628225803375244, acc: 0.9370529055595398)
[2025-02-13 02:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12][root][INFO] - Training Epoch: 1/2, step 1240/7134 completed (loss: 0.18379773199558258, acc: 0.9391534328460693)
[2025-02-13 02:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12][root][INFO] - Training Epoch: 1/2, step 1241/7134 completed (loss: 0.1822226494550705, acc: 0.9558233022689819)
[2025-02-13 02:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12][root][INFO] - Training Epoch: 1/2, step 1242/7134 completed (loss: 0.1515377014875412, acc: 0.9543269276618958)
[2025-02-13 02:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:13][root][INFO] - Training Epoch: 1/2, step 1243/7134 completed (loss: 0.13064083456993103, acc: 0.9665272235870361)
[2025-02-13 02:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:13][root][INFO] - Training Epoch: 1/2, step 1244/7134 completed (loss: 0.14700935781002045, acc: 0.9569536447525024)
[2025-02-13 02:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:14][root][INFO] - Training Epoch: 1/2, step 1245/7134 completed (loss: 0.12382715940475464, acc: 0.9647355079650879)
[2025-02-13 02:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:14][root][INFO] - Training Epoch: 1/2, step 1246/7134 completed (loss: 0.1630374640226364, acc: 0.9550706148147583)
[2025-02-13 02:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15][root][INFO] - Training Epoch: 1/2, step 1247/7134 completed (loss: 0.10681479424238205, acc: 0.9710564613342285)
[2025-02-13 02:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15][root][INFO] - Training Epoch: 1/2, step 1248/7134 completed (loss: 0.11156601458787918, acc: 0.970251739025116)
[2025-02-13 02:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15][root][INFO] - Training Epoch: 1/2, step 1249/7134 completed (loss: 0.09479645639657974, acc: 0.9786477088928223)
[2025-02-13 02:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:16][root][INFO] - Training Epoch: 1/2, step 1250/7134 completed (loss: 0.062451187521219254, acc: 0.9830028414726257)
[2025-02-13 02:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:16][root][INFO] - Training Epoch: 1/2, step 1251/7134 completed (loss: 0.07299906015396118, acc: 0.9807956218719482)
[2025-02-13 02:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17][root][INFO] - Training Epoch: 1/2, step 1252/7134 completed (loss: 0.0838264673948288, acc: 0.9845678806304932)
[2025-02-13 02:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17][root][INFO] - Training Epoch: 1/2, step 1253/7134 completed (loss: 0.11183685809373856, acc: 0.9710424542427063)
[2025-02-13 02:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17][root][INFO] - Training Epoch: 1/2, step 1254/7134 completed (loss: 0.15808185935020447, acc: 0.9560605883598328)
[2025-02-13 02:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:18][root][INFO] - Training Epoch: 1/2, step 1255/7134 completed (loss: 0.10855992138385773, acc: 0.966472327709198)
[2025-02-13 02:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:18][root][INFO] - Training Epoch: 1/2, step 1256/7134 completed (loss: 0.09849613904953003, acc: 0.9758842587471008)
[2025-02-13 02:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:19][root][INFO] - Training Epoch: 1/2, step 1257/7134 completed (loss: 0.06379608809947968, acc: 0.9789983630180359)
[2025-02-13 02:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:19][root][INFO] - Training Epoch: 1/2, step 1258/7134 completed (loss: 0.1225210577249527, acc: 0.9648609161376953)
[2025-02-13 02:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20][root][INFO] - Training Epoch: 1/2, step 1259/7134 completed (loss: 0.09990391880273819, acc: 0.9806867241859436)
[2025-02-13 02:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20][root][INFO] - Training Epoch: 1/2, step 1260/7134 completed (loss: 0.08466695249080658, acc: 0.9768785834312439)
[2025-02-13 02:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20][root][INFO] - Training Epoch: 1/2, step 1261/7134 completed (loss: 0.07693175226449966, acc: 0.9792284965515137)
[2025-02-13 02:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:21][root][INFO] - Training Epoch: 1/2, step 1262/7134 completed (loss: 0.018489621579647064, acc: 0.9981684684753418)
[2025-02-13 02:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:21][root][INFO] - Training Epoch: 1/2, step 1263/7134 completed (loss: 0.07047509402036667, acc: 0.9857142567634583)
[2025-02-13 02:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:22][root][INFO] - Training Epoch: 1/2, step 1264/7134 completed (loss: 0.0840713232755661, acc: 0.9844236969947815)
[2025-02-13 02:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:22][root][INFO] - Training Epoch: 1/2, step 1265/7134 completed (loss: 0.08106668293476105, acc: 0.9786967635154724)
[2025-02-13 02:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23][root][INFO] - Training Epoch: 1/2, step 1266/7134 completed (loss: 0.06732013821601868, acc: 0.979345977306366)
[2025-02-13 02:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23][root][INFO] - Training Epoch: 1/2, step 1267/7134 completed (loss: 0.08586306869983673, acc: 0.9806896448135376)
[2025-02-13 02:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23][root][INFO] - Training Epoch: 1/2, step 1268/7134 completed (loss: 0.09052757918834686, acc: 0.9734120965003967)
[2025-02-13 02:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24][root][INFO] - Training Epoch: 1/2, step 1269/7134 completed (loss: 0.11536660045385361, acc: 0.9689655303955078)
[2025-02-13 02:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24][root][INFO] - Training Epoch: 1/2, step 1270/7134 completed (loss: 0.064107246696949, acc: 0.9804741740226746)
[2025-02-13 02:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25][root][INFO] - Training Epoch: 1/2, step 1271/7134 completed (loss: 0.10880377143621445, acc: 0.9740740656852722)
[2025-02-13 02:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25][root][INFO] - Training Epoch: 1/2, step 1272/7134 completed (loss: 0.053223151713609695, acc: 0.9867899417877197)
[2025-02-13 02:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25][root][INFO] - Training Epoch: 1/2, step 1273/7134 completed (loss: 0.04676980525255203, acc: 0.9887096881866455)
[2025-02-13 02:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:26][root][INFO] - Training Epoch: 1/2, step 1274/7134 completed (loss: 0.06334507465362549, acc: 0.9864864945411682)
[2025-02-13 02:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:26][root][INFO] - Training Epoch: 1/2, step 1275/7134 completed (loss: 0.08296963572502136, acc: 0.9766606688499451)
[2025-02-13 02:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:27][root][INFO] - Training Epoch: 1/2, step 1276/7134 completed (loss: 0.03950925171375275, acc: 0.9870967864990234)
[2025-02-13 02:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:27][root][INFO] - Training Epoch: 1/2, step 1277/7134 completed (loss: 0.05405675619840622, acc: 0.9854604005813599)
[2025-02-13 02:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28][root][INFO] - Training Epoch: 1/2, step 1278/7134 completed (loss: 0.09996471554040909, acc: 0.9673405885696411)
[2025-02-13 02:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28][root][INFO] - Training Epoch: 1/2, step 1279/7134 completed (loss: 0.13404175639152527, acc: 0.9652605652809143)
[2025-02-13 02:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28][root][INFO] - Training Epoch: 1/2, step 1280/7134 completed (loss: 0.13434316217899323, acc: 0.9724047183990479)
[2025-02-13 02:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:29][root][INFO] - Training Epoch: 1/2, step 1281/7134 completed (loss: 0.1199185773730278, acc: 0.956970751285553)
[2025-02-13 02:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:29][root][INFO] - Training Epoch: 1/2, step 1282/7134 completed (loss: 0.15258318185806274, acc: 0.9639037251472473)
[2025-02-13 02:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:30][root][INFO] - Training Epoch: 1/2, step 1283/7134 completed (loss: 0.13640454411506653, acc: 0.9718309640884399)
[2025-02-13 02:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:30][root][INFO] - Training Epoch: 1/2, step 1284/7134 completed (loss: 0.10366468876600266, acc: 0.971389651298523)
[2025-02-13 02:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31][root][INFO] - Training Epoch: 1/2, step 1285/7134 completed (loss: 0.09087608009576797, acc: 0.9783549904823303)
[2025-02-13 02:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31][root][INFO] - Training Epoch: 1/2, step 1286/7134 completed (loss: 0.11224519461393356, acc: 0.9742489457130432)
[2025-02-13 02:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31][root][INFO] - Training Epoch: 1/2, step 1287/7134 completed (loss: 0.2112448662519455, acc: 0.9485049843788147)
[2025-02-13 02:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:32][root][INFO] - Training Epoch: 1/2, step 1288/7134 completed (loss: 0.09439975023269653, acc: 0.9746192693710327)
[2025-02-13 02:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:32][root][INFO] - Training Epoch: 1/2, step 1289/7134 completed (loss: 0.09655850380659103, acc: 0.9782244563102722)
[2025-02-13 02:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:33][root][INFO] - Training Epoch: 1/2, step 1290/7134 completed (loss: 0.16664721071720123, acc: 0.9591549038887024)
[2025-02-13 02:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:33][root][INFO] - Training Epoch: 1/2, step 1291/7134 completed (loss: 0.10818012058734894, acc: 0.9762901067733765)
[2025-02-13 02:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:34][root][INFO] - Training Epoch: 1/2, step 1292/7134 completed (loss: 0.11885552108287811, acc: 0.9636363387107849)
[2025-02-13 02:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:34][root][INFO] - Training Epoch: 1/2, step 1293/7134 completed (loss: 0.08497601002454758, acc: 0.9697322249412537)
[2025-02-13 02:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35][root][INFO] - Training Epoch: 1/2, step 1294/7134 completed (loss: 0.09565119445323944, acc: 0.9778933525085449)
[2025-02-13 02:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35][root][INFO] - Training Epoch: 1/2, step 1295/7134 completed (loss: 0.10240615904331207, acc: 0.9745330810546875)
[2025-02-13 02:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35][root][INFO] - Training Epoch: 1/2, step 1296/7134 completed (loss: 0.08299387991428375, acc: 0.982300877571106)
[2025-02-13 02:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:36][root][INFO] - Training Epoch: 1/2, step 1297/7134 completed (loss: 0.09904748946428299, acc: 0.9708939790725708)
[2025-02-13 02:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:36][root][INFO] - Training Epoch: 1/2, step 1298/7134 completed (loss: 0.16791847348213196, acc: 0.9658273458480835)
[2025-02-13 02:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37][root][INFO] - Training Epoch: 1/2, step 1299/7134 completed (loss: 0.0850517749786377, acc: 0.9713541865348816)
[2025-02-13 02:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37][root][INFO] - Training Epoch: 1/2, step 1300/7134 completed (loss: 0.10236531496047974, acc: 0.9671132564544678)
[2025-02-13 02:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37][root][INFO] - Training Epoch: 1/2, step 1301/7134 completed (loss: 0.05453462526202202, acc: 0.9866468906402588)
[2025-02-13 02:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:38][root][INFO] - Training Epoch: 1/2, step 1302/7134 completed (loss: 0.0963534265756607, acc: 0.9743303656578064)
[2025-02-13 02:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:38][root][INFO] - Training Epoch: 1/2, step 1303/7134 completed (loss: 0.12636134028434753, acc: 0.9615384340286255)
[2025-02-13 02:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:39][root][INFO] - Training Epoch: 1/2, step 1304/7134 completed (loss: 0.14699673652648926, acc: 0.9644268751144409)
[2025-02-13 02:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:39][root][INFO] - Training Epoch: 1/2, step 1305/7134 completed (loss: 0.10872256755828857, acc: 0.9784263968467712)
[2025-02-13 02:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:40][root][INFO] - Training Epoch: 1/2, step 1306/7134 completed (loss: 0.1498776078224182, acc: 0.9635812044143677)
[2025-02-13 02:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:40][root][INFO] - Training Epoch: 1/2, step 1307/7134 completed (loss: 0.07529011368751526, acc: 0.9826224446296692)
[2025-02-13 02:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41][root][INFO] - Training Epoch: 1/2, step 1308/7134 completed (loss: 0.0720706582069397, acc: 0.9774436354637146)
[2025-02-13 02:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41][root][INFO] - Training Epoch: 1/2, step 1309/7134 completed (loss: 0.08777355402708054, acc: 0.9772727489471436)
[2025-02-13 02:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41][root][INFO] - Training Epoch: 1/2, step 1310/7134 completed (loss: 0.13281041383743286, acc: 0.9646258354187012)
[2025-02-13 02:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42][root][INFO] - Training Epoch: 1/2, step 1311/7134 completed (loss: 0.15834978222846985, acc: 0.9547511339187622)
[2025-02-13 02:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42][root][INFO] - Training Epoch: 1/2, step 1312/7134 completed (loss: 0.1504109650850296, acc: 0.9636118412017822)
[2025-02-13 02:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:43][root][INFO] - Training Epoch: 1/2, step 1313/7134 completed (loss: 0.13703514635562897, acc: 0.9560906291007996)
[2025-02-13 02:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:43][root][INFO] - Training Epoch: 1/2, step 1314/7134 completed (loss: 0.09811475872993469, acc: 0.9719298481941223)
[2025-02-13 02:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44][root][INFO] - Training Epoch: 1/2, step 1315/7134 completed (loss: 0.23889604210853577, acc: 0.9453333616256714)
[2025-02-13 02:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44][root][INFO] - Training Epoch: 1/2, step 1316/7134 completed (loss: 0.09232211858034134, acc: 0.9811676144599915)
[2025-02-13 02:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44][root][INFO] - Training Epoch: 1/2, step 1317/7134 completed (loss: 0.17620618641376495, acc: 0.9512194991111755)
[2025-02-13 02:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:45][root][INFO] - Training Epoch: 1/2, step 1318/7134 completed (loss: 0.1630779504776001, acc: 0.9591280817985535)
[2025-02-13 02:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:45][root][INFO] - Training Epoch: 1/2, step 1319/7134 completed (loss: 0.18133951723575592, acc: 0.9450801014900208)
[2025-02-13 02:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46][root][INFO] - Training Epoch: 1/2, step 1320/7134 completed (loss: 0.20703867077827454, acc: 0.9527687430381775)
[2025-02-13 02:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46][root][INFO] - Training Epoch: 1/2, step 1321/7134 completed (loss: 0.09940335899591446, acc: 0.968137264251709)
[2025-02-13 02:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:47][root][INFO] - Training Epoch: 1/2, step 1322/7134 completed (loss: 0.11180248111486435, acc: 0.972176730632782)
[2025-02-13 02:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:47][root][INFO] - Training Epoch: 1/2, step 1323/7134 completed (loss: 0.1416015923023224, acc: 0.9571428298950195)
[2025-02-13 02:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48][root][INFO] - Training Epoch: 1/2, step 1324/7134 completed (loss: 0.08274990320205688, acc: 0.9753289222717285)
[2025-02-13 02:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48][root][INFO] - Training Epoch: 1/2, step 1325/7134 completed (loss: 0.1132286936044693, acc: 0.9614512324333191)
[2025-02-13 02:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48][root][INFO] - Training Epoch: 1/2, step 1326/7134 completed (loss: 0.1399780958890915, acc: 0.9604830145835876)
[2025-02-13 02:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:49][root][INFO] - Training Epoch: 1/2, step 1327/7134 completed (loss: 0.12046664208173752, acc: 0.9700149893760681)
[2025-02-13 02:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:49][root][INFO] - Training Epoch: 1/2, step 1328/7134 completed (loss: 0.13682477176189423, acc: 0.9674418568611145)
[2025-02-13 02:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50][root][INFO] - Training Epoch: 1/2, step 1329/7134 completed (loss: 0.13069036602973938, acc: 0.9640883803367615)
[2025-02-13 02:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50][root][INFO] - Training Epoch: 1/2, step 1330/7134 completed (loss: 0.08495518565177917, acc: 0.9794167876243591)
[2025-02-13 02:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50][root][INFO] - Training Epoch: 1/2, step 1331/7134 completed (loss: 0.04592924565076828, acc: 0.9820846915245056)
[2025-02-13 02:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:51][root][INFO] - Training Epoch: 1/2, step 1332/7134 completed (loss: 0.07516098767518997, acc: 0.9803493618965149)
[2025-02-13 02:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:51][root][INFO] - Training Epoch: 1/2, step 1333/7134 completed (loss: 0.12224748730659485, acc: 0.9676870703697205)
[2025-02-13 02:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:52][root][INFO] - Training Epoch: 1/2, step 1334/7134 completed (loss: 0.15663138031959534, acc: 0.9596273303031921)
[2025-02-13 02:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:52][root][INFO] - Training Epoch: 1/2, step 1335/7134 completed (loss: 0.13472184538841248, acc: 0.9733688235282898)
[2025-02-13 02:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53][root][INFO] - Training Epoch: 1/2, step 1336/7134 completed (loss: 0.0685448870062828, acc: 0.9824086427688599)
[2025-02-13 02:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53][root][INFO] - Training Epoch: 1/2, step 1337/7134 completed (loss: 0.13155290484428406, acc: 0.9662576913833618)
[2025-02-13 02:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53][root][INFO] - Training Epoch: 1/2, step 1338/7134 completed (loss: 0.11606166511774063, acc: 0.9598393440246582)
[2025-02-13 02:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54][root][INFO] - Training Epoch: 1/2, step 1339/7134 completed (loss: 0.1553344577550888, acc: 0.9634831547737122)
[2025-02-13 02:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54][root][INFO] - Training Epoch: 1/2, step 1340/7134 completed (loss: 0.1111854612827301, acc: 0.9731743931770325)
[2025-02-13 02:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54][root][INFO] - Training Epoch: 1/2, step 1341/7134 completed (loss: 0.1330728530883789, acc: 0.9689265489578247)
[2025-02-13 02:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:55][root][INFO] - Training Epoch: 1/2, step 1342/7134 completed (loss: 0.06508311629295349, acc: 0.9806763529777527)
[2025-02-13 02:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:55][root][INFO] - Training Epoch: 1/2, step 1343/7134 completed (loss: 0.1115497499704361, acc: 0.971377432346344)
[2025-02-13 02:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56][root][INFO] - Training Epoch: 1/2, step 1344/7134 completed (loss: 0.07588529586791992, acc: 0.9841628670692444)
[2025-02-13 02:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56][root][INFO] - Training Epoch: 1/2, step 1345/7134 completed (loss: 0.10319004207849503, acc: 0.9655172228813171)
[2025-02-13 02:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56][root][INFO] - Training Epoch: 1/2, step 1346/7134 completed (loss: 0.09889517724514008, acc: 0.9786381721496582)
[2025-02-13 02:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:57][root][INFO] - Training Epoch: 1/2, step 1347/7134 completed (loss: 0.09917855262756348, acc: 0.9808362126350403)
[2025-02-13 02:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:57][root][INFO] - Training Epoch: 1/2, step 1348/7134 completed (loss: 0.06902182102203369, acc: 0.9816513657569885)
[2025-02-13 02:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58][root][INFO] - Training Epoch: 1/2, step 1349/7134 completed (loss: 0.07963472604751587, acc: 0.9743589758872986)
[2025-02-13 02:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58][root][INFO] - Training Epoch: 1/2, step 1350/7134 completed (loss: 0.051138993352651596, acc: 0.9855282306671143)
[2025-02-13 02:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58][root][INFO] - Training Epoch: 1/2, step 1351/7134 completed (loss: 0.04874909296631813, acc: 0.990176796913147)
[2025-02-13 02:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:59][root][INFO] - Training Epoch: 1/2, step 1352/7134 completed (loss: 0.09889630228281021, acc: 0.9732142686843872)
[2025-02-13 02:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:59][root][INFO] - Training Epoch: 1/2, step 1353/7134 completed (loss: 0.08952530473470688, acc: 0.977011501789093)
[2025-02-13 02:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:00][root][INFO] - Training Epoch: 1/2, step 1354/7134 completed (loss: 0.048026274889707565, acc: 0.9854133129119873)
[2025-02-13 02:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:00][root][INFO] - Training Epoch: 1/2, step 1355/7134 completed (loss: 0.03148442506790161, acc: 0.9890965819358826)
[2025-02-13 02:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01][root][INFO] - Training Epoch: 1/2, step 1356/7134 completed (loss: 0.04687460511922836, acc: 0.9868420958518982)
[2025-02-13 02:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01][root][INFO] - Training Epoch: 1/2, step 1357/7134 completed (loss: 0.021161818876862526, acc: 0.994955837726593)
[2025-02-13 02:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01][root][INFO] - Training Epoch: 1/2, step 1358/7134 completed (loss: 0.05599473789334297, acc: 0.9818181991577148)
[2025-02-13 02:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:02][root][INFO] - Training Epoch: 1/2, step 1359/7134 completed (loss: 0.04406527057290077, acc: 0.9878542423248291)
[2025-02-13 02:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:02][root][INFO] - Training Epoch: 1/2, step 1360/7134 completed (loss: 0.048188213258981705, acc: 0.9852150678634644)
[2025-02-13 02:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:03][root][INFO] - Training Epoch: 1/2, step 1361/7134 completed (loss: 0.053023725748062134, acc: 0.9883211851119995)
[2025-02-13 02:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:03][root][INFO] - Training Epoch: 1/2, step 1362/7134 completed (loss: 0.0567546971142292, acc: 0.9808743000030518)
[2025-02-13 02:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:04][root][INFO] - Training Epoch: 1/2, step 1363/7134 completed (loss: 0.04065096005797386, acc: 0.9865951538085938)
[2025-02-13 02:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:04][root][INFO] - Training Epoch: 1/2, step 1364/7134 completed (loss: 0.03809265047311783, acc: 0.9900000095367432)
[2025-02-13 02:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05][root][INFO] - Training Epoch: 1/2, step 1365/7134 completed (loss: 0.039849959313869476, acc: 0.9871794581413269)
[2025-02-13 02:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05][root][INFO] - Training Epoch: 1/2, step 1366/7134 completed (loss: 0.04540395364165306, acc: 0.9857142567634583)
[2025-02-13 02:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05][root][INFO] - Training Epoch: 1/2, step 1367/7134 completed (loss: 0.04710985720157623, acc: 0.9879310131072998)
[2025-02-13 02:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:06][root][INFO] - Training Epoch: 1/2, step 1368/7134 completed (loss: 0.08536726236343384, acc: 0.9725610017776489)
[2025-02-13 02:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:06][root][INFO] - Training Epoch: 1/2, step 1369/7134 completed (loss: 0.07362129539251328, acc: 0.985989511013031)
[2025-02-13 02:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:07][root][INFO] - Training Epoch: 1/2, step 1370/7134 completed (loss: 0.07046903669834137, acc: 0.982300877571106)
[2025-02-13 02:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:07][root][INFO] - Training Epoch: 1/2, step 1371/7134 completed (loss: 0.0473385825753212, acc: 0.983988344669342)
[2025-02-13 02:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08][root][INFO] - Training Epoch: 1/2, step 1372/7134 completed (loss: 0.16152995824813843, acc: 0.9700374603271484)
[2025-02-13 02:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08][root][INFO] - Training Epoch: 1/2, step 1373/7134 completed (loss: 0.06579618155956268, acc: 0.9803149700164795)
[2025-02-13 02:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08][root][INFO] - Training Epoch: 1/2, step 1374/7134 completed (loss: 0.0815993994474411, acc: 0.9742574095726013)
[2025-02-13 02:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09][root][INFO] - Training Epoch: 1/2, step 1375/7134 completed (loss: 0.05687055364251137, acc: 0.9855769276618958)
[2025-02-13 02:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09][root][INFO] - Training Epoch: 1/2, step 1376/7134 completed (loss: 0.10987012833356857, acc: 0.9603340029716492)
[2025-02-13 02:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10][root][INFO] - Training Epoch: 1/2, step 1377/7134 completed (loss: 0.0595376193523407, acc: 0.9858823418617249)
[2025-02-13 02:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10][root][INFO] - Training Epoch: 1/2, step 1378/7134 completed (loss: 0.12998290359973907, acc: 0.961013674736023)
[2025-02-13 02:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10][root][INFO] - Training Epoch: 1/2, step 1379/7134 completed (loss: 0.09223896265029907, acc: 0.9655913710594177)
[2025-02-13 02:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:11][root][INFO] - Training Epoch: 1/2, step 1380/7134 completed (loss: 0.10630878806114197, acc: 0.9738317728042603)
[2025-02-13 02:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:11][root][INFO] - Training Epoch: 1/2, step 1381/7134 completed (loss: 0.0920681282877922, acc: 0.9751243591308594)
[2025-02-13 02:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12][root][INFO] - Training Epoch: 1/2, step 1382/7134 completed (loss: 0.10548147559165955, acc: 0.9666666388511658)
[2025-02-13 02:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12][root][INFO] - Training Epoch: 1/2, step 1383/7134 completed (loss: 0.054002467542886734, acc: 0.9862306118011475)
[2025-02-13 02:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12][root][INFO] - Training Epoch: 1/2, step 1384/7134 completed (loss: 0.17587809264659882, acc: 0.9517470598220825)
[2025-02-13 02:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:13][root][INFO] - Training Epoch: 1/2, step 1385/7134 completed (loss: 0.14927546679973602, acc: 0.9669724702835083)
[2025-02-13 02:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:13][root][INFO] - Training Epoch: 1/2, step 1386/7134 completed (loss: 0.07090316712856293, acc: 0.9719763994216919)
[2025-02-13 02:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:14][root][INFO] - Training Epoch: 1/2, step 1387/7134 completed (loss: 0.09992638975381851, acc: 0.9742120504379272)
[2025-02-13 02:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:14][root][INFO] - Training Epoch: 1/2, step 1388/7134 completed (loss: 0.08632174134254456, acc: 0.9795570969581604)
[2025-02-13 02:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15][root][INFO] - Training Epoch: 1/2, step 1389/7134 completed (loss: 0.0847511887550354, acc: 0.9724473357200623)
[2025-02-13 02:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15][root][INFO] - Training Epoch: 1/2, step 1390/7134 completed (loss: 0.10487788170576096, acc: 0.9719188809394836)
[2025-02-13 02:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15][root][INFO] - Training Epoch: 1/2, step 1391/7134 completed (loss: 0.10884248465299606, acc: 0.9736379384994507)
[2025-02-13 02:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:16][root][INFO] - Training Epoch: 1/2, step 1392/7134 completed (loss: 0.07889479398727417, acc: 0.9775086641311646)
[2025-02-13 02:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:16][root][INFO] - Training Epoch: 1/2, step 1393/7134 completed (loss: 0.10108254849910736, acc: 0.9677419066429138)
[2025-02-13 02:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17][root][INFO] - Training Epoch: 1/2, step 1394/7134 completed (loss: 0.06791964173316956, acc: 0.9801587462425232)
[2025-02-13 02:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17][root][INFO] - Training Epoch: 1/2, step 1395/7134 completed (loss: 0.07661699503660202, acc: 0.9802761077880859)
[2025-02-13 02:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17][root][INFO] - Training Epoch: 1/2, step 1396/7134 completed (loss: 0.10783746093511581, acc: 0.962837815284729)
[2025-02-13 02:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:18][root][INFO] - Training Epoch: 1/2, step 1397/7134 completed (loss: 0.13319866359233856, acc: 0.9679595232009888)
[2025-02-13 02:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:18][root][INFO] - Training Epoch: 1/2, step 1398/7134 completed (loss: 0.09305298328399658, acc: 0.9736308455467224)
[2025-02-13 02:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19][root][INFO] - Training Epoch: 1/2, step 1399/7134 completed (loss: 0.15231621265411377, acc: 0.9692028760910034)
[2025-02-13 02:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19][root][INFO] - Training Epoch: 1/2, step 1400/7134 completed (loss: 0.06655026227235794, acc: 0.9825479984283447)
[2025-02-13 02:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19][root][INFO] - Training Epoch: 1/2, step 1401/7134 completed (loss: 0.06564794480800629, acc: 0.9818781018257141)
[2025-02-13 02:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:20][root][INFO] - Training Epoch: 1/2, step 1402/7134 completed (loss: 0.04046947881579399, acc: 0.9948186278343201)
[2025-02-13 02:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:20][root][INFO] - Training Epoch: 1/2, step 1403/7134 completed (loss: 0.07182014733552933, acc: 0.9799196720123291)
[2025-02-13 02:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:21][root][INFO] - Training Epoch: 1/2, step 1404/7134 completed (loss: 0.041431352496147156, acc: 0.9893898963928223)
[2025-02-13 02:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:21][root][INFO] - Training Epoch: 1/2, step 1405/7134 completed (loss: 0.07402192056179047, acc: 0.9800570011138916)
[2025-02-13 02:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22][root][INFO] - Training Epoch: 1/2, step 1406/7134 completed (loss: 0.023317400366067886, acc: 0.9942528605461121)
[2025-02-13 02:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22][root][INFO] - Training Epoch: 1/2, step 1407/7134 completed (loss: 0.03916383907198906, acc: 0.9882659912109375)
[2025-02-13 02:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22][root][INFO] - Training Epoch: 1/2, step 1408/7134 completed (loss: 0.06622745096683502, acc: 0.9775640964508057)
[2025-02-13 02:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:23][root][INFO] - Training Epoch: 1/2, step 1409/7134 completed (loss: 0.03575119003653526, acc: 0.9880775213241577)
[2025-02-13 02:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:23][root][INFO] - Training Epoch: 1/2, step 1410/7134 completed (loss: 0.06036399304866791, acc: 0.9869791865348816)
[2025-02-13 02:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:24][root][INFO] - Training Epoch: 1/2, step 1411/7134 completed (loss: 0.026943784207105637, acc: 0.9893617033958435)
[2025-02-13 02:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:24][root][INFO] - Training Epoch: 1/2, step 1412/7134 completed (loss: 0.05000007897615433, acc: 0.9881235361099243)
[2025-02-13 02:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:25][root][INFO] - Training Epoch: 1/2, step 1413/7134 completed (loss: 0.05008146911859512, acc: 0.9870503544807434)
[2025-02-13 02:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:25][root][INFO] - Training Epoch: 1/2, step 1414/7134 completed (loss: 0.04578826203942299, acc: 0.9875389337539673)
[2025-02-13 02:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26][root][INFO] - Training Epoch: 1/2, step 1415/7134 completed (loss: 0.10917085409164429, acc: 0.9735099077224731)
[2025-02-13 02:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26][root][INFO] - Training Epoch: 1/2, step 1416/7134 completed (loss: 0.04520450159907341, acc: 0.9887640476226807)
[2025-02-13 02:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26][root][INFO] - Training Epoch: 1/2, step 1417/7134 completed (loss: 0.03605664148926735, acc: 0.9914966225624084)
[2025-02-13 02:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:27][root][INFO] - Training Epoch: 1/2, step 1418/7134 completed (loss: 0.025346342474222183, acc: 0.9923664331436157)
[2025-02-13 02:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:27][root][INFO] - Training Epoch: 1/2, step 1419/7134 completed (loss: 0.03911126032471657, acc: 0.9905533194541931)
[2025-02-13 02:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:28][root][INFO] - Training Epoch: 1/2, step 1420/7134 completed (loss: 0.04299630597233772, acc: 0.9892183542251587)
[2025-02-13 02:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:28][root][INFO] - Training Epoch: 1/2, step 1421/7134 completed (loss: 0.04711167514324188, acc: 0.9839743375778198)
[2025-02-13 02:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29][root][INFO] - Training Epoch: 1/2, step 1422/7134 completed (loss: 0.12089785188436508, acc: 0.9672364592552185)
[2025-02-13 02:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29][root][INFO] - Training Epoch: 1/2, step 1423/7134 completed (loss: 0.11079372465610504, acc: 0.9755101799964905)
[2025-02-13 02:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29][root][INFO] - Training Epoch: 1/2, step 1424/7134 completed (loss: 0.14172418415546417, acc: 0.9524714946746826)
[2025-02-13 02:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:30][root][INFO] - Training Epoch: 1/2, step 1425/7134 completed (loss: 0.054876916110515594, acc: 0.9821162223815918)
[2025-02-13 02:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:30][root][INFO] - Training Epoch: 1/2, step 1426/7134 completed (loss: 0.08256598562002182, acc: 0.9768637418746948)
[2025-02-13 02:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:31][root][INFO] - Training Epoch: 1/2, step 1427/7134 completed (loss: 0.08910579979419708, acc: 0.9719029664993286)
[2025-02-13 02:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:31][root][INFO] - Training Epoch: 1/2, step 1428/7134 completed (loss: 0.36748814582824707, acc: 0.9123867154121399)
[2025-02-13 02:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32][root][INFO] - Training Epoch: 1/2, step 1429/7134 completed (loss: 0.05083182081580162, acc: 0.9854469895362854)
[2025-02-13 02:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32][root][INFO] - Training Epoch: 1/2, step 1430/7134 completed (loss: 0.11074615269899368, acc: 0.9686346650123596)
[2025-02-13 02:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32][root][INFO] - Training Epoch: 1/2, step 1431/7134 completed (loss: 0.08591097593307495, acc: 0.9701492786407471)
[2025-02-13 02:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:33][root][INFO] - Training Epoch: 1/2, step 1432/7134 completed (loss: 0.08638685941696167, acc: 0.9768339991569519)
[2025-02-13 02:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:33][root][INFO] - Training Epoch: 1/2, step 1433/7134 completed (loss: 0.11205462366342545, acc: 0.9659686088562012)
[2025-02-13 02:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:34][root][INFO] - Training Epoch: 1/2, step 1434/7134 completed (loss: 0.11493043601512909, acc: 0.9651941061019897)
[2025-02-13 02:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:34][root][INFO] - Training Epoch: 1/2, step 1435/7134 completed (loss: 0.10876839607954025, acc: 0.9662756323814392)
[2025-02-13 02:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:35][root][INFO] - Training Epoch: 1/2, step 1436/7134 completed (loss: 0.14004144072532654, acc: 0.9619289636611938)
[2025-02-13 02:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:35][root][INFO] - Training Epoch: 1/2, step 1437/7134 completed (loss: 0.09716420620679855, acc: 0.976323127746582)
[2025-02-13 02:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36][root][INFO] - Training Epoch: 1/2, step 1438/7134 completed (loss: 0.14113947749137878, acc: 0.9693430662155151)
[2025-02-13 02:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36][root][INFO] - Training Epoch: 1/2, step 1439/7134 completed (loss: 0.12015590071678162, acc: 0.9684600830078125)
[2025-02-13 02:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36][root][INFO] - Training Epoch: 1/2, step 1440/7134 completed (loss: 0.18042075634002686, acc: 0.9535284042358398)
[2025-02-13 02:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:37][root][INFO] - Training Epoch: 1/2, step 1441/7134 completed (loss: 0.08214861899614334, acc: 0.9794721603393555)
[2025-02-13 02:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:37][root][INFO] - Training Epoch: 1/2, step 1442/7134 completed (loss: 0.06758284568786621, acc: 0.980861246585846)
[2025-02-13 02:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:38][root][INFO] - Training Epoch: 1/2, step 1443/7134 completed (loss: 0.12915821373462677, acc: 0.9630931615829468)
[2025-02-13 02:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:38][root][INFO] - Training Epoch: 1/2, step 1444/7134 completed (loss: 0.07894086092710495, acc: 0.9727626442909241)
[2025-02-13 02:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39][root][INFO] - Training Epoch: 1/2, step 1445/7134 completed (loss: 0.11522084474563599, acc: 0.9667221307754517)
[2025-02-13 02:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39][root][INFO] - Training Epoch: 1/2, step 1446/7134 completed (loss: 0.08077318966388702, acc: 0.9729729890823364)
[2025-02-13 02:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39][root][INFO] - Training Epoch: 1/2, step 1447/7134 completed (loss: 0.09851997345685959, acc: 0.9659574627876282)
[2025-02-13 02:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:40][root][INFO] - Training Epoch: 1/2, step 1448/7134 completed (loss: 0.14649857580661774, acc: 0.9512711763381958)
[2025-02-13 02:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:40][root][INFO] - Training Epoch: 1/2, step 1449/7134 completed (loss: 0.1366056203842163, acc: 0.966292142868042)
[2025-02-13 02:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41][root][INFO] - Training Epoch: 1/2, step 1450/7134 completed (loss: 0.07858465611934662, acc: 0.9782293438911438)
[2025-02-13 02:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41][root][INFO] - Training Epoch: 1/2, step 1451/7134 completed (loss: 0.07226718217134476, acc: 0.9751098155975342)
[2025-02-13 02:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41][root][INFO] - Training Epoch: 1/2, step 1452/7134 completed (loss: 0.10832001268863678, acc: 0.9791666865348816)
[2025-02-13 02:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:42][root][INFO] - Training Epoch: 1/2, step 1453/7134 completed (loss: 0.11565843969583511, acc: 0.9683042764663696)
[2025-02-13 02:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:42][root][INFO] - Training Epoch: 1/2, step 1454/7134 completed (loss: 0.13583411276340485, acc: 0.9636608362197876)
[2025-02-13 02:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43][root][INFO] - Training Epoch: 1/2, step 1455/7134 completed (loss: 0.11102032661437988, acc: 0.9706422090530396)
[2025-02-13 02:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43][root][INFO] - Training Epoch: 1/2, step 1456/7134 completed (loss: 0.06516686826944351, acc: 0.9868995547294617)
[2025-02-13 02:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43][root][INFO] - Training Epoch: 1/2, step 1457/7134 completed (loss: 0.06132437661290169, acc: 0.9813953638076782)
[2025-02-13 02:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:44][root][INFO] - Training Epoch: 1/2, step 1458/7134 completed (loss: 0.0624658465385437, acc: 0.9852724671363831)
[2025-02-13 02:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:44][root][INFO] - Training Epoch: 1/2, step 1459/7134 completed (loss: 0.0743027850985527, acc: 0.980461835861206)
[2025-02-13 02:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:45][root][INFO] - Training Epoch: 1/2, step 1460/7134 completed (loss: 0.13018471002578735, acc: 0.9684210419654846)
[2025-02-13 02:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:45][root][INFO] - Training Epoch: 1/2, step 1461/7134 completed (loss: 0.09592199325561523, acc: 0.9712918400764465)
[2025-02-13 02:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46][root][INFO] - Training Epoch: 1/2, step 1462/7134 completed (loss: 0.15980833768844604, acc: 0.9583333134651184)
[2025-02-13 02:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46][root][INFO] - Training Epoch: 1/2, step 1463/7134 completed (loss: 0.1461898237466812, acc: 0.9581589698791504)
[2025-02-13 02:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46][root][INFO] - Training Epoch: 1/2, step 1464/7134 completed (loss: 0.18659113347530365, acc: 0.954954981803894)
[2025-02-13 02:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:47][root][INFO] - Training Epoch: 1/2, step 1465/7134 completed (loss: 0.10152346640825272, acc: 0.9780564308166504)
[2025-02-13 02:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:47][root][INFO] - Training Epoch: 1/2, step 1466/7134 completed (loss: 0.05668921023607254, acc: 0.9886685609817505)
[2025-02-13 02:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:48][root][INFO] - Training Epoch: 1/2, step 1467/7134 completed (loss: 0.046869952231645584, acc: 0.9870316982269287)
[2025-02-13 02:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:48][root][INFO] - Training Epoch: 1/2, step 1468/7134 completed (loss: 0.06803742796182632, acc: 0.9775429368019104)
[2025-02-13 02:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49][root][INFO] - Training Epoch: 1/2, step 1469/7134 completed (loss: 0.043304529041051865, acc: 0.9892328381538391)
[2025-02-13 02:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49][root][INFO] - Training Epoch: 1/2, step 1470/7134 completed (loss: 0.09087593108415604, acc: 0.9682080745697021)
[2025-02-13 02:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49][root][INFO] - Training Epoch: 1/2, step 1471/7134 completed (loss: 0.11281362920999527, acc: 0.9698046445846558)
[2025-02-13 02:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:50][root][INFO] - Training Epoch: 1/2, step 1472/7134 completed (loss: 0.055806100368499756, acc: 0.9861634969711304)
[2025-02-13 02:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:50][root][INFO] - Training Epoch: 1/2, step 1473/7134 completed (loss: 0.08799475431442261, acc: 0.9762611389160156)
[2025-02-13 02:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:51][root][INFO] - Training Epoch: 1/2, step 1474/7134 completed (loss: 0.11824335157871246, acc: 0.968120813369751)
[2025-02-13 02:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:51][root][INFO] - Training Epoch: 1/2, step 1475/7134 completed (loss: 0.12605337798595428, acc: 0.9709762334823608)
[2025-02-13 02:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52][root][INFO] - Training Epoch: 1/2, step 1476/7134 completed (loss: 0.08286212384700775, acc: 0.9736024737358093)
[2025-02-13 02:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52][root][INFO] - Training Epoch: 1/2, step 1477/7134 completed (loss: 0.08951842039823532, acc: 0.9720767736434937)
[2025-02-13 02:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52][root][INFO] - Training Epoch: 1/2, step 1478/7134 completed (loss: 0.06364431232213974, acc: 0.9824817776679993)
[2025-02-13 02:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:53][root][INFO] - Training Epoch: 1/2, step 1479/7134 completed (loss: 0.09956087172031403, acc: 0.9741379022598267)
[2025-02-13 02:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:53][root][INFO] - Training Epoch: 1/2, step 1480/7134 completed (loss: 0.046242740005254745, acc: 0.984415590763092)
[2025-02-13 02:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:54][root][INFO] - Training Epoch: 1/2, step 1481/7134 completed (loss: 0.06031445413827896, acc: 0.9894737005233765)
[2025-02-13 02:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:54][root][INFO] - Training Epoch: 1/2, step 1482/7134 completed (loss: 0.07526577264070511, acc: 0.9815340638160706)
[2025-02-13 02:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55][root][INFO] - Training Epoch: 1/2, step 1483/7134 completed (loss: 0.04721022769808769, acc: 0.9842022061347961)
[2025-02-13 02:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55][root][INFO] - Training Epoch: 1/2, step 1484/7134 completed (loss: 0.10552909970283508, acc: 0.9725433588027954)
[2025-02-13 02:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55][root][INFO] - Training Epoch: 1/2, step 1485/7134 completed (loss: 0.061524152755737305, acc: 0.984829306602478)
[2025-02-13 02:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:56][root][INFO] - Training Epoch: 1/2, step 1486/7134 completed (loss: 0.07181882113218307, acc: 0.9837618470191956)
[2025-02-13 02:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:56][root][INFO] - Training Epoch: 1/2, step 1487/7134 completed (loss: 0.06068607047200203, acc: 0.9790849685668945)
[2025-02-13 02:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:57][root][INFO] - Training Epoch: 1/2, step 1488/7134 completed (loss: 0.08437664806842804, acc: 0.9743315577507019)
[2025-02-13 02:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:57][root][INFO] - Training Epoch: 1/2, step 1489/7134 completed (loss: 0.09106642007827759, acc: 0.9766355156898499)
[2025-02-13 02:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:58][root][INFO] - Training Epoch: 1/2, step 1490/7134 completed (loss: 0.05212206020951271, acc: 0.9894179701805115)
[2025-02-13 02:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:58][root][INFO] - Training Epoch: 1/2, step 1491/7134 completed (loss: 0.05125166475772858, acc: 0.9885057210922241)
[2025-02-13 02:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59][root][INFO] - Training Epoch: 1/2, step 1492/7134 completed (loss: 0.08509264886379242, acc: 0.9802784323692322)
[2025-02-13 02:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59][root][INFO] - Training Epoch: 1/2, step 1493/7134 completed (loss: 0.08151793479919434, acc: 0.975806474685669)
[2025-02-13 02:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59][root][INFO] - Training Epoch: 1/2, step 1494/7134 completed (loss: 0.05705331638455391, acc: 0.985401451587677)
[2025-02-13 02:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:00][root][INFO] - Training Epoch: 1/2, step 1495/7134 completed (loss: 0.05743769183754921, acc: 0.9844311475753784)
[2025-02-13 02:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:00][root][INFO] - Training Epoch: 1/2, step 1496/7134 completed (loss: 0.086618572473526, acc: 0.9786324501037598)
[2025-02-13 02:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:01][root][INFO] - Training Epoch: 1/2, step 1497/7134 completed (loss: 0.05066433548927307, acc: 0.9852941036224365)
[2025-02-13 02:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:01][root][INFO] - Training Epoch: 1/2, step 1498/7134 completed (loss: 0.0519879050552845, acc: 0.9894598126411438)
[2025-02-13 02:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:02][root][INFO] - Training Epoch: 1/2, step 1499/7134 completed (loss: 0.04665766656398773, acc: 0.9869513511657715)
[2025-02-13 02:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:02][root][INFO] - Training Epoch: 1/2, step 1500/7134 completed (loss: 0.0657409057021141, acc: 0.9817880988121033)
[2025-02-13 02:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03][root][INFO] - Training Epoch: 1/2, step 1501/7134 completed (loss: 0.0771222785115242, acc: 0.9793510437011719)
[2025-02-13 02:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03][root][INFO] - Training Epoch: 1/2, step 1502/7134 completed (loss: 0.06564945727586746, acc: 0.9864712357521057)
[2025-02-13 02:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03][root][INFO] - Training Epoch: 1/2, step 1503/7134 completed (loss: 0.11217796802520752, acc: 0.9744318127632141)
[2025-02-13 02:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:04][root][INFO] - Training Epoch: 1/2, step 1504/7134 completed (loss: 0.039833154529333115, acc: 0.9899371266365051)
[2025-02-13 02:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:04][root][INFO] - Training Epoch: 1/2, step 1505/7134 completed (loss: 0.05800265446305275, acc: 0.9781931638717651)
[2025-02-13 02:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:05][root][INFO] - Training Epoch: 1/2, step 1506/7134 completed (loss: 0.1260114461183548, acc: 0.9680851101875305)
[2025-02-13 02:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:05][root][INFO] - Training Epoch: 1/2, step 1507/7134 completed (loss: 0.050639547407627106, acc: 0.9869109988212585)
[2025-02-13 02:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06][root][INFO] - Training Epoch: 1/2, step 1508/7134 completed (loss: 0.05041135847568512, acc: 0.9824945330619812)
[2025-02-13 02:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06][root][INFO] - Training Epoch: 1/2, step 1509/7134 completed (loss: 0.036018241196870804, acc: 0.9887955188751221)
[2025-02-13 02:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06][root][INFO] - Training Epoch: 1/2, step 1510/7134 completed (loss: 0.28791284561157227, acc: 0.9599999785423279)
[2025-02-13 02:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:07][root][INFO] - Training Epoch: 1/2, step 1511/7134 completed (loss: 0.1455758661031723, acc: 0.9602836966514587)
[2025-02-13 02:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:07][root][INFO] - Training Epoch: 1/2, step 1512/7134 completed (loss: 0.07007680088281631, acc: 0.9817351698875427)
[2025-02-13 02:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:08][root][INFO] - Training Epoch: 1/2, step 1513/7134 completed (loss: 0.03846703842282295, acc: 0.9907407164573669)
[2025-02-13 02:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:08][root][INFO] - Training Epoch: 1/2, step 1514/7134 completed (loss: 0.07855076342821121, acc: 0.9813664555549622)
[2025-02-13 02:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09][root][INFO] - Training Epoch: 1/2, step 1515/7134 completed (loss: 0.12348451465368271, acc: 0.9649532437324524)
[2025-02-13 02:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09][root][INFO] - Training Epoch: 1/2, step 1516/7134 completed (loss: 0.12681537866592407, acc: 0.9601139426231384)
[2025-02-13 02:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09][root][INFO] - Training Epoch: 1/2, step 1517/7134 completed (loss: 0.12098609656095505, acc: 0.9600840210914612)
[2025-02-13 02:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:10][root][INFO] - Training Epoch: 1/2, step 1518/7134 completed (loss: 0.08412528038024902, acc: 0.9759036302566528)
[2025-02-13 02:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:10][root][INFO] - Training Epoch: 1/2, step 1519/7134 completed (loss: 0.07770945876836777, acc: 0.9746543765068054)
[2025-02-13 02:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11][root][INFO] - Training Epoch: 1/2, step 1520/7134 completed (loss: 0.1030198186635971, acc: 0.9734789133071899)
[2025-02-13 02:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11][root][INFO] - Training Epoch: 1/2, step 1521/7134 completed (loss: 0.1759728044271469, acc: 0.9441340565681458)
[2025-02-13 02:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11][root][INFO] - Training Epoch: 1/2, step 1522/7134 completed (loss: 0.20671895146369934, acc: 0.9460501074790955)
[2025-02-13 02:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:12][root][INFO] - Training Epoch: 1/2, step 1523/7134 completed (loss: 0.2157672792673111, acc: 0.9355322122573853)
[2025-02-13 02:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:12][root][INFO] - Training Epoch: 1/2, step 1524/7134 completed (loss: 0.0776425153017044, acc: 0.9798657894134521)
[2025-02-13 02:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:13][root][INFO] - Training Epoch: 1/2, step 1525/7134 completed (loss: 0.10364031046628952, acc: 0.9683257937431335)
[2025-02-13 02:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:13][root][INFO] - Training Epoch: 1/2, step 1526/7134 completed (loss: 0.14434830844402313, acc: 0.9535558819770813)
[2025-02-13 02:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:14][root][INFO] - Training Epoch: 1/2, step 1527/7134 completed (loss: 0.22358085215091705, acc: 0.9365609288215637)
[2025-02-13 02:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:14][root][INFO] - Training Epoch: 1/2, step 1528/7134 completed (loss: 0.16841638088226318, acc: 0.9533011317253113)
[2025-02-13 02:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15][root][INFO] - Training Epoch: 1/2, step 1529/7134 completed (loss: 0.11149933934211731, acc: 0.9619500637054443)
[2025-02-13 02:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15][root][INFO] - Training Epoch: 1/2, step 1530/7134 completed (loss: 0.10390729457139969, acc: 0.971319317817688)
[2025-02-13 02:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15][root][INFO] - Training Epoch: 1/2, step 1531/7134 completed (loss: 0.2567809224128723, acc: 0.9414893388748169)
[2025-02-13 02:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:16][root][INFO] - Training Epoch: 1/2, step 1532/7134 completed (loss: 0.06884635239839554, acc: 0.9768785834312439)
[2025-02-13 02:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:16][root][INFO] - Training Epoch: 1/2, step 1533/7134 completed (loss: 0.08970478177070618, acc: 0.9737206101417542)
[2025-02-13 02:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:17][root][INFO] - Training Epoch: 1/2, step 1534/7134 completed (loss: 0.08006501197814941, acc: 0.9764543175697327)
[2025-02-13 02:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:17][root][INFO] - Training Epoch: 1/2, step 1535/7134 completed (loss: 0.07971484959125519, acc: 0.9748502969741821)
[2025-02-13 02:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18][root][INFO] - Training Epoch: 1/2, step 1536/7134 completed (loss: 0.053324636071920395, acc: 0.9873577952384949)
[2025-02-13 02:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18][root][INFO] - Training Epoch: 1/2, step 1537/7134 completed (loss: 0.10156793892383575, acc: 0.9738219976425171)
[2025-02-13 02:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18][root][INFO] - Training Epoch: 1/2, step 1538/7134 completed (loss: 0.10218892991542816, acc: 0.9782330393791199)
[2025-02-13 02:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:19][root][INFO] - Training Epoch: 1/2, step 1539/7134 completed (loss: 0.08402568101882935, acc: 0.9749670624732971)
[2025-02-13 02:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:19][root][INFO] - Training Epoch: 1/2, step 1540/7134 completed (loss: 0.06885416805744171, acc: 0.9772117733955383)
[2025-02-13 02:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:20][root][INFO] - Training Epoch: 1/2, step 1541/7134 completed (loss: 0.10907808691263199, acc: 0.9723270535469055)
[2025-02-13 02:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:20][root][INFO] - Training Epoch: 1/2, step 1542/7134 completed (loss: 0.06362289935350418, acc: 0.9743935465812683)
[2025-02-13 02:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:21][root][INFO] - Training Epoch: 1/2, step 1543/7134 completed (loss: 0.053689248859882355, acc: 0.9836065769195557)
[2025-02-13 02:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:21][root][INFO] - Training Epoch: 1/2, step 1544/7134 completed (loss: 0.05224664509296417, acc: 0.987864077091217)
[2025-02-13 02:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:22][root][INFO] - Training Epoch: 1/2, step 1545/7134 completed (loss: 0.056570347398519516, acc: 0.9856528043746948)
[2025-02-13 02:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:22][root][INFO] - Training Epoch: 1/2, step 1546/7134 completed (loss: 0.1142774373292923, acc: 0.9719495177268982)
[2025-02-13 02:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23][root][INFO] - Training Epoch: 1/2, step 1547/7134 completed (loss: 0.0751791000366211, acc: 0.9776119589805603)
[2025-02-13 02:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23][root][INFO] - Training Epoch: 1/2, step 1548/7134 completed (loss: 0.09128591418266296, acc: 0.9761092066764832)
[2025-02-13 02:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23][root][INFO] - Training Epoch: 1/2, step 1549/7134 completed (loss: 0.057419683784246445, acc: 0.9811738729476929)
[2025-02-13 02:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:24][root][INFO] - Training Epoch: 1/2, step 1550/7134 completed (loss: 0.05650980770587921, acc: 0.9822559952735901)
[2025-02-13 02:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:24][root][INFO] - Training Epoch: 1/2, step 1551/7134 completed (loss: 0.07476866245269775, acc: 0.9753246903419495)
[2025-02-13 02:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:25][root][INFO] - Training Epoch: 1/2, step 1552/7134 completed (loss: 0.07031513005495071, acc: 0.9795258641242981)
[2025-02-13 02:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:25][root][INFO] - Training Epoch: 1/2, step 1553/7134 completed (loss: 0.052741825580596924, acc: 0.9830949306488037)
[2025-02-13 02:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:26][root][INFO] - Training Epoch: 1/2, step 1554/7134 completed (loss: 0.07713449746370316, acc: 0.9650507569313049)
[2025-02-13 02:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:26][root][INFO] - Training Epoch: 1/2, step 1555/7134 completed (loss: 0.06639819592237473, acc: 0.9820828437805176)
[2025-02-13 02:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27][root][INFO] - Training Epoch: 1/2, step 1556/7134 completed (loss: 0.07044993340969086, acc: 0.9823736548423767)
[2025-02-13 02:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27][root][INFO] - Training Epoch: 1/2, step 1557/7134 completed (loss: 0.07405383884906769, acc: 0.9742729067802429)
[2025-02-13 02:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27][root][INFO] - Training Epoch: 1/2, step 1558/7134 completed (loss: 0.07876028120517731, acc: 0.9774965047836304)
[2025-02-13 02:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:28][root][INFO] - Training Epoch: 1/2, step 1559/7134 completed (loss: 0.06058323010802269, acc: 0.9799196720123291)
[2025-02-13 02:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:28][root][INFO] - Training Epoch: 1/2, step 1560/7134 completed (loss: 0.12777939438819885, acc: 0.963151216506958)
[2025-02-13 02:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:29][root][INFO] - Training Epoch: 1/2, step 1561/7134 completed (loss: 0.1455748826265335, acc: 0.9631636142730713)
[2025-02-13 02:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:29][root][INFO] - Training Epoch: 1/2, step 1562/7134 completed (loss: 0.08529495447874069, acc: 0.9773070812225342)
[2025-02-13 02:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30][root][INFO] - Training Epoch: 1/2, step 1563/7134 completed (loss: 0.09134548157453537, acc: 0.973714292049408)
[2025-02-13 02:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30][root][INFO] - Training Epoch: 1/2, step 1564/7134 completed (loss: 0.13637788593769073, acc: 0.9641088843345642)
[2025-02-13 02:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30][root][INFO] - Training Epoch: 1/2, step 1565/7134 completed (loss: 0.09576886892318726, acc: 0.9680672287940979)
[2025-02-13 02:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:31][root][INFO] - Training Epoch: 1/2, step 1566/7134 completed (loss: 0.09488651901483536, acc: 0.9726402163505554)
[2025-02-13 02:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:31][root][INFO] - Training Epoch: 1/2, step 1567/7134 completed (loss: 0.08878348022699356, acc: 0.9763092398643494)
[2025-02-13 02:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:32][root][INFO] - Training Epoch: 1/2, step 1568/7134 completed (loss: 0.0783449187874794, acc: 0.9857988357543945)
[2025-02-13 02:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:32][root][INFO] - Training Epoch: 1/2, step 1569/7134 completed (loss: 0.10667870193719864, acc: 0.9663742780685425)
[2025-02-13 02:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:33][root][INFO] - Training Epoch: 1/2, step 1570/7134 completed (loss: 0.11315619945526123, acc: 0.9713340401649475)
[2025-02-13 02:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:33][root][INFO] - Training Epoch: 1/2, step 1571/7134 completed (loss: 0.10763447731733322, acc: 0.9679715037345886)
[2025-02-13 02:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:34][root][INFO] - Training Epoch: 1/2, step 1572/7134 completed (loss: 0.08246856182813644, acc: 0.971731424331665)
[2025-02-13 02:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:34][root][INFO] - Training Epoch: 1/2, step 1573/7134 completed (loss: 0.06340835243463516, acc: 0.9840348362922668)
[2025-02-13 02:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35][root][INFO] - Training Epoch: 1/2, step 1574/7134 completed (loss: 0.10670159757137299, acc: 0.9682713150978088)
[2025-02-13 02:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35][root][INFO] - Training Epoch: 1/2, step 1575/7134 completed (loss: 0.09147007018327713, acc: 0.9767184257507324)
[2025-02-13 02:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35][root][INFO] - Training Epoch: 1/2, step 1576/7134 completed (loss: 0.09213052690029144, acc: 0.9742990732192993)
[2025-02-13 02:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:36][root][INFO] - Training Epoch: 1/2, step 1577/7134 completed (loss: 0.08591719716787338, acc: 0.9799764156341553)
[2025-02-13 02:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:36][root][INFO] - Training Epoch: 1/2, step 1578/7134 completed (loss: 0.09126674383878708, acc: 0.97428959608078)
[2025-02-13 02:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:37][root][INFO] - Training Epoch: 1/2, step 1579/7134 completed (loss: 0.08621030300855637, acc: 0.9769585132598877)
[2025-02-13 02:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:37][root][INFO] - Training Epoch: 1/2, step 1580/7134 completed (loss: 0.12093289196491241, acc: 0.9669603705406189)
[2025-02-13 02:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38][root][INFO] - Training Epoch: 1/2, step 1581/7134 completed (loss: 0.17463473975658417, acc: 0.9537037014961243)
[2025-02-13 02:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38][root][INFO] - Training Epoch: 1/2, step 1582/7134 completed (loss: 0.1601848602294922, acc: 0.949921727180481)
[2025-02-13 02:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38][root][INFO] - Training Epoch: 1/2, step 1583/7134 completed (loss: 0.08949844539165497, acc: 0.9713466763496399)
[2025-02-13 02:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:39][root][INFO] - Training Epoch: 1/2, step 1584/7134 completed (loss: 0.08802356570959091, acc: 0.9732334017753601)
[2025-02-13 02:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:39][root][INFO] - Training Epoch: 1/2, step 1585/7134 completed (loss: 0.07862772047519684, acc: 0.9740871787071228)
[2025-02-13 02:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:40][root][INFO] - Training Epoch: 1/2, step 1586/7134 completed (loss: 0.10916119813919067, acc: 0.9679334759712219)
[2025-02-13 02:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:40][root][INFO] - Training Epoch: 1/2, step 1587/7134 completed (loss: 0.07195291668176651, acc: 0.9811617136001587)
[2025-02-13 02:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:41][root][INFO] - Training Epoch: 1/2, step 1588/7134 completed (loss: 0.07997780293226242, acc: 0.9853528738021851)
[2025-02-13 02:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:41][root][INFO] - Training Epoch: 1/2, step 1589/7134 completed (loss: 0.05809160694479942, acc: 0.986146092414856)
[2025-02-13 02:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42][root][INFO] - Training Epoch: 1/2, step 1590/7134 completed (loss: 0.06795790046453476, acc: 0.981873095035553)
[2025-02-13 02:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42][root][INFO] - Training Epoch: 1/2, step 1591/7134 completed (loss: 0.06754589080810547, acc: 0.9796954393386841)
[2025-02-13 02:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42][root][INFO] - Training Epoch: 1/2, step 1592/7134 completed (loss: 0.0719749704003334, acc: 0.980028510093689)
[2025-02-13 02:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:43][root][INFO] - Training Epoch: 1/2, step 1593/7134 completed (loss: 0.0769614726305008, acc: 0.9804597496986389)
[2025-02-13 02:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:43][root][INFO] - Training Epoch: 1/2, step 1594/7134 completed (loss: 0.0630694180727005, acc: 0.9818181991577148)
[2025-02-13 02:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44][root][INFO] - Training Epoch: 1/2, step 1595/7134 completed (loss: 0.05060090497136116, acc: 0.9874551892280579)
[2025-02-13 02:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44][root][INFO] - Training Epoch: 1/2, step 1596/7134 completed (loss: 0.1165924072265625, acc: 0.9685039520263672)
[2025-02-13 02:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45][root][INFO] - Training Epoch: 1/2, step 1597/7134 completed (loss: 0.0552542619407177, acc: 0.984240710735321)
[2025-02-13 02:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45][root][INFO] - Training Epoch: 1/2, step 1598/7134 completed (loss: 0.10390454530715942, acc: 0.9720559120178223)
[2025-02-13 02:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45][root][INFO] - Training Epoch: 1/2, step 1599/7134 completed (loss: 0.1627344787120819, acc: 0.9586777091026306)
[2025-02-13 02:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:46][root][INFO] - Training Epoch: 1/2, step 1600/7134 completed (loss: 0.09755710512399673, acc: 0.9792147874832153)
[2025-02-13 02:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:46][root][INFO] - Training Epoch: 1/2, step 1601/7134 completed (loss: 0.05278977006673813, acc: 0.9848693013191223)
[2025-02-13 02:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:47][root][INFO] - Training Epoch: 1/2, step 1602/7134 completed (loss: 0.0670228973031044, acc: 0.9924585223197937)
[2025-02-13 02:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:47][root][INFO] - Training Epoch: 1/2, step 1603/7134 completed (loss: 0.10254515707492828, acc: 0.9795396327972412)
[2025-02-13 02:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48][root][INFO] - Training Epoch: 1/2, step 1604/7134 completed (loss: 0.05726270377635956, acc: 0.9840849041938782)
[2025-02-13 02:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48][root][INFO] - Training Epoch: 1/2, step 1605/7134 completed (loss: 0.0786152184009552, acc: 0.9778357148170471)
[2025-02-13 02:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48][root][INFO] - Training Epoch: 1/2, step 1606/7134 completed (loss: 0.030223362147808075, acc: 0.9908116459846497)
[2025-02-13 02:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:49][root][INFO] - Training Epoch: 1/2, step 1607/7134 completed (loss: 0.04553784057497978, acc: 0.9857434034347534)
[2025-02-13 02:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:49][root][INFO] - Training Epoch: 1/2, step 1608/7134 completed (loss: 0.04555385932326317, acc: 0.9869888424873352)
[2025-02-13 02:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:50][root][INFO] - Training Epoch: 1/2, step 1609/7134 completed (loss: 0.0743120089173317, acc: 0.9766355156898499)
[2025-02-13 02:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:50][root][INFO] - Training Epoch: 1/2, step 1610/7134 completed (loss: 0.09383732825517654, acc: 0.9799666404724121)
[2025-02-13 02:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51][root][INFO] - Training Epoch: 1/2, step 1611/7134 completed (loss: 0.09975079447031021, acc: 0.9775132536888123)
[2025-02-13 02:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51][root][INFO] - Training Epoch: 1/2, step 1612/7134 completed (loss: 0.10054008662700653, acc: 0.9796437621116638)
[2025-02-13 02:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51][root][INFO] - Training Epoch: 1/2, step 1613/7134 completed (loss: 0.08930566906929016, acc: 0.9802371263504028)
[2025-02-13 02:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:52][root][INFO] - Training Epoch: 1/2, step 1614/7134 completed (loss: 0.09122305363416672, acc: 0.9790301322937012)
[2025-02-13 02:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:52][root][INFO] - Training Epoch: 1/2, step 1615/7134 completed (loss: 0.07936551421880722, acc: 0.9798657894134521)
[2025-02-13 02:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53][root][INFO] - Training Epoch: 1/2, step 1616/7134 completed (loss: 0.03292550891637802, acc: 0.9942280054092407)
[2025-02-13 02:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53][root][INFO] - Training Epoch: 1/2, step 1617/7134 completed (loss: 0.09248445928096771, acc: 0.9781591296195984)
[2025-02-13 02:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:54][root][INFO] - Training Epoch: 1/2, step 1618/7134 completed (loss: 0.12731212377548218, acc: 0.961403489112854)
[2025-02-13 02:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:54][root][INFO] - Training Epoch: 1/2, step 1619/7134 completed (loss: 0.08820756524801254, acc: 0.968664824962616)
[2025-02-13 02:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55][root][INFO] - Training Epoch: 1/2, step 1620/7134 completed (loss: 0.13235192000865936, acc: 0.9652042388916016)
[2025-02-13 02:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55][root][INFO] - Training Epoch: 1/2, step 1621/7134 completed (loss: 0.1129293292760849, acc: 0.9729729890823364)
[2025-02-13 02:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55][root][INFO] - Training Epoch: 1/2, step 1622/7134 completed (loss: 0.09639842808246613, acc: 0.9791086316108704)
[2025-02-13 02:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:56][root][INFO] - Training Epoch: 1/2, step 1623/7134 completed (loss: 0.09463983029127121, acc: 0.9789325594902039)
[2025-02-13 02:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:56][root][INFO] - Training Epoch: 1/2, step 1624/7134 completed (loss: 0.10926449298858643, acc: 0.9691630005836487)
[2025-02-13 02:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:57][root][INFO] - Training Epoch: 1/2, step 1625/7134 completed (loss: 0.06887950003147125, acc: 0.9857549667358398)
[2025-02-13 02:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:57][root][INFO] - Training Epoch: 1/2, step 1626/7134 completed (loss: 0.12534233927726746, acc: 0.9699074029922485)
[2025-02-13 02:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58][root][INFO] - Training Epoch: 1/2, step 1627/7134 completed (loss: 0.05423378571867943, acc: 0.985401451587677)
[2025-02-13 02:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58][root][INFO] - Training Epoch: 1/2, step 1628/7134 completed (loss: 0.027826540172100067, acc: 0.9941691160202026)
[2025-02-13 02:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58][root][INFO] - Training Epoch: 1/2, step 1629/7134 completed (loss: 0.062250081449747086, acc: 0.9800000190734863)
[2025-02-13 02:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:59][root][INFO] - Training Epoch: 1/2, step 1630/7134 completed (loss: 0.06511662900447845, acc: 0.9792592525482178)
[2025-02-13 02:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:59][root][INFO] - Training Epoch: 1/2, step 1631/7134 completed (loss: 0.08661488443613052, acc: 0.9742765426635742)
[2025-02-13 02:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00][root][INFO] - Training Epoch: 1/2, step 1632/7134 completed (loss: 0.05321968346834183, acc: 0.9817578792572021)
[2025-02-13 02:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00][root][INFO] - Training Epoch: 1/2, step 1633/7134 completed (loss: 0.04851188510656357, acc: 0.9874411225318909)
[2025-02-13 02:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00][root][INFO] - Training Epoch: 1/2, step 1634/7134 completed (loss: 0.0600096732378006, acc: 0.9831546545028687)
[2025-02-13 02:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:01][root][INFO] - Training Epoch: 1/2, step 1635/7134 completed (loss: 0.07634443044662476, acc: 0.9775967597961426)
[2025-02-13 02:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:01][root][INFO] - Training Epoch: 1/2, step 1636/7134 completed (loss: 0.08162060379981995, acc: 0.9790475964546204)
[2025-02-13 02:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:02][root][INFO] - Training Epoch: 1/2, step 1637/7134 completed (loss: 0.04181554168462753, acc: 0.9858757257461548)
[2025-02-13 02:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:02][root][INFO] - Training Epoch: 1/2, step 1638/7134 completed (loss: 0.12083014100790024, acc: 0.9677419066429138)
[2025-02-13 02:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03][root][INFO] - Training Epoch: 1/2, step 1639/7134 completed (loss: 0.1777188628911972, acc: 0.95716392993927)
[2025-02-13 02:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03][root][INFO] - Training Epoch: 1/2, step 1640/7134 completed (loss: 0.10069115459918976, acc: 0.9649389982223511)
[2025-02-13 02:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03][root][INFO] - Training Epoch: 1/2, step 1641/7134 completed (loss: 0.06797727197408676, acc: 0.9841954112052917)
[2025-02-13 02:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:04][root][INFO] - Training Epoch: 1/2, step 1642/7134 completed (loss: 0.11511635780334473, acc: 0.9697368144989014)
[2025-02-13 02:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:04][root][INFO] - Training Epoch: 1/2, step 1643/7134 completed (loss: 0.1526901125907898, acc: 0.9618717432022095)
[2025-02-13 02:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:05][root][INFO] - Training Epoch: 1/2, step 1644/7134 completed (loss: 0.11597897857427597, acc: 0.9726206064224243)
[2025-02-13 02:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:05][root][INFO] - Training Epoch: 1/2, step 1645/7134 completed (loss: 0.10756123065948486, acc: 0.977011501789093)
[2025-02-13 02:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06][root][INFO] - Training Epoch: 1/2, step 1646/7134 completed (loss: 0.12661218643188477, acc: 0.9728434681892395)
[2025-02-13 02:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06][root][INFO] - Training Epoch: 1/2, step 1647/7134 completed (loss: 0.07031452655792236, acc: 0.978723406791687)
[2025-02-13 02:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06][root][INFO] - Training Epoch: 1/2, step 1648/7134 completed (loss: 0.10839103162288666, acc: 0.977393627166748)
[2025-02-13 02:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:07][root][INFO] - Training Epoch: 1/2, step 1649/7134 completed (loss: 0.06503259390592575, acc: 0.9833518266677856)
[2025-02-13 02:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:07][root][INFO] - Training Epoch: 1/2, step 1650/7134 completed (loss: 0.09206240624189377, acc: 0.969072163105011)
[2025-02-13 02:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:08][root][INFO] - Training Epoch: 1/2, step 1651/7134 completed (loss: 0.13421370089054108, acc: 0.9646017551422119)
[2025-02-13 02:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:08][root][INFO] - Training Epoch: 1/2, step 1652/7134 completed (loss: 0.05866850167512894, acc: 0.9836423397064209)
[2025-02-13 02:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:09][root][INFO] - Training Epoch: 1/2, step 1653/7134 completed (loss: 0.07481177896261215, acc: 0.9775840640068054)
[2025-02-13 02:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:09][root][INFO] - Training Epoch: 1/2, step 1654/7134 completed (loss: 0.10851188749074936, acc: 0.9745157957077026)
[2025-02-13 02:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:10][root][INFO] - Training Epoch: 1/2, step 1655/7134 completed (loss: 0.08894234895706177, acc: 0.9764559864997864)
[2025-02-13 02:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:10][root][INFO] - Training Epoch: 1/2, step 1656/7134 completed (loss: 0.101343534886837, acc: 0.9729729890823364)
[2025-02-13 02:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11][root][INFO] - Training Epoch: 1/2, step 1657/7134 completed (loss: 0.0924689993262291, acc: 0.9698629975318909)
[2025-02-13 02:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11][root][INFO] - Training Epoch: 1/2, step 1658/7134 completed (loss: 0.07525880634784698, acc: 0.9755101799964905)
[2025-02-13 02:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:12][root][INFO] - Training Epoch: 1/2, step 1659/7134 completed (loss: 0.10027870535850525, acc: 0.9725118279457092)
[2025-02-13 02:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:12][root][INFO] - Training Epoch: 1/2, step 1660/7134 completed (loss: 0.08905858546495438, acc: 0.979468584060669)
[2025-02-13 02:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:13][root][INFO] - Training Epoch: 1/2, step 1661/7134 completed (loss: 0.10497333854436874, acc: 0.9715536236763)
[2025-02-13 02:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:13][root][INFO] - Training Epoch: 1/2, step 1662/7134 completed (loss: 0.09055083990097046, acc: 0.9766702055931091)
[2025-02-13 02:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14][root][INFO] - Training Epoch: 1/2, step 1663/7134 completed (loss: 0.06673810631036758, acc: 0.9825282692909241)
[2025-02-13 02:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14][root][INFO] - Training Epoch: 1/2, step 1664/7134 completed (loss: 0.06659799069166183, acc: 0.9809296727180481)
[2025-02-13 02:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14][root][INFO] - Training Epoch: 1/2, step 1665/7134 completed (loss: 0.08340855687856674, acc: 0.9827337861061096)
[2025-02-13 02:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:15][root][INFO] - Training Epoch: 1/2, step 1666/7134 completed (loss: 0.07804583013057709, acc: 0.9750499129295349)
[2025-02-13 02:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:15][root][INFO] - Training Epoch: 1/2, step 1667/7134 completed (loss: 0.16536031663417816, acc: 0.9501187801361084)
[2025-02-13 02:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:16][root][INFO] - Training Epoch: 1/2, step 1668/7134 completed (loss: 0.06458035856485367, acc: 0.9821428656578064)
[2025-02-13 02:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:16][root][INFO] - Training Epoch: 1/2, step 1669/7134 completed (loss: 0.07514791935682297, acc: 0.9783427715301514)
[2025-02-13 02:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:17][root][INFO] - Training Epoch: 1/2, step 1670/7134 completed (loss: 0.09514964371919632, acc: 0.9751908183097839)
[2025-02-13 02:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:17][root][INFO] - Training Epoch: 1/2, step 1671/7134 completed (loss: 0.09208764135837555, acc: 0.9701313972473145)
[2025-02-13 02:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:18][root][INFO] - Training Epoch: 1/2, step 1672/7134 completed (loss: 0.1473245471715927, acc: 0.9521912336349487)
[2025-02-13 02:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:18][root][INFO] - Training Epoch: 1/2, step 1673/7134 completed (loss: 0.11773623526096344, acc: 0.9690860509872437)
[2025-02-13 02:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:19][root][INFO] - Training Epoch: 1/2, step 1674/7134 completed (loss: 0.10247055441141129, acc: 0.9720670580863953)
[2025-02-13 02:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:19][root][INFO] - Training Epoch: 1/2, step 1675/7134 completed (loss: 0.16492165625095367, acc: 0.9578231573104858)
[2025-02-13 02:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:20][root][INFO] - Training Epoch: 1/2, step 1676/7134 completed (loss: 0.08557986468076706, acc: 0.9737569093704224)
[2025-02-13 02:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:20][root][INFO] - Training Epoch: 1/2, step 1677/7134 completed (loss: 0.1391785889863968, acc: 0.9607201218605042)
[2025-02-13 02:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21][root][INFO] - Training Epoch: 1/2, step 1678/7134 completed (loss: 0.058072615414857864, acc: 0.983561635017395)
[2025-02-13 02:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21][root][INFO] - Training Epoch: 1/2, step 1679/7134 completed (loss: 0.13002459704875946, acc: 0.9707174301147461)
[2025-02-13 02:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21][root][INFO] - Training Epoch: 1/2, step 1680/7134 completed (loss: 0.09310658276081085, acc: 0.974530816078186)
[2025-02-13 02:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:22][root][INFO] - Training Epoch: 1/2, step 1681/7134 completed (loss: 0.0823550894856453, acc: 0.984375)
[2025-02-13 02:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:22][root][INFO] - Training Epoch: 1/2, step 1682/7134 completed (loss: 0.15245942771434784, acc: 0.9610950946807861)
[2025-02-13 02:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:23][root][INFO] - Training Epoch: 1/2, step 1683/7134 completed (loss: 0.09579343348741531, acc: 0.9724919199943542)
[2025-02-13 02:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:23][root][INFO] - Training Epoch: 1/2, step 1684/7134 completed (loss: 0.08643898367881775, acc: 0.9777397513389587)
[2025-02-13 02:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24][root][INFO] - Training Epoch: 1/2, step 1685/7134 completed (loss: 0.10215871781110764, acc: 0.9694019556045532)
[2025-02-13 02:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24][root][INFO] - Training Epoch: 1/2, step 1686/7134 completed (loss: 0.07123962789773941, acc: 0.974662184715271)
[2025-02-13 02:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24][root][INFO] - Training Epoch: 1/2, step 1687/7134 completed (loss: 0.07498003542423248, acc: 0.980988621711731)
[2025-02-13 02:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:25][root][INFO] - Training Epoch: 1/2, step 1688/7134 completed (loss: 0.07556275278329849, acc: 0.9799072742462158)
[2025-02-13 02:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:25][root][INFO] - Training Epoch: 1/2, step 1689/7134 completed (loss: 0.0714515894651413, acc: 0.9841269850730896)
[2025-02-13 02:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26][root][INFO] - Training Epoch: 1/2, step 1690/7134 completed (loss: 0.04579188674688339, acc: 0.9898374080657959)
[2025-02-13 02:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26][root][INFO] - Training Epoch: 1/2, step 1691/7134 completed (loss: 0.04877173528075218, acc: 0.9900662302970886)
[2025-02-13 02:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26][root][INFO] - Training Epoch: 1/2, step 1692/7134 completed (loss: 0.08932261914014816, acc: 0.9679595232009888)
[2025-02-13 02:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:27][root][INFO] - Training Epoch: 1/2, step 1693/7134 completed (loss: 0.05198940262198448, acc: 0.9865269660949707)
[2025-02-13 02:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:27][root][INFO] - Training Epoch: 1/2, step 1694/7134 completed (loss: 0.06011116877198219, acc: 0.9843505620956421)
[2025-02-13 02:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:28][root][INFO] - Training Epoch: 1/2, step 1695/7134 completed (loss: 0.041973184794187546, acc: 0.9891892075538635)
[2025-02-13 02:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:28][root][INFO] - Training Epoch: 1/2, step 1696/7134 completed (loss: 0.08044079691171646, acc: 0.9748743772506714)
[2025-02-13 02:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29][root][INFO] - Training Epoch: 1/2, step 1697/7134 completed (loss: 0.08267192542552948, acc: 0.9827855825424194)
[2025-02-13 02:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29][root][INFO] - Training Epoch: 1/2, step 1698/7134 completed (loss: 0.04636729136109352, acc: 0.9905956387519836)
[2025-02-13 02:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29][root][INFO] - Training Epoch: 1/2, step 1699/7134 completed (loss: 0.05885394290089607, acc: 0.9844290614128113)
[2025-02-13 02:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:30][root][INFO] - Training Epoch: 1/2, step 1700/7134 completed (loss: 0.1096593588590622, acc: 0.9731663465499878)
[2025-02-13 02:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:30][root][INFO] - Training Epoch: 1/2, step 1701/7134 completed (loss: 0.10191988945007324, acc: 0.9680111408233643)
[2025-02-13 02:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:31][root][INFO] - Training Epoch: 1/2, step 1702/7134 completed (loss: 0.1050390899181366, acc: 0.9701863527297974)
[2025-02-13 02:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:31][root][INFO] - Training Epoch: 1/2, step 1703/7134 completed (loss: 0.11594220250844955, acc: 0.9709944725036621)
[2025-02-13 02:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32][root][INFO] - Training Epoch: 1/2, step 1704/7134 completed (loss: 0.1764964610338211, acc: 0.95652174949646)
[2025-02-13 02:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32][root][INFO] - Training Epoch: 1/2, step 1705/7134 completed (loss: 0.09724564105272293, acc: 0.9723865985870361)
[2025-02-13 02:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32][root][INFO] - Training Epoch: 1/2, step 1706/7134 completed (loss: 0.07484585046768188, acc: 0.9848484992980957)
[2025-02-13 02:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33][root][INFO] - Training Epoch: 1/2, step 1707/7134 completed (loss: 0.07985176146030426, acc: 0.9809725284576416)
[2025-02-13 02:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33][root][INFO] - Training Epoch: 1/2, step 1708/7134 completed (loss: 0.06519985944032669, acc: 0.9772357940673828)
[2025-02-13 02:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33][root][INFO] - Training Epoch: 1/2, step 1709/7134 completed (loss: 0.08410942554473877, acc: 0.9741379022598267)
[2025-02-13 02:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:34][root][INFO] - Training Epoch: 1/2, step 1710/7134 completed (loss: 0.13438363373279572, acc: 0.9626436829566956)
[2025-02-13 02:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:34][root][INFO] - Training Epoch: 1/2, step 1711/7134 completed (loss: 0.08425981551408768, acc: 0.9827288389205933)
[2025-02-13 02:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:35][root][INFO] - Training Epoch: 1/2, step 1712/7134 completed (loss: 0.14780479669570923, acc: 0.9596491456031799)
[2025-02-13 02:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:35][root][INFO] - Training Epoch: 1/2, step 1713/7134 completed (loss: 0.07023824006319046, acc: 0.9840319156646729)
[2025-02-13 02:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36][root][INFO] - Training Epoch: 1/2, step 1714/7134 completed (loss: 0.1385376900434494, acc: 0.9572368264198303)
[2025-02-13 02:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36][root][INFO] - Training Epoch: 1/2, step 1715/7134 completed (loss: 0.11743089556694031, acc: 0.9675456285476685)
[2025-02-13 02:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36][root][INFO] - Training Epoch: 1/2, step 1716/7134 completed (loss: 0.13820023834705353, acc: 0.9566666483879089)
[2025-02-13 02:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:37][root][INFO] - Training Epoch: 1/2, step 1717/7134 completed (loss: 0.06749285012483597, acc: 0.9727626442909241)
[2025-02-13 02:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:37][root][INFO] - Training Epoch: 1/2, step 1718/7134 completed (loss: 0.10237745195627213, acc: 0.9643652439117432)
[2025-02-13 02:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38][root][INFO] - Training Epoch: 1/2, step 1719/7134 completed (loss: 0.1130572035908699, acc: 0.9739726185798645)
[2025-02-13 02:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38][root][INFO] - Training Epoch: 1/2, step 1720/7134 completed (loss: 0.15867094695568085, acc: 0.9482401609420776)
[2025-02-13 02:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38][root][INFO] - Training Epoch: 1/2, step 1721/7134 completed (loss: 0.09066988527774811, acc: 0.9730185270309448)
[2025-02-13 02:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:39][root][INFO] - Training Epoch: 1/2, step 1722/7134 completed (loss: 0.07134957611560822, acc: 0.9763157963752747)
[2025-02-13 02:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:39][root][INFO] - Training Epoch: 1/2, step 1723/7134 completed (loss: 0.14189597964286804, acc: 0.9597615599632263)
[2025-02-13 02:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40][root][INFO] - Training Epoch: 1/2, step 1724/7134 completed (loss: 0.1507122665643692, acc: 0.9623233675956726)
[2025-02-13 02:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40][root][INFO] - Training Epoch: 1/2, step 1725/7134 completed (loss: 0.08427729457616806, acc: 0.971061110496521)
[2025-02-13 02:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40][root][INFO] - Training Epoch: 1/2, step 1726/7134 completed (loss: 0.11567030102014542, acc: 0.9641255736351013)
[2025-02-13 02:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:41][root][INFO] - Training Epoch: 1/2, step 1727/7134 completed (loss: 0.08014289289712906, acc: 0.9799270033836365)
[2025-02-13 02:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:41][root][INFO] - Training Epoch: 1/2, step 1728/7134 completed (loss: 0.07687969505786896, acc: 0.9834437370300293)
[2025-02-13 02:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42][root][INFO] - Training Epoch: 1/2, step 1729/7134 completed (loss: 0.10913554579019547, acc: 0.9653767943382263)
[2025-02-13 02:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42][root][INFO] - Training Epoch: 1/2, step 1730/7134 completed (loss: 0.0658811703324318, acc: 0.9850106835365295)
[2025-02-13 02:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42][root][INFO] - Training Epoch: 1/2, step 1731/7134 completed (loss: 0.14079459011554718, acc: 0.9537572264671326)
[2025-02-13 02:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43][root][INFO] - Training Epoch: 1/2, step 1732/7134 completed (loss: 0.09039907902479172, acc: 0.9712525606155396)
[2025-02-13 02:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43][root][INFO] - Training Epoch: 1/2, step 1733/7134 completed (loss: 0.12115778774023056, acc: 0.9670782089233398)
[2025-02-13 02:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43][root][INFO] - Training Epoch: 1/2, step 1734/7134 completed (loss: 0.12409896403551102, acc: 0.961916446685791)
[2025-02-13 02:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:44][root][INFO] - Training Epoch: 1/2, step 1735/7134 completed (loss: 0.04430762305855751, acc: 0.9845722317695618)
[2025-02-13 02:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:44][root][INFO] - Training Epoch: 1/2, step 1736/7134 completed (loss: 0.08994115889072418, acc: 0.9769697189331055)
[2025-02-13 02:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:45][root][INFO] - Training Epoch: 1/2, step 1737/7134 completed (loss: 0.08119289577007294, acc: 0.9789325594902039)
[2025-02-13 02:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:45][root][INFO] - Training Epoch: 1/2, step 1738/7134 completed (loss: 0.058952055871486664, acc: 0.9855247139930725)
[2025-02-13 02:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:46][root][INFO] - Training Epoch: 1/2, step 1739/7134 completed (loss: 0.07768534123897552, acc: 0.9730538725852966)
[2025-02-13 02:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:46][root][INFO] - Training Epoch: 1/2, step 1740/7134 completed (loss: 0.04861990734934807, acc: 0.9839181303977966)
[2025-02-13 02:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47][root][INFO] - Training Epoch: 1/2, step 1741/7134 completed (loss: 0.08298400789499283, acc: 0.9814814925193787)
[2025-02-13 02:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47][root][INFO] - Training Epoch: 1/2, step 1742/7134 completed (loss: 0.08482806384563446, acc: 0.9758865237236023)
[2025-02-13 02:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47][root][INFO] - Training Epoch: 1/2, step 1743/7134 completed (loss: 0.1144706979393959, acc: 0.9719887971878052)
[2025-02-13 02:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48][root][INFO] - Training Epoch: 1/2, step 1744/7134 completed (loss: 0.06512261927127838, acc: 0.9820846915245056)
[2025-02-13 02:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48][root][INFO] - Training Epoch: 1/2, step 1745/7134 completed (loss: 0.06830140203237534, acc: 0.9837092757225037)
[2025-02-13 02:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:49][root][INFO] - Training Epoch: 1/2, step 1746/7134 completed (loss: 0.04369296506047249, acc: 0.9811046719551086)
[2025-02-13 02:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:49][root][INFO] - Training Epoch: 1/2, step 1747/7134 completed (loss: 0.060617364943027496, acc: 0.9824780821800232)
[2025-02-13 02:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:50][root][INFO] - Training Epoch: 1/2, step 1748/7134 completed (loss: 0.09686576575040817, acc: 0.9689826369285583)
[2025-02-13 02:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:50][root][INFO] - Training Epoch: 1/2, step 1749/7134 completed (loss: 0.06075378134846687, acc: 0.9829738736152649)
[2025-02-13 02:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:51][root][INFO] - Training Epoch: 1/2, step 1750/7134 completed (loss: 0.07135534286499023, acc: 0.9760192036628723)
[2025-02-13 02:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:51][root][INFO] - Training Epoch: 1/2, step 1751/7134 completed (loss: 0.06590928882360458, acc: 0.9818181991577148)
[2025-02-13 02:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52][root][INFO] - Training Epoch: 1/2, step 1752/7134 completed (loss: 0.09966158121824265, acc: 0.9716494679450989)
[2025-02-13 02:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52][root][INFO] - Training Epoch: 1/2, step 1753/7134 completed (loss: 0.09708517789840698, acc: 0.9778933525085449)
[2025-02-13 02:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52][root][INFO] - Training Epoch: 1/2, step 1754/7134 completed (loss: 0.06870339065790176, acc: 0.9812679886817932)
[2025-02-13 02:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:53][root][INFO] - Training Epoch: 1/2, step 1755/7134 completed (loss: 0.06812762469053268, acc: 0.976190447807312)
[2025-02-13 02:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:53][root][INFO] - Training Epoch: 1/2, step 1756/7134 completed (loss: 0.08214221894741058, acc: 0.9765929579734802)
[2025-02-13 02:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:54][root][INFO] - Training Epoch: 1/2, step 1757/7134 completed (loss: 0.08296548575162888, acc: 0.981697142124176)
[2025-02-13 02:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:54][root][INFO] - Training Epoch: 1/2, step 1758/7134 completed (loss: 0.05483477935194969, acc: 0.9823232293128967)
[2025-02-13 02:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55][root][INFO] - Training Epoch: 1/2, step 1759/7134 completed (loss: 0.061952587217092514, acc: 0.9819193482398987)
[2025-02-13 02:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55][root][INFO] - Training Epoch: 1/2, step 1760/7134 completed (loss: 0.07182938605546951, acc: 0.9798319339752197)
[2025-02-13 02:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55][root][INFO] - Training Epoch: 1/2, step 1761/7134 completed (loss: 0.037274330854415894, acc: 0.9900568127632141)
[2025-02-13 02:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:56][root][INFO] - Training Epoch: 1/2, step 1762/7134 completed (loss: 0.06097990274429321, acc: 0.9731958508491516)
[2025-02-13 02:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:56][root][INFO] - Training Epoch: 1/2, step 1763/7134 completed (loss: 0.08630349487066269, acc: 0.9743589758872986)
[2025-02-13 02:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57][root][INFO] - Training Epoch: 1/2, step 1764/7134 completed (loss: 0.08996870368719101, acc: 0.9851301312446594)
[2025-02-13 02:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57][root][INFO] - Training Epoch: 1/2, step 1765/7134 completed (loss: 0.06239454075694084, acc: 0.9861111044883728)
[2025-02-13 02:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57][root][INFO] - Training Epoch: 1/2, step 1766/7134 completed (loss: 0.07067476958036423, acc: 0.9824047088623047)
[2025-02-13 02:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:58][root][INFO] - Training Epoch: 1/2, step 1767/7134 completed (loss: 0.1204092875123024, acc: 0.9660537242889404)
[2025-02-13 02:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:58][root][INFO] - Training Epoch: 1/2, step 1768/7134 completed (loss: 0.05474289506673813, acc: 0.9845857620239258)
[2025-02-13 02:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:59][root][INFO] - Training Epoch: 1/2, step 1769/7134 completed (loss: 0.05664779618382454, acc: 0.9808823466300964)
[2025-02-13 02:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:59][root][INFO] - Training Epoch: 1/2, step 1770/7134 completed (loss: 0.0497538223862648, acc: 0.987860381603241)
[2025-02-13 02:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00][root][INFO] - Training Epoch: 1/2, step 1771/7134 completed (loss: 0.044671643525362015, acc: 0.9902234673500061)
[2025-02-13 02:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00][root][INFO] - Training Epoch: 1/2, step 1772/7134 completed (loss: 0.05069073289632797, acc: 0.9849246144294739)
[2025-02-13 02:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00][root][INFO] - Training Epoch: 1/2, step 1773/7134 completed (loss: 0.040415916591882706, acc: 0.9898256063461304)
[2025-02-13 02:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:01][root][INFO] - Training Epoch: 1/2, step 1774/7134 completed (loss: 0.045341312885284424, acc: 0.9833887219429016)
[2025-02-13 02:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:01][root][INFO] - Training Epoch: 1/2, step 1775/7134 completed (loss: 0.0658586323261261, acc: 0.984375)
[2025-02-13 02:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02][root][INFO] - Training Epoch: 1/2, step 1776/7134 completed (loss: 0.06013965606689453, acc: 0.9835329055786133)
[2025-02-13 02:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02][root][INFO] - Training Epoch: 1/2, step 1777/7134 completed (loss: 0.04006959870457649, acc: 0.9911242723464966)
[2025-02-13 02:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02][root][INFO] - Training Epoch: 1/2, step 1778/7134 completed (loss: 0.08716431260108948, acc: 0.9807692170143127)
[2025-02-13 02:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:03][root][INFO] - Training Epoch: 1/2, step 1779/7134 completed (loss: 0.05499262735247612, acc: 0.9777777791023254)
[2025-02-13 02:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:03][root][INFO] - Training Epoch: 1/2, step 1780/7134 completed (loss: 0.05382787063717842, acc: 0.987270176410675)
[2025-02-13 02:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:04][root][INFO] - Training Epoch: 1/2, step 1781/7134 completed (loss: 0.04409083351492882, acc: 0.9832060933113098)
[2025-02-13 02:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:04][root][INFO] - Training Epoch: 1/2, step 1782/7134 completed (loss: 0.04615529254078865, acc: 0.990867555141449)
[2025-02-13 02:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:13][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.1078, device='cuda:0') eval_epoch_loss=tensor(0.1024, device='cuda:0') eval_epoch_acc=tensor(0.9736, device='cuda:0')
[2025-02-13 02:41:13][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 02:41:13][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 02:41:14][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_1783_loss_0.10236252099275589/model.pt
[2025-02-13 02:41:14][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 02:41:14][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.10236252099275589
[2025-02-13 02:41:14][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9736115336418152
[2025-02-13 02:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14][root][INFO] - Training Epoch: 1/2, step 1783/7134 completed (loss: 0.06162058934569359, acc: 0.9810810685157776)
[2025-02-13 02:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14][root][INFO] - Training Epoch: 1/2, step 1784/7134 completed (loss: 0.06468158215284348, acc: 0.9868035316467285)
[2025-02-13 02:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:15][root][INFO] - Training Epoch: 1/2, step 1785/7134 completed (loss: 0.042545389384031296, acc: 0.9821183085441589)
[2025-02-13 02:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:15][root][INFO] - Training Epoch: 1/2, step 1786/7134 completed (loss: 0.04852744936943054, acc: 0.9859594106674194)
[2025-02-13 02:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16][root][INFO] - Training Epoch: 1/2, step 1787/7134 completed (loss: 0.04811589792370796, acc: 0.9821428656578064)
[2025-02-13 02:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16][root][INFO] - Training Epoch: 1/2, step 1788/7134 completed (loss: 0.04067447409033775, acc: 0.9901800155639648)
[2025-02-13 02:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16][root][INFO] - Training Epoch: 1/2, step 1789/7134 completed (loss: 0.04392261058092117, acc: 0.9861751198768616)
[2025-02-13 02:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17][root][INFO] - Training Epoch: 1/2, step 1790/7134 completed (loss: 0.15568560361862183, acc: 0.9710144996643066)
[2025-02-13 02:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17][root][INFO] - Training Epoch: 1/2, step 1791/7134 completed (loss: 0.17068777978420258, acc: 0.9657632112503052)
[2025-02-13 02:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:18][root][INFO] - Training Epoch: 1/2, step 1792/7134 completed (loss: 0.22615380585193634, acc: 0.9579945802688599)
[2025-02-13 02:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:18][root][INFO] - Training Epoch: 1/2, step 1793/7134 completed (loss: 0.20197056233882904, acc: 0.954054057598114)
[2025-02-13 02:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][root][INFO] - Training Epoch: 1/2, step 1794/7134 completed (loss: 0.1358654648065567, acc: 0.9606918096542358)
[2025-02-13 02:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][root][INFO] - Training Epoch: 1/2, step 1795/7134 completed (loss: 0.17503796517848969, acc: 0.9609929323196411)
[2025-02-13 02:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][root][INFO] - Training Epoch: 1/2, step 1796/7134 completed (loss: 0.16497789323329926, acc: 0.9576802253723145)
[2025-02-13 02:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:20][root][INFO] - Training Epoch: 1/2, step 1797/7134 completed (loss: 0.11595756560564041, acc: 0.9677870869636536)
[2025-02-13 02:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:20][root][INFO] - Training Epoch: 1/2, step 1798/7134 completed (loss: 0.09224940091371536, acc: 0.9749034643173218)
[2025-02-13 02:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21][root][INFO] - Training Epoch: 1/2, step 1799/7134 completed (loss: 0.15740688145160675, acc: 0.9623115658760071)
[2025-02-13 02:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21][root][INFO] - Training Epoch: 1/2, step 1800/7134 completed (loss: 0.16901734471321106, acc: 0.9591121673583984)
[2025-02-13 02:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22][root][INFO] - Training Epoch: 1/2, step 1801/7134 completed (loss: 0.1390424519777298, acc: 0.9682203531265259)
[2025-02-13 02:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22][root][INFO] - Training Epoch: 1/2, step 1802/7134 completed (loss: 0.09163151681423187, acc: 0.9737171530723572)
[2025-02-13 02:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22][root][INFO] - Training Epoch: 1/2, step 1803/7134 completed (loss: 0.09476964920759201, acc: 0.973805844783783)
[2025-02-13 02:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:23][root][INFO] - Training Epoch: 1/2, step 1804/7134 completed (loss: 0.04793009161949158, acc: 0.9889435172080994)
[2025-02-13 02:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:23][root][INFO] - Training Epoch: 1/2, step 1805/7134 completed (loss: 0.11829874664545059, acc: 0.966810941696167)
[2025-02-13 02:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24][root][INFO] - Training Epoch: 1/2, step 1806/7134 completed (loss: 0.07804740220308304, acc: 0.9815789461135864)
[2025-02-13 02:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24][root][INFO] - Training Epoch: 1/2, step 1807/7134 completed (loss: 0.06025197356939316, acc: 0.9801980257034302)
[2025-02-13 02:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24][root][INFO] - Training Epoch: 1/2, step 1808/7134 completed (loss: 0.06549960374832153, acc: 0.9791937470436096)
[2025-02-13 02:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:25][root][INFO] - Training Epoch: 1/2, step 1809/7134 completed (loss: 0.10021666437387466, acc: 0.9729729890823364)
[2025-02-13 02:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:25][root][INFO] - Training Epoch: 1/2, step 1810/7134 completed (loss: 0.04473481327295303, acc: 0.9896907210350037)
[2025-02-13 02:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26][root][INFO] - Training Epoch: 1/2, step 1811/7134 completed (loss: 0.16050556302070618, acc: 0.9565807580947876)
[2025-02-13 02:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26][root][INFO] - Training Epoch: 1/2, step 1812/7134 completed (loss: 0.0771486833691597, acc: 0.9821428656578064)
[2025-02-13 02:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:27][root][INFO] - Training Epoch: 1/2, step 1813/7134 completed (loss: 0.09503336250782013, acc: 0.9748502969741821)
[2025-02-13 02:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:27][root][INFO] - Training Epoch: 1/2, step 1814/7134 completed (loss: 0.05279838293790817, acc: 0.9836829900741577)
[2025-02-13 02:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28][root][INFO] - Training Epoch: 1/2, step 1815/7134 completed (loss: 0.08372093737125397, acc: 0.9755747318267822)
[2025-02-13 02:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28][root][INFO] - Training Epoch: 1/2, step 1816/7134 completed (loss: 0.10215200483798981, acc: 0.9749608635902405)
[2025-02-13 02:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28][root][INFO] - Training Epoch: 1/2, step 1817/7134 completed (loss: 0.06179141625761986, acc: 0.9832935333251953)
[2025-02-13 02:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29][root][INFO] - Training Epoch: 1/2, step 1818/7134 completed (loss: 0.08414340764284134, acc: 0.9801734685897827)
[2025-02-13 02:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29][root][INFO] - Training Epoch: 1/2, step 1819/7134 completed (loss: 0.05394047871232033, acc: 0.9788029789924622)
[2025-02-13 02:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30][root][INFO] - Training Epoch: 1/2, step 1820/7134 completed (loss: 0.06903218477964401, acc: 0.9814410209655762)
[2025-02-13 02:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30][root][INFO] - Training Epoch: 1/2, step 1821/7134 completed (loss: 0.07272331416606903, acc: 0.9857767820358276)
[2025-02-13 02:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31][root][INFO] - Training Epoch: 1/2, step 1822/7134 completed (loss: 0.02800985798239708, acc: 0.9912280440330505)
[2025-02-13 02:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31][root][INFO] - Training Epoch: 1/2, step 1823/7134 completed (loss: 0.045145463198423386, acc: 0.9890859723091125)
[2025-02-13 02:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31][root][INFO] - Training Epoch: 1/2, step 1824/7134 completed (loss: 0.051047325134277344, acc: 0.9851668477058411)
[2025-02-13 02:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:32][root][INFO] - Training Epoch: 1/2, step 1825/7134 completed (loss: 0.04524977505207062, acc: 0.9865471124649048)
[2025-02-13 02:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:32][root][INFO] - Training Epoch: 1/2, step 1826/7134 completed (loss: 0.04633399099111557, acc: 0.9878869652748108)
[2025-02-13 02:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:33][root][INFO] - Training Epoch: 1/2, step 1827/7134 completed (loss: 0.0892157033085823, acc: 0.9801543354988098)
[2025-02-13 02:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:33][root][INFO] - Training Epoch: 1/2, step 1828/7134 completed (loss: 0.014704427681863308, acc: 0.9974259734153748)
[2025-02-13 02:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:34][root][INFO] - Training Epoch: 1/2, step 1829/7134 completed (loss: 0.058917269110679626, acc: 0.9813664555549622)
[2025-02-13 02:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:34][root][INFO] - Training Epoch: 1/2, step 1830/7134 completed (loss: 0.05964454263448715, acc: 0.981840193271637)
[2025-02-13 02:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35][root][INFO] - Training Epoch: 1/2, step 1831/7134 completed (loss: 0.03357556834816933, acc: 0.987500011920929)
[2025-02-13 02:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35][root][INFO] - Training Epoch: 1/2, step 1832/7134 completed (loss: 0.11407670378684998, acc: 0.9734982252120972)
[2025-02-13 02:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35][root][INFO] - Training Epoch: 1/2, step 1833/7134 completed (loss: 0.2600329518318176, acc: 0.9492600560188293)
[2025-02-13 02:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:36][root][INFO] - Training Epoch: 1/2, step 1834/7134 completed (loss: 0.093546561896801, acc: 0.9788732528686523)
[2025-02-13 02:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:36][root][INFO] - Training Epoch: 1/2, step 1835/7134 completed (loss: 0.1569998860359192, acc: 0.9738371968269348)
[2025-02-13 02:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37][root][INFO] - Training Epoch: 1/2, step 1836/7134 completed (loss: 0.08689770102500916, acc: 0.9669030904769897)
[2025-02-13 02:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37][root][INFO] - Training Epoch: 1/2, step 1837/7134 completed (loss: 0.060070328414440155, acc: 0.9828009605407715)
[2025-02-13 02:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37][root][INFO] - Training Epoch: 1/2, step 1838/7134 completed (loss: 0.03795278072357178, acc: 0.9857142567634583)
[2025-02-13 02:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:38][root][INFO] - Training Epoch: 1/2, step 1839/7134 completed (loss: 0.05817554518580437, acc: 0.9870466589927673)
[2025-02-13 02:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:38][root][INFO] - Training Epoch: 1/2, step 1840/7134 completed (loss: 0.09998999536037445, acc: 0.9721059799194336)
[2025-02-13 02:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39][root][INFO] - Training Epoch: 1/2, step 1841/7134 completed (loss: 0.08388538658618927, acc: 0.9744681119918823)
[2025-02-13 02:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39][root][INFO] - Training Epoch: 1/2, step 1842/7134 completed (loss: 0.06710179150104523, acc: 0.9836734533309937)
[2025-02-13 02:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40][root][INFO] - Training Epoch: 1/2, step 1843/7134 completed (loss: 0.10653465986251831, acc: 0.9662500023841858)
[2025-02-13 02:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40][root][INFO] - Training Epoch: 1/2, step 1844/7134 completed (loss: 0.08061635494232178, acc: 0.9788618087768555)
[2025-02-13 02:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40][root][INFO] - Training Epoch: 1/2, step 1845/7134 completed (loss: 0.1007595807313919, acc: 0.968684732913971)
[2025-02-13 02:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41][root][INFO] - Training Epoch: 1/2, step 1846/7134 completed (loss: 0.05334996059536934, acc: 0.9914529919624329)
[2025-02-13 02:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41][root][INFO] - Training Epoch: 1/2, step 1847/7134 completed (loss: 0.06488347798585892, acc: 0.981566846370697)
[2025-02-13 02:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:42][root][INFO] - Training Epoch: 1/2, step 1848/7134 completed (loss: 0.0800391137599945, acc: 0.9788135886192322)
[2025-02-13 02:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:42][root][INFO] - Training Epoch: 1/2, step 1849/7134 completed (loss: 0.08260981738567352, acc: 0.9841827750205994)
[2025-02-13 02:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:43][root][INFO] - Training Epoch: 1/2, step 1850/7134 completed (loss: 0.0424320325255394, acc: 0.9889975786209106)
[2025-02-13 02:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:43][root][INFO] - Training Epoch: 1/2, step 1851/7134 completed (loss: 0.07107584178447723, acc: 0.9792429804801941)
[2025-02-13 02:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44][root][INFO] - Training Epoch: 1/2, step 1852/7134 completed (loss: 0.047902461141347885, acc: 0.9829192757606506)
[2025-02-13 02:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44][root][INFO] - Training Epoch: 1/2, step 1853/7134 completed (loss: 0.059519775211811066, acc: 0.9844497442245483)
[2025-02-13 02:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44][root][INFO] - Training Epoch: 1/2, step 1854/7134 completed (loss: 0.0923941507935524, acc: 0.9738406538963318)
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45][root][INFO] - Training Epoch: 1/2, step 1855/7134 completed (loss: 0.06961512565612793, acc: 0.9849711060523987)
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45][root][INFO] - Training Epoch: 1/2, step 1856/7134 completed (loss: 0.05548938736319542, acc: 0.9815497994422913)
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:46][root][INFO] - Training Epoch: 1/2, step 1857/7134 completed (loss: 0.1103324443101883, acc: 0.9778324961662292)
[2025-02-13 02:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:46][root][INFO] - Training Epoch: 1/2, step 1858/7134 completed (loss: 0.03892164304852486, acc: 0.9880383014678955)
[2025-02-13 02:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47][root][INFO] - Training Epoch: 1/2, step 1859/7134 completed (loss: 0.033214956521987915, acc: 0.9908987283706665)
[2025-02-13 02:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47][root][INFO] - Training Epoch: 1/2, step 1860/7134 completed (loss: 0.03823331743478775, acc: 0.9911392331123352)
[2025-02-13 02:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47][root][INFO] - Training Epoch: 1/2, step 1861/7134 completed (loss: 0.07417028397321701, acc: 0.9844961166381836)
[2025-02-13 02:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48][root][INFO] - Training Epoch: 1/2, step 1862/7134 completed (loss: 0.04811489209532738, acc: 0.9864314794540405)
[2025-02-13 02:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48][root][INFO] - Training Epoch: 1/2, step 1863/7134 completed (loss: 0.08295633643865585, acc: 0.9796162843704224)
[2025-02-13 02:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49][root][INFO] - Training Epoch: 1/2, step 1864/7134 completed (loss: 0.16723409295082092, acc: 0.9646258354187012)
[2025-02-13 02:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49][root][INFO] - Training Epoch: 1/2, step 1865/7134 completed (loss: 0.08745283633470535, acc: 0.9746192693710327)
[2025-02-13 02:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50][root][INFO] - Training Epoch: 1/2, step 1866/7134 completed (loss: 0.03355460241436958, acc: 0.9901960492134094)
[2025-02-13 02:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50][root][INFO] - Training Epoch: 1/2, step 1867/7134 completed (loss: 0.05154263228178024, acc: 0.9836065769195557)
[2025-02-13 02:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50][root][INFO] - Training Epoch: 1/2, step 1868/7134 completed (loss: 0.11734180897474289, acc: 0.9653465151786804)
[2025-02-13 02:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51][root][INFO] - Training Epoch: 1/2, step 1869/7134 completed (loss: 0.0831141546368599, acc: 0.975530207157135)
[2025-02-13 02:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51][root][INFO] - Training Epoch: 1/2, step 1870/7134 completed (loss: 0.0547659695148468, acc: 0.9856770634651184)
[2025-02-13 02:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52][root][INFO] - Training Epoch: 1/2, step 1871/7134 completed (loss: 0.10576221346855164, acc: 0.9738988876342773)
[2025-02-13 02:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52][root][INFO] - Training Epoch: 1/2, step 1872/7134 completed (loss: 0.5851127505302429, acc: 0.889502763748169)
[2025-02-13 02:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52][root][INFO] - Training Epoch: 1/2, step 1873/7134 completed (loss: 0.20612090826034546, acc: 0.9382422566413879)
[2025-02-13 02:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:53][root][INFO] - Training Epoch: 1/2, step 1874/7134 completed (loss: 0.0907038003206253, acc: 0.9816513657569885)
[2025-02-13 02:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:53][root][INFO] - Training Epoch: 1/2, step 1875/7134 completed (loss: 0.07656264305114746, acc: 0.9825327396392822)
[2025-02-13 02:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54][root][INFO] - Training Epoch: 1/2, step 1876/7134 completed (loss: 0.1629561483860016, acc: 0.9490662217140198)
[2025-02-13 02:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54][root][INFO] - Training Epoch: 1/2, step 1877/7134 completed (loss: 0.11344070732593536, acc: 0.9647355079650879)
[2025-02-13 02:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54][root][INFO] - Training Epoch: 1/2, step 1878/7134 completed (loss: 0.11471405625343323, acc: 0.9753466844558716)
[2025-02-13 02:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:55][root][INFO] - Training Epoch: 1/2, step 1879/7134 completed (loss: 0.1320120245218277, acc: 0.9657422304153442)
[2025-02-13 02:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:55][root][INFO] - Training Epoch: 1/2, step 1880/7134 completed (loss: 0.0856490284204483, acc: 0.9798792600631714)
[2025-02-13 02:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56][root][INFO] - Training Epoch: 1/2, step 1881/7134 completed (loss: 0.10289875417947769, acc: 0.9754902124404907)
[2025-02-13 02:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56][root][INFO] - Training Epoch: 1/2, step 1882/7134 completed (loss: 0.15981510281562805, acc: 0.9482758641242981)
[2025-02-13 02:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56][root][INFO] - Training Epoch: 1/2, step 1883/7134 completed (loss: 0.09687181562185287, acc: 0.9783464670181274)
[2025-02-13 02:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:57][root][INFO] - Training Epoch: 1/2, step 1884/7134 completed (loss: 0.1458214521408081, acc: 0.9605055451393127)
[2025-02-13 02:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:57][root][INFO] - Training Epoch: 1/2, step 1885/7134 completed (loss: 0.10862749069929123, acc: 0.9699812531471252)
[2025-02-13 02:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58][root][INFO] - Training Epoch: 1/2, step 1886/7134 completed (loss: 0.13719019293785095, acc: 0.9728600978851318)
[2025-02-13 02:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58][root][INFO] - Training Epoch: 1/2, step 1887/7134 completed (loss: 0.11213788390159607, acc: 0.9715369939804077)
[2025-02-13 02:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58][root][INFO] - Training Epoch: 1/2, step 1888/7134 completed (loss: 0.12357228994369507, acc: 0.9692898392677307)
[2025-02-13 02:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:59][root][INFO] - Training Epoch: 1/2, step 1889/7134 completed (loss: 0.06618513911962509, acc: 0.9797688126564026)
[2025-02-13 02:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:59][root][INFO] - Training Epoch: 1/2, step 1890/7134 completed (loss: 0.0494811125099659, acc: 0.9876161217689514)
[2025-02-13 02:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:00][root][INFO] - Training Epoch: 1/2, step 1891/7134 completed (loss: 0.12973764538764954, acc: 0.9594155550003052)
[2025-02-13 02:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:00][root][INFO] - Training Epoch: 1/2, step 1892/7134 completed (loss: 0.11148083209991455, acc: 0.9768160581588745)
[2025-02-13 02:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01][root][INFO] - Training Epoch: 1/2, step 1893/7134 completed (loss: 0.07097089290618896, acc: 0.975039005279541)
[2025-02-13 02:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01][root][INFO] - Training Epoch: 1/2, step 1894/7134 completed (loss: 0.1521114856004715, acc: 0.96875)
[2025-02-13 02:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01][root][INFO] - Training Epoch: 1/2, step 1895/7134 completed (loss: 0.327192097902298, acc: 0.9281045794487)
[2025-02-13 02:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:02][root][INFO] - Training Epoch: 1/2, step 1896/7134 completed (loss: 0.11512450128793716, acc: 0.9694117903709412)
[2025-02-13 02:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:02][root][INFO] - Training Epoch: 1/2, step 1897/7134 completed (loss: 0.0471147783100605, acc: 0.9870370626449585)
[2025-02-13 02:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03][root][INFO] - Training Epoch: 1/2, step 1898/7134 completed (loss: 0.06577220559120178, acc: 0.9800994992256165)
[2025-02-13 02:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03][root][INFO] - Training Epoch: 1/2, step 1899/7134 completed (loss: 0.09337121993303299, acc: 0.9819168448448181)
[2025-02-13 02:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03][root][INFO] - Training Epoch: 1/2, step 1900/7134 completed (loss: 0.06577663868665695, acc: 0.9870550036430359)
[2025-02-13 02:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:04][root][INFO] - Training Epoch: 1/2, step 1901/7134 completed (loss: 0.1019907221198082, acc: 0.9780621528625488)
[2025-02-13 02:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:04][root][INFO] - Training Epoch: 1/2, step 1902/7134 completed (loss: 0.05485296994447708, acc: 0.9876977205276489)
[2025-02-13 02:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05][root][INFO] - Training Epoch: 1/2, step 1903/7134 completed (loss: 0.06594018638134003, acc: 0.9739583134651184)
[2025-02-13 02:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05][root][INFO] - Training Epoch: 1/2, step 1904/7134 completed (loss: 0.06757748126983643, acc: 0.9809321761131287)
[2025-02-13 02:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05][root][INFO] - Training Epoch: 1/2, step 1905/7134 completed (loss: 0.08978830277919769, acc: 0.9672801494598389)
[2025-02-13 02:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:06][root][INFO] - Training Epoch: 1/2, step 1906/7134 completed (loss: 0.07512813061475754, acc: 0.9824561476707458)
[2025-02-13 02:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:06][root][INFO] - Training Epoch: 1/2, step 1907/7134 completed (loss: 0.064520925283432, acc: 0.9815126061439514)
[2025-02-13 02:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][root][INFO] - Training Epoch: 1/2, step 1908/7134 completed (loss: 0.0732489675283432, acc: 0.9828141927719116)
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][root][INFO] - Training Epoch: 1/2, step 1909/7134 completed (loss: 0.06835026293992996, acc: 0.9801324605941772)
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][root][INFO] - Training Epoch: 1/2, step 1910/7134 completed (loss: 0.10253617912530899, acc: 0.9802631735801697)
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:08][root][INFO] - Training Epoch: 1/2, step 1911/7134 completed (loss: 0.07356300204992294, acc: 0.9890776872634888)
[2025-02-13 02:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:08][root][INFO] - Training Epoch: 1/2, step 1912/7134 completed (loss: 0.036280836910009384, acc: 0.9866844415664673)
[2025-02-13 02:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09][root][INFO] - Training Epoch: 1/2, step 1913/7134 completed (loss: 0.09272617101669312, acc: 0.9870129823684692)
[2025-02-13 02:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09][root][INFO] - Training Epoch: 1/2, step 1914/7134 completed (loss: 0.08136270940303802, acc: 0.9824086427688599)
[2025-02-13 02:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09][root][INFO] - Training Epoch: 1/2, step 1915/7134 completed (loss: 0.08927936851978302, acc: 0.9787928462028503)
[2025-02-13 02:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:10][root][INFO] - Training Epoch: 1/2, step 1916/7134 completed (loss: 0.07603966444730759, acc: 0.9829476475715637)
[2025-02-13 02:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:10][root][INFO] - Training Epoch: 1/2, step 1917/7134 completed (loss: 0.07796840369701385, acc: 0.9845938086509705)
[2025-02-13 02:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11][root][INFO] - Training Epoch: 1/2, step 1918/7134 completed (loss: 0.0845232605934143, acc: 0.9811912178993225)
[2025-02-13 02:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11][root][INFO] - Training Epoch: 1/2, step 1919/7134 completed (loss: 0.0874391421675682, acc: 0.9842180609703064)
[2025-02-13 02:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12][root][INFO] - Training Epoch: 1/2, step 1920/7134 completed (loss: 0.05156620591878891, acc: 0.9876543283462524)
[2025-02-13 02:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12][root][INFO] - Training Epoch: 1/2, step 1921/7134 completed (loss: 0.07911542803049088, acc: 0.9806094169616699)
[2025-02-13 02:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12][root][INFO] - Training Epoch: 1/2, step 1922/7134 completed (loss: 0.05225379765033722, acc: 0.987077534198761)
[2025-02-13 02:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13][root][INFO] - Training Epoch: 1/2, step 1923/7134 completed (loss: 0.05668526515364647, acc: 0.9865996837615967)
[2025-02-13 02:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13][root][INFO] - Training Epoch: 1/2, step 1924/7134 completed (loss: 0.08583808690309525, acc: 0.9763948321342468)
[2025-02-13 02:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:14][root][INFO] - Training Epoch: 1/2, step 1925/7134 completed (loss: 0.10366278141736984, acc: 0.9714285731315613)
[2025-02-13 02:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:14][root][INFO] - Training Epoch: 1/2, step 1926/7134 completed (loss: 0.039774224162101746, acc: 0.9877426028251648)
[2025-02-13 02:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15][root][INFO] - Training Epoch: 1/2, step 1927/7134 completed (loss: 0.0812566801905632, acc: 0.9834254384040833)
[2025-02-13 02:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15][root][INFO] - Training Epoch: 1/2, step 1928/7134 completed (loss: 0.0468655601143837, acc: 0.9863221645355225)
[2025-02-13 02:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15][root][INFO] - Training Epoch: 1/2, step 1929/7134 completed (loss: 0.05905218794941902, acc: 0.9858757257461548)
[2025-02-13 02:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:16][root][INFO] - Training Epoch: 1/2, step 1930/7134 completed (loss: 0.10926609486341476, acc: 0.9737532734870911)
[2025-02-13 02:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:16][root][INFO] - Training Epoch: 1/2, step 1931/7134 completed (loss: 0.12515544891357422, acc: 0.9734513163566589)
[2025-02-13 02:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:17][root][INFO] - Training Epoch: 1/2, step 1932/7134 completed (loss: 0.13847343623638153, acc: 0.9611940383911133)
[2025-02-13 02:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:17][root][INFO] - Training Epoch: 1/2, step 1933/7134 completed (loss: 0.19067108631134033, acc: 0.9565826058387756)
[2025-02-13 02:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:18][root][INFO] - Training Epoch: 1/2, step 1934/7134 completed (loss: 0.05887918919324875, acc: 0.9872881174087524)
[2025-02-13 02:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:18][root][INFO] - Training Epoch: 1/2, step 1935/7134 completed (loss: 0.15609849989414215, acc: 0.9650654792785645)
[2025-02-13 02:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19][root][INFO] - Training Epoch: 1/2, step 1936/7134 completed (loss: 0.036848392337560654, acc: 0.9921011328697205)
[2025-02-13 02:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19][root][INFO] - Training Epoch: 1/2, step 1937/7134 completed (loss: 0.07260099053382874, acc: 0.9821428656578064)
[2025-02-13 02:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19][root][INFO] - Training Epoch: 1/2, step 1938/7134 completed (loss: 0.12156292796134949, acc: 0.9700854420661926)
[2025-02-13 02:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:20][root][INFO] - Training Epoch: 1/2, step 1939/7134 completed (loss: 0.1009925976395607, acc: 0.9788135886192322)
[2025-02-13 02:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:20][root][INFO] - Training Epoch: 1/2, step 1940/7134 completed (loss: 0.11767613142728806, acc: 0.9673469662666321)
[2025-02-13 02:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21][root][INFO] - Training Epoch: 1/2, step 1941/7134 completed (loss: 0.12418978661298752, acc: 0.9625668525695801)
[2025-02-13 02:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21][root][INFO] - Training Epoch: 1/2, step 1942/7134 completed (loss: 0.06956101208925247, acc: 0.9817073345184326)
[2025-02-13 02:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21][root][INFO] - Training Epoch: 1/2, step 1943/7134 completed (loss: 0.15197107195854187, acc: 0.959756076335907)
[2025-02-13 02:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:22][root][INFO] - Training Epoch: 1/2, step 1944/7134 completed (loss: 0.11326968669891357, acc: 0.9634369015693665)
[2025-02-13 02:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:22][root][INFO] - Training Epoch: 1/2, step 1945/7134 completed (loss: 0.09942067414522171, acc: 0.9724409580230713)
[2025-02-13 02:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:23][root][INFO] - Training Epoch: 1/2, step 1946/7134 completed (loss: 0.10246841609477997, acc: 0.9677419066429138)
[2025-02-13 02:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:23][root][INFO] - Training Epoch: 1/2, step 1947/7134 completed (loss: 0.11610354483127594, acc: 0.9644736647605896)
[2025-02-13 02:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24][root][INFO] - Training Epoch: 1/2, step 1948/7134 completed (loss: 0.19269894063472748, acc: 0.9506641626358032)
[2025-02-13 02:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24][root][INFO] - Training Epoch: 1/2, step 1949/7134 completed (loss: 0.08432666212320328, acc: 0.9732441306114197)
[2025-02-13 02:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24][root][INFO] - Training Epoch: 1/2, step 1950/7134 completed (loss: 0.14054393768310547, acc: 0.9616788029670715)
[2025-02-13 02:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25][root][INFO] - Training Epoch: 1/2, step 1951/7134 completed (loss: 0.09218259155750275, acc: 0.9765886068344116)
[2025-02-13 02:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25][root][INFO] - Training Epoch: 1/2, step 1952/7134 completed (loss: 0.12091758847236633, acc: 0.9733777046203613)
[2025-02-13 02:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26][root][INFO] - Training Epoch: 1/2, step 1953/7134 completed (loss: 0.07183900475502014, acc: 0.9814241528511047)
[2025-02-13 02:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26][root][INFO] - Training Epoch: 1/2, step 1954/7134 completed (loss: 0.06562955677509308, acc: 0.9780488014221191)
[2025-02-13 02:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26][root][INFO] - Training Epoch: 1/2, step 1955/7134 completed (loss: 0.0962735041975975, acc: 0.9789103865623474)
[2025-02-13 02:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27][root][INFO] - Training Epoch: 1/2, step 1956/7134 completed (loss: 0.03991976007819176, acc: 0.9865591526031494)
[2025-02-13 02:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27][root][INFO] - Training Epoch: 1/2, step 1957/7134 completed (loss: 0.08471669256687164, acc: 0.9759259223937988)
[2025-02-13 02:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27][root][INFO] - Training Epoch: 1/2, step 1958/7134 completed (loss: 0.10726620256900787, acc: 0.9770773649215698)
[2025-02-13 02:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28][root][INFO] - Training Epoch: 1/2, step 1959/7134 completed (loss: 0.047030817717313766, acc: 0.9914712309837341)
[2025-02-13 02:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28][root][INFO] - Training Epoch: 1/2, step 1960/7134 completed (loss: 0.051783692091703415, acc: 0.9830827116966248)
[2025-02-13 02:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28][root][INFO] - Training Epoch: 1/2, step 1961/7134 completed (loss: 0.06298092007637024, acc: 0.9765886068344116)
[2025-02-13 02:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29][root][INFO] - Training Epoch: 1/2, step 1962/7134 completed (loss: 0.08434110134840012, acc: 0.9804469347000122)
[2025-02-13 02:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29][root][INFO] - Training Epoch: 1/2, step 1963/7134 completed (loss: 0.08479185402393341, acc: 0.9723661541938782)
[2025-02-13 02:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:30][root][INFO] - Training Epoch: 1/2, step 1964/7134 completed (loss: 0.0755910649895668, acc: 0.9853479862213135)
[2025-02-13 02:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:30][root][INFO] - Training Epoch: 1/2, step 1965/7134 completed (loss: 0.06986062228679657, acc: 0.9832636117935181)
[2025-02-13 02:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31][root][INFO] - Training Epoch: 1/2, step 1966/7134 completed (loss: 0.06801178306341171, acc: 0.9816513657569885)
[2025-02-13 02:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31][root][INFO] - Training Epoch: 1/2, step 1967/7134 completed (loss: 0.09860185533761978, acc: 0.9745222926139832)
[2025-02-13 02:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31][root][INFO] - Training Epoch: 1/2, step 1968/7134 completed (loss: 0.043409351259469986, acc: 0.9864197373390198)
[2025-02-13 02:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:32][root][INFO] - Training Epoch: 1/2, step 1969/7134 completed (loss: 0.09853547811508179, acc: 0.9746666550636292)
[2025-02-13 02:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:32][root][INFO] - Training Epoch: 1/2, step 1970/7134 completed (loss: 0.07261078804731369, acc: 0.9785276055335999)
[2025-02-13 02:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33][root][INFO] - Training Epoch: 1/2, step 1971/7134 completed (loss: 0.0735052227973938, acc: 0.9787798523902893)
[2025-02-13 02:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33][root][INFO] - Training Epoch: 1/2, step 1972/7134 completed (loss: 0.3068964183330536, acc: 0.9238329529762268)
[2025-02-13 02:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33][root][INFO] - Training Epoch: 1/2, step 1973/7134 completed (loss: 0.17449164390563965, acc: 0.9523809552192688)
[2025-02-13 02:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:34][root][INFO] - Training Epoch: 1/2, step 1974/7134 completed (loss: 0.06831949204206467, acc: 0.9786432385444641)
[2025-02-13 02:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:34][root][INFO] - Training Epoch: 1/2, step 1975/7134 completed (loss: 0.06232406198978424, acc: 0.9848484992980957)
[2025-02-13 02:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35][root][INFO] - Training Epoch: 1/2, step 1976/7134 completed (loss: 0.08015524595975876, acc: 0.9750849604606628)
[2025-02-13 02:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35][root][INFO] - Training Epoch: 1/2, step 1977/7134 completed (loss: 0.09552676230669022, acc: 0.9710144996643066)
[2025-02-13 02:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36][root][INFO] - Training Epoch: 1/2, step 1978/7134 completed (loss: 0.043886084109544754, acc: 0.9892617464065552)
[2025-02-13 02:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36][root][INFO] - Training Epoch: 1/2, step 1979/7134 completed (loss: 0.06622646003961563, acc: 0.9752604365348816)
[2025-02-13 02:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36][root][INFO] - Training Epoch: 1/2, step 1980/7134 completed (loss: 0.16803482174873352, acc: 0.958279013633728)
[2025-02-13 02:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:37][root][INFO] - Training Epoch: 1/2, step 1981/7134 completed (loss: 0.07363443076610565, acc: 0.9762237668037415)
[2025-02-13 02:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:37][root][INFO] - Training Epoch: 1/2, step 1982/7134 completed (loss: 0.047928545624017715, acc: 0.9871299862861633)
[2025-02-13 02:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38][root][INFO] - Training Epoch: 1/2, step 1983/7134 completed (loss: 0.0583144910633564, acc: 0.9885495901107788)
[2025-02-13 02:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38][root][INFO] - Training Epoch: 1/2, step 1984/7134 completed (loss: 0.04658478870987892, acc: 0.983849287033081)
[2025-02-13 02:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38][root][INFO] - Training Epoch: 1/2, step 1985/7134 completed (loss: 0.05688508599996567, acc: 0.9818941354751587)
[2025-02-13 02:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39][root][INFO] - Training Epoch: 1/2, step 1986/7134 completed (loss: 0.07359912246465683, acc: 0.9806950092315674)
[2025-02-13 02:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39][root][INFO] - Training Epoch: 1/2, step 1987/7134 completed (loss: 0.0734405443072319, acc: 0.9844192862510681)
[2025-02-13 02:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:40][root][INFO] - Training Epoch: 1/2, step 1988/7134 completed (loss: 0.07834611088037491, acc: 0.980028510093689)
[2025-02-13 02:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:40][root][INFO] - Training Epoch: 1/2, step 1989/7134 completed (loss: 0.12273750454187393, acc: 0.9728867411613464)
[2025-02-13 02:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41][root][INFO] - Training Epoch: 1/2, step 1990/7134 completed (loss: 0.0771106407046318, acc: 0.981796145439148)
[2025-02-13 02:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41][root][INFO] - Training Epoch: 1/2, step 1991/7134 completed (loss: 0.05388493090867996, acc: 0.985049843788147)
[2025-02-13 02:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41][root][INFO] - Training Epoch: 1/2, step 1992/7134 completed (loss: 0.08430106192827225, acc: 0.9689922332763672)
[2025-02-13 02:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42][root][INFO] - Training Epoch: 1/2, step 1993/7134 completed (loss: 0.03681822866201401, acc: 0.9894551634788513)
[2025-02-13 02:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42][root][INFO] - Training Epoch: 1/2, step 1994/7134 completed (loss: 0.020638303831219673, acc: 0.9931034445762634)
[2025-02-13 02:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43][root][INFO] - Training Epoch: 1/2, step 1995/7134 completed (loss: 0.04281969740986824, acc: 0.9860896468162537)
[2025-02-13 02:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43][root][INFO] - Training Epoch: 1/2, step 1996/7134 completed (loss: 0.05767170339822769, acc: 0.9851239919662476)
[2025-02-13 02:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43][root][INFO] - Training Epoch: 1/2, step 1997/7134 completed (loss: 0.04290864244103432, acc: 0.9912663698196411)
[2025-02-13 02:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:44][root][INFO] - Training Epoch: 1/2, step 1998/7134 completed (loss: 0.0334036722779274, acc: 0.9905808568000793)
[2025-02-13 02:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:44][root][INFO] - Training Epoch: 1/2, step 1999/7134 completed (loss: 0.0685838833451271, acc: 0.9762309193611145)
[2025-02-13 02:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45][root][INFO] - Training Epoch: 1/2, step 2000/7134 completed (loss: 0.0523269847035408, acc: 0.988959014415741)
[2025-02-13 02:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45][root][INFO] - Training Epoch: 1/2, step 2001/7134 completed (loss: 0.04515669122338295, acc: 0.989393949508667)
[2025-02-13 02:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45][root][INFO] - Training Epoch: 1/2, step 2002/7134 completed (loss: 0.051602933555841446, acc: 0.9843478202819824)
[2025-02-13 02:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:46][root][INFO] - Training Epoch: 1/2, step 2003/7134 completed (loss: 0.07021267712116241, acc: 0.9833333492279053)
[2025-02-13 02:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:46][root][INFO] - Training Epoch: 1/2, step 2004/7134 completed (loss: 0.26962268352508545, acc: 0.947257399559021)
[2025-02-13 02:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47][root][INFO] - Training Epoch: 1/2, step 2005/7134 completed (loss: 0.2018922120332718, acc: 0.9589040875434875)
[2025-02-13 02:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47][root][INFO] - Training Epoch: 1/2, step 2006/7134 completed (loss: 0.07792042195796967, acc: 0.9827089309692383)
[2025-02-13 02:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47][root][INFO] - Training Epoch: 1/2, step 2007/7134 completed (loss: 0.05989371985197067, acc: 0.9859437942504883)
[2025-02-13 02:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:48][root][INFO] - Training Epoch: 1/2, step 2008/7134 completed (loss: 0.06153611093759537, acc: 0.9786477088928223)
[2025-02-13 02:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:48][root][INFO] - Training Epoch: 1/2, step 2009/7134 completed (loss: 0.046134449541568756, acc: 0.9855595827102661)
[2025-02-13 02:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49][root][INFO] - Training Epoch: 1/2, step 2010/7134 completed (loss: 0.05581769719719887, acc: 0.9879931211471558)
[2025-02-13 02:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49][root][INFO] - Training Epoch: 1/2, step 2011/7134 completed (loss: 0.06142307445406914, acc: 0.9784736037254333)
[2025-02-13 02:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49][root][INFO] - Training Epoch: 1/2, step 2012/7134 completed (loss: 0.0719066709280014, acc: 0.9767441749572754)
[2025-02-13 02:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50][root][INFO] - Training Epoch: 1/2, step 2013/7134 completed (loss: 0.05530672147870064, acc: 0.984000027179718)
[2025-02-13 02:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50][root][INFO] - Training Epoch: 1/2, step 2014/7134 completed (loss: 0.05042102187871933, acc: 0.9858267903327942)
[2025-02-13 02:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51][root][INFO] - Training Epoch: 1/2, step 2015/7134 completed (loss: 0.0579390786588192, acc: 0.9786184430122375)
[2025-02-13 02:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51][root][INFO] - Training Epoch: 1/2, step 2016/7134 completed (loss: 0.10531274974346161, acc: 0.9723076820373535)
[2025-02-13 02:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51][root][INFO] - Training Epoch: 1/2, step 2017/7134 completed (loss: 0.13086238503456116, acc: 0.9675675630569458)
[2025-02-13 02:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:52][root][INFO] - Training Epoch: 1/2, step 2018/7134 completed (loss: 0.04880882799625397, acc: 0.9794871807098389)
[2025-02-13 02:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:52][root][INFO] - Training Epoch: 1/2, step 2019/7134 completed (loss: 0.08262024819850922, acc: 0.9765517115592957)
[2025-02-13 02:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53][root][INFO] - Training Epoch: 1/2, step 2020/7134 completed (loss: 0.03873734921216965, acc: 0.989847719669342)
[2025-02-13 02:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53][root][INFO] - Training Epoch: 1/2, step 2021/7134 completed (loss: 0.08257029950618744, acc: 0.9779220819473267)
[2025-02-13 02:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53][root][INFO] - Training Epoch: 1/2, step 2022/7134 completed (loss: 0.10249274969100952, acc: 0.9809358716011047)
[2025-02-13 02:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:54][root][INFO] - Training Epoch: 1/2, step 2023/7134 completed (loss: 0.06691692024469376, acc: 0.9810218811035156)
[2025-02-13 02:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:54][root][INFO] - Training Epoch: 1/2, step 2024/7134 completed (loss: 0.10448480397462845, acc: 0.9708333611488342)
[2025-02-13 02:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55][root][INFO] - Training Epoch: 1/2, step 2025/7134 completed (loss: 0.04953208938241005, acc: 0.9879999756813049)
[2025-02-13 02:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55][root][INFO] - Training Epoch: 1/2, step 2026/7134 completed (loss: 0.10559798032045364, acc: 0.9789156913757324)
[2025-02-13 02:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56][root][INFO] - Training Epoch: 1/2, step 2027/7134 completed (loss: 0.04906849190592766, acc: 0.9817276000976562)
[2025-02-13 02:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56][root][INFO] - Training Epoch: 1/2, step 2028/7134 completed (loss: 0.07106757909059525, acc: 0.9795321822166443)
[2025-02-13 02:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56][root][INFO] - Training Epoch: 1/2, step 2029/7134 completed (loss: 0.10141091048717499, acc: 0.9758522510528564)
[2025-02-13 02:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:57][root][INFO] - Training Epoch: 1/2, step 2030/7134 completed (loss: 0.10068144649267197, acc: 0.9723926186561584)
[2025-02-13 02:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:57][root][INFO] - Training Epoch: 1/2, step 2031/7134 completed (loss: 0.06841520220041275, acc: 0.979141116142273)
[2025-02-13 02:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58][root][INFO] - Training Epoch: 1/2, step 2032/7134 completed (loss: 0.06153026595711708, acc: 0.9852150678634644)
[2025-02-13 02:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58][root][INFO] - Training Epoch: 1/2, step 2033/7134 completed (loss: 0.054241541773080826, acc: 0.989708423614502)
[2025-02-13 02:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58][root][INFO] - Training Epoch: 1/2, step 2034/7134 completed (loss: 0.08545147627592087, acc: 0.9739663004875183)
[2025-02-13 02:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59][root][INFO] - Training Epoch: 1/2, step 2035/7134 completed (loss: 0.04046344757080078, acc: 0.9900373816490173)
[2025-02-13 02:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59][root][INFO] - Training Epoch: 1/2, step 2036/7134 completed (loss: 0.0564749576151371, acc: 0.9826989769935608)
[2025-02-13 02:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:00][root][INFO] - Training Epoch: 1/2, step 2037/7134 completed (loss: 0.050705116242170334, acc: 0.9792284965515137)
[2025-02-13 02:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:00][root][INFO] - Training Epoch: 1/2, step 2038/7134 completed (loss: 0.06002742052078247, acc: 0.9810874462127686)
[2025-02-13 02:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01][root][INFO] - Training Epoch: 1/2, step 2039/7134 completed (loss: 0.029421532526612282, acc: 0.9909909963607788)
[2025-02-13 02:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01][root][INFO] - Training Epoch: 1/2, step 2040/7134 completed (loss: 0.05776222050189972, acc: 0.9897119402885437)
[2025-02-13 02:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01][root][INFO] - Training Epoch: 1/2, step 2041/7134 completed (loss: 0.030496753752231598, acc: 0.9931318759918213)
[2025-02-13 02:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:02][root][INFO] - Training Epoch: 1/2, step 2042/7134 completed (loss: 0.06058549880981445, acc: 0.9818548560142517)
[2025-02-13 02:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:02][root][INFO] - Training Epoch: 1/2, step 2043/7134 completed (loss: 0.0662979856133461, acc: 0.9828125238418579)
[2025-02-13 02:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03][root][INFO] - Training Epoch: 1/2, step 2044/7134 completed (loss: 0.05614285543560982, acc: 0.989266574382782)
[2025-02-13 02:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03][root][INFO] - Training Epoch: 1/2, step 2045/7134 completed (loss: 0.055694542825222015, acc: 0.9837251305580139)
[2025-02-13 02:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03][root][INFO] - Training Epoch: 1/2, step 2046/7134 completed (loss: 0.055524613708257675, acc: 0.9837398529052734)
[2025-02-13 02:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04][root][INFO] - Training Epoch: 1/2, step 2047/7134 completed (loss: 0.05661813169717789, acc: 0.9833055138587952)
[2025-02-13 02:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04][root][INFO] - Training Epoch: 1/2, step 2048/7134 completed (loss: 0.07811398059129715, acc: 0.974588930606842)
[2025-02-13 02:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04][root][INFO] - Training Epoch: 1/2, step 2049/7134 completed (loss: 0.10715097188949585, acc: 0.9694117903709412)
[2025-02-13 02:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:05][root][INFO] - Training Epoch: 1/2, step 2050/7134 completed (loss: 0.10339067876338959, acc: 0.9794871807098389)
[2025-02-13 02:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:05][root][INFO] - Training Epoch: 1/2, step 2051/7134 completed (loss: 0.07944958657026291, acc: 0.978515625)
[2025-02-13 02:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06][root][INFO] - Training Epoch: 1/2, step 2052/7134 completed (loss: 0.12276873737573624, acc: 0.9692832827568054)
[2025-02-13 02:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06][root][INFO] - Training Epoch: 1/2, step 2053/7134 completed (loss: 0.0728374570608139, acc: 0.9779999852180481)
[2025-02-13 02:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06][root][INFO] - Training Epoch: 1/2, step 2054/7134 completed (loss: 0.15309526026248932, acc: 0.9538216590881348)
[2025-02-13 02:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:07][root][INFO] - Training Epoch: 1/2, step 2055/7134 completed (loss: 0.12143374234437943, acc: 0.9716535210609436)
[2025-02-13 02:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:07][root][INFO] - Training Epoch: 1/2, step 2056/7134 completed (loss: 0.10804925858974457, acc: 0.9675745964050293)
[2025-02-13 02:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:08][root][INFO] - Training Epoch: 1/2, step 2057/7134 completed (loss: 0.12208712100982666, acc: 0.9660786986351013)
[2025-02-13 02:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:08][root][INFO] - Training Epoch: 1/2, step 2058/7134 completed (loss: 0.14166828989982605, acc: 0.9670014381408691)
[2025-02-13 02:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09][root][INFO] - Training Epoch: 1/2, step 2059/7134 completed (loss: 0.14835621416568756, acc: 0.9653379321098328)
[2025-02-13 02:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09][root][INFO] - Training Epoch: 1/2, step 2060/7134 completed (loss: 0.07786858081817627, acc: 0.9814019799232483)
[2025-02-13 02:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10][root][INFO] - Training Epoch: 1/2, step 2061/7134 completed (loss: 0.10311432182788849, acc: 0.9716714024543762)
[2025-02-13 02:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10][root][INFO] - Training Epoch: 1/2, step 2062/7134 completed (loss: 0.13967281579971313, acc: 0.9696394801139832)
[2025-02-13 02:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10][root][INFO] - Training Epoch: 1/2, step 2063/7134 completed (loss: 0.12031589448451996, acc: 0.9664179086685181)
[2025-02-13 02:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11][root][INFO] - Training Epoch: 1/2, step 2064/7134 completed (loss: 0.04345455393195152, acc: 0.9859374761581421)
[2025-02-13 02:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11][root][INFO] - Training Epoch: 1/2, step 2065/7134 completed (loss: 0.10828984528779984, acc: 0.9748031497001648)
[2025-02-13 02:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11][root][INFO] - Training Epoch: 1/2, step 2066/7134 completed (loss: 0.07993722707033157, acc: 0.9819639325141907)
[2025-02-13 02:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:12][root][INFO] - Training Epoch: 1/2, step 2067/7134 completed (loss: 0.07290273159742355, acc: 0.9791271090507507)
[2025-02-13 02:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:12][root][INFO] - Training Epoch: 1/2, step 2068/7134 completed (loss: 0.10641591995954514, acc: 0.9695550203323364)
[2025-02-13 02:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13][root][INFO] - Training Epoch: 1/2, step 2069/7134 completed (loss: 0.07280776649713516, acc: 0.9784615635871887)
[2025-02-13 02:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13][root][INFO] - Training Epoch: 1/2, step 2070/7134 completed (loss: 0.07860364019870758, acc: 0.9771863222122192)
[2025-02-13 02:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13][root][INFO] - Training Epoch: 1/2, step 2071/7134 completed (loss: 0.050233691930770874, acc: 0.9906322956085205)
[2025-02-13 02:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:14][root][INFO] - Training Epoch: 1/2, step 2072/7134 completed (loss: 0.03699996694922447, acc: 0.9925742745399475)
[2025-02-13 02:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:14][root][INFO] - Training Epoch: 1/2, step 2073/7134 completed (loss: 0.06179337203502655, acc: 0.9865384697914124)
[2025-02-13 02:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15][root][INFO] - Training Epoch: 1/2, step 2074/7134 completed (loss: 0.07899865508079529, acc: 0.9788732528686523)
[2025-02-13 02:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15][root][INFO] - Training Epoch: 1/2, step 2075/7134 completed (loss: 0.14755870401859283, acc: 0.9565943479537964)
[2025-02-13 02:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15][root][INFO] - Training Epoch: 1/2, step 2076/7134 completed (loss: 0.0784996822476387, acc: 0.9830769300460815)
[2025-02-13 02:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:16][root][INFO] - Training Epoch: 1/2, step 2077/7134 completed (loss: 0.05809067189693451, acc: 0.9814077019691467)
[2025-02-13 02:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:16][root][INFO] - Training Epoch: 1/2, step 2078/7134 completed (loss: 0.03821096569299698, acc: 0.9930232763290405)
[2025-02-13 02:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:17][root][INFO] - Training Epoch: 1/2, step 2079/7134 completed (loss: 0.04156043007969856, acc: 0.988095223903656)
[2025-02-13 02:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:17][root][INFO] - Training Epoch: 1/2, step 2080/7134 completed (loss: 0.06848443299531937, acc: 0.9808823466300964)
[2025-02-13 02:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18][root][INFO] - Training Epoch: 1/2, step 2081/7134 completed (loss: 0.03218783810734749, acc: 0.9912917017936707)
[2025-02-13 02:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18][root][INFO] - Training Epoch: 1/2, step 2082/7134 completed (loss: 0.1316496580839157, acc: 0.9685534834861755)
[2025-02-13 02:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18][root][INFO] - Training Epoch: 1/2, step 2083/7134 completed (loss: 0.08208214491605759, acc: 0.9851852059364319)
[2025-02-13 02:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19][root][INFO] - Training Epoch: 1/2, step 2084/7134 completed (loss: 0.04970519244670868, acc: 0.9875776171684265)
[2025-02-13 02:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19][root][INFO] - Training Epoch: 1/2, step 2085/7134 completed (loss: 0.035762496292591095, acc: 0.9907578825950623)
[2025-02-13 02:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20][root][INFO] - Training Epoch: 1/2, step 2086/7134 completed (loss: 0.06062322482466698, acc: 0.9817351698875427)
[2025-02-13 02:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20][root][INFO] - Training Epoch: 1/2, step 2087/7134 completed (loss: 0.030218224972486496, acc: 0.9900426864624023)
[2025-02-13 02:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20][root][INFO] - Training Epoch: 1/2, step 2088/7134 completed (loss: 0.07399408519268036, acc: 0.9798271059989929)
[2025-02-13 02:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:21][root][INFO] - Training Epoch: 1/2, step 2089/7134 completed (loss: 0.04743511974811554, acc: 0.9873816967010498)
[2025-02-13 02:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:21][root][INFO] - Training Epoch: 1/2, step 2090/7134 completed (loss: 0.035455089062452316, acc: 0.9936808943748474)
[2025-02-13 02:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:22][root][INFO] - Training Epoch: 1/2, step 2091/7134 completed (loss: 0.04580188915133476, acc: 0.9864864945411682)
[2025-02-13 02:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:22][root][INFO] - Training Epoch: 1/2, step 2092/7134 completed (loss: 0.02811521850526333, acc: 0.988959014415741)
[2025-02-13 02:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23][root][INFO] - Training Epoch: 1/2, step 2093/7134 completed (loss: 0.0380694717168808, acc: 0.9918588995933533)
[2025-02-13 02:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23][root][INFO] - Training Epoch: 1/2, step 2094/7134 completed (loss: 0.09722744673490524, acc: 0.9776875972747803)
[2025-02-13 02:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23][root][INFO] - Training Epoch: 1/2, step 2095/7134 completed (loss: 0.06092366948723793, acc: 0.9881266355514526)
[2025-02-13 02:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24][root][INFO] - Training Epoch: 1/2, step 2096/7134 completed (loss: 0.060212522745132446, acc: 0.982758641242981)
[2025-02-13 02:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24][root][INFO] - Training Epoch: 1/2, step 2097/7134 completed (loss: 0.03833223879337311, acc: 0.991584837436676)
[2025-02-13 02:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25][root][INFO] - Training Epoch: 1/2, step 2098/7134 completed (loss: 0.05691881105303764, acc: 0.9887459874153137)
[2025-02-13 02:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25][root][INFO] - Training Epoch: 1/2, step 2099/7134 completed (loss: 0.057380590587854385, acc: 0.9798164963722229)
[2025-02-13 02:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25][root][INFO] - Training Epoch: 1/2, step 2100/7134 completed (loss: 0.048248499631881714, acc: 0.9920318722724915)
[2025-02-13 02:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:26][root][INFO] - Training Epoch: 1/2, step 2101/7134 completed (loss: 0.05733390524983406, acc: 0.982367753982544)
[2025-02-13 02:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:26][root][INFO] - Training Epoch: 1/2, step 2102/7134 completed (loss: 0.07346589863300323, acc: 0.9732142686843872)
[2025-02-13 02:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27][root][INFO] - Training Epoch: 1/2, step 2103/7134 completed (loss: 0.058068133890628815, acc: 0.9800613522529602)
[2025-02-13 02:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27][root][INFO] - Training Epoch: 1/2, step 2104/7134 completed (loss: 0.07146835327148438, acc: 0.9780927896499634)
[2025-02-13 02:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27][root][INFO] - Training Epoch: 1/2, step 2105/7134 completed (loss: 0.12754830718040466, acc: 0.9718309640884399)
[2025-02-13 02:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:28][root][INFO] - Training Epoch: 1/2, step 2106/7134 completed (loss: 0.09308674931526184, acc: 0.9755747318267822)
[2025-02-13 02:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:28][root][INFO] - Training Epoch: 1/2, step 2107/7134 completed (loss: 0.07433295249938965, acc: 0.9838056564331055)
[2025-02-13 02:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:29][root][INFO] - Training Epoch: 1/2, step 2108/7134 completed (loss: 0.05882088094949722, acc: 0.9820627570152283)
[2025-02-13 02:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:29][root][INFO] - Training Epoch: 1/2, step 2109/7134 completed (loss: 0.09963913261890411, acc: 0.9682539701461792)
[2025-02-13 02:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30][root][INFO] - Training Epoch: 1/2, step 2110/7134 completed (loss: 0.09832781553268433, acc: 0.9752907156944275)
[2025-02-13 02:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30][root][INFO] - Training Epoch: 1/2, step 2111/7134 completed (loss: 0.10099297761917114, acc: 0.9740596413612366)
[2025-02-13 02:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30][root][INFO] - Training Epoch: 1/2, step 2112/7134 completed (loss: 0.12577566504478455, acc: 0.9665604829788208)
[2025-02-13 02:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31][root][INFO] - Training Epoch: 1/2, step 2113/7134 completed (loss: 0.04056428372859955, acc: 0.9880775213241577)
[2025-02-13 02:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31][root][INFO] - Training Epoch: 1/2, step 2114/7134 completed (loss: 0.03446284681558609, acc: 0.9882352948188782)
[2025-02-13 02:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:32][root][INFO] - Training Epoch: 1/2, step 2115/7134 completed (loss: 0.07440705597400665, acc: 0.9799749851226807)
[2025-02-13 02:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:32][root][INFO] - Training Epoch: 1/2, step 2116/7134 completed (loss: 0.08470410853624344, acc: 0.9831288456916809)
[2025-02-13 02:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:32][root][INFO] - Training Epoch: 1/2, step 2117/7134 completed (loss: 0.06577371060848236, acc: 0.9813432693481445)
[2025-02-13 02:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:33][root][INFO] - Training Epoch: 1/2, step 2118/7134 completed (loss: 0.051039185374975204, acc: 0.9890859723091125)
[2025-02-13 02:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:33][root][INFO] - Training Epoch: 1/2, step 2119/7134 completed (loss: 0.07017521560192108, acc: 0.9737532734870911)
[2025-02-13 02:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34][root][INFO] - Training Epoch: 1/2, step 2120/7134 completed (loss: 0.07327532768249512, acc: 0.9802131056785583)
[2025-02-13 02:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34][root][INFO] - Training Epoch: 1/2, step 2121/7134 completed (loss: 0.08429855853319168, acc: 0.9793650507926941)
[2025-02-13 02:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34][root][INFO] - Training Epoch: 1/2, step 2122/7134 completed (loss: 0.04064815863966942, acc: 0.9908854365348816)
[2025-02-13 02:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:35][root][INFO] - Training Epoch: 1/2, step 2123/7134 completed (loss: 0.0527268722653389, acc: 0.9842767119407654)
[2025-02-13 02:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:35][root][INFO] - Training Epoch: 1/2, step 2124/7134 completed (loss: 0.050685152411460876, acc: 0.9844236969947815)
[2025-02-13 02:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36][root][INFO] - Training Epoch: 1/2, step 2125/7134 completed (loss: 0.03819590061903, acc: 0.989393949508667)
[2025-02-13 02:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36][root][INFO] - Training Epoch: 1/2, step 2126/7134 completed (loss: 0.0403900071978569, acc: 0.9848739504814148)
[2025-02-13 02:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36][root][INFO] - Training Epoch: 1/2, step 2127/7134 completed (loss: 0.05246635153889656, acc: 0.983775794506073)
[2025-02-13 02:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:37][root][INFO] - Training Epoch: 1/2, step 2128/7134 completed (loss: 0.06663788855075836, acc: 0.9790025949478149)
[2025-02-13 02:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:37][root][INFO] - Training Epoch: 1/2, step 2129/7134 completed (loss: 0.07490861415863037, acc: 0.9773755669593811)
[2025-02-13 02:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38][root][INFO] - Training Epoch: 1/2, step 2130/7134 completed (loss: 0.0646895170211792, acc: 0.9851484894752502)
[2025-02-13 02:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38][root][INFO] - Training Epoch: 1/2, step 2131/7134 completed (loss: 0.0351191870868206, acc: 0.9876161217689514)
[2025-02-13 02:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38][root][INFO] - Training Epoch: 1/2, step 2132/7134 completed (loss: 0.03501848131418228, acc: 0.9892183542251587)
[2025-02-13 02:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:39][root][INFO] - Training Epoch: 1/2, step 2133/7134 completed (loss: 0.05829589068889618, acc: 0.9815059304237366)
[2025-02-13 02:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:39][root][INFO] - Training Epoch: 1/2, step 2134/7134 completed (loss: 0.04280771687626839, acc: 0.9892904758453369)
[2025-02-13 02:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:40][root][INFO] - Training Epoch: 1/2, step 2135/7134 completed (loss: 0.06132648140192032, acc: 0.9774096608161926)
[2025-02-13 02:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:40][root][INFO] - Training Epoch: 1/2, step 2136/7134 completed (loss: 0.044881671667099, acc: 0.9879840016365051)
[2025-02-13 02:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41][root][INFO] - Training Epoch: 1/2, step 2137/7134 completed (loss: 0.02872944436967373, acc: 0.9909677505493164)
[2025-02-13 02:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41][root][INFO] - Training Epoch: 1/2, step 2138/7134 completed (loss: 0.02386852726340294, acc: 0.9893292784690857)
[2025-02-13 02:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41][root][INFO] - Training Epoch: 1/2, step 2139/7134 completed (loss: 0.04059857502579689, acc: 0.9916782379150391)
[2025-02-13 02:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:42][root][INFO] - Training Epoch: 1/2, step 2140/7134 completed (loss: 0.07970599830150604, acc: 0.9793388247489929)
[2025-02-13 02:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:42][root][INFO] - Training Epoch: 1/2, step 2141/7134 completed (loss: 0.04962393641471863, acc: 0.9853801131248474)
[2025-02-13 02:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43][root][INFO] - Training Epoch: 1/2, step 2142/7134 completed (loss: 0.0655631348490715, acc: 0.9830028414726257)
[2025-02-13 02:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43][root][INFO] - Training Epoch: 1/2, step 2143/7134 completed (loss: 0.07129501551389694, acc: 0.9824561476707458)
[2025-02-13 02:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43][root][INFO] - Training Epoch: 1/2, step 2144/7134 completed (loss: 0.028865238651633263, acc: 0.9960159659385681)
[2025-02-13 02:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:44][root][INFO] - Training Epoch: 1/2, step 2145/7134 completed (loss: 0.10073493421077728, acc: 0.9771101474761963)
[2025-02-13 02:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:44][root][INFO] - Training Epoch: 1/2, step 2146/7134 completed (loss: 0.07867904752492905, acc: 0.9771126508712769)
[2025-02-13 02:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:45][root][INFO] - Training Epoch: 1/2, step 2147/7134 completed (loss: 0.07043776661157608, acc: 0.9850373864173889)
[2025-02-13 02:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:45][root][INFO] - Training Epoch: 1/2, step 2148/7134 completed (loss: 0.07930674403905869, acc: 0.9825327396392822)
[2025-02-13 02:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46][root][INFO] - Training Epoch: 1/2, step 2149/7134 completed (loss: 0.03589805215597153, acc: 0.9901408553123474)
[2025-02-13 02:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46][root][INFO] - Training Epoch: 1/2, step 2150/7134 completed (loss: 0.05438452586531639, acc: 0.981333315372467)
[2025-02-13 02:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46][root][INFO] - Training Epoch: 1/2, step 2151/7134 completed (loss: 0.053336251527071, acc: 0.9842857122421265)
[2025-02-13 02:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:47][root][INFO] - Training Epoch: 1/2, step 2152/7134 completed (loss: 0.13559693098068237, acc: 0.9632353186607361)
[2025-02-13 02:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:47][root][INFO] - Training Epoch: 1/2, step 2153/7134 completed (loss: 0.06402622163295746, acc: 0.985571563243866)
[2025-02-13 02:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:48][root][INFO] - Training Epoch: 1/2, step 2154/7134 completed (loss: 0.08800759166479111, acc: 0.9772440195083618)
[2025-02-13 02:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:48][root][INFO] - Training Epoch: 1/2, step 2155/7134 completed (loss: 0.12882551550865173, acc: 0.9752747416496277)
[2025-02-13 02:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:49][root][INFO] - Training Epoch: 1/2, step 2156/7134 completed (loss: 0.07675597071647644, acc: 0.9739583134651184)
[2025-02-13 02:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:49][root][INFO] - Training Epoch: 1/2, step 2157/7134 completed (loss: 0.07235032320022583, acc: 0.9743016958236694)
[2025-02-13 02:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50][root][INFO] - Training Epoch: 1/2, step 2158/7134 completed (loss: 0.0999193862080574, acc: 0.976190447807312)
[2025-02-13 02:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50][root][INFO] - Training Epoch: 1/2, step 2159/7134 completed (loss: 0.06129726767539978, acc: 0.9885057210922241)
[2025-02-13 02:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50][root][INFO] - Training Epoch: 1/2, step 2160/7134 completed (loss: 0.08287634700536728, acc: 0.9752407073974609)
[2025-02-13 02:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:51][root][INFO] - Training Epoch: 1/2, step 2161/7134 completed (loss: 0.09967400133609772, acc: 0.9795134663581848)
[2025-02-13 02:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:51][root][INFO] - Training Epoch: 1/2, step 2162/7134 completed (loss: 0.054782070219516754, acc: 0.9878970980644226)
[2025-02-13 02:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:52][root][INFO] - Training Epoch: 1/2, step 2163/7134 completed (loss: 0.07222327589988708, acc: 0.9795275330543518)
[2025-02-13 02:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:52][root][INFO] - Training Epoch: 1/2, step 2164/7134 completed (loss: 0.06408669799566269, acc: 0.9862778782844543)
[2025-02-13 02:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53][root][INFO] - Training Epoch: 1/2, step 2165/7134 completed (loss: 0.055460136383771896, acc: 0.9842180609703064)
[2025-02-13 02:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53][root][INFO] - Training Epoch: 1/2, step 2166/7134 completed (loss: 0.07447526603937149, acc: 0.9778645634651184)
[2025-02-13 02:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53][root][INFO] - Training Epoch: 1/2, step 2167/7134 completed (loss: 0.06017766892910004, acc: 0.9844098091125488)
[2025-02-13 02:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:54][root][INFO] - Training Epoch: 1/2, step 2168/7134 completed (loss: 0.043214842677116394, acc: 0.9904761910438538)
[2025-02-13 02:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:54][root][INFO] - Training Epoch: 1/2, step 2169/7134 completed (loss: 0.06296350061893463, acc: 0.9837586879730225)
[2025-02-13 02:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:55][root][INFO] - Training Epoch: 1/2, step 2170/7134 completed (loss: 0.06031397730112076, acc: 0.9868995547294617)
[2025-02-13 02:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:55][root][INFO] - Training Epoch: 1/2, step 2171/7134 completed (loss: 0.035936158150434494, acc: 0.9909399747848511)
[2025-02-13 02:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56][root][INFO] - Training Epoch: 1/2, step 2172/7134 completed (loss: 0.04192347079515457, acc: 0.9886363744735718)
[2025-02-13 02:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56][root][INFO] - Training Epoch: 1/2, step 2173/7134 completed (loss: 0.06653988361358643, acc: 0.9864048361778259)
[2025-02-13 02:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56][root][INFO] - Training Epoch: 1/2, step 2174/7134 completed (loss: 0.04036494344472885, acc: 0.9900709390640259)
[2025-02-13 02:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:57][root][INFO] - Training Epoch: 1/2, step 2175/7134 completed (loss: 0.10837287455797195, acc: 0.9819944500923157)
[2025-02-13 02:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:57][root][INFO] - Training Epoch: 1/2, step 2176/7134 completed (loss: 0.06211499124765396, acc: 0.9845971465110779)
[2025-02-13 02:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:58][root][INFO] - Training Epoch: 1/2, step 2177/7134 completed (loss: 0.06623717397451401, acc: 0.984415590763092)
[2025-02-13 02:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:58][root][INFO] - Training Epoch: 1/2, step 2178/7134 completed (loss: 0.041232846677303314, acc: 0.9867549538612366)
[2025-02-13 02:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:59][root][INFO] - Training Epoch: 1/2, step 2179/7134 completed (loss: 0.04173710569739342, acc: 0.9873417615890503)
[2025-02-13 02:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:59][root][INFO] - Training Epoch: 1/2, step 2180/7134 completed (loss: 0.07019716501235962, acc: 0.9822695255279541)
[2025-02-13 02:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:00][root][INFO] - Training Epoch: 1/2, step 2181/7134 completed (loss: 0.028759626671671867, acc: 0.9928469061851501)
[2025-02-13 02:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:00][root][INFO] - Training Epoch: 1/2, step 2182/7134 completed (loss: 0.08412669599056244, acc: 0.9780853390693665)
[2025-02-13 02:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01][root][INFO] - Training Epoch: 1/2, step 2183/7134 completed (loss: 0.05937085673213005, acc: 0.9802095293998718)
[2025-02-13 02:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01][root][INFO] - Training Epoch: 1/2, step 2184/7134 completed (loss: 0.0468713715672493, acc: 0.9936102032661438)
[2025-02-13 02:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01][root][INFO] - Training Epoch: 1/2, step 2185/7134 completed (loss: 0.08319951593875885, acc: 0.9830028414726257)
[2025-02-13 02:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:02][root][INFO] - Training Epoch: 1/2, step 2186/7134 completed (loss: 0.1118365153670311, acc: 0.9724473357200623)
[2025-02-13 02:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:02][root][INFO] - Training Epoch: 1/2, step 2187/7134 completed (loss: 0.1376023292541504, acc: 0.9728000164031982)
[2025-02-13 02:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:03][root][INFO] - Training Epoch: 1/2, step 2188/7134 completed (loss: 0.05329349264502525, acc: 0.9822161197662354)
[2025-02-13 02:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:03][root][INFO] - Training Epoch: 1/2, step 2189/7134 completed (loss: 0.0830911174416542, acc: 0.9816513657569885)
[2025-02-13 02:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04][root][INFO] - Training Epoch: 1/2, step 2190/7134 completed (loss: 0.07796233147382736, acc: 0.9749340415000916)
[2025-02-13 02:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04][root][INFO] - Training Epoch: 1/2, step 2191/7134 completed (loss: 0.09440615773200989, acc: 0.9647696614265442)
[2025-02-13 02:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04][root][INFO] - Training Epoch: 1/2, step 2192/7134 completed (loss: 0.03419389948248863, acc: 0.9887005686759949)
[2025-02-13 02:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:05][root][INFO] - Training Epoch: 1/2, step 2193/7134 completed (loss: 0.05581029877066612, acc: 0.9827127456665039)
[2025-02-13 02:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:05][root][INFO] - Training Epoch: 1/2, step 2194/7134 completed (loss: 0.05433408170938492, acc: 0.9849246144294739)
[2025-02-13 02:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06][root][INFO] - Training Epoch: 1/2, step 2195/7134 completed (loss: 0.06149277836084366, acc: 0.984000027179718)
[2025-02-13 02:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06][root][INFO] - Training Epoch: 1/2, step 2196/7134 completed (loss: 0.06619994342327118, acc: 0.9792746305465698)
[2025-02-13 02:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06][root][INFO] - Training Epoch: 1/2, step 2197/7134 completed (loss: 0.06705502420663834, acc: 0.9781420826911926)
[2025-02-13 02:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:07][root][INFO] - Training Epoch: 1/2, step 2198/7134 completed (loss: 0.09885641187429428, acc: 0.9743589758872986)
[2025-02-13 02:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:07][root][INFO] - Training Epoch: 1/2, step 2199/7134 completed (loss: 0.05206593871116638, acc: 0.9803625345230103)
[2025-02-13 02:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08][root][INFO] - Training Epoch: 1/2, step 2200/7134 completed (loss: 0.053813353180885315, acc: 0.9819168448448181)
[2025-02-13 02:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08][root][INFO] - Training Epoch: 1/2, step 2201/7134 completed (loss: 0.03945806622505188, acc: 0.9848484992980957)
[2025-02-13 02:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08][root][INFO] - Training Epoch: 1/2, step 2202/7134 completed (loss: 0.07023901492357254, acc: 0.9782971739768982)
[2025-02-13 02:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:09][root][INFO] - Training Epoch: 1/2, step 2203/7134 completed (loss: 0.039731938391923904, acc: 0.9890410900115967)
[2025-02-13 02:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:09][root][INFO] - Training Epoch: 1/2, step 2204/7134 completed (loss: 0.040964510291814804, acc: 0.9853801131248474)
[2025-02-13 02:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:10][root][INFO] - Training Epoch: 1/2, step 2205/7134 completed (loss: 0.04479724168777466, acc: 0.9859648942947388)
[2025-02-13 02:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:10][root][INFO] - Training Epoch: 1/2, step 2206/7134 completed (loss: 0.10333037376403809, acc: 0.9718804955482483)
[2025-02-13 02:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11][root][INFO] - Training Epoch: 1/2, step 2207/7134 completed (loss: 0.04397626593708992, acc: 0.9919224381446838)
[2025-02-13 02:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11][root][INFO] - Training Epoch: 1/2, step 2208/7134 completed (loss: 0.07671336829662323, acc: 0.9801653027534485)
[2025-02-13 02:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11][root][INFO] - Training Epoch: 1/2, step 2209/7134 completed (loss: 0.06785273551940918, acc: 0.987270176410675)
[2025-02-13 02:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:12][root][INFO] - Training Epoch: 1/2, step 2210/7134 completed (loss: 0.07470902800559998, acc: 0.9772382378578186)
[2025-02-13 02:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:12][root][INFO] - Training Epoch: 1/2, step 2211/7134 completed (loss: 0.05013114586472511, acc: 0.9861496090888977)
[2025-02-13 02:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13][root][INFO] - Training Epoch: 1/2, step 2212/7134 completed (loss: 0.023819169029593468, acc: 0.9930555820465088)
[2025-02-13 02:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13][root][INFO] - Training Epoch: 1/2, step 2213/7134 completed (loss: 0.042160917073488235, acc: 0.9830508232116699)
[2025-02-13 02:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13][root][INFO] - Training Epoch: 1/2, step 2214/7134 completed (loss: 0.06762583553791046, acc: 0.9820895791053772)
[2025-02-13 02:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:14][root][INFO] - Training Epoch: 1/2, step 2215/7134 completed (loss: 0.03468063846230507, acc: 0.9901685118675232)
[2025-02-13 02:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:14][root][INFO] - Training Epoch: 1/2, step 2216/7134 completed (loss: 0.023022376000881195, acc: 0.9946595430374146)
[2025-02-13 02:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15][root][INFO] - Training Epoch: 1/2, step 2217/7134 completed (loss: 0.04191729053854942, acc: 0.9850560426712036)
[2025-02-13 02:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15][root][INFO] - Training Epoch: 1/2, step 2218/7134 completed (loss: 0.05253821983933449, acc: 0.9885057210922241)
[2025-02-13 02:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15][root][INFO] - Training Epoch: 1/2, step 2219/7134 completed (loss: 0.026337070390582085, acc: 0.9907264113426208)
[2025-02-13 02:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:16][root][INFO] - Training Epoch: 1/2, step 2220/7134 completed (loss: 0.032832708209753036, acc: 0.9858490824699402)
[2025-02-13 02:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:16][root][INFO] - Training Epoch: 1/2, step 2221/7134 completed (loss: 0.05271846801042557, acc: 0.9896103739738464)
[2025-02-13 02:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:17][root][INFO] - Training Epoch: 1/2, step 2222/7134 completed (loss: 0.05855535343289375, acc: 0.9826589822769165)
[2025-02-13 02:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:17][root][INFO] - Training Epoch: 1/2, step 2223/7134 completed (loss: 0.05106281861662865, acc: 0.9832636117935181)
[2025-02-13 02:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18][root][INFO] - Training Epoch: 1/2, step 2224/7134 completed (loss: 0.05412605032324791, acc: 0.9849108457565308)
[2025-02-13 02:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18][root][INFO] - Training Epoch: 1/2, step 2225/7134 completed (loss: 0.042014941573143005, acc: 0.986975371837616)
[2025-02-13 02:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18][root][INFO] - Training Epoch: 1/2, step 2226/7134 completed (loss: 0.04177513346076012, acc: 0.9832572340965271)
[2025-02-13 02:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:19][root][INFO] - Training Epoch: 1/2, step 2227/7134 completed (loss: 0.034613557159900665, acc: 0.9926793575286865)
[2025-02-13 02:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:19][root][INFO] - Training Epoch: 1/2, step 2228/7134 completed (loss: 0.03941618278622627, acc: 0.9837398529052734)
[2025-02-13 02:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20][root][INFO] - Training Epoch: 1/2, step 2229/7134 completed (loss: 0.04645083472132683, acc: 0.9900000095367432)
[2025-02-13 02:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20][root][INFO] - Training Epoch: 1/2, step 2230/7134 completed (loss: 0.03120037168264389, acc: 0.991909384727478)
[2025-02-13 02:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20][root][INFO] - Training Epoch: 1/2, step 2231/7134 completed (loss: 0.06063256785273552, acc: 0.9820716977119446)
[2025-02-13 02:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:21][root][INFO] - Training Epoch: 1/2, step 2232/7134 completed (loss: 0.019352195784449577, acc: 0.9959893226623535)
[2025-02-13 02:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:21][root][INFO] - Training Epoch: 1/2, step 2233/7134 completed (loss: 0.04719172790646553, acc: 0.9886202216148376)
[2025-02-13 02:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22][root][INFO] - Training Epoch: 1/2, step 2234/7134 completed (loss: 0.023993486538529396, acc: 0.994535505771637)
[2025-02-13 02:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22][root][INFO] - Training Epoch: 1/2, step 2235/7134 completed (loss: 0.06781645119190216, acc: 0.9890282154083252)
[2025-02-13 02:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22][root][INFO] - Training Epoch: 1/2, step 2236/7134 completed (loss: 0.034998904913663864, acc: 0.9850075244903564)
[2025-02-13 02:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:23][root][INFO] - Training Epoch: 1/2, step 2237/7134 completed (loss: 0.04920360445976257, acc: 0.9830268621444702)
[2025-02-13 02:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:23][root][INFO] - Training Epoch: 1/2, step 2238/7134 completed (loss: 0.09595649689435959, acc: 0.9739508032798767)
[2025-02-13 02:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:24][root][INFO] - Training Epoch: 1/2, step 2239/7134 completed (loss: 0.10847272723913193, acc: 0.9727272987365723)
[2025-02-13 02:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:24][root][INFO] - Training Epoch: 1/2, step 2240/7134 completed (loss: 0.08387075364589691, acc: 0.9818181991577148)
[2025-02-13 02:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25][root][INFO] - Training Epoch: 1/2, step 2241/7134 completed (loss: 0.11602950096130371, acc: 0.9692671298980713)
[2025-02-13 02:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25][root][INFO] - Training Epoch: 1/2, step 2242/7134 completed (loss: 0.15516535937786102, acc: 0.9643705487251282)
[2025-02-13 02:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25][root][INFO] - Training Epoch: 1/2, step 2243/7134 completed (loss: 0.18277274072170258, acc: 0.9557251930236816)
[2025-02-13 02:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:26][root][INFO] - Training Epoch: 1/2, step 2244/7134 completed (loss: 0.07590775191783905, acc: 0.9685534834861755)
[2025-02-13 02:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:26][root][INFO] - Training Epoch: 1/2, step 2245/7134 completed (loss: 0.1367776244878769, acc: 0.9579287767410278)
[2025-02-13 02:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27][root][INFO] - Training Epoch: 1/2, step 2246/7134 completed (loss: 0.17920690774917603, acc: 0.9445946216583252)
[2025-02-13 02:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27][root][INFO] - Training Epoch: 1/2, step 2247/7134 completed (loss: 0.12588588893413544, acc: 0.9655172228813171)
[2025-02-13 02:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27][root][INFO] - Training Epoch: 1/2, step 2248/7134 completed (loss: 0.10529250651597977, acc: 0.9806362390518188)
[2025-02-13 02:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:28][root][INFO] - Training Epoch: 1/2, step 2249/7134 completed (loss: 0.0800449550151825, acc: 0.9706704020500183)
[2025-02-13 02:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:28][root][INFO] - Training Epoch: 1/2, step 2250/7134 completed (loss: 0.09988632053136826, acc: 0.970588207244873)
[2025-02-13 02:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29][root][INFO] - Training Epoch: 1/2, step 2251/7134 completed (loss: 0.06877855956554413, acc: 0.9773242473602295)
[2025-02-13 02:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29][root][INFO] - Training Epoch: 1/2, step 2252/7134 completed (loss: 0.09268501400947571, acc: 0.9789915680885315)
[2025-02-13 02:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29][root][INFO] - Training Epoch: 1/2, step 2253/7134 completed (loss: 0.07624952495098114, acc: 0.97826087474823)
[2025-02-13 02:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:30][root][INFO] - Training Epoch: 1/2, step 2254/7134 completed (loss: 0.1520271897315979, acc: 0.9482071995735168)
[2025-02-13 02:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:30][root][INFO] - Training Epoch: 1/2, step 2255/7134 completed (loss: 0.0629008412361145, acc: 0.9833080172538757)
[2025-02-13 02:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31][root][INFO] - Training Epoch: 1/2, step 2256/7134 completed (loss: 0.1855425089597702, acc: 0.9424778819084167)
[2025-02-13 02:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31][root][INFO] - Training Epoch: 1/2, step 2257/7134 completed (loss: 0.061366114765405655, acc: 0.9833837151527405)
[2025-02-13 02:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31][root][INFO] - Training Epoch: 1/2, step 2258/7134 completed (loss: 0.07440921664237976, acc: 0.9769230484962463)
[2025-02-13 02:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:32][root][INFO] - Training Epoch: 1/2, step 2259/7134 completed (loss: 0.04905666410923004, acc: 0.9844444394111633)
[2025-02-13 02:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:32][root][INFO] - Training Epoch: 1/2, step 2260/7134 completed (loss: 0.08563117682933807, acc: 0.978787899017334)
[2025-02-13 02:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33][root][INFO] - Training Epoch: 1/2, step 2261/7134 completed (loss: 0.1271073967218399, acc: 0.9616788029670715)
[2025-02-13 02:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33][root][INFO] - Training Epoch: 1/2, step 2262/7134 completed (loss: 0.04521191120147705, acc: 0.9815837740898132)
[2025-02-13 02:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33][root][INFO] - Training Epoch: 1/2, step 2263/7134 completed (loss: 0.08559446781873703, acc: 0.9783783555030823)
[2025-02-13 02:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:34][root][INFO] - Training Epoch: 1/2, step 2264/7134 completed (loss: 0.1040656790137291, acc: 0.9732770919799805)
[2025-02-13 02:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:34][root][INFO] - Training Epoch: 1/2, step 2265/7134 completed (loss: 0.1189773678779602, acc: 0.9673539400100708)
[2025-02-13 02:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35][root][INFO] - Training Epoch: 1/2, step 2266/7134 completed (loss: 0.07343093305826187, acc: 0.9839141964912415)
[2025-02-13 02:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35][root][INFO] - Training Epoch: 1/2, step 2267/7134 completed (loss: 0.031151311472058296, acc: 0.9958847761154175)
[2025-02-13 02:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35][root][INFO] - Training Epoch: 1/2, step 2268/7134 completed (loss: 0.04893533140420914, acc: 0.9856528043746948)
[2025-02-13 02:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:36][root][INFO] - Training Epoch: 1/2, step 2269/7134 completed (loss: 0.031626954674720764, acc: 0.9911949634552002)
[2025-02-13 02:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:36][root][INFO] - Training Epoch: 1/2, step 2270/7134 completed (loss: 0.08593706041574478, acc: 0.9684908986091614)
[2025-02-13 02:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:37][root][INFO] - Training Epoch: 1/2, step 2271/7134 completed (loss: 0.08608914166688919, acc: 0.970080554485321)
[2025-02-13 02:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:37][root][INFO] - Training Epoch: 1/2, step 2272/7134 completed (loss: 0.05858729034662247, acc: 0.9865525960922241)
[2025-02-13 02:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:38][root][INFO] - Training Epoch: 1/2, step 2273/7134 completed (loss: 0.03435450419783592, acc: 0.9880239367485046)
[2025-02-13 02:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:38][root][INFO] - Training Epoch: 1/2, step 2274/7134 completed (loss: 0.14522825181484222, acc: 0.9707006216049194)
[2025-02-13 02:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39][root][INFO] - Training Epoch: 1/2, step 2275/7134 completed (loss: 0.045057229697704315, acc: 0.98591548204422)
[2025-02-13 02:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39][root][INFO] - Training Epoch: 1/2, step 2276/7134 completed (loss: 0.05250241979956627, acc: 0.9893364906311035)
[2025-02-13 02:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39][root][INFO] - Training Epoch: 1/2, step 2277/7134 completed (loss: 0.07015778124332428, acc: 0.980369508266449)
[2025-02-13 02:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40][root][INFO] - Training Epoch: 1/2, step 2278/7134 completed (loss: 0.15427415072917938, acc: 0.969648540019989)
[2025-02-13 02:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40][root][INFO] - Training Epoch: 1/2, step 2279/7134 completed (loss: 0.060022395104169846, acc: 0.9811097979545593)
[2025-02-13 02:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:41][root][INFO] - Training Epoch: 1/2, step 2280/7134 completed (loss: 0.09032189100980759, acc: 0.9775679111480713)
[2025-02-13 02:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:41][root][INFO] - Training Epoch: 1/2, step 2281/7134 completed (loss: 0.12955018877983093, acc: 0.9746666550636292)
[2025-02-13 02:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42][root][INFO] - Training Epoch: 1/2, step 2282/7134 completed (loss: 0.022821327671408653, acc: 0.9962916970252991)
[2025-02-13 02:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42][root][INFO] - Training Epoch: 1/2, step 2283/7134 completed (loss: 0.06407184898853302, acc: 0.97555011510849)
[2025-02-13 02:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42][root][INFO] - Training Epoch: 1/2, step 2284/7134 completed (loss: 0.060592904686927795, acc: 0.9807976484298706)
[2025-02-13 02:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:43][root][INFO] - Training Epoch: 1/2, step 2285/7134 completed (loss: 0.058747537434101105, acc: 0.9777034521102905)
[2025-02-13 02:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:43][root][INFO] - Training Epoch: 1/2, step 2286/7134 completed (loss: 0.09523255378007889, acc: 0.9727626442909241)
[2025-02-13 02:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44][root][INFO] - Training Epoch: 1/2, step 2287/7134 completed (loss: 0.04989015683531761, acc: 0.9812332391738892)
[2025-02-13 02:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44][root][INFO] - Training Epoch: 1/2, step 2288/7134 completed (loss: 0.04736209660768509, acc: 0.9893993139266968)
[2025-02-13 02:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:45][root][INFO] - Training Epoch: 1/2, step 2289/7134 completed (loss: 0.07079611718654633, acc: 0.9833119511604309)
[2025-02-13 02:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:45][root][INFO] - Training Epoch: 1/2, step 2290/7134 completed (loss: 0.05818162485957146, acc: 0.9834395051002502)
[2025-02-13 02:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46][root][INFO] - Training Epoch: 1/2, step 2291/7134 completed (loss: 0.09988659620285034, acc: 0.9689608812332153)
[2025-02-13 02:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46][root][INFO] - Training Epoch: 1/2, step 2292/7134 completed (loss: 0.12163815647363663, acc: 0.9767742156982422)
[2025-02-13 02:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46][root][INFO] - Training Epoch: 1/2, step 2293/7134 completed (loss: 0.07577493041753769, acc: 0.9755011200904846)
[2025-02-13 02:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:47][root][INFO] - Training Epoch: 1/2, step 2294/7134 completed (loss: 0.06825757771730423, acc: 0.9820022583007812)
[2025-02-13 02:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:47][root][INFO] - Training Epoch: 1/2, step 2295/7134 completed (loss: 0.05938339978456497, acc: 0.9800570011138916)
[2025-02-13 02:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48][root][INFO] - Training Epoch: 1/2, step 2296/7134 completed (loss: 0.039607059210538864, acc: 0.9893617033958435)
[2025-02-13 02:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48][root][INFO] - Training Epoch: 1/2, step 2297/7134 completed (loss: 0.048800237476825714, acc: 0.9847561120986938)
[2025-02-13 02:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:49][root][INFO] - Training Epoch: 1/2, step 2298/7134 completed (loss: 0.09145668894052505, acc: 0.9726603627204895)
[2025-02-13 02:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:49][root][INFO] - Training Epoch: 1/2, step 2299/7134 completed (loss: 0.0483667366206646, acc: 0.9817975163459778)
[2025-02-13 02:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:50][root][INFO] - Training Epoch: 1/2, step 2300/7134 completed (loss: 0.023884519934654236, acc: 0.990980863571167)
[2025-02-13 02:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:50][root][INFO] - Training Epoch: 1/2, step 2301/7134 completed (loss: 0.07411409914493561, acc: 0.9761589169502258)
[2025-02-13 02:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51][root][INFO] - Training Epoch: 1/2, step 2302/7134 completed (loss: 0.07814191281795502, acc: 0.9767156839370728)
[2025-02-13 02:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51][root][INFO] - Training Epoch: 1/2, step 2303/7134 completed (loss: 0.05730360001325607, acc: 0.980637788772583)
[2025-02-13 02:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51][root][INFO] - Training Epoch: 1/2, step 2304/7134 completed (loss: 0.08657228946685791, acc: 0.9753340482711792)
[2025-02-13 02:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:52][root][INFO] - Training Epoch: 1/2, step 2305/7134 completed (loss: 0.06281857937574387, acc: 0.9776358008384705)
[2025-02-13 02:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:52][root][INFO] - Training Epoch: 1/2, step 2306/7134 completed (loss: 0.04635602608323097, acc: 0.9855875968933105)
[2025-02-13 02:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:53][root][INFO] - Training Epoch: 1/2, step 2307/7134 completed (loss: 0.05882301926612854, acc: 0.9865702390670776)
[2025-02-13 02:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:53][root][INFO] - Training Epoch: 1/2, step 2308/7134 completed (loss: 0.08668570965528488, acc: 0.9775280952453613)
[2025-02-13 02:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:54][root][INFO] - Training Epoch: 1/2, step 2309/7134 completed (loss: 0.040372323244810104, acc: 0.9904076457023621)
[2025-02-13 02:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:54][root][INFO] - Training Epoch: 1/2, step 2310/7134 completed (loss: 0.04574696347117424, acc: 0.9864406585693359)
[2025-02-13 02:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55][root][INFO] - Training Epoch: 1/2, step 2311/7134 completed (loss: 0.05594801530241966, acc: 0.9862385392189026)
[2025-02-13 02:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55][root][INFO] - Training Epoch: 1/2, step 2312/7134 completed (loss: 0.03444042429327965, acc: 0.988120973110199)
[2025-02-13 02:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55][root][INFO] - Training Epoch: 1/2, step 2313/7134 completed (loss: 0.0849807858467102, acc: 0.9805936217308044)
[2025-02-13 02:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:56][root][INFO] - Training Epoch: 1/2, step 2314/7134 completed (loss: 0.03282054513692856, acc: 0.9922308325767517)
[2025-02-13 02:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:56][root][INFO] - Training Epoch: 1/2, step 2315/7134 completed (loss: 0.05346524715423584, acc: 0.9847058653831482)
[2025-02-13 02:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:57][root][INFO] - Training Epoch: 1/2, step 2316/7134 completed (loss: 0.04988912120461464, acc: 0.9896432757377625)
[2025-02-13 02:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:57][root][INFO] - Training Epoch: 1/2, step 2317/7134 completed (loss: 0.04902170971035957, acc: 0.9894179701805115)
[2025-02-13 02:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:58][root][INFO] - Training Epoch: 1/2, step 2318/7134 completed (loss: 0.03785150498151779, acc: 0.9864016771316528)
[2025-02-13 02:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:58][root][INFO] - Training Epoch: 1/2, step 2319/7134 completed (loss: 0.04712870717048645, acc: 0.9846827387809753)
[2025-02-13 02:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59][root][INFO] - Training Epoch: 1/2, step 2320/7134 completed (loss: 0.04447834566235542, acc: 0.9852272868156433)
[2025-02-13 02:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59][root][INFO] - Training Epoch: 1/2, step 2321/7134 completed (loss: 0.05357950180768967, acc: 0.987500011920929)
[2025-02-13 02:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59][root][INFO] - Training Epoch: 1/2, step 2322/7134 completed (loss: 0.04916693642735481, acc: 0.9837037324905396)
[2025-02-13 02:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:00][root][INFO] - Training Epoch: 1/2, step 2323/7134 completed (loss: 0.08019968122243881, acc: 0.9731861352920532)
[2025-02-13 02:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:00][root][INFO] - Training Epoch: 1/2, step 2324/7134 completed (loss: 0.0902344286441803, acc: 0.9800570011138916)
[2025-02-13 02:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:01][root][INFO] - Training Epoch: 1/2, step 2325/7134 completed (loss: 0.057885460555553436, acc: 0.9777777791023254)
[2025-02-13 02:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:01][root][INFO] - Training Epoch: 1/2, step 2326/7134 completed (loss: 0.0328967459499836, acc: 0.9871323704719543)
[2025-02-13 02:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02][root][INFO] - Training Epoch: 1/2, step 2327/7134 completed (loss: 0.0328562892973423, acc: 0.9868852496147156)
[2025-02-13 02:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02][root][INFO] - Training Epoch: 1/2, step 2328/7134 completed (loss: 0.10026513040065765, acc: 0.9784946441650391)
[2025-02-13 02:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02][root][INFO] - Training Epoch: 1/2, step 2329/7134 completed (loss: 0.03859023377299309, acc: 0.9906976819038391)
[2025-02-13 02:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:03][root][INFO] - Training Epoch: 1/2, step 2330/7134 completed (loss: 0.03129611164331436, acc: 0.989313006401062)
[2025-02-13 02:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:03][root][INFO] - Training Epoch: 1/2, step 2331/7134 completed (loss: 0.03959321230649948, acc: 0.9892638325691223)
[2025-02-13 02:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04][root][INFO] - Training Epoch: 1/2, step 2332/7134 completed (loss: 0.058596838265657425, acc: 0.9925705790519714)
[2025-02-13 02:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04][root][INFO] - Training Epoch: 1/2, step 2333/7134 completed (loss: 0.023226866498589516, acc: 0.9910714030265808)
[2025-02-13 02:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04][root][INFO] - Training Epoch: 1/2, step 2334/7134 completed (loss: 0.037071287631988525, acc: 0.9895522594451904)
[2025-02-13 02:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:05][root][INFO] - Training Epoch: 1/2, step 2335/7134 completed (loss: 0.06874258816242218, acc: 0.9838709831237793)
[2025-02-13 02:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:05][root][INFO] - Training Epoch: 1/2, step 2336/7134 completed (loss: 0.012371938675642014, acc: 0.9984543919563293)
[2025-02-13 02:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06][root][INFO] - Training Epoch: 1/2, step 2337/7134 completed (loss: 0.008875719271600246, acc: 0.9983525276184082)
[2025-02-13 02:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06][root][INFO] - Training Epoch: 1/2, step 2338/7134 completed (loss: 0.054682157933712006, acc: 0.984544038772583)
[2025-02-13 02:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06][root][INFO] - Training Epoch: 1/2, step 2339/7134 completed (loss: 0.04662219434976578, acc: 0.985318124294281)
[2025-02-13 02:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:07][root][INFO] - Training Epoch: 1/2, step 2340/7134 completed (loss: 0.046654269099235535, acc: 0.9834087491035461)
[2025-02-13 02:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:07][root][INFO] - Training Epoch: 1/2, step 2341/7134 completed (loss: 0.02430560253560543, acc: 0.9943609237670898)
[2025-02-13 02:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08][root][INFO] - Training Epoch: 1/2, step 2342/7134 completed (loss: 0.04380332678556442, acc: 0.9822747707366943)
[2025-02-13 02:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08][root][INFO] - Training Epoch: 1/2, step 2343/7134 completed (loss: 0.038968708366155624, acc: 0.9904000163078308)
[2025-02-13 02:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08][root][INFO] - Training Epoch: 1/2, step 2344/7134 completed (loss: 0.0274912491440773, acc: 0.992175281047821)
[2025-02-13 02:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:09][root][INFO] - Training Epoch: 1/2, step 2345/7134 completed (loss: 0.038409523665905, acc: 0.9860140085220337)
[2025-02-13 02:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:09][root][INFO] - Training Epoch: 1/2, step 2346/7134 completed (loss: 0.03146470710635185, acc: 0.9886685609817505)
[2025-02-13 02:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:10][root][INFO] - Training Epoch: 1/2, step 2347/7134 completed (loss: 0.08002131432294846, acc: 0.9802631735801697)
[2025-02-13 02:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:10][root][INFO] - Training Epoch: 1/2, step 2348/7134 completed (loss: 0.09910975396633148, acc: 0.9709962010383606)
[2025-02-13 02:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11][root][INFO] - Training Epoch: 1/2, step 2349/7134 completed (loss: 0.09823240339756012, acc: 0.9777777791023254)
[2025-02-13 02:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11][root][INFO] - Training Epoch: 1/2, step 2350/7134 completed (loss: 0.1299685537815094, acc: 0.9649389982223511)
[2025-02-13 02:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11][root][INFO] - Training Epoch: 1/2, step 2351/7134 completed (loss: 0.08273652195930481, acc: 0.9726775884628296)
[2025-02-13 02:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:12][root][INFO] - Training Epoch: 1/2, step 2352/7134 completed (loss: 0.07681626826524734, acc: 0.9727563858032227)
[2025-02-13 02:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:12][root][INFO] - Training Epoch: 1/2, step 2353/7134 completed (loss: 0.05178897827863693, acc: 0.9829787015914917)
[2025-02-13 02:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:13][root][INFO] - Training Epoch: 1/2, step 2354/7134 completed (loss: 0.056509315967559814, acc: 0.9848101139068604)
[2025-02-13 02:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:13][root][INFO] - Training Epoch: 1/2, step 2355/7134 completed (loss: 0.06585702300071716, acc: 0.9876543283462524)
[2025-02-13 02:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14][root][INFO] - Training Epoch: 1/2, step 2356/7134 completed (loss: 0.07265644520521164, acc: 0.982332170009613)
[2025-02-13 02:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14][root][INFO] - Training Epoch: 1/2, step 2357/7134 completed (loss: 0.0971696674823761, acc: 0.9702380895614624)
[2025-02-13 02:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14][root][INFO] - Training Epoch: 1/2, step 2358/7134 completed (loss: 0.10794425755739212, acc: 0.9759036302566528)
[2025-02-13 02:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:15][root][INFO] - Training Epoch: 1/2, step 2359/7134 completed (loss: 0.09215930849313736, acc: 0.9735682606697083)
[2025-02-13 02:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:15][root][INFO] - Training Epoch: 1/2, step 2360/7134 completed (loss: 0.07599196583032608, acc: 0.9780380725860596)
[2025-02-13 02:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16][root][INFO] - Training Epoch: 1/2, step 2361/7134 completed (loss: 0.08722491562366486, acc: 0.9778106212615967)
[2025-02-13 02:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16][root][INFO] - Training Epoch: 1/2, step 2362/7134 completed (loss: 0.07573173195123672, acc: 0.9753363132476807)
[2025-02-13 02:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16][root][INFO] - Training Epoch: 1/2, step 2363/7134 completed (loss: 0.07492478936910629, acc: 0.9779837727546692)
[2025-02-13 02:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:17][root][INFO] - Training Epoch: 1/2, step 2364/7134 completed (loss: 0.06917892396450043, acc: 0.9802131056785583)
[2025-02-13 02:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:17][root][INFO] - Training Epoch: 1/2, step 2365/7134 completed (loss: 0.05099163204431534, acc: 0.9820936918258667)
[2025-02-13 02:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:18][root][INFO] - Training Epoch: 1/2, step 2366/7134 completed (loss: 0.044033437967300415, acc: 0.9907407164573669)
[2025-02-13 02:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:18][root][INFO] - Training Epoch: 1/2, step 2367/7134 completed (loss: 0.09329953044652939, acc: 0.9849246144294739)
[2025-02-13 02:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19][root][INFO] - Training Epoch: 1/2, step 2368/7134 completed (loss: 0.045394450426101685, acc: 0.9915151596069336)
[2025-02-13 02:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19][root][INFO] - Training Epoch: 1/2, step 2369/7134 completed (loss: 0.045020461082458496, acc: 0.9847058653831482)
[2025-02-13 02:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19][root][INFO] - Training Epoch: 1/2, step 2370/7134 completed (loss: 0.06923262029886246, acc: 0.9814814925193787)
[2025-02-13 02:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:20][root][INFO] - Training Epoch: 1/2, step 2371/7134 completed (loss: 0.035029999911785126, acc: 0.9884726405143738)
[2025-02-13 02:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:20][root][INFO] - Training Epoch: 1/2, step 2372/7134 completed (loss: 0.0426335372030735, acc: 0.9863945841789246)
[2025-02-13 02:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:21][root][INFO] - Training Epoch: 1/2, step 2373/7134 completed (loss: 0.03925188630819321, acc: 0.9925558567047119)
[2025-02-13 02:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:21][root][INFO] - Training Epoch: 1/2, step 2374/7134 completed (loss: 0.04989724978804588, acc: 0.9892904758453369)
[2025-02-13 02:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22][root][INFO] - Training Epoch: 1/2, step 2375/7134 completed (loss: 0.09059037268161774, acc: 0.9818181991577148)
[2025-02-13 02:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22][root][INFO] - Training Epoch: 1/2, step 2376/7134 completed (loss: 0.1321137398481369, acc: 0.9727427363395691)
[2025-02-13 02:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22][root][INFO] - Training Epoch: 1/2, step 2377/7134 completed (loss: 0.10055896639823914, acc: 0.9737156629562378)
[2025-02-13 02:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:23][root][INFO] - Training Epoch: 1/2, step 2378/7134 completed (loss: 0.09943676739931107, acc: 0.9741784334182739)
[2025-02-13 02:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:23][root][INFO] - Training Epoch: 1/2, step 2379/7134 completed (loss: 0.12736739218235016, acc: 0.973793089389801)
[2025-02-13 02:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:24][root][INFO] - Training Epoch: 1/2, step 2380/7134 completed (loss: 0.058355025947093964, acc: 0.9818621277809143)
[2025-02-13 02:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:24][root][INFO] - Training Epoch: 1/2, step 2381/7134 completed (loss: 0.1729007065296173, acc: 0.9593908786773682)
[2025-02-13 02:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25][root][INFO] - Training Epoch: 1/2, step 2382/7134 completed (loss: 0.07430163770914078, acc: 0.9785932898521423)
[2025-02-13 02:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25][root][INFO] - Training Epoch: 1/2, step 2383/7134 completed (loss: 0.11652422696352005, acc: 0.9698340892791748)
[2025-02-13 02:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25][root][INFO] - Training Epoch: 1/2, step 2384/7134 completed (loss: 0.0890730693936348, acc: 0.9803921580314636)
[2025-02-13 02:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:26][root][INFO] - Training Epoch: 1/2, step 2385/7134 completed (loss: 0.06986568868160248, acc: 0.9820442199707031)
[2025-02-13 02:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:26][root][INFO] - Training Epoch: 1/2, step 2386/7134 completed (loss: 0.08163554966449738, acc: 0.9803664684295654)
[2025-02-13 02:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:27][root][INFO] - Training Epoch: 1/2, step 2387/7134 completed (loss: 0.07062245160341263, acc: 0.9837586879730225)
[2025-02-13 02:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:27][root][INFO] - Training Epoch: 1/2, step 2388/7134 completed (loss: 0.09566035121679306, acc: 0.9785459041595459)
[2025-02-13 02:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28][root][INFO] - Training Epoch: 1/2, step 2389/7134 completed (loss: 0.07042401283979416, acc: 0.9827784299850464)
[2025-02-13 02:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28][root][INFO] - Training Epoch: 1/2, step 2390/7134 completed (loss: 0.07879377156496048, acc: 0.9784946441650391)
[2025-02-13 02:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28][root][INFO] - Training Epoch: 1/2, step 2391/7134 completed (loss: 0.10806921124458313, acc: 0.975857675075531)
[2025-02-13 02:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:29][root][INFO] - Training Epoch: 1/2, step 2392/7134 completed (loss: 0.07205332815647125, acc: 0.9853137731552124)
[2025-02-13 02:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:29][root][INFO] - Training Epoch: 1/2, step 2393/7134 completed (loss: 0.12585437297821045, acc: 0.9740596413612366)
[2025-02-13 02:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30][root][INFO] - Training Epoch: 1/2, step 2394/7134 completed (loss: 0.05320160090923309, acc: 0.9842424392700195)
[2025-02-13 02:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30][root][INFO] - Training Epoch: 1/2, step 2395/7134 completed (loss: 0.18121130764484406, acc: 0.9592834115028381)
[2025-02-13 02:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31][root][INFO] - Training Epoch: 1/2, step 2396/7134 completed (loss: 0.11820799112319946, acc: 0.9600551128387451)
[2025-02-13 02:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31][root][INFO] - Training Epoch: 1/2, step 2397/7134 completed (loss: 0.11310034245252609, acc: 0.9723435044288635)
[2025-02-13 02:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31][root][INFO] - Training Epoch: 1/2, step 2398/7134 completed (loss: 0.056136876344680786, acc: 0.9793672561645508)
[2025-02-13 02:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:32][root][INFO] - Training Epoch: 1/2, step 2399/7134 completed (loss: 0.093553826212883, acc: 0.9771101474761963)
[2025-02-13 02:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:32][root][INFO] - Training Epoch: 1/2, step 2400/7134 completed (loss: 0.06691795587539673, acc: 0.9751724004745483)
[2025-02-13 02:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33][root][INFO] - Training Epoch: 1/2, step 2401/7134 completed (loss: 0.1388232558965683, acc: 0.9651898741722107)
[2025-02-13 02:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33][root][INFO] - Training Epoch: 1/2, step 2402/7134 completed (loss: 0.10183737426996231, acc: 0.9654714465141296)
[2025-02-13 02:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34][root][INFO] - Training Epoch: 1/2, step 2403/7134 completed (loss: 0.062114082276821136, acc: 0.978723406791687)
[2025-02-13 02:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34][root][INFO] - Training Epoch: 1/2, step 2404/7134 completed (loss: 0.05796452984213829, acc: 0.9795082211494446)
[2025-02-13 02:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34][root][INFO] - Training Epoch: 1/2, step 2405/7134 completed (loss: 0.09217400848865509, acc: 0.9732441306114197)
[2025-02-13 02:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:35][root][INFO] - Training Epoch: 1/2, step 2406/7134 completed (loss: 0.03341259807348251, acc: 0.9920212626457214)
[2025-02-13 02:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:35][root][INFO] - Training Epoch: 1/2, step 2407/7134 completed (loss: 0.09231190383434296, acc: 0.9726443886756897)
[2025-02-13 02:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36][root][INFO] - Training Epoch: 1/2, step 2408/7134 completed (loss: 0.14971959590911865, acc: 0.9671052694320679)
[2025-02-13 02:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36][root][INFO] - Training Epoch: 1/2, step 2409/7134 completed (loss: 0.04613383859395981, acc: 0.9877049326896667)
[2025-02-13 02:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36][root][INFO] - Training Epoch: 1/2, step 2410/7134 completed (loss: 0.054008107632398605, acc: 0.9856528043746948)
[2025-02-13 02:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:37][root][INFO] - Training Epoch: 1/2, step 2411/7134 completed (loss: 0.06753972917795181, acc: 0.9818887710571289)
[2025-02-13 02:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:37][root][INFO] - Training Epoch: 1/2, step 2412/7134 completed (loss: 0.06888145208358765, acc: 0.9807692170143127)
[2025-02-13 02:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:38][root][INFO] - Training Epoch: 1/2, step 2413/7134 completed (loss: 0.05051133781671524, acc: 0.9822109341621399)
[2025-02-13 02:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:38][root][INFO] - Training Epoch: 1/2, step 2414/7134 completed (loss: 0.05834534019231796, acc: 0.9813432693481445)
[2025-02-13 02:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:39][root][INFO] - Training Epoch: 1/2, step 2415/7134 completed (loss: 0.08914104104042053, acc: 0.973936915397644)
[2025-02-13 02:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:39][root][INFO] - Training Epoch: 1/2, step 2416/7134 completed (loss: 0.06765822321176529, acc: 0.9820193648338318)
[2025-02-13 02:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40][root][INFO] - Training Epoch: 1/2, step 2417/7134 completed (loss: 0.08923213183879852, acc: 0.9799330830574036)
[2025-02-13 02:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40][root][INFO] - Training Epoch: 1/2, step 2418/7134 completed (loss: 0.07159227877855301, acc: 0.9789029359817505)
[2025-02-13 02:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40][root][INFO] - Training Epoch: 1/2, step 2419/7134 completed (loss: 0.04242078214883804, acc: 0.9871244430541992)
[2025-02-13 02:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:41][root][INFO] - Training Epoch: 1/2, step 2420/7134 completed (loss: 0.05154185742139816, acc: 0.9834162592887878)
[2025-02-13 02:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:41][root][INFO] - Training Epoch: 1/2, step 2421/7134 completed (loss: 0.10500461608171463, acc: 0.9738805890083313)
[2025-02-13 02:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42][root][INFO] - Training Epoch: 1/2, step 2422/7134 completed (loss: 0.052939143031835556, acc: 0.986997663974762)
[2025-02-13 02:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42][root][INFO] - Training Epoch: 1/2, step 2423/7134 completed (loss: 0.054484836757183075, acc: 0.9832636117935181)
[2025-02-13 02:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42][root][INFO] - Training Epoch: 1/2, step 2424/7134 completed (loss: 0.05152285471558571, acc: 0.983146071434021)
[2025-02-13 02:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:43][root][INFO] - Training Epoch: 1/2, step 2425/7134 completed (loss: 0.05638112500309944, acc: 0.9800000190734863)
[2025-02-13 02:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:43][root][INFO] - Training Epoch: 1/2, step 2426/7134 completed (loss: 0.04823111370205879, acc: 0.9869203567504883)
[2025-02-13 02:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44][root][INFO] - Training Epoch: 1/2, step 2427/7134 completed (loss: 0.04452880099415779, acc: 0.9802784323692322)
[2025-02-13 02:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44][root][INFO] - Training Epoch: 1/2, step 2428/7134 completed (loss: 0.05553225800395012, acc: 0.9847133755683899)
[2025-02-13 02:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45][root][INFO] - Training Epoch: 1/2, step 2429/7134 completed (loss: 0.07455674558877945, acc: 0.9813084006309509)
[2025-02-13 02:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45][root][INFO] - Training Epoch: 1/2, step 2430/7134 completed (loss: 0.05700891837477684, acc: 0.9850968718528748)
[2025-02-13 02:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45][root][INFO] - Training Epoch: 1/2, step 2431/7134 completed (loss: 0.09738150984048843, acc: 0.9750733375549316)
[2025-02-13 02:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:46][root][INFO] - Training Epoch: 1/2, step 2432/7134 completed (loss: 0.14145441353321075, acc: 0.9677419066429138)
[2025-02-13 02:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:46][root][INFO] - Training Epoch: 1/2, step 2433/7134 completed (loss: 0.08612006157636642, acc: 0.9758522510528564)
[2025-02-13 02:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:47][root][INFO] - Training Epoch: 1/2, step 2434/7134 completed (loss: 0.09511640667915344, acc: 0.9767080545425415)
[2025-02-13 02:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:47][root][INFO] - Training Epoch: 1/2, step 2435/7134 completed (loss: 0.057966198772192, acc: 0.9868247509002686)
[2025-02-13 02:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48][root][INFO] - Training Epoch: 1/2, step 2436/7134 completed (loss: 0.06465546041727066, acc: 0.9778812527656555)
[2025-02-13 02:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48][root][INFO] - Training Epoch: 1/2, step 2437/7134 completed (loss: 0.06686603277921677, acc: 0.9803921580314636)
[2025-02-13 02:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48][root][INFO] - Training Epoch: 1/2, step 2438/7134 completed (loss: 0.08180549740791321, acc: 0.980861246585846)
[2025-02-13 02:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49][root][INFO] - Training Epoch: 1/2, step 2439/7134 completed (loss: 0.07468089461326599, acc: 0.973089337348938)
[2025-02-13 02:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49][root][INFO] - Training Epoch: 1/2, step 2440/7134 completed (loss: 0.08042261004447937, acc: 0.9798206090927124)
[2025-02-13 02:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:50][root][INFO] - Training Epoch: 1/2, step 2441/7134 completed (loss: 0.15667372941970825, acc: 0.9577735066413879)
[2025-02-13 02:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:50][root][INFO] - Training Epoch: 1/2, step 2442/7134 completed (loss: 0.09938371926546097, acc: 0.9650349617004395)
[2025-02-13 02:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51][root][INFO] - Training Epoch: 1/2, step 2443/7134 completed (loss: 0.08519962430000305, acc: 0.9847009778022766)
[2025-02-13 02:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51][root][INFO] - Training Epoch: 1/2, step 2444/7134 completed (loss: 0.08285796642303467, acc: 0.9696969985961914)
[2025-02-13 02:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51][root][INFO] - Training Epoch: 1/2, step 2445/7134 completed (loss: 0.10297758877277374, acc: 0.9733502268791199)
[2025-02-13 02:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:52][root][INFO] - Training Epoch: 1/2, step 2446/7134 completed (loss: 0.12273933738470078, acc: 0.9666666388511658)
[2025-02-13 02:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:52][root][INFO] - Training Epoch: 1/2, step 2447/7134 completed (loss: 0.0678771585226059, acc: 0.9800221920013428)
[2025-02-13 02:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:53][root][INFO] - Training Epoch: 1/2, step 2448/7134 completed (loss: 0.07664892822504044, acc: 0.9745989441871643)
[2025-02-13 02:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:53][root][INFO] - Training Epoch: 1/2, step 2449/7134 completed (loss: 0.08781083673238754, acc: 0.9774535894393921)
[2025-02-13 02:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54][root][INFO] - Training Epoch: 1/2, step 2450/7134 completed (loss: 0.09278187155723572, acc: 0.9707750678062439)
[2025-02-13 02:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54][root][INFO] - Training Epoch: 1/2, step 2451/7134 completed (loss: 0.04370945692062378, acc: 0.9885350465774536)
[2025-02-13 02:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54][root][INFO] - Training Epoch: 1/2, step 2452/7134 completed (loss: 0.06385237723588943, acc: 0.9804941415786743)
[2025-02-13 02:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:55][root][INFO] - Training Epoch: 1/2, step 2453/7134 completed (loss: 0.06329629570245743, acc: 0.9819355010986328)
[2025-02-13 02:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:55][root][INFO] - Training Epoch: 1/2, step 2454/7134 completed (loss: 0.0783040001988411, acc: 0.9698996543884277)
[2025-02-13 02:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:56][root][INFO] - Training Epoch: 1/2, step 2455/7134 completed (loss: 0.06300844997167587, acc: 0.9815863966941833)
[2025-02-13 02:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:56][root][INFO] - Training Epoch: 1/2, step 2456/7134 completed (loss: 0.07410205155611038, acc: 0.9753566980361938)
[2025-02-13 02:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57][root][INFO] - Training Epoch: 1/2, step 2457/7134 completed (loss: 0.0818769708275795, acc: 0.9767054915428162)
[2025-02-13 02:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57][root][INFO] - Training Epoch: 1/2, step 2458/7134 completed (loss: 0.1194070428609848, acc: 0.9732704162597656)
[2025-02-13 02:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57][root][INFO] - Training Epoch: 1/2, step 2459/7134 completed (loss: 0.03580963984131813, acc: 0.9896449446678162)
[2025-02-13 02:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:58][root][INFO] - Training Epoch: 1/2, step 2460/7134 completed (loss: 0.04335285723209381, acc: 0.9903181195259094)
[2025-02-13 02:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:58][root][INFO] - Training Epoch: 1/2, step 2461/7134 completed (loss: 0.04493299871683121, acc: 0.9869565367698669)
[2025-02-13 02:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:59][root][INFO] - Training Epoch: 1/2, step 2462/7134 completed (loss: 0.07198057323694229, acc: 0.9793103337287903)
[2025-02-13 02:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:59][root][INFO] - Training Epoch: 1/2, step 2463/7134 completed (loss: 0.040455881506204605, acc: 0.9863842725753784)
[2025-02-13 02:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00][root][INFO] - Training Epoch: 1/2, step 2464/7134 completed (loss: 0.08544377237558365, acc: 0.9794007539749146)
[2025-02-13 02:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00][root][INFO] - Training Epoch: 1/2, step 2465/7134 completed (loss: 0.06141289696097374, acc: 0.9827814698219299)
[2025-02-13 02:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00][root][INFO] - Training Epoch: 1/2, step 2466/7134 completed (loss: 0.04713650792837143, acc: 0.9814569354057312)
[2025-02-13 02:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:01][root][INFO] - Training Epoch: 1/2, step 2467/7134 completed (loss: 0.05548122152686119, acc: 0.9884318709373474)
[2025-02-13 02:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:01][root][INFO] - Training Epoch: 1/2, step 2468/7134 completed (loss: 0.03865901008248329, acc: 0.988811194896698)
[2025-02-13 02:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:02][root][INFO] - Training Epoch: 1/2, step 2469/7134 completed (loss: 0.03346603363752365, acc: 0.9890859723091125)
[2025-02-13 02:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:02][root][INFO] - Training Epoch: 1/2, step 2470/7134 completed (loss: 0.07133618742227554, acc: 0.9831649661064148)
[2025-02-13 02:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03][root][INFO] - Training Epoch: 1/2, step 2471/7134 completed (loss: 0.041223883628845215, acc: 0.9849498271942139)
[2025-02-13 02:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03][root][INFO] - Training Epoch: 1/2, step 2472/7134 completed (loss: 0.02097008004784584, acc: 0.9939393997192383)
[2025-02-13 02:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03][root][INFO] - Training Epoch: 1/2, step 2473/7134 completed (loss: 0.05722624063491821, acc: 0.9815573692321777)
[2025-02-13 02:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:04][root][INFO] - Training Epoch: 1/2, step 2474/7134 completed (loss: 0.028237128630280495, acc: 0.9929577708244324)
[2025-02-13 02:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:04][root][INFO] - Training Epoch: 1/2, step 2475/7134 completed (loss: 0.03212297335267067, acc: 0.9879931211471558)
[2025-02-13 02:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05][root][INFO] - Training Epoch: 1/2, step 2476/7134 completed (loss: 0.08133348077535629, acc: 0.984679639339447)
[2025-02-13 02:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05][root][INFO] - Training Epoch: 1/2, step 2477/7134 completed (loss: 0.05257907882332802, acc: 0.988950252532959)
[2025-02-13 02:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05][root][INFO] - Training Epoch: 1/2, step 2478/7134 completed (loss: 0.023554516956210136, acc: 0.9906542301177979)
[2025-02-13 02:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:06][root][INFO] - Training Epoch: 1/2, step 2479/7134 completed (loss: 0.10954387485980988, acc: 0.9777424335479736)
[2025-02-13 02:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:06][root][INFO] - Training Epoch: 1/2, step 2480/7134 completed (loss: 0.020555764436721802, acc: 0.995555579662323)
[2025-02-13 02:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07][root][INFO] - Training Epoch: 1/2, step 2481/7134 completed (loss: 0.05678209289908409, acc: 0.9776119589805603)
[2025-02-13 02:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07][root][INFO] - Training Epoch: 1/2, step 2482/7134 completed (loss: 0.0713474228978157, acc: 0.9775280952453613)
[2025-02-13 02:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07][root][INFO] - Training Epoch: 1/2, step 2483/7134 completed (loss: 0.023899385705590248, acc: 0.9902912378311157)
[2025-02-13 02:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08][root][INFO] - Training Epoch: 1/2, step 2484/7134 completed (loss: 0.040995024144649506, acc: 0.987500011920929)
[2025-02-13 02:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08][root][INFO] - Training Epoch: 1/2, step 2485/7134 completed (loss: 0.04562827944755554, acc: 0.9952380657196045)
[2025-02-13 02:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09][root][INFO] - Training Epoch: 1/2, step 2486/7134 completed (loss: 0.04281822219491005, acc: 0.9913793206214905)
[2025-02-13 02:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09][root][INFO] - Training Epoch: 1/2, step 2487/7134 completed (loss: 0.10749964416027069, acc: 0.9795275330543518)
[2025-02-13 02:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09][root][INFO] - Training Epoch: 1/2, step 2488/7134 completed (loss: 0.049696799367666245, acc: 0.9861878156661987)
[2025-02-13 02:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:10][root][INFO] - Training Epoch: 1/2, step 2489/7134 completed (loss: 0.04374902695417404, acc: 0.9902507066726685)
[2025-02-13 02:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:10][root][INFO] - Training Epoch: 1/2, step 2490/7134 completed (loss: 0.031845007091760635, acc: 0.9885877370834351)
[2025-02-13 02:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:11][root][INFO] - Training Epoch: 1/2, step 2491/7134 completed (loss: 0.16340158879756927, acc: 0.9669312238693237)
[2025-02-13 02:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:11][root][INFO] - Training Epoch: 1/2, step 2492/7134 completed (loss: 0.06434150040149689, acc: 0.981389582157135)
[2025-02-13 02:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12][root][INFO] - Training Epoch: 1/2, step 2493/7134 completed (loss: 0.0813862681388855, acc: 0.9694835543632507)
[2025-02-13 02:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12][root][INFO] - Training Epoch: 1/2, step 2494/7134 completed (loss: 0.06538883596658707, acc: 0.9849812388420105)
[2025-02-13 02:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12][root][INFO] - Training Epoch: 1/2, step 2495/7134 completed (loss: 0.0639699324965477, acc: 0.9782903790473938)
[2025-02-13 02:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:13][root][INFO] - Training Epoch: 1/2, step 2496/7134 completed (loss: 0.03721249848604202, acc: 0.9873595237731934)
[2025-02-13 02:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:13][root][INFO] - Training Epoch: 1/2, step 2497/7134 completed (loss: 0.055698417127132416, acc: 0.9829683899879456)
[2025-02-13 02:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:14][root][INFO] - Training Epoch: 1/2, step 2498/7134 completed (loss: 0.05666924640536308, acc: 0.9874857664108276)
[2025-02-13 02:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:14][root][INFO] - Training Epoch: 1/2, step 2499/7134 completed (loss: 0.05058615654706955, acc: 0.9842995405197144)
[2025-02-13 02:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15][root][INFO] - Training Epoch: 1/2, step 2500/7134 completed (loss: 0.048082947731018066, acc: 0.9858430027961731)
[2025-02-13 02:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15][root][INFO] - Training Epoch: 1/2, step 2501/7134 completed (loss: 0.06389295309782028, acc: 0.9805492162704468)
[2025-02-13 02:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15][root][INFO] - Training Epoch: 1/2, step 2502/7134 completed (loss: 0.0258872602134943, acc: 0.9929328560829163)
[2025-02-13 02:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:16][root][INFO] - Training Epoch: 1/2, step 2503/7134 completed (loss: 0.05323193222284317, acc: 0.9829059839248657)
[2025-02-13 02:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:16][root][INFO] - Training Epoch: 1/2, step 2504/7134 completed (loss: 0.0361737385392189, acc: 0.9906666874885559)
[2025-02-13 02:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17][root][INFO] - Training Epoch: 1/2, step 2505/7134 completed (loss: 0.03773903474211693, acc: 0.9897611141204834)
[2025-02-13 02:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17][root][INFO] - Training Epoch: 1/2, step 2506/7134 completed (loss: 0.04683239758014679, acc: 0.9839743375778198)
[2025-02-13 02:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18][root][INFO] - Training Epoch: 1/2, step 2507/7134 completed (loss: 0.020073652267456055, acc: 0.99303138256073)
[2025-02-13 02:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18][root][INFO] - Training Epoch: 1/2, step 2508/7134 completed (loss: 0.04723937809467316, acc: 0.9884792566299438)
[2025-02-13 02:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18][root][INFO] - Training Epoch: 1/2, step 2509/7134 completed (loss: 0.03384565934538841, acc: 0.990208089351654)
[2025-02-13 02:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:19][root][INFO] - Training Epoch: 1/2, step 2510/7134 completed (loss: 0.03525438532233238, acc: 0.9893778562545776)
[2025-02-13 02:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:19][root][INFO] - Training Epoch: 1/2, step 2511/7134 completed (loss: 0.06630893796682358, acc: 0.9865471124649048)
[2025-02-13 02:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:20][root][INFO] - Training Epoch: 1/2, step 2512/7134 completed (loss: 0.09677824378013611, acc: 0.9716598987579346)
[2025-02-13 02:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:20][root][INFO] - Training Epoch: 1/2, step 2513/7134 completed (loss: 0.04137960448861122, acc: 0.9905213117599487)
[2025-02-13 02:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21][root][INFO] - Training Epoch: 1/2, step 2514/7134 completed (loss: 0.027813220396637917, acc: 0.9906651377677917)
[2025-02-13 02:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21][root][INFO] - Training Epoch: 1/2, step 2515/7134 completed (loss: 0.023593872785568237, acc: 0.9921976327896118)
[2025-02-13 02:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21][root][INFO] - Training Epoch: 1/2, step 2516/7134 completed (loss: 0.025748591870069504, acc: 0.9931818246841431)
[2025-02-13 02:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:22][root][INFO] - Training Epoch: 1/2, step 2517/7134 completed (loss: 0.038904402405023575, acc: 0.9873708486557007)
[2025-02-13 02:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:22][root][INFO] - Training Epoch: 1/2, step 2518/7134 completed (loss: 0.041012488305568695, acc: 0.9871244430541992)
[2025-02-13 02:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:23][root][INFO] - Training Epoch: 1/2, step 2519/7134 completed (loss: 0.02592659741640091, acc: 0.9917012453079224)
[2025-02-13 02:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:23][root][INFO] - Training Epoch: 1/2, step 2520/7134 completed (loss: 0.028496000915765762, acc: 0.9857549667358398)
[2025-02-13 02:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24][root][INFO] - Training Epoch: 1/2, step 2521/7134 completed (loss: 0.09412366151809692, acc: 0.9819819927215576)
[2025-02-13 02:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24][root][INFO] - Training Epoch: 1/2, step 2522/7134 completed (loss: 0.046107180416584015, acc: 0.9856733679771423)
[2025-02-13 02:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24][root][INFO] - Training Epoch: 1/2, step 2523/7134 completed (loss: 0.06514309346675873, acc: 0.985602080821991)
[2025-02-13 02:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25][root][INFO] - Training Epoch: 1/2, step 2524/7134 completed (loss: 0.020376306027173996, acc: 0.9958563446998596)
[2025-02-13 02:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25][root][INFO] - Training Epoch: 1/2, step 2525/7134 completed (loss: 0.04730549082159996, acc: 0.9849749803543091)
[2025-02-13 02:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26][root][INFO] - Training Epoch: 1/2, step 2526/7134 completed (loss: 0.012090528383851051, acc: 0.9976470470428467)
[2025-02-13 02:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26][root][INFO] - Training Epoch: 1/2, step 2527/7134 completed (loss: 0.0482071153819561, acc: 0.9890244007110596)
[2025-02-13 02:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26][root][INFO] - Training Epoch: 1/2, step 2528/7134 completed (loss: 0.020366767421364784, acc: 0.9931600689888)
[2025-02-13 02:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:27][root][INFO] - Training Epoch: 1/2, step 2529/7134 completed (loss: 0.02682371251285076, acc: 0.9906914830207825)
[2025-02-13 02:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:27][root][INFO] - Training Epoch: 1/2, step 2530/7134 completed (loss: 0.03681672737002373, acc: 0.9922077655792236)
[2025-02-13 02:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:28][root][INFO] - Training Epoch: 1/2, step 2531/7134 completed (loss: 0.03413848951458931, acc: 0.9875690340995789)
[2025-02-13 02:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:28][root][INFO] - Training Epoch: 1/2, step 2532/7134 completed (loss: 0.03257676959037781, acc: 0.9861325025558472)
[2025-02-13 02:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29][root][INFO] - Training Epoch: 1/2, step 2533/7134 completed (loss: 0.04772641882300377, acc: 0.9879336357116699)
[2025-02-13 02:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29][root][INFO] - Training Epoch: 1/2, step 2534/7134 completed (loss: 0.04757208377122879, acc: 0.9910827875137329)
[2025-02-13 02:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29][root][INFO] - Training Epoch: 1/2, step 2535/7134 completed (loss: 0.05370974913239479, acc: 0.9826589822769165)
[2025-02-13 02:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:30][root][INFO] - Training Epoch: 1/2, step 2536/7134 completed (loss: 0.049143966287374496, acc: 0.984009861946106)
[2025-02-13 02:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:30][root][INFO] - Training Epoch: 1/2, step 2537/7134 completed (loss: 0.08797068893909454, acc: 0.9785932898521423)
[2025-02-13 02:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31][root][INFO] - Training Epoch: 1/2, step 2538/7134 completed (loss: 0.10093770921230316, acc: 0.9745042324066162)
[2025-02-13 02:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31][root][INFO] - Training Epoch: 1/2, step 2539/7134 completed (loss: 0.029410213232040405, acc: 0.9928571581840515)
[2025-02-13 02:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31][root][INFO] - Training Epoch: 1/2, step 2540/7134 completed (loss: 0.10466749221086502, acc: 0.9785932898521423)
[2025-02-13 02:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:32][root][INFO] - Training Epoch: 1/2, step 2541/7134 completed (loss: 0.03473243862390518, acc: 0.9884105920791626)
[2025-02-13 02:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:32][root][INFO] - Training Epoch: 1/2, step 2542/7134 completed (loss: 0.10739221423864365, acc: 0.9792899489402771)
[2025-02-13 02:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:33][root][INFO] - Training Epoch: 1/2, step 2543/7134 completed (loss: 0.11380577832460403, acc: 0.9754098653793335)
[2025-02-13 02:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:33][root][INFO] - Training Epoch: 1/2, step 2544/7134 completed (loss: 0.05007214844226837, acc: 0.9909793734550476)
[2025-02-13 02:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34][root][INFO] - Training Epoch: 1/2, step 2545/7134 completed (loss: 0.07632250338792801, acc: 0.9808027744293213)
[2025-02-13 02:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34][root][INFO] - Training Epoch: 1/2, step 2546/7134 completed (loss: 0.18011249601840973, acc: 0.9555822610855103)
[2025-02-13 02:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34][root][INFO] - Training Epoch: 1/2, step 2547/7134 completed (loss: 0.08375207334756851, acc: 0.9710144996643066)
[2025-02-13 02:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:35][root][INFO] - Training Epoch: 1/2, step 2548/7134 completed (loss: 0.061952296644449234, acc: 0.9862204790115356)
[2025-02-13 02:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:35][root][INFO] - Training Epoch: 1/2, step 2549/7134 completed (loss: 0.07266224175691605, acc: 0.9815837740898132)
[2025-02-13 02:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36][root][INFO] - Training Epoch: 1/2, step 2550/7134 completed (loss: 0.043681565672159195, acc: 0.9869494438171387)
[2025-02-13 02:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36][root][INFO] - Training Epoch: 1/2, step 2551/7134 completed (loss: 0.06906382739543915, acc: 0.9834983348846436)
[2025-02-13 02:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36][root][INFO] - Training Epoch: 1/2, step 2552/7134 completed (loss: 0.03993799164891243, acc: 0.9873816967010498)
[2025-02-13 02:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37][root][INFO] - Training Epoch: 1/2, step 2553/7134 completed (loss: 0.05606386065483093, acc: 0.981873095035553)
[2025-02-13 02:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37][root][INFO] - Training Epoch: 1/2, step 2554/7134 completed (loss: 0.10208238661289215, acc: 0.9715242981910706)
[2025-02-13 02:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37][root][INFO] - Training Epoch: 1/2, step 2555/7134 completed (loss: 0.07886064797639847, acc: 0.9814049601554871)
[2025-02-13 02:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:38][root][INFO] - Training Epoch: 1/2, step 2556/7134 completed (loss: 0.08198510110378265, acc: 0.9832285046577454)
[2025-02-13 02:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:38][root][INFO] - Training Epoch: 1/2, step 2557/7134 completed (loss: 0.0770169198513031, acc: 0.9786885380744934)
[2025-02-13 02:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:39][root][INFO] - Training Epoch: 1/2, step 2558/7134 completed (loss: 0.024584809318184853, acc: 0.9935275316238403)
[2025-02-13 02:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:39][root][INFO] - Training Epoch: 1/2, step 2559/7134 completed (loss: 0.021810302510857582, acc: 0.9945873022079468)
[2025-02-13 02:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40][root][INFO] - Training Epoch: 1/2, step 2560/7134 completed (loss: 0.07924826443195343, acc: 0.9788618087768555)
[2025-02-13 02:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40][root][INFO] - Training Epoch: 1/2, step 2561/7134 completed (loss: 0.08437597006559372, acc: 0.9782134890556335)
[2025-02-13 02:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40][root][INFO] - Training Epoch: 1/2, step 2562/7134 completed (loss: 0.08357375860214233, acc: 0.9794871807098389)
[2025-02-13 02:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41][root][INFO] - Training Epoch: 1/2, step 2563/7134 completed (loss: 0.04831251502037048, acc: 0.9912023544311523)
[2025-02-13 02:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41][root][INFO] - Training Epoch: 1/2, step 2564/7134 completed (loss: 0.1423823982477188, acc: 0.9647650718688965)
[2025-02-13 02:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42][root][INFO] - Training Epoch: 1/2, step 2565/7134 completed (loss: 0.03849862888455391, acc: 0.9894259572029114)
[2025-02-13 02:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42][root][INFO] - Training Epoch: 1/2, step 2566/7134 completed (loss: 0.07106431573629379, acc: 0.9817415475845337)
[2025-02-13 02:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42][root][INFO] - Training Epoch: 1/2, step 2567/7134 completed (loss: 0.060143742710351944, acc: 0.9805996417999268)
[2025-02-13 02:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:43][root][INFO] - Training Epoch: 1/2, step 2568/7134 completed (loss: 0.06561800837516785, acc: 0.9812138676643372)
[2025-02-13 02:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:43][root][INFO] - Training Epoch: 1/2, step 2569/7134 completed (loss: 0.08231955021619797, acc: 0.9774096608161926)
[2025-02-13 02:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44][root][INFO] - Training Epoch: 1/2, step 2570/7134 completed (loss: 0.06559699773788452, acc: 0.9865067601203918)
[2025-02-13 02:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44][root][INFO] - Training Epoch: 1/2, step 2571/7134 completed (loss: 0.0315830260515213, acc: 0.9912126660346985)
[2025-02-13 02:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44][root][INFO] - Training Epoch: 1/2, step 2572/7134 completed (loss: 0.06232401356101036, acc: 0.9826498627662659)
[2025-02-13 02:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:45][root][INFO] - Training Epoch: 1/2, step 2573/7134 completed (loss: 0.10094748437404633, acc: 0.9770491719245911)
[2025-02-13 02:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:45][root][INFO] - Training Epoch: 1/2, step 2574/7134 completed (loss: 0.06318672746419907, acc: 0.9847161769866943)
[2025-02-13 02:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46][root][INFO] - Training Epoch: 1/2, step 2575/7134 completed (loss: 0.04947628453373909, acc: 0.9814126491546631)
[2025-02-13 02:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46][root][INFO] - Training Epoch: 1/2, step 2576/7134 completed (loss: 0.06342046707868576, acc: 0.982758641242981)
[2025-02-13 02:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46][root][INFO] - Training Epoch: 1/2, step 2577/7134 completed (loss: 0.09884529560804367, acc: 0.9777777791023254)
[2025-02-13 02:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:47][root][INFO] - Training Epoch: 1/2, step 2578/7134 completed (loss: 0.08346101641654968, acc: 0.9770290851593018)
[2025-02-13 02:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:47][root][INFO] - Training Epoch: 1/2, step 2579/7134 completed (loss: 0.11750378459692001, acc: 0.9715189933776855)
[2025-02-13 02:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48][root][INFO] - Training Epoch: 1/2, step 2580/7134 completed (loss: 0.08243626356124878, acc: 0.9738903641700745)
[2025-02-13 02:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48][root][INFO] - Training Epoch: 1/2, step 2581/7134 completed (loss: 0.0631699487566948, acc: 0.983849287033081)
[2025-02-13 02:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49][root][INFO] - Training Epoch: 1/2, step 2582/7134 completed (loss: 0.055377230048179626, acc: 0.9810298085212708)
[2025-02-13 02:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49][root][INFO] - Training Epoch: 1/2, step 2583/7134 completed (loss: 0.06727415323257446, acc: 0.9815078377723694)
[2025-02-13 02:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49][root][INFO] - Training Epoch: 1/2, step 2584/7134 completed (loss: 0.10320910811424255, acc: 0.9738219976425171)
[2025-02-13 02:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:50][root][INFO] - Training Epoch: 1/2, step 2585/7134 completed (loss: 0.07489141821861267, acc: 0.9827337861061096)
[2025-02-13 02:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:50][root][INFO] - Training Epoch: 1/2, step 2586/7134 completed (loss: 0.10372777283191681, acc: 0.9753521084785461)
[2025-02-13 02:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51][root][INFO] - Training Epoch: 1/2, step 2587/7134 completed (loss: 0.08002051711082458, acc: 0.9766584634780884)
[2025-02-13 02:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51][root][INFO] - Training Epoch: 1/2, step 2588/7134 completed (loss: 0.03377654403448105, acc: 0.9849931597709656)
[2025-02-13 02:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51][root][INFO] - Training Epoch: 1/2, step 2589/7134 completed (loss: 0.07467380166053772, acc: 0.982332170009613)
[2025-02-13 02:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:52][root][INFO] - Training Epoch: 1/2, step 2590/7134 completed (loss: 0.05784669145941734, acc: 0.9803921580314636)
[2025-02-13 02:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:52][root][INFO] - Training Epoch: 1/2, step 2591/7134 completed (loss: 0.09803752601146698, acc: 0.970588207244873)
[2025-02-13 02:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:53][root][INFO] - Training Epoch: 1/2, step 2592/7134 completed (loss: 0.08099224418401718, acc: 0.9729397296905518)
[2025-02-13 02:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:53][root][INFO] - Training Epoch: 1/2, step 2593/7134 completed (loss: 0.06459911912679672, acc: 0.9842932224273682)
[2025-02-13 02:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54][root][INFO] - Training Epoch: 1/2, step 2594/7134 completed (loss: 0.05586869269609451, acc: 0.9877675771713257)
[2025-02-13 02:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54][root][INFO] - Training Epoch: 1/2, step 2595/7134 completed (loss: 0.04315607622265816, acc: 0.9865591526031494)
[2025-02-13 02:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54][root][INFO] - Training Epoch: 1/2, step 2596/7134 completed (loss: 0.07246056944131851, acc: 0.9849624037742615)
[2025-02-13 02:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:55][root][INFO] - Training Epoch: 1/2, step 2597/7134 completed (loss: 0.06651616841554642, acc: 0.9810810685157776)
[2025-02-13 02:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:55][root][INFO] - Training Epoch: 1/2, step 2598/7134 completed (loss: 0.10002060979604721, acc: 0.9783281683921814)
[2025-02-13 02:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:56][root][INFO] - Training Epoch: 1/2, step 2599/7134 completed (loss: 0.017001021653413773, acc: 0.9947229623794556)
[2025-02-13 02:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:56][root][INFO] - Training Epoch: 1/2, step 2600/7134 completed (loss: 0.059758834540843964, acc: 0.9820742607116699)
[2025-02-13 02:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57][root][INFO] - Training Epoch: 1/2, step 2601/7134 completed (loss: 0.08008553087711334, acc: 0.9797022938728333)
[2025-02-13 02:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57][root][INFO] - Training Epoch: 1/2, step 2602/7134 completed (loss: 0.0726800486445427, acc: 0.981840193271637)
[2025-02-13 02:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57][root][INFO] - Training Epoch: 1/2, step 2603/7134 completed (loss: 0.08640225976705551, acc: 0.9767441749572754)
[2025-02-13 02:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:58][root][INFO] - Training Epoch: 1/2, step 2604/7134 completed (loss: 0.06851337105035782, acc: 0.9780701994895935)
[2025-02-13 02:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:58][root][INFO] - Training Epoch: 1/2, step 2605/7134 completed (loss: 0.1641482710838318, acc: 0.9476534128189087)
[2025-02-13 02:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:59][root][INFO] - Training Epoch: 1/2, step 2606/7134 completed (loss: 0.08054936677217484, acc: 0.9812949895858765)
[2025-02-13 02:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:59][root][INFO] - Training Epoch: 1/2, step 2607/7134 completed (loss: 0.09243277460336685, acc: 0.9694117903709412)
[2025-02-13 02:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00][root][INFO] - Training Epoch: 1/2, step 2608/7134 completed (loss: 0.10393340140581131, acc: 0.9653893709182739)
[2025-02-13 02:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00][root][INFO] - Training Epoch: 1/2, step 2609/7134 completed (loss: 0.10187403857707977, acc: 0.9721448421478271)
[2025-02-13 02:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00][root][INFO] - Training Epoch: 1/2, step 2610/7134 completed (loss: 0.11282828450202942, acc: 0.9725190997123718)
[2025-02-13 02:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:01][root][INFO] - Training Epoch: 1/2, step 2611/7134 completed (loss: 0.11063863337039948, acc: 0.9690011739730835)
[2025-02-13 02:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:01][root][INFO] - Training Epoch: 1/2, step 2612/7134 completed (loss: 0.10130510479211807, acc: 0.9681093096733093)
[2025-02-13 02:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:02][root][INFO] - Training Epoch: 1/2, step 2613/7134 completed (loss: 0.05059395357966423, acc: 0.9848101139068604)
[2025-02-13 02:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:02][root][INFO] - Training Epoch: 1/2, step 2614/7134 completed (loss: 0.12493017315864563, acc: 0.9692482948303223)
[2025-02-13 02:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:03][root][INFO] - Training Epoch: 1/2, step 2615/7134 completed (loss: 0.12981757521629333, acc: 0.9640102982521057)
[2025-02-13 02:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:03][root][INFO] - Training Epoch: 1/2, step 2616/7134 completed (loss: 0.12304608523845673, acc: 0.9773662686347961)
[2025-02-13 02:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04][root][INFO] - Training Epoch: 1/2, step 2617/7134 completed (loss: 0.12588059902191162, acc: 0.9649122953414917)
[2025-02-13 02:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04][root][INFO] - Training Epoch: 1/2, step 2618/7134 completed (loss: 0.08896172791719437, acc: 0.9733059406280518)
[2025-02-13 02:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04][root][INFO] - Training Epoch: 1/2, step 2619/7134 completed (loss: 0.0924542248249054, acc: 0.9731543660163879)
[2025-02-13 02:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:05][root][INFO] - Training Epoch: 1/2, step 2620/7134 completed (loss: 0.07740335911512375, acc: 0.9789343476295471)
[2025-02-13 02:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:05][root][INFO] - Training Epoch: 1/2, step 2621/7134 completed (loss: 0.0659494623541832, acc: 0.9810426831245422)
[2025-02-13 02:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:06][root][INFO] - Training Epoch: 1/2, step 2622/7134 completed (loss: 0.08353810757398605, acc: 0.9738134145736694)
[2025-02-13 02:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:06][root][INFO] - Training Epoch: 1/2, step 2623/7134 completed (loss: 0.10135931521654129, acc: 0.9701149463653564)
[2025-02-13 02:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07][root][INFO] - Training Epoch: 1/2, step 2624/7134 completed (loss: 0.0715472549200058, acc: 0.9786276817321777)
[2025-02-13 02:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07][root][INFO] - Training Epoch: 1/2, step 2625/7134 completed (loss: 0.07214982062578201, acc: 0.9736111164093018)
[2025-02-13 02:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07][root][INFO] - Training Epoch: 1/2, step 2626/7134 completed (loss: 0.06685127317905426, acc: 0.9764851331710815)
[2025-02-13 02:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08][root][INFO] - Training Epoch: 1/2, step 2627/7134 completed (loss: 0.05452127382159233, acc: 0.9851149916648865)
[2025-02-13 02:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08][root][INFO] - Training Epoch: 1/2, step 2628/7134 completed (loss: 0.07672674208879471, acc: 0.9797979593276978)
[2025-02-13 02:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:09][root][INFO] - Training Epoch: 1/2, step 2629/7134 completed (loss: 0.047371651977300644, acc: 0.982300877571106)
[2025-02-13 02:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:09][root][INFO] - Training Epoch: 1/2, step 2630/7134 completed (loss: 0.06643980741500854, acc: 0.9728434681892395)
[2025-02-13 02:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10][root][INFO] - Training Epoch: 1/2, step 2631/7134 completed (loss: 0.044815681874752045, acc: 0.9824561476707458)
[2025-02-13 02:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10][root][INFO] - Training Epoch: 1/2, step 2632/7134 completed (loss: 0.056327659636735916, acc: 0.9823182821273804)
[2025-02-13 02:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10][root][INFO] - Training Epoch: 1/2, step 2633/7134 completed (loss: 0.09843097627162933, acc: 0.980169951915741)
[2025-02-13 02:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:11][root][INFO] - Training Epoch: 1/2, step 2634/7134 completed (loss: 0.0318559966981411, acc: 0.9880059957504272)
[2025-02-13 02:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:11][root][INFO] - Training Epoch: 1/2, step 2635/7134 completed (loss: 0.13690558075904846, acc: 0.9708333611488342)
[2025-02-13 02:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:12][root][INFO] - Training Epoch: 1/2, step 2636/7134 completed (loss: 0.0880301222205162, acc: 0.9800853729248047)
[2025-02-13 02:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:12][root][INFO] - Training Epoch: 1/2, step 2637/7134 completed (loss: 0.24940545856952667, acc: 0.9371816515922546)
[2025-02-13 02:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13][root][INFO] - Training Epoch: 1/2, step 2638/7134 completed (loss: 0.12345060706138611, acc: 0.9674054980278015)
[2025-02-13 02:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13][root][INFO] - Training Epoch: 1/2, step 2639/7134 completed (loss: 0.15778610110282898, acc: 0.9522821307182312)
[2025-02-13 02:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13][root][INFO] - Training Epoch: 1/2, step 2640/7134 completed (loss: 0.1340397298336029, acc: 0.9687890410423279)
[2025-02-13 02:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14][root][INFO] - Training Epoch: 1/2, step 2641/7134 completed (loss: 0.10830003023147583, acc: 0.9749631881713867)
[2025-02-13 02:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14][root][INFO] - Training Epoch: 1/2, step 2642/7134 completed (loss: 0.09743945300579071, acc: 0.9697508811950684)
[2025-02-13 02:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:15][root][INFO] - Training Epoch: 1/2, step 2643/7134 completed (loss: 0.1046706959605217, acc: 0.9617940187454224)
[2025-02-13 02:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:15][root][INFO] - Training Epoch: 1/2, step 2644/7134 completed (loss: 0.07642603665590286, acc: 0.9793689250946045)
[2025-02-13 02:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16][root][INFO] - Training Epoch: 1/2, step 2645/7134 completed (loss: 0.10438980162143707, acc: 0.9737609624862671)
[2025-02-13 02:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16][root][INFO] - Training Epoch: 1/2, step 2646/7134 completed (loss: 0.1894746720790863, acc: 0.9522293210029602)
[2025-02-13 02:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16][root][INFO] - Training Epoch: 1/2, step 2647/7134 completed (loss: 0.0746116116642952, acc: 0.981794536113739)
[2025-02-13 02:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:17][root][INFO] - Training Epoch: 1/2, step 2648/7134 completed (loss: 0.05276777222752571, acc: 0.9845161437988281)
[2025-02-13 02:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:17][root][INFO] - Training Epoch: 1/2, step 2649/7134 completed (loss: 0.15567423403263092, acc: 0.9657227993011475)
[2025-02-13 02:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:18][root][INFO] - Training Epoch: 1/2, step 2650/7134 completed (loss: 0.13953527808189392, acc: 0.9531013369560242)
[2025-02-13 02:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:18][root][INFO] - Training Epoch: 1/2, step 2651/7134 completed (loss: 0.06768620759248734, acc: 0.9797377586364746)
[2025-02-13 02:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19][root][INFO] - Training Epoch: 1/2, step 2652/7134 completed (loss: 0.06842373311519623, acc: 0.9836065769195557)
[2025-02-13 02:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19][root][INFO] - Training Epoch: 1/2, step 2653/7134 completed (loss: 0.09795782715082169, acc: 0.9654605388641357)
[2025-02-13 02:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19][root][INFO] - Training Epoch: 1/2, step 2654/7134 completed (loss: 0.06101159378886223, acc: 0.9901639223098755)
[2025-02-13 02:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:20][root][INFO] - Training Epoch: 1/2, step 2655/7134 completed (loss: 0.07077501714229584, acc: 0.9848713874816895)
[2025-02-13 02:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:20][root][INFO] - Training Epoch: 1/2, step 2656/7134 completed (loss: 0.07973799109458923, acc: 0.9819193482398987)
[2025-02-13 02:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21][root][INFO] - Training Epoch: 1/2, step 2657/7134 completed (loss: 0.07877293229103088, acc: 0.977419376373291)
[2025-02-13 02:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21][root][INFO] - Training Epoch: 1/2, step 2658/7134 completed (loss: 0.12549515068531036, acc: 0.9746328592300415)
[2025-02-13 02:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21][root][INFO] - Training Epoch: 1/2, step 2659/7134 completed (loss: 0.04954536631703377, acc: 0.9865900278091431)
[2025-02-13 02:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:22][root][INFO] - Training Epoch: 1/2, step 2660/7134 completed (loss: 0.059944409877061844, acc: 0.9818548560142517)
[2025-02-13 02:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:22][root][INFO] - Training Epoch: 1/2, step 2661/7134 completed (loss: 0.05773204192519188, acc: 0.9901823401451111)
[2025-02-13 02:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23][root][INFO] - Training Epoch: 1/2, step 2662/7134 completed (loss: 0.03252991661429405, acc: 0.991465151309967)
[2025-02-13 02:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23][root][INFO] - Training Epoch: 1/2, step 2663/7134 completed (loss: 0.06078600138425827, acc: 0.9836257100105286)
[2025-02-13 02:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23][root][INFO] - Training Epoch: 1/2, step 2664/7134 completed (loss: 0.04321535676717758, acc: 0.9909677505493164)
[2025-02-13 02:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24][root][INFO] - Training Epoch: 1/2, step 2665/7134 completed (loss: 0.026183223351836205, acc: 0.99609375)
[2025-02-13 02:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24][root][INFO] - Training Epoch: 1/2, step 2666/7134 completed (loss: 0.04420224204659462, acc: 0.9889841079711914)
[2025-02-13 02:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:25][root][INFO] - Training Epoch: 1/2, step 2667/7134 completed (loss: 0.02666579745709896, acc: 0.9943342804908752)
[2025-02-13 02:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:25][root][INFO] - Training Epoch: 1/2, step 2668/7134 completed (loss: 0.03137403726577759, acc: 0.9932705163955688)
[2025-02-13 02:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26][root][INFO] - Training Epoch: 1/2, step 2669/7134 completed (loss: 0.029193582013249397, acc: 0.9896432757377625)
[2025-02-13 02:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26][root][INFO] - Training Epoch: 1/2, step 2670/7134 completed (loss: 0.04284277185797691, acc: 0.9895150661468506)
[2025-02-13 02:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26][root][INFO] - Training Epoch: 1/2, step 2671/7134 completed (loss: 0.04642116650938988, acc: 0.9866310358047485)
[2025-02-13 02:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:27][root][INFO] - Training Epoch: 1/2, step 2672/7134 completed (loss: 0.027989482507109642, acc: 0.9941691160202026)
[2025-02-13 02:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:27][root][INFO] - Training Epoch: 1/2, step 2673/7134 completed (loss: 0.032473888248205185, acc: 0.9898989796638489)
[2025-02-13 02:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:28][root][INFO] - Training Epoch: 1/2, step 2674/7134 completed (loss: 0.03813879191875458, acc: 0.9902371168136597)
[2025-02-13 02:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:28][root][INFO] - Training Epoch: 1/2, step 2675/7134 completed (loss: 0.01940993033349514, acc: 0.9945205450057983)
[2025-02-13 02:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:29][root][INFO] - Training Epoch: 1/2, step 2676/7134 completed (loss: 0.035364847630262375, acc: 0.9896103739738464)
[2025-02-13 02:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:29][root][INFO] - Training Epoch: 1/2, step 2677/7134 completed (loss: 0.03686118870973587, acc: 0.9895833134651184)
[2025-02-13 02:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30][root][INFO] - Training Epoch: 1/2, step 2678/7134 completed (loss: 0.009493588469922543, acc: 0.9986807107925415)
[2025-02-13 02:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30][root][INFO] - Training Epoch: 1/2, step 2679/7134 completed (loss: 0.021458633244037628, acc: 0.9921436309814453)
[2025-02-13 02:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30][root][INFO] - Training Epoch: 1/2, step 2680/7134 completed (loss: 0.03178840130567551, acc: 0.992601752281189)
[2025-02-13 02:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:31][root][INFO] - Training Epoch: 1/2, step 2681/7134 completed (loss: 0.020057473331689835, acc: 0.9954476356506348)
[2025-02-13 02:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:31][root][INFO] - Training Epoch: 1/2, step 2682/7134 completed (loss: 0.010672427713871002, acc: 0.9968101978302002)
[2025-02-13 02:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32][root][INFO] - Training Epoch: 1/2, step 2683/7134 completed (loss: 0.01185566559433937, acc: 0.9969651103019714)
[2025-02-13 02:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32][root][INFO] - Training Epoch: 1/2, step 2684/7134 completed (loss: 0.05664682760834694, acc: 0.9892473220825195)
[2025-02-13 02:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32][root][INFO] - Training Epoch: 1/2, step 2685/7134 completed (loss: 0.030558820813894272, acc: 0.9921976327896118)
[2025-02-13 02:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:33][root][INFO] - Training Epoch: 1/2, step 2686/7134 completed (loss: 0.06931792199611664, acc: 0.9768518805503845)
[2025-02-13 02:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:33][root][INFO] - Training Epoch: 1/2, step 2687/7134 completed (loss: 0.08753754943609238, acc: 0.9752650260925293)
[2025-02-13 02:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:34][root][INFO] - Training Epoch: 1/2, step 2688/7134 completed (loss: 0.13346053659915924, acc: 0.9669811129570007)
[2025-02-13 02:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:34][root][INFO] - Training Epoch: 1/2, step 2689/7134 completed (loss: 0.10494770109653473, acc: 0.9801084995269775)
[2025-02-13 02:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35][root][INFO] - Training Epoch: 1/2, step 2690/7134 completed (loss: 0.10126582533121109, acc: 0.9629057049751282)
[2025-02-13 02:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35][root][INFO] - Training Epoch: 1/2, step 2691/7134 completed (loss: 0.1150054857134819, acc: 0.96875)
[2025-02-13 02:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35][root][INFO] - Training Epoch: 1/2, step 2692/7134 completed (loss: 0.1619207113981247, acc: 0.9627560377120972)
[2025-02-13 02:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:36][root][INFO] - Training Epoch: 1/2, step 2693/7134 completed (loss: 0.13910792768001556, acc: 0.9634615182876587)
[2025-02-13 02:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:36][root][INFO] - Training Epoch: 1/2, step 2694/7134 completed (loss: 0.1257428377866745, acc: 0.9720588326454163)
[2025-02-13 02:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37][root][INFO] - Training Epoch: 1/2, step 2695/7134 completed (loss: 0.10109732300043106, acc: 0.9715242981910706)
[2025-02-13 02:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37][root][INFO] - Training Epoch: 1/2, step 2696/7134 completed (loss: 0.16854040324687958, acc: 0.9570747017860413)
[2025-02-13 02:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37][root][INFO] - Training Epoch: 1/2, step 2697/7134 completed (loss: 0.1482178419828415, acc: 0.9491525292396545)
[2025-02-13 02:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:38][root][INFO] - Training Epoch: 1/2, step 2698/7134 completed (loss: 0.10652276873588562, acc: 0.9658119678497314)
[2025-02-13 02:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:38][root][INFO] - Training Epoch: 1/2, step 2699/7134 completed (loss: 0.08361395448446274, acc: 0.9690949320793152)
[2025-02-13 02:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39][root][INFO] - Training Epoch: 1/2, step 2700/7134 completed (loss: 0.13648776710033417, acc: 0.9584569931030273)
[2025-02-13 02:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39][root][INFO] - Training Epoch: 1/2, step 2701/7134 completed (loss: 0.10632620751857758, acc: 0.9714285731315613)
[2025-02-13 02:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39][root][INFO] - Training Epoch: 1/2, step 2702/7134 completed (loss: 0.12586398422718048, acc: 0.9635416865348816)
[2025-02-13 02:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39][root][INFO] - Training Epoch: 1/2, step 2703/7134 completed (loss: 0.0830405130982399, acc: 0.9808917045593262)
[2025-02-13 02:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:40][root][INFO] - Training Epoch: 1/2, step 2704/7134 completed (loss: 0.08448726683855057, acc: 0.979784369468689)
[2025-02-13 02:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:40][root][INFO] - Training Epoch: 1/2, step 2705/7134 completed (loss: 0.13334831595420837, acc: 0.9722222089767456)
[2025-02-13 02:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:41][root][INFO] - Training Epoch: 1/2, step 2706/7134 completed (loss: 0.06492623686790466, acc: 0.9796807169914246)
[2025-02-13 02:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:41][root][INFO] - Training Epoch: 1/2, step 2707/7134 completed (loss: 0.11079993098974228, acc: 0.9706896543502808)
[2025-02-13 02:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:42][root][INFO] - Training Epoch: 1/2, step 2708/7134 completed (loss: 0.11024941504001617, acc: 0.9635343551635742)
[2025-02-13 02:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:42][root][INFO] - Training Epoch: 1/2, step 2709/7134 completed (loss: 0.062243442982435226, acc: 0.9902597665786743)
[2025-02-13 02:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43][root][INFO] - Training Epoch: 1/2, step 2710/7134 completed (loss: 0.06667324155569077, acc: 0.9860140085220337)
[2025-02-13 02:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43][root][INFO] - Training Epoch: 1/2, step 2711/7134 completed (loss: 0.06059658154845238, acc: 0.9811320900917053)
[2025-02-13 02:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43][root][INFO] - Training Epoch: 1/2, step 2712/7134 completed (loss: 0.11589827388525009, acc: 0.9598214030265808)
[2025-02-13 02:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:44][root][INFO] - Training Epoch: 1/2, step 2713/7134 completed (loss: 0.11794435232877731, acc: 0.9693593382835388)
[2025-02-13 02:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:44][root][INFO] - Training Epoch: 1/2, step 2714/7134 completed (loss: 0.06224663555622101, acc: 0.981697142124176)
[2025-02-13 02:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:45][root][INFO] - Training Epoch: 1/2, step 2715/7134 completed (loss: 0.06221575662493706, acc: 0.9834586381912231)
[2025-02-13 02:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:45][root][INFO] - Training Epoch: 1/2, step 2716/7134 completed (loss: 0.07827413082122803, acc: 0.9737876653671265)
[2025-02-13 02:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46][root][INFO] - Training Epoch: 1/2, step 2717/7134 completed (loss: 0.13828013837337494, acc: 0.9527720808982849)
[2025-02-13 02:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46][root][INFO] - Training Epoch: 1/2, step 2718/7134 completed (loss: 0.19763541221618652, acc: 0.952464759349823)
[2025-02-13 02:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46][root][INFO] - Training Epoch: 1/2, step 2719/7134 completed (loss: 0.10311713814735413, acc: 0.962774932384491)
[2025-02-13 02:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47][root][INFO] - Training Epoch: 1/2, step 2720/7134 completed (loss: 0.08996610343456268, acc: 0.9700315594673157)
[2025-02-13 02:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47][root][INFO] - Training Epoch: 1/2, step 2721/7134 completed (loss: 0.0903225764632225, acc: 0.9811594486236572)
[2025-02-13 02:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47][root][INFO] - Training Epoch: 1/2, step 2722/7134 completed (loss: 0.18544906377792358, acc: 0.967391312122345)
[2025-02-13 02:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:48][root][INFO] - Training Epoch: 1/2, step 2723/7134 completed (loss: 0.14449000358581543, acc: 0.9581795930862427)
[2025-02-13 02:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:48][root][INFO] - Training Epoch: 1/2, step 2724/7134 completed (loss: 0.12100765109062195, acc: 0.9659781455993652)
[2025-02-13 02:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:49][root][INFO] - Training Epoch: 1/2, step 2725/7134 completed (loss: 0.07948772609233856, acc: 0.9819375872612)
[2025-02-13 02:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:49][root][INFO] - Training Epoch: 1/2, step 2726/7134 completed (loss: 0.09357663244009018, acc: 0.9778597950935364)
[2025-02-13 02:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50][root][INFO] - Training Epoch: 1/2, step 2727/7134 completed (loss: 0.07903773337602615, acc: 0.9759259223937988)
[2025-02-13 02:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50][root][INFO] - Training Epoch: 1/2, step 2728/7134 completed (loss: 0.20049770176410675, acc: 0.9380733966827393)
[2025-02-13 02:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50][root][INFO] - Training Epoch: 1/2, step 2729/7134 completed (loss: 0.11287634819746017, acc: 0.9634146094322205)
[2025-02-13 02:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:51][root][INFO] - Training Epoch: 1/2, step 2730/7134 completed (loss: 0.10418626666069031, acc: 0.9729166626930237)
[2025-02-13 02:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:51][root][INFO] - Training Epoch: 1/2, step 2731/7134 completed (loss: 0.13162316381931305, acc: 0.9537712931632996)
[2025-02-13 02:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52][root][INFO] - Training Epoch: 1/2, step 2732/7134 completed (loss: 0.09398076683282852, acc: 0.9670050740242004)
[2025-02-13 02:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52][root][INFO] - Training Epoch: 1/2, step 2733/7134 completed (loss: 0.10619136691093445, acc: 0.9660786986351013)
[2025-02-13 02:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52][root][INFO] - Training Epoch: 1/2, step 2734/7134 completed (loss: 0.0879514142870903, acc: 0.9758551120758057)
[2025-02-13 02:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53][root][INFO] - Training Epoch: 1/2, step 2735/7134 completed (loss: 0.07443489879369736, acc: 0.9728997349739075)
[2025-02-13 02:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53][root][INFO] - Training Epoch: 1/2, step 2736/7134 completed (loss: 0.1183667778968811, acc: 0.9682835936546326)
[2025-02-13 02:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:54][root][INFO] - Training Epoch: 1/2, step 2737/7134 completed (loss: 0.09714733064174652, acc: 0.971947193145752)
[2025-02-13 02:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:54][root][INFO] - Training Epoch: 1/2, step 2738/7134 completed (loss: 0.07771077007055283, acc: 0.9728571176528931)
[2025-02-13 02:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55][root][INFO] - Training Epoch: 1/2, step 2739/7134 completed (loss: 0.11693094670772552, acc: 0.9709091186523438)
[2025-02-13 02:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55][root][INFO] - Training Epoch: 1/2, step 2740/7134 completed (loss: 0.05934596434235573, acc: 0.9811320900917053)
[2025-02-13 02:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55][root][INFO] - Training Epoch: 1/2, step 2741/7134 completed (loss: 0.10728966444730759, acc: 0.9626168012619019)
[2025-02-13 02:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56][root][INFO] - Training Epoch: 1/2, step 2742/7134 completed (loss: 0.05733763054013252, acc: 0.9791666865348816)
[2025-02-13 02:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56][root][INFO] - Training Epoch: 1/2, step 2743/7134 completed (loss: 0.03766877204179764, acc: 0.9842105507850647)
[2025-02-13 02:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56][root][INFO] - Training Epoch: 1/2, step 2744/7134 completed (loss: 0.037449341267347336, acc: 0.992337167263031)
[2025-02-13 02:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57][root][INFO] - Training Epoch: 1/2, step 2745/7134 completed (loss: 0.12646718323230743, acc: 0.9686411023139954)
[2025-02-13 02:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57][root][INFO] - Training Epoch: 1/2, step 2746/7134 completed (loss: 0.06434794515371323, acc: 0.9871794581413269)
[2025-02-13 02:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57][root][INFO] - Training Epoch: 1/2, step 2747/7134 completed (loss: 0.06018752604722977, acc: 0.9803439974784851)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58][root][INFO] - Training Epoch: 1/2, step 2748/7134 completed (loss: 0.04133542999625206, acc: 0.993630588054657)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58][root][INFO] - Training Epoch: 1/2, step 2749/7134 completed (loss: 0.05808589607477188, acc: 0.9781420826911926)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58][root][INFO] - Training Epoch: 1/2, step 2750/7134 completed (loss: 0.05388304591178894, acc: 0.9849624037742615)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:59][root][INFO] - Training Epoch: 1/2, step 2751/7134 completed (loss: 0.06323983520269394, acc: 0.9822784662246704)
[2025-02-13 02:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:59][root][INFO] - Training Epoch: 1/2, step 2752/7134 completed (loss: 0.07233192771673203, acc: 0.9790874719619751)
[2025-02-13 02:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00][root][INFO] - Training Epoch: 1/2, step 2753/7134 completed (loss: 0.11787916719913483, acc: 0.9624060392379761)
[2025-02-13 02:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00][root][INFO] - Training Epoch: 1/2, step 2754/7134 completed (loss: 0.0899413675069809, acc: 0.9634888172149658)
[2025-02-13 02:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00][root][INFO] - Training Epoch: 1/2, step 2755/7134 completed (loss: 0.031624577939510345, acc: 0.9922480583190918)
[2025-02-13 02:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01][root][INFO] - Training Epoch: 1/2, step 2756/7134 completed (loss: 0.059740617871284485, acc: 0.9790475964546204)
[2025-02-13 02:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01][root][INFO] - Training Epoch: 1/2, step 2757/7134 completed (loss: 0.08923118561506271, acc: 0.9726775884628296)
[2025-02-13 02:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01][root][INFO] - Training Epoch: 1/2, step 2758/7134 completed (loss: 0.09607428312301636, acc: 0.9726027250289917)
[2025-02-13 02:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02][root][INFO] - Training Epoch: 1/2, step 2759/7134 completed (loss: 0.057198796421289444, acc: 0.9832776188850403)
[2025-02-13 02:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02][root][INFO] - Training Epoch: 1/2, step 2760/7134 completed (loss: 0.08211736381053925, acc: 0.961904764175415)
[2025-02-13 02:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02][root][INFO] - Training Epoch: 1/2, step 2761/7134 completed (loss: 0.12094111740589142, acc: 0.9671717286109924)
[2025-02-13 02:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:03][root][INFO] - Training Epoch: 1/2, step 2762/7134 completed (loss: 0.08488292992115021, acc: 0.9769850373268127)
[2025-02-13 02:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:03][root][INFO] - Training Epoch: 1/2, step 2763/7134 completed (loss: 0.0824185162782669, acc: 0.9811557531356812)
[2025-02-13 02:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:04][root][INFO] - Training Epoch: 1/2, step 2764/7134 completed (loss: 0.10537943243980408, acc: 0.9657443761825562)
[2025-02-13 02:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:04][root][INFO] - Training Epoch: 1/2, step 2765/7134 completed (loss: 0.06819485872983932, acc: 0.977011501789093)
[2025-02-13 02:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05][root][INFO] - Training Epoch: 1/2, step 2766/7134 completed (loss: 0.04515961557626724, acc: 0.987075924873352)
[2025-02-13 02:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05][root][INFO] - Training Epoch: 1/2, step 2767/7134 completed (loss: 0.12734535336494446, acc: 0.9654321074485779)
[2025-02-13 02:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06][root][INFO] - Training Epoch: 1/2, step 2768/7134 completed (loss: 0.0701797604560852, acc: 0.9801980257034302)
[2025-02-13 02:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06][root][INFO] - Training Epoch: 1/2, step 2769/7134 completed (loss: 0.0943291187286377, acc: 0.9763513803482056)
[2025-02-13 02:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06][root][INFO] - Training Epoch: 1/2, step 2770/7134 completed (loss: 0.05554141476750374, acc: 0.9860917925834656)
[2025-02-13 02:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:07][root][INFO] - Training Epoch: 1/2, step 2771/7134 completed (loss: 0.09312877058982849, acc: 0.9742765426635742)
[2025-02-13 02:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:07][root][INFO] - Training Epoch: 1/2, step 2772/7134 completed (loss: 0.05836917459964752, acc: 0.9798657894134521)
[2025-02-13 02:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:08][root][INFO] - Training Epoch: 1/2, step 2773/7134 completed (loss: 0.090673066675663, acc: 0.9760000109672546)
[2025-02-13 02:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:08][root][INFO] - Training Epoch: 1/2, step 2774/7134 completed (loss: 0.08781901001930237, acc: 0.9736841917037964)
[2025-02-13 02:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:09][root][INFO] - Training Epoch: 1/2, step 2775/7134 completed (loss: 0.065469890832901, acc: 0.9852761030197144)
[2025-02-13 02:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:09][root][INFO] - Training Epoch: 1/2, step 2776/7134 completed (loss: 0.060671355575323105, acc: 0.9815950989723206)
[2025-02-13 02:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:10][root][INFO] - Training Epoch: 1/2, step 2777/7134 completed (loss: 0.06307504326105118, acc: 0.9780488014221191)
[2025-02-13 02:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:10][root][INFO] - Training Epoch: 1/2, step 2778/7134 completed (loss: 0.06210770457983017, acc: 0.9849362969398499)
[2025-02-13 02:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11][root][INFO] - Training Epoch: 1/2, step 2779/7134 completed (loss: 0.10219905525445938, acc: 0.9736286997795105)
[2025-02-13 02:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11][root][INFO] - Training Epoch: 1/2, step 2780/7134 completed (loss: 0.08040283620357513, acc: 0.9746835231781006)
[2025-02-13 02:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11][root][INFO] - Training Epoch: 1/2, step 2781/7134 completed (loss: 0.06441625207662582, acc: 0.9831606149673462)
[2025-02-13 02:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:12][root][INFO] - Training Epoch: 1/2, step 2782/7134 completed (loss: 0.061899177730083466, acc: 0.9852941036224365)
[2025-02-13 02:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:12][root][INFO] - Training Epoch: 1/2, step 2783/7134 completed (loss: 0.06136176362633705, acc: 0.9759759902954102)
[2025-02-13 02:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:13][root][INFO] - Training Epoch: 1/2, step 2784/7134 completed (loss: 0.07645855098962784, acc: 0.9789695143699646)
[2025-02-13 02:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:13][root][INFO] - Training Epoch: 1/2, step 2785/7134 completed (loss: 0.035073183476924896, acc: 0.9879912734031677)
[2025-02-13 02:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:14][root][INFO] - Training Epoch: 1/2, step 2786/7134 completed (loss: 0.07126062363386154, acc: 0.9833852648735046)
[2025-02-13 02:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:14][root][INFO] - Training Epoch: 1/2, step 2787/7134 completed (loss: 0.05755063146352768, acc: 0.9840637445449829)
[2025-02-13 02:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15][root][INFO] - Training Epoch: 1/2, step 2788/7134 completed (loss: 0.07603650540113449, acc: 0.9725130796432495)
[2025-02-13 02:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15][root][INFO] - Training Epoch: 1/2, step 2789/7134 completed (loss: 0.2114304155111313, acc: 0.9481267929077148)
[2025-02-13 02:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15][root][INFO] - Training Epoch: 1/2, step 2790/7134 completed (loss: 0.21138830482959747, acc: 0.9580574035644531)
[2025-02-13 02:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16][root][INFO] - Training Epoch: 1/2, step 2791/7134 completed (loss: 0.05880828946828842, acc: 0.9865471124649048)
[2025-02-13 02:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16][root][INFO] - Training Epoch: 1/2, step 2792/7134 completed (loss: 0.045567747205495834, acc: 0.9871944189071655)
[2025-02-13 02:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17][root][INFO] - Training Epoch: 1/2, step 2793/7134 completed (loss: 0.07033555954694748, acc: 0.9757084846496582)
[2025-02-13 02:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17][root][INFO] - Training Epoch: 1/2, step 2794/7134 completed (loss: 0.09466876834630966, acc: 0.9706840515136719)
[2025-02-13 02:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17][root][INFO] - Training Epoch: 1/2, step 2795/7134 completed (loss: 0.22642453014850616, acc: 0.9373549818992615)
[2025-02-13 02:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18][root][INFO] - Training Epoch: 1/2, step 2796/7134 completed (loss: 0.2616545557975769, acc: 0.9377880096435547)
[2025-02-13 02:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18][root][INFO] - Training Epoch: 1/2, step 2797/7134 completed (loss: 0.20693786442279816, acc: 0.9683544039726257)
[2025-02-13 02:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18][root][INFO] - Training Epoch: 1/2, step 2798/7134 completed (loss: 0.16972051560878754, acc: 0.9474790096282959)
[2025-02-13 02:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:19][root][INFO] - Training Epoch: 1/2, step 2799/7134 completed (loss: 0.14008434116840363, acc: 0.9597197771072388)
[2025-02-13 02:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:19][root][INFO] - Training Epoch: 1/2, step 2800/7134 completed (loss: 0.12580488622188568, acc: 0.9648506045341492)
[2025-02-13 02:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20][root][INFO] - Training Epoch: 1/2, step 2801/7134 completed (loss: 0.08116389065980911, acc: 0.9744245409965515)
[2025-02-13 02:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20][root][INFO] - Training Epoch: 1/2, step 2802/7134 completed (loss: 0.11318041384220123, acc: 0.971285879611969)
[2025-02-13 02:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21][root][INFO] - Training Epoch: 1/2, step 2803/7134 completed (loss: 0.03916902467608452, acc: 0.985855758190155)
[2025-02-13 02:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21][root][INFO] - Training Epoch: 1/2, step 2804/7134 completed (loss: 0.0808897614479065, acc: 0.9801980257034302)
[2025-02-13 02:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21][root][INFO] - Training Epoch: 1/2, step 2805/7134 completed (loss: 0.047602616250514984, acc: 0.9881423115730286)
[2025-02-13 02:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:22][root][INFO] - Training Epoch: 1/2, step 2806/7134 completed (loss: 0.09443306922912598, acc: 0.9721311330795288)
[2025-02-13 02:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:22][root][INFO] - Training Epoch: 1/2, step 2807/7134 completed (loss: 0.1522539258003235, acc: 0.9480249285697937)
[2025-02-13 02:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:23][root][INFO] - Training Epoch: 1/2, step 2808/7134 completed (loss: 0.05739273503422737, acc: 0.980571448802948)
[2025-02-13 02:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:23][root][INFO] - Training Epoch: 1/2, step 2809/7134 completed (loss: 0.08617125451564789, acc: 0.9750445485115051)
[2025-02-13 02:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24][root][INFO] - Training Epoch: 1/2, step 2810/7134 completed (loss: 0.04368316009640694, acc: 0.9869358539581299)
[2025-02-13 02:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24][root][INFO] - Training Epoch: 1/2, step 2811/7134 completed (loss: 0.024308687075972557, acc: 0.9964200258255005)
[2025-02-13 02:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24][root][INFO] - Training Epoch: 1/2, step 2812/7134 completed (loss: 0.037866346538066864, acc: 0.9861111044883728)
[2025-02-13 02:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:25][root][INFO] - Training Epoch: 1/2, step 2813/7134 completed (loss: 0.028334233909845352, acc: 0.988664984703064)
[2025-02-13 02:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:25][root][INFO] - Training Epoch: 1/2, step 2814/7134 completed (loss: 0.048793893307447433, acc: 0.9896103739738464)
[2025-02-13 02:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:26][root][INFO] - Training Epoch: 1/2, step 2815/7134 completed (loss: 0.07522937655448914, acc: 0.982206404209137)
[2025-02-13 02:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:26][root][INFO] - Training Epoch: 1/2, step 2816/7134 completed (loss: 0.08047566562891006, acc: 0.9736495614051819)
[2025-02-13 02:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:27][root][INFO] - Training Epoch: 1/2, step 2817/7134 completed (loss: 0.07226202636957169, acc: 0.9821802973747253)
[2025-02-13 02:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:27][root][INFO] - Training Epoch: 1/2, step 2818/7134 completed (loss: 0.05118832364678383, acc: 0.984649121761322)
[2025-02-13 02:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28][root][INFO] - Training Epoch: 1/2, step 2819/7134 completed (loss: 0.04830564185976982, acc: 0.9886914491653442)
[2025-02-13 02:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28][root][INFO] - Training Epoch: 1/2, step 2820/7134 completed (loss: 0.1506357491016388, acc: 0.9631811380386353)
[2025-02-13 02:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28][root][INFO] - Training Epoch: 1/2, step 2821/7134 completed (loss: 0.09589725732803345, acc: 0.9737156629562378)
[2025-02-13 02:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:29][root][INFO] - Training Epoch: 1/2, step 2822/7134 completed (loss: 0.13258592784404755, acc: 0.9633375406265259)
[2025-02-13 02:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:29][root][INFO] - Training Epoch: 1/2, step 2823/7134 completed (loss: 0.15366990864276886, acc: 0.9531915187835693)
[2025-02-13 02:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:30][root][INFO] - Training Epoch: 1/2, step 2824/7134 completed (loss: 0.1539275199174881, acc: 0.948051929473877)
[2025-02-13 02:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:30][root][INFO] - Training Epoch: 1/2, step 2825/7134 completed (loss: 0.09997677057981491, acc: 0.9812080264091492)
[2025-02-13 02:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31][root][INFO] - Training Epoch: 1/2, step 2826/7134 completed (loss: 0.13902251422405243, acc: 0.9608433842658997)
[2025-02-13 02:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31][root][INFO] - Training Epoch: 1/2, step 2827/7134 completed (loss: 0.13649000227451324, acc: 0.961685836315155)
[2025-02-13 02:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31][root][INFO] - Training Epoch: 1/2, step 2828/7134 completed (loss: 0.07727234810590744, acc: 0.984000027179718)
[2025-02-13 02:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:32][root][INFO] - Training Epoch: 1/2, step 2829/7134 completed (loss: 0.11630535125732422, acc: 0.9604365825653076)
[2025-02-13 02:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:32][root][INFO] - Training Epoch: 1/2, step 2830/7134 completed (loss: 0.035807445645332336, acc: 0.9908925294876099)
[2025-02-13 02:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:33][root][INFO] - Training Epoch: 1/2, step 2831/7134 completed (loss: 0.06391674280166626, acc: 0.9810426831245422)
[2025-02-13 02:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:33][root][INFO] - Training Epoch: 1/2, step 2832/7134 completed (loss: 0.07028891146183014, acc: 0.9903846383094788)
[2025-02-13 02:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34][root][INFO] - Training Epoch: 1/2, step 2833/7134 completed (loss: 0.07038391381502151, acc: 0.9836257100105286)
[2025-02-13 02:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34][root][INFO] - Training Epoch: 1/2, step 2834/7134 completed (loss: 0.10320018976926804, acc: 0.9746192693710327)
[2025-02-13 02:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34][root][INFO] - Training Epoch: 1/2, step 2835/7134 completed (loss: 0.10053552687168121, acc: 0.9757673740386963)
[2025-02-13 02:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:35][root][INFO] - Training Epoch: 1/2, step 2836/7134 completed (loss: 0.11817017942667007, acc: 0.971563994884491)
[2025-02-13 02:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:35][root][INFO] - Training Epoch: 1/2, step 2837/7134 completed (loss: 0.0763377696275711, acc: 0.9770269989967346)
[2025-02-13 02:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:36][root][INFO] - Training Epoch: 1/2, step 2838/7134 completed (loss: 0.12534219026565552, acc: 0.965481162071228)
[2025-02-13 02:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:36][root][INFO] - Training Epoch: 1/2, step 2839/7134 completed (loss: 0.08317571878433228, acc: 0.9772382378578186)
[2025-02-13 02:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37][root][INFO] - Training Epoch: 1/2, step 2840/7134 completed (loss: 0.045146748423576355, acc: 0.9875690340995789)
[2025-02-13 02:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37][root][INFO] - Training Epoch: 1/2, step 2841/7134 completed (loss: 0.10253603756427765, acc: 0.9723502397537231)
[2025-02-13 02:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38][root][INFO] - Training Epoch: 1/2, step 2842/7134 completed (loss: 0.11835864931344986, acc: 0.9623376727104187)
[2025-02-13 02:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38][root][INFO] - Training Epoch: 1/2, step 2843/7134 completed (loss: 0.04939357936382294, acc: 0.9808823466300964)
[2025-02-13 02:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38][root][INFO] - Training Epoch: 1/2, step 2844/7134 completed (loss: 0.16637185215950012, acc: 0.9575923681259155)
[2025-02-13 02:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:39][root][INFO] - Training Epoch: 1/2, step 2845/7134 completed (loss: 0.05587654933333397, acc: 0.9826689958572388)
[2025-02-13 02:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:39][root][INFO] - Training Epoch: 1/2, step 2846/7134 completed (loss: 0.15254880487918854, acc: 0.9571663737297058)
[2025-02-13 02:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40][root][INFO] - Training Epoch: 1/2, step 2847/7134 completed (loss: 0.10533461719751358, acc: 0.9677419066429138)
[2025-02-13 02:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40][root][INFO] - Training Epoch: 1/2, step 2848/7134 completed (loss: 0.09731823951005936, acc: 0.9673659801483154)
[2025-02-13 02:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40][root][INFO] - Training Epoch: 1/2, step 2849/7134 completed (loss: 0.06882885843515396, acc: 0.9792531132698059)
[2025-02-13 02:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:41][root][INFO] - Training Epoch: 1/2, step 2850/7134 completed (loss: 0.06705249845981598, acc: 0.9806451797485352)
[2025-02-13 02:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:41][root][INFO] - Training Epoch: 1/2, step 2851/7134 completed (loss: 0.06178374961018562, acc: 0.979742169380188)
[2025-02-13 02:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42][root][INFO] - Training Epoch: 1/2, step 2852/7134 completed (loss: 0.03504588082432747, acc: 0.9920477271080017)
[2025-02-13 02:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42][root][INFO] - Training Epoch: 1/2, step 2853/7134 completed (loss: 0.06565356254577637, acc: 0.9822335243225098)
[2025-02-13 02:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42][root][INFO] - Training Epoch: 1/2, step 2854/7134 completed (loss: 0.07671904563903809, acc: 0.9806835055351257)
[2025-02-13 02:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:43][root][INFO] - Training Epoch: 1/2, step 2855/7134 completed (loss: 0.06738285720348358, acc: 0.9903069734573364)
[2025-02-13 02:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:43][root][INFO] - Training Epoch: 1/2, step 2856/7134 completed (loss: 0.030828239396214485, acc: 0.9887164831161499)
[2025-02-13 02:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44][root][INFO] - Training Epoch: 1/2, step 2857/7134 completed (loss: 0.08397217839956284, acc: 0.9797688126564026)
[2025-02-13 02:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44][root][INFO] - Training Epoch: 1/2, step 2858/7134 completed (loss: 0.023042961955070496, acc: 0.9936908483505249)
[2025-02-13 02:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44][root][INFO] - Training Epoch: 1/2, step 2859/7134 completed (loss: 0.07678532600402832, acc: 0.9776847958564758)
[2025-02-13 02:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:45][root][INFO] - Training Epoch: 1/2, step 2860/7134 completed (loss: 0.055940110236406326, acc: 0.9798164963722229)
[2025-02-13 02:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:45][root][INFO] - Training Epoch: 1/2, step 2861/7134 completed (loss: 0.03154701739549637, acc: 0.9918864369392395)
[2025-02-13 02:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46][root][INFO] - Training Epoch: 1/2, step 2862/7134 completed (loss: 0.04191764071583748, acc: 0.9879931211471558)
[2025-02-13 02:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46][root][INFO] - Training Epoch: 1/2, step 2863/7134 completed (loss: 0.024868115782737732, acc: 0.993630588054657)
[2025-02-13 02:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46][root][INFO] - Training Epoch: 1/2, step 2864/7134 completed (loss: 0.06738810241222382, acc: 0.984415590763092)
[2025-02-13 02:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:47][root][INFO] - Training Epoch: 1/2, step 2865/7134 completed (loss: 0.050949838012456894, acc: 0.9847826361656189)
[2025-02-13 02:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:47][root][INFO] - Training Epoch: 1/2, step 2866/7134 completed (loss: 0.0831986591219902, acc: 0.9760836958885193)
[2025-02-13 02:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48][root][INFO] - Training Epoch: 1/2, step 2867/7134 completed (loss: 0.039618607610464096, acc: 0.9907975196838379)
[2025-02-13 02:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48][root][INFO] - Training Epoch: 1/2, step 2868/7134 completed (loss: 0.023962076753377914, acc: 0.9943820238113403)
[2025-02-13 02:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48][root][INFO] - Training Epoch: 1/2, step 2869/7134 completed (loss: 0.06796078383922577, acc: 0.9912152290344238)
[2025-02-13 02:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:49][root][INFO] - Training Epoch: 1/2, step 2870/7134 completed (loss: 0.0969700813293457, acc: 0.9790356159210205)
[2025-02-13 02:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:49][root][INFO] - Training Epoch: 1/2, step 2871/7134 completed (loss: 0.07808828353881836, acc: 0.9859594106674194)
[2025-02-13 02:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50][root][INFO] - Training Epoch: 1/2, step 2872/7134 completed (loss: 0.023151149973273277, acc: 0.9921875)
[2025-02-13 02:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50][root][INFO] - Training Epoch: 1/2, step 2873/7134 completed (loss: 0.03821418806910515, acc: 0.985401451587677)
[2025-02-13 02:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50][root][INFO] - Training Epoch: 1/2, step 2874/7134 completed (loss: 0.044334352016448975, acc: 0.9859594106674194)
[2025-02-13 02:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:51][root][INFO] - Training Epoch: 1/2, step 2875/7134 completed (loss: 0.03786740079522133, acc: 0.9843527674674988)
[2025-02-13 02:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:51][root][INFO] - Training Epoch: 1/2, step 2876/7134 completed (loss: 0.16320595145225525, acc: 0.9723502397537231)
[2025-02-13 02:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52][root][INFO] - Training Epoch: 1/2, step 2877/7134 completed (loss: 0.028596652671694756, acc: 0.9912663698196411)
[2025-02-13 02:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52][root][INFO] - Training Epoch: 1/2, step 2878/7134 completed (loss: 0.04248370602726936, acc: 0.9957627058029175)
[2025-02-13 02:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52][root][INFO] - Training Epoch: 1/2, step 2879/7134 completed (loss: 0.019977951422333717, acc: 0.9952977895736694)
[2025-02-13 02:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53][root][INFO] - Training Epoch: 1/2, step 2880/7134 completed (loss: 0.03286002203822136, acc: 0.9921875)
[2025-02-13 02:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53][root][INFO] - Training Epoch: 1/2, step 2881/7134 completed (loss: 0.06892910599708557, acc: 0.980322003364563)
[2025-02-13 02:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54][root][INFO] - Training Epoch: 1/2, step 2882/7134 completed (loss: 0.09825997054576874, acc: 0.9728729724884033)
[2025-02-13 02:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54][root][INFO] - Training Epoch: 1/2, step 2883/7134 completed (loss: 0.06303750723600388, acc: 0.9814814925193787)
[2025-02-13 02:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54][root][INFO] - Training Epoch: 1/2, step 2884/7134 completed (loss: 0.12412692606449127, acc: 0.9698340892791748)
[2025-02-13 02:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:55][root][INFO] - Training Epoch: 1/2, step 2885/7134 completed (loss: 0.07365154474973679, acc: 0.9802731275558472)
[2025-02-13 02:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:55][root][INFO] - Training Epoch: 1/2, step 2886/7134 completed (loss: 0.13509327173233032, acc: 0.9686567187309265)
[2025-02-13 02:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56][root][INFO] - Training Epoch: 1/2, step 2887/7134 completed (loss: 0.08361051976680756, acc: 0.9798816442489624)
[2025-02-13 02:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56][root][INFO] - Training Epoch: 1/2, step 2888/7134 completed (loss: 0.08781543374061584, acc: 0.9778085947036743)
[2025-02-13 02:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56][root][INFO] - Training Epoch: 1/2, step 2889/7134 completed (loss: 0.05887099727988243, acc: 0.9833794832229614)
[2025-02-13 02:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:57][root][INFO] - Training Epoch: 1/2, step 2890/7134 completed (loss: 0.052509695291519165, acc: 0.9826897382736206)
[2025-02-13 02:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:57][root][INFO] - Training Epoch: 1/2, step 2891/7134 completed (loss: 0.10478372126817703, acc: 0.9707692265510559)
[2025-02-13 02:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:58][root][INFO] - Training Epoch: 1/2, step 2892/7134 completed (loss: 0.08694005757570267, acc: 0.9722921848297119)
[2025-02-13 02:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:58][root][INFO] - Training Epoch: 1/2, step 2893/7134 completed (loss: 0.10895869880914688, acc: 0.9646910429000854)
[2025-02-13 02:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59][root][INFO] - Training Epoch: 1/2, step 2894/7134 completed (loss: 0.06120289862155914, acc: 0.9764705896377563)
[2025-02-13 02:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59][root][INFO] - Training Epoch: 1/2, step 2895/7134 completed (loss: 0.04911333695054054, acc: 0.9867674708366394)
[2025-02-13 02:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59][root][INFO] - Training Epoch: 1/2, step 2896/7134 completed (loss: 0.04304690659046173, acc: 0.9885495901107788)
[2025-02-13 02:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:00][root][INFO] - Training Epoch: 1/2, step 2897/7134 completed (loss: 0.09055212140083313, acc: 0.9698046445846558)
[2025-02-13 02:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:00][root][INFO] - Training Epoch: 1/2, step 2898/7134 completed (loss: 0.05231244117021561, acc: 0.9869281053543091)
[2025-02-13 02:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:01][root][INFO] - Training Epoch: 1/2, step 2899/7134 completed (loss: 0.057020120322704315, acc: 0.9883720874786377)
[2025-02-13 02:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:01][root][INFO] - Training Epoch: 1/2, step 2900/7134 completed (loss: 0.06587263941764832, acc: 0.9864197373390198)
[2025-02-13 02:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02][root][INFO] - Training Epoch: 1/2, step 2901/7134 completed (loss: 0.06262579560279846, acc: 0.977707028388977)
[2025-02-13 02:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02][root][INFO] - Training Epoch: 1/2, step 2902/7134 completed (loss: 0.073072150349617, acc: 0.981203019618988)
[2025-02-13 02:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02][root][INFO] - Training Epoch: 1/2, step 2903/7134 completed (loss: 0.09173420071601868, acc: 0.9743589758872986)
[2025-02-13 02:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:03][root][INFO] - Training Epoch: 1/2, step 2904/7134 completed (loss: 0.04270809888839722, acc: 0.9875156283378601)
[2025-02-13 02:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:03][root][INFO] - Training Epoch: 1/2, step 2905/7134 completed (loss: 0.08370975404977798, acc: 0.9762202501296997)
[2025-02-13 02:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04][root][INFO] - Training Epoch: 1/2, step 2906/7134 completed (loss: 0.04621085152029991, acc: 0.9880239367485046)
[2025-02-13 02:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04][root][INFO] - Training Epoch: 1/2, step 2907/7134 completed (loss: 0.0778098776936531, acc: 0.9757738709449768)
[2025-02-13 02:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04][root][INFO] - Training Epoch: 1/2, step 2908/7134 completed (loss: 0.051414258778095245, acc: 0.9857697486877441)
[2025-02-13 02:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05][root][INFO] - Training Epoch: 1/2, step 2909/7134 completed (loss: 0.055948514491319656, acc: 0.9856114983558655)
[2025-02-13 02:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05][root][INFO] - Training Epoch: 1/2, step 2910/7134 completed (loss: 0.1040862426161766, acc: 0.9721627235412598)
[2025-02-13 02:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:06][root][INFO] - Training Epoch: 1/2, step 2911/7134 completed (loss: 0.053912900388240814, acc: 0.982550323009491)
[2025-02-13 02:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:06][root][INFO] - Training Epoch: 1/2, step 2912/7134 completed (loss: 0.05986235663294792, acc: 0.9820689558982849)
[2025-02-13 02:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:07][root][INFO] - Training Epoch: 1/2, step 2913/7134 completed (loss: 0.07062757760286331, acc: 0.9814814925193787)
[2025-02-13 02:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:07][root][INFO] - Training Epoch: 1/2, step 2914/7134 completed (loss: 0.056634970009326935, acc: 0.9819059371948242)
[2025-02-13 02:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08][root][INFO] - Training Epoch: 1/2, step 2915/7134 completed (loss: 0.105211041867733, acc: 0.9769452214241028)
[2025-02-13 02:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08][root][INFO] - Training Epoch: 1/2, step 2916/7134 completed (loss: 0.08505905419588089, acc: 0.9725086092948914)
[2025-02-13 02:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08][root][INFO] - Training Epoch: 1/2, step 2917/7134 completed (loss: 0.06371670216321945, acc: 0.9782244563102722)
[2025-02-13 02:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:09][root][INFO] - Training Epoch: 1/2, step 2918/7134 completed (loss: 0.041901446878910065, acc: 0.9895104765892029)
[2025-02-13 02:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:09][root][INFO] - Training Epoch: 1/2, step 2919/7134 completed (loss: 0.0916796624660492, acc: 0.9764982461929321)
[2025-02-13 02:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:10][root][INFO] - Training Epoch: 1/2, step 2920/7134 completed (loss: 0.051665835082530975, acc: 0.9827814698219299)
[2025-02-13 02:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:10][root][INFO] - Training Epoch: 1/2, step 2921/7134 completed (loss: 0.042340945452451706, acc: 0.9885057210922241)
[2025-02-13 02:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11][root][INFO] - Training Epoch: 1/2, step 2922/7134 completed (loss: 0.09960300475358963, acc: 0.9723618030548096)
[2025-02-13 02:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11][root][INFO] - Training Epoch: 1/2, step 2923/7134 completed (loss: 0.1932213008403778, acc: 0.9585253596305847)
[2025-02-13 02:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11][root][INFO] - Training Epoch: 1/2, step 2924/7134 completed (loss: 0.05788489431142807, acc: 0.9799528121948242)
[2025-02-13 02:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:12][root][INFO] - Training Epoch: 1/2, step 2925/7134 completed (loss: 0.0661623552441597, acc: 0.9864864945411682)
[2025-02-13 02:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:12][root][INFO] - Training Epoch: 1/2, step 2926/7134 completed (loss: 0.14072738587856293, acc: 0.979567289352417)
[2025-02-13 02:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:13][root][INFO] - Training Epoch: 1/2, step 2927/7134 completed (loss: 0.029631931334733963, acc: 0.990326464176178)
[2025-02-13 02:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:13][root][INFO] - Training Epoch: 1/2, step 2928/7134 completed (loss: 0.058714382350444794, acc: 0.9817073345184326)
[2025-02-13 02:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:14][root][INFO] - Training Epoch: 1/2, step 2929/7134 completed (loss: 0.04881411790847778, acc: 0.9921671152114868)
[2025-02-13 02:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:14][root][INFO] - Training Epoch: 1/2, step 2930/7134 completed (loss: 0.05219605565071106, acc: 0.9809104204177856)
[2025-02-13 02:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15][root][INFO] - Training Epoch: 1/2, step 2931/7134 completed (loss: 0.08165224641561508, acc: 0.974588930606842)
[2025-02-13 02:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15][root][INFO] - Training Epoch: 1/2, step 2932/7134 completed (loss: 0.0701727569103241, acc: 0.9803012609481812)
[2025-02-13 02:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15][root][INFO] - Training Epoch: 1/2, step 2933/7134 completed (loss: 0.04307544231414795, acc: 0.9841269850730896)
[2025-02-13 02:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:16][root][INFO] - Training Epoch: 1/2, step 2934/7134 completed (loss: 0.038668468594551086, acc: 0.9858956336975098)
[2025-02-13 02:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:16][root][INFO] - Training Epoch: 1/2, step 2935/7134 completed (loss: 0.04466968774795532, acc: 0.9889867901802063)
[2025-02-13 02:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:17][root][INFO] - Training Epoch: 1/2, step 2936/7134 completed (loss: 0.06397642940282822, acc: 0.9837232828140259)
[2025-02-13 02:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:17][root][INFO] - Training Epoch: 1/2, step 2937/7134 completed (loss: 0.09901094436645508, acc: 0.9756097793579102)
[2025-02-13 02:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18][root][INFO] - Training Epoch: 1/2, step 2938/7134 completed (loss: 0.04200087487697601, acc: 0.9910614490509033)
[2025-02-13 02:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18][root][INFO] - Training Epoch: 1/2, step 2939/7134 completed (loss: 0.0400862954556942, acc: 0.9887820482254028)
[2025-02-13 02:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18][root][INFO] - Training Epoch: 1/2, step 2940/7134 completed (loss: 0.0890134647488594, acc: 0.9822161197662354)
[2025-02-13 02:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:19][root][INFO] - Training Epoch: 1/2, step 2941/7134 completed (loss: 0.03850066289305687, acc: 0.9869375824928284)
[2025-02-13 02:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:19][root][INFO] - Training Epoch: 1/2, step 2942/7134 completed (loss: 0.04464525729417801, acc: 0.9910846948623657)
[2025-02-13 02:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20][root][INFO] - Training Epoch: 1/2, step 2943/7134 completed (loss: 0.02817261591553688, acc: 0.9915682673454285)
[2025-02-13 02:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20][root][INFO] - Training Epoch: 1/2, step 2944/7134 completed (loss: 0.04778377339243889, acc: 0.988252580165863)
[2025-02-13 02:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20][root][INFO] - Training Epoch: 1/2, step 2945/7134 completed (loss: 0.04312299191951752, acc: 0.9844478964805603)
[2025-02-13 02:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:21][root][INFO] - Training Epoch: 1/2, step 2946/7134 completed (loss: 0.05589735880494118, acc: 0.9854651093482971)
[2025-02-13 02:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:21][root][INFO] - Training Epoch: 1/2, step 2947/7134 completed (loss: 0.07234115153551102, acc: 0.9779874086380005)
[2025-02-13 02:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22][root][INFO] - Training Epoch: 1/2, step 2948/7134 completed (loss: 0.07780655473470688, acc: 0.9785832166671753)
[2025-02-13 02:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22][root][INFO] - Training Epoch: 1/2, step 2949/7134 completed (loss: 0.017298193648457527, acc: 0.9938837885856628)
[2025-02-13 02:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22][root][INFO] - Training Epoch: 1/2, step 2950/7134 completed (loss: 0.03520976006984711, acc: 0.9875862002372742)
[2025-02-13 02:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:23][root][INFO] - Training Epoch: 1/2, step 2951/7134 completed (loss: 0.01625112257897854, acc: 0.9927536249160767)
[2025-02-13 02:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:23][root][INFO] - Training Epoch: 1/2, step 2952/7134 completed (loss: 0.021638574078679085, acc: 0.9955489635467529)
[2025-02-13 02:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24][root][INFO] - Training Epoch: 1/2, step 2953/7134 completed (loss: 0.04322713613510132, acc: 0.9847972989082336)
[2025-02-13 02:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24][root][INFO] - Training Epoch: 1/2, step 2954/7134 completed (loss: 0.04071129113435745, acc: 0.9884169697761536)
[2025-02-13 02:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24][root][INFO] - Training Epoch: 1/2, step 2955/7134 completed (loss: 0.046571698039770126, acc: 0.9872159361839294)
[2025-02-13 02:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:25][root][INFO] - Training Epoch: 1/2, step 2956/7134 completed (loss: 0.04350907355546951, acc: 0.9868228435516357)
[2025-02-13 02:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:25][root][INFO] - Training Epoch: 1/2, step 2957/7134 completed (loss: 0.02597196027636528, acc: 0.9946523904800415)
[2025-02-13 02:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:26][root][INFO] - Training Epoch: 1/2, step 2958/7134 completed (loss: 0.0283347200602293, acc: 0.9914529919624329)
[2025-02-13 02:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:26][root][INFO] - Training Epoch: 1/2, step 2959/7134 completed (loss: 0.030855970457196236, acc: 0.9897660613059998)
[2025-02-13 02:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27][root][INFO] - Training Epoch: 1/2, step 2960/7134 completed (loss: 0.02861919067800045, acc: 0.9903581142425537)
[2025-02-13 02:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27][root][INFO] - Training Epoch: 1/2, step 2961/7134 completed (loss: 0.03471580147743225, acc: 0.9863945841789246)
[2025-02-13 02:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27][root][INFO] - Training Epoch: 1/2, step 2962/7134 completed (loss: 0.028264909982681274, acc: 0.9909443855285645)
[2025-02-13 02:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:28][root][INFO] - Training Epoch: 1/2, step 2963/7134 completed (loss: 0.036672722548246384, acc: 0.9869109988212585)
[2025-02-13 02:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:28][root][INFO] - Training Epoch: 1/2, step 2964/7134 completed (loss: 0.029900113120675087, acc: 0.9882869720458984)
[2025-02-13 02:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29][root][INFO] - Training Epoch: 1/2, step 2965/7134 completed (loss: 0.07945048809051514, acc: 0.9746268391609192)
[2025-02-13 02:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29][root][INFO] - Training Epoch: 1/2, step 2966/7134 completed (loss: 0.013188916258513927, acc: 0.9957355856895447)
[2025-02-13 02:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29][root][INFO] - Training Epoch: 1/2, step 2967/7134 completed (loss: 0.04684417322278023, acc: 0.9860334992408752)
[2025-02-13 02:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:30][root][INFO] - Training Epoch: 1/2, step 2968/7134 completed (loss: 0.15118426084518433, acc: 0.9678456783294678)
[2025-02-13 02:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:30][root][INFO] - Training Epoch: 1/2, step 2969/7134 completed (loss: 0.2811494767665863, acc: 0.942307710647583)
[2025-02-13 02:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:31][root][INFO] - Training Epoch: 1/2, step 2970/7134 completed (loss: 0.3564087450504303, acc: 0.9281045794487)
[2025-02-13 02:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:31][root][INFO] - Training Epoch: 1/2, step 2971/7134 completed (loss: 0.07270176708698273, acc: 0.980289101600647)
[2025-02-13 02:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32][root][INFO] - Training Epoch: 1/2, step 2972/7134 completed (loss: 0.05581648647785187, acc: 0.9792284965515137)
[2025-02-13 02:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32][root][INFO] - Training Epoch: 1/2, step 2973/7134 completed (loss: 0.09570300579071045, acc: 0.9721518754959106)
[2025-02-13 02:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32][root][INFO] - Training Epoch: 1/2, step 2974/7134 completed (loss: 0.07176979631185532, acc: 0.9805447459220886)
[2025-02-13 02:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:33][root][INFO] - Training Epoch: 1/2, step 2975/7134 completed (loss: 0.0704331248998642, acc: 0.9820442199707031)
[2025-02-13 02:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:33][root][INFO] - Training Epoch: 1/2, step 2976/7134 completed (loss: 0.059305787086486816, acc: 0.98617023229599)
[2025-02-13 02:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:34][root][INFO] - Training Epoch: 1/2, step 2977/7134 completed (loss: 0.06428039073944092, acc: 0.9855491518974304)
[2025-02-13 02:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:34][root][INFO] - Training Epoch: 1/2, step 2978/7134 completed (loss: 0.05429598689079285, acc: 0.9900709390640259)
[2025-02-13 02:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35][root][INFO] - Training Epoch: 1/2, step 2979/7134 completed (loss: 0.028803406283259392, acc: 0.9922118186950684)
[2025-02-13 02:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35][root][INFO] - Training Epoch: 1/2, step 2980/7134 completed (loss: 0.07027410715818405, acc: 0.9821656346321106)
[2025-02-13 02:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35][root][INFO] - Training Epoch: 1/2, step 2981/7134 completed (loss: 0.09570157527923584, acc: 0.9823608994483948)
[2025-02-13 02:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:36][root][INFO] - Training Epoch: 1/2, step 2982/7134 completed (loss: 0.07263875752687454, acc: 0.9790576100349426)
[2025-02-13 02:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:36][root][INFO] - Training Epoch: 1/2, step 2983/7134 completed (loss: 0.11969330161809921, acc: 0.9775132536888123)
[2025-02-13 02:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:37][root][INFO] - Training Epoch: 1/2, step 2984/7134 completed (loss: 0.06044716015458107, acc: 0.9823899269104004)
[2025-02-13 02:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:37][root][INFO] - Training Epoch: 1/2, step 2985/7134 completed (loss: 0.06979063898324966, acc: 0.9809296727180481)
[2025-02-13 02:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:38][root][INFO] - Training Epoch: 1/2, step 2986/7134 completed (loss: 0.040145620703697205, acc: 0.9881796836853027)
[2025-02-13 02:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:38][root][INFO] - Training Epoch: 1/2, step 2987/7134 completed (loss: 0.04484378546476364, acc: 0.9858657121658325)
[2025-02-13 02:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39][root][INFO] - Training Epoch: 1/2, step 2988/7134 completed (loss: 0.06011766940355301, acc: 0.985401451587677)
[2025-02-13 02:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39][root][INFO] - Training Epoch: 1/2, step 2989/7134 completed (loss: 0.05253003165125847, acc: 0.9869281053543091)
[2025-02-13 02:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39][root][INFO] - Training Epoch: 1/2, step 2990/7134 completed (loss: 0.18703609704971313, acc: 0.9597902297973633)
[2025-02-13 02:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:40][root][INFO] - Training Epoch: 1/2, step 2991/7134 completed (loss: 0.2035103142261505, acc: 0.9514563083648682)
[2025-02-13 02:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:40][root][INFO] - Training Epoch: 1/2, step 2992/7134 completed (loss: 0.1047588512301445, acc: 0.9803921580314636)
[2025-02-13 02:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41][root][INFO] - Training Epoch: 1/2, step 2993/7134 completed (loss: 0.05330916866660118, acc: 0.9889975786209106)
[2025-02-13 02:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41][root][INFO] - Training Epoch: 1/2, step 2994/7134 completed (loss: 0.05438336357474327, acc: 0.9892086386680603)
[2025-02-13 02:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41][root][INFO] - Training Epoch: 1/2, step 2995/7134 completed (loss: 0.042256470769643784, acc: 0.989830493927002)
[2025-02-13 02:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:42][root][INFO] - Training Epoch: 1/2, step 2996/7134 completed (loss: 0.06799330562353134, acc: 0.9789473414421082)
[2025-02-13 02:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:42][root][INFO] - Training Epoch: 1/2, step 2997/7134 completed (loss: 0.03776940330862999, acc: 0.9906445145606995)
[2025-02-13 02:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:43][root][INFO] - Training Epoch: 1/2, step 2998/7134 completed (loss: 0.0554412305355072, acc: 0.9854545593261719)
[2025-02-13 02:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:43][root][INFO] - Training Epoch: 1/2, step 2999/7134 completed (loss: 0.033445365726947784, acc: 0.9927641153335571)
[2025-02-13 02:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:44][root][INFO] - Training Epoch: 1/2, step 3000/7134 completed (loss: 0.038050275295972824, acc: 0.9903640151023865)
[2025-02-13 02:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:44][root][INFO] - Training Epoch: 1/2, step 3001/7134 completed (loss: 0.04814504459500313, acc: 0.9834123253822327)
[2025-02-13 02:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:45][root][INFO] - Training Epoch: 1/2, step 3002/7134 completed (loss: 0.057722557336091995, acc: 0.9829931855201721)
[2025-02-13 02:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:45][root][INFO] - Training Epoch: 1/2, step 3003/7134 completed (loss: 0.039169903844594955, acc: 0.9872390031814575)
[2025-02-13 02:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46][root][INFO] - Training Epoch: 1/2, step 3004/7134 completed (loss: 0.028202170506119728, acc: 0.9910614490509033)
[2025-02-13 02:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46][root][INFO] - Training Epoch: 1/2, step 3005/7134 completed (loss: 0.024139368906617165, acc: 0.9920182228088379)
[2025-02-13 02:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46][root][INFO] - Training Epoch: 1/2, step 3006/7134 completed (loss: 0.029122784733772278, acc: 0.992337167263031)
[2025-02-13 02:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:47][root][INFO] - Training Epoch: 1/2, step 3007/7134 completed (loss: 0.06275517493486404, acc: 0.9824355840682983)
[2025-02-13 02:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:47][root][INFO] - Training Epoch: 1/2, step 3008/7134 completed (loss: 0.09546219557523727, acc: 0.9748252034187317)
[2025-02-13 02:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:48][root][INFO] - Training Epoch: 1/2, step 3009/7134 completed (loss: 0.12039288878440857, acc: 0.9721518754959106)
[2025-02-13 02:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:48][root][INFO] - Training Epoch: 1/2, step 3010/7134 completed (loss: 0.07595647126436234, acc: 0.9790055155754089)
[2025-02-13 02:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49][root][INFO] - Training Epoch: 1/2, step 3011/7134 completed (loss: 0.11900563538074493, acc: 0.9671897292137146)
[2025-02-13 02:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49][root][INFO] - Training Epoch: 1/2, step 3012/7134 completed (loss: 0.09961362928152084, acc: 0.9714738726615906)
[2025-02-13 02:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49][root][INFO] - Training Epoch: 1/2, step 3013/7134 completed (loss: 0.035179730504751205, acc: 0.985023021697998)
[2025-02-13 02:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:50][root][INFO] - Training Epoch: 1/2, step 3014/7134 completed (loss: 0.0530804842710495, acc: 0.9835873246192932)
[2025-02-13 02:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:50][root][INFO] - Training Epoch: 1/2, step 3015/7134 completed (loss: 0.04192691296339035, acc: 0.9917355179786682)
[2025-02-13 02:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51][root][INFO] - Training Epoch: 1/2, step 3016/7134 completed (loss: 0.038590531796216965, acc: 0.9887482523918152)
[2025-02-13 02:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51][root][INFO] - Training Epoch: 1/2, step 3017/7134 completed (loss: 0.13512907922267914, acc: 0.9559321999549866)
[2025-02-13 02:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51][root][INFO] - Training Epoch: 1/2, step 3018/7134 completed (loss: 0.0627746507525444, acc: 0.9867768883705139)
[2025-02-13 02:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:52][root][INFO] - Training Epoch: 1/2, step 3019/7134 completed (loss: 0.05324168503284454, acc: 0.9852941036224365)
[2025-02-13 02:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:52][root][INFO] - Training Epoch: 1/2, step 3020/7134 completed (loss: 0.08618726581335068, acc: 0.9786096215248108)
[2025-02-13 02:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53][root][INFO] - Training Epoch: 1/2, step 3021/7134 completed (loss: 0.02816953882575035, acc: 0.9927272796630859)
[2025-02-13 02:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53][root][INFO] - Training Epoch: 1/2, step 3022/7134 completed (loss: 0.06470485776662827, acc: 0.9871382713317871)
[2025-02-13 02:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53][root][INFO] - Training Epoch: 1/2, step 3023/7134 completed (loss: 0.07327862083911896, acc: 0.9761431217193604)
[2025-02-13 02:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:54][root][INFO] - Training Epoch: 1/2, step 3024/7134 completed (loss: 0.03462626412510872, acc: 0.992337167263031)
[2025-02-13 02:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:54][root][INFO] - Training Epoch: 1/2, step 3025/7134 completed (loss: 0.09821932762861252, acc: 0.9793103337287903)
[2025-02-13 02:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55][root][INFO] - Training Epoch: 1/2, step 3026/7134 completed (loss: 0.041743699461221695, acc: 0.9910233616828918)
[2025-02-13 02:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55][root][INFO] - Training Epoch: 1/2, step 3027/7134 completed (loss: 0.0910908505320549, acc: 0.9858585596084595)
[2025-02-13 02:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55][root][INFO] - Training Epoch: 1/2, step 3028/7134 completed (loss: 0.04095238819718361, acc: 0.9931972622871399)
[2025-02-13 02:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:56][root][INFO] - Training Epoch: 1/2, step 3029/7134 completed (loss: 0.054521139711141586, acc: 0.989180862903595)
[2025-02-13 02:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:56][root][INFO] - Training Epoch: 1/2, step 3030/7134 completed (loss: 0.05380451679229736, acc: 0.9862595200538635)
[2025-02-13 02:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57][root][INFO] - Training Epoch: 1/2, step 3031/7134 completed (loss: 0.02336866408586502, acc: 0.9894067645072937)
[2025-02-13 02:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57][root][INFO] - Training Epoch: 1/2, step 3032/7134 completed (loss: 0.06145688518881798, acc: 0.9872958064079285)
[2025-02-13 02:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57][root][INFO] - Training Epoch: 1/2, step 3033/7134 completed (loss: 0.051761649549007416, acc: 0.9841269850730896)
[2025-02-13 02:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:58][root][INFO] - Training Epoch: 1/2, step 3034/7134 completed (loss: 0.06073285639286041, acc: 0.9851064085960388)
[2025-02-13 02:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:58][root][INFO] - Training Epoch: 1/2, step 3035/7134 completed (loss: 0.03335979953408241, acc: 0.9937008023262024)
[2025-02-13 02:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:59][root][INFO] - Training Epoch: 1/2, step 3036/7134 completed (loss: 0.07599400728940964, acc: 0.9799330830574036)
[2025-02-13 02:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:59][root][INFO] - Training Epoch: 1/2, step 3037/7134 completed (loss: 0.061306439340114594, acc: 0.9799692034721375)
[2025-02-13 02:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00][root][INFO] - Training Epoch: 1/2, step 3038/7134 completed (loss: 0.02593580260872841, acc: 0.9930434823036194)
[2025-02-13 02:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00][root][INFO] - Training Epoch: 1/2, step 3039/7134 completed (loss: 0.05494775250554085, acc: 0.9892802238464355)
[2025-02-13 02:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00][root][INFO] - Training Epoch: 1/2, step 3040/7134 completed (loss: 0.0739886611700058, acc: 0.9777365326881409)
[2025-02-13 02:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:01][root][INFO] - Training Epoch: 1/2, step 3041/7134 completed (loss: 0.08901496231555939, acc: 0.979266345500946)
[2025-02-13 02:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:01][root][INFO] - Training Epoch: 1/2, step 3042/7134 completed (loss: 0.04511919245123863, acc: 0.9867060780525208)
[2025-02-13 02:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02][root][INFO] - Training Epoch: 1/2, step 3043/7134 completed (loss: 0.052934430539608, acc: 0.980988621711731)
[2025-02-13 02:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02][root][INFO] - Training Epoch: 1/2, step 3044/7134 completed (loss: 0.031367331743240356, acc: 0.9913644194602966)
[2025-02-13 02:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02][root][INFO] - Training Epoch: 1/2, step 3045/7134 completed (loss: 0.021597977727651596, acc: 0.994915246963501)
[2025-02-13 02:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:03][root][INFO] - Training Epoch: 1/2, step 3046/7134 completed (loss: 0.0480547770857811, acc: 0.9877049326896667)
[2025-02-13 02:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:03][root][INFO] - Training Epoch: 1/2, step 3047/7134 completed (loss: 0.02867068722844124, acc: 0.9886792302131653)
[2025-02-13 02:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04][root][INFO] - Training Epoch: 1/2, step 3048/7134 completed (loss: 0.017883403226733208, acc: 0.996221661567688)
[2025-02-13 02:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04][root][INFO] - Training Epoch: 1/2, step 3049/7134 completed (loss: 0.03785666078329086, acc: 0.9883720874786377)
[2025-02-13 02:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04][root][INFO] - Training Epoch: 1/2, step 3050/7134 completed (loss: 0.050886377692222595, acc: 0.980997622013092)
[2025-02-13 02:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:05][root][INFO] - Training Epoch: 1/2, step 3051/7134 completed (loss: 0.06916862726211548, acc: 0.9789473414421082)
[2025-02-13 02:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:05][root][INFO] - Training Epoch: 1/2, step 3052/7134 completed (loss: 0.04655073583126068, acc: 0.9896907210350037)
[2025-02-13 02:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:06][root][INFO] - Training Epoch: 1/2, step 3053/7134 completed (loss: 0.042100805789232254, acc: 0.989130437374115)
[2025-02-13 02:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:06][root][INFO] - Training Epoch: 1/2, step 3054/7134 completed (loss: 0.06445170193910599, acc: 0.9818181991577148)
[2025-02-13 02:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07][root][INFO] - Training Epoch: 1/2, step 3055/7134 completed (loss: 0.04496138542890549, acc: 0.988034188747406)
[2025-02-13 02:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07][root][INFO] - Training Epoch: 1/2, step 3056/7134 completed (loss: 0.03171231970191002, acc: 0.9922879338264465)
[2025-02-13 02:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07][root][INFO] - Training Epoch: 1/2, step 3057/7134 completed (loss: 0.028119632974267006, acc: 0.9916782379150391)
[2025-02-13 02:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:08][root][INFO] - Training Epoch: 1/2, step 3058/7134 completed (loss: 0.10188226401805878, acc: 0.977979302406311)
[2025-02-13 02:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:08][root][INFO] - Training Epoch: 1/2, step 3059/7134 completed (loss: 0.04368109628558159, acc: 0.9876712560653687)
[2025-02-13 02:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:09][root][INFO] - Training Epoch: 1/2, step 3060/7134 completed (loss: 0.04128577560186386, acc: 0.9868735074996948)
[2025-02-13 02:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:09][root][INFO] - Training Epoch: 1/2, step 3061/7134 completed (loss: 0.03480317443609238, acc: 0.9939393997192383)
[2025-02-13 02:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10][root][INFO] - Training Epoch: 1/2, step 3062/7134 completed (loss: 0.01937512867152691, acc: 0.9946091771125793)
[2025-02-13 02:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10][root][INFO] - Training Epoch: 1/2, step 3063/7134 completed (loss: 0.04318973794579506, acc: 0.9858793616294861)
[2025-02-13 02:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10][root][INFO] - Training Epoch: 1/2, step 3064/7134 completed (loss: 0.02188006602227688, acc: 0.9957746267318726)
[2025-02-13 02:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:11][root][INFO] - Training Epoch: 1/2, step 3065/7134 completed (loss: 0.036675285547971725, acc: 0.9919354915618896)
[2025-02-13 02:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:11][root][INFO] - Training Epoch: 1/2, step 3066/7134 completed (loss: 0.025771034881472588, acc: 0.9935483932495117)
[2025-02-13 02:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:12][root][INFO] - Training Epoch: 1/2, step 3067/7134 completed (loss: 0.04065828025341034, acc: 0.9854545593261719)
[2025-02-13 02:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:12][root][INFO] - Training Epoch: 1/2, step 3068/7134 completed (loss: 0.05378852039575577, acc: 0.9864681959152222)
[2025-02-13 02:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13][root][INFO] - Training Epoch: 1/2, step 3069/7134 completed (loss: 0.031418245285749435, acc: 0.9873272180557251)
[2025-02-13 02:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13][root][INFO] - Training Epoch: 1/2, step 3070/7134 completed (loss: 0.022185813635587692, acc: 0.9925768971443176)
[2025-02-13 02:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13][root][INFO] - Training Epoch: 1/2, step 3071/7134 completed (loss: 0.036526910960674286, acc: 0.9897040128707886)
[2025-02-13 02:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:14][root][INFO] - Training Epoch: 1/2, step 3072/7134 completed (loss: 0.03289273381233215, acc: 0.9894982576370239)
[2025-02-13 02:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:14][root][INFO] - Training Epoch: 1/2, step 3073/7134 completed (loss: 0.02142731472849846, acc: 0.9923664331436157)
[2025-02-13 02:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:15][root][INFO] - Training Epoch: 1/2, step 3074/7134 completed (loss: 0.03279414772987366, acc: 0.9933444261550903)
[2025-02-13 02:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:15][root][INFO] - Training Epoch: 1/2, step 3075/7134 completed (loss: 0.02902207151055336, acc: 0.9916666746139526)
[2025-02-13 02:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:16][root][INFO] - Training Epoch: 1/2, step 3076/7134 completed (loss: 0.054079052060842514, acc: 0.9866814613342285)
[2025-02-13 02:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:16][root][INFO] - Training Epoch: 1/2, step 3077/7134 completed (loss: 0.09384164214134216, acc: 0.9748634099960327)
[2025-02-13 02:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17][root][INFO] - Training Epoch: 1/2, step 3078/7134 completed (loss: 0.08276176452636719, acc: 0.9748634099960327)
[2025-02-13 02:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17][root][INFO] - Training Epoch: 1/2, step 3079/7134 completed (loss: 0.03452474623918533, acc: 0.9913580417633057)
[2025-02-13 02:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17][root][INFO] - Training Epoch: 1/2, step 3080/7134 completed (loss: 0.07171569019556046, acc: 0.9749181866645813)
[2025-02-13 02:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18][root][INFO] - Training Epoch: 1/2, step 3081/7134 completed (loss: 0.03128276765346527, acc: 0.9870689511299133)
[2025-02-13 02:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18][root][INFO] - Training Epoch: 1/2, step 3082/7134 completed (loss: 0.10302101075649261, acc: 0.973525881767273)
[2025-02-13 02:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19][root][INFO] - Training Epoch: 1/2, step 3083/7134 completed (loss: 0.0870368629693985, acc: 0.9751309156417847)
[2025-02-13 02:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19][root][INFO] - Training Epoch: 1/2, step 3084/7134 completed (loss: 0.09170883148908615, acc: 0.976190447807312)
[2025-02-13 02:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19][root][INFO] - Training Epoch: 1/2, step 3085/7134 completed (loss: 0.08833938837051392, acc: 0.970802903175354)
[2025-02-13 02:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:20][root][INFO] - Training Epoch: 1/2, step 3086/7134 completed (loss: 0.087478868663311, acc: 0.9753086566925049)
[2025-02-13 02:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:20][root][INFO] - Training Epoch: 1/2, step 3087/7134 completed (loss: 0.06779951602220535, acc: 0.9799138903617859)
[2025-02-13 02:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:21][root][INFO] - Training Epoch: 1/2, step 3088/7134 completed (loss: 0.05370344594120979, acc: 0.9830890893936157)
[2025-02-13 02:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:21][root][INFO] - Training Epoch: 1/2, step 3089/7134 completed (loss: 0.0514712929725647, acc: 0.9859762787818909)
[2025-02-13 02:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:22][root][INFO] - Training Epoch: 1/2, step 3090/7134 completed (loss: 0.10303656756877899, acc: 0.9741848111152649)
[2025-02-13 02:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:22][root][INFO] - Training Epoch: 1/2, step 3091/7134 completed (loss: 0.06373463571071625, acc: 0.9781106114387512)
[2025-02-13 02:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23][root][INFO] - Training Epoch: 1/2, step 3092/7134 completed (loss: 0.07235445827245712, acc: 0.9852125644683838)
[2025-02-13 02:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23][root][INFO] - Training Epoch: 1/2, step 3093/7134 completed (loss: 0.06951071321964264, acc: 0.9801324605941772)
[2025-02-13 02:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24][root][INFO] - Training Epoch: 1/2, step 3094/7134 completed (loss: 0.028369206935167313, acc: 0.9904631972312927)
[2025-02-13 02:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24][root][INFO] - Training Epoch: 1/2, step 3095/7134 completed (loss: 0.07568120211362839, acc: 0.980555534362793)
[2025-02-13 02:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24][root][INFO] - Training Epoch: 1/2, step 3096/7134 completed (loss: 0.09387365728616714, acc: 0.9756097793579102)
[2025-02-13 02:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:25][root][INFO] - Training Epoch: 1/2, step 3097/7134 completed (loss: 0.0725945457816124, acc: 0.9786324501037598)
[2025-02-13 02:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:25][root][INFO] - Training Epoch: 1/2, step 3098/7134 completed (loss: 0.045020755380392075, acc: 0.9889937043190002)
[2025-02-13 02:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:26][root][INFO] - Training Epoch: 1/2, step 3099/7134 completed (loss: 0.05375480279326439, acc: 0.9854133129119873)
[2025-02-13 02:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:26][root][INFO] - Training Epoch: 1/2, step 3100/7134 completed (loss: 0.046439364552497864, acc: 0.9882978796958923)
[2025-02-13 02:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:27][root][INFO] - Training Epoch: 1/2, step 3101/7134 completed (loss: 0.051253776997327805, acc: 0.9817517995834351)
[2025-02-13 02:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:27][root][INFO] - Training Epoch: 1/2, step 3102/7134 completed (loss: 0.05450621247291565, acc: 0.9822221994400024)
[2025-02-13 02:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28][root][INFO] - Training Epoch: 1/2, step 3103/7134 completed (loss: 0.06490189582109451, acc: 0.98296058177948)
[2025-02-13 02:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28][root][INFO] - Training Epoch: 1/2, step 3104/7134 completed (loss: 0.031132129952311516, acc: 0.9923664331436157)
[2025-02-13 02:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28][root][INFO] - Training Epoch: 1/2, step 3105/7134 completed (loss: 0.0681525245308876, acc: 0.9835164546966553)
[2025-02-13 02:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:29][root][INFO] - Training Epoch: 1/2, step 3106/7134 completed (loss: 0.040954332798719406, acc: 0.9849498271942139)
[2025-02-13 02:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:29][root][INFO] - Training Epoch: 1/2, step 3107/7134 completed (loss: 0.02859492041170597, acc: 0.9882352948188782)
[2025-02-13 02:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30][root][INFO] - Training Epoch: 1/2, step 3108/7134 completed (loss: 0.08154552429914474, acc: 0.9830769300460815)
[2025-02-13 02:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30][root][INFO] - Training Epoch: 1/2, step 3109/7134 completed (loss: 0.0512651726603508, acc: 0.9815497994422913)
[2025-02-13 02:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30][root][INFO] - Training Epoch: 1/2, step 3110/7134 completed (loss: 0.039924319833517075, acc: 0.9853658676147461)
[2025-02-13 02:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:31][root][INFO] - Training Epoch: 1/2, step 3111/7134 completed (loss: 0.1543227732181549, acc: 0.9639175534248352)
[2025-02-13 02:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:31][root][INFO] - Training Epoch: 1/2, step 3112/7134 completed (loss: 0.05277210846543312, acc: 0.9877192974090576)
[2025-02-13 02:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:32][root][INFO] - Training Epoch: 1/2, step 3113/7134 completed (loss: 0.04318845272064209, acc: 0.9887640476226807)
[2025-02-13 02:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:32][root][INFO] - Training Epoch: 1/2, step 3114/7134 completed (loss: 0.0607185885310173, acc: 0.9844827651977539)
[2025-02-13 02:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33][root][INFO] - Training Epoch: 1/2, step 3115/7134 completed (loss: 0.07708294689655304, acc: 0.9876543283462524)
[2025-02-13 02:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33][root][INFO] - Training Epoch: 1/2, step 3116/7134 completed (loss: 0.047913119196891785, acc: 0.9871794581413269)
[2025-02-13 02:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33][root][INFO] - Training Epoch: 1/2, step 3117/7134 completed (loss: 0.10476875305175781, acc: 0.979619562625885)
[2025-02-13 02:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:34][root][INFO] - Training Epoch: 1/2, step 3118/7134 completed (loss: 0.06274521350860596, acc: 0.9841897487640381)
[2025-02-13 02:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:34][root][INFO] - Training Epoch: 1/2, step 3119/7134 completed (loss: 0.0795479491353035, acc: 0.9779506921768188)
[2025-02-13 02:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:35][root][INFO] - Training Epoch: 1/2, step 3120/7134 completed (loss: 0.07233750820159912, acc: 0.9873060584068298)
[2025-02-13 02:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:35][root][INFO] - Training Epoch: 1/2, step 3121/7134 completed (loss: 0.05713042616844177, acc: 0.980719804763794)
[2025-02-13 02:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36][root][INFO] - Training Epoch: 1/2, step 3122/7134 completed (loss: 0.050454333424568176, acc: 0.9883720874786377)
[2025-02-13 02:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36][root][INFO] - Training Epoch: 1/2, step 3123/7134 completed (loss: 0.07226444035768509, acc: 0.9788557291030884)
[2025-02-13 02:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36][root][INFO] - Training Epoch: 1/2, step 3124/7134 completed (loss: 0.06179297715425491, acc: 0.9888613820075989)
[2025-02-13 02:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:37][root][INFO] - Training Epoch: 1/2, step 3125/7134 completed (loss: 0.02559502050280571, acc: 0.9924356937408447)
[2025-02-13 02:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:37][root][INFO] - Training Epoch: 1/2, step 3126/7134 completed (loss: 0.06083093583583832, acc: 0.9809402823448181)
[2025-02-13 02:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:38][root][INFO] - Training Epoch: 1/2, step 3127/7134 completed (loss: 0.04316425323486328, acc: 0.9872093200683594)
[2025-02-13 02:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:38][root][INFO] - Training Epoch: 1/2, step 3128/7134 completed (loss: 0.03623034432530403, acc: 0.9909677505493164)
[2025-02-13 02:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39][root][INFO] - Training Epoch: 1/2, step 3129/7134 completed (loss: 0.03336842730641365, acc: 0.9925834536552429)
[2025-02-13 02:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39][root][INFO] - Training Epoch: 1/2, step 3130/7134 completed (loss: 0.04542739689350128, acc: 0.9885321259498596)
[2025-02-13 02:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39][root][INFO] - Training Epoch: 1/2, step 3131/7134 completed (loss: 0.06005968898534775, acc: 0.9811594486236572)
[2025-02-13 02:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:40][root][INFO] - Training Epoch: 1/2, step 3132/7134 completed (loss: 0.02675151266157627, acc: 0.9913232326507568)
[2025-02-13 02:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:40][root][INFO] - Training Epoch: 1/2, step 3133/7134 completed (loss: 0.1211988553404808, acc: 0.9752747416496277)
[2025-02-13 02:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:41][root][INFO] - Training Epoch: 1/2, step 3134/7134 completed (loss: 0.046137213706970215, acc: 0.9835164546966553)
[2025-02-13 02:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:41][root][INFO] - Training Epoch: 1/2, step 3135/7134 completed (loss: 0.09993899613618851, acc: 0.9699769020080566)
[2025-02-13 02:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42][root][INFO] - Training Epoch: 1/2, step 3136/7134 completed (loss: 0.04342573881149292, acc: 0.9878261089324951)
[2025-02-13 02:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42][root][INFO] - Training Epoch: 1/2, step 3137/7134 completed (loss: 0.1007070541381836, acc: 0.9642857313156128)
[2025-02-13 02:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42][root][INFO] - Training Epoch: 1/2, step 3138/7134 completed (loss: 0.0756877064704895, acc: 0.9852941036224365)
[2025-02-13 02:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43][root][INFO] - Training Epoch: 1/2, step 3139/7134 completed (loss: 0.13649190962314606, acc: 0.9613733887672424)
[2025-02-13 02:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43][root][INFO] - Training Epoch: 1/2, step 3140/7134 completed (loss: 0.04597003012895584, acc: 0.9844357967376709)
[2025-02-13 02:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43][root][INFO] - Training Epoch: 1/2, step 3141/7134 completed (loss: 0.07527323067188263, acc: 0.9772727489471436)
[2025-02-13 02:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:44][root][INFO] - Training Epoch: 1/2, step 3142/7134 completed (loss: 0.035519443452358246, acc: 0.9910314083099365)
[2025-02-13 02:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:44][root][INFO] - Training Epoch: 1/2, step 3143/7134 completed (loss: 0.039411917328834534, acc: 0.9887459874153137)
[2025-02-13 02:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:45][root][INFO] - Training Epoch: 1/2, step 3144/7134 completed (loss: 0.03195885568857193, acc: 0.9921383857727051)
[2025-02-13 02:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:45][root][INFO] - Training Epoch: 1/2, step 3145/7134 completed (loss: 0.07424086332321167, acc: 0.9824945330619812)
[2025-02-13 02:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46][root][INFO] - Training Epoch: 1/2, step 3146/7134 completed (loss: 0.059984151273965836, acc: 0.9830729365348816)
[2025-02-13 02:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46][root][INFO] - Training Epoch: 1/2, step 3147/7134 completed (loss: 0.05167340487241745, acc: 0.9855538010597229)
[2025-02-13 02:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46][root][INFO] - Training Epoch: 1/2, step 3148/7134 completed (loss: 0.07818514108657837, acc: 0.9770408272743225)
[2025-02-13 02:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:47][root][INFO] - Training Epoch: 1/2, step 3149/7134 completed (loss: 0.03047315590083599, acc: 0.9912152290344238)
[2025-02-13 02:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:47][root][INFO] - Training Epoch: 1/2, step 3150/7134 completed (loss: 0.22289669513702393, acc: 0.9388560056686401)
[2025-02-13 02:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48][root][INFO] - Training Epoch: 1/2, step 3151/7134 completed (loss: 0.14600029587745667, acc: 0.961685836315155)
[2025-02-13 02:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48][root][INFO] - Training Epoch: 1/2, step 3152/7134 completed (loss: 0.0812898799777031, acc: 0.9742424488067627)
[2025-02-13 02:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48][root][INFO] - Training Epoch: 1/2, step 3153/7134 completed (loss: 0.0633108839392662, acc: 0.9824561476707458)
[2025-02-13 02:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:49][root][INFO] - Training Epoch: 1/2, step 3154/7134 completed (loss: 0.0530182346701622, acc: 0.980461835861206)
[2025-02-13 02:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:49][root][INFO] - Training Epoch: 1/2, step 3155/7134 completed (loss: 0.11372166126966476, acc: 0.9756592512130737)
[2025-02-13 02:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50][root][INFO] - Training Epoch: 1/2, step 3156/7134 completed (loss: 0.1105366051197052, acc: 0.9732540845870972)
[2025-02-13 02:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50][root][INFO] - Training Epoch: 1/2, step 3157/7134 completed (loss: 0.032135430723428726, acc: 0.9913978576660156)
[2025-02-13 02:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50][root][INFO] - Training Epoch: 1/2, step 3158/7134 completed (loss: 0.060874782502651215, acc: 0.9852941036224365)
[2025-02-13 02:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:51][root][INFO] - Training Epoch: 1/2, step 3159/7134 completed (loss: 0.0769151896238327, acc: 0.9779411554336548)
[2025-02-13 02:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:51][root][INFO] - Training Epoch: 1/2, step 3160/7134 completed (loss: 0.026213325560092926, acc: 0.9930843710899353)
[2025-02-13 02:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52][root][INFO] - Training Epoch: 1/2, step 3161/7134 completed (loss: 0.050277046859264374, acc: 0.9845938086509705)
[2025-02-13 02:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52][root][INFO] - Training Epoch: 1/2, step 3162/7134 completed (loss: 0.02995612844824791, acc: 0.9883381724357605)
[2025-02-13 02:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52][root][INFO] - Training Epoch: 1/2, step 3163/7134 completed (loss: 0.07875184714794159, acc: 0.9816360473632812)
[2025-02-13 02:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:53][root][INFO] - Training Epoch: 1/2, step 3164/7134 completed (loss: 0.09521260112524033, acc: 0.9748743772506714)
[2025-02-13 02:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:53][root][INFO] - Training Epoch: 1/2, step 3165/7134 completed (loss: 0.044477056711912155, acc: 0.9866443872451782)
[2025-02-13 02:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:54][root][INFO] - Training Epoch: 1/2, step 3166/7134 completed (loss: 0.1143251359462738, acc: 0.9684210419654846)
[2025-02-13 02:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:54][root][INFO] - Training Epoch: 1/2, step 3167/7134 completed (loss: 0.13320443034172058, acc: 0.9593750238418579)
[2025-02-13 02:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55][root][INFO] - Training Epoch: 1/2, step 3168/7134 completed (loss: 0.1272735595703125, acc: 0.9710366129875183)
[2025-02-13 02:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55][root][INFO] - Training Epoch: 1/2, step 3169/7134 completed (loss: 0.09118787944316864, acc: 0.9774590134620667)
[2025-02-13 02:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55][root][INFO] - Training Epoch: 1/2, step 3170/7134 completed (loss: 0.04107668995857239, acc: 0.9818181991577148)
[2025-02-13 02:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:56][root][INFO] - Training Epoch: 1/2, step 3171/7134 completed (loss: 0.08499694615602493, acc: 0.9829620122909546)
[2025-02-13 02:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:56][root][INFO] - Training Epoch: 1/2, step 3172/7134 completed (loss: 0.03219272196292877, acc: 0.984674334526062)
[2025-02-13 02:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57][root][INFO] - Training Epoch: 1/2, step 3173/7134 completed (loss: 0.04910384491086006, acc: 0.9846938848495483)
[2025-02-13 02:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57][root][INFO] - Training Epoch: 1/2, step 3174/7134 completed (loss: 0.09300647675991058, acc: 0.9689348936080933)
[2025-02-13 02:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57][root][INFO] - Training Epoch: 1/2, step 3175/7134 completed (loss: 0.06643017381429672, acc: 0.9845070242881775)
[2025-02-13 02:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:58][root][INFO] - Training Epoch: 1/2, step 3176/7134 completed (loss: 0.09544873982667923, acc: 0.9729323387145996)
[2025-02-13 02:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:58][root][INFO] - Training Epoch: 1/2, step 3177/7134 completed (loss: 0.08697804063558578, acc: 0.9809027910232544)
[2025-02-13 02:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59][root][INFO] - Training Epoch: 1/2, step 3178/7134 completed (loss: 0.03915329650044441, acc: 0.9901477694511414)
[2025-02-13 02:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59][root][INFO] - Training Epoch: 1/2, step 3179/7134 completed (loss: 0.10114047676324844, acc: 0.9734659790992737)
[2025-02-13 02:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59][root][INFO] - Training Epoch: 1/2, step 3180/7134 completed (loss: 0.05227513983845711, acc: 0.9857142567634583)
[2025-02-13 02:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:00][root][INFO] - Training Epoch: 1/2, step 3181/7134 completed (loss: 0.07872821390628815, acc: 0.9781591296195984)
[2025-02-13 02:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:00][root][INFO] - Training Epoch: 1/2, step 3182/7134 completed (loss: 0.03972196206450462, acc: 0.992548406124115)
[2025-02-13 02:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01][root][INFO] - Training Epoch: 1/2, step 3183/7134 completed (loss: 0.06620568037033081, acc: 0.9830795526504517)
[2025-02-13 02:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01][root][INFO] - Training Epoch: 1/2, step 3184/7134 completed (loss: 0.07052048295736313, acc: 0.9836309552192688)
[2025-02-13 02:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01][root][INFO] - Training Epoch: 1/2, step 3185/7134 completed (loss: 0.014355923980474472, acc: 0.9971510171890259)
[2025-02-13 02:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02][root][INFO] - Training Epoch: 1/2, step 3186/7134 completed (loss: 0.047300953418016434, acc: 0.9837037324905396)
[2025-02-13 02:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02][root][INFO] - Training Epoch: 1/2, step 3187/7134 completed (loss: 0.054049715399742126, acc: 0.9857594966888428)
[2025-02-13 02:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03][root][INFO] - Training Epoch: 1/2, step 3188/7134 completed (loss: 0.045993197709321976, acc: 0.9870634078979492)
[2025-02-13 02:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03][root][INFO] - Training Epoch: 1/2, step 3189/7134 completed (loss: 0.07970146834850311, acc: 0.9797394871711731)
[2025-02-13 02:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03][root][INFO] - Training Epoch: 1/2, step 3190/7134 completed (loss: 0.058382101356983185, acc: 0.9860759377479553)
[2025-02-13 02:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:04][root][INFO] - Training Epoch: 1/2, step 3191/7134 completed (loss: 0.05502113327383995, acc: 0.9857142567634583)
[2025-02-13 02:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:04][root][INFO] - Training Epoch: 1/2, step 3192/7134 completed (loss: 0.06013472005724907, acc: 0.9810963869094849)
[2025-02-13 02:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05][root][INFO] - Training Epoch: 1/2, step 3193/7134 completed (loss: 0.11378617584705353, acc: 0.9677891731262207)
[2025-02-13 02:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05][root][INFO] - Training Epoch: 1/2, step 3194/7134 completed (loss: 0.06325767189264297, acc: 0.9803921580314636)
[2025-02-13 02:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05][root][INFO] - Training Epoch: 1/2, step 3195/7134 completed (loss: 0.07040486484766006, acc: 0.9828473329544067)
[2025-02-13 02:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:06][root][INFO] - Training Epoch: 1/2, step 3196/7134 completed (loss: 0.02542666159570217, acc: 0.9925233721733093)
[2025-02-13 02:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:06][root][INFO] - Training Epoch: 1/2, step 3197/7134 completed (loss: 0.06287124752998352, acc: 0.9887323975563049)
[2025-02-13 02:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07][root][INFO] - Training Epoch: 1/2, step 3198/7134 completed (loss: 0.05723900720477104, acc: 0.9863842725753784)
[2025-02-13 02:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07][root][INFO] - Training Epoch: 1/2, step 3199/7134 completed (loss: 0.04698461294174194, acc: 0.9867841601371765)
[2025-02-13 02:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07][root][INFO] - Training Epoch: 1/2, step 3200/7134 completed (loss: 0.0598195344209671, acc: 0.982425332069397)
[2025-02-13 02:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:08][root][INFO] - Training Epoch: 1/2, step 3201/7134 completed (loss: 0.14094063639640808, acc: 0.9674620628356934)
[2025-02-13 02:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:08][root][INFO] - Training Epoch: 1/2, step 3202/7134 completed (loss: 0.050910040736198425, acc: 0.9875518679618835)
[2025-02-13 02:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09][root][INFO] - Training Epoch: 1/2, step 3203/7134 completed (loss: 0.054433390498161316, acc: 0.9842022061347961)
[2025-02-13 02:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09][root][INFO] - Training Epoch: 1/2, step 3204/7134 completed (loss: 0.10607073456048965, acc: 0.9755638837814331)
[2025-02-13 02:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09][root][INFO] - Training Epoch: 1/2, step 3205/7134 completed (loss: 0.020489206537604332, acc: 0.9929328560829163)
[2025-02-13 02:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10][root][INFO] - Training Epoch: 1/2, step 3206/7134 completed (loss: 0.04161296784877777, acc: 0.9898697733879089)
[2025-02-13 02:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10][root][INFO] - Training Epoch: 1/2, step 3207/7134 completed (loss: 0.059916093945503235, acc: 0.9785605072975159)
[2025-02-13 02:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11][root][INFO] - Training Epoch: 1/2, step 3208/7134 completed (loss: 0.09059017896652222, acc: 0.9820261597633362)
[2025-02-13 02:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11][root][INFO] - Training Epoch: 1/2, step 3209/7134 completed (loss: 0.0466860830783844, acc: 0.988095223903656)
[2025-02-13 02:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11][root][INFO] - Training Epoch: 1/2, step 3210/7134 completed (loss: 0.020718643441796303, acc: 0.9924699068069458)
[2025-02-13 02:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:12][root][INFO] - Training Epoch: 1/2, step 3211/7134 completed (loss: 0.054773204028606415, acc: 0.9916840195655823)
[2025-02-13 02:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:12][root][INFO] - Training Epoch: 1/2, step 3212/7134 completed (loss: 0.0161698330193758, acc: 0.9944055676460266)
[2025-02-13 02:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:13][root][INFO] - Training Epoch: 1/2, step 3213/7134 completed (loss: 0.031391676515340805, acc: 0.9956958293914795)
[2025-02-13 02:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:13][root][INFO] - Training Epoch: 1/2, step 3214/7134 completed (loss: 0.03820713609457016, acc: 0.9866130948066711)
[2025-02-13 02:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14][root][INFO] - Training Epoch: 1/2, step 3215/7134 completed (loss: 0.041003238409757614, acc: 0.9915966391563416)
[2025-02-13 02:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14][root][INFO] - Training Epoch: 1/2, step 3216/7134 completed (loss: 0.024190539494156837, acc: 0.9928366541862488)
[2025-02-13 02:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14][root][INFO] - Training Epoch: 1/2, step 3217/7134 completed (loss: 0.02645748481154442, acc: 0.9896774291992188)
[2025-02-13 02:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15][root][INFO] - Training Epoch: 1/2, step 3218/7134 completed (loss: 0.07448667287826538, acc: 0.9796556830406189)
[2025-02-13 02:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15][root][INFO] - Training Epoch: 1/2, step 3219/7134 completed (loss: 0.06921742111444473, acc: 0.9778434038162231)
[2025-02-13 02:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:16][root][INFO] - Training Epoch: 1/2, step 3220/7134 completed (loss: 0.0612347237765789, acc: 0.979973316192627)
[2025-02-13 02:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:16][root][INFO] - Training Epoch: 1/2, step 3221/7134 completed (loss: 0.046287573873996735, acc: 0.9904109835624695)
[2025-02-13 02:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17][root][INFO] - Training Epoch: 1/2, step 3222/7134 completed (loss: 0.08030740916728973, acc: 0.9837296605110168)
[2025-02-13 02:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17][root][INFO] - Training Epoch: 1/2, step 3223/7134 completed (loss: 0.07660041749477386, acc: 0.9857142567634583)
[2025-02-13 02:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17][root][INFO] - Training Epoch: 1/2, step 3224/7134 completed (loss: 0.09870954602956772, acc: 0.9739508032798767)
[2025-02-13 02:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:18][root][INFO] - Training Epoch: 1/2, step 3225/7134 completed (loss: 0.05605188012123108, acc: 0.982300877571106)
[2025-02-13 02:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:18][root][INFO] - Training Epoch: 1/2, step 3226/7134 completed (loss: 0.05648680776357651, acc: 0.9748653769493103)
[2025-02-13 02:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:19][root][INFO] - Training Epoch: 1/2, step 3227/7134 completed (loss: 0.10714586824178696, acc: 0.9703296422958374)
[2025-02-13 02:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:19][root][INFO] - Training Epoch: 1/2, step 3228/7134 completed (loss: 0.04002729058265686, acc: 0.9881734848022461)
[2025-02-13 02:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20][root][INFO] - Training Epoch: 1/2, step 3229/7134 completed (loss: 0.05661078542470932, acc: 0.9804161787033081)
[2025-02-13 02:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20][root][INFO] - Training Epoch: 1/2, step 3230/7134 completed (loss: 0.07723993808031082, acc: 0.9748520851135254)
[2025-02-13 02:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20][root][INFO] - Training Epoch: 1/2, step 3231/7134 completed (loss: 0.0783492922782898, acc: 0.978723406791687)
[2025-02-13 02:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:21][root][INFO] - Training Epoch: 1/2, step 3232/7134 completed (loss: 0.03937834873795509, acc: 0.9898989796638489)
[2025-02-13 02:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:21][root][INFO] - Training Epoch: 1/2, step 3233/7134 completed (loss: 0.09272659569978714, acc: 0.9822485446929932)
[2025-02-13 02:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22][root][INFO] - Training Epoch: 1/2, step 3234/7134 completed (loss: 0.03491116687655449, acc: 0.9921011328697205)
[2025-02-13 02:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22][root][INFO] - Training Epoch: 1/2, step 3235/7134 completed (loss: 0.0372503288090229, acc: 0.9847618937492371)
[2025-02-13 02:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22][root][INFO] - Training Epoch: 1/2, step 3236/7134 completed (loss: 0.13956226408481598, acc: 0.9720176458358765)
[2025-02-13 02:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:23][root][INFO] - Training Epoch: 1/2, step 3237/7134 completed (loss: 0.034694984555244446, acc: 0.9919484853744507)
[2025-02-13 02:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:23][root][INFO] - Training Epoch: 1/2, step 3238/7134 completed (loss: 0.09068156778812408, acc: 0.9723926186561584)
[2025-02-13 02:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24][root][INFO] - Training Epoch: 1/2, step 3239/7134 completed (loss: 0.039362650364637375, acc: 0.9873816967010498)
[2025-02-13 02:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24][root][INFO] - Training Epoch: 1/2, step 3240/7134 completed (loss: 0.03778544440865517, acc: 0.9895366430282593)
[2025-02-13 02:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24][root][INFO] - Training Epoch: 1/2, step 3241/7134 completed (loss: 0.03385346755385399, acc: 0.9896142482757568)
[2025-02-13 02:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:25][root][INFO] - Training Epoch: 1/2, step 3242/7134 completed (loss: 0.03632485494017601, acc: 0.991830050945282)
[2025-02-13 02:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:25][root][INFO] - Training Epoch: 1/2, step 3243/7134 completed (loss: 0.042767804116010666, acc: 0.9850136041641235)
[2025-02-13 02:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26][root][INFO] - Training Epoch: 1/2, step 3244/7134 completed (loss: 0.030897771939635277, acc: 0.991391658782959)
[2025-02-13 02:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26][root][INFO] - Training Epoch: 1/2, step 3245/7134 completed (loss: 0.03389393910765648, acc: 0.9922680258750916)
[2025-02-13 02:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26][root][INFO] - Training Epoch: 1/2, step 3246/7134 completed (loss: 0.06252171099185944, acc: 0.9807692170143127)
[2025-02-13 02:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:27][root][INFO] - Training Epoch: 1/2, step 3247/7134 completed (loss: 0.04182249307632446, acc: 0.9837925434112549)
[2025-02-13 02:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:27][root][INFO] - Training Epoch: 1/2, step 3248/7134 completed (loss: 0.033993225544691086, acc: 0.9885057210922241)
[2025-02-13 02:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:28][root][INFO] - Training Epoch: 1/2, step 3249/7134 completed (loss: 0.03244401142001152, acc: 0.9916782379150391)
[2025-02-13 02:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:28][root][INFO] - Training Epoch: 1/2, step 3250/7134 completed (loss: 0.054100025445222855, acc: 0.9841726422309875)
[2025-02-13 02:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29][root][INFO] - Training Epoch: 1/2, step 3251/7134 completed (loss: 0.0221809484064579, acc: 0.9914407730102539)
[2025-02-13 02:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29][root][INFO] - Training Epoch: 1/2, step 3252/7134 completed (loss: 0.011093872599303722, acc: 0.9957447052001953)
[2025-02-13 02:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29][root][INFO] - Training Epoch: 1/2, step 3253/7134 completed (loss: 0.028330696746706963, acc: 0.9935794472694397)
[2025-02-13 02:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:30][root][INFO] - Training Epoch: 1/2, step 3254/7134 completed (loss: 0.019564004614949226, acc: 0.9932546615600586)
[2025-02-13 02:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:30][root][INFO] - Training Epoch: 1/2, step 3255/7134 completed (loss: 0.018665144219994545, acc: 0.989347517490387)
[2025-02-13 02:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31][root][INFO] - Training Epoch: 1/2, step 3256/7134 completed (loss: 0.02560385875403881, acc: 0.9940740466117859)
[2025-02-13 02:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31][root][INFO] - Training Epoch: 1/2, step 3257/7134 completed (loss: 0.02714863233268261, acc: 0.994452178478241)
[2025-02-13 02:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31][root][INFO] - Training Epoch: 1/2, step 3258/7134 completed (loss: 0.04616211727261543, acc: 0.9846153855323792)
[2025-02-13 02:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:32][root][INFO] - Training Epoch: 1/2, step 3259/7134 completed (loss: 0.04482176899909973, acc: 0.9911764860153198)
[2025-02-13 02:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:32][root][INFO] - Training Epoch: 1/2, step 3260/7134 completed (loss: 0.11153688281774521, acc: 0.9777448177337646)
[2025-02-13 02:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33][root][INFO] - Training Epoch: 1/2, step 3261/7134 completed (loss: 0.0782281905412674, acc: 0.9748322367668152)
[2025-02-13 02:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33][root][INFO] - Training Epoch: 1/2, step 3262/7134 completed (loss: 0.045122601091861725, acc: 0.9855282306671143)
[2025-02-13 02:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33][root][INFO] - Training Epoch: 1/2, step 3263/7134 completed (loss: 0.07424512505531311, acc: 0.9838472604751587)
[2025-02-13 02:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:34][root][INFO] - Training Epoch: 1/2, step 3264/7134 completed (loss: 0.03258388489484787, acc: 0.9888888597488403)
[2025-02-13 02:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:34][root][INFO] - Training Epoch: 1/2, step 3265/7134 completed (loss: 0.1016487404704094, acc: 0.9786324501037598)
[2025-02-13 02:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35][root][INFO] - Training Epoch: 1/2, step 3266/7134 completed (loss: 0.059480220079422, acc: 0.9849849939346313)
[2025-02-13 02:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35][root][INFO] - Training Epoch: 1/2, step 3267/7134 completed (loss: 0.0870145633816719, acc: 0.9742120504379272)
[2025-02-13 02:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35][root][INFO] - Training Epoch: 1/2, step 3268/7134 completed (loss: 0.062126971781253815, acc: 0.9861325025558472)
[2025-02-13 02:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:36][root][INFO] - Training Epoch: 1/2, step 3269/7134 completed (loss: 0.045395173132419586, acc: 0.9873737096786499)
[2025-02-13 02:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:36][root][INFO] - Training Epoch: 1/2, step 3270/7134 completed (loss: 0.021607598289847374, acc: 0.9964115023612976)
[2025-02-13 02:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37][root][INFO] - Training Epoch: 1/2, step 3271/7134 completed (loss: 0.008853198029100895, acc: 1.0)
[2025-02-13 02:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37][root][INFO] - Training Epoch: 1/2, step 3272/7134 completed (loss: 0.03427458927035332, acc: 0.9969465732574463)
[2025-02-13 02:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37][root][INFO] - Training Epoch: 1/2, step 3273/7134 completed (loss: 0.03726586326956749, acc: 0.9872286319732666)
[2025-02-13 02:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:38][root][INFO] - Training Epoch: 1/2, step 3274/7134 completed (loss: 0.06240957975387573, acc: 0.98531574010849)
[2025-02-13 02:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:38][root][INFO] - Training Epoch: 1/2, step 3275/7134 completed (loss: 0.1653832495212555, acc: 0.957602322101593)
[2025-02-13 02:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:39][root][INFO] - Training Epoch: 1/2, step 3276/7134 completed (loss: 0.08333807438611984, acc: 0.9738292098045349)
[2025-02-13 02:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:39][root][INFO] - Training Epoch: 1/2, step 3277/7134 completed (loss: 0.07151763886213303, acc: 0.9756097793579102)
[2025-02-13 02:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40][root][INFO] - Training Epoch: 1/2, step 3278/7134 completed (loss: 0.054200679063797, acc: 0.9827916026115417)
[2025-02-13 02:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40][root][INFO] - Training Epoch: 1/2, step 3279/7134 completed (loss: 0.066375732421875, acc: 0.9776397347450256)
[2025-02-13 02:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40][root][INFO] - Training Epoch: 1/2, step 3280/7134 completed (loss: 0.20332154631614685, acc: 0.9585062265396118)
[2025-02-13 02:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:41][root][INFO] - Training Epoch: 1/2, step 3281/7134 completed (loss: 0.06315647065639496, acc: 0.9855072498321533)
[2025-02-13 02:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:41][root][INFO] - Training Epoch: 1/2, step 3282/7134 completed (loss: 0.039274588227272034, acc: 0.9873417615890503)
[2025-02-13 02:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42][root][INFO] - Training Epoch: 1/2, step 3283/7134 completed (loss: 0.06313595175743103, acc: 0.9793814420700073)
[2025-02-13 02:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42][root][INFO] - Training Epoch: 1/2, step 3284/7134 completed (loss: 0.10110288113355637, acc: 0.9726027250289917)
[2025-02-13 02:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42][root][INFO] - Training Epoch: 1/2, step 3285/7134 completed (loss: 0.05510735139250755, acc: 0.9783281683921814)
[2025-02-13 02:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:43][root][INFO] - Training Epoch: 1/2, step 3286/7134 completed (loss: 0.04777628183364868, acc: 0.9909774661064148)
[2025-02-13 02:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:43][root][INFO] - Training Epoch: 1/2, step 3287/7134 completed (loss: 0.12070974707603455, acc: 0.9736841917037964)
[2025-02-13 02:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44][root][INFO] - Training Epoch: 1/2, step 3288/7134 completed (loss: 0.06990937888622284, acc: 0.976047933101654)
[2025-02-13 02:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44][root][INFO] - Training Epoch: 1/2, step 3289/7134 completed (loss: 0.1127791553735733, acc: 0.9719298481941223)
[2025-02-13 02:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44][root][INFO] - Training Epoch: 1/2, step 3290/7134 completed (loss: 0.09563228487968445, acc: 0.9791666865348816)
[2025-02-13 02:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:45][root][INFO] - Training Epoch: 1/2, step 3291/7134 completed (loss: 0.08203326910734177, acc: 0.9755011200904846)
[2025-02-13 02:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:45][root][INFO] - Training Epoch: 1/2, step 3292/7134 completed (loss: 0.05565173551440239, acc: 0.9876543283462524)
[2025-02-13 02:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46][root][INFO] - Training Epoch: 1/2, step 3293/7134 completed (loss: 0.049087148159742355, acc: 0.982300877571106)
[2025-02-13 02:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46][root][INFO] - Training Epoch: 1/2, step 3294/7134 completed (loss: 0.10888057202100754, acc: 0.9709035158157349)
[2025-02-13 02:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46][root][INFO] - Training Epoch: 1/2, step 3295/7134 completed (loss: 0.032662179321050644, acc: 0.9897611141204834)
[2025-02-13 02:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:47][root][INFO] - Training Epoch: 1/2, step 3296/7134 completed (loss: 0.07444953918457031, acc: 0.9780621528625488)
[2025-02-13 02:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:47][root][INFO] - Training Epoch: 1/2, step 3297/7134 completed (loss: 0.06098296493291855, acc: 0.9812206625938416)
[2025-02-13 02:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48][root][INFO] - Training Epoch: 1/2, step 3298/7134 completed (loss: 0.11527428030967712, acc: 0.97444087266922)
[2025-02-13 02:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48][root][INFO] - Training Epoch: 1/2, step 3299/7134 completed (loss: 0.055327050387859344, acc: 0.9848993420600891)
[2025-02-13 02:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48][root][INFO] - Training Epoch: 1/2, step 3300/7134 completed (loss: 0.08156032115221024, acc: 0.9732620120048523)
[2025-02-13 02:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49][root][INFO] - Training Epoch: 1/2, step 3301/7134 completed (loss: 0.1075192540884018, acc: 0.9730290174484253)
[2025-02-13 02:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49][root][INFO] - Training Epoch: 1/2, step 3302/7134 completed (loss: 0.06998640298843384, acc: 0.9848484992980957)
[2025-02-13 02:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49][root][INFO] - Training Epoch: 1/2, step 3303/7134 completed (loss: 0.06518405675888062, acc: 0.9867841601371765)
[2025-02-13 02:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:50][root][INFO] - Training Epoch: 1/2, step 3304/7134 completed (loss: 0.03719234839081764, acc: 0.9892601370811462)
[2025-02-13 02:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:50][root][INFO] - Training Epoch: 1/2, step 3305/7134 completed (loss: 0.05692116171121597, acc: 0.9879336357116699)
[2025-02-13 02:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51][root][INFO] - Training Epoch: 1/2, step 3306/7134 completed (loss: 0.053272545337677, acc: 0.9844357967376709)
[2025-02-13 02:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51][root][INFO] - Training Epoch: 1/2, step 3307/7134 completed (loss: 0.07904108613729477, acc: 0.9813874959945679)
[2025-02-13 02:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51][root][INFO] - Training Epoch: 1/2, step 3308/7134 completed (loss: 0.05945845693349838, acc: 0.9826589822769165)
[2025-02-13 02:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:52][root][INFO] - Training Epoch: 1/2, step 3309/7134 completed (loss: 0.10208597779273987, acc: 0.9734513163566589)
[2025-02-13 02:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:52][root][INFO] - Training Epoch: 1/2, step 3310/7134 completed (loss: 0.07224518060684204, acc: 0.9829424023628235)
[2025-02-13 02:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53][root][INFO] - Training Epoch: 1/2, step 3311/7134 completed (loss: 0.05692223832011223, acc: 0.9784946441650391)
[2025-02-13 02:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53][root][INFO] - Training Epoch: 1/2, step 3312/7134 completed (loss: 0.07591760158538818, acc: 0.9745454788208008)
[2025-02-13 02:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53][root][INFO] - Training Epoch: 1/2, step 3313/7134 completed (loss: 0.03133293613791466, acc: 0.9952267408370972)
[2025-02-13 02:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:54][root][INFO] - Training Epoch: 1/2, step 3314/7134 completed (loss: 0.052120503038167953, acc: 0.9909420013427734)
[2025-02-13 02:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:54][root][INFO] - Training Epoch: 1/2, step 3315/7134 completed (loss: 0.10480587184429169, acc: 0.972862958908081)
[2025-02-13 02:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55][root][INFO] - Training Epoch: 1/2, step 3316/7134 completed (loss: 0.03370155021548271, acc: 0.9899159669876099)
[2025-02-13 02:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55][root][INFO] - Training Epoch: 1/2, step 3317/7134 completed (loss: 0.13830238580703735, acc: 0.9634615182876587)
[2025-02-13 02:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55][root][INFO] - Training Epoch: 1/2, step 3318/7134 completed (loss: 0.08774642646312714, acc: 0.9783393740653992)
[2025-02-13 02:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56][root][INFO] - Training Epoch: 1/2, step 3319/7134 completed (loss: 0.05736122280359268, acc: 0.9820846915245056)
[2025-02-13 02:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56][root][INFO] - Training Epoch: 1/2, step 3320/7134 completed (loss: 0.08528248965740204, acc: 0.9764397740364075)
[2025-02-13 02:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:57][root][INFO] - Training Epoch: 1/2, step 3321/7134 completed (loss: 0.12622925639152527, acc: 0.9743119478225708)
[2025-02-13 02:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:57][root][INFO] - Training Epoch: 1/2, step 3322/7134 completed (loss: 0.06642577797174454, acc: 0.9804804921150208)
[2025-02-13 02:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58][root][INFO] - Training Epoch: 1/2, step 3323/7134 completed (loss: 0.057929299771785736, acc: 0.9765517115592957)
[2025-02-13 02:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58][root][INFO] - Training Epoch: 1/2, step 3324/7134 completed (loss: 0.09520923346281052, acc: 0.9740871787071228)
[2025-02-13 02:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58][root][INFO] - Training Epoch: 1/2, step 3325/7134 completed (loss: 0.06535019725561142, acc: 0.9717742204666138)
[2025-02-13 02:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:59][root][INFO] - Training Epoch: 1/2, step 3326/7134 completed (loss: 0.07358334958553314, acc: 0.9858267903327942)
[2025-02-13 02:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:59][root][INFO] - Training Epoch: 1/2, step 3327/7134 completed (loss: 0.0571221187710762, acc: 0.9797022938728333)
[2025-02-13 02:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:00][root][INFO] - Training Epoch: 1/2, step 3328/7134 completed (loss: 0.06405014544725418, acc: 0.9862155318260193)
[2025-02-13 02:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:00][root][INFO] - Training Epoch: 1/2, step 3329/7134 completed (loss: 0.03517099469900131, acc: 0.9914737939834595)
[2025-02-13 02:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01][root][INFO] - Training Epoch: 1/2, step 3330/7134 completed (loss: 0.07915133982896805, acc: 0.981566846370697)
[2025-02-13 02:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01][root][INFO] - Training Epoch: 1/2, step 3331/7134 completed (loss: 0.12117014080286026, acc: 0.9665697813034058)
[2025-02-13 02:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01][root][INFO] - Training Epoch: 1/2, step 3332/7134 completed (loss: 0.16181580722332, acc: 0.9498270153999329)
[2025-02-13 02:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:02][root][INFO] - Training Epoch: 1/2, step 3333/7134 completed (loss: 0.12522906064987183, acc: 0.9733606576919556)
[2025-02-13 02:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:02][root][INFO] - Training Epoch: 1/2, step 3334/7134 completed (loss: 0.12058838456869125, acc: 0.9635134935379028)
[2025-02-13 02:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03][root][INFO] - Training Epoch: 1/2, step 3335/7134 completed (loss: 0.06105237081646919, acc: 0.9857594966888428)
[2025-02-13 02:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03][root][INFO] - Training Epoch: 1/2, step 3336/7134 completed (loss: 0.037755195051431656, acc: 0.991253674030304)
[2025-02-13 02:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03][root][INFO] - Training Epoch: 1/2, step 3337/7134 completed (loss: 0.02443217672407627, acc: 0.990138053894043)
[2025-02-13 02:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:04][root][INFO] - Training Epoch: 1/2, step 3338/7134 completed (loss: 0.03373149037361145, acc: 0.991304337978363)
[2025-02-13 02:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:04][root][INFO] - Training Epoch: 1/2, step 3339/7134 completed (loss: 0.08245424181222916, acc: 0.9809069037437439)
[2025-02-13 02:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:05][root][INFO] - Training Epoch: 1/2, step 3340/7134 completed (loss: 0.13969111442565918, acc: 0.9638009071350098)
[2025-02-13 02:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:05][root][INFO] - Training Epoch: 1/2, step 3341/7134 completed (loss: 0.07406839728355408, acc: 0.9792592525482178)
[2025-02-13 02:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06][root][INFO] - Training Epoch: 1/2, step 3342/7134 completed (loss: 0.056796472519636154, acc: 0.9831081032752991)
[2025-02-13 02:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06][root][INFO] - Training Epoch: 1/2, step 3343/7134 completed (loss: 0.04461053013801575, acc: 0.9879032373428345)
[2025-02-13 02:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06][root][INFO] - Training Epoch: 1/2, step 3344/7134 completed (loss: 0.04115397110581398, acc: 0.9914236664772034)
[2025-02-13 02:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:07][root][INFO] - Training Epoch: 1/2, step 3345/7134 completed (loss: 0.0860162302851677, acc: 0.987034022808075)
[2025-02-13 02:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:07][root][INFO] - Training Epoch: 1/2, step 3346/7134 completed (loss: 0.045130569487810135, acc: 0.9868420958518982)
[2025-02-13 02:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08][root][INFO] - Training Epoch: 1/2, step 3347/7134 completed (loss: 0.09632175415754318, acc: 0.9739413857460022)
[2025-02-13 02:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08][root][INFO] - Training Epoch: 1/2, step 3348/7134 completed (loss: 0.0698128268122673, acc: 0.9807407259941101)
[2025-02-13 02:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08][root][INFO] - Training Epoch: 1/2, step 3349/7134 completed (loss: 0.032453618943691254, acc: 0.9911816716194153)
[2025-02-13 02:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:09][root][INFO] - Training Epoch: 1/2, step 3350/7134 completed (loss: 0.043968938291072845, acc: 0.9857723712921143)
[2025-02-13 02:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:09][root][INFO] - Training Epoch: 1/2, step 3351/7134 completed (loss: 0.10149271041154861, acc: 0.9734748005867004)
[2025-02-13 02:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10][root][INFO] - Training Epoch: 1/2, step 3352/7134 completed (loss: 0.027867399156093597, acc: 0.9895651936531067)
[2025-02-13 02:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10][root][INFO] - Training Epoch: 1/2, step 3353/7134 completed (loss: 0.05383554846048355, acc: 0.9881656765937805)
[2025-02-13 02:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10][root][INFO] - Training Epoch: 1/2, step 3354/7134 completed (loss: 0.0495205782353878, acc: 0.9891473054885864)
[2025-02-13 02:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:11][root][INFO] - Training Epoch: 1/2, step 3355/7134 completed (loss: 0.024666298180818558, acc: 0.9929478168487549)
[2025-02-13 02:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:11][root][INFO] - Training Epoch: 1/2, step 3356/7134 completed (loss: 0.019010411575436592, acc: 0.9930555820465088)
[2025-02-13 02:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12][root][INFO] - Training Epoch: 1/2, step 3357/7134 completed (loss: 0.04083910584449768, acc: 0.9913420081138611)
[2025-02-13 02:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12][root][INFO] - Training Epoch: 1/2, step 3358/7134 completed (loss: 0.07663088291883469, acc: 0.9826989769935608)
[2025-02-13 02:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12][root][INFO] - Training Epoch: 1/2, step 3359/7134 completed (loss: 0.04546106979250908, acc: 0.991909384727478)
[2025-02-13 02:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:13][root][INFO] - Training Epoch: 1/2, step 3360/7134 completed (loss: 0.1095646545290947, acc: 0.9721029996871948)
[2025-02-13 02:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:13][root][INFO] - Training Epoch: 1/2, step 3361/7134 completed (loss: 0.08944954723119736, acc: 0.9731543660163879)
[2025-02-13 02:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14][root][INFO] - Training Epoch: 1/2, step 3362/7134 completed (loss: 0.0705702155828476, acc: 0.9773242473602295)
[2025-02-13 02:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14][root][INFO] - Training Epoch: 1/2, step 3363/7134 completed (loss: 0.018807152286171913, acc: 0.9953051805496216)
[2025-02-13 02:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14][root][INFO] - Training Epoch: 1/2, step 3364/7134 completed (loss: 0.10385002940893173, acc: 0.9754224419593811)
[2025-02-13 02:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:15][root][INFO] - Training Epoch: 1/2, step 3365/7134 completed (loss: 0.09649312496185303, acc: 0.9807229042053223)
[2025-02-13 02:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:15][root][INFO] - Training Epoch: 1/2, step 3366/7134 completed (loss: 0.03590104356408119, acc: 0.9887892603874207)
[2025-02-13 02:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16][root][INFO] - Training Epoch: 1/2, step 3367/7134 completed (loss: 0.04244095832109451, acc: 0.9903581142425537)
[2025-02-13 02:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16][root][INFO] - Training Epoch: 1/2, step 3368/7134 completed (loss: 0.07030218094587326, acc: 0.9828392863273621)
[2025-02-13 02:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16][root][INFO] - Training Epoch: 1/2, step 3369/7134 completed (loss: 0.07979758083820343, acc: 0.9812949895858765)
[2025-02-13 02:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:17][root][INFO] - Training Epoch: 1/2, step 3370/7134 completed (loss: 0.04090864956378937, acc: 0.9899857044219971)
[2025-02-13 02:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:17][root][INFO] - Training Epoch: 1/2, step 3371/7134 completed (loss: 0.038210429251194, acc: 0.9884868264198303)
[2025-02-13 02:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18][root][INFO] - Training Epoch: 1/2, step 3372/7134 completed (loss: 0.04723739996552467, acc: 0.9890282154083252)
[2025-02-13 02:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18][root][INFO] - Training Epoch: 1/2, step 3373/7134 completed (loss: 0.04187435656785965, acc: 0.9907407164573669)
[2025-02-13 02:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18][root][INFO] - Training Epoch: 1/2, step 3374/7134 completed (loss: 0.04233952611684799, acc: 0.9902507066726685)
[2025-02-13 02:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:19][root][INFO] - Training Epoch: 1/2, step 3375/7134 completed (loss: 0.06026361510157585, acc: 0.9855282306671143)
[2025-02-13 02:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:19][root][INFO] - Training Epoch: 1/2, step 3376/7134 completed (loss: 0.036713022738695145, acc: 0.9885057210922241)
[2025-02-13 02:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:20][root][INFO] - Training Epoch: 1/2, step 3377/7134 completed (loss: 0.019146448001265526, acc: 0.9940405488014221)
[2025-02-13 02:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:20][root][INFO] - Training Epoch: 1/2, step 3378/7134 completed (loss: 0.024418942630290985, acc: 0.9934895634651184)
[2025-02-13 02:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21][root][INFO] - Training Epoch: 1/2, step 3379/7134 completed (loss: 0.03551414608955383, acc: 0.9894737005233765)
[2025-02-13 02:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21][root][INFO] - Training Epoch: 1/2, step 3380/7134 completed (loss: 0.06615924835205078, acc: 0.9861538410186768)
[2025-02-13 02:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21][root][INFO] - Training Epoch: 1/2, step 3381/7134 completed (loss: 0.059367839246988297, acc: 0.9806201457977295)
[2025-02-13 02:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:22][root][INFO] - Training Epoch: 1/2, step 3382/7134 completed (loss: 0.02476818487048149, acc: 0.991349458694458)
[2025-02-13 02:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:22][root][INFO] - Training Epoch: 1/2, step 3383/7134 completed (loss: 0.0559057779610157, acc: 0.9870874881744385)
[2025-02-13 02:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:23][root][INFO] - Training Epoch: 1/2, step 3384/7134 completed (loss: 0.05258970707654953, acc: 0.9877675771713257)
[2025-02-13 02:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:23][root][INFO] - Training Epoch: 1/2, step 3385/7134 completed (loss: 0.03799370303750038, acc: 0.9884892106056213)
[2025-02-13 02:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24][root][INFO] - Training Epoch: 1/2, step 3386/7134 completed (loss: 0.030219323933124542, acc: 0.991525411605835)
[2025-02-13 02:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24][root][INFO] - Training Epoch: 1/2, step 3387/7134 completed (loss: 0.06180219724774361, acc: 0.9863945841789246)
[2025-02-13 02:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24][root][INFO] - Training Epoch: 1/2, step 3388/7134 completed (loss: 0.060257717967033386, acc: 0.9819168448448181)
[2025-02-13 02:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:25][root][INFO] - Training Epoch: 1/2, step 3389/7134 completed (loss: 0.03467588871717453, acc: 0.990777313709259)
[2025-02-13 02:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:25][root][INFO] - Training Epoch: 1/2, step 3390/7134 completed (loss: 0.03844977170228958, acc: 0.9896373152732849)
[2025-02-13 02:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26][root][INFO] - Training Epoch: 1/2, step 3391/7134 completed (loss: 0.021845459938049316, acc: 0.9958791136741638)
[2025-02-13 02:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26][root][INFO] - Training Epoch: 1/2, step 3392/7134 completed (loss: 0.03825175762176514, acc: 0.9827044010162354)
[2025-02-13 02:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26][root][INFO] - Training Epoch: 1/2, step 3393/7134 completed (loss: 0.06697845458984375, acc: 0.979938268661499)
[2025-02-13 02:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:27][root][INFO] - Training Epoch: 1/2, step 3394/7134 completed (loss: 0.04680405557155609, acc: 0.9857594966888428)
[2025-02-13 02:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:27][root][INFO] - Training Epoch: 1/2, step 3395/7134 completed (loss: 0.09235308319330215, acc: 0.975649356842041)
[2025-02-13 02:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28][root][INFO] - Training Epoch: 1/2, step 3396/7134 completed (loss: 0.026590809226036072, acc: 0.9940119981765747)
[2025-02-13 02:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28][root][INFO] - Training Epoch: 1/2, step 3397/7134 completed (loss: 0.04827937111258507, acc: 0.9914425611495972)
[2025-02-13 02:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28][root][INFO] - Training Epoch: 1/2, step 3398/7134 completed (loss: 0.05180090665817261, acc: 0.9892473220825195)
[2025-02-13 02:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:29][root][INFO] - Training Epoch: 1/2, step 3399/7134 completed (loss: 0.13330936431884766, acc: 0.9724264740943909)
[2025-02-13 02:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:29][root][INFO] - Training Epoch: 1/2, step 3400/7134 completed (loss: 0.10576120764017105, acc: 0.9756592512130737)
[2025-02-13 02:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:30][root][INFO] - Training Epoch: 1/2, step 3401/7134 completed (loss: 0.08791552484035492, acc: 0.9790209531784058)
[2025-02-13 02:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:30][root][INFO] - Training Epoch: 1/2, step 3402/7134 completed (loss: 0.08537822216749191, acc: 0.9723320007324219)
[2025-02-13 02:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31][root][INFO] - Training Epoch: 1/2, step 3403/7134 completed (loss: 0.10787669569253922, acc: 0.96484375)
[2025-02-13 02:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31][root][INFO] - Training Epoch: 1/2, step 3404/7134 completed (loss: 0.025951862335205078, acc: 0.9925261735916138)
[2025-02-13 02:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31][root][INFO] - Training Epoch: 1/2, step 3405/7134 completed (loss: 0.09119529277086258, acc: 0.9838129281997681)
[2025-02-13 02:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:32][root][INFO] - Training Epoch: 1/2, step 3406/7134 completed (loss: 0.05184956267476082, acc: 0.9832473993301392)
[2025-02-13 02:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:32][root][INFO] - Training Epoch: 1/2, step 3407/7134 completed (loss: 0.06467820703983307, acc: 0.9828392863273621)
[2025-02-13 02:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33][root][INFO] - Training Epoch: 1/2, step 3408/7134 completed (loss: 0.07624659687280655, acc: 0.9769585132598877)
[2025-02-13 02:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33][root][INFO] - Training Epoch: 1/2, step 3409/7134 completed (loss: 0.07659567147493362, acc: 0.981055498123169)
[2025-02-13 02:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33][root][INFO] - Training Epoch: 1/2, step 3410/7134 completed (loss: 0.06846463680267334, acc: 0.9848484992980957)
[2025-02-13 02:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:34][root][INFO] - Training Epoch: 1/2, step 3411/7134 completed (loss: 0.05919405817985535, acc: 0.9857697486877441)
[2025-02-13 02:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:34][root][INFO] - Training Epoch: 1/2, step 3412/7134 completed (loss: 0.06455070525407791, acc: 0.9735350012779236)
[2025-02-13 02:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:35][root][INFO] - Training Epoch: 1/2, step 3413/7134 completed (loss: 0.07499147206544876, acc: 0.9760000109672546)
[2025-02-13 02:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:35][root][INFO] - Training Epoch: 1/2, step 3414/7134 completed (loss: 0.06261016428470612, acc: 0.9866443872451782)
[2025-02-13 02:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36][root][INFO] - Training Epoch: 1/2, step 3415/7134 completed (loss: 0.07259152829647064, acc: 0.9839743375778198)
[2025-02-13 02:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36][root][INFO] - Training Epoch: 1/2, step 3416/7134 completed (loss: 0.061480529606342316, acc: 0.9807923436164856)
[2025-02-13 02:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36][root][INFO] - Training Epoch: 1/2, step 3417/7134 completed (loss: 0.04045938700437546, acc: 0.9905533194541931)
[2025-02-13 02:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:37][root][INFO] - Training Epoch: 1/2, step 3418/7134 completed (loss: 0.045528609305620193, acc: 0.9896907210350037)
[2025-02-13 02:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:37][root][INFO] - Training Epoch: 1/2, step 3419/7134 completed (loss: 0.02230105921626091, acc: 0.9899135231971741)
[2025-02-13 02:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38][root][INFO] - Training Epoch: 1/2, step 3420/7134 completed (loss: 0.017572524026036263, acc: 0.9952606558799744)
[2025-02-13 02:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38][root][INFO] - Training Epoch: 1/2, step 3421/7134 completed (loss: 0.03836100921034813, acc: 0.993779182434082)
[2025-02-13 02:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38][root][INFO] - Training Epoch: 1/2, step 3422/7134 completed (loss: 0.015268013812601566, acc: 0.9947460889816284)
[2025-02-13 02:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:39][root][INFO] - Training Epoch: 1/2, step 3423/7134 completed (loss: 0.015930896624922752, acc: 0.9954545497894287)
[2025-02-13 02:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:39][root][INFO] - Training Epoch: 1/2, step 3424/7134 completed (loss: 0.10303545743227005, acc: 0.974397599697113)
[2025-02-13 02:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:40][root][INFO] - Training Epoch: 1/2, step 3425/7134 completed (loss: 0.05690060555934906, acc: 0.9820512533187866)
[2025-02-13 02:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:40][root][INFO] - Training Epoch: 1/2, step 3426/7134 completed (loss: 0.05842124670743942, acc: 0.9903448224067688)
[2025-02-13 02:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41][root][INFO] - Training Epoch: 1/2, step 3427/7134 completed (loss: 0.04255533963441849, acc: 0.9936808943748474)
[2025-02-13 02:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41][root][INFO] - Training Epoch: 1/2, step 3428/7134 completed (loss: 0.03160357102751732, acc: 0.9928366541862488)
[2025-02-13 02:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41][root][INFO] - Training Epoch: 1/2, step 3429/7134 completed (loss: 0.05646931380033493, acc: 0.9822006225585938)
[2025-02-13 02:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42][root][INFO] - Training Epoch: 1/2, step 3430/7134 completed (loss: 0.09021386504173279, acc: 0.9789302945137024)
[2025-02-13 02:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42][root][INFO] - Training Epoch: 1/2, step 3431/7134 completed (loss: 0.059052031487226486, acc: 0.9847198724746704)
[2025-02-13 02:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:43][root][INFO] - Training Epoch: 1/2, step 3432/7134 completed (loss: 0.05389287695288658, acc: 0.9876543283462524)
[2025-02-13 02:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:43][root][INFO] - Training Epoch: 1/2, step 3433/7134 completed (loss: 0.1049424558877945, acc: 0.9738562107086182)
[2025-02-13 02:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44][root][INFO] - Training Epoch: 1/2, step 3434/7134 completed (loss: 0.05484180897474289, acc: 0.9841017723083496)
[2025-02-13 02:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44][root][INFO] - Training Epoch: 1/2, step 3435/7134 completed (loss: 0.059565044939517975, acc: 0.9871382713317871)
[2025-02-13 02:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44][root][INFO] - Training Epoch: 1/2, step 3436/7134 completed (loss: 0.054544974118471146, acc: 0.9756097793579102)
[2025-02-13 02:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45][root][INFO] - Training Epoch: 1/2, step 3437/7134 completed (loss: 0.04030180722475052, acc: 0.9921875)
[2025-02-13 02:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45][root][INFO] - Training Epoch: 1/2, step 3438/7134 completed (loss: 0.04617907851934433, acc: 0.991919219493866)
[2025-02-13 02:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45][root][INFO] - Training Epoch: 1/2, step 3439/7134 completed (loss: 0.05153696611523628, acc: 0.9819168448448181)
[2025-02-13 02:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:46][root][INFO] - Training Epoch: 1/2, step 3440/7134 completed (loss: 0.11717455089092255, acc: 0.9629629850387573)
[2025-02-13 02:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:46][root][INFO] - Training Epoch: 1/2, step 3441/7134 completed (loss: 0.11532764881849289, acc: 0.97398841381073)
[2025-02-13 02:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47][root][INFO] - Training Epoch: 1/2, step 3442/7134 completed (loss: 0.031625889241695404, acc: 0.989051103591919)
[2025-02-13 02:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47][root][INFO] - Training Epoch: 1/2, step 3443/7134 completed (loss: 0.08312931656837463, acc: 0.9803370833396912)
[2025-02-13 02:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47][root][INFO] - Training Epoch: 1/2, step 3444/7134 completed (loss: 0.06961595267057419, acc: 0.9841772317886353)
[2025-02-13 02:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:48][root][INFO] - Training Epoch: 1/2, step 3445/7134 completed (loss: 0.15800601243972778, acc: 0.9591397643089294)
[2025-02-13 02:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:48][root][INFO] - Training Epoch: 1/2, step 3446/7134 completed (loss: 0.10987913608551025, acc: 0.9648829698562622)
[2025-02-13 02:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49][root][INFO] - Training Epoch: 1/2, step 3447/7134 completed (loss: 0.08700519800186157, acc: 0.9706840515136719)
[2025-02-13 02:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49][root][INFO] - Training Epoch: 1/2, step 3448/7134 completed (loss: 0.04135292023420334, acc: 0.9864864945411682)
[2025-02-13 02:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49][root][INFO] - Training Epoch: 1/2, step 3449/7134 completed (loss: 0.06689862161874771, acc: 0.9846938848495483)
[2025-02-13 02:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:50][root][INFO] - Training Epoch: 1/2, step 3450/7134 completed (loss: 0.0786467120051384, acc: 0.9822784662246704)
[2025-02-13 02:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:50][root][INFO] - Training Epoch: 1/2, step 3451/7134 completed (loss: 0.06411489844322205, acc: 0.982332170009613)
[2025-02-13 02:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51][root][INFO] - Training Epoch: 1/2, step 3452/7134 completed (loss: 0.06155698746442795, acc: 0.9848155975341797)
[2025-02-13 02:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51][root][INFO] - Training Epoch: 1/2, step 3453/7134 completed (loss: 0.07387903332710266, acc: 0.9779286980628967)
[2025-02-13 02:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51][root][INFO] - Training Epoch: 1/2, step 3454/7134 completed (loss: 0.06001739576458931, acc: 0.9858956336975098)
[2025-02-13 02:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:52][root][INFO] - Training Epoch: 1/2, step 3455/7134 completed (loss: 0.029088299721479416, acc: 0.9927536249160767)
[2025-02-13 02:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:52][root][INFO] - Training Epoch: 1/2, step 3456/7134 completed (loss: 0.04823901504278183, acc: 0.9917647242546082)
[2025-02-13 02:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:53][root][INFO] - Training Epoch: 1/2, step 3457/7134 completed (loss: 0.038888365030288696, acc: 0.988135576248169)
[2025-02-13 02:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:53][root][INFO] - Training Epoch: 1/2, step 3458/7134 completed (loss: 0.07638239115476608, acc: 0.9837618470191956)
[2025-02-13 02:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54][root][INFO] - Training Epoch: 1/2, step 3459/7134 completed (loss: 0.05169881507754326, acc: 0.9878234267234802)
[2025-02-13 02:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54][root][INFO] - Training Epoch: 1/2, step 3460/7134 completed (loss: 0.18525683879852295, acc: 0.9615384340286255)
[2025-02-13 02:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54][root][INFO] - Training Epoch: 1/2, step 3461/7134 completed (loss: 0.23801231384277344, acc: 0.951724112033844)
[2025-02-13 02:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:55][root][INFO] - Training Epoch: 1/2, step 3462/7134 completed (loss: 0.10485837608575821, acc: 0.9748822450637817)
[2025-02-13 02:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:55][root][INFO] - Training Epoch: 1/2, step 3463/7134 completed (loss: 0.029652876779437065, acc: 0.9909502267837524)
[2025-02-13 02:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56][root][INFO] - Training Epoch: 1/2, step 3464/7134 completed (loss: 0.06063991039991379, acc: 0.9772117733955383)
[2025-02-13 02:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56][root][INFO] - Training Epoch: 1/2, step 3465/7134 completed (loss: 0.09413054585456848, acc: 0.9755600690841675)
[2025-02-13 02:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56][root][INFO] - Training Epoch: 1/2, step 3466/7134 completed (loss: 0.027210356667637825, acc: 0.9904912710189819)
[2025-02-13 02:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:57][root][INFO] - Training Epoch: 1/2, step 3467/7134 completed (loss: 0.06826667487621307, acc: 0.987261176109314)
[2025-02-13 02:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:57][root][INFO] - Training Epoch: 1/2, step 3468/7134 completed (loss: 0.01601741649210453, acc: 0.9971056580543518)
[2025-02-13 02:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58][root][INFO] - Training Epoch: 1/2, step 3469/7134 completed (loss: 0.06589747220277786, acc: 0.9754716753959656)
[2025-02-13 02:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58][root][INFO] - Training Epoch: 1/2, step 3470/7134 completed (loss: 0.025018665939569473, acc: 0.9930875301361084)
[2025-02-13 02:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58][root][INFO] - Training Epoch: 1/2, step 3471/7134 completed (loss: 0.08444678783416748, acc: 0.9861878156661987)
[2025-02-13 02:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:59][root][INFO] - Training Epoch: 1/2, step 3472/7134 completed (loss: 0.029562991112470627, acc: 0.992548406124115)
[2025-02-13 02:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:59][root][INFO] - Training Epoch: 1/2, step 3473/7134 completed (loss: 0.0466805174946785, acc: 0.9867549538612366)
[2025-02-13 02:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00][root][INFO] - Training Epoch: 1/2, step 3474/7134 completed (loss: 0.058413516730070114, acc: 0.9858657121658325)
[2025-02-13 02:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00][root][INFO] - Training Epoch: 1/2, step 3475/7134 completed (loss: 0.037619587033987045, acc: 0.9893428087234497)
[2025-02-13 02:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00][root][INFO] - Training Epoch: 1/2, step 3476/7134 completed (loss: 0.07249592989683151, acc: 0.9794520735740662)
[2025-02-13 02:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:01][root][INFO] - Training Epoch: 1/2, step 3477/7134 completed (loss: 0.054757531732320786, acc: 0.9917762875556946)
[2025-02-13 02:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:01][root][INFO] - Training Epoch: 1/2, step 3478/7134 completed (loss: 0.0194509606808424, acc: 0.9955089688301086)
[2025-02-13 02:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02][root][INFO] - Training Epoch: 1/2, step 3479/7134 completed (loss: 0.011073325760662556, acc: 0.998487114906311)
[2025-02-13 02:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02][root][INFO] - Training Epoch: 1/2, step 3480/7134 completed (loss: 0.04206903278827667, acc: 0.9891008138656616)
[2025-02-13 02:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02][root][INFO] - Training Epoch: 1/2, step 3481/7134 completed (loss: 0.027181630954146385, acc: 0.9946808218955994)
[2025-02-13 02:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:03][root][INFO] - Training Epoch: 1/2, step 3482/7134 completed (loss: 0.026811707764863968, acc: 0.9910072088241577)
[2025-02-13 02:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:03][root][INFO] - Training Epoch: 1/2, step 3483/7134 completed (loss: 0.01042621023952961, acc: 0.9969183206558228)
[2025-02-13 02:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04][root][INFO] - Training Epoch: 1/2, step 3484/7134 completed (loss: 0.03246404603123665, acc: 0.991150438785553)
[2025-02-13 02:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04][root][INFO] - Training Epoch: 1/2, step 3485/7134 completed (loss: 0.05275481194257736, acc: 0.9826839566230774)
[2025-02-13 02:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04][root][INFO] - Training Epoch: 1/2, step 3486/7134 completed (loss: 0.02730075642466545, acc: 0.9910072088241577)
[2025-02-13 02:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:05][root][INFO] - Training Epoch: 1/2, step 3487/7134 completed (loss: 0.05200433358550072, acc: 0.9779086709022522)
[2025-02-13 02:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:05][root][INFO] - Training Epoch: 1/2, step 3488/7134 completed (loss: 0.038708947598934174, acc: 0.9901315569877625)
[2025-02-13 02:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:06][root][INFO] - Training Epoch: 1/2, step 3489/7134 completed (loss: 0.10362135618925095, acc: 0.9723756909370422)
[2025-02-13 02:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:06][root][INFO] - Training Epoch: 1/2, step 3490/7134 completed (loss: 0.09949275106191635, acc: 0.9793205261230469)
[2025-02-13 02:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07][root][INFO] - Training Epoch: 1/2, step 3491/7134 completed (loss: 0.060381025075912476, acc: 0.9858155846595764)
[2025-02-13 02:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07][root][INFO] - Training Epoch: 1/2, step 3492/7134 completed (loss: 0.03787795826792717, acc: 0.9897330403327942)
[2025-02-13 02:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07][root][INFO] - Training Epoch: 1/2, step 3493/7134 completed (loss: 0.032155439257621765, acc: 0.9901960492134094)
[2025-02-13 02:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08][root][INFO] - Training Epoch: 1/2, step 3494/7134 completed (loss: 0.040924180299043655, acc: 0.9913793206214905)
[2025-02-13 02:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08][root][INFO] - Training Epoch: 1/2, step 3495/7134 completed (loss: 0.024468228220939636, acc: 0.9880095720291138)
[2025-02-13 02:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08][root][INFO] - Training Epoch: 1/2, step 3496/7134 completed (loss: 0.03905148431658745, acc: 0.9887387156486511)
[2025-02-13 02:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:09][root][INFO] - Training Epoch: 1/2, step 3497/7134 completed (loss: 0.04520973190665245, acc: 0.9821958541870117)
[2025-02-13 02:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:09][root][INFO] - Training Epoch: 1/2, step 3498/7134 completed (loss: 0.04036881402134895, acc: 0.9862805008888245)
[2025-02-13 02:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10][root][INFO] - Training Epoch: 1/2, step 3499/7134 completed (loss: 0.01862630806863308, acc: 0.9967585206031799)
[2025-02-13 02:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10][root][INFO] - Training Epoch: 1/2, step 3500/7134 completed (loss: 0.0832945853471756, acc: 0.9765051603317261)
[2025-02-13 02:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10][root][INFO] - Training Epoch: 1/2, step 3501/7134 completed (loss: 0.10650470852851868, acc: 0.9637826681137085)
[2025-02-13 02:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:11][root][INFO] - Training Epoch: 1/2, step 3502/7134 completed (loss: 0.11733101308345795, acc: 0.9684210419654846)
[2025-02-13 02:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:11][root][INFO] - Training Epoch: 1/2, step 3503/7134 completed (loss: 0.06895142793655396, acc: 0.984402060508728)
[2025-02-13 02:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12][root][INFO] - Training Epoch: 1/2, step 3504/7134 completed (loss: 0.02201869525015354, acc: 0.9954751133918762)
[2025-02-13 02:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12][root][INFO] - Training Epoch: 1/2, step 3505/7134 completed (loss: 0.03284181281924248, acc: 0.9946332573890686)
[2025-02-13 02:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12][root][INFO] - Training Epoch: 1/2, step 3506/7134 completed (loss: 0.04751824587583542, acc: 0.9848771095275879)
[2025-02-13 02:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:13][root][INFO] - Training Epoch: 1/2, step 3507/7134 completed (loss: 0.0160084031522274, acc: 0.9933775067329407)
[2025-02-13 02:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:13][root][INFO] - Training Epoch: 1/2, step 3508/7134 completed (loss: 0.029530957341194153, acc: 0.9926900863647461)
[2025-02-13 02:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:14][root][INFO] - Training Epoch: 1/2, step 3509/7134 completed (loss: 0.03226916119456291, acc: 0.988950252532959)
[2025-02-13 02:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:14][root][INFO] - Training Epoch: 1/2, step 3510/7134 completed (loss: 0.05830981209874153, acc: 0.9845626354217529)
[2025-02-13 02:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15][root][INFO] - Training Epoch: 1/2, step 3511/7134 completed (loss: 0.014442658983170986, acc: 0.9963964223861694)
[2025-02-13 02:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15][root][INFO] - Training Epoch: 1/2, step 3512/7134 completed (loss: 0.04598354920744896, acc: 0.9841521382331848)
[2025-02-13 02:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15][root][INFO] - Training Epoch: 1/2, step 3513/7134 completed (loss: 0.044137321412563324, acc: 0.9893805384635925)
[2025-02-13 02:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:16][root][INFO] - Training Epoch: 1/2, step 3514/7134 completed (loss: 0.04182438924908638, acc: 0.9930875301361084)
[2025-02-13 02:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:16][root][INFO] - Training Epoch: 1/2, step 3515/7134 completed (loss: 0.027105553075671196, acc: 0.9916201233863831)
[2025-02-13 02:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:17][root][INFO] - Training Epoch: 1/2, step 3516/7134 completed (loss: 0.02319939434528351, acc: 0.9945828914642334)
[2025-02-13 02:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:17][root][INFO] - Training Epoch: 1/2, step 3517/7134 completed (loss: 0.037446774542331696, acc: 0.9916567206382751)
[2025-02-13 02:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18][root][INFO] - Training Epoch: 1/2, step 3518/7134 completed (loss: 0.03981838375329971, acc: 0.9870610237121582)
[2025-02-13 02:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18][root][INFO] - Training Epoch: 1/2, step 3519/7134 completed (loss: 0.03417579457163811, acc: 0.9921348094940186)
[2025-02-13 02:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18][root][INFO] - Training Epoch: 1/2, step 3520/7134 completed (loss: 0.09302235394716263, acc: 0.981176495552063)
[2025-02-13 02:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:19][root][INFO] - Training Epoch: 1/2, step 3521/7134 completed (loss: 0.028282681480050087, acc: 0.9900426864624023)
[2025-02-13 02:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:19][root][INFO] - Training Epoch: 1/2, step 3522/7134 completed (loss: 0.04914878308773041, acc: 0.9866310358047485)
[2025-02-13 02:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:20][root][INFO] - Training Epoch: 1/2, step 3523/7134 completed (loss: 0.01686832308769226, acc: 0.9969183206558228)
[2025-02-13 02:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:20][root][INFO] - Training Epoch: 1/2, step 3524/7134 completed (loss: 0.06670469790697098, acc: 0.9830729365348816)
[2025-02-13 02:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21][root][INFO] - Training Epoch: 1/2, step 3525/7134 completed (loss: 0.02433675341308117, acc: 0.9934102296829224)
[2025-02-13 02:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21][root][INFO] - Training Epoch: 1/2, step 3526/7134 completed (loss: 0.052377697080373764, acc: 0.9873257279396057)
[2025-02-13 02:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21][root][INFO] - Training Epoch: 1/2, step 3527/7134 completed (loss: 0.04461197182536125, acc: 0.98591548204422)
[2025-02-13 02:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:22][root][INFO] - Training Epoch: 1/2, step 3528/7134 completed (loss: 0.016980130225419998, acc: 0.9965397715568542)
[2025-02-13 02:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:22][root][INFO] - Training Epoch: 1/2, step 3529/7134 completed (loss: 0.019468188285827637, acc: 0.9952940940856934)
[2025-02-13 02:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:23][root][INFO] - Training Epoch: 1/2, step 3530/7134 completed (loss: 0.028477856889367104, acc: 0.9885321259498596)
[2025-02-13 02:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:23][root][INFO] - Training Epoch: 1/2, step 3531/7134 completed (loss: 0.03679652512073517, acc: 0.9849397540092468)
[2025-02-13 02:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:24][root][INFO] - Training Epoch: 1/2, step 3532/7134 completed (loss: 0.014218193478882313, acc: 0.9949495196342468)
[2025-02-13 02:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:24][root][INFO] - Training Epoch: 1/2, step 3533/7134 completed (loss: 0.02448258362710476, acc: 0.9953434467315674)
[2025-02-13 02:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25][root][INFO] - Training Epoch: 1/2, step 3534/7134 completed (loss: 0.03959580138325691, acc: 0.9888734221458435)
[2025-02-13 02:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25][root][INFO] - Training Epoch: 1/2, step 3535/7134 completed (loss: 0.035977818071842194, acc: 0.9892473220825195)
[2025-02-13 02:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25][root][INFO] - Training Epoch: 1/2, step 3536/7134 completed (loss: 0.06717849522829056, acc: 0.9836065769195557)
[2025-02-13 02:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26][root][INFO] - Training Epoch: 1/2, step 3537/7134 completed (loss: 0.04012472182512283, acc: 0.9899328947067261)
[2025-02-13 02:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26][root][INFO] - Training Epoch: 1/2, step 3538/7134 completed (loss: 0.05328286439180374, acc: 0.9923954606056213)
[2025-02-13 02:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26][root][INFO] - Training Epoch: 1/2, step 3539/7134 completed (loss: 0.05628732964396477, acc: 0.9805825352668762)
[2025-02-13 02:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:27][root][INFO] - Training Epoch: 1/2, step 3540/7134 completed (loss: 0.06380140036344528, acc: 0.9772382378578186)
[2025-02-13 02:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:27][root][INFO] - Training Epoch: 1/2, step 3541/7134 completed (loss: 0.0605417899787426, acc: 0.9809104204177856)
[2025-02-13 02:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28][root][INFO] - Training Epoch: 1/2, step 3542/7134 completed (loss: 0.04430296644568443, acc: 0.9830769300460815)
[2025-02-13 02:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28][root][INFO] - Training Epoch: 1/2, step 3543/7134 completed (loss: 0.07150966674089432, acc: 0.9771126508712769)
[2025-02-13 02:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28][root][INFO] - Training Epoch: 1/2, step 3544/7134 completed (loss: 0.04853329807519913, acc: 0.9846153855323792)
[2025-02-13 02:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:29][root][INFO] - Training Epoch: 1/2, step 3545/7134 completed (loss: 0.03723033517599106, acc: 0.9906832575798035)
[2025-02-13 02:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:29][root][INFO] - Training Epoch: 1/2, step 3546/7134 completed (loss: 0.05508837103843689, acc: 0.9851484894752502)
[2025-02-13 02:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30][root][INFO] - Training Epoch: 1/2, step 3547/7134 completed (loss: 0.07152824848890305, acc: 0.9790209531784058)
[2025-02-13 02:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30][root][INFO] - Training Epoch: 1/2, step 3548/7134 completed (loss: 0.05243685469031334, acc: 0.980088472366333)
[2025-02-13 02:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31][root][INFO] - Training Epoch: 1/2, step 3549/7134 completed (loss: 0.07160061597824097, acc: 0.971781313419342)
[2025-02-13 02:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31][root][INFO] - Training Epoch: 1/2, step 3550/7134 completed (loss: 0.043397143483161926, acc: 0.9824868440628052)
[2025-02-13 02:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31][root][INFO] - Training Epoch: 1/2, step 3551/7134 completed (loss: 0.03807375580072403, acc: 0.9865319728851318)
[2025-02-13 02:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:32][root][INFO] - Training Epoch: 1/2, step 3552/7134 completed (loss: 0.1470404863357544, acc: 0.9569288492202759)
[2025-02-13 02:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:32][root][INFO] - Training Epoch: 1/2, step 3553/7134 completed (loss: 0.08546306937932968, acc: 0.9819079041481018)
[2025-02-13 02:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33][root][INFO] - Training Epoch: 1/2, step 3554/7134 completed (loss: 0.06472164392471313, acc: 0.9864457845687866)
[2025-02-13 02:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33][root][INFO] - Training Epoch: 1/2, step 3555/7134 completed (loss: 0.04184548184275627, acc: 0.9858712553977966)
[2025-02-13 02:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33][root][INFO] - Training Epoch: 1/2, step 3556/7134 completed (loss: 0.047817427664995193, acc: 0.9865642786026001)
[2025-02-13 02:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:34][root][INFO] - Training Epoch: 1/2, step 3557/7134 completed (loss: 0.06108615919947624, acc: 0.9854809641838074)
[2025-02-13 02:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:34][root][INFO] - Training Epoch: 1/2, step 3558/7134 completed (loss: 0.09618311375379562, acc: 0.9793814420700073)
[2025-02-13 02:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35][root][INFO] - Training Epoch: 1/2, step 3559/7134 completed (loss: 0.031521543860435486, acc: 0.9883720874786377)
[2025-02-13 02:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35][root][INFO] - Training Epoch: 1/2, step 3560/7134 completed (loss: 0.02684570476412773, acc: 0.9931507110595703)
[2025-02-13 02:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36][root][INFO] - Training Epoch: 1/2, step 3561/7134 completed (loss: 0.04989643767476082, acc: 0.9859943985939026)
[2025-02-13 02:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36][root][INFO] - Training Epoch: 1/2, step 3562/7134 completed (loss: 0.0478706881403923, acc: 0.9821428656578064)
[2025-02-13 02:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36][root][INFO] - Training Epoch: 1/2, step 3563/7134 completed (loss: 0.05384189262986183, acc: 0.9872813820838928)
[2025-02-13 02:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:37][root][INFO] - Training Epoch: 1/2, step 3564/7134 completed (loss: 0.0768338143825531, acc: 0.9774436354637146)
[2025-02-13 02:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:37][root][INFO] - Training Epoch: 1/2, step 3565/7134 completed (loss: 0.09972067177295685, acc: 0.9661654233932495)
[2025-02-13 02:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:35][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0706, device='cuda:0') eval_epoch_loss=tensor(0.0682, device='cuda:0') eval_epoch_acc=tensor(0.9810, device='cuda:0')
[2025-02-13 02:57:35][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 02:57:35][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 02:57:35][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_3566_loss_0.06821627169847488/model.pt
[2025-02-13 02:57:35][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 02:57:35][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.06821627169847488
[2025-02-13 02:57:35][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.980992317199707
[2025-02-13 02:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:35][root][INFO] - Training Epoch: 1/2, step 3566/7134 completed (loss: 0.08537968248128891, acc: 0.9681274890899658)
[2025-02-13 02:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36][root][INFO] - Training Epoch: 1/2, step 3567/7134 completed (loss: 0.10528469830751419, acc: 0.9805951118469238)
[2025-02-13 02:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36][root][INFO] - Training Epoch: 1/2, step 3568/7134 completed (loss: 0.10556302219629288, acc: 0.9803600907325745)
[2025-02-13 02:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36][root][INFO] - Training Epoch: 1/2, step 3569/7134 completed (loss: 0.07967160642147064, acc: 0.9820788502693176)
[2025-02-13 02:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37][root][INFO] - Training Epoch: 1/2, step 3570/7134 completed (loss: 0.08984451740980148, acc: 0.9785124063491821)
[2025-02-13 02:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37][root][INFO] - Training Epoch: 1/2, step 3571/7134 completed (loss: 0.0783318355679512, acc: 0.9854111671447754)
[2025-02-13 02:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38][root][INFO] - Training Epoch: 1/2, step 3572/7134 completed (loss: 0.044288262724876404, acc: 0.9881423115730286)
[2025-02-13 02:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38][root][INFO] - Training Epoch: 1/2, step 3573/7134 completed (loss: 0.07635103166103363, acc: 0.9777777791023254)
[2025-02-13 02:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39][root][INFO] - Training Epoch: 1/2, step 3574/7134 completed (loss: 0.11173631250858307, acc: 0.9578543901443481)
[2025-02-13 02:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39][root][INFO] - Training Epoch: 1/2, step 3575/7134 completed (loss: 0.1379968374967575, acc: 0.9639794230461121)
[2025-02-13 02:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39][root][INFO] - Training Epoch: 1/2, step 3576/7134 completed (loss: 0.1344069540500641, acc: 0.9650092124938965)
[2025-02-13 02:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40][root][INFO] - Training Epoch: 1/2, step 3577/7134 completed (loss: 0.08016601949930191, acc: 0.9734513163566589)
[2025-02-13 02:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40][root][INFO] - Training Epoch: 1/2, step 3578/7134 completed (loss: 0.054435890167951584, acc: 0.976710319519043)
[2025-02-13 02:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41][root][INFO] - Training Epoch: 1/2, step 3579/7134 completed (loss: 0.0631067082285881, acc: 0.9826666712760925)
[2025-02-13 02:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41][root][INFO] - Training Epoch: 1/2, step 3580/7134 completed (loss: 0.11031627655029297, acc: 0.9559054970741272)
[2025-02-13 02:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41][root][INFO] - Training Epoch: 1/2, step 3581/7134 completed (loss: 0.10121535509824753, acc: 0.9675993919372559)
[2025-02-13 02:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:42][root][INFO] - Training Epoch: 1/2, step 3582/7134 completed (loss: 0.06563553959131241, acc: 0.9810426831245422)
[2025-02-13 02:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:42][root][INFO] - Training Epoch: 1/2, step 3583/7134 completed (loss: 0.08393928408622742, acc: 0.9772036671638489)
[2025-02-13 02:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43][root][INFO] - Training Epoch: 1/2, step 3584/7134 completed (loss: 0.08757317066192627, acc: 0.9894179701805115)
[2025-02-13 02:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43][root][INFO] - Training Epoch: 1/2, step 3585/7134 completed (loss: 0.07544617354869843, acc: 0.9722222089767456)
[2025-02-13 02:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43][root][INFO] - Training Epoch: 1/2, step 3586/7134 completed (loss: 0.08033118396997452, acc: 0.9743177890777588)
[2025-02-13 02:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44][root][INFO] - Training Epoch: 1/2, step 3587/7134 completed (loss: 0.06037154048681259, acc: 0.9830795526504517)
[2025-02-13 02:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44][root][INFO] - Training Epoch: 1/2, step 3588/7134 completed (loss: 0.10496637225151062, acc: 0.976331353187561)
[2025-02-13 02:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:45][root][INFO] - Training Epoch: 1/2, step 3589/7134 completed (loss: 0.07653158158063889, acc: 0.9848713874816895)
[2025-02-13 02:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:45][root][INFO] - Training Epoch: 1/2, step 3590/7134 completed (loss: 0.047598015516996384, acc: 0.983146071434021)
[2025-02-13 02:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46][root][INFO] - Training Epoch: 1/2, step 3591/7134 completed (loss: 0.04957710579037666, acc: 0.9879879951477051)
[2025-02-13 02:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46][root][INFO] - Training Epoch: 1/2, step 3592/7134 completed (loss: 0.028292739763855934, acc: 0.9933884143829346)
[2025-02-13 02:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46][root][INFO] - Training Epoch: 1/2, step 3593/7134 completed (loss: 0.07418064773082733, acc: 0.9842382073402405)
[2025-02-13 02:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47][root][INFO] - Training Epoch: 1/2, step 3594/7134 completed (loss: 0.04144833981990814, acc: 0.9886363744735718)
[2025-02-13 02:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47][root][INFO] - Training Epoch: 1/2, step 3595/7134 completed (loss: 0.06382565945386887, acc: 0.9785605072975159)
[2025-02-13 02:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:48][root][INFO] - Training Epoch: 1/2, step 3596/7134 completed (loss: 0.05993245542049408, acc: 0.9834938049316406)
[2025-02-13 02:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:48][root][INFO] - Training Epoch: 1/2, step 3597/7134 completed (loss: 0.048990242183208466, acc: 0.9870503544807434)
[2025-02-13 02:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49][root][INFO] - Training Epoch: 1/2, step 3598/7134 completed (loss: 0.06080111488699913, acc: 0.9794050455093384)
[2025-02-13 02:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49][root][INFO] - Training Epoch: 1/2, step 3599/7134 completed (loss: 0.09265528619289398, acc: 0.9760403633117676)
[2025-02-13 02:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49][root][INFO] - Training Epoch: 1/2, step 3600/7134 completed (loss: 0.06421954929828644, acc: 0.9876543283462524)
[2025-02-13 02:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50][root][INFO] - Training Epoch: 1/2, step 3601/7134 completed (loss: 0.156468003988266, acc: 0.9613792896270752)
[2025-02-13 02:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50][root][INFO] - Training Epoch: 1/2, step 3602/7134 completed (loss: 0.12593737244606018, acc: 0.9708284735679626)
[2025-02-13 02:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51][root][INFO] - Training Epoch: 1/2, step 3603/7134 completed (loss: 0.0426853783428669, acc: 0.987500011920929)
[2025-02-13 02:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51][root][INFO] - Training Epoch: 1/2, step 3604/7134 completed (loss: 0.05845976248383522, acc: 0.9812382459640503)
[2025-02-13 02:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52][root][INFO] - Training Epoch: 1/2, step 3605/7134 completed (loss: 0.08683203160762787, acc: 0.9741200804710388)
[2025-02-13 02:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52][root][INFO] - Training Epoch: 1/2, step 3606/7134 completed (loss: 0.07510124146938324, acc: 0.9822379946708679)
[2025-02-13 02:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52][root][INFO] - Training Epoch: 1/2, step 3607/7134 completed (loss: 0.04983721300959587, acc: 0.9798657894134521)
[2025-02-13 02:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:53][root][INFO] - Training Epoch: 1/2, step 3608/7134 completed (loss: 0.11759304255247116, acc: 0.9745575189590454)
[2025-02-13 02:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:53][root][INFO] - Training Epoch: 1/2, step 3609/7134 completed (loss: 0.0560680590569973, acc: 0.9847715497016907)
[2025-02-13 02:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54][root][INFO] - Training Epoch: 1/2, step 3610/7134 completed (loss: 0.11823046952486038, acc: 0.9610389471054077)
[2025-02-13 02:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54][root][INFO] - Training Epoch: 1/2, step 3611/7134 completed (loss: 0.09195351600646973, acc: 0.975806474685669)
[2025-02-13 02:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:55][root][INFO] - Training Epoch: 1/2, step 3612/7134 completed (loss: 0.13727524876594543, acc: 0.9688796401023865)
[2025-02-13 02:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:55][root][INFO] - Training Epoch: 1/2, step 3613/7134 completed (loss: 0.08864881098270416, acc: 0.9720101952552795)
[2025-02-13 02:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][root][INFO] - Training Epoch: 1/2, step 3614/7134 completed (loss: 0.07000015676021576, acc: 0.9718875288963318)
[2025-02-13 02:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][root][INFO] - Training Epoch: 1/2, step 3615/7134 completed (loss: 0.036339085549116135, acc: 0.9877675771713257)
[2025-02-13 02:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][root][INFO] - Training Epoch: 1/2, step 3616/7134 completed (loss: 0.06435280293226242, acc: 0.9795918464660645)
[2025-02-13 02:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:57][root][INFO] - Training Epoch: 1/2, step 3617/7134 completed (loss: 0.07804417610168457, acc: 0.9764851331710815)
[2025-02-13 02:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:57][root][INFO] - Training Epoch: 1/2, step 3618/7134 completed (loss: 0.1564219444990158, acc: 0.9493464231491089)
[2025-02-13 02:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:58][root][INFO] - Training Epoch: 1/2, step 3619/7134 completed (loss: 0.1143123060464859, acc: 0.961904764175415)
[2025-02-13 02:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:58][root][INFO] - Training Epoch: 1/2, step 3620/7134 completed (loss: 0.0826425552368164, acc: 0.9690576791763306)
[2025-02-13 02:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59][root][INFO] - Training Epoch: 1/2, step 3621/7134 completed (loss: 0.05968885496258736, acc: 0.9819694757461548)
[2025-02-13 02:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59][root][INFO] - Training Epoch: 1/2, step 3622/7134 completed (loss: 0.04970121383666992, acc: 0.9819819927215576)
[2025-02-13 02:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59][root][INFO] - Training Epoch: 1/2, step 3623/7134 completed (loss: 0.04486257955431938, acc: 0.9851301312446594)
[2025-02-13 02:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:00][root][INFO] - Training Epoch: 1/2, step 3624/7134 completed (loss: 0.07253853231668472, acc: 0.9783333539962769)
[2025-02-13 02:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:00][root][INFO] - Training Epoch: 1/2, step 3625/7134 completed (loss: 0.07395526766777039, acc: 0.9778671860694885)
[2025-02-13 02:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01][root][INFO] - Training Epoch: 1/2, step 3626/7134 completed (loss: 0.2268841713666916, acc: 0.9466666579246521)
[2025-02-13 02:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01][root][INFO] - Training Epoch: 1/2, step 3627/7134 completed (loss: 0.13577406108379364, acc: 0.9692671298980713)
[2025-02-13 02:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01][root][INFO] - Training Epoch: 1/2, step 3628/7134 completed (loss: 0.16050422191619873, acc: 0.9659090638160706)
[2025-02-13 02:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02][root][INFO] - Training Epoch: 1/2, step 3629/7134 completed (loss: 0.15526588261127472, acc: 0.957317054271698)
[2025-02-13 02:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02][root][INFO] - Training Epoch: 1/2, step 3630/7134 completed (loss: 0.1259458363056183, acc: 0.9638242721557617)
[2025-02-13 02:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02][root][INFO] - Training Epoch: 1/2, step 3631/7134 completed (loss: 0.12253441661596298, acc: 0.9672130942344666)
[2025-02-13 02:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:03][root][INFO] - Training Epoch: 1/2, step 3632/7134 completed (loss: 0.10078484565019608, acc: 0.9758551120758057)
[2025-02-13 02:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:03][root][INFO] - Training Epoch: 1/2, step 3633/7134 completed (loss: 0.10836393386125565, acc: 0.9784792065620422)
[2025-02-13 02:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04][root][INFO] - Training Epoch: 1/2, step 3634/7134 completed (loss: 0.12105415761470795, acc: 0.97773277759552)
[2025-02-13 02:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04][root][INFO] - Training Epoch: 1/2, step 3635/7134 completed (loss: 0.049485787749290466, acc: 0.9856630563735962)
[2025-02-13 02:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04][root][INFO] - Training Epoch: 1/2, step 3636/7134 completed (loss: 0.05231757462024689, acc: 0.9862204790115356)
[2025-02-13 02:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:05][root][INFO] - Training Epoch: 1/2, step 3637/7134 completed (loss: 0.058816805481910706, acc: 0.98591548204422)
[2025-02-13 02:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:05][root][INFO] - Training Epoch: 1/2, step 3638/7134 completed (loss: 0.05308759957551956, acc: 0.9788235425949097)
[2025-02-13 02:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06][root][INFO] - Training Epoch: 1/2, step 3639/7134 completed (loss: 0.04236428067088127, acc: 0.9864864945411682)
[2025-02-13 02:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06][root][INFO] - Training Epoch: 1/2, step 3640/7134 completed (loss: 0.054972946643829346, acc: 0.982824444770813)
[2025-02-13 02:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06][root][INFO] - Training Epoch: 1/2, step 3641/7134 completed (loss: 0.0456508994102478, acc: 0.9883268475532532)
[2025-02-13 02:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07][root][INFO] - Training Epoch: 1/2, step 3642/7134 completed (loss: 0.04303893819451332, acc: 0.9907264113426208)
[2025-02-13 02:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07][root][INFO] - Training Epoch: 1/2, step 3643/7134 completed (loss: 0.04599545896053314, acc: 0.9896907210350037)
[2025-02-13 02:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08][root][INFO] - Training Epoch: 1/2, step 3644/7134 completed (loss: 0.06866943836212158, acc: 0.9821138381958008)
[2025-02-13 02:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08][root][INFO] - Training Epoch: 1/2, step 3645/7134 completed (loss: 0.09290286153554916, acc: 0.9823943376541138)
[2025-02-13 02:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08][root][INFO] - Training Epoch: 1/2, step 3646/7134 completed (loss: 0.14831538498401642, acc: 0.9652777910232544)
[2025-02-13 02:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:09][root][INFO] - Training Epoch: 1/2, step 3647/7134 completed (loss: 0.029061218723654747, acc: 0.9922480583190918)
[2025-02-13 02:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:09][root][INFO] - Training Epoch: 1/2, step 3648/7134 completed (loss: 0.10367745161056519, acc: 0.9711934328079224)
[2025-02-13 02:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10][root][INFO] - Training Epoch: 1/2, step 3649/7134 completed (loss: 0.12411519140005112, acc: 0.970588207244873)
[2025-02-13 02:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10][root][INFO] - Training Epoch: 1/2, step 3650/7134 completed (loss: 0.09173690527677536, acc: 0.97124183177948)
[2025-02-13 02:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11][root][INFO] - Training Epoch: 1/2, step 3651/7134 completed (loss: 0.09029704332351685, acc: 0.9772117733955383)
[2025-02-13 02:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11][root][INFO] - Training Epoch: 1/2, step 3652/7134 completed (loss: 0.08849649876356125, acc: 0.9758672714233398)
[2025-02-13 02:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11][root][INFO] - Training Epoch: 1/2, step 3653/7134 completed (loss: 0.11226597428321838, acc: 0.9698340892791748)
[2025-02-13 02:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:12][root][INFO] - Training Epoch: 1/2, step 3654/7134 completed (loss: 0.0356975682079792, acc: 0.9930915236473083)
[2025-02-13 02:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:12][root][INFO] - Training Epoch: 1/2, step 3655/7134 completed (loss: 0.07055700570344925, acc: 0.9821428656578064)
[2025-02-13 02:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13][root][INFO] - Training Epoch: 1/2, step 3656/7134 completed (loss: 0.10441315919160843, acc: 0.9709091186523438)
[2025-02-13 02:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13][root][INFO] - Training Epoch: 1/2, step 3657/7134 completed (loss: 0.025786494836211205, acc: 0.9919871687889099)
[2025-02-13 02:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13][root][INFO] - Training Epoch: 1/2, step 3658/7134 completed (loss: 0.06262364983558655, acc: 0.9844236969947815)
[2025-02-13 02:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14][root][INFO] - Training Epoch: 1/2, step 3659/7134 completed (loss: 0.04431238770484924, acc: 0.9850522875785828)
[2025-02-13 02:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14][root][INFO] - Training Epoch: 1/2, step 3660/7134 completed (loss: 0.08040158450603485, acc: 0.980327844619751)
[2025-02-13 02:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14][root][INFO] - Training Epoch: 1/2, step 3661/7134 completed (loss: 0.05278356745839119, acc: 0.9857142567634583)
[2025-02-13 02:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:15][root][INFO] - Training Epoch: 1/2, step 3662/7134 completed (loss: 0.10318339616060257, acc: 0.9756097793579102)
[2025-02-13 02:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:15][root][INFO] - Training Epoch: 1/2, step 3663/7134 completed (loss: 0.06764334440231323, acc: 0.9835164546966553)
[2025-02-13 02:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16][root][INFO] - Training Epoch: 1/2, step 3664/7134 completed (loss: 0.07788874953985214, acc: 0.9908814430236816)
[2025-02-13 02:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16][root][INFO] - Training Epoch: 1/2, step 3665/7134 completed (loss: 0.049994081258773804, acc: 0.9868637323379517)
[2025-02-13 02:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16][root][INFO] - Training Epoch: 1/2, step 3666/7134 completed (loss: 0.07016441971063614, acc: 0.9762773513793945)
[2025-02-13 02:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17][root][INFO] - Training Epoch: 1/2, step 3667/7134 completed (loss: 0.1953091323375702, acc: 0.9544419050216675)
[2025-02-13 02:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17][root][INFO] - Training Epoch: 1/2, step 3668/7134 completed (loss: 0.05445992201566696, acc: 0.9912739992141724)
[2025-02-13 02:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18][root][INFO] - Training Epoch: 1/2, step 3669/7134 completed (loss: 0.031152667477726936, acc: 0.9886578321456909)
[2025-02-13 02:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18][root][INFO] - Training Epoch: 1/2, step 3670/7134 completed (loss: 0.03233049064874649, acc: 0.9945054650306702)
[2025-02-13 02:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18][root][INFO] - Training Epoch: 1/2, step 3671/7134 completed (loss: 0.0467655248939991, acc: 0.9841040372848511)
[2025-02-13 02:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:19][root][INFO] - Training Epoch: 1/2, step 3672/7134 completed (loss: 0.033228617161512375, acc: 0.993686854839325)
[2025-02-13 02:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:19][root][INFO] - Training Epoch: 1/2, step 3673/7134 completed (loss: 0.07336578518152237, acc: 0.9832776188850403)
[2025-02-13 02:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20][root][INFO] - Training Epoch: 1/2, step 3674/7134 completed (loss: 0.033066511154174805, acc: 0.9905511736869812)
[2025-02-13 02:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20][root][INFO] - Training Epoch: 1/2, step 3675/7134 completed (loss: 0.03535868972539902, acc: 0.9923469424247742)
[2025-02-13 02:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20][root][INFO] - Training Epoch: 1/2, step 3676/7134 completed (loss: 0.03684959560632706, acc: 0.9883381724357605)
[2025-02-13 02:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:21][root][INFO] - Training Epoch: 1/2, step 3677/7134 completed (loss: 0.04051906242966652, acc: 0.9876712560653687)
[2025-02-13 02:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:21][root][INFO] - Training Epoch: 1/2, step 3678/7134 completed (loss: 0.05337075516581535, acc: 0.9838056564331055)
[2025-02-13 02:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22][root][INFO] - Training Epoch: 1/2, step 3679/7134 completed (loss: 0.03858770430088043, acc: 0.9950413107872009)
[2025-02-13 02:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22][root][INFO] - Training Epoch: 1/2, step 3680/7134 completed (loss: 0.07094091922044754, acc: 0.9821428656578064)
[2025-02-13 02:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23][root][INFO] - Training Epoch: 1/2, step 3681/7134 completed (loss: 0.043662797659635544, acc: 0.9846368432044983)
[2025-02-13 02:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23][root][INFO] - Training Epoch: 1/2, step 3682/7134 completed (loss: 0.037579990923404694, acc: 0.990777313709259)
[2025-02-13 02:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24][root][INFO] - Training Epoch: 1/2, step 3683/7134 completed (loss: 0.035946279764175415, acc: 0.9905511736869812)
[2025-02-13 02:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24][root][INFO] - Training Epoch: 1/2, step 3684/7134 completed (loss: 0.050420790910720825, acc: 0.9884560108184814)
[2025-02-13 02:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24][root][INFO] - Training Epoch: 1/2, step 3685/7134 completed (loss: 0.02960021421313286, acc: 0.9865125417709351)
[2025-02-13 02:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25][root][INFO] - Training Epoch: 1/2, step 3686/7134 completed (loss: 0.023638401180505753, acc: 0.995502233505249)
[2025-02-13 02:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25][root][INFO] - Training Epoch: 1/2, step 3687/7134 completed (loss: 0.026597002521157265, acc: 0.9923076629638672)
[2025-02-13 02:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26][root][INFO] - Training Epoch: 1/2, step 3688/7134 completed (loss: 0.014612731523811817, acc: 0.9979715943336487)
[2025-02-13 02:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26][root][INFO] - Training Epoch: 1/2, step 3689/7134 completed (loss: 0.02445107139647007, acc: 0.9901960492134094)
[2025-02-13 02:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26][root][INFO] - Training Epoch: 1/2, step 3690/7134 completed (loss: 0.02993319183588028, acc: 0.9885550737380981)
[2025-02-13 02:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:27][root][INFO] - Training Epoch: 1/2, step 3691/7134 completed (loss: 0.04597479850053787, acc: 0.985111653804779)
[2025-02-13 02:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:27][root][INFO] - Training Epoch: 1/2, step 3692/7134 completed (loss: 0.05738535523414612, acc: 0.978691041469574)
[2025-02-13 02:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28][root][INFO] - Training Epoch: 1/2, step 3693/7134 completed (loss: 0.04065852612257004, acc: 0.9810725450515747)
[2025-02-13 02:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28][root][INFO] - Training Epoch: 1/2, step 3694/7134 completed (loss: 0.03475603088736534, acc: 0.9928876161575317)
[2025-02-13 02:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28][root][INFO] - Training Epoch: 1/2, step 3695/7134 completed (loss: 0.04973920062184334, acc: 0.9858356714248657)
[2025-02-13 02:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:29][root][INFO] - Training Epoch: 1/2, step 3696/7134 completed (loss: 0.03905165567994118, acc: 0.9884318709373474)
[2025-02-13 02:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:29][root][INFO] - Training Epoch: 1/2, step 3697/7134 completed (loss: 0.019143851473927498, acc: 0.9927113652229309)
[2025-02-13 02:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30][root][INFO] - Training Epoch: 1/2, step 3698/7134 completed (loss: 0.04565422609448433, acc: 0.9855538010597229)
[2025-02-13 02:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30][root][INFO] - Training Epoch: 1/2, step 3699/7134 completed (loss: 0.01082685124129057, acc: 0.9966216087341309)
[2025-02-13 02:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31][root][INFO] - Training Epoch: 1/2, step 3700/7134 completed (loss: 0.040950365364551544, acc: 0.9883381724357605)
[2025-02-13 02:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31][root][INFO] - Training Epoch: 1/2, step 3701/7134 completed (loss: 0.05675385147333145, acc: 0.9900662302970886)
[2025-02-13 02:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31][root][INFO] - Training Epoch: 1/2, step 3702/7134 completed (loss: 0.05183079466223717, acc: 0.9862259030342102)
[2025-02-13 02:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32][root][INFO] - Training Epoch: 1/2, step 3703/7134 completed (loss: 0.0372745618224144, acc: 0.9910600185394287)
[2025-02-13 02:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32][root][INFO] - Training Epoch: 1/2, step 3704/7134 completed (loss: 0.021691132336854935, acc: 0.991631805896759)
[2025-02-13 02:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33][root][INFO] - Training Epoch: 1/2, step 3705/7134 completed (loss: 0.07737758755683899, acc: 0.9824841022491455)
[2025-02-13 02:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33][root][INFO] - Training Epoch: 1/2, step 3706/7134 completed (loss: 0.11758523434400558, acc: 0.9766297936439514)
[2025-02-13 02:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33][root][INFO] - Training Epoch: 1/2, step 3707/7134 completed (loss: 0.02423524297773838, acc: 0.9873772859573364)
[2025-02-13 02:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:34][root][INFO] - Training Epoch: 1/2, step 3708/7134 completed (loss: 0.06845994293689728, acc: 0.9809523820877075)
[2025-02-13 02:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:34][root][INFO] - Training Epoch: 1/2, step 3709/7134 completed (loss: 0.03809528797864914, acc: 0.9868153929710388)
[2025-02-13 02:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:35][root][INFO] - Training Epoch: 1/2, step 3710/7134 completed (loss: 0.06138192117214203, acc: 0.9784836173057556)
[2025-02-13 02:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:35][root][INFO] - Training Epoch: 1/2, step 3711/7134 completed (loss: 0.05600902438163757, acc: 0.9826302528381348)
[2025-02-13 02:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:36][root][INFO] - Training Epoch: 1/2, step 3712/7134 completed (loss: 0.05809580162167549, acc: 0.9844683408737183)
[2025-02-13 02:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:36][root][INFO] - Training Epoch: 1/2, step 3713/7134 completed (loss: 0.05192684009671211, acc: 0.9868637323379517)
[2025-02-13 02:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37][root][INFO] - Training Epoch: 1/2, step 3714/7134 completed (loss: 0.05475025624036789, acc: 0.988950252532959)
[2025-02-13 02:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37][root][INFO] - Training Epoch: 1/2, step 3715/7134 completed (loss: 0.03635847568511963, acc: 0.987908124923706)
[2025-02-13 02:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37][root][INFO] - Training Epoch: 1/2, step 3716/7134 completed (loss: 0.05794883519411087, acc: 0.9790382385253906)
[2025-02-13 02:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38][root][INFO] - Training Epoch: 1/2, step 3717/7134 completed (loss: 0.04084880277514458, acc: 0.9890710115432739)
[2025-02-13 02:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38][root][INFO] - Training Epoch: 1/2, step 3718/7134 completed (loss: 0.06320925056934357, acc: 0.9824150204658508)
[2025-02-13 02:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39][root][INFO] - Training Epoch: 1/2, step 3719/7134 completed (loss: 0.07171257585287094, acc: 0.9838235378265381)
[2025-02-13 02:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39][root][INFO] - Training Epoch: 1/2, step 3720/7134 completed (loss: 0.02601785399019718, acc: 0.992668628692627)
[2025-02-13 02:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:40][root][INFO] - Training Epoch: 1/2, step 3721/7134 completed (loss: 0.049973443150520325, acc: 0.9853528738021851)
[2025-02-13 02:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:40][root][INFO] - Training Epoch: 1/2, step 3722/7134 completed (loss: 0.03591827675700188, acc: 0.988916277885437)
[2025-02-13 02:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41][root][INFO] - Training Epoch: 1/2, step 3723/7134 completed (loss: 0.06214792653918266, acc: 0.9827337861061096)
[2025-02-13 02:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41][root][INFO] - Training Epoch: 1/2, step 3724/7134 completed (loss: 0.05859211087226868, acc: 0.977011501789093)
[2025-02-13 02:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41][root][INFO] - Training Epoch: 1/2, step 3725/7134 completed (loss: 0.11128008365631104, acc: 0.9719763994216919)
[2025-02-13 02:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:42][root][INFO] - Training Epoch: 1/2, step 3726/7134 completed (loss: 0.04937324300408363, acc: 0.9904357194900513)
[2025-02-13 02:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:42][root][INFO] - Training Epoch: 1/2, step 3727/7134 completed (loss: 0.03237605839967728, acc: 0.9933244585990906)
[2025-02-13 02:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:43][root][INFO] - Training Epoch: 1/2, step 3728/7134 completed (loss: 0.017535489052534103, acc: 0.9954904317855835)
[2025-02-13 02:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:43][root][INFO] - Training Epoch: 1/2, step 3729/7134 completed (loss: 0.03908228129148483, acc: 0.9897727370262146)
[2025-02-13 02:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44][root][INFO] - Training Epoch: 1/2, step 3730/7134 completed (loss: 0.043769508600234985, acc: 0.9891451597213745)
[2025-02-13 02:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44][root][INFO] - Training Epoch: 1/2, step 3731/7134 completed (loss: 0.027115333825349808, acc: 0.993773341178894)
[2025-02-13 02:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44][root][INFO] - Training Epoch: 1/2, step 3732/7134 completed (loss: 0.02214295230805874, acc: 0.9920544624328613)
[2025-02-13 02:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45][root][INFO] - Training Epoch: 1/2, step 3733/7134 completed (loss: 0.07218144834041595, acc: 0.9775596261024475)
[2025-02-13 02:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45][root][INFO] - Training Epoch: 1/2, step 3734/7134 completed (loss: 0.1154264435172081, acc: 0.9644013047218323)
[2025-02-13 02:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:46][root][INFO] - Training Epoch: 1/2, step 3735/7134 completed (loss: 0.062344443053007126, acc: 0.9789156913757324)
[2025-02-13 02:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:46][root][INFO] - Training Epoch: 1/2, step 3736/7134 completed (loss: 0.1246088296175003, acc: 0.9634782671928406)
[2025-02-13 02:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47][root][INFO] - Training Epoch: 1/2, step 3737/7134 completed (loss: 0.07377078384160995, acc: 0.9758672714233398)
[2025-02-13 02:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47][root][INFO] - Training Epoch: 1/2, step 3738/7134 completed (loss: 0.050244465470314026, acc: 0.9884488582611084)
[2025-02-13 02:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47][root][INFO] - Training Epoch: 1/2, step 3739/7134 completed (loss: 0.15355852246284485, acc: 0.9606741666793823)
[2025-02-13 02:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48][root][INFO] - Training Epoch: 1/2, step 3740/7134 completed (loss: 0.07265427708625793, acc: 0.9788960814476013)
[2025-02-13 02:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48][root][INFO] - Training Epoch: 1/2, step 3741/7134 completed (loss: 0.0893857330083847, acc: 0.9786019921302795)
[2025-02-13 02:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48][root][INFO] - Training Epoch: 1/2, step 3742/7134 completed (loss: 0.06224449351429939, acc: 0.9829059839248657)
[2025-02-13 02:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49][root][INFO] - Training Epoch: 1/2, step 3743/7134 completed (loss: 0.08766255527734756, acc: 0.9719763994216919)
[2025-02-13 02:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49][root][INFO] - Training Epoch: 1/2, step 3744/7134 completed (loss: 0.11431422084569931, acc: 0.9657443761825562)
[2025-02-13 02:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:50][root][INFO] - Training Epoch: 1/2, step 3745/7134 completed (loss: 0.1100250855088234, acc: 0.9605568647384644)
[2025-02-13 02:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:50][root][INFO] - Training Epoch: 1/2, step 3746/7134 completed (loss: 0.09462746232748032, acc: 0.9698953032493591)
[2025-02-13 02:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51][root][INFO] - Training Epoch: 1/2, step 3747/7134 completed (loss: 0.06368552893400192, acc: 0.9854227304458618)
[2025-02-13 02:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51][root][INFO] - Training Epoch: 1/2, step 3748/7134 completed (loss: 0.0849418044090271, acc: 0.9788732528686523)
[2025-02-13 02:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51][root][INFO] - Training Epoch: 1/2, step 3749/7134 completed (loss: 0.10203338414430618, acc: 0.9745127558708191)
[2025-02-13 02:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52][root][INFO] - Training Epoch: 1/2, step 3750/7134 completed (loss: 0.04142684489488602, acc: 0.9832636117935181)
[2025-02-13 02:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52][root][INFO] - Training Epoch: 1/2, step 3751/7134 completed (loss: 0.08431702107191086, acc: 0.9701279997825623)
[2025-02-13 02:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53][root][INFO] - Training Epoch: 1/2, step 3752/7134 completed (loss: 0.07754135131835938, acc: 0.9791044592857361)
[2025-02-13 02:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53][root][INFO] - Training Epoch: 1/2, step 3753/7134 completed (loss: 0.11110219359397888, acc: 0.9626308083534241)
[2025-02-13 02:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53][root][INFO] - Training Epoch: 1/2, step 3754/7134 completed (loss: 0.04611583054065704, acc: 0.9848254919052124)
[2025-02-13 02:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:54][root][INFO] - Training Epoch: 1/2, step 3755/7134 completed (loss: 0.05543949827551842, acc: 0.9870129823684692)
[2025-02-13 02:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:54][root][INFO] - Training Epoch: 1/2, step 3756/7134 completed (loss: 0.07264377921819687, acc: 0.9833794832229614)
[2025-02-13 02:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][root][INFO] - Training Epoch: 1/2, step 3757/7134 completed (loss: 0.11026203632354736, acc: 0.9638752341270447)
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][root][INFO] - Training Epoch: 1/2, step 3758/7134 completed (loss: 0.059025414288043976, acc: 0.9855855703353882)
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][root][INFO] - Training Epoch: 1/2, step 3759/7134 completed (loss: 0.10901005566120148, acc: 0.9679054021835327)
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:56][root][INFO] - Training Epoch: 1/2, step 3760/7134 completed (loss: 0.086147241294384, acc: 0.9774011373519897)
[2025-02-13 02:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:56][root][INFO] - Training Epoch: 1/2, step 3761/7134 completed (loss: 0.05763380229473114, acc: 0.984544038772583)
[2025-02-13 02:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57][root][INFO] - Training Epoch: 1/2, step 3762/7134 completed (loss: 0.051399245858192444, acc: 0.9862448573112488)
[2025-02-13 02:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57][root][INFO] - Training Epoch: 1/2, step 3763/7134 completed (loss: 0.04223832115530968, acc: 0.9876543283462524)
[2025-02-13 02:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57][root][INFO] - Training Epoch: 1/2, step 3764/7134 completed (loss: 0.07243961095809937, acc: 0.9822161197662354)
[2025-02-13 02:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58][root][INFO] - Training Epoch: 1/2, step 3765/7134 completed (loss: 0.047556668519973755, acc: 0.9843937754631042)
[2025-02-13 02:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58][root][INFO] - Training Epoch: 1/2, step 3766/7134 completed (loss: 0.09487704932689667, acc: 0.9800570011138916)
[2025-02-13 02:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59][root][INFO] - Training Epoch: 1/2, step 3767/7134 completed (loss: 0.029340451583266258, acc: 0.9900249242782593)
[2025-02-13 02:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59][root][INFO] - Training Epoch: 1/2, step 3768/7134 completed (loss: 0.041840676218271255, acc: 0.9875518679618835)
[2025-02-13 02:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00][root][INFO] - Training Epoch: 1/2, step 3769/7134 completed (loss: 0.042008254677057266, acc: 0.984886646270752)
[2025-02-13 02:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00][root][INFO] - Training Epoch: 1/2, step 3770/7134 completed (loss: 0.053862858563661575, acc: 0.9844632744789124)
[2025-02-13 02:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00][root][INFO] - Training Epoch: 1/2, step 3771/7134 completed (loss: 0.040919579565525055, acc: 0.9883117079734802)
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01][root][INFO] - Training Epoch: 1/2, step 3772/7134 completed (loss: 0.07877088338136673, acc: 0.9730941653251648)
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01][root][INFO] - Training Epoch: 1/2, step 3773/7134 completed (loss: 0.050101201981306076, acc: 0.9888392686843872)
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02][root][INFO] - Training Epoch: 1/2, step 3774/7134 completed (loss: 0.05976022034883499, acc: 0.9802784323692322)
[2025-02-13 02:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02][root][INFO] - Training Epoch: 1/2, step 3775/7134 completed (loss: 0.03616444393992424, acc: 0.9914039969444275)
[2025-02-13 02:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:03][root][INFO] - Training Epoch: 1/2, step 3776/7134 completed (loss: 0.06409450620412827, acc: 0.9822221994400024)
[2025-02-13 02:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:03][root][INFO] - Training Epoch: 1/2, step 3777/7134 completed (loss: 0.043598998337984085, acc: 0.9874686598777771)
[2025-02-13 02:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04][root][INFO] - Training Epoch: 1/2, step 3778/7134 completed (loss: 0.08401001989841461, acc: 0.9800853729248047)
[2025-02-13 02:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04][root][INFO] - Training Epoch: 1/2, step 3779/7134 completed (loss: 0.036248985677957535, acc: 0.9886040091514587)
[2025-02-13 02:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04][root][INFO] - Training Epoch: 1/2, step 3780/7134 completed (loss: 0.037242528051137924, acc: 0.9855907559394836)
[2025-02-13 02:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05][root][INFO] - Training Epoch: 1/2, step 3781/7134 completed (loss: 0.06503051519393921, acc: 0.9835391044616699)
[2025-02-13 02:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05][root][INFO] - Training Epoch: 1/2, step 3782/7134 completed (loss: 0.06049628555774689, acc: 0.9800570011138916)
[2025-02-13 02:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06][root][INFO] - Training Epoch: 1/2, step 3783/7134 completed (loss: 0.05963291600346565, acc: 0.9819355010986328)
[2025-02-13 02:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06][root][INFO] - Training Epoch: 1/2, step 3784/7134 completed (loss: 0.06268659234046936, acc: 0.989180862903595)
[2025-02-13 02:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06][root][INFO] - Training Epoch: 1/2, step 3785/7134 completed (loss: 0.01989010162651539, acc: 0.9921996593475342)
[2025-02-13 02:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07][root][INFO] - Training Epoch: 1/2, step 3786/7134 completed (loss: 0.06829292327165604, acc: 0.9818887710571289)
[2025-02-13 02:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07][root][INFO] - Training Epoch: 1/2, step 3787/7134 completed (loss: 0.057744100689888, acc: 0.9872408509254456)
[2025-02-13 02:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08][root][INFO] - Training Epoch: 1/2, step 3788/7134 completed (loss: 0.052341435104608536, acc: 0.9851552248001099)
[2025-02-13 02:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08][root][INFO] - Training Epoch: 1/2, step 3789/7134 completed (loss: 0.06944704055786133, acc: 0.9746835231781006)
[2025-02-13 02:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08][root][INFO] - Training Epoch: 1/2, step 3790/7134 completed (loss: 0.11265504360198975, acc: 0.9733840227127075)
[2025-02-13 02:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09][root][INFO] - Training Epoch: 1/2, step 3791/7134 completed (loss: 0.048539478331804276, acc: 0.9883871078491211)
[2025-02-13 02:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09][root][INFO] - Training Epoch: 1/2, step 3792/7134 completed (loss: 0.045880213379859924, acc: 0.9802955389022827)
[2025-02-13 02:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:10][root][INFO] - Training Epoch: 1/2, step 3793/7134 completed (loss: 0.059518154710531235, acc: 0.9851351380348206)
[2025-02-13 02:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:10][root][INFO] - Training Epoch: 1/2, step 3794/7134 completed (loss: 0.052815813571214676, acc: 0.9857327938079834)
[2025-02-13 02:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][root][INFO] - Training Epoch: 1/2, step 3795/7134 completed (loss: 0.05665110424160957, acc: 0.983132541179657)
[2025-02-13 02:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][root][INFO] - Training Epoch: 1/2, step 3796/7134 completed (loss: 0.04964751750230789, acc: 0.983668327331543)
[2025-02-13 02:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][root][INFO] - Training Epoch: 1/2, step 3797/7134 completed (loss: 0.03216593340039253, acc: 0.9895697236061096)
[2025-02-13 02:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12][root][INFO] - Training Epoch: 1/2, step 3798/7134 completed (loss: 0.07285585254430771, acc: 0.9841954112052917)
[2025-02-13 02:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12][root][INFO] - Training Epoch: 1/2, step 3799/7134 completed (loss: 0.06734766811132431, acc: 0.9755469560623169)
[2025-02-13 02:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:13][root][INFO] - Training Epoch: 1/2, step 3800/7134 completed (loss: 0.08127834647893906, acc: 0.9785932898521423)
[2025-02-13 02:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:13][root][INFO] - Training Epoch: 1/2, step 3801/7134 completed (loss: 0.03803747519850731, acc: 0.9876543283462524)
[2025-02-13 02:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14][root][INFO] - Training Epoch: 1/2, step 3802/7134 completed (loss: 0.04335545003414154, acc: 0.9881796836853027)
[2025-02-13 02:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14][root][INFO] - Training Epoch: 1/2, step 3803/7134 completed (loss: 0.06708645075559616, acc: 0.9789915680885315)
[2025-02-13 02:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14][root][INFO] - Training Epoch: 1/2, step 3804/7134 completed (loss: 0.04279168322682381, acc: 0.9860031008720398)
[2025-02-13 02:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15][root][INFO] - Training Epoch: 1/2, step 3805/7134 completed (loss: 0.028814101591706276, acc: 0.9895287752151489)
[2025-02-13 02:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15][root][INFO] - Training Epoch: 1/2, step 3806/7134 completed (loss: 0.061691321432590485, acc: 0.9802761077880859)
[2025-02-13 02:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16][root][INFO] - Training Epoch: 1/2, step 3807/7134 completed (loss: 0.019849035888910294, acc: 0.9970760345458984)
[2025-02-13 02:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16][root][INFO] - Training Epoch: 1/2, step 3808/7134 completed (loss: 0.04411899670958519, acc: 0.9873417615890503)
[2025-02-13 02:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17][root][INFO] - Training Epoch: 1/2, step 3809/7134 completed (loss: 0.08060409128665924, acc: 0.9771573543548584)
[2025-02-13 02:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17][root][INFO] - Training Epoch: 1/2, step 3810/7134 completed (loss: 0.04685826599597931, acc: 0.9821428656578064)
[2025-02-13 02:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17][root][INFO] - Training Epoch: 1/2, step 3811/7134 completed (loss: 0.06543909758329391, acc: 0.9806896448135376)
[2025-02-13 02:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18][root][INFO] - Training Epoch: 1/2, step 3812/7134 completed (loss: 0.07825677841901779, acc: 0.9811320900917053)
[2025-02-13 02:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18][root][INFO] - Training Epoch: 1/2, step 3813/7134 completed (loss: 0.07882381975650787, acc: 0.9762532711029053)
[2025-02-13 02:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19][root][INFO] - Training Epoch: 1/2, step 3814/7134 completed (loss: 0.031263552606105804, acc: 0.9906166195869446)
[2025-02-13 02:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19][root][INFO] - Training Epoch: 1/2, step 3815/7134 completed (loss: 0.055849604308605194, acc: 0.9807407259941101)
[2025-02-13 02:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19][root][INFO] - Training Epoch: 1/2, step 3816/7134 completed (loss: 0.04194406792521477, acc: 0.9873816967010498)
[2025-02-13 02:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20][root][INFO] - Training Epoch: 1/2, step 3817/7134 completed (loss: 0.08215484023094177, acc: 0.9784172773361206)
[2025-02-13 02:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20][root][INFO] - Training Epoch: 1/2, step 3818/7134 completed (loss: 0.10533298552036285, acc: 0.9818181991577148)
[2025-02-13 02:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21][root][INFO] - Training Epoch: 1/2, step 3819/7134 completed (loss: 0.07241108268499374, acc: 0.9759036302566528)
[2025-02-13 02:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21][root][INFO] - Training Epoch: 1/2, step 3820/7134 completed (loss: 0.07341429591178894, acc: 0.9691211581230164)
[2025-02-13 02:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21][root][INFO] - Training Epoch: 1/2, step 3821/7134 completed (loss: 0.050010137259960175, acc: 0.9813874959945679)
[2025-02-13 02:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:22][root][INFO] - Training Epoch: 1/2, step 3822/7134 completed (loss: 0.06736333668231964, acc: 0.9783890247344971)
[2025-02-13 02:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:22][root][INFO] - Training Epoch: 1/2, step 3823/7134 completed (loss: 0.04263549670577049, acc: 0.9820193648338318)
[2025-02-13 02:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23][root][INFO] - Training Epoch: 1/2, step 3824/7134 completed (loss: 0.09353847056627274, acc: 0.9730185270309448)
[2025-02-13 02:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23][root][INFO] - Training Epoch: 1/2, step 3825/7134 completed (loss: 0.04689982533454895, acc: 0.9820359349250793)
[2025-02-13 02:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23][root][INFO] - Training Epoch: 1/2, step 3826/7134 completed (loss: 0.05592501536011696, acc: 0.9808823466300964)
[2025-02-13 02:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24][root][INFO] - Training Epoch: 1/2, step 3827/7134 completed (loss: 0.0518646240234375, acc: 0.9887955188751221)
[2025-02-13 02:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24][root][INFO] - Training Epoch: 1/2, step 3828/7134 completed (loss: 0.07012426853179932, acc: 0.9761273264884949)
[2025-02-13 02:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:25][root][INFO] - Training Epoch: 1/2, step 3829/7134 completed (loss: 0.040166810154914856, acc: 0.9909502267837524)
[2025-02-13 02:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:25][root][INFO] - Training Epoch: 1/2, step 3830/7134 completed (loss: 0.06955690681934357, acc: 0.979899525642395)
[2025-02-13 02:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26][root][INFO] - Training Epoch: 1/2, step 3831/7134 completed (loss: 0.04996654763817787, acc: 0.9879879951477051)
[2025-02-13 02:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26][root][INFO] - Training Epoch: 1/2, step 3832/7134 completed (loss: 0.051558490842580795, acc: 0.990176796913147)
[2025-02-13 02:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26][root][INFO] - Training Epoch: 1/2, step 3833/7134 completed (loss: 0.032015204429626465, acc: 0.9913366436958313)
[2025-02-13 02:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27][root][INFO] - Training Epoch: 1/2, step 3834/7134 completed (loss: 0.012059648521244526, acc: 0.998701274394989)
[2025-02-13 02:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27][root][INFO] - Training Epoch: 1/2, step 3835/7134 completed (loss: 0.015388806350529194, acc: 0.9939320683479309)
[2025-02-13 02:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27][root][INFO] - Training Epoch: 1/2, step 3836/7134 completed (loss: 0.07779832184314728, acc: 0.9826086759567261)
[2025-02-13 02:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28][root][INFO] - Training Epoch: 1/2, step 3837/7134 completed (loss: 0.036193761974573135, acc: 0.9942528605461121)
[2025-02-13 02:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28][root][INFO] - Training Epoch: 1/2, step 3838/7134 completed (loss: 0.031198972836136818, acc: 0.9880478382110596)
[2025-02-13 02:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:29][root][INFO] - Training Epoch: 1/2, step 3839/7134 completed (loss: 0.03426605463027954, acc: 0.9917126893997192)
[2025-02-13 02:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:29][root][INFO] - Training Epoch: 1/2, step 3840/7134 completed (loss: 0.03301693871617317, acc: 0.9892904758453369)
[2025-02-13 02:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30][root][INFO] - Training Epoch: 1/2, step 3841/7134 completed (loss: 0.0582684688270092, acc: 0.9789122939109802)
[2025-02-13 02:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30][root][INFO] - Training Epoch: 1/2, step 3842/7134 completed (loss: 0.016219042241573334, acc: 0.9926470518112183)
[2025-02-13 02:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30][root][INFO] - Training Epoch: 1/2, step 3843/7134 completed (loss: 0.04319492354989052, acc: 0.9874869585037231)
[2025-02-13 02:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:31][root][INFO] - Training Epoch: 1/2, step 3844/7134 completed (loss: 0.04214965179562569, acc: 0.9885641932487488)
[2025-02-13 02:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:31][root][INFO] - Training Epoch: 1/2, step 3845/7134 completed (loss: 0.05510702729225159, acc: 0.9882155060768127)
[2025-02-13 02:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32][root][INFO] - Training Epoch: 1/2, step 3846/7134 completed (loss: 0.028087906539440155, acc: 0.9932088255882263)
[2025-02-13 02:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32][root][INFO] - Training Epoch: 1/2, step 3847/7134 completed (loss: 0.03498926758766174, acc: 0.9904000163078308)
[2025-02-13 02:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32][root][INFO] - Training Epoch: 1/2, step 3848/7134 completed (loss: 0.038323767483234406, acc: 0.991094172000885)
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33][root][INFO] - Training Epoch: 1/2, step 3849/7134 completed (loss: 0.07315316051244736, acc: 0.9803921580314636)
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33][root][INFO] - Training Epoch: 1/2, step 3850/7134 completed (loss: 0.08740708976984024, acc: 0.980440080165863)
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34][root][INFO] - Training Epoch: 1/2, step 3851/7134 completed (loss: 0.06547769159078598, acc: 0.9776847958564758)
[2025-02-13 02:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34][root][INFO] - Training Epoch: 1/2, step 3852/7134 completed (loss: 0.035992883145809174, acc: 0.9903537034988403)
[2025-02-13 02:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34][root][INFO] - Training Epoch: 1/2, step 3853/7134 completed (loss: 0.06925762444734573, acc: 0.9746835231781006)
[2025-02-13 02:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:35][root][INFO] - Training Epoch: 1/2, step 3854/7134 completed (loss: 0.02574753761291504, acc: 0.9915730357170105)
[2025-02-13 02:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:35][root][INFO] - Training Epoch: 1/2, step 3855/7134 completed (loss: 0.0700722485780716, acc: 0.9825436472892761)
[2025-02-13 02:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:36][root][INFO] - Training Epoch: 1/2, step 3856/7134 completed (loss: 0.07445996254682541, acc: 0.9790794849395752)
[2025-02-13 02:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:36][root][INFO] - Training Epoch: 1/2, step 3857/7134 completed (loss: 0.06162692606449127, acc: 0.9807074069976807)
[2025-02-13 02:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37][root][INFO] - Training Epoch: 1/2, step 3858/7134 completed (loss: 0.08269961178302765, acc: 0.977337121963501)
[2025-02-13 02:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37][root][INFO] - Training Epoch: 1/2, step 3859/7134 completed (loss: 0.0516609326004982, acc: 0.9872773289680481)
[2025-02-13 02:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37][root][INFO] - Training Epoch: 1/2, step 3860/7134 completed (loss: 0.03932691365480423, acc: 0.9910614490509033)
[2025-02-13 02:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38][root][INFO] - Training Epoch: 1/2, step 3861/7134 completed (loss: 0.06272495537996292, acc: 0.9768707752227783)
[2025-02-13 02:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38][root][INFO] - Training Epoch: 1/2, step 3862/7134 completed (loss: 0.10236213356256485, acc: 0.9720853567123413)
[2025-02-13 02:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39][root][INFO] - Training Epoch: 1/2, step 3863/7134 completed (loss: 0.0504881925880909, acc: 0.9864197373390198)
[2025-02-13 02:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39][root][INFO] - Training Epoch: 1/2, step 3864/7134 completed (loss: 0.05145932361483574, acc: 0.9882506728172302)
[2025-02-13 02:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40][root][INFO] - Training Epoch: 1/2, step 3865/7134 completed (loss: 0.041308678686618805, acc: 0.9900398254394531)
[2025-02-13 02:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40][root][INFO] - Training Epoch: 1/2, step 3866/7134 completed (loss: 0.07327152788639069, acc: 0.980463981628418)
[2025-02-13 02:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40][root][INFO] - Training Epoch: 1/2, step 3867/7134 completed (loss: 0.05108550190925598, acc: 0.9867424368858337)
[2025-02-13 02:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41][root][INFO] - Training Epoch: 1/2, step 3868/7134 completed (loss: 0.04119047522544861, acc: 0.9881556630134583)
[2025-02-13 02:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41][root][INFO] - Training Epoch: 1/2, step 3869/7134 completed (loss: 0.05642315372824669, acc: 0.9878970980644226)
[2025-02-13 02:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:42][root][INFO] - Training Epoch: 1/2, step 3870/7134 completed (loss: 0.08237773925065994, acc: 0.9791666865348816)
[2025-02-13 02:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:42][root][INFO] - Training Epoch: 1/2, step 3871/7134 completed (loss: 0.06027395650744438, acc: 0.9800498485565186)
[2025-02-13 02:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43][root][INFO] - Training Epoch: 1/2, step 3872/7134 completed (loss: 0.08449956029653549, acc: 0.9753747582435608)
[2025-02-13 02:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43][root][INFO] - Training Epoch: 1/2, step 3873/7134 completed (loss: 0.04165000468492508, acc: 0.9872123003005981)
[2025-02-13 02:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43][root][INFO] - Training Epoch: 1/2, step 3874/7134 completed (loss: 0.08131557703018188, acc: 0.9792531132698059)
[2025-02-13 02:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44][root][INFO] - Training Epoch: 1/2, step 3875/7134 completed (loss: 0.08536113798618317, acc: 0.978723406791687)
[2025-02-13 02:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44][root][INFO] - Training Epoch: 1/2, step 3876/7134 completed (loss: 0.05609104037284851, acc: 0.9849905967712402)
[2025-02-13 02:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45][root][INFO] - Training Epoch: 1/2, step 3877/7134 completed (loss: 0.09794881939888, acc: 0.977806806564331)
[2025-02-13 02:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45][root][INFO] - Training Epoch: 1/2, step 3878/7134 completed (loss: 0.09950277209281921, acc: 0.9757412672042847)
[2025-02-13 02:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45][root][INFO] - Training Epoch: 1/2, step 3879/7134 completed (loss: 0.031034398823976517, acc: 0.9926062822341919)
[2025-02-13 02:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46][root][INFO] - Training Epoch: 1/2, step 3880/7134 completed (loss: 0.14222218096256256, acc: 0.9676945805549622)
[2025-02-13 02:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46][root][INFO] - Training Epoch: 1/2, step 3881/7134 completed (loss: 0.11933250725269318, acc: 0.9693333506584167)
[2025-02-13 02:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:47][root][INFO] - Training Epoch: 1/2, step 3882/7134 completed (loss: 0.0793592780828476, acc: 0.9787499904632568)
[2025-02-13 02:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:47][root][INFO] - Training Epoch: 1/2, step 3883/7134 completed (loss: 0.05056744068861008, acc: 0.9818181991577148)
[2025-02-13 02:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48][root][INFO] - Training Epoch: 1/2, step 3884/7134 completed (loss: 0.03844139352440834, acc: 0.9904761910438538)
[2025-02-13 02:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48][root][INFO] - Training Epoch: 1/2, step 3885/7134 completed (loss: 0.05016527697443962, acc: 0.9869451522827148)
[2025-02-13 02:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48][root][INFO] - Training Epoch: 1/2, step 3886/7134 completed (loss: 0.05050121247768402, acc: 0.9818840622901917)
[2025-02-13 02:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49][root][INFO] - Training Epoch: 1/2, step 3887/7134 completed (loss: 0.05069180205464363, acc: 0.9864253401756287)
[2025-02-13 02:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49][root][INFO] - Training Epoch: 1/2, step 3888/7134 completed (loss: 0.06168462708592415, acc: 0.981249988079071)
[2025-02-13 02:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50][root][INFO] - Training Epoch: 1/2, step 3889/7134 completed (loss: 0.033035434782505035, acc: 0.9910600185394287)
[2025-02-13 02:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50][root][INFO] - Training Epoch: 1/2, step 3890/7134 completed (loss: 0.03346896916627884, acc: 0.9903730154037476)
[2025-02-13 02:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51][root][INFO] - Training Epoch: 1/2, step 3891/7134 completed (loss: 0.0390196219086647, acc: 0.9869281053543091)
[2025-02-13 02:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51][root][INFO] - Training Epoch: 1/2, step 3892/7134 completed (loss: 0.05306015536189079, acc: 0.9830769300460815)
[2025-02-13 02:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51][root][INFO] - Training Epoch: 1/2, step 3893/7134 completed (loss: 0.095330148935318, acc: 0.9703608155250549)
[2025-02-13 02:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52][root][INFO] - Training Epoch: 1/2, step 3894/7134 completed (loss: 0.030309302732348442, acc: 0.9915730357170105)
[2025-02-13 02:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52][root][INFO] - Training Epoch: 1/2, step 3895/7134 completed (loss: 0.052042584866285324, acc: 0.9845161437988281)
[2025-02-13 02:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:53][root][INFO] - Training Epoch: 1/2, step 3896/7134 completed (loss: 0.01930464804172516, acc: 0.9954853057861328)
[2025-02-13 02:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:53][root][INFO] - Training Epoch: 1/2, step 3897/7134 completed (loss: 0.07915183156728745, acc: 0.9802817106246948)
[2025-02-13 02:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54][root][INFO] - Training Epoch: 1/2, step 3898/7134 completed (loss: 0.07412916421890259, acc: 0.9809160232543945)
[2025-02-13 02:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54][root][INFO] - Training Epoch: 1/2, step 3899/7134 completed (loss: 0.03956142067909241, acc: 0.9909443855285645)
[2025-02-13 02:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55][root][INFO] - Training Epoch: 1/2, step 3900/7134 completed (loss: 0.055019717663526535, acc: 0.9837775230407715)
[2025-02-13 02:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55][root][INFO] - Training Epoch: 1/2, step 3901/7134 completed (loss: 0.09672636538743973, acc: 0.9743935465812683)
[2025-02-13 02:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55][root][INFO] - Training Epoch: 1/2, step 3902/7134 completed (loss: 0.11545126140117645, acc: 0.9721854329109192)
[2025-02-13 02:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:56][root][INFO] - Training Epoch: 1/2, step 3903/7134 completed (loss: 0.1254776418209076, acc: 0.9638069868087769)
[2025-02-13 02:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:56][root][INFO] - Training Epoch: 1/2, step 3904/7134 completed (loss: 0.0767739787697792, acc: 0.9817351698875427)
[2025-02-13 02:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57][root][INFO] - Training Epoch: 1/2, step 3905/7134 completed (loss: 0.050217315554618835, acc: 0.9844852089881897)
[2025-02-13 02:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57][root][INFO] - Training Epoch: 1/2, step 3906/7134 completed (loss: 0.04463570937514305, acc: 0.982758641242981)
[2025-02-13 02:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57][root][INFO] - Training Epoch: 1/2, step 3907/7134 completed (loss: 0.036579180508852005, acc: 0.9871465563774109)
[2025-02-13 02:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:58][root][INFO] - Training Epoch: 1/2, step 3908/7134 completed (loss: 0.08946322649717331, acc: 0.9693721532821655)
[2025-02-13 02:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:58][root][INFO] - Training Epoch: 1/2, step 3909/7134 completed (loss: 0.05029044300317764, acc: 0.9806138873100281)
[2025-02-13 02:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59][root][INFO] - Training Epoch: 1/2, step 3910/7134 completed (loss: 0.06334603577852249, acc: 0.9791377186775208)
[2025-02-13 02:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59][root][INFO] - Training Epoch: 1/2, step 3911/7134 completed (loss: 0.04476119577884674, acc: 0.9886731505393982)
[2025-02-13 02:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00][root][INFO] - Training Epoch: 1/2, step 3912/7134 completed (loss: 0.016416264697909355, acc: 0.9955357313156128)
[2025-02-13 03:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00][root][INFO] - Training Epoch: 1/2, step 3913/7134 completed (loss: 0.05606553331017494, acc: 0.9884615540504456)
[2025-02-13 03:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00][root][INFO] - Training Epoch: 1/2, step 3914/7134 completed (loss: 0.046880677342414856, acc: 0.9865319728851318)
[2025-02-13 03:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:01][root][INFO] - Training Epoch: 1/2, step 3915/7134 completed (loss: 0.03040170669555664, acc: 0.9929478168487549)
[2025-02-13 03:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:01][root][INFO] - Training Epoch: 1/2, step 3916/7134 completed (loss: 0.03793978691101074, acc: 0.9910827875137329)
[2025-02-13 03:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02][root][INFO] - Training Epoch: 1/2, step 3917/7134 completed (loss: 0.029869774356484413, acc: 0.9879662990570068)
[2025-02-13 03:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02][root][INFO] - Training Epoch: 1/2, step 3918/7134 completed (loss: 0.055061016231775284, acc: 0.9778645634651184)
[2025-02-13 03:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03][root][INFO] - Training Epoch: 1/2, step 3919/7134 completed (loss: 0.09867081791162491, acc: 0.9723126888275146)
[2025-02-13 03:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03][root][INFO] - Training Epoch: 1/2, step 3920/7134 completed (loss: 0.07845646142959595, acc: 0.9781931638717651)
[2025-02-13 03:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03][root][INFO] - Training Epoch: 1/2, step 3921/7134 completed (loss: 0.07224913686513901, acc: 0.9739999771118164)
[2025-02-13 03:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04][root][INFO] - Training Epoch: 1/2, step 3922/7134 completed (loss: 0.13861441612243652, acc: 0.9580712914466858)
[2025-02-13 03:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04][root][INFO] - Training Epoch: 1/2, step 3923/7134 completed (loss: 0.1527145355939865, acc: 0.9640411138534546)
[2025-02-13 03:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05][root][INFO] - Training Epoch: 1/2, step 3924/7134 completed (loss: 0.07376966625452042, acc: 0.9741379022598267)
[2025-02-13 03:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05][root][INFO] - Training Epoch: 1/2, step 3925/7134 completed (loss: 0.07846737653017044, acc: 0.9795022010803223)
[2025-02-13 03:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05][root][INFO] - Training Epoch: 1/2, step 3926/7134 completed (loss: 0.0689552053809166, acc: 0.9856114983558655)
[2025-02-13 03:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:06][root][INFO] - Training Epoch: 1/2, step 3927/7134 completed (loss: 0.04504364728927612, acc: 0.9862448573112488)
[2025-02-13 03:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:06][root][INFO] - Training Epoch: 1/2, step 3928/7134 completed (loss: 0.08756953477859497, acc: 0.9741379022598267)
[2025-02-13 03:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07][root][INFO] - Training Epoch: 1/2, step 3929/7134 completed (loss: 0.04581762105226517, acc: 0.9856915473937988)
[2025-02-13 03:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07][root][INFO] - Training Epoch: 1/2, step 3930/7134 completed (loss: 0.09754418581724167, acc: 0.9663865566253662)
[2025-02-13 03:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08][root][INFO] - Training Epoch: 1/2, step 3931/7134 completed (loss: 0.051423151046037674, acc: 0.9775784611701965)
[2025-02-13 03:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08][root][INFO] - Training Epoch: 1/2, step 3932/7134 completed (loss: 0.06307591497898102, acc: 0.9822134375572205)
[2025-02-13 03:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08][root][INFO] - Training Epoch: 1/2, step 3933/7134 completed (loss: 0.03104155883193016, acc: 0.9934210777282715)
[2025-02-13 03:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09][root][INFO] - Training Epoch: 1/2, step 3934/7134 completed (loss: 0.045124322175979614, acc: 0.9812646508216858)
[2025-02-13 03:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09][root][INFO] - Training Epoch: 1/2, step 3935/7134 completed (loss: 0.012472599744796753, acc: 0.9964664578437805)
[2025-02-13 03:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09][root][INFO] - Training Epoch: 1/2, step 3936/7134 completed (loss: 0.05664411559700966, acc: 0.9849056601524353)
[2025-02-13 03:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10][root][INFO] - Training Epoch: 1/2, step 3937/7134 completed (loss: 0.06848885864019394, acc: 0.9795918464660645)
[2025-02-13 03:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10][root][INFO] - Training Epoch: 1/2, step 3938/7134 completed (loss: 0.04123639315366745, acc: 0.9886877536773682)
[2025-02-13 03:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11][root][INFO] - Training Epoch: 1/2, step 3939/7134 completed (loss: 0.09765361249446869, acc: 0.9695431590080261)
[2025-02-13 03:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11][root][INFO] - Training Epoch: 1/2, step 3940/7134 completed (loss: 0.07969480752944946, acc: 0.9849246144294739)
[2025-02-13 03:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11][root][INFO] - Training Epoch: 1/2, step 3941/7134 completed (loss: 0.039872556924819946, acc: 0.9847561120986938)
[2025-02-13 03:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12][root][INFO] - Training Epoch: 1/2, step 3942/7134 completed (loss: 0.04703109711408615, acc: 0.9877049326896667)
[2025-02-13 03:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12][root][INFO] - Training Epoch: 1/2, step 3943/7134 completed (loss: 0.05629928037524223, acc: 0.9768339991569519)
[2025-02-13 03:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12][root][INFO] - Training Epoch: 1/2, step 3944/7134 completed (loss: 0.046519502997398376, acc: 0.9870848655700684)
[2025-02-13 03:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:13][root][INFO] - Training Epoch: 1/2, step 3945/7134 completed (loss: 0.052702851593494415, acc: 0.9817073345184326)
[2025-02-13 03:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:13][root][INFO] - Training Epoch: 1/2, step 3946/7134 completed (loss: 0.052502941340208054, acc: 0.9874739050865173)
[2025-02-13 03:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:13][root][INFO] - Training Epoch: 1/2, step 3947/7134 completed (loss: 0.06514734774827957, acc: 0.9870466589927673)
[2025-02-13 03:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:14][root][INFO] - Training Epoch: 1/2, step 3948/7134 completed (loss: 0.02497986890375614, acc: 0.9974874258041382)
[2025-02-13 03:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:14][root][INFO] - Training Epoch: 1/2, step 3949/7134 completed (loss: 0.048665594309568405, acc: 0.9813432693481445)
[2025-02-13 03:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15][root][INFO] - Training Epoch: 1/2, step 3950/7134 completed (loss: 0.06732393801212311, acc: 0.9776785969734192)
[2025-02-13 03:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15][root][INFO] - Training Epoch: 1/2, step 3951/7134 completed (loss: 0.059706296771764755, acc: 0.9849624037742615)
[2025-02-13 03:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15][root][INFO] - Training Epoch: 1/2, step 3952/7134 completed (loss: 0.04552195966243744, acc: 0.9871794581413269)
[2025-02-13 03:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16][root][INFO] - Training Epoch: 1/2, step 3953/7134 completed (loss: 0.05258164927363396, acc: 0.9829221963882446)
[2025-02-13 03:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16][root][INFO] - Training Epoch: 1/2, step 3954/7134 completed (loss: 0.04351966455578804, acc: 0.9845361113548279)
[2025-02-13 03:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16][root][INFO] - Training Epoch: 1/2, step 3955/7134 completed (loss: 0.041571542620658875, acc: 0.9865092635154724)
[2025-02-13 03:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:17][root][INFO] - Training Epoch: 1/2, step 3956/7134 completed (loss: 0.03361790254712105, acc: 0.9919999837875366)
[2025-02-13 03:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:17][root][INFO] - Training Epoch: 1/2, step 3957/7134 completed (loss: 0.04619000479578972, acc: 0.9864864945411682)
[2025-02-13 03:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18][root][INFO] - Training Epoch: 1/2, step 3958/7134 completed (loss: 0.11116168648004532, acc: 0.9615384340286255)
[2025-02-13 03:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18][root][INFO] - Training Epoch: 1/2, step 3959/7134 completed (loss: 0.03631899878382683, acc: 0.9912087917327881)
[2025-02-13 03:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18][root][INFO] - Training Epoch: 1/2, step 3960/7134 completed (loss: 0.04118311405181885, acc: 0.9871794581413269)
[2025-02-13 03:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:19][root][INFO] - Training Epoch: 1/2, step 3961/7134 completed (loss: 0.018594585359096527, acc: 0.9940029978752136)
[2025-02-13 03:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:19][root][INFO] - Training Epoch: 1/2, step 3962/7134 completed (loss: 0.030715368688106537, acc: 0.9886731505393982)
[2025-02-13 03:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20][root][INFO] - Training Epoch: 1/2, step 3963/7134 completed (loss: 0.12047497183084488, acc: 0.9794721603393555)
[2025-02-13 03:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20][root][INFO] - Training Epoch: 1/2, step 3964/7134 completed (loss: 0.09109478443861008, acc: 0.979629635810852)
[2025-02-13 03:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20][root][INFO] - Training Epoch: 1/2, step 3965/7134 completed (loss: 0.04381361976265907, acc: 0.9885550737380981)
[2025-02-13 03:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:21][root][INFO] - Training Epoch: 1/2, step 3966/7134 completed (loss: 0.027246898040175438, acc: 0.9909747242927551)
[2025-02-13 03:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:21][root][INFO] - Training Epoch: 1/2, step 3967/7134 completed (loss: 0.09688418358564377, acc: 0.9705372452735901)
[2025-02-13 03:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22][root][INFO] - Training Epoch: 1/2, step 3968/7134 completed (loss: 0.03237106278538704, acc: 0.9882965087890625)
[2025-02-13 03:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22][root][INFO] - Training Epoch: 1/2, step 3969/7134 completed (loss: 0.10359303653240204, acc: 0.9685863852500916)
[2025-02-13 03:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22][root][INFO] - Training Epoch: 1/2, step 3970/7134 completed (loss: 0.10757610201835632, acc: 0.9784172773361206)
[2025-02-13 03:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23][root][INFO] - Training Epoch: 1/2, step 3971/7134 completed (loss: 0.13972169160842896, acc: 0.9693094491958618)
[2025-02-13 03:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23][root][INFO] - Training Epoch: 1/2, step 3972/7134 completed (loss: 0.19588607549667358, acc: 0.9432989954948425)
[2025-02-13 03:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24][root][INFO] - Training Epoch: 1/2, step 3973/7134 completed (loss: 0.05512892082333565, acc: 0.9792746305465698)
[2025-02-13 03:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24][root][INFO] - Training Epoch: 1/2, step 3974/7134 completed (loss: 0.12423627823591232, acc: 0.9661246538162231)
[2025-02-13 03:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25][root][INFO] - Training Epoch: 1/2, step 3975/7134 completed (loss: 0.06171474978327751, acc: 0.9758307933807373)
[2025-02-13 03:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25][root][INFO] - Training Epoch: 1/2, step 3976/7134 completed (loss: 0.11418947577476501, acc: 0.9734513163566589)
[2025-02-13 03:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25][root][INFO] - Training Epoch: 1/2, step 3977/7134 completed (loss: 0.04050887003540993, acc: 0.9895651936531067)
[2025-02-13 03:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:26][root][INFO] - Training Epoch: 1/2, step 3978/7134 completed (loss: 0.02877197414636612, acc: 0.9948253631591797)
[2025-02-13 03:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:26][root][INFO] - Training Epoch: 1/2, step 3979/7134 completed (loss: 0.09016838669776917, acc: 0.9744991064071655)
[2025-02-13 03:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27][root][INFO] - Training Epoch: 1/2, step 3980/7134 completed (loss: 0.03020077757537365, acc: 0.9939024448394775)
[2025-02-13 03:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27][root][INFO] - Training Epoch: 1/2, step 3981/7134 completed (loss: 0.06218278035521507, acc: 0.9847095012664795)
[2025-02-13 03:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27][root][INFO] - Training Epoch: 1/2, step 3982/7134 completed (loss: 0.11150777339935303, acc: 0.9713056087493896)
[2025-02-13 03:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28][root][INFO] - Training Epoch: 1/2, step 3983/7134 completed (loss: 0.32325974106788635, acc: 0.936170220375061)
[2025-02-13 03:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28][root][INFO] - Training Epoch: 1/2, step 3984/7134 completed (loss: 0.09236349165439606, acc: 0.9779220819473267)
[2025-02-13 03:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:29][root][INFO] - Training Epoch: 1/2, step 3985/7134 completed (loss: 0.0326758474111557, acc: 0.9893048405647278)
[2025-02-13 03:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:29][root][INFO] - Training Epoch: 1/2, step 3986/7134 completed (loss: 0.20175020396709442, acc: 0.9575757384300232)
[2025-02-13 03:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30][root][INFO] - Training Epoch: 1/2, step 3987/7134 completed (loss: 0.08234516531229019, acc: 0.9808061122894287)
[2025-02-13 03:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30][root][INFO] - Training Epoch: 1/2, step 3988/7134 completed (loss: 0.1139543280005455, acc: 0.9646017551422119)
[2025-02-13 03:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30][root][INFO] - Training Epoch: 1/2, step 3989/7134 completed (loss: 0.2295597791671753, acc: 0.9530916810035706)
[2025-02-13 03:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:31][root][INFO] - Training Epoch: 1/2, step 3990/7134 completed (loss: 0.05127489194273949, acc: 0.9883913993835449)
[2025-02-13 03:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:31][root][INFO] - Training Epoch: 1/2, step 3991/7134 completed (loss: 0.06684441864490509, acc: 0.9834983348846436)
[2025-02-13 03:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:31][root][INFO] - Training Epoch: 1/2, step 3992/7134 completed (loss: 0.09186078608036041, acc: 0.9756757020950317)
[2025-02-13 03:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:32][root][INFO] - Training Epoch: 1/2, step 3993/7134 completed (loss: 0.07931357622146606, acc: 0.9747706651687622)
[2025-02-13 03:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:32][root][INFO] - Training Epoch: 1/2, step 3994/7134 completed (loss: 0.08430436998605728, acc: 0.9833333492279053)
[2025-02-13 03:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33][root][INFO] - Training Epoch: 1/2, step 3995/7134 completed (loss: 0.14833322167396545, acc: 0.9651162624359131)
[2025-02-13 03:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33][root][INFO] - Training Epoch: 1/2, step 3996/7134 completed (loss: 0.045814577490091324, acc: 0.9897330403327942)
[2025-02-13 03:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33][root][INFO] - Training Epoch: 1/2, step 3997/7134 completed (loss: 0.1899225413799286, acc: 0.9582417607307434)
[2025-02-13 03:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34][root][INFO] - Training Epoch: 1/2, step 3998/7134 completed (loss: 0.1050586923956871, acc: 0.9693654179573059)
[2025-02-13 03:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34][root][INFO] - Training Epoch: 1/2, step 3999/7134 completed (loss: 0.05894836038351059, acc: 0.9840319156646729)
[2025-02-13 03:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34][root][INFO] - Training Epoch: 1/2, step 4000/7134 completed (loss: 0.09256835281848907, acc: 0.9806950092315674)
[2025-02-13 03:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:35][root][INFO] - Training Epoch: 1/2, step 4001/7134 completed (loss: 0.08637748658657074, acc: 0.9761431217193604)
[2025-02-13 03:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:35][root][INFO] - Training Epoch: 1/2, step 4002/7134 completed (loss: 0.12958331406116486, acc: 0.9734939932823181)
[2025-02-13 03:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36][root][INFO] - Training Epoch: 1/2, step 4003/7134 completed (loss: 0.10543995350599289, acc: 0.9760147333145142)
[2025-02-13 03:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36][root][INFO] - Training Epoch: 1/2, step 4004/7134 completed (loss: 0.0846341997385025, acc: 0.9696969985961914)
[2025-02-13 03:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36][root][INFO] - Training Epoch: 1/2, step 4005/7134 completed (loss: 0.044493645429611206, acc: 0.9858712553977966)
[2025-02-13 03:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:37][root][INFO] - Training Epoch: 1/2, step 4006/7134 completed (loss: 0.031174173578619957, acc: 0.9927158951759338)
[2025-02-13 03:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:37][root][INFO] - Training Epoch: 1/2, step 4007/7134 completed (loss: 0.05153069645166397, acc: 0.9835082292556763)
[2025-02-13 03:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:38][root][INFO] - Training Epoch: 1/2, step 4008/7134 completed (loss: 0.03232442960143089, acc: 0.9926004409790039)
[2025-02-13 03:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:38][root][INFO] - Training Epoch: 1/2, step 4009/7134 completed (loss: 0.0241027120500803, acc: 0.9933422207832336)
[2025-02-13 03:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39][root][INFO] - Training Epoch: 1/2, step 4010/7134 completed (loss: 0.03586515039205551, acc: 0.991051435470581)
[2025-02-13 03:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39][root][INFO] - Training Epoch: 1/2, step 4011/7134 completed (loss: 0.03832605108618736, acc: 0.9880478382110596)
[2025-02-13 03:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39][root][INFO] - Training Epoch: 1/2, step 4012/7134 completed (loss: 0.02502833679318428, acc: 0.991525411605835)
[2025-02-13 03:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:40][root][INFO] - Training Epoch: 1/2, step 4013/7134 completed (loss: 0.013613261282444, acc: 0.9967845678329468)
[2025-02-13 03:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:40][root][INFO] - Training Epoch: 1/2, step 4014/7134 completed (loss: 0.021281834691762924, acc: 0.9921630024909973)
[2025-02-13 03:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:41][root][INFO] - Training Epoch: 1/2, step 4015/7134 completed (loss: 0.03038198873400688, acc: 0.9892473220825195)
[2025-02-13 03:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:41][root][INFO] - Training Epoch: 1/2, step 4016/7134 completed (loss: 0.054845184087753296, acc: 0.9885203838348389)
[2025-02-13 03:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42][root][INFO] - Training Epoch: 1/2, step 4017/7134 completed (loss: 0.0469590462744236, acc: 0.9872978925704956)
[2025-02-13 03:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42][root][INFO] - Training Epoch: 1/2, step 4018/7134 completed (loss: 0.05028169974684715, acc: 0.9890909194946289)
[2025-02-13 03:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42][root][INFO] - Training Epoch: 1/2, step 4019/7134 completed (loss: 0.05528701841831207, acc: 0.9847715497016907)
[2025-02-13 03:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:43][root][INFO] - Training Epoch: 1/2, step 4020/7134 completed (loss: 0.0478900782763958, acc: 0.9836065769195557)
[2025-02-13 03:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:43][root][INFO] - Training Epoch: 1/2, step 4021/7134 completed (loss: 0.03807755932211876, acc: 0.9893048405647278)
[2025-02-13 03:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44][root][INFO] - Training Epoch: 1/2, step 4022/7134 completed (loss: 0.060099370777606964, acc: 0.9889570474624634)
[2025-02-13 03:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44][root][INFO] - Training Epoch: 1/2, step 4023/7134 completed (loss: 0.049424219876527786, acc: 0.9886792302131653)
[2025-02-13 03:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45][root][INFO] - Training Epoch: 1/2, step 4024/7134 completed (loss: 0.04851139336824417, acc: 0.9868766665458679)
[2025-02-13 03:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45][root][INFO] - Training Epoch: 1/2, step 4025/7134 completed (loss: 0.07499013096094131, acc: 0.9807474613189697)
[2025-02-13 03:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45][root][INFO] - Training Epoch: 1/2, step 4026/7134 completed (loss: 0.10631468892097473, acc: 0.976401150226593)
[2025-02-13 03:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:46][root][INFO] - Training Epoch: 1/2, step 4027/7134 completed (loss: 0.07119631767272949, acc: 0.9808153510093689)
[2025-02-13 03:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:46][root][INFO] - Training Epoch: 1/2, step 4028/7134 completed (loss: 0.06784145534038544, acc: 0.979345977306366)
[2025-02-13 03:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47][root][INFO] - Training Epoch: 1/2, step 4029/7134 completed (loss: 0.035445328801870346, acc: 0.9889298677444458)
[2025-02-13 03:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47][root][INFO] - Training Epoch: 1/2, step 4030/7134 completed (loss: 0.04024605080485344, acc: 0.987679660320282)
[2025-02-13 03:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47][root][INFO] - Training Epoch: 1/2, step 4031/7134 completed (loss: 0.06501225382089615, acc: 0.9882698059082031)
[2025-02-13 03:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48][root][INFO] - Training Epoch: 1/2, step 4032/7134 completed (loss: 0.1056801900267601, acc: 0.9759036302566528)
[2025-02-13 03:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48][root][INFO] - Training Epoch: 1/2, step 4033/7134 completed (loss: 0.03449302166700363, acc: 0.9861634969711304)
[2025-02-13 03:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:49][root][INFO] - Training Epoch: 1/2, step 4034/7134 completed (loss: 0.013828838244080544, acc: 0.9939098954200745)
[2025-02-13 03:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:49][root][INFO] - Training Epoch: 1/2, step 4035/7134 completed (loss: 0.06041887030005455, acc: 0.9834710955619812)
[2025-02-13 03:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50][root][INFO] - Training Epoch: 1/2, step 4036/7134 completed (loss: 0.039895713329315186, acc: 0.9835164546966553)
[2025-02-13 03:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50][root][INFO] - Training Epoch: 1/2, step 4037/7134 completed (loss: 0.02833711914718151, acc: 0.9903730154037476)
[2025-02-13 03:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50][root][INFO] - Training Epoch: 1/2, step 4038/7134 completed (loss: 0.06566393375396729, acc: 0.9834938049316406)
[2025-02-13 03:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:51][root][INFO] - Training Epoch: 1/2, step 4039/7134 completed (loss: 0.05628045275807381, acc: 0.978723406791687)
[2025-02-13 03:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:51][root][INFO] - Training Epoch: 1/2, step 4040/7134 completed (loss: 0.03517017140984535, acc: 0.9901477694511414)
[2025-02-13 03:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52][root][INFO] - Training Epoch: 1/2, step 4041/7134 completed (loss: 0.04087797552347183, acc: 0.9919354915618896)
[2025-02-13 03:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52][root][INFO] - Training Epoch: 1/2, step 4042/7134 completed (loss: 0.02530978061258793, acc: 0.9955882430076599)
[2025-02-13 03:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52][root][INFO] - Training Epoch: 1/2, step 4043/7134 completed (loss: 0.03199838101863861, acc: 0.9867841601371765)
[2025-02-13 03:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:53][root][INFO] - Training Epoch: 1/2, step 4044/7134 completed (loss: 0.015961507335305214, acc: 0.9949495196342468)
[2025-02-13 03:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:53][root][INFO] - Training Epoch: 1/2, step 4045/7134 completed (loss: 0.07862616330385208, acc: 0.9833333492279053)
[2025-02-13 03:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54][root][INFO] - Training Epoch: 1/2, step 4046/7134 completed (loss: 0.054252028465270996, acc: 0.9848901033401489)
[2025-02-13 03:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54][root][INFO] - Training Epoch: 1/2, step 4047/7134 completed (loss: 0.050072863698005676, acc: 0.9868735074996948)
[2025-02-13 03:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:55][root][INFO] - Training Epoch: 1/2, step 4048/7134 completed (loss: 0.03617233410477638, acc: 0.9871794581413269)
[2025-02-13 03:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:55][root][INFO] - Training Epoch: 1/2, step 4049/7134 completed (loss: 0.045183151960372925, acc: 0.9885057210922241)
[2025-02-13 03:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56][root][INFO] - Training Epoch: 1/2, step 4050/7134 completed (loss: 0.07195759564638138, acc: 0.9815602898597717)
[2025-02-13 03:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56][root][INFO] - Training Epoch: 1/2, step 4051/7134 completed (loss: 0.08677524328231812, acc: 0.9844497442245483)
[2025-02-13 03:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56][root][INFO] - Training Epoch: 1/2, step 4052/7134 completed (loss: 0.03215145319700241, acc: 0.9896640777587891)
[2025-02-13 03:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:57][root][INFO] - Training Epoch: 1/2, step 4053/7134 completed (loss: 0.05123361945152283, acc: 0.9840849041938782)
[2025-02-13 03:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:57][root][INFO] - Training Epoch: 1/2, step 4054/7134 completed (loss: 0.04716994985938072, acc: 0.9900000095367432)
[2025-02-13 03:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:58][root][INFO] - Training Epoch: 1/2, step 4055/7134 completed (loss: 0.04585317149758339, acc: 0.981796145439148)
[2025-02-13 03:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:58][root][INFO] - Training Epoch: 1/2, step 4056/7134 completed (loss: 0.04342474415898323, acc: 0.9878787994384766)
[2025-02-13 03:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59][root][INFO] - Training Epoch: 1/2, step 4057/7134 completed (loss: 0.08152464032173157, acc: 0.9833101630210876)
[2025-02-13 03:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59][root][INFO] - Training Epoch: 1/2, step 4058/7134 completed (loss: 0.1125737652182579, acc: 0.9751434326171875)
[2025-02-13 03:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59][root][INFO] - Training Epoch: 1/2, step 4059/7134 completed (loss: 0.06930842995643616, acc: 0.9817517995834351)
[2025-02-13 03:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00][root][INFO] - Training Epoch: 1/2, step 4060/7134 completed (loss: 0.040934085845947266, acc: 0.9860896468162537)
[2025-02-13 03:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00][root][INFO] - Training Epoch: 1/2, step 4061/7134 completed (loss: 0.035710953176021576, acc: 0.9886040091514587)
[2025-02-13 03:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01][root][INFO] - Training Epoch: 1/2, step 4062/7134 completed (loss: 0.02762693539261818, acc: 0.9937008023262024)
[2025-02-13 03:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01][root][INFO] - Training Epoch: 1/2, step 4063/7134 completed (loss: 0.03543464094400406, acc: 0.9858657121658325)
[2025-02-13 03:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01][root][INFO] - Training Epoch: 1/2, step 4064/7134 completed (loss: 0.02710001915693283, acc: 0.9954614043235779)
[2025-02-13 03:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02][root][INFO] - Training Epoch: 1/2, step 4065/7134 completed (loss: 0.03867180645465851, acc: 0.9875862002372742)
[2025-02-13 03:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02][root][INFO] - Training Epoch: 1/2, step 4066/7134 completed (loss: 0.03768385574221611, acc: 0.9902777671813965)
[2025-02-13 03:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03][root][INFO] - Training Epoch: 1/2, step 4067/7134 completed (loss: 0.039429839700460434, acc: 0.9885621070861816)
[2025-02-13 03:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03][root][INFO] - Training Epoch: 1/2, step 4068/7134 completed (loss: 0.03796442598104477, acc: 0.9851852059364319)
[2025-02-13 03:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03][root][INFO] - Training Epoch: 1/2, step 4069/7134 completed (loss: 0.035538699477910995, acc: 0.9906103014945984)
[2025-02-13 03:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:04][root][INFO] - Training Epoch: 1/2, step 4070/7134 completed (loss: 0.04408784583210945, acc: 0.9878234267234802)
[2025-02-13 03:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:04][root][INFO] - Training Epoch: 1/2, step 4071/7134 completed (loss: 0.0489220954477787, acc: 0.9851729869842529)
[2025-02-13 03:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05][root][INFO] - Training Epoch: 1/2, step 4072/7134 completed (loss: 0.057099197059869766, acc: 0.9870129823684692)
[2025-02-13 03:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05][root][INFO] - Training Epoch: 1/2, step 4073/7134 completed (loss: 0.029200896620750427, acc: 0.9899425506591797)
[2025-02-13 03:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05][root][INFO] - Training Epoch: 1/2, step 4074/7134 completed (loss: 0.04068648815155029, acc: 0.9899857044219971)
[2025-02-13 03:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:06][root][INFO] - Training Epoch: 1/2, step 4075/7134 completed (loss: 0.01971801184117794, acc: 0.9925037622451782)
[2025-02-13 03:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:06][root][INFO] - Training Epoch: 1/2, step 4076/7134 completed (loss: 0.017378075048327446, acc: 0.9971056580543518)
[2025-02-13 03:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07][root][INFO] - Training Epoch: 1/2, step 4077/7134 completed (loss: 0.05814392492175102, acc: 0.9796954393386841)
[2025-02-13 03:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07][root][INFO] - Training Epoch: 1/2, step 4078/7134 completed (loss: 0.03174036368727684, acc: 0.9871794581413269)
[2025-02-13 03:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07][root][INFO] - Training Epoch: 1/2, step 4079/7134 completed (loss: 0.03581591695547104, acc: 0.9928698539733887)
[2025-02-13 03:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:08][root][INFO] - Training Epoch: 1/2, step 4080/7134 completed (loss: 0.025914911180734634, acc: 0.9889240264892578)
[2025-02-13 03:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:08][root][INFO] - Training Epoch: 1/2, step 4081/7134 completed (loss: 0.012879876419901848, acc: 0.9983498454093933)
[2025-02-13 03:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09][root][INFO] - Training Epoch: 1/2, step 4082/7134 completed (loss: 0.09682638943195343, acc: 0.9704251289367676)
[2025-02-13 03:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09][root][INFO] - Training Epoch: 1/2, step 4083/7134 completed (loss: 0.04824412241578102, acc: 0.9880239367485046)
[2025-02-13 03:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09][root][INFO] - Training Epoch: 1/2, step 4084/7134 completed (loss: 0.10278045386075974, acc: 0.95686274766922)
[2025-02-13 03:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:10][root][INFO] - Training Epoch: 1/2, step 4085/7134 completed (loss: 0.05945199355483055, acc: 0.979784369468689)
[2025-02-13 03:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:10][root][INFO] - Training Epoch: 1/2, step 4086/7134 completed (loss: 0.09545760601758957, acc: 0.9770867228507996)
[2025-02-13 03:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:11][root][INFO] - Training Epoch: 1/2, step 4087/7134 completed (loss: 0.029156159609556198, acc: 0.9961240291595459)
[2025-02-13 03:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:11][root][INFO] - Training Epoch: 1/2, step 4088/7134 completed (loss: 0.10155394673347473, acc: 0.9701492786407471)
[2025-02-13 03:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12][root][INFO] - Training Epoch: 1/2, step 4089/7134 completed (loss: 0.09622666239738464, acc: 0.9769093990325928)
[2025-02-13 03:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12][root][INFO] - Training Epoch: 1/2, step 4090/7134 completed (loss: 0.14519430696964264, acc: 0.9670184850692749)
[2025-02-13 03:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12][root][INFO] - Training Epoch: 1/2, step 4091/7134 completed (loss: 0.12028997391462326, acc: 0.9730185270309448)
[2025-02-13 03:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:13][root][INFO] - Training Epoch: 1/2, step 4092/7134 completed (loss: 0.07658155262470245, acc: 0.9768339991569519)
[2025-02-13 03:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:13][root][INFO] - Training Epoch: 1/2, step 4093/7134 completed (loss: 0.04869712516665459, acc: 0.9817850589752197)
[2025-02-13 03:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14][root][INFO] - Training Epoch: 1/2, step 4094/7134 completed (loss: 0.09905066341161728, acc: 0.9729729890823364)
[2025-02-13 03:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14][root][INFO] - Training Epoch: 1/2, step 4095/7134 completed (loss: 0.04850244149565697, acc: 0.9826338887214661)
[2025-02-13 03:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14][root][INFO] - Training Epoch: 1/2, step 4096/7134 completed (loss: 0.1067223846912384, acc: 0.9791044592857361)
[2025-02-13 03:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:15][root][INFO] - Training Epoch: 1/2, step 4097/7134 completed (loss: 0.08670787513256073, acc: 0.9838709831237793)
[2025-02-13 03:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:15][root][INFO] - Training Epoch: 1/2, step 4098/7134 completed (loss: 0.04538683220744133, acc: 0.989159882068634)
[2025-02-13 03:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16][root][INFO] - Training Epoch: 1/2, step 4099/7134 completed (loss: 0.05851441249251366, acc: 0.98591548204422)
[2025-02-13 03:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16][root][INFO] - Training Epoch: 1/2, step 4100/7134 completed (loss: 0.008317586965858936, acc: 1.0)
[2025-02-13 03:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16][root][INFO] - Training Epoch: 1/2, step 4101/7134 completed (loss: 0.019133517518639565, acc: 0.9961977005004883)
[2025-02-13 03:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:17][root][INFO] - Training Epoch: 1/2, step 4102/7134 completed (loss: 0.05284043028950691, acc: 0.9817184805870056)
[2025-02-13 03:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:17][root][INFO] - Training Epoch: 1/2, step 4103/7134 completed (loss: 0.011094165965914726, acc: 0.9955157041549683)
[2025-02-13 03:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:18][root][INFO] - Training Epoch: 1/2, step 4104/7134 completed (loss: 0.02205689065158367, acc: 0.9932318329811096)
[2025-02-13 03:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:18][root][INFO] - Training Epoch: 1/2, step 4105/7134 completed (loss: 0.02292529121041298, acc: 0.9924471378326416)
[2025-02-13 03:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19][root][INFO] - Training Epoch: 1/2, step 4106/7134 completed (loss: 0.039433859288692474, acc: 0.9861963391304016)
[2025-02-13 03:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19][root][INFO] - Training Epoch: 1/2, step 4107/7134 completed (loss: 0.035672254860401154, acc: 0.9915966391563416)
[2025-02-13 03:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19][root][INFO] - Training Epoch: 1/2, step 4108/7134 completed (loss: 0.04945174604654312, acc: 0.9870550036430359)
[2025-02-13 03:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:20][root][INFO] - Training Epoch: 1/2, step 4109/7134 completed (loss: 0.02120617963373661, acc: 0.9919742941856384)
[2025-02-13 03:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:20][root][INFO] - Training Epoch: 1/2, step 4110/7134 completed (loss: 0.012676323764026165, acc: 0.9982993006706238)
[2025-02-13 03:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21][root][INFO] - Training Epoch: 1/2, step 4111/7134 completed (loss: 0.021029924973845482, acc: 0.9921135902404785)
[2025-02-13 03:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21][root][INFO] - Training Epoch: 1/2, step 4112/7134 completed (loss: 0.04149477183818817, acc: 0.9856630563735962)
[2025-02-13 03:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21][root][INFO] - Training Epoch: 1/2, step 4113/7134 completed (loss: 0.05573062598705292, acc: 0.9844961166381836)
[2025-02-13 03:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:22][root][INFO] - Training Epoch: 1/2, step 4114/7134 completed (loss: 0.11007090657949448, acc: 0.9674796462059021)
[2025-02-13 03:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:22][root][INFO] - Training Epoch: 1/2, step 4115/7134 completed (loss: 0.0895395576953888, acc: 0.9776119589805603)
[2025-02-13 03:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23][root][INFO] - Training Epoch: 1/2, step 4116/7134 completed (loss: 0.013040147721767426, acc: 0.9983713626861572)
[2025-02-13 03:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23][root][INFO] - Training Epoch: 1/2, step 4117/7134 completed (loss: 0.04933224245905876, acc: 0.988959014415741)
[2025-02-13 03:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23][root][INFO] - Training Epoch: 1/2, step 4118/7134 completed (loss: 0.023542338982224464, acc: 0.9926289916038513)
[2025-02-13 03:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24][root][INFO] - Training Epoch: 1/2, step 4119/7134 completed (loss: 0.06447931379079819, acc: 0.9838449358940125)
[2025-02-13 03:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24][root][INFO] - Training Epoch: 1/2, step 4120/7134 completed (loss: 0.041728027164936066, acc: 0.9917627573013306)
[2025-02-13 03:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24][root][INFO] - Training Epoch: 1/2, step 4121/7134 completed (loss: 0.06286437064409256, acc: 0.9857954382896423)
[2025-02-13 03:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:25][root][INFO] - Training Epoch: 1/2, step 4122/7134 completed (loss: 0.023992210626602173, acc: 0.9904580116271973)
[2025-02-13 03:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:25][root][INFO] - Training Epoch: 1/2, step 4123/7134 completed (loss: 0.030776333063840866, acc: 0.9909502267837524)
[2025-02-13 03:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26][root][INFO] - Training Epoch: 1/2, step 4124/7134 completed (loss: 0.040726758539676666, acc: 0.9886845946311951)
[2025-02-13 03:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26][root][INFO] - Training Epoch: 1/2, step 4125/7134 completed (loss: 0.027296466752886772, acc: 0.9926470518112183)
[2025-02-13 03:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27][root][INFO] - Training Epoch: 1/2, step 4126/7134 completed (loss: 0.024075694382190704, acc: 0.9943342804908752)
[2025-02-13 03:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27][root][INFO] - Training Epoch: 1/2, step 4127/7134 completed (loss: 0.043919824063777924, acc: 0.989062488079071)
[2025-02-13 03:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27][root][INFO] - Training Epoch: 1/2, step 4128/7134 completed (loss: 0.02508183754980564, acc: 0.992175281047821)
[2025-02-13 03:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:28][root][INFO] - Training Epoch: 1/2, step 4129/7134 completed (loss: 0.028904074802994728, acc: 0.9940652847290039)
[2025-02-13 03:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:28][root][INFO] - Training Epoch: 1/2, step 4130/7134 completed (loss: 0.04913530498743057, acc: 0.9903661012649536)
[2025-02-13 03:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29][root][INFO] - Training Epoch: 1/2, step 4131/7134 completed (loss: 0.01656055822968483, acc: 0.996303141117096)
[2025-02-13 03:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29][root][INFO] - Training Epoch: 1/2, step 4132/7134 completed (loss: 0.024420902132987976, acc: 0.9910714030265808)
[2025-02-13 03:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29][root][INFO] - Training Epoch: 1/2, step 4133/7134 completed (loss: 0.05876535177230835, acc: 0.9814814925193787)
[2025-02-13 03:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30][root][INFO] - Training Epoch: 1/2, step 4134/7134 completed (loss: 0.08969661593437195, acc: 0.971563994884491)
[2025-02-13 03:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30][root][INFO] - Training Epoch: 1/2, step 4135/7134 completed (loss: 0.02955467253923416, acc: 0.9893428087234497)
[2025-02-13 03:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30][root][INFO] - Training Epoch: 1/2, step 4136/7134 completed (loss: 0.020841805264353752, acc: 0.9946902394294739)
[2025-02-13 03:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:31][root][INFO] - Training Epoch: 1/2, step 4137/7134 completed (loss: 0.05399439111351967, acc: 0.984375)
[2025-02-13 03:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:31][root][INFO] - Training Epoch: 1/2, step 4138/7134 completed (loss: 0.04910273477435112, acc: 0.982758641242981)
[2025-02-13 03:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32][root][INFO] - Training Epoch: 1/2, step 4139/7134 completed (loss: 0.04223030433058739, acc: 0.991416335105896)
[2025-02-13 03:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32][root][INFO] - Training Epoch: 1/2, step 4140/7134 completed (loss: 0.04648946225643158, acc: 0.9875862002372742)
[2025-02-13 03:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33][root][INFO] - Training Epoch: 1/2, step 4141/7134 completed (loss: 0.12114432454109192, acc: 0.9785100221633911)
[2025-02-13 03:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33][root][INFO] - Training Epoch: 1/2, step 4142/7134 completed (loss: 0.1295461654663086, acc: 0.9656750559806824)
[2025-02-13 03:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33][root][INFO] - Training Epoch: 1/2, step 4143/7134 completed (loss: 0.06005246192216873, acc: 0.9867346882820129)
[2025-02-13 03:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:34][root][INFO] - Training Epoch: 1/2, step 4144/7134 completed (loss: 0.102351114153862, acc: 0.9703872203826904)
[2025-02-13 03:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:34][root][INFO] - Training Epoch: 1/2, step 4145/7134 completed (loss: 0.06765174865722656, acc: 0.9765343070030212)
[2025-02-13 03:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:35][root][INFO] - Training Epoch: 1/2, step 4146/7134 completed (loss: 0.14472848176956177, acc: 0.9676767587661743)
[2025-02-13 03:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:35][root][INFO] - Training Epoch: 1/2, step 4147/7134 completed (loss: 0.10741299390792847, acc: 0.969348669052124)
[2025-02-13 03:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36][root][INFO] - Training Epoch: 1/2, step 4148/7134 completed (loss: 0.21384868025779724, acc: 0.9440154433250427)
[2025-02-13 03:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36][root][INFO] - Training Epoch: 1/2, step 4149/7134 completed (loss: 0.16827267408370972, acc: 0.9599999785423279)
[2025-02-13 03:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36][root][INFO] - Training Epoch: 1/2, step 4150/7134 completed (loss: 0.19433365762233734, acc: 0.9416499137878418)
[2025-02-13 03:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:37][root][INFO] - Training Epoch: 1/2, step 4151/7134 completed (loss: 0.21266911923885345, acc: 0.945652186870575)
[2025-02-13 03:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:37][root][INFO] - Training Epoch: 1/2, step 4152/7134 completed (loss: 0.08000664412975311, acc: 0.973724901676178)
[2025-02-13 03:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38][root][INFO] - Training Epoch: 1/2, step 4153/7134 completed (loss: 0.12100213021039963, acc: 0.9662576913833618)
[2025-02-13 03:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38][root][INFO] - Training Epoch: 1/2, step 4154/7134 completed (loss: 0.10085337609052658, acc: 0.9722222089767456)
[2025-02-13 03:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38][root][INFO] - Training Epoch: 1/2, step 4155/7134 completed (loss: 0.09842199087142944, acc: 0.9691252112388611)
[2025-02-13 03:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:39][root][INFO] - Training Epoch: 1/2, step 4156/7134 completed (loss: 0.09291639924049377, acc: 0.9638386368751526)
[2025-02-13 03:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:39][root][INFO] - Training Epoch: 1/2, step 4157/7134 completed (loss: 0.07640653103590012, acc: 0.9740518927574158)
[2025-02-13 03:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40][root][INFO] - Training Epoch: 1/2, step 4158/7134 completed (loss: 0.23567557334899902, acc: 0.9434447288513184)
[2025-02-13 03:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40][root][INFO] - Training Epoch: 1/2, step 4159/7134 completed (loss: 0.08713412284851074, acc: 0.9694397449493408)
[2025-02-13 03:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40][root][INFO] - Training Epoch: 1/2, step 4160/7134 completed (loss: 0.1544312983751297, acc: 0.9641509652137756)
[2025-02-13 03:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:41][root][INFO] - Training Epoch: 1/2, step 4161/7134 completed (loss: 0.08877916634082794, acc: 0.977911651134491)
[2025-02-13 03:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:41][root][INFO] - Training Epoch: 1/2, step 4162/7134 completed (loss: 0.1029646024107933, acc: 0.9761549830436707)
[2025-02-13 03:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:42][root][INFO] - Training Epoch: 1/2, step 4163/7134 completed (loss: 0.13325735926628113, acc: 0.9667171239852905)
[2025-02-13 03:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:42][root][INFO] - Training Epoch: 1/2, step 4164/7134 completed (loss: 0.0869215726852417, acc: 0.9825834631919861)
[2025-02-13 03:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:43][root][INFO] - Training Epoch: 1/2, step 4165/7134 completed (loss: 0.08699031919240952, acc: 0.9787765145301819)
[2025-02-13 03:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:43][root][INFO] - Training Epoch: 1/2, step 4166/7134 completed (loss: 0.07234950363636017, acc: 0.9823788404464722)
[2025-02-13 03:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44][root][INFO] - Training Epoch: 1/2, step 4167/7134 completed (loss: 0.04852014407515526, acc: 0.984375)
[2025-02-13 03:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44][root][INFO] - Training Epoch: 1/2, step 4168/7134 completed (loss: 0.059833161532878876, acc: 0.984375)
[2025-02-13 03:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44][root][INFO] - Training Epoch: 1/2, step 4169/7134 completed (loss: 0.08797768503427505, acc: 0.9831045269966125)
[2025-02-13 03:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:45][root][INFO] - Training Epoch: 1/2, step 4170/7134 completed (loss: 0.14176134765148163, acc: 0.9554139971733093)
[2025-02-13 03:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:45][root][INFO] - Training Epoch: 1/2, step 4171/7134 completed (loss: 0.048919159919023514, acc: 0.979238748550415)
[2025-02-13 03:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:46][root][INFO] - Training Epoch: 1/2, step 4172/7134 completed (loss: 0.07833784073591232, acc: 0.9812967777252197)
[2025-02-13 03:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:46][root][INFO] - Training Epoch: 1/2, step 4173/7134 completed (loss: 0.09378174692392349, acc: 0.9806867241859436)
[2025-02-13 03:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47][root][INFO] - Training Epoch: 1/2, step 4174/7134 completed (loss: 0.024591343477368355, acc: 0.99042147397995)
[2025-02-13 03:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47][root][INFO] - Training Epoch: 1/2, step 4175/7134 completed (loss: 0.0352872833609581, acc: 0.993127167224884)
[2025-02-13 03:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47][root][INFO] - Training Epoch: 1/2, step 4176/7134 completed (loss: 0.030866147950291634, acc: 0.9910581111907959)
[2025-02-13 03:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:48][root][INFO] - Training Epoch: 1/2, step 4177/7134 completed (loss: 0.029056517407298088, acc: 0.9934297204017639)
[2025-02-13 03:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:48][root][INFO] - Training Epoch: 1/2, step 4178/7134 completed (loss: 0.041464004665613174, acc: 0.9879153966903687)
[2025-02-13 03:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:49][root][INFO] - Training Epoch: 1/2, step 4179/7134 completed (loss: 0.031726591289043427, acc: 0.9895012974739075)
[2025-02-13 03:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:49][root][INFO] - Training Epoch: 1/2, step 4180/7134 completed (loss: 0.02570929005742073, acc: 0.991525411605835)
[2025-02-13 03:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50][root][INFO] - Training Epoch: 1/2, step 4181/7134 completed (loss: 0.021134093403816223, acc: 0.9967373609542847)
[2025-02-13 03:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50][root][INFO] - Training Epoch: 1/2, step 4182/7134 completed (loss: 0.015820251777768135, acc: 0.9958275556564331)
[2025-02-13 03:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51][root][INFO] - Training Epoch: 1/2, step 4183/7134 completed (loss: 0.02730812132358551, acc: 0.9902642369270325)
[2025-02-13 03:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51][root][INFO] - Training Epoch: 1/2, step 4184/7134 completed (loss: 0.054869238287210464, acc: 0.9840849041938782)
[2025-02-13 03:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51][root][INFO] - Training Epoch: 1/2, step 4185/7134 completed (loss: 0.024048607796430588, acc: 0.9931507110595703)
[2025-02-13 03:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:52][root][INFO] - Training Epoch: 1/2, step 4186/7134 completed (loss: 0.08843453973531723, acc: 0.9831528067588806)
[2025-02-13 03:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:52][root][INFO] - Training Epoch: 1/2, step 4187/7134 completed (loss: 0.011636457405984402, acc: 0.99609375)
[2025-02-13 03:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:53][root][INFO] - Training Epoch: 1/2, step 4188/7134 completed (loss: 0.014107867144048214, acc: 0.9950000047683716)
[2025-02-13 03:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:53][root][INFO] - Training Epoch: 1/2, step 4189/7134 completed (loss: 0.02978353761136532, acc: 0.9916083812713623)
[2025-02-13 03:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54][root][INFO] - Training Epoch: 1/2, step 4190/7134 completed (loss: 0.030344940721988678, acc: 0.9943181872367859)
[2025-02-13 03:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54][root][INFO] - Training Epoch: 1/2, step 4191/7134 completed (loss: 0.05735376849770546, acc: 0.9861751198768616)
[2025-02-13 03:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54][root][INFO] - Training Epoch: 1/2, step 4192/7134 completed (loss: 0.022594578564167023, acc: 0.9920127987861633)
[2025-02-13 03:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:55][root][INFO] - Training Epoch: 1/2, step 4193/7134 completed (loss: 0.04756541550159454, acc: 0.9866310358047485)
[2025-02-13 03:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:55][root][INFO] - Training Epoch: 1/2, step 4194/7134 completed (loss: 0.010486072860658169, acc: 0.9954614043235779)
[2025-02-13 03:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56][root][INFO] - Training Epoch: 1/2, step 4195/7134 completed (loss: 0.04868068918585777, acc: 0.989159882068634)
[2025-02-13 03:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56][root][INFO] - Training Epoch: 1/2, step 4196/7134 completed (loss: 0.03148331865668297, acc: 0.9896193742752075)
[2025-02-13 03:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56][root][INFO] - Training Epoch: 1/2, step 4197/7134 completed (loss: 0.04782147333025932, acc: 0.9870634078979492)
[2025-02-13 03:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:57][root][INFO] - Training Epoch: 1/2, step 4198/7134 completed (loss: 0.02752143330872059, acc: 0.9972222447395325)
[2025-02-13 03:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:57][root][INFO] - Training Epoch: 1/2, step 4199/7134 completed (loss: 0.05641058832406998, acc: 0.9835766553878784)
[2025-02-13 03:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:58][root][INFO] - Training Epoch: 1/2, step 4200/7134 completed (loss: 0.04254641756415367, acc: 0.9892473220825195)
[2025-02-13 03:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:58][root][INFO] - Training Epoch: 1/2, step 4201/7134 completed (loss: 0.033031679689884186, acc: 0.9931507110595703)
[2025-02-13 03:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59][root][INFO] - Training Epoch: 1/2, step 4202/7134 completed (loss: 0.06877080351114273, acc: 0.9844961166381836)
[2025-02-13 03:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59][root][INFO] - Training Epoch: 1/2, step 4203/7134 completed (loss: 0.05627838522195816, acc: 0.9828392863273621)
[2025-02-13 03:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59][root][INFO] - Training Epoch: 1/2, step 4204/7134 completed (loss: 0.11137948930263519, acc: 0.9663742780685425)
[2025-02-13 03:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:00][root][INFO] - Training Epoch: 1/2, step 4205/7134 completed (loss: 0.07376623153686523, acc: 0.9798561334609985)
[2025-02-13 03:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:00][root][INFO] - Training Epoch: 1/2, step 4206/7134 completed (loss: 0.08937615901231766, acc: 0.9788434505462646)
[2025-02-13 03:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01][root][INFO] - Training Epoch: 1/2, step 4207/7134 completed (loss: 0.09620381891727448, acc: 0.9840764403343201)
[2025-02-13 03:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01][root][INFO] - Training Epoch: 1/2, step 4208/7134 completed (loss: 0.0989360511302948, acc: 0.9776714444160461)
[2025-02-13 03:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01][root][INFO] - Training Epoch: 1/2, step 4209/7134 completed (loss: 0.10140355676412582, acc: 0.980169951915741)
[2025-02-13 03:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02][root][INFO] - Training Epoch: 1/2, step 4210/7134 completed (loss: 0.012267989106476307, acc: 0.9979550242424011)
[2025-02-13 03:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02][root][INFO] - Training Epoch: 1/2, step 4211/7134 completed (loss: 0.04667958617210388, acc: 0.9907833933830261)
[2025-02-13 03:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02][root][INFO] - Training Epoch: 1/2, step 4212/7134 completed (loss: 0.06777417659759521, acc: 0.978723406791687)
[2025-02-13 03:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:03][root][INFO] - Training Epoch: 1/2, step 4213/7134 completed (loss: 0.03904328867793083, acc: 0.9871244430541992)
[2025-02-13 03:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:03][root][INFO] - Training Epoch: 1/2, step 4214/7134 completed (loss: 0.08615124970674515, acc: 0.9791044592857361)
[2025-02-13 03:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:04][root][INFO] - Training Epoch: 1/2, step 4215/7134 completed (loss: 0.05859052762389183, acc: 0.9868131875991821)
[2025-02-13 03:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:04][root][INFO] - Training Epoch: 1/2, step 4216/7134 completed (loss: 0.06504829972982407, acc: 0.9817184805870056)
[2025-02-13 03:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05][root][INFO] - Training Epoch: 1/2, step 4217/7134 completed (loss: 0.028984781354665756, acc: 0.988811194896698)
[2025-02-13 03:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05][root][INFO] - Training Epoch: 1/2, step 4218/7134 completed (loss: 0.06272564083337784, acc: 0.9876288771629333)
[2025-02-13 03:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05][root][INFO] - Training Epoch: 1/2, step 4219/7134 completed (loss: 0.054426275193691254, acc: 0.9879518151283264)
[2025-02-13 03:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06][root][INFO] - Training Epoch: 1/2, step 4220/7134 completed (loss: 0.04606940224766731, acc: 0.9899280667304993)
[2025-02-13 03:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06][root][INFO] - Training Epoch: 1/2, step 4221/7134 completed (loss: 0.036787085235118866, acc: 0.991150438785553)
[2025-02-13 03:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06][root][INFO] - Training Epoch: 1/2, step 4222/7134 completed (loss: 0.17072108387947083, acc: 0.9733333587646484)
[2025-02-13 03:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:07][root][INFO] - Training Epoch: 1/2, step 4223/7134 completed (loss: 0.07290636748075485, acc: 0.9801324605941772)
[2025-02-13 03:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:07][root][INFO] - Training Epoch: 1/2, step 4224/7134 completed (loss: 0.0815969705581665, acc: 0.9762418866157532)
[2025-02-13 03:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08][root][INFO] - Training Epoch: 1/2, step 4225/7134 completed (loss: 0.1270361691713333, acc: 0.9779286980628967)
[2025-02-13 03:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08][root][INFO] - Training Epoch: 1/2, step 4226/7134 completed (loss: 0.07682664692401886, acc: 0.9795657992362976)
[2025-02-13 03:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08][root][INFO] - Training Epoch: 1/2, step 4227/7134 completed (loss: 0.12526080012321472, acc: 0.9705159664154053)
[2025-02-13 03:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:09][root][INFO] - Training Epoch: 1/2, step 4228/7134 completed (loss: 0.17014309763908386, acc: 0.9620689749717712)
[2025-02-13 03:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:09][root][INFO] - Training Epoch: 1/2, step 4229/7134 completed (loss: 0.1115124300122261, acc: 0.9790356159210205)
[2025-02-13 03:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10][root][INFO] - Training Epoch: 1/2, step 4230/7134 completed (loss: 0.08798816055059433, acc: 0.9735293984413147)
[2025-02-13 03:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10][root][INFO] - Training Epoch: 1/2, step 4231/7134 completed (loss: 0.19061043858528137, acc: 0.9558011293411255)
[2025-02-13 03:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10][root][INFO] - Training Epoch: 1/2, step 4232/7134 completed (loss: 0.12031377851963043, acc: 0.9648854732513428)
[2025-02-13 03:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:11][root][INFO] - Training Epoch: 1/2, step 4233/7134 completed (loss: 0.12095588445663452, acc: 0.9716193675994873)
[2025-02-13 03:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:11][root][INFO] - Training Epoch: 1/2, step 4234/7134 completed (loss: 0.07934112846851349, acc: 0.9825119376182556)
[2025-02-13 03:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12][root][INFO] - Training Epoch: 1/2, step 4235/7134 completed (loss: 0.05319846794009209, acc: 0.9842022061347961)
[2025-02-13 03:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12][root][INFO] - Training Epoch: 1/2, step 4236/7134 completed (loss: 0.02932715229690075, acc: 0.997474730014801)
[2025-02-13 03:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12][root][INFO] - Training Epoch: 1/2, step 4237/7134 completed (loss: 0.06872045993804932, acc: 0.9814432859420776)
[2025-02-13 03:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:13][root][INFO] - Training Epoch: 1/2, step 4238/7134 completed (loss: 0.07668844610452652, acc: 0.9803600907325745)
[2025-02-13 03:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:13][root][INFO] - Training Epoch: 1/2, step 4239/7134 completed (loss: 0.04057253897190094, acc: 0.9872000217437744)
[2025-02-13 03:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14][root][INFO] - Training Epoch: 1/2, step 4240/7134 completed (loss: 0.04998043179512024, acc: 0.9886914491653442)
[2025-02-13 03:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14][root][INFO] - Training Epoch: 1/2, step 4241/7134 completed (loss: 0.019851578399538994, acc: 0.9956616163253784)
[2025-02-13 03:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14][root][INFO] - Training Epoch: 1/2, step 4242/7134 completed (loss: 0.06709857285022736, acc: 0.9799554347991943)
[2025-02-13 03:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:15][root][INFO] - Training Epoch: 1/2, step 4243/7134 completed (loss: 0.08764436095952988, acc: 0.9819168448448181)
[2025-02-13 03:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:15][root][INFO] - Training Epoch: 1/2, step 4244/7134 completed (loss: 0.05782921612262726, acc: 0.9822616577148438)
[2025-02-13 03:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16][root][INFO] - Training Epoch: 1/2, step 4245/7134 completed (loss: 0.052569180727005005, acc: 0.9902200698852539)
[2025-02-13 03:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16][root][INFO] - Training Epoch: 1/2, step 4246/7134 completed (loss: 0.057443633675575256, acc: 0.9857369065284729)
[2025-02-13 03:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16][root][INFO] - Training Epoch: 1/2, step 4247/7134 completed (loss: 0.053577519953250885, acc: 0.9821746945381165)
[2025-02-13 03:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17][root][INFO] - Training Epoch: 1/2, step 4248/7134 completed (loss: 0.07929842174053192, acc: 0.9775640964508057)
[2025-02-13 03:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17][root][INFO] - Training Epoch: 1/2, step 4249/7134 completed (loss: 0.0884961411356926, acc: 0.9825396537780762)
[2025-02-13 03:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18][root][INFO] - Training Epoch: 1/2, step 4250/7134 completed (loss: 0.09461652487516403, acc: 0.9830148816108704)
[2025-02-13 03:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18][root][INFO] - Training Epoch: 1/2, step 4251/7134 completed (loss: 0.03931783139705658, acc: 0.9901315569877625)
[2025-02-13 03:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18][root][INFO] - Training Epoch: 1/2, step 4252/7134 completed (loss: 0.054532960057258606, acc: 0.9841017723083496)
[2025-02-13 03:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:19][root][INFO] - Training Epoch: 1/2, step 4253/7134 completed (loss: 0.08025220781564713, acc: 0.9804772138595581)
[2025-02-13 03:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:19][root][INFO] - Training Epoch: 1/2, step 4254/7134 completed (loss: 0.03708591312170029, acc: 0.9925187230110168)
[2025-02-13 03:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20][root][INFO] - Training Epoch: 1/2, step 4255/7134 completed (loss: 0.06099383533000946, acc: 0.9811643958091736)
[2025-02-13 03:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20][root][INFO] - Training Epoch: 1/2, step 4256/7134 completed (loss: 0.05295968055725098, acc: 0.9841628670692444)
[2025-02-13 03:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20][root][INFO] - Training Epoch: 1/2, step 4257/7134 completed (loss: 0.03079075738787651, acc: 0.9928698539733887)
[2025-02-13 03:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:21][root][INFO] - Training Epoch: 1/2, step 4258/7134 completed (loss: 0.022044984623789787, acc: 0.9942747950553894)
[2025-02-13 03:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:21][root][INFO] - Training Epoch: 1/2, step 4259/7134 completed (loss: 0.09057672321796417, acc: 0.9742120504379272)
[2025-02-13 03:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22][root][INFO] - Training Epoch: 1/2, step 4260/7134 completed (loss: 0.03822070732712746, acc: 0.9855072498321533)
[2025-02-13 03:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22][root][INFO] - Training Epoch: 1/2, step 4261/7134 completed (loss: 0.04523714631795883, acc: 0.9847036600112915)
[2025-02-13 03:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22][root][INFO] - Training Epoch: 1/2, step 4262/7134 completed (loss: 0.04420832544565201, acc: 0.9874804615974426)
[2025-02-13 03:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:23][root][INFO] - Training Epoch: 1/2, step 4263/7134 completed (loss: 0.08937152475118637, acc: 0.9847972989082336)
[2025-02-13 03:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:23][root][INFO] - Training Epoch: 1/2, step 4264/7134 completed (loss: 0.07222887873649597, acc: 0.9762202501296997)
[2025-02-13 03:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:24][root][INFO] - Training Epoch: 1/2, step 4265/7134 completed (loss: 0.04401366040110588, acc: 0.9861111044883728)
[2025-02-13 03:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:24][root][INFO] - Training Epoch: 1/2, step 4266/7134 completed (loss: 0.037747565656900406, acc: 0.9916167855262756)
[2025-02-13 03:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25][root][INFO] - Training Epoch: 1/2, step 4267/7134 completed (loss: 0.04916933551430702, acc: 0.9866666793823242)
[2025-02-13 03:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25][root][INFO] - Training Epoch: 1/2, step 4268/7134 completed (loss: 0.13754266500473022, acc: 0.9639278650283813)
[2025-02-13 03:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25][root][INFO] - Training Epoch: 1/2, step 4269/7134 completed (loss: 0.1758345514535904, acc: 0.9553956985473633)
[2025-02-13 03:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26][root][INFO] - Training Epoch: 1/2, step 4270/7134 completed (loss: 0.07875265926122665, acc: 0.9765625)
[2025-02-13 03:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26][root][INFO] - Training Epoch: 1/2, step 4271/7134 completed (loss: 0.05202551186084747, acc: 0.9825654029846191)
[2025-02-13 03:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:27][root][INFO] - Training Epoch: 1/2, step 4272/7134 completed (loss: 0.04873013496398926, acc: 0.9847418069839478)
[2025-02-13 03:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:27][root][INFO] - Training Epoch: 1/2, step 4273/7134 completed (loss: 0.05512956157326698, acc: 0.9849435091018677)
[2025-02-13 03:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:28][root][INFO] - Training Epoch: 1/2, step 4274/7134 completed (loss: 0.10377593338489532, acc: 0.9719626307487488)
[2025-02-13 03:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:28][root][INFO] - Training Epoch: 1/2, step 4275/7134 completed (loss: 0.041805706918239594, acc: 0.9873737096786499)
[2025-02-13 03:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29][root][INFO] - Training Epoch: 1/2, step 4276/7134 completed (loss: 0.07254400104284286, acc: 0.9793814420700073)
[2025-02-13 03:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29][root][INFO] - Training Epoch: 1/2, step 4277/7134 completed (loss: 0.03595510125160217, acc: 0.9904761910438538)
[2025-02-13 03:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29][root][INFO] - Training Epoch: 1/2, step 4278/7134 completed (loss: 0.06946293264627457, acc: 0.9814814925193787)
[2025-02-13 03:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:30][root][INFO] - Training Epoch: 1/2, step 4279/7134 completed (loss: 0.05456411838531494, acc: 0.9851190447807312)
[2025-02-13 03:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:30][root][INFO] - Training Epoch: 1/2, step 4280/7134 completed (loss: 0.027304688468575478, acc: 0.9873015880584717)
[2025-02-13 03:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:31][root][INFO] - Training Epoch: 1/2, step 4281/7134 completed (loss: 0.07321923971176147, acc: 0.9814356565475464)
[2025-02-13 03:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:31][root][INFO] - Training Epoch: 1/2, step 4282/7134 completed (loss: 0.025756031274795532, acc: 0.9925925731658936)
[2025-02-13 03:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32][root][INFO] - Training Epoch: 1/2, step 4283/7134 completed (loss: 0.03749583661556244, acc: 0.9896789193153381)
[2025-02-13 03:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32][root][INFO] - Training Epoch: 1/2, step 4284/7134 completed (loss: 0.06502556800842285, acc: 0.9818840622901917)
[2025-02-13 03:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32][root][INFO] - Training Epoch: 1/2, step 4285/7134 completed (loss: 0.054294150322675705, acc: 0.982876718044281)
[2025-02-13 03:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33][root][INFO] - Training Epoch: 1/2, step 4286/7134 completed (loss: 0.05376520752906799, acc: 0.9766187071800232)
[2025-02-13 03:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33][root][INFO] - Training Epoch: 1/2, step 4287/7134 completed (loss: 0.07182604819536209, acc: 0.9806700944900513)
[2025-02-13 03:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:34][root][INFO] - Training Epoch: 1/2, step 4288/7134 completed (loss: 0.07240170240402222, acc: 0.9815602898597717)
[2025-02-13 03:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:34][root][INFO] - Training Epoch: 1/2, step 4289/7134 completed (loss: 0.08396419137716293, acc: 0.9759398698806763)
[2025-02-13 03:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35][root][INFO] - Training Epoch: 1/2, step 4290/7134 completed (loss: 0.04531767591834068, acc: 0.9851149916648865)
[2025-02-13 03:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35][root][INFO] - Training Epoch: 1/2, step 4291/7134 completed (loss: 0.036891963332891464, acc: 0.9880810379981995)
[2025-02-13 03:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35][root][INFO] - Training Epoch: 1/2, step 4292/7134 completed (loss: 0.03569861128926277, acc: 0.990963876247406)
[2025-02-13 03:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36][root][INFO] - Training Epoch: 1/2, step 4293/7134 completed (loss: 0.034235741943120956, acc: 0.9879032373428345)
[2025-02-13 03:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36][root][INFO] - Training Epoch: 1/2, step 4294/7134 completed (loss: 0.05323725938796997, acc: 0.9883551597595215)
[2025-02-13 03:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:37][root][INFO] - Training Epoch: 1/2, step 4295/7134 completed (loss: 0.03488798066973686, acc: 0.989051103591919)
[2025-02-13 03:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:37][root][INFO] - Training Epoch: 1/2, step 4296/7134 completed (loss: 0.02545844204723835, acc: 0.9951338171958923)
[2025-02-13 03:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38][root][INFO] - Training Epoch: 1/2, step 4297/7134 completed (loss: 0.024380099028348923, acc: 0.9976525902748108)
[2025-02-13 03:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38][root][INFO] - Training Epoch: 1/2, step 4298/7134 completed (loss: 0.042454566806554794, acc: 0.9876203536987305)
[2025-02-13 03:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38][root][INFO] - Training Epoch: 1/2, step 4299/7134 completed (loss: 0.042472533881664276, acc: 0.9871060252189636)
[2025-02-13 03:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:39][root][INFO] - Training Epoch: 1/2, step 4300/7134 completed (loss: 0.024049686267971992, acc: 0.9912280440330505)
[2025-02-13 03:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:39][root][INFO] - Training Epoch: 1/2, step 4301/7134 completed (loss: 0.07038301229476929, acc: 0.9836065769195557)
[2025-02-13 03:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40][root][INFO] - Training Epoch: 1/2, step 4302/7134 completed (loss: 0.03404577076435089, acc: 0.9865671396255493)
[2025-02-13 03:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40][root][INFO] - Training Epoch: 1/2, step 4303/7134 completed (loss: 0.04601277783513069, acc: 0.9856801629066467)
[2025-02-13 03:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40][root][INFO] - Training Epoch: 1/2, step 4304/7134 completed (loss: 0.04519791156053543, acc: 0.9881129264831543)
[2025-02-13 03:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:41][root][INFO] - Training Epoch: 1/2, step 4305/7134 completed (loss: 0.05188006907701492, acc: 0.9882628917694092)
[2025-02-13 03:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:41][root][INFO] - Training Epoch: 1/2, step 4306/7134 completed (loss: 0.08930420875549316, acc: 0.9816031455993652)
[2025-02-13 03:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:42][root][INFO] - Training Epoch: 1/2, step 4307/7134 completed (loss: 0.08605636656284332, acc: 0.9759229421615601)
[2025-02-13 03:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:42][root][INFO] - Training Epoch: 1/2, step 4308/7134 completed (loss: 0.04868929088115692, acc: 0.9849187731742859)
[2025-02-13 03:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:43][root][INFO] - Training Epoch: 1/2, step 4309/7134 completed (loss: 0.03928327560424805, acc: 0.9901356101036072)
[2025-02-13 03:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:43][root][INFO] - Training Epoch: 1/2, step 4310/7134 completed (loss: 0.03165077045559883, acc: 0.9941245317459106)
[2025-02-13 03:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44][root][INFO] - Training Epoch: 1/2, step 4311/7134 completed (loss: 0.02311769314110279, acc: 0.9942029118537903)
[2025-02-13 03:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44][root][INFO] - Training Epoch: 1/2, step 4312/7134 completed (loss: 0.024105129763484, acc: 0.9931880235671997)
[2025-02-13 03:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44][root][INFO] - Training Epoch: 1/2, step 4313/7134 completed (loss: 0.03940580412745476, acc: 0.9922077655792236)
[2025-02-13 03:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:45][root][INFO] - Training Epoch: 1/2, step 4314/7134 completed (loss: 0.043800003826618195, acc: 0.9864048361778259)
[2025-02-13 03:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:45][root][INFO] - Training Epoch: 1/2, step 4315/7134 completed (loss: 0.058156080543994904, acc: 0.9803921580314636)
[2025-02-13 03:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46][root][INFO] - Training Epoch: 1/2, step 4316/7134 completed (loss: 0.04434603080153465, acc: 0.9872449040412903)
[2025-02-13 03:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46][root][INFO] - Training Epoch: 1/2, step 4317/7134 completed (loss: 0.046429168432950974, acc: 0.986369252204895)
[2025-02-13 03:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46][root][INFO] - Training Epoch: 1/2, step 4318/7134 completed (loss: 0.03450918570160866, acc: 0.9899103045463562)
[2025-02-13 03:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:47][root][INFO] - Training Epoch: 1/2, step 4319/7134 completed (loss: 0.02163904905319214, acc: 0.9910813570022583)
[2025-02-13 03:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:47][root][INFO] - Training Epoch: 1/2, step 4320/7134 completed (loss: 0.035084716975688934, acc: 0.9878378510475159)
[2025-02-13 03:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:48][root][INFO] - Training Epoch: 1/2, step 4321/7134 completed (loss: 0.05859041213989258, acc: 0.9838337302207947)
[2025-02-13 03:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:48][root][INFO] - Training Epoch: 1/2, step 4322/7134 completed (loss: 0.07261795550584793, acc: 0.9874213933944702)
[2025-02-13 03:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49][root][INFO] - Training Epoch: 1/2, step 4323/7134 completed (loss: 0.06308211386203766, acc: 0.9807322025299072)
[2025-02-13 03:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49][root][INFO] - Training Epoch: 1/2, step 4324/7134 completed (loss: 0.0216510072350502, acc: 0.9922480583190918)
[2025-02-13 03:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49][root][INFO] - Training Epoch: 1/2, step 4325/7134 completed (loss: 0.03156513720750809, acc: 0.9920254945755005)
[2025-02-13 03:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:50][root][INFO] - Training Epoch: 1/2, step 4326/7134 completed (loss: 0.056854233145713806, acc: 0.9823943376541138)
[2025-02-13 03:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:50][root][INFO] - Training Epoch: 1/2, step 4327/7134 completed (loss: 0.037314273416996, acc: 0.9860140085220337)
[2025-02-13 03:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51][root][INFO] - Training Epoch: 1/2, step 4328/7134 completed (loss: 0.06915774941444397, acc: 0.981203019618988)
[2025-02-13 03:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51][root][INFO] - Training Epoch: 1/2, step 4329/7134 completed (loss: 0.04607038199901581, acc: 0.9921362996101379)
[2025-02-13 03:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51][root][INFO] - Training Epoch: 1/2, step 4330/7134 completed (loss: 0.08087698370218277, acc: 0.9808102250099182)
[2025-02-13 03:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52][root][INFO] - Training Epoch: 1/2, step 4331/7134 completed (loss: 0.02542133443057537, acc: 0.9915013909339905)
[2025-02-13 03:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52][root][INFO] - Training Epoch: 1/2, step 4332/7134 completed (loss: 0.06876865774393082, acc: 0.9790874719619751)
[2025-02-13 03:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52][root][INFO] - Training Epoch: 1/2, step 4333/7134 completed (loss: 0.04385019838809967, acc: 0.9848254919052124)
[2025-02-13 03:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:53][root][INFO] - Training Epoch: 1/2, step 4334/7134 completed (loss: 0.06756501644849777, acc: 0.9805389046669006)
[2025-02-13 03:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:53][root][INFO] - Training Epoch: 1/2, step 4335/7134 completed (loss: 0.06403237581253052, acc: 0.9838709831237793)
[2025-02-13 03:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:54][root][INFO] - Training Epoch: 1/2, step 4336/7134 completed (loss: 0.024330873042345047, acc: 0.9886731505393982)
[2025-02-13 03:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:54][root][INFO] - Training Epoch: 1/2, step 4337/7134 completed (loss: 0.03302309662103653, acc: 0.9889240264892578)
[2025-02-13 03:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55][root][INFO] - Training Epoch: 1/2, step 4338/7134 completed (loss: 0.022725122049450874, acc: 0.9952606558799744)
[2025-02-13 03:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55][root][INFO] - Training Epoch: 1/2, step 4339/7134 completed (loss: 0.06192612648010254, acc: 0.9839650392532349)
[2025-02-13 03:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55][root][INFO] - Training Epoch: 1/2, step 4340/7134 completed (loss: 0.029967082664370537, acc: 0.9915540814399719)
[2025-02-13 03:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:56][root][INFO] - Training Epoch: 1/2, step 4341/7134 completed (loss: 0.043960507959127426, acc: 0.9866270422935486)
[2025-02-13 03:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:56][root][INFO] - Training Epoch: 1/2, step 4342/7134 completed (loss: 0.047098785638809204, acc: 0.9839743375778198)
[2025-02-13 03:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57][root][INFO] - Training Epoch: 1/2, step 4343/7134 completed (loss: 0.030066492035984993, acc: 0.9933444261550903)
[2025-02-13 03:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57][root][INFO] - Training Epoch: 1/2, step 4344/7134 completed (loss: 0.01861075684428215, acc: 0.9979209899902344)
[2025-02-13 03:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57][root][INFO] - Training Epoch: 1/2, step 4345/7134 completed (loss: 0.03562312573194504, acc: 0.9875776171684265)
[2025-02-13 03:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:58][root][INFO] - Training Epoch: 1/2, step 4346/7134 completed (loss: 0.020075827836990356, acc: 0.9937888383865356)
[2025-02-13 03:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:58][root][INFO] - Training Epoch: 1/2, step 4347/7134 completed (loss: 0.07880668342113495, acc: 0.9801980257034302)
[2025-02-13 03:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59][root][INFO] - Training Epoch: 1/2, step 4348/7134 completed (loss: 0.02954866550862789, acc: 0.9919999837875366)
[2025-02-13 03:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59][root][INFO] - Training Epoch: 1/2, step 4349/7134 completed (loss: 0.01831357553601265, acc: 0.9930716156959534)
[2025-02-13 03:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59][root][INFO] - Training Epoch: 1/2, step 4350/7134 completed (loss: 0.03758615627884865, acc: 0.9908376932144165)
[2025-02-13 03:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:00][root][INFO] - Training Epoch: 1/2, step 4351/7134 completed (loss: 0.09010186046361923, acc: 0.9794238805770874)
[2025-02-13 03:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:00][root][INFO] - Training Epoch: 1/2, step 4352/7134 completed (loss: 0.06598355621099472, acc: 0.9759188890457153)
[2025-02-13 03:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01][root][INFO] - Training Epoch: 1/2, step 4353/7134 completed (loss: 0.05951413884758949, acc: 0.9801324605941772)
[2025-02-13 03:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01][root][INFO] - Training Epoch: 1/2, step 4354/7134 completed (loss: 0.039393506944179535, acc: 0.9853333234786987)
[2025-02-13 03:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01][root][INFO] - Training Epoch: 1/2, step 4355/7134 completed (loss: 0.03776835650205612, acc: 0.991239070892334)
[2025-02-13 03:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:02][root][INFO] - Training Epoch: 1/2, step 4356/7134 completed (loss: 0.0466778539121151, acc: 0.9844683408737183)
[2025-02-13 03:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:02][root][INFO] - Training Epoch: 1/2, step 4357/7134 completed (loss: 0.07224535942077637, acc: 0.9795275330543518)
[2025-02-13 03:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:03][root][INFO] - Training Epoch: 1/2, step 4358/7134 completed (loss: 0.05662797763943672, acc: 0.9796407222747803)
[2025-02-13 03:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:03][root][INFO] - Training Epoch: 1/2, step 4359/7134 completed (loss: 0.07020162791013718, acc: 0.9778130054473877)
[2025-02-13 03:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04][root][INFO] - Training Epoch: 1/2, step 4360/7134 completed (loss: 0.07467364519834518, acc: 0.9776536226272583)
[2025-02-13 03:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04][root][INFO] - Training Epoch: 1/2, step 4361/7134 completed (loss: 0.03898446261882782, acc: 0.986994206905365)
[2025-02-13 03:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04][root][INFO] - Training Epoch: 1/2, step 4362/7134 completed (loss: 0.04613867402076721, acc: 0.9839704036712646)
[2025-02-13 03:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:05][root][INFO] - Training Epoch: 1/2, step 4363/7134 completed (loss: 0.05014681816101074, acc: 0.9851239919662476)
[2025-02-13 03:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:05][root][INFO] - Training Epoch: 1/2, step 4364/7134 completed (loss: 0.029446681961417198, acc: 0.989062488079071)
[2025-02-13 03:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06][root][INFO] - Training Epoch: 1/2, step 4365/7134 completed (loss: 0.04054871201515198, acc: 0.9874213933944702)
[2025-02-13 03:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06][root][INFO] - Training Epoch: 1/2, step 4366/7134 completed (loss: 0.07173724472522736, acc: 0.9742268323898315)
[2025-02-13 03:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07][root][INFO] - Training Epoch: 1/2, step 4367/7134 completed (loss: 0.044869303703308105, acc: 0.984240710735321)
[2025-02-13 03:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07][root][INFO] - Training Epoch: 1/2, step 4368/7134 completed (loss: 0.025614002719521523, acc: 0.99370276927948)
[2025-02-13 03:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07][root][INFO] - Training Epoch: 1/2, step 4369/7134 completed (loss: 0.032409995794296265, acc: 0.9866666793823242)
[2025-02-13 03:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:08][root][INFO] - Training Epoch: 1/2, step 4370/7134 completed (loss: 0.052056826651096344, acc: 0.9886506795883179)
[2025-02-13 03:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:08][root][INFO] - Training Epoch: 1/2, step 4371/7134 completed (loss: 0.05317091569304466, acc: 0.9828495979309082)
[2025-02-13 03:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:09][root][INFO] - Training Epoch: 1/2, step 4372/7134 completed (loss: 0.04446778818964958, acc: 0.9879518151283264)
[2025-02-13 03:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:09][root][INFO] - Training Epoch: 1/2, step 4373/7134 completed (loss: 0.029119832441210747, acc: 0.9881094098091125)
[2025-02-13 03:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10][root][INFO] - Training Epoch: 1/2, step 4374/7134 completed (loss: 0.048356831073760986, acc: 0.9856114983558655)
[2025-02-13 03:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10][root][INFO] - Training Epoch: 1/2, step 4375/7134 completed (loss: 0.02960095927119255, acc: 0.9897040128707886)
[2025-02-13 03:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10][root][INFO] - Training Epoch: 1/2, step 4376/7134 completed (loss: 0.04933098331093788, acc: 0.9886877536773682)
[2025-02-13 03:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:11][root][INFO] - Training Epoch: 1/2, step 4377/7134 completed (loss: 0.06721169501543045, acc: 0.988875150680542)
[2025-02-13 03:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:11][root][INFO] - Training Epoch: 1/2, step 4378/7134 completed (loss: 0.06215411424636841, acc: 0.9852941036224365)
[2025-02-13 03:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:12][root][INFO] - Training Epoch: 1/2, step 4379/7134 completed (loss: 0.10614745318889618, acc: 0.9678030014038086)
[2025-02-13 03:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:12][root][INFO] - Training Epoch: 1/2, step 4380/7134 completed (loss: 0.060516390949487686, acc: 0.985989511013031)
[2025-02-13 03:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13][root][INFO] - Training Epoch: 1/2, step 4381/7134 completed (loss: 0.029450982809066772, acc: 0.9889655113220215)
[2025-02-13 03:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13][root][INFO] - Training Epoch: 1/2, step 4382/7134 completed (loss: 0.02920255996286869, acc: 0.9920508861541748)
[2025-02-13 03:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13][root][INFO] - Training Epoch: 1/2, step 4383/7134 completed (loss: 0.0653877854347229, acc: 0.9807427525520325)
[2025-02-13 03:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:14][root][INFO] - Training Epoch: 1/2, step 4384/7134 completed (loss: 0.03161996975541115, acc: 0.9912060499191284)
[2025-02-13 03:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:14][root][INFO] - Training Epoch: 1/2, step 4385/7134 completed (loss: 0.07891152054071426, acc: 0.9739243984222412)
[2025-02-13 03:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:15][root][INFO] - Training Epoch: 1/2, step 4386/7134 completed (loss: 0.061654459685087204, acc: 0.9810426831245422)
[2025-02-13 03:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:15][root][INFO] - Training Epoch: 1/2, step 4387/7134 completed (loss: 0.06417697668075562, acc: 0.9838969111442566)
[2025-02-13 03:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16][root][INFO] - Training Epoch: 1/2, step 4388/7134 completed (loss: 0.050456516444683075, acc: 0.9813664555549622)
[2025-02-13 03:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16][root][INFO] - Training Epoch: 1/2, step 4389/7134 completed (loss: 0.0912180095911026, acc: 0.9771615266799927)
[2025-02-13 03:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16][root][INFO] - Training Epoch: 1/2, step 4390/7134 completed (loss: 0.09485353529453278, acc: 0.9761570692062378)
[2025-02-13 03:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:17][root][INFO] - Training Epoch: 1/2, step 4391/7134 completed (loss: 0.10034910589456558, acc: 0.9641693830490112)
[2025-02-13 03:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:17][root][INFO] - Training Epoch: 1/2, step 4392/7134 completed (loss: 0.038182202726602554, acc: 0.9914215803146362)
[2025-02-13 03:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:18][root][INFO] - Training Epoch: 1/2, step 4393/7134 completed (loss: 0.03881922364234924, acc: 0.9887482523918152)
[2025-02-13 03:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:18][root][INFO] - Training Epoch: 1/2, step 4394/7134 completed (loss: 0.043413881212472916, acc: 0.9873060584068298)
[2025-02-13 03:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19][root][INFO] - Training Epoch: 1/2, step 4395/7134 completed (loss: 0.07300885766744614, acc: 0.971742570400238)
[2025-02-13 03:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19][root][INFO] - Training Epoch: 1/2, step 4396/7134 completed (loss: 0.12262777239084244, acc: 0.9607046246528625)
[2025-02-13 03:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19][root][INFO] - Training Epoch: 1/2, step 4397/7134 completed (loss: 0.06386398524045944, acc: 0.9788235425949097)
[2025-02-13 03:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:20][root][INFO] - Training Epoch: 1/2, step 4398/7134 completed (loss: 0.024808615446090698, acc: 0.9924242496490479)
[2025-02-13 03:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:20][root][INFO] - Training Epoch: 1/2, step 4399/7134 completed (loss: 0.07889090478420258, acc: 0.9807074069976807)
[2025-02-13 03:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21][root][INFO] - Training Epoch: 1/2, step 4400/7134 completed (loss: 0.09806086122989655, acc: 0.9764559864997864)
[2025-02-13 03:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21][root][INFO] - Training Epoch: 1/2, step 4401/7134 completed (loss: 0.10770546644926071, acc: 0.97826087474823)
[2025-02-13 03:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21][root][INFO] - Training Epoch: 1/2, step 4402/7134 completed (loss: 0.16169659793376923, acc: 0.9563953280448914)
[2025-02-13 03:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:22][root][INFO] - Training Epoch: 1/2, step 4403/7134 completed (loss: 0.22174951434135437, acc: 0.9455958604812622)
[2025-02-13 03:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:22][root][INFO] - Training Epoch: 1/2, step 4404/7134 completed (loss: 0.17522525787353516, acc: 0.9507978558540344)
[2025-02-13 03:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:23][root][INFO] - Training Epoch: 1/2, step 4405/7134 completed (loss: 0.04628541320562363, acc: 0.988120973110199)
[2025-02-13 03:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:23][root][INFO] - Training Epoch: 1/2, step 4406/7134 completed (loss: 0.05121300369501114, acc: 0.9844098091125488)
[2025-02-13 03:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:24][root][INFO] - Training Epoch: 1/2, step 4407/7134 completed (loss: 0.07828184217214584, acc: 0.981502890586853)
[2025-02-13 03:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:24][root][INFO] - Training Epoch: 1/2, step 4408/7134 completed (loss: 0.11875498294830322, acc: 0.9662398099899292)
[2025-02-13 03:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25][root][INFO] - Training Epoch: 1/2, step 4409/7134 completed (loss: 0.09933669120073318, acc: 0.9706601500511169)
[2025-02-13 03:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25][root][INFO] - Training Epoch: 1/2, step 4410/7134 completed (loss: 0.13425861299037933, acc: 0.9695340394973755)
[2025-02-13 03:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25][root][INFO] - Training Epoch: 1/2, step 4411/7134 completed (loss: 0.1519332379102707, acc: 0.9582734107971191)
[2025-02-13 03:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:26][root][INFO] - Training Epoch: 1/2, step 4412/7134 completed (loss: 0.2076592743396759, acc: 0.940119743347168)
[2025-02-13 03:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:26][root][INFO] - Training Epoch: 1/2, step 4413/7134 completed (loss: 0.11501890420913696, acc: 0.9727891087532043)
[2025-02-13 03:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27][root][INFO] - Training Epoch: 1/2, step 4414/7134 completed (loss: 0.05161118134856224, acc: 0.9834123253822327)
[2025-02-13 03:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27][root][INFO] - Training Epoch: 1/2, step 4415/7134 completed (loss: 0.11232408881187439, acc: 0.9713574051856995)
[2025-02-13 03:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27][root][INFO] - Training Epoch: 1/2, step 4416/7134 completed (loss: 0.08098859339952469, acc: 0.9789965152740479)
[2025-02-13 03:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:28][root][INFO] - Training Epoch: 1/2, step 4417/7134 completed (loss: 0.09958937764167786, acc: 0.973557710647583)
[2025-02-13 03:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:28][root][INFO] - Training Epoch: 1/2, step 4418/7134 completed (loss: 0.0782356783747673, acc: 0.9778645634651184)
[2025-02-13 03:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:29][root][INFO] - Training Epoch: 1/2, step 4419/7134 completed (loss: 0.0980226993560791, acc: 0.9699519276618958)
[2025-02-13 03:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:29][root][INFO] - Training Epoch: 1/2, step 4420/7134 completed (loss: 0.11981969326734543, acc: 0.9681881070137024)
[2025-02-13 03:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:30][root][INFO] - Training Epoch: 1/2, step 4421/7134 completed (loss: 0.07875373959541321, acc: 0.9820022583007812)
[2025-02-13 03:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:30][root][INFO] - Training Epoch: 1/2, step 4422/7134 completed (loss: 0.13066640496253967, acc: 0.9670329689979553)
[2025-02-13 03:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31][root][INFO] - Training Epoch: 1/2, step 4423/7134 completed (loss: 0.0861087515950203, acc: 0.9714611768722534)
[2025-02-13 03:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31][root][INFO] - Training Epoch: 1/2, step 4424/7134 completed (loss: 0.08305086940526962, acc: 0.9759759902954102)
[2025-02-13 03:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31][root][INFO] - Training Epoch: 1/2, step 4425/7134 completed (loss: 0.05435898154973984, acc: 0.9812865257263184)
[2025-02-13 03:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:32][root][INFO] - Training Epoch: 1/2, step 4426/7134 completed (loss: 0.07526024430990219, acc: 0.9782833456993103)
[2025-02-13 03:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:32][root][INFO] - Training Epoch: 1/2, step 4427/7134 completed (loss: 0.15914183855056763, acc: 0.9614458084106445)
[2025-02-13 03:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33][root][INFO] - Training Epoch: 1/2, step 4428/7134 completed (loss: 0.03556542843580246, acc: 0.98591548204422)
[2025-02-13 03:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33][root][INFO] - Training Epoch: 1/2, step 4429/7134 completed (loss: 0.08390901237726212, acc: 0.969072163105011)
[2025-02-13 03:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33][root][INFO] - Training Epoch: 1/2, step 4430/7134 completed (loss: 0.0827905461192131, acc: 0.9710366129875183)
[2025-02-13 03:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34][root][INFO] - Training Epoch: 1/2, step 4431/7134 completed (loss: 0.08833363652229309, acc: 0.9657853841781616)
[2025-02-13 03:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34][root][INFO] - Training Epoch: 1/2, step 4432/7134 completed (loss: 0.03300505504012108, acc: 0.9923195242881775)
[2025-02-13 03:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35][root][INFO] - Training Epoch: 1/2, step 4433/7134 completed (loss: 0.05455012619495392, acc: 0.9825174808502197)
[2025-02-13 03:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35][root][INFO] - Training Epoch: 1/2, step 4434/7134 completed (loss: 0.04197283461689949, acc: 0.9864864945411682)
[2025-02-13 03:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35][root][INFO] - Training Epoch: 1/2, step 4435/7134 completed (loss: 0.041979141533374786, acc: 0.9852125644683838)
[2025-02-13 03:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36][root][INFO] - Training Epoch: 1/2, step 4436/7134 completed (loss: 0.0656137764453888, acc: 0.9762569665908813)
[2025-02-13 03:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36][root][INFO] - Training Epoch: 1/2, step 4437/7134 completed (loss: 0.02710968255996704, acc: 0.990813672542572)
[2025-02-13 03:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:37][root][INFO] - Training Epoch: 1/2, step 4438/7134 completed (loss: 0.04972745105624199, acc: 0.9813559055328369)
[2025-02-13 03:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:37][root][INFO] - Training Epoch: 1/2, step 4439/7134 completed (loss: 0.02737530693411827, acc: 0.9902439117431641)
[2025-02-13 03:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38][root][INFO] - Training Epoch: 1/2, step 4440/7134 completed (loss: 0.037382014095783234, acc: 0.9848066568374634)
[2025-02-13 03:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38][root][INFO] - Training Epoch: 1/2, step 4441/7134 completed (loss: 0.04769976809620857, acc: 0.9846389889717102)
[2025-02-13 03:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38][root][INFO] - Training Epoch: 1/2, step 4442/7134 completed (loss: 0.08861701935529709, acc: 0.9794079661369324)
[2025-02-13 03:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39][root][INFO] - Training Epoch: 1/2, step 4443/7134 completed (loss: 0.07934464514255524, acc: 0.9807692170143127)
[2025-02-13 03:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39][root][INFO] - Training Epoch: 1/2, step 4444/7134 completed (loss: 0.060589902102947235, acc: 0.980424165725708)
[2025-02-13 03:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:40][root][INFO] - Training Epoch: 1/2, step 4445/7134 completed (loss: 0.0728655681014061, acc: 0.9790502786636353)
[2025-02-13 03:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:40][root][INFO] - Training Epoch: 1/2, step 4446/7134 completed (loss: 0.053263064473867416, acc: 0.9806763529777527)
[2025-02-13 03:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41][root][INFO] - Training Epoch: 1/2, step 4447/7134 completed (loss: 0.03890063241124153, acc: 0.9905660152435303)
[2025-02-13 03:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41][root][INFO] - Training Epoch: 1/2, step 4448/7134 completed (loss: 0.04647884890437126, acc: 0.9817184805870056)
[2025-02-13 03:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41][root][INFO] - Training Epoch: 1/2, step 4449/7134 completed (loss: 0.10888209939002991, acc: 0.9702602028846741)
[2025-02-13 03:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:42][root][INFO] - Training Epoch: 1/2, step 4450/7134 completed (loss: 0.07756026089191437, acc: 0.9688888788223267)
[2025-02-13 03:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:42][root][INFO] - Training Epoch: 1/2, step 4451/7134 completed (loss: 0.12571369111537933, acc: 0.9751243591308594)
[2025-02-13 03:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43][root][INFO] - Training Epoch: 1/2, step 4452/7134 completed (loss: 0.029105056077241898, acc: 0.9872000217437744)
[2025-02-13 03:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43][root][INFO] - Training Epoch: 1/2, step 4453/7134 completed (loss: 0.06061476841568947, acc: 0.9836065769195557)
[2025-02-13 03:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43][root][INFO] - Training Epoch: 1/2, step 4454/7134 completed (loss: 0.05153375491499901, acc: 0.9858356714248657)
[2025-02-13 03:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44][root][INFO] - Training Epoch: 1/2, step 4455/7134 completed (loss: 0.14318643510341644, acc: 0.9613259434700012)
[2025-02-13 03:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44][root][INFO] - Training Epoch: 1/2, step 4456/7134 completed (loss: 0.029078202322125435, acc: 0.9886731505393982)
[2025-02-13 03:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44][root][INFO] - Training Epoch: 1/2, step 4457/7134 completed (loss: 0.08358924090862274, acc: 0.9700374603271484)
[2025-02-13 03:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:45][root][INFO] - Training Epoch: 1/2, step 4458/7134 completed (loss: 0.01753390021622181, acc: 0.9967266917228699)
[2025-02-13 03:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:45][root][INFO] - Training Epoch: 1/2, step 4459/7134 completed (loss: 0.030773570761084557, acc: 0.9903846383094788)
[2025-02-13 03:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:46][root][INFO] - Training Epoch: 1/2, step 4460/7134 completed (loss: 0.03437807038426399, acc: 0.9889975786209106)
[2025-02-13 03:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:46][root][INFO] - Training Epoch: 1/2, step 4461/7134 completed (loss: 0.08397602289915085, acc: 0.984009861946106)
[2025-02-13 03:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47][root][INFO] - Training Epoch: 1/2, step 4462/7134 completed (loss: 0.01727842353284359, acc: 0.9945504069328308)
[2025-02-13 03:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47][root][INFO] - Training Epoch: 1/2, step 4463/7134 completed (loss: 0.030556609854102135, acc: 0.9911949634552002)
[2025-02-13 03:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47][root][INFO] - Training Epoch: 1/2, step 4464/7134 completed (loss: 0.061037901788949966, acc: 0.9831365942955017)
[2025-02-13 03:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:48][root][INFO] - Training Epoch: 1/2, step 4465/7134 completed (loss: 0.032837435603141785, acc: 0.9896907210350037)
[2025-02-13 03:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:48][root][INFO] - Training Epoch: 1/2, step 4466/7134 completed (loss: 0.024887682870030403, acc: 0.9925925731658936)
[2025-02-13 03:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:49][root][INFO] - Training Epoch: 1/2, step 4467/7134 completed (loss: 0.04687916859984398, acc: 0.9861809015274048)
[2025-02-13 03:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:49][root][INFO] - Training Epoch: 1/2, step 4468/7134 completed (loss: 0.01567968726158142, acc: 0.9964328408241272)
[2025-02-13 03:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:50][root][INFO] - Training Epoch: 1/2, step 4469/7134 completed (loss: 0.04912152141332626, acc: 0.9884393215179443)
[2025-02-13 03:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:50][root][INFO] - Training Epoch: 1/2, step 4470/7134 completed (loss: 0.03646228089928627, acc: 0.9878453016281128)
[2025-02-13 03:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51][root][INFO] - Training Epoch: 1/2, step 4471/7134 completed (loss: 0.024033909663558006, acc: 0.9950860142707825)
[2025-02-13 03:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51][root][INFO] - Training Epoch: 1/2, step 4472/7134 completed (loss: 0.037641383707523346, acc: 0.992094874382019)
[2025-02-13 03:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51][root][INFO] - Training Epoch: 1/2, step 4473/7134 completed (loss: 0.081339992582798, acc: 0.976190447807312)
[2025-02-13 03:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:52][root][INFO] - Training Epoch: 1/2, step 4474/7134 completed (loss: 0.08500714600086212, acc: 0.9721059799194336)
[2025-02-13 03:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:52][root][INFO] - Training Epoch: 1/2, step 4475/7134 completed (loss: 0.1085677519440651, acc: 0.9714640378952026)
[2025-02-13 03:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:53][root][INFO] - Training Epoch: 1/2, step 4476/7134 completed (loss: 0.055495087057352066, acc: 0.9876543283462524)
[2025-02-13 03:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:53][root][INFO] - Training Epoch: 1/2, step 4477/7134 completed (loss: 0.012179004959762096, acc: 0.9963325262069702)
[2025-02-13 03:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54][root][INFO] - Training Epoch: 1/2, step 4478/7134 completed (loss: 0.026311377063393593, acc: 0.995055615901947)
[2025-02-13 03:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54][root][INFO] - Training Epoch: 1/2, step 4479/7134 completed (loss: 0.026762237772345543, acc: 0.993261456489563)
[2025-02-13 03:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54][root][INFO] - Training Epoch: 1/2, step 4480/7134 completed (loss: 0.03867131099104881, acc: 0.9906542301177979)
[2025-02-13 03:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:55][root][INFO] - Training Epoch: 1/2, step 4481/7134 completed (loss: 0.04740181565284729, acc: 0.9906542301177979)
[2025-02-13 03:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:55][root][INFO] - Training Epoch: 1/2, step 4482/7134 completed (loss: 0.014507905580103397, acc: 0.9976019263267517)
[2025-02-13 03:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56][root][INFO] - Training Epoch: 1/2, step 4483/7134 completed (loss: 0.03346889093518257, acc: 0.9940564632415771)
[2025-02-13 03:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56][root][INFO] - Training Epoch: 1/2, step 4484/7134 completed (loss: 0.024796301499009132, acc: 0.988095223903656)
[2025-02-13 03:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57][root][INFO] - Training Epoch: 1/2, step 4485/7134 completed (loss: 0.01600920595228672, acc: 0.9938744306564331)
[2025-02-13 03:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57][root][INFO] - Training Epoch: 1/2, step 4486/7134 completed (loss: 0.01579219102859497, acc: 0.9940298795700073)
[2025-02-13 03:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57][root][INFO] - Training Epoch: 1/2, step 4487/7134 completed (loss: 0.010830446146428585, acc: 0.9972413778305054)
[2025-02-13 03:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58][root][INFO] - Training Epoch: 1/2, step 4488/7134 completed (loss: 0.024527741596102715, acc: 0.9910072088241577)
[2025-02-13 03:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58][root][INFO] - Training Epoch: 1/2, step 4489/7134 completed (loss: 0.023853642866015434, acc: 0.993127167224884)
[2025-02-13 03:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58][root][INFO] - Training Epoch: 1/2, step 4490/7134 completed (loss: 0.038095057010650635, acc: 0.9888888597488403)
[2025-02-13 03:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:59][root][INFO] - Training Epoch: 1/2, step 4491/7134 completed (loss: 0.007433377206325531, acc: 1.0)
[2025-02-13 03:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:59][root][INFO] - Training Epoch: 1/2, step 4492/7134 completed (loss: 0.029511848464608192, acc: 0.9889705777168274)
[2025-02-13 03:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:00][root][INFO] - Training Epoch: 1/2, step 4493/7134 completed (loss: 0.011767924763262272, acc: 0.9972489476203918)
[2025-02-13 03:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:00][root][INFO] - Training Epoch: 1/2, step 4494/7134 completed (loss: 0.0498298704624176, acc: 0.9855282306671143)
[2025-02-13 03:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01][root][INFO] - Training Epoch: 1/2, step 4495/7134 completed (loss: 0.02141014114022255, acc: 0.9936143159866333)
[2025-02-13 03:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01][root][INFO] - Training Epoch: 1/2, step 4496/7134 completed (loss: 0.03422000631690025, acc: 0.9900285005569458)
[2025-02-13 03:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01][root][INFO] - Training Epoch: 1/2, step 4497/7134 completed (loss: 0.01993642747402191, acc: 0.9947019815444946)
[2025-02-13 03:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:02][root][INFO] - Training Epoch: 1/2, step 4498/7134 completed (loss: 0.06748487055301666, acc: 0.9912917017936707)
[2025-02-13 03:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:02][root][INFO] - Training Epoch: 1/2, step 4499/7134 completed (loss: 0.059311989694833755, acc: 0.9895651936531067)
[2025-02-13 03:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03][root][INFO] - Training Epoch: 1/2, step 4500/7134 completed (loss: 0.03350725769996643, acc: 0.9902098178863525)
[2025-02-13 03:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03][root][INFO] - Training Epoch: 1/2, step 4501/7134 completed (loss: 0.02738683670759201, acc: 0.9917920827865601)
[2025-02-13 03:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03][root][INFO] - Training Epoch: 1/2, step 4502/7134 completed (loss: 0.025486884638667107, acc: 0.9940029978752136)
[2025-02-13 03:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:04][root][INFO] - Training Epoch: 1/2, step 4503/7134 completed (loss: 0.018506713211536407, acc: 0.9935794472694397)
[2025-02-13 03:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:04][root][INFO] - Training Epoch: 1/2, step 4504/7134 completed (loss: 0.046635713428258896, acc: 0.9865471124649048)
[2025-02-13 03:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05][root][INFO] - Training Epoch: 1/2, step 4505/7134 completed (loss: 0.04579545930027962, acc: 0.9852579832077026)
[2025-02-13 03:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05][root][INFO] - Training Epoch: 1/2, step 4506/7134 completed (loss: 0.039132583886384964, acc: 0.9909909963607788)
[2025-02-13 03:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05][root][INFO] - Training Epoch: 1/2, step 4507/7134 completed (loss: 0.03494919091463089, acc: 0.9900426864624023)
[2025-02-13 03:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:06][root][INFO] - Training Epoch: 1/2, step 4508/7134 completed (loss: 0.044107094407081604, acc: 0.9886040091514587)
[2025-02-13 03:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:06][root][INFO] - Training Epoch: 1/2, step 4509/7134 completed (loss: 0.04993801936507225, acc: 0.9865771532058716)
[2025-02-13 03:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:07][root][INFO] - Training Epoch: 1/2, step 4510/7134 completed (loss: 0.048309326171875, acc: 0.9830747246742249)
[2025-02-13 03:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:07][root][INFO] - Training Epoch: 1/2, step 4511/7134 completed (loss: 0.07944121211767197, acc: 0.9789029359817505)
[2025-02-13 03:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08][root][INFO] - Training Epoch: 1/2, step 4512/7134 completed (loss: 0.07953772693872452, acc: 0.9767025113105774)
[2025-02-13 03:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08][root][INFO] - Training Epoch: 1/2, step 4513/7134 completed (loss: 0.036342885345220566, acc: 0.9901685118675232)
[2025-02-13 03:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08][root][INFO] - Training Epoch: 1/2, step 4514/7134 completed (loss: 0.09758376330137253, acc: 0.9766423106193542)
[2025-02-13 03:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:09][root][INFO] - Training Epoch: 1/2, step 4515/7134 completed (loss: 0.07388823479413986, acc: 0.9761193990707397)
[2025-02-13 03:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:09][root][INFO] - Training Epoch: 1/2, step 4516/7134 completed (loss: 0.061220843344926834, acc: 0.9791921377182007)
[2025-02-13 03:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:10][root][INFO] - Training Epoch: 1/2, step 4517/7134 completed (loss: 0.04602528363466263, acc: 0.98591548204422)
[2025-02-13 03:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:10][root][INFO] - Training Epoch: 1/2, step 4518/7134 completed (loss: 0.08633333444595337, acc: 0.980867326259613)
[2025-02-13 03:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11][root][INFO] - Training Epoch: 1/2, step 4519/7134 completed (loss: 0.04353965446352959, acc: 0.987908124923706)
[2025-02-13 03:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11][root][INFO] - Training Epoch: 1/2, step 4520/7134 completed (loss: 0.0536949448287487, acc: 0.9858757257461548)
[2025-02-13 03:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11][root][INFO] - Training Epoch: 1/2, step 4521/7134 completed (loss: 0.061887066811323166, acc: 0.9807460904121399)
[2025-02-13 03:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:12][root][INFO] - Training Epoch: 1/2, step 4522/7134 completed (loss: 0.07283712923526764, acc: 0.9798488616943359)
[2025-02-13 03:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:12][root][INFO] - Training Epoch: 1/2, step 4523/7134 completed (loss: 0.03544878959655762, acc: 0.9869822263717651)
[2025-02-13 03:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:13][root][INFO] - Training Epoch: 1/2, step 4524/7134 completed (loss: 0.06445399671792984, acc: 0.9838056564331055)
[2025-02-13 03:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:13][root][INFO] - Training Epoch: 1/2, step 4525/7134 completed (loss: 0.031568363308906555, acc: 0.9900000095367432)
[2025-02-13 03:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:14][root][INFO] - Training Epoch: 1/2, step 4526/7134 completed (loss: 0.07183372974395752, acc: 0.9780645370483398)
[2025-02-13 03:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:14][root][INFO] - Training Epoch: 1/2, step 4527/7134 completed (loss: 0.0321914367377758, acc: 0.9888517260551453)
[2025-02-13 03:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15][root][INFO] - Training Epoch: 1/2, step 4528/7134 completed (loss: 0.09287372976541519, acc: 0.9781491160392761)
[2025-02-13 03:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15][root][INFO] - Training Epoch: 1/2, step 4529/7134 completed (loss: 0.088809534907341, acc: 0.984455943107605)
[2025-02-13 03:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15][root][INFO] - Training Epoch: 1/2, step 4530/7134 completed (loss: 0.039838455617427826, acc: 0.9901960492134094)
[2025-02-13 03:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:16][root][INFO] - Training Epoch: 1/2, step 4531/7134 completed (loss: 0.04109371826052666, acc: 0.985637366771698)
[2025-02-13 03:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:16][root][INFO] - Training Epoch: 1/2, step 4532/7134 completed (loss: 0.06597134470939636, acc: 0.9850597381591797)
[2025-02-13 03:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:17][root][INFO] - Training Epoch: 1/2, step 4533/7134 completed (loss: 0.043342288583517075, acc: 0.9857650995254517)
[2025-02-13 03:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:17][root][INFO] - Training Epoch: 1/2, step 4534/7134 completed (loss: 0.029371963813900948, acc: 0.9906103014945984)
[2025-02-13 03:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18][root][INFO] - Training Epoch: 1/2, step 4535/7134 completed (loss: 0.048037491738796234, acc: 0.980637788772583)
[2025-02-13 03:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18][root][INFO] - Training Epoch: 1/2, step 4536/7134 completed (loss: 0.04938049614429474, acc: 0.9837164878845215)
[2025-02-13 03:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18][root][INFO] - Training Epoch: 1/2, step 4537/7134 completed (loss: 0.020480258390307426, acc: 0.9921787977218628)
[2025-02-13 03:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:19][root][INFO] - Training Epoch: 1/2, step 4538/7134 completed (loss: 0.06307316571474075, acc: 0.9839080572128296)
[2025-02-13 03:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:19][root][INFO] - Training Epoch: 1/2, step 4539/7134 completed (loss: 0.05685391649603844, acc: 0.9848974943161011)
[2025-02-13 03:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:20][root][INFO] - Training Epoch: 1/2, step 4540/7134 completed (loss: 0.03887207806110382, acc: 0.9898843765258789)
[2025-02-13 03:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:20][root][INFO] - Training Epoch: 1/2, step 4541/7134 completed (loss: 0.08431807160377502, acc: 0.975359320640564)
[2025-02-13 03:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21][root][INFO] - Training Epoch: 1/2, step 4542/7134 completed (loss: 0.08830282837152481, acc: 0.9783549904823303)
[2025-02-13 03:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21][root][INFO] - Training Epoch: 1/2, step 4543/7134 completed (loss: 0.09703224152326584, acc: 0.9696969985961914)
[2025-02-13 03:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21][root][INFO] - Training Epoch: 1/2, step 4544/7134 completed (loss: 0.06294696778059006, acc: 0.9832317233085632)
[2025-02-13 03:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:22][root][INFO] - Training Epoch: 1/2, step 4545/7134 completed (loss: 0.08009220659732819, acc: 0.978805422782898)
[2025-02-13 03:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:22][root][INFO] - Training Epoch: 1/2, step 4546/7134 completed (loss: 0.0733477994799614, acc: 0.9831932783126831)
[2025-02-13 03:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23][root][INFO] - Training Epoch: 1/2, step 4547/7134 completed (loss: 0.036849088966846466, acc: 0.987889289855957)
[2025-02-13 03:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23][root][INFO] - Training Epoch: 1/2, step 4548/7134 completed (loss: 0.061142485588788986, acc: 0.9850746393203735)
[2025-02-13 03:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23][root][INFO] - Training Epoch: 1/2, step 4549/7134 completed (loss: 0.06136101111769676, acc: 0.9871244430541992)
[2025-02-13 03:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:24][root][INFO] - Training Epoch: 1/2, step 4550/7134 completed (loss: 0.0861542671918869, acc: 0.9782244563102722)
[2025-02-13 03:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:24][root][INFO] - Training Epoch: 1/2, step 4551/7134 completed (loss: 0.04020385071635246, acc: 0.9885714054107666)
[2025-02-13 03:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25][root][INFO] - Training Epoch: 1/2, step 4552/7134 completed (loss: 0.04957966133952141, acc: 0.9866666793823242)
[2025-02-13 03:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25][root][INFO] - Training Epoch: 1/2, step 4553/7134 completed (loss: 0.040162939578294754, acc: 0.9915397763252258)
[2025-02-13 03:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25][root][INFO] - Training Epoch: 1/2, step 4554/7134 completed (loss: 0.0646645575761795, acc: 0.9819819927215576)
[2025-02-13 03:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26][root][INFO] - Training Epoch: 1/2, step 4555/7134 completed (loss: 0.06932257860898972, acc: 0.9806094169616699)
[2025-02-13 03:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26][root][INFO] - Training Epoch: 1/2, step 4556/7134 completed (loss: 0.07111102342605591, acc: 0.9744361042976379)
[2025-02-13 03:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26][root][INFO] - Training Epoch: 1/2, step 4557/7134 completed (loss: 0.09884094446897507, acc: 0.9781022071838379)
[2025-02-13 03:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:27][root][INFO] - Training Epoch: 1/2, step 4558/7134 completed (loss: 0.058290570974349976, acc: 0.9882155060768127)
[2025-02-13 03:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:27][root][INFO] - Training Epoch: 1/2, step 4559/7134 completed (loss: 0.0866270437836647, acc: 0.9831932783126831)
[2025-02-13 03:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28][root][INFO] - Training Epoch: 1/2, step 4560/7134 completed (loss: 0.04539749026298523, acc: 0.989130437374115)
[2025-02-13 03:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28][root][INFO] - Training Epoch: 1/2, step 4561/7134 completed (loss: 0.0686287060379982, acc: 0.9843137264251709)
[2025-02-13 03:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28][root][INFO] - Training Epoch: 1/2, step 4562/7134 completed (loss: 0.05176447704434395, acc: 0.9834254384040833)
[2025-02-13 03:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:29][root][INFO] - Training Epoch: 1/2, step 4563/7134 completed (loss: 0.03725460544228554, acc: 0.9908397197723389)
[2025-02-13 03:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:29][root][INFO] - Training Epoch: 1/2, step 4564/7134 completed (loss: 0.06416067481040955, acc: 0.9858956336975098)
[2025-02-13 03:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30][root][INFO] - Training Epoch: 1/2, step 4565/7134 completed (loss: 0.06680995225906372, acc: 0.9827916026115417)
[2025-02-13 03:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30][root][INFO] - Training Epoch: 1/2, step 4566/7134 completed (loss: 0.046264417469501495, acc: 0.9856630563735962)
[2025-02-13 03:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30][root][INFO] - Training Epoch: 1/2, step 4567/7134 completed (loss: 0.03017420694231987, acc: 0.9908088445663452)
[2025-02-13 03:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:31][root][INFO] - Training Epoch: 1/2, step 4568/7134 completed (loss: 0.06772956252098083, acc: 0.9839449524879456)
[2025-02-13 03:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:31][root][INFO] - Training Epoch: 1/2, step 4569/7134 completed (loss: 0.03385855257511139, acc: 0.9921568632125854)
[2025-02-13 03:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32][root][INFO] - Training Epoch: 1/2, step 4570/7134 completed (loss: 0.03727085888385773, acc: 0.9839650392532349)
[2025-02-13 03:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32][root][INFO] - Training Epoch: 1/2, step 4571/7134 completed (loss: 0.036130622029304504, acc: 0.9897304177284241)
[2025-02-13 03:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32][root][INFO] - Training Epoch: 1/2, step 4572/7134 completed (loss: 0.053947288542985916, acc: 0.9890965819358826)
[2025-02-13 03:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:33][root][INFO] - Training Epoch: 1/2, step 4573/7134 completed (loss: 0.047877855598926544, acc: 0.9859943985939026)
[2025-02-13 03:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:33][root][INFO] - Training Epoch: 1/2, step 4574/7134 completed (loss: 0.021945904940366745, acc: 0.9946996569633484)
[2025-02-13 03:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34][root][INFO] - Training Epoch: 1/2, step 4575/7134 completed (loss: 0.0456765852868557, acc: 0.9840425252914429)
[2025-02-13 03:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34][root][INFO] - Training Epoch: 1/2, step 4576/7134 completed (loss: 0.024114077910780907, acc: 0.9937264919281006)
[2025-02-13 03:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35][root][INFO] - Training Epoch: 1/2, step 4577/7134 completed (loss: 0.045370738953351974, acc: 0.9901685118675232)
[2025-02-13 03:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35][root][INFO] - Training Epoch: 1/2, step 4578/7134 completed (loss: 0.03940611332654953, acc: 0.9883551597595215)
[2025-02-13 03:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35][root][INFO] - Training Epoch: 1/2, step 4579/7134 completed (loss: 0.06396709382534027, acc: 0.9889065027236938)
[2025-02-13 03:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:36][root][INFO] - Training Epoch: 1/2, step 4580/7134 completed (loss: 0.023876557126641273, acc: 0.9925373196601868)
[2025-02-13 03:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:36][root][INFO] - Training Epoch: 1/2, step 4581/7134 completed (loss: 0.03271518275141716, acc: 0.9911816716194153)
[2025-02-13 03:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37][root][INFO] - Training Epoch: 1/2, step 4582/7134 completed (loss: 0.03383641690015793, acc: 0.9884868264198303)
[2025-02-13 03:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37][root][INFO] - Training Epoch: 1/2, step 4583/7134 completed (loss: 0.017832236364483833, acc: 0.9934640526771545)
[2025-02-13 03:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37][root][INFO] - Training Epoch: 1/2, step 4584/7134 completed (loss: 0.023399079218506813, acc: 0.9919484853744507)
[2025-02-13 03:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:38][root][INFO] - Training Epoch: 1/2, step 4585/7134 completed (loss: 0.03771935775876045, acc: 0.9854369163513184)
[2025-02-13 03:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:38][root][INFO] - Training Epoch: 1/2, step 4586/7134 completed (loss: 0.041503824293613434, acc: 0.9856957197189331)
[2025-02-13 03:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39][root][INFO] - Training Epoch: 1/2, step 4587/7134 completed (loss: 0.04458075761795044, acc: 0.9863013625144958)
[2025-02-13 03:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39][root][INFO] - Training Epoch: 1/2, step 4588/7134 completed (loss: 0.05621534213423729, acc: 0.9825581312179565)
[2025-02-13 03:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39][root][INFO] - Training Epoch: 1/2, step 4589/7134 completed (loss: 0.07490728795528412, acc: 0.976047933101654)
[2025-02-13 03:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:40][root][INFO] - Training Epoch: 1/2, step 4590/7134 completed (loss: 0.04525010287761688, acc: 0.9848066568374634)
[2025-02-13 03:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:40][root][INFO] - Training Epoch: 1/2, step 4591/7134 completed (loss: 0.04537573829293251, acc: 0.9828495979309082)
[2025-02-13 03:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41][root][INFO] - Training Epoch: 1/2, step 4592/7134 completed (loss: 0.049669601023197174, acc: 0.9854862093925476)
[2025-02-13 03:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41][root][INFO] - Training Epoch: 1/2, step 4593/7134 completed (loss: 0.05137477070093155, acc: 0.9837037324905396)
[2025-02-13 03:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42][root][INFO] - Training Epoch: 1/2, step 4594/7134 completed (loss: 0.05171738192439079, acc: 0.9800000190734863)
[2025-02-13 03:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42][root][INFO] - Training Epoch: 1/2, step 4595/7134 completed (loss: 0.047103818506002426, acc: 0.9836552739143372)
[2025-02-13 03:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42][root][INFO] - Training Epoch: 1/2, step 4596/7134 completed (loss: 0.052236393094062805, acc: 0.9825396537780762)
[2025-02-13 03:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:43][root][INFO] - Training Epoch: 1/2, step 4597/7134 completed (loss: 0.024310950189828873, acc: 0.9890643954277039)
[2025-02-13 03:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:43][root][INFO] - Training Epoch: 1/2, step 4598/7134 completed (loss: 0.07435090839862823, acc: 0.9840490818023682)
[2025-02-13 03:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:44][root][INFO] - Training Epoch: 1/2, step 4599/7134 completed (loss: 0.01606796309351921, acc: 0.9936407208442688)
[2025-02-13 03:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:44][root][INFO] - Training Epoch: 1/2, step 4600/7134 completed (loss: 0.02791845239698887, acc: 0.9918808937072754)
[2025-02-13 03:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45][root][INFO] - Training Epoch: 1/2, step 4601/7134 completed (loss: 0.046277426183223724, acc: 0.9843546152114868)
[2025-02-13 03:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45][root][INFO] - Training Epoch: 1/2, step 4602/7134 completed (loss: 0.04355844855308533, acc: 0.9863760471343994)
[2025-02-13 03:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45][root][INFO] - Training Epoch: 1/2, step 4603/7134 completed (loss: 0.06348279118537903, acc: 0.9820689558982849)
[2025-02-13 03:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:46][root][INFO] - Training Epoch: 1/2, step 4604/7134 completed (loss: 0.03397279977798462, acc: 0.9855538010597229)
[2025-02-13 03:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:46][root][INFO] - Training Epoch: 1/2, step 4605/7134 completed (loss: 0.06899403035640717, acc: 0.9737991094589233)
[2025-02-13 03:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47][root][INFO] - Training Epoch: 1/2, step 4606/7134 completed (loss: 0.02073054574429989, acc: 0.9938837885856628)
[2025-02-13 03:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47][root][INFO] - Training Epoch: 1/2, step 4607/7134 completed (loss: 0.02578791044652462, acc: 0.9831649661064148)
[2025-02-13 03:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47][root][INFO] - Training Epoch: 1/2, step 4608/7134 completed (loss: 0.02277049794793129, acc: 0.9940740466117859)
[2025-02-13 03:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:48][root][INFO] - Training Epoch: 1/2, step 4609/7134 completed (loss: 0.04320888966321945, acc: 0.9944953918457031)
[2025-02-13 03:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:48][root][INFO] - Training Epoch: 1/2, step 4610/7134 completed (loss: 0.029690710827708244, acc: 0.9867197871208191)
[2025-02-13 03:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49][root][INFO] - Training Epoch: 1/2, step 4611/7134 completed (loss: 0.013089890591800213, acc: 0.9968701004981995)
[2025-02-13 03:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49][root][INFO] - Training Epoch: 1/2, step 4612/7134 completed (loss: 0.0270058736205101, acc: 0.9944238066673279)
[2025-02-13 03:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49][root][INFO] - Training Epoch: 1/2, step 4613/7134 completed (loss: 0.02091568149626255, acc: 0.9930555820465088)
[2025-02-13 03:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:50][root][INFO] - Training Epoch: 1/2, step 4614/7134 completed (loss: 0.05578906461596489, acc: 0.9848484992980957)
[2025-02-13 03:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:50][root][INFO] - Training Epoch: 1/2, step 4615/7134 completed (loss: 0.05388568341732025, acc: 0.9810040593147278)
[2025-02-13 03:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:51][root][INFO] - Training Epoch: 1/2, step 4616/7134 completed (loss: 0.05660165846347809, acc: 0.9837278127670288)
[2025-02-13 03:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:51][root][INFO] - Training Epoch: 1/2, step 4617/7134 completed (loss: 0.03785233944654465, acc: 0.9864864945411682)
[2025-02-13 03:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52][root][INFO] - Training Epoch: 1/2, step 4618/7134 completed (loss: 0.05068963021039963, acc: 0.9728155136108398)
[2025-02-13 03:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52][root][INFO] - Training Epoch: 1/2, step 4619/7134 completed (loss: 0.01501941867172718, acc: 0.9954751133918762)
[2025-02-13 03:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52][root][INFO] - Training Epoch: 1/2, step 4620/7134 completed (loss: 0.016589177772402763, acc: 0.9942029118537903)
[2025-02-13 03:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:53][root][INFO] - Training Epoch: 1/2, step 4621/7134 completed (loss: 0.018031880259513855, acc: 0.9907407164573669)
[2025-02-13 03:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:53][root][INFO] - Training Epoch: 1/2, step 4622/7134 completed (loss: 0.04991687461733818, acc: 0.9875156283378601)
[2025-02-13 03:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54][root][INFO] - Training Epoch: 1/2, step 4623/7134 completed (loss: 0.061008963733911514, acc: 0.9816993474960327)
[2025-02-13 03:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54][root][INFO] - Training Epoch: 1/2, step 4624/7134 completed (loss: 0.07808521389961243, acc: 0.9807956218719482)
[2025-02-13 03:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54][root][INFO] - Training Epoch: 1/2, step 4625/7134 completed (loss: 0.1283523440361023, acc: 0.9655172228813171)
[2025-02-13 03:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:55][root][INFO] - Training Epoch: 1/2, step 4626/7134 completed (loss: 0.07362980395555496, acc: 0.9798657894134521)
[2025-02-13 03:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:55][root][INFO] - Training Epoch: 1/2, step 4627/7134 completed (loss: 0.10794766992330551, acc: 0.97420334815979)
[2025-02-13 03:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:56][root][INFO] - Training Epoch: 1/2, step 4628/7134 completed (loss: 0.0767667219042778, acc: 0.9739130139350891)
[2025-02-13 03:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:56][root][INFO] - Training Epoch: 1/2, step 4629/7134 completed (loss: 0.09884609282016754, acc: 0.9726495742797852)
[2025-02-13 03:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:57][root][INFO] - Training Epoch: 1/2, step 4630/7134 completed (loss: 0.07147597521543503, acc: 0.9801849126815796)
[2025-02-13 03:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:57][root][INFO] - Training Epoch: 1/2, step 4631/7134 completed (loss: 0.05972960591316223, acc: 0.9811946749687195)
[2025-02-13 03:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58][root][INFO] - Training Epoch: 1/2, step 4632/7134 completed (loss: 0.05942185968160629, acc: 0.9852941036224365)
[2025-02-13 03:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58][root][INFO] - Training Epoch: 1/2, step 4633/7134 completed (loss: 0.08286351710557938, acc: 0.9780346751213074)
[2025-02-13 03:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58][root][INFO] - Training Epoch: 1/2, step 4634/7134 completed (loss: 0.035955801606178284, acc: 0.9902912378311157)
[2025-02-13 03:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59][root][INFO] - Training Epoch: 1/2, step 4635/7134 completed (loss: 0.08509615063667297, acc: 0.9817073345184326)
[2025-02-13 03:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59][root][INFO] - Training Epoch: 1/2, step 4636/7134 completed (loss: 0.02366240695118904, acc: 0.9922680258750916)
[2025-02-13 03:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59][root][INFO] - Training Epoch: 1/2, step 4637/7134 completed (loss: 0.04708751663565636, acc: 0.9879032373428345)
[2025-02-13 03:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:00][root][INFO] - Training Epoch: 1/2, step 4638/7134 completed (loss: 0.07958097755908966, acc: 0.9775429368019104)
[2025-02-13 03:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:00][root][INFO] - Training Epoch: 1/2, step 4639/7134 completed (loss: 0.07381074130535126, acc: 0.9817517995834351)
[2025-02-13 03:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01][root][INFO] - Training Epoch: 1/2, step 4640/7134 completed (loss: 0.07715389877557755, acc: 0.9807692170143127)
[2025-02-13 03:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01][root][INFO] - Training Epoch: 1/2, step 4641/7134 completed (loss: 0.04590639844536781, acc: 0.984375)
[2025-02-13 03:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:02][root][INFO] - Training Epoch: 1/2, step 4642/7134 completed (loss: 0.020850062370300293, acc: 0.9960317611694336)
[2025-02-13 03:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:02][root][INFO] - Training Epoch: 1/2, step 4643/7134 completed (loss: 0.03311498835682869, acc: 0.9940828680992126)
[2025-02-13 03:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03][root][INFO] - Training Epoch: 1/2, step 4644/7134 completed (loss: 0.03454212844371796, acc: 0.9917159676551819)
[2025-02-13 03:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03][root][INFO] - Training Epoch: 1/2, step 4645/7134 completed (loss: 0.028264297172427177, acc: 0.9944367408752441)
[2025-02-13 03:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03][root][INFO] - Training Epoch: 1/2, step 4646/7134 completed (loss: 0.02650129422545433, acc: 0.9890109896659851)
[2025-02-13 03:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:04][root][INFO] - Training Epoch: 1/2, step 4647/7134 completed (loss: 0.09967508167028427, acc: 0.9830729365348816)
[2025-02-13 03:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:04][root][INFO] - Training Epoch: 1/2, step 4648/7134 completed (loss: 0.05760832875967026, acc: 0.9807956218719482)
[2025-02-13 03:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:05][root][INFO] - Training Epoch: 1/2, step 4649/7134 completed (loss: 0.06211197003722191, acc: 0.9842382073402405)
[2025-02-13 03:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:05][root][INFO] - Training Epoch: 1/2, step 4650/7134 completed (loss: 0.07537221908569336, acc: 0.9747023582458496)
[2025-02-13 03:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:06][root][INFO] - Training Epoch: 1/2, step 4651/7134 completed (loss: 0.1396346539258957, acc: 0.9619771838188171)
[2025-02-13 03:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:06][root][INFO] - Training Epoch: 1/2, step 4652/7134 completed (loss: 0.0436544194817543, acc: 0.9878048896789551)
[2025-02-13 03:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07][root][INFO] - Training Epoch: 1/2, step 4653/7134 completed (loss: 0.06305874139070511, acc: 0.9838709831237793)
[2025-02-13 03:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07][root][INFO] - Training Epoch: 1/2, step 4654/7134 completed (loss: 0.10016963630914688, acc: 0.9707673788070679)
[2025-02-13 03:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07][root][INFO] - Training Epoch: 1/2, step 4655/7134 completed (loss: 0.04719610884785652, acc: 0.9890561103820801)
[2025-02-13 03:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08][root][INFO] - Training Epoch: 1/2, step 4656/7134 completed (loss: 0.051618605852127075, acc: 0.9848484992980957)
[2025-02-13 03:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08][root][INFO] - Training Epoch: 1/2, step 4657/7134 completed (loss: 0.06394980102777481, acc: 0.9787535667419434)
[2025-02-13 03:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:09][root][INFO] - Training Epoch: 1/2, step 4658/7134 completed (loss: 0.0732404962182045, acc: 0.9702300429344177)
[2025-02-13 03:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:09][root][INFO] - Training Epoch: 1/2, step 4659/7134 completed (loss: 0.10076193511486053, acc: 0.9755784273147583)
[2025-02-13 03:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10][root][INFO] - Training Epoch: 1/2, step 4660/7134 completed (loss: 0.10047099739313126, acc: 0.9717868566513062)
[2025-02-13 03:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10][root][INFO] - Training Epoch: 1/2, step 4661/7134 completed (loss: 0.05834417790174484, acc: 0.9832317233085632)
[2025-02-13 03:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10][root][INFO] - Training Epoch: 1/2, step 4662/7134 completed (loss: 0.09426315128803253, acc: 0.9744898080825806)
[2025-02-13 03:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11][root][INFO] - Training Epoch: 1/2, step 4663/7134 completed (loss: 0.08128092437982559, acc: 0.9799465537071228)
[2025-02-13 03:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11][root][INFO] - Training Epoch: 1/2, step 4664/7134 completed (loss: 0.06362979859113693, acc: 0.9828269481658936)
[2025-02-13 03:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12][root][INFO] - Training Epoch: 1/2, step 4665/7134 completed (loss: 0.06143694743514061, acc: 0.9822646379470825)
[2025-02-13 03:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12][root][INFO] - Training Epoch: 1/2, step 4666/7134 completed (loss: 0.07866561412811279, acc: 0.9817470908164978)
[2025-02-13 03:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12][root][INFO] - Training Epoch: 1/2, step 4667/7134 completed (loss: 0.08107472956180573, acc: 0.9741824269294739)
[2025-02-13 03:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:13][root][INFO] - Training Epoch: 1/2, step 4668/7134 completed (loss: 0.05131412670016289, acc: 0.9820788502693176)
[2025-02-13 03:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:13][root][INFO] - Training Epoch: 1/2, step 4669/7134 completed (loss: 0.03458966687321663, acc: 0.9896449446678162)
[2025-02-13 03:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14][root][INFO] - Training Epoch: 1/2, step 4670/7134 completed (loss: 0.08003740757703781, acc: 0.9805068373680115)
[2025-02-13 03:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14][root][INFO] - Training Epoch: 1/2, step 4671/7134 completed (loss: 0.056263238191604614, acc: 0.9858956336975098)
[2025-02-13 03:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14][root][INFO] - Training Epoch: 1/2, step 4672/7134 completed (loss: 0.07304167002439499, acc: 0.9773519039154053)
[2025-02-13 03:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:15][root][INFO] - Training Epoch: 1/2, step 4673/7134 completed (loss: 0.07230844348669052, acc: 0.9828392863273621)
[2025-02-13 03:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:15][root][INFO] - Training Epoch: 1/2, step 4674/7134 completed (loss: 0.0601990707218647, acc: 0.9825673699378967)
[2025-02-13 03:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:16][root][INFO] - Training Epoch: 1/2, step 4675/7134 completed (loss: 0.05185910314321518, acc: 0.9859872460365295)
[2025-02-13 03:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:16][root][INFO] - Training Epoch: 1/2, step 4676/7134 completed (loss: 0.031296659260988235, acc: 0.9939758777618408)
[2025-02-13 03:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17][root][INFO] - Training Epoch: 1/2, step 4677/7134 completed (loss: 0.051617905497550964, acc: 0.984649121761322)
[2025-02-13 03:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17][root][INFO] - Training Epoch: 1/2, step 4678/7134 completed (loss: 0.0796770229935646, acc: 0.97919762134552)
[2025-02-13 03:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17][root][INFO] - Training Epoch: 1/2, step 4679/7134 completed (loss: 0.06872754544019699, acc: 0.9848275780677795)
[2025-02-13 03:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:18][root][INFO] - Training Epoch: 1/2, step 4680/7134 completed (loss: 0.06854552775621414, acc: 0.9766355156898499)
[2025-02-13 03:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:18][root][INFO] - Training Epoch: 1/2, step 4681/7134 completed (loss: 0.05984823405742645, acc: 0.9876922965049744)
[2025-02-13 03:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19][root][INFO] - Training Epoch: 1/2, step 4682/7134 completed (loss: 0.0393068790435791, acc: 0.9866179823875427)
[2025-02-13 03:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19][root][INFO] - Training Epoch: 1/2, step 4683/7134 completed (loss: 0.05122193321585655, acc: 0.9857346415519714)
[2025-02-13 03:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19][root][INFO] - Training Epoch: 1/2, step 4684/7134 completed (loss: 0.08236267417669296, acc: 0.9800838828086853)
[2025-02-13 03:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:20][root][INFO] - Training Epoch: 1/2, step 4685/7134 completed (loss: 0.04842643812298775, acc: 0.9907786846160889)
[2025-02-13 03:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:20][root][INFO] - Training Epoch: 1/2, step 4686/7134 completed (loss: 0.042981017380952835, acc: 0.9883585572242737)
[2025-02-13 03:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21][root][INFO] - Training Epoch: 1/2, step 4687/7134 completed (loss: 0.18678069114685059, acc: 0.953987717628479)
[2025-02-13 03:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21][root][INFO] - Training Epoch: 1/2, step 4688/7134 completed (loss: 0.2289094626903534, acc: 0.9445585012435913)
[2025-02-13 03:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22][root][INFO] - Training Epoch: 1/2, step 4689/7134 completed (loss: 0.1641816347837448, acc: 0.9513990879058838)
[2025-02-13 03:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22][root][INFO] - Training Epoch: 1/2, step 4690/7134 completed (loss: 0.1352670043706894, acc: 0.9727463126182556)
[2025-02-13 03:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22][root][INFO] - Training Epoch: 1/2, step 4691/7134 completed (loss: 0.05415485054254532, acc: 0.9807256460189819)
[2025-02-13 03:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:23][root][INFO] - Training Epoch: 1/2, step 4692/7134 completed (loss: 0.1910410076379776, acc: 0.9575371742248535)
[2025-02-13 03:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:23][root][INFO] - Training Epoch: 1/2, step 4693/7134 completed (loss: 0.18020427227020264, acc: 0.9624060392379761)
[2025-02-13 03:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24][root][INFO] - Training Epoch: 1/2, step 4694/7134 completed (loss: 0.16685788333415985, acc: 0.9553752541542053)
[2025-02-13 03:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24][root][INFO] - Training Epoch: 1/2, step 4695/7134 completed (loss: 0.09285756200551987, acc: 0.9737274050712585)
[2025-02-13 03:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25][root][INFO] - Training Epoch: 1/2, step 4696/7134 completed (loss: 0.04935067519545555, acc: 0.9866270422935486)
[2025-02-13 03:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25][root][INFO] - Training Epoch: 1/2, step 4697/7134 completed (loss: 0.061039675027132034, acc: 0.9830827116966248)
[2025-02-13 03:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25][root][INFO] - Training Epoch: 1/2, step 4698/7134 completed (loss: 0.12834812700748444, acc: 0.9675572514533997)
[2025-02-13 03:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26][root][INFO] - Training Epoch: 1/2, step 4699/7134 completed (loss: 0.07437658309936523, acc: 0.9762340188026428)
[2025-02-13 03:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26][root][INFO] - Training Epoch: 1/2, step 4700/7134 completed (loss: 0.08505330979824066, acc: 0.9746646881103516)
[2025-02-13 03:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26][root][INFO] - Training Epoch: 1/2, step 4701/7134 completed (loss: 0.12344987690448761, acc: 0.9648562073707581)
[2025-02-13 03:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:27][root][INFO] - Training Epoch: 1/2, step 4702/7134 completed (loss: 0.046169716864824295, acc: 0.9856687784194946)
[2025-02-13 03:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:27][root][INFO] - Training Epoch: 1/2, step 4703/7134 completed (loss: 0.05769224464893341, acc: 0.9878378510475159)
[2025-02-13 03:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:28][root][INFO] - Training Epoch: 1/2, step 4704/7134 completed (loss: 0.037785232067108154, acc: 0.991793692111969)
[2025-02-13 03:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:28][root][INFO] - Training Epoch: 1/2, step 4705/7134 completed (loss: 0.036169227212667465, acc: 0.9871345162391663)
[2025-02-13 03:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29][root][INFO] - Training Epoch: 1/2, step 4706/7134 completed (loss: 0.09827370941638947, acc: 0.9766355156898499)
[2025-02-13 03:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29][root][INFO] - Training Epoch: 1/2, step 4707/7134 completed (loss: 0.2318684309720993, acc: 0.942176878452301)
[2025-02-13 03:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29][root][INFO] - Training Epoch: 1/2, step 4708/7134 completed (loss: 0.04320431873202324, acc: 0.9800498485565186)
[2025-02-13 03:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:30][root][INFO] - Training Epoch: 1/2, step 4709/7134 completed (loss: 0.03549576550722122, acc: 0.9894598126411438)
[2025-02-13 03:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:30][root][INFO] - Training Epoch: 1/2, step 4710/7134 completed (loss: 0.05704732611775398, acc: 0.9810771346092224)
[2025-02-13 03:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:31][root][INFO] - Training Epoch: 1/2, step 4711/7134 completed (loss: 0.08082439005374908, acc: 0.9745330810546875)
[2025-02-13 03:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:31][root][INFO] - Training Epoch: 1/2, step 4712/7134 completed (loss: 0.10633529722690582, acc: 0.9651514887809753)
[2025-02-13 03:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32][root][INFO] - Training Epoch: 1/2, step 4713/7134 completed (loss: 0.033170152455568314, acc: 0.987500011920929)
[2025-02-13 03:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32][root][INFO] - Training Epoch: 1/2, step 4714/7134 completed (loss: 0.08374268561601639, acc: 0.9757009148597717)
[2025-02-13 03:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32][root][INFO] - Training Epoch: 1/2, step 4715/7134 completed (loss: 0.1279473900794983, acc: 0.9658848643302917)
[2025-02-13 03:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:33][root][INFO] - Training Epoch: 1/2, step 4716/7134 completed (loss: 0.016759952530264854, acc: 0.9923780560493469)
[2025-02-13 03:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:33][root][INFO] - Training Epoch: 1/2, step 4717/7134 completed (loss: 0.047645844519138336, acc: 0.9911110997200012)
[2025-02-13 03:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34][root][INFO] - Training Epoch: 1/2, step 4718/7134 completed (loss: 0.0476064458489418, acc: 0.9850993156433105)
[2025-02-13 03:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34][root][INFO] - Training Epoch: 1/2, step 4719/7134 completed (loss: 0.03490709885954857, acc: 0.991150438785553)
[2025-02-13 03:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34][root][INFO] - Training Epoch: 1/2, step 4720/7134 completed (loss: 0.011201859451830387, acc: 0.9971949458122253)
[2025-02-13 03:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:35][root][INFO] - Training Epoch: 1/2, step 4721/7134 completed (loss: 0.012798802927136421, acc: 0.9942362904548645)
[2025-02-13 03:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:35][root][INFO] - Training Epoch: 1/2, step 4722/7134 completed (loss: 0.027563033625483513, acc: 0.9896449446678162)
[2025-02-13 03:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36][root][INFO] - Training Epoch: 1/2, step 4723/7134 completed (loss: 0.015693118795752525, acc: 0.9939024448394775)
[2025-02-13 03:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36][root][INFO] - Training Epoch: 1/2, step 4724/7134 completed (loss: 0.0252826064825058, acc: 0.9911242723464966)
[2025-02-13 03:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36][root][INFO] - Training Epoch: 1/2, step 4725/7134 completed (loss: 0.028419051319360733, acc: 0.9960421919822693)
[2025-02-13 03:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:37][root][INFO] - Training Epoch: 1/2, step 4726/7134 completed (loss: 0.053562115877866745, acc: 0.980141818523407)
[2025-02-13 03:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:37][root][INFO] - Training Epoch: 1/2, step 4727/7134 completed (loss: 0.046052731573581696, acc: 0.9874607920646667)
[2025-02-13 03:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38][root][INFO] - Training Epoch: 1/2, step 4728/7134 completed (loss: 0.057042550295591354, acc: 0.9901153445243835)
[2025-02-13 03:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38][root][INFO] - Training Epoch: 1/2, step 4729/7134 completed (loss: 0.02262914925813675, acc: 0.9957746267318726)
[2025-02-13 03:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38][root][INFO] - Training Epoch: 1/2, step 4730/7134 completed (loss: 0.03378643840551376, acc: 0.9877488613128662)
[2025-02-13 03:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:39][root][INFO] - Training Epoch: 1/2, step 4731/7134 completed (loss: 0.052598413079977036, acc: 0.989313006401062)
[2025-02-13 03:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:39][root][INFO] - Training Epoch: 1/2, step 4732/7134 completed (loss: 0.06570355594158173, acc: 0.9818511605262756)
[2025-02-13 03:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:40][root][INFO] - Training Epoch: 1/2, step 4733/7134 completed (loss: 0.028147079050540924, acc: 0.9919224381446838)
[2025-02-13 03:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:40][root][INFO] - Training Epoch: 1/2, step 4734/7134 completed (loss: 0.06778741627931595, acc: 0.9825119376182556)
[2025-02-13 03:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41][root][INFO] - Training Epoch: 1/2, step 4735/7134 completed (loss: 0.04030763730406761, acc: 0.9864864945411682)
[2025-02-13 03:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41][root][INFO] - Training Epoch: 1/2, step 4736/7134 completed (loss: 0.06411555409431458, acc: 0.9842209219932556)
[2025-02-13 03:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41][root][INFO] - Training Epoch: 1/2, step 4737/7134 completed (loss: 0.03875328227877617, acc: 0.9890282154083252)
[2025-02-13 03:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:42][root][INFO] - Training Epoch: 1/2, step 4738/7134 completed (loss: 0.03874102234840393, acc: 0.987860381603241)
[2025-02-13 03:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:42][root][INFO] - Training Epoch: 1/2, step 4739/7134 completed (loss: 0.09398359060287476, acc: 0.9728434681892395)
[2025-02-13 03:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43][root][INFO] - Training Epoch: 1/2, step 4740/7134 completed (loss: 0.08954616636037827, acc: 0.9805389046669006)
[2025-02-13 03:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43][root][INFO] - Training Epoch: 1/2, step 4741/7134 completed (loss: 0.06460072845220566, acc: 0.9741219878196716)
[2025-02-13 03:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43][root][INFO] - Training Epoch: 1/2, step 4742/7134 completed (loss: 0.11611182242631912, acc: 0.9663716554641724)
[2025-02-13 03:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44][root][INFO] - Training Epoch: 1/2, step 4743/7134 completed (loss: 0.07806964963674545, acc: 0.9804270267486572)
[2025-02-13 03:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44][root][INFO] - Training Epoch: 1/2, step 4744/7134 completed (loss: 0.06138934940099716, acc: 0.9814049601554871)
[2025-02-13 03:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44][root][INFO] - Training Epoch: 1/2, step 4745/7134 completed (loss: 0.08423440158367157, acc: 0.9799330830574036)
[2025-02-13 03:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:45][root][INFO] - Training Epoch: 1/2, step 4746/7134 completed (loss: 0.07150478661060333, acc: 0.9852941036224365)
[2025-02-13 03:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:45][root][INFO] - Training Epoch: 1/2, step 4747/7134 completed (loss: 0.05419868975877762, acc: 0.9851552248001099)
[2025-02-13 03:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46][root][INFO] - Training Epoch: 1/2, step 4748/7134 completed (loss: 0.0904470831155777, acc: 0.9734513163566589)
[2025-02-13 03:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46][root][INFO] - Training Epoch: 1/2, step 4749/7134 completed (loss: 0.07284730672836304, acc: 0.9773333072662354)
[2025-02-13 03:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46][root][INFO] - Training Epoch: 1/2, step 4750/7134 completed (loss: 0.09847886860370636, acc: 0.9710982441902161)
[2025-02-13 03:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:47][root][INFO] - Training Epoch: 1/2, step 4751/7134 completed (loss: 0.04078076407313347, acc: 0.9886845946311951)
[2025-02-13 03:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:47][root][INFO] - Training Epoch: 1/2, step 4752/7134 completed (loss: 0.09407695382833481, acc: 0.9846389889717102)
[2025-02-13 03:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:48][root][INFO] - Training Epoch: 1/2, step 4753/7134 completed (loss: 0.09023396670818329, acc: 0.9789156913757324)
[2025-02-13 03:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:48][root][INFO] - Training Epoch: 1/2, step 4754/7134 completed (loss: 0.03243110701441765, acc: 0.9941262602806091)
[2025-02-13 03:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49][root][INFO] - Training Epoch: 1/2, step 4755/7134 completed (loss: 0.06630443036556244, acc: 0.9833564758300781)
[2025-02-13 03:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49][root][INFO] - Training Epoch: 1/2, step 4756/7134 completed (loss: 0.09733136743307114, acc: 0.9769874215126038)
[2025-02-13 03:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49][root][INFO] - Training Epoch: 1/2, step 4757/7134 completed (loss: 0.056002192199230194, acc: 0.9862227439880371)
[2025-02-13 03:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50][root][INFO] - Training Epoch: 1/2, step 4758/7134 completed (loss: 0.04618339613080025, acc: 0.9887955188751221)
[2025-02-13 03:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50][root][INFO] - Training Epoch: 1/2, step 4759/7134 completed (loss: 0.03181974217295647, acc: 0.9913793206214905)
[2025-02-13 03:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:51][root][INFO] - Training Epoch: 1/2, step 4760/7134 completed (loss: 0.0763629600405693, acc: 0.977225661277771)
[2025-02-13 03:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:51][root][INFO] - Training Epoch: 1/2, step 4761/7134 completed (loss: 0.05639109015464783, acc: 0.9908397197723389)
[2025-02-13 03:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52][root][INFO] - Training Epoch: 1/2, step 4762/7134 completed (loss: 0.05960577726364136, acc: 0.9891892075538635)
[2025-02-13 03:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52][root][INFO] - Training Epoch: 1/2, step 4763/7134 completed (loss: 0.06449899822473526, acc: 0.9817629456520081)
[2025-02-13 03:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52][root][INFO] - Training Epoch: 1/2, step 4764/7134 completed (loss: 0.06121167540550232, acc: 0.9814586043357849)
[2025-02-13 03:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:53][root][INFO] - Training Epoch: 1/2, step 4765/7134 completed (loss: 0.09096556901931763, acc: 0.9827337861061096)
[2025-02-13 03:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:53][root][INFO] - Training Epoch: 1/2, step 4766/7134 completed (loss: 0.06083700805902481, acc: 0.9884169697761536)
[2025-02-13 03:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54][root][INFO] - Training Epoch: 1/2, step 4767/7134 completed (loss: 0.06540115922689438, acc: 0.9821882843971252)
[2025-02-13 03:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54][root][INFO] - Training Epoch: 1/2, step 4768/7134 completed (loss: 0.09089236706495285, acc: 0.9756097793579102)
[2025-02-13 03:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54][root][INFO] - Training Epoch: 1/2, step 4769/7134 completed (loss: 0.0807182714343071, acc: 0.9807383418083191)
[2025-02-13 03:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:55][root][INFO] - Training Epoch: 1/2, step 4770/7134 completed (loss: 0.03659799322485924, acc: 0.9878787994384766)
[2025-02-13 03:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:55][root][INFO] - Training Epoch: 1/2, step 4771/7134 completed (loss: 0.12240784615278244, acc: 0.9680232405662537)
[2025-02-13 03:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56][root][INFO] - Training Epoch: 1/2, step 4772/7134 completed (loss: 0.06623566150665283, acc: 0.9904305934906006)
[2025-02-13 03:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56][root][INFO] - Training Epoch: 1/2, step 4773/7134 completed (loss: 0.06599946320056915, acc: 0.9849315285682678)
[2025-02-13 03:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56][root][INFO] - Training Epoch: 1/2, step 4774/7134 completed (loss: 0.07079442590475082, acc: 0.983146071434021)
[2025-02-13 03:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:57][root][INFO] - Training Epoch: 1/2, step 4775/7134 completed (loss: 0.0744667649269104, acc: 0.9822866320610046)
[2025-02-13 03:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:57][root][INFO] - Training Epoch: 1/2, step 4776/7134 completed (loss: 0.059872787445783615, acc: 0.9855538010597229)
[2025-02-13 03:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58][root][INFO] - Training Epoch: 1/2, step 4777/7134 completed (loss: 0.03847119212150574, acc: 0.989983320236206)
[2025-02-13 03:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58][root][INFO] - Training Epoch: 1/2, step 4778/7134 completed (loss: 0.04100906103849411, acc: 0.9912472367286682)
[2025-02-13 03:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58][root][INFO] - Training Epoch: 1/2, step 4779/7134 completed (loss: 0.05602320656180382, acc: 0.9871086478233337)
[2025-02-13 03:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:59][root][INFO] - Training Epoch: 1/2, step 4780/7134 completed (loss: 0.012705488130450249, acc: 0.9953271150588989)
[2025-02-13 03:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:59][root][INFO] - Training Epoch: 1/2, step 4781/7134 completed (loss: 0.02068391442298889, acc: 0.9941291809082031)
[2025-02-13 03:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:00][root][INFO] - Training Epoch: 1/2, step 4782/7134 completed (loss: 0.045401398092508316, acc: 0.9837925434112549)
[2025-02-13 03:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:00][root][INFO] - Training Epoch: 1/2, step 4783/7134 completed (loss: 0.06790091097354889, acc: 0.9839857816696167)
[2025-02-13 03:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01][root][INFO] - Training Epoch: 1/2, step 4784/7134 completed (loss: 0.02978350594639778, acc: 0.9918144345283508)
[2025-02-13 03:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01][root][INFO] - Training Epoch: 1/2, step 4785/7134 completed (loss: 0.07737353444099426, acc: 0.983146071434021)
[2025-02-13 03:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01][root][INFO] - Training Epoch: 1/2, step 4786/7134 completed (loss: 0.063389353454113, acc: 0.9860248565673828)
[2025-02-13 03:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:02][root][INFO] - Training Epoch: 1/2, step 4787/7134 completed (loss: 0.01248709112405777, acc: 0.996835470199585)
[2025-02-13 03:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:02][root][INFO] - Training Epoch: 1/2, step 4788/7134 completed (loss: 0.020111599937081337, acc: 0.9946808218955994)
[2025-02-13 03:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03][root][INFO] - Training Epoch: 1/2, step 4789/7134 completed (loss: 0.04192616418004036, acc: 0.993630588054657)
[2025-02-13 03:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03][root][INFO] - Training Epoch: 1/2, step 4790/7134 completed (loss: 0.03502460569143295, acc: 0.991847813129425)
[2025-02-13 03:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03][root][INFO] - Training Epoch: 1/2, step 4791/7134 completed (loss: 0.026516200974583626, acc: 0.9925512075424194)
[2025-02-13 03:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04][root][INFO] - Training Epoch: 1/2, step 4792/7134 completed (loss: 0.02776404283940792, acc: 0.9933110475540161)
[2025-02-13 03:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04][root][INFO] - Training Epoch: 1/2, step 4793/7134 completed (loss: 0.06255519390106201, acc: 0.9780821800231934)
[2025-02-13 03:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05][root][INFO] - Training Epoch: 1/2, step 4794/7134 completed (loss: 0.07835842669010162, acc: 0.9752380847930908)
[2025-02-13 03:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05][root][INFO] - Training Epoch: 1/2, step 4795/7134 completed (loss: 0.030061662197113037, acc: 0.9872881174087524)
[2025-02-13 03:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05][root][INFO] - Training Epoch: 1/2, step 4796/7134 completed (loss: 0.03257684037089348, acc: 0.9874213933944702)
[2025-02-13 03:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:06][root][INFO] - Training Epoch: 1/2, step 4797/7134 completed (loss: 0.030470633879303932, acc: 0.9900621175765991)
[2025-02-13 03:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:06][root][INFO] - Training Epoch: 1/2, step 4798/7134 completed (loss: 0.044527873396873474, acc: 0.989347517490387)
[2025-02-13 03:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:07][root][INFO] - Training Epoch: 1/2, step 4799/7134 completed (loss: 0.03754888102412224, acc: 0.985855758190155)
[2025-02-13 03:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:07][root][INFO] - Training Epoch: 1/2, step 4800/7134 completed (loss: 0.04536178708076477, acc: 0.9880775213241577)
[2025-02-13 03:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08][root][INFO] - Training Epoch: 1/2, step 4801/7134 completed (loss: 0.024392375722527504, acc: 0.9920814633369446)
[2025-02-13 03:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08][root][INFO] - Training Epoch: 1/2, step 4802/7134 completed (loss: 0.06113926321268082, acc: 0.9869109988212585)
[2025-02-13 03:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08][root][INFO] - Training Epoch: 1/2, step 4803/7134 completed (loss: 0.06439409404993057, acc: 0.9867647290229797)
[2025-02-13 03:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:09][root][INFO] - Training Epoch: 1/2, step 4804/7134 completed (loss: 0.05924694985151291, acc: 0.9815384745597839)
[2025-02-13 03:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:09][root][INFO] - Training Epoch: 1/2, step 4805/7134 completed (loss: 0.05263892188668251, acc: 0.991909384727478)
[2025-02-13 03:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10][root][INFO] - Training Epoch: 1/2, step 4806/7134 completed (loss: 0.04728153347969055, acc: 0.9933444261550903)
[2025-02-13 03:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10][root][INFO] - Training Epoch: 1/2, step 4807/7134 completed (loss: 0.10186424106359482, acc: 0.9848024249076843)
[2025-02-13 03:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10][root][INFO] - Training Epoch: 1/2, step 4808/7134 completed (loss: 0.06690650433301926, acc: 0.9858155846595764)
[2025-02-13 03:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:11][root][INFO] - Training Epoch: 1/2, step 4809/7134 completed (loss: 0.03958404064178467, acc: 0.9841269850730896)
[2025-02-13 03:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:11][root][INFO] - Training Epoch: 1/2, step 4810/7134 completed (loss: 0.06855463981628418, acc: 0.9871612191200256)
[2025-02-13 03:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12][root][INFO] - Training Epoch: 1/2, step 4811/7134 completed (loss: 0.06574945151805878, acc: 0.9864341020584106)
[2025-02-13 03:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12][root][INFO] - Training Epoch: 1/2, step 4812/7134 completed (loss: 0.09915582090616226, acc: 0.9819193482398987)
[2025-02-13 03:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12][root][INFO] - Training Epoch: 1/2, step 4813/7134 completed (loss: 0.013933036476373672, acc: 0.9974026083946228)
[2025-02-13 03:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:13][root][INFO] - Training Epoch: 1/2, step 4814/7134 completed (loss: 0.041970282793045044, acc: 0.9897959232330322)
[2025-02-13 03:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:13][root][INFO] - Training Epoch: 1/2, step 4815/7134 completed (loss: 0.0051893265917897224, acc: 1.0)
[2025-02-13 03:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14][root][INFO] - Training Epoch: 1/2, step 4816/7134 completed (loss: 0.022266598418354988, acc: 0.9938119053840637)
[2025-02-13 03:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14][root][INFO] - Training Epoch: 1/2, step 4817/7134 completed (loss: 0.020887622609734535, acc: 0.9927710890769958)
[2025-02-13 03:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14][root][INFO] - Training Epoch: 1/2, step 4818/7134 completed (loss: 0.028018997982144356, acc: 0.9923076629638672)
[2025-02-13 03:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:15][root][INFO] - Training Epoch: 1/2, step 4819/7134 completed (loss: 0.03666108846664429, acc: 0.9918864369392395)
[2025-02-13 03:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:15][root][INFO] - Training Epoch: 1/2, step 4820/7134 completed (loss: 0.018783433362841606, acc: 0.993565022945404)
[2025-02-13 03:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:16][root][INFO] - Training Epoch: 1/2, step 4821/7134 completed (loss: 0.022839024662971497, acc: 0.994413435459137)
[2025-02-13 03:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:16][root][INFO] - Training Epoch: 1/2, step 4822/7134 completed (loss: 0.028719501569867134, acc: 0.9943289160728455)
[2025-02-13 03:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17][root][INFO] - Training Epoch: 1/2, step 4823/7134 completed (loss: 0.07062459737062454, acc: 0.9842022061347961)
[2025-02-13 03:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17][root][INFO] - Training Epoch: 1/2, step 4824/7134 completed (loss: 0.06368143856525421, acc: 0.9771689772605896)
[2025-02-13 03:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17][root][INFO] - Training Epoch: 1/2, step 4825/7134 completed (loss: 0.01928945630788803, acc: 0.9927007555961609)
[2025-02-13 03:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:18][root][INFO] - Training Epoch: 1/2, step 4826/7134 completed (loss: 0.03318942338228226, acc: 0.9892703890800476)
[2025-02-13 03:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:18][root][INFO] - Training Epoch: 1/2, step 4827/7134 completed (loss: 0.04152815043926239, acc: 0.9896193742752075)
[2025-02-13 03:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19][root][INFO] - Training Epoch: 1/2, step 4828/7134 completed (loss: 0.024217650294303894, acc: 0.9939246773719788)
[2025-02-13 03:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19][root][INFO] - Training Epoch: 1/2, step 4829/7134 completed (loss: 0.02531890943646431, acc: 0.9961538314819336)
[2025-02-13 03:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20][root][INFO] - Training Epoch: 1/2, step 4830/7134 completed (loss: 0.02089468017220497, acc: 0.9955621361732483)
[2025-02-13 03:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20][root][INFO] - Training Epoch: 1/2, step 4831/7134 completed (loss: 0.013824683614075184, acc: 0.9956140518188477)
[2025-02-13 03:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20][root][INFO] - Training Epoch: 1/2, step 4832/7134 completed (loss: 0.013716625981032848, acc: 0.9983713626861572)
[2025-02-13 03:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:21][root][INFO] - Training Epoch: 1/2, step 4833/7134 completed (loss: 0.018257584422826767, acc: 0.9949173927307129)
[2025-02-13 03:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:21][root][INFO] - Training Epoch: 1/2, step 4834/7134 completed (loss: 0.02179713174700737, acc: 0.9952380657196045)
[2025-02-13 03:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:22][root][INFO] - Training Epoch: 1/2, step 4835/7134 completed (loss: 0.02118881605565548, acc: 0.995945930480957)
[2025-02-13 03:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:22][root][INFO] - Training Epoch: 1/2, step 4836/7134 completed (loss: 0.014457744546234608, acc: 0.9938931465148926)
[2025-02-13 03:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23][root][INFO] - Training Epoch: 1/2, step 4837/7134 completed (loss: 0.09146725386381149, acc: 0.9834558963775635)
[2025-02-13 03:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23][root][INFO] - Training Epoch: 1/2, step 4838/7134 completed (loss: 0.0386158749461174, acc: 0.9919484853744507)
[2025-02-13 03:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23][root][INFO] - Training Epoch: 1/2, step 4839/7134 completed (loss: 0.08616741746664047, acc: 0.9810671210289001)
[2025-02-13 03:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:24][root][INFO] - Training Epoch: 1/2, step 4840/7134 completed (loss: 0.07942910492420197, acc: 0.9783236980438232)
[2025-02-13 03:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:24][root][INFO] - Training Epoch: 1/2, step 4841/7134 completed (loss: 0.034877460449934006, acc: 0.9940387606620789)
[2025-02-13 03:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25][root][INFO] - Training Epoch: 1/2, step 4842/7134 completed (loss: 0.05614126846194267, acc: 0.9821428656578064)
[2025-02-13 03:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25][root][INFO] - Training Epoch: 1/2, step 4843/7134 completed (loss: 0.03960786387324333, acc: 0.9881129264831543)
[2025-02-13 03:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25][root][INFO] - Training Epoch: 1/2, step 4844/7134 completed (loss: 0.033987198024988174, acc: 0.9854604005813599)
[2025-02-13 03:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:26][root][INFO] - Training Epoch: 1/2, step 4845/7134 completed (loss: 0.03925475850701332, acc: 0.9890710115432739)
[2025-02-13 03:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:26][root][INFO] - Training Epoch: 1/2, step 4846/7134 completed (loss: 0.055718794465065, acc: 0.9837837815284729)
[2025-02-13 03:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27][root][INFO] - Training Epoch: 1/2, step 4847/7134 completed (loss: 0.038589466363191605, acc: 0.9919137358665466)
[2025-02-13 03:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27][root][INFO] - Training Epoch: 1/2, step 4848/7134 completed (loss: 0.03549918904900551, acc: 0.9944751262664795)
[2025-02-13 03:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27][root][INFO] - Training Epoch: 1/2, step 4849/7134 completed (loss: 0.03013860248029232, acc: 0.9925705790519714)
[2025-02-13 03:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:28][root][INFO] - Training Epoch: 1/2, step 4850/7134 completed (loss: 0.04159647598862648, acc: 0.984088122844696)
[2025-02-13 03:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:28][root][INFO] - Training Epoch: 1/2, step 4851/7134 completed (loss: 0.04933162406086922, acc: 0.9848130941390991)
[2025-02-13 03:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29][root][INFO] - Training Epoch: 1/2, step 4852/7134 completed (loss: 0.044504158198833466, acc: 0.9866844415664673)
[2025-02-13 03:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29][root][INFO] - Training Epoch: 1/2, step 4853/7134 completed (loss: 0.10263673216104507, acc: 0.9771754741668701)
[2025-02-13 03:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29][root][INFO] - Training Epoch: 1/2, step 4854/7134 completed (loss: 0.043342433869838715, acc: 0.9853333234786987)
[2025-02-13 03:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:30][root][INFO] - Training Epoch: 1/2, step 4855/7134 completed (loss: 0.04388769716024399, acc: 0.987908124923706)
[2025-02-13 03:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:30][root][INFO] - Training Epoch: 1/2, step 4856/7134 completed (loss: 0.04585477337241173, acc: 0.9885495901107788)
[2025-02-13 03:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:31][root][INFO] - Training Epoch: 1/2, step 4857/7134 completed (loss: 0.016168050467967987, acc: 0.9956834316253662)
[2025-02-13 03:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:31][root][INFO] - Training Epoch: 1/2, step 4858/7134 completed (loss: 0.03312135487794876, acc: 0.9900285005569458)
[2025-02-13 03:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32][root][INFO] - Training Epoch: 1/2, step 4859/7134 completed (loss: 0.017992453649640083, acc: 0.9948520064353943)
[2025-02-13 03:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32][root][INFO] - Training Epoch: 1/2, step 4860/7134 completed (loss: 0.03485703468322754, acc: 0.9891008138656616)
[2025-02-13 03:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32][root][INFO] - Training Epoch: 1/2, step 4861/7134 completed (loss: 0.03736647963523865, acc: 0.986522912979126)
[2025-02-13 03:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:33][root][INFO] - Training Epoch: 1/2, step 4862/7134 completed (loss: 0.03181181475520134, acc: 0.991416335105896)
[2025-02-13 03:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:33][root][INFO] - Training Epoch: 1/2, step 4863/7134 completed (loss: 0.03538290783762932, acc: 0.9908758997917175)
[2025-02-13 03:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:34][root][INFO] - Training Epoch: 1/2, step 4864/7134 completed (loss: 0.04952557012438774, acc: 0.9815384745597839)
[2025-02-13 03:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:34][root][INFO] - Training Epoch: 1/2, step 4865/7134 completed (loss: 0.04234844073653221, acc: 0.9890109896659851)
[2025-02-13 03:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35][root][INFO] - Training Epoch: 1/2, step 4866/7134 completed (loss: 0.038505446165800095, acc: 0.9870298504829407)
[2025-02-13 03:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35][root][INFO] - Training Epoch: 1/2, step 4867/7134 completed (loss: 0.0541209913790226, acc: 0.9894179701805115)
[2025-02-13 03:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35][root][INFO] - Training Epoch: 1/2, step 4868/7134 completed (loss: 0.06390774250030518, acc: 0.9825581312179565)
[2025-02-13 03:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:36][root][INFO] - Training Epoch: 1/2, step 4869/7134 completed (loss: 0.060795001685619354, acc: 0.9822221994400024)
[2025-02-13 03:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:36][root][INFO] - Training Epoch: 1/2, step 4870/7134 completed (loss: 0.027997085824608803, acc: 0.9953051805496216)
[2025-02-13 03:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37][root][INFO] - Training Epoch: 1/2, step 4871/7134 completed (loss: 0.053336430341005325, acc: 0.9907407164573669)
[2025-02-13 03:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37][root][INFO] - Training Epoch: 1/2, step 4872/7134 completed (loss: 0.029841233044862747, acc: 0.9916550517082214)
[2025-02-13 03:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37][root][INFO] - Training Epoch: 1/2, step 4873/7134 completed (loss: 0.03125368431210518, acc: 0.9908854365348816)
[2025-02-13 03:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:38][root][INFO] - Training Epoch: 1/2, step 4874/7134 completed (loss: 0.02366515062749386, acc: 0.99314284324646)
[2025-02-13 03:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:38][root][INFO] - Training Epoch: 1/2, step 4875/7134 completed (loss: 0.047537174075841904, acc: 0.9875173568725586)
[2025-02-13 03:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:39][root][INFO] - Training Epoch: 1/2, step 4876/7134 completed (loss: 0.08781658858060837, acc: 0.9788583517074585)
[2025-02-13 03:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:39][root][INFO] - Training Epoch: 1/2, step 4877/7134 completed (loss: 0.05072226747870445, acc: 0.9832258224487305)
[2025-02-13 03:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40][root][INFO] - Training Epoch: 1/2, step 4878/7134 completed (loss: 0.04138650372624397, acc: 0.9808542132377625)
[2025-02-13 03:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40][root][INFO] - Training Epoch: 1/2, step 4879/7134 completed (loss: 0.06732991337776184, acc: 0.983433723449707)
[2025-02-13 03:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40][root][INFO] - Training Epoch: 1/2, step 4880/7134 completed (loss: 0.02178230695426464, acc: 0.9948119521141052)
[2025-02-13 03:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:41][root][INFO] - Training Epoch: 1/2, step 4881/7134 completed (loss: 0.06570742279291153, acc: 0.9824817776679993)
[2025-02-13 03:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:41][root][INFO] - Training Epoch: 1/2, step 4882/7134 completed (loss: 0.05089528113603592, acc: 0.9851973652839661)
[2025-02-13 03:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:42][root][INFO] - Training Epoch: 1/2, step 4883/7134 completed (loss: 0.04603395611047745, acc: 0.9824817776679993)
[2025-02-13 03:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:42][root][INFO] - Training Epoch: 1/2, step 4884/7134 completed (loss: 0.015142683871090412, acc: 0.996927797794342)
[2025-02-13 03:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43][root][INFO] - Training Epoch: 1/2, step 4885/7134 completed (loss: 0.013507749885320663, acc: 0.9973614811897278)
[2025-02-13 03:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43][root][INFO] - Training Epoch: 1/2, step 4886/7134 completed (loss: 0.02808978408575058, acc: 0.9897435903549194)
[2025-02-13 03:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43][root][INFO] - Training Epoch: 1/2, step 4887/7134 completed (loss: 0.018134841695427895, acc: 0.9940263032913208)
[2025-02-13 03:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:44][root][INFO] - Training Epoch: 1/2, step 4888/7134 completed (loss: 0.05495075508952141, acc: 0.9903846383094788)
[2025-02-13 03:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:44][root][INFO] - Training Epoch: 1/2, step 4889/7134 completed (loss: 0.02122177742421627, acc: 0.9951865077018738)
[2025-02-13 03:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45][root][INFO] - Training Epoch: 1/2, step 4890/7134 completed (loss: 0.052691105753183365, acc: 0.9928571581840515)
[2025-02-13 03:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45][root][INFO] - Training Epoch: 1/2, step 4891/7134 completed (loss: 0.05271458998322487, acc: 0.9888059496879578)
[2025-02-13 03:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45][root][INFO] - Training Epoch: 1/2, step 4892/7134 completed (loss: 0.07360534369945526, acc: 0.9798657894134521)
[2025-02-13 03:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:46][root][INFO] - Training Epoch: 1/2, step 4893/7134 completed (loss: 0.19538557529449463, acc: 0.960422158241272)
[2025-02-13 03:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:46][root][INFO] - Training Epoch: 1/2, step 4894/7134 completed (loss: 0.028179600834846497, acc: 0.9913544654846191)
[2025-02-13 03:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:47][root][INFO] - Training Epoch: 1/2, step 4895/7134 completed (loss: 0.019965605810284615, acc: 0.9925000071525574)
[2025-02-13 03:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:47][root][INFO] - Training Epoch: 1/2, step 4896/7134 completed (loss: 0.01575072482228279, acc: 0.9943820238113403)
[2025-02-13 03:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48][root][INFO] - Training Epoch: 1/2, step 4897/7134 completed (loss: 0.041865911334753036, acc: 0.9859648942947388)
[2025-02-13 03:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48][root][INFO] - Training Epoch: 1/2, step 4898/7134 completed (loss: 0.03900172933936119, acc: 0.9899371266365051)
[2025-02-13 03:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48][root][INFO] - Training Epoch: 1/2, step 4899/7134 completed (loss: 0.031345296651124954, acc: 0.9898107647895813)
[2025-02-13 03:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:49][root][INFO] - Training Epoch: 1/2, step 4900/7134 completed (loss: 0.04250211641192436, acc: 0.9894737005233765)
[2025-02-13 03:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:49][root][INFO] - Training Epoch: 1/2, step 4901/7134 completed (loss: 0.03296959772706032, acc: 0.9909774661064148)
[2025-02-13 03:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:50][root][INFO] - Training Epoch: 1/2, step 4902/7134 completed (loss: 0.030231082811951637, acc: 0.9933686852455139)
[2025-02-13 03:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:50][root][INFO] - Training Epoch: 1/2, step 4903/7134 completed (loss: 0.02767939865589142, acc: 0.9928315281867981)
[2025-02-13 03:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51][root][INFO] - Training Epoch: 1/2, step 4904/7134 completed (loss: 0.03405436500906944, acc: 0.9925373196601868)
[2025-02-13 03:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51][root][INFO] - Training Epoch: 1/2, step 4905/7134 completed (loss: 0.04704746976494789, acc: 0.987261176109314)
[2025-02-13 03:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51][root][INFO] - Training Epoch: 1/2, step 4906/7134 completed (loss: 0.03134351223707199, acc: 0.9932546615600586)
[2025-02-13 03:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:52][root][INFO] - Training Epoch: 1/2, step 4907/7134 completed (loss: 0.031822219491004944, acc: 0.9936708807945251)
[2025-02-13 03:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:52][root][INFO] - Training Epoch: 1/2, step 4908/7134 completed (loss: 0.018743285909295082, acc: 0.9904000163078308)
[2025-02-13 03:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:53][root][INFO] - Training Epoch: 1/2, step 4909/7134 completed (loss: 0.011760630644857883, acc: 0.9967585206031799)
[2025-02-13 03:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:53][root][INFO] - Training Epoch: 1/2, step 4910/7134 completed (loss: 0.02908473089337349, acc: 0.9921630024909973)
[2025-02-13 03:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54][root][INFO] - Training Epoch: 1/2, step 4911/7134 completed (loss: 0.04045233130455017, acc: 0.9894459247589111)
[2025-02-13 03:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54][root][INFO] - Training Epoch: 1/2, step 4912/7134 completed (loss: 0.038874004036188126, acc: 0.991525411605835)
[2025-02-13 03:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54][root][INFO] - Training Epoch: 1/2, step 4913/7134 completed (loss: 0.029918190091848373, acc: 0.9932432174682617)
[2025-02-13 03:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:55][root][INFO] - Training Epoch: 1/2, step 4914/7134 completed (loss: 0.02822388894855976, acc: 0.9908257126808167)
[2025-02-13 03:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:55][root][INFO] - Training Epoch: 1/2, step 4915/7134 completed (loss: 0.005226584151387215, acc: 1.0)
[2025-02-13 03:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56][root][INFO] - Training Epoch: 1/2, step 4916/7134 completed (loss: 0.08685826510190964, acc: 0.9781553149223328)
[2025-02-13 03:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56][root][INFO] - Training Epoch: 1/2, step 4917/7134 completed (loss: 0.0564860962331295, acc: 0.9874826073646545)
[2025-02-13 03:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56][root][INFO] - Training Epoch: 1/2, step 4918/7134 completed (loss: 0.006262057926505804, acc: 1.0)
[2025-02-13 03:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:57][root][INFO] - Training Epoch: 1/2, step 4919/7134 completed (loss: 0.05203823372721672, acc: 0.9868420958518982)
[2025-02-13 03:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:57][root][INFO] - Training Epoch: 1/2, step 4920/7134 completed (loss: 0.08628246188163757, acc: 0.9793281555175781)
[2025-02-13 03:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:58][root][INFO] - Training Epoch: 1/2, step 4921/7134 completed (loss: 0.04299062117934227, acc: 0.991391658782959)
[2025-02-13 03:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:58][root][INFO] - Training Epoch: 1/2, step 4922/7134 completed (loss: 0.03131145238876343, acc: 0.9931972622871399)
[2025-02-13 03:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59][root][INFO] - Training Epoch: 1/2, step 4923/7134 completed (loss: 0.024423489347100258, acc: 0.9926578402519226)
[2025-02-13 03:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59][root][INFO] - Training Epoch: 1/2, step 4924/7134 completed (loss: 0.03787682205438614, acc: 0.9879999756813049)
[2025-02-13 03:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59][root][INFO] - Training Epoch: 1/2, step 4925/7134 completed (loss: 0.03362417593598366, acc: 0.9856938719749451)
[2025-02-13 03:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00][root][INFO] - Training Epoch: 1/2, step 4926/7134 completed (loss: 0.021821223199367523, acc: 0.9942611455917358)
[2025-02-13 03:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00][root][INFO] - Training Epoch: 1/2, step 4927/7134 completed (loss: 0.03934550657868385, acc: 0.9887955188751221)
[2025-02-13 03:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01][root][INFO] - Training Epoch: 1/2, step 4928/7134 completed (loss: 0.025968005880713463, acc: 0.9884124994277954)
[2025-02-13 03:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01][root][INFO] - Training Epoch: 1/2, step 4929/7134 completed (loss: 0.028439121320843697, acc: 0.9891473054885864)
[2025-02-13 03:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01][root][INFO] - Training Epoch: 1/2, step 4930/7134 completed (loss: 0.04881108179688454, acc: 0.9909502267837524)
[2025-02-13 03:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:02][root][INFO] - Training Epoch: 1/2, step 4931/7134 completed (loss: 0.07323174923658371, acc: 0.9847792983055115)
[2025-02-13 03:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:02][root][INFO] - Training Epoch: 1/2, step 4932/7134 completed (loss: 0.03148490563035011, acc: 0.9905533194541931)
[2025-02-13 03:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:03][root][INFO] - Training Epoch: 1/2, step 4933/7134 completed (loss: 0.048910755664110184, acc: 0.9864029884338379)
[2025-02-13 03:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:03][root][INFO] - Training Epoch: 1/2, step 4934/7134 completed (loss: 0.01477776188403368, acc: 0.9945873022079468)
[2025-02-13 03:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04][root][INFO] - Training Epoch: 1/2, step 4935/7134 completed (loss: 0.026943378150463104, acc: 0.9917920827865601)
[2025-02-13 03:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04][root][INFO] - Training Epoch: 1/2, step 4936/7134 completed (loss: 0.04259340092539787, acc: 0.9907407164573669)
[2025-02-13 03:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04][root][INFO] - Training Epoch: 1/2, step 4937/7134 completed (loss: 0.03259760141372681, acc: 0.988041877746582)
[2025-02-13 03:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:05][root][INFO] - Training Epoch: 1/2, step 4938/7134 completed (loss: 0.03748833388090134, acc: 0.9878542423248291)
[2025-02-13 03:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:05][root][INFO] - Training Epoch: 1/2, step 4939/7134 completed (loss: 0.02489818073809147, acc: 0.990212082862854)
[2025-02-13 03:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06][root][INFO] - Training Epoch: 1/2, step 4940/7134 completed (loss: 0.041392166167497635, acc: 0.9863636493682861)
[2025-02-13 03:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06][root][INFO] - Training Epoch: 1/2, step 4941/7134 completed (loss: 0.03376920521259308, acc: 0.9919246435165405)
[2025-02-13 03:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06][root][INFO] - Training Epoch: 1/2, step 4942/7134 completed (loss: 0.03173296898603439, acc: 0.9906166195869446)
[2025-02-13 03:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:07][root][INFO] - Training Epoch: 1/2, step 4943/7134 completed (loss: 0.04671965539455414, acc: 0.9863201379776001)
[2025-02-13 03:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:07][root][INFO] - Training Epoch: 1/2, step 4944/7134 completed (loss: 0.03660508990287781, acc: 0.9852631688117981)
[2025-02-13 03:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08][root][INFO] - Training Epoch: 1/2, step 4945/7134 completed (loss: 0.06512901186943054, acc: 0.9737903475761414)
[2025-02-13 03:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08][root][INFO] - Training Epoch: 1/2, step 4946/7134 completed (loss: 0.07152879983186722, acc: 0.9714285731315613)
[2025-02-13 03:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08][root][INFO] - Training Epoch: 1/2, step 4947/7134 completed (loss: 0.04450368881225586, acc: 0.9844789505004883)
[2025-02-13 03:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:09][root][INFO] - Training Epoch: 1/2, step 4948/7134 completed (loss: 0.061062008142471313, acc: 0.9866310358047485)
[2025-02-13 03:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:09][root][INFO] - Training Epoch: 1/2, step 4949/7134 completed (loss: 0.06439220905303955, acc: 0.9777777791023254)
[2025-02-13 03:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10][root][INFO] - Training Epoch: 1/2, step 4950/7134 completed (loss: 0.046377599239349365, acc: 0.9894921183586121)
[2025-02-13 03:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10][root][INFO] - Training Epoch: 1/2, step 4951/7134 completed (loss: 0.04873676970601082, acc: 0.9857549667358398)
[2025-02-13 03:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10][root][INFO] - Training Epoch: 1/2, step 4952/7134 completed (loss: 0.038549527525901794, acc: 0.9933775067329407)
[2025-02-13 03:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11][root][INFO] - Training Epoch: 1/2, step 4953/7134 completed (loss: 0.02686372585594654, acc: 0.9912853837013245)
[2025-02-13 03:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11][root][INFO] - Training Epoch: 1/2, step 4954/7134 completed (loss: 0.03105892799794674, acc: 0.9857142567634583)
[2025-02-13 03:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11][root][INFO] - Training Epoch: 1/2, step 4955/7134 completed (loss: 0.03160353749990463, acc: 0.9882903695106506)
[2025-02-13 03:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:12][root][INFO] - Training Epoch: 1/2, step 4956/7134 completed (loss: 0.02783791534602642, acc: 0.991465151309967)
[2025-02-13 03:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:12][root][INFO] - Training Epoch: 1/2, step 4957/7134 completed (loss: 0.0274769589304924, acc: 0.9894179701805115)
[2025-02-13 03:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:13][root][INFO] - Training Epoch: 1/2, step 4958/7134 completed (loss: 0.012926180846989155, acc: 0.9956834316253662)
[2025-02-13 03:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:13][root][INFO] - Training Epoch: 1/2, step 4959/7134 completed (loss: 0.021700505167245865, acc: 0.9911764860153198)
[2025-02-13 03:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14][root][INFO] - Training Epoch: 1/2, step 4960/7134 completed (loss: 0.021977907046675682, acc: 0.9931034445762634)
[2025-02-13 03:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14][root][INFO] - Training Epoch: 1/2, step 4961/7134 completed (loss: 0.008321765810251236, acc: 0.9981784820556641)
[2025-02-13 03:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14][root][INFO] - Training Epoch: 1/2, step 4962/7134 completed (loss: 0.006492553278803825, acc: 0.9984447956085205)
[2025-02-13 03:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:15][root][INFO] - Training Epoch: 1/2, step 4963/7134 completed (loss: 0.0371323823928833, acc: 0.9902597665786743)
[2025-02-13 03:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:15][root][INFO] - Training Epoch: 1/2, step 4964/7134 completed (loss: 0.033600103110075, acc: 0.9902439117431641)
[2025-02-13 03:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16][root][INFO] - Training Epoch: 1/2, step 4965/7134 completed (loss: 0.04541727155447006, acc: 0.9852941036224365)
[2025-02-13 03:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16][root][INFO] - Training Epoch: 1/2, step 4966/7134 completed (loss: 0.07800248265266418, acc: 0.9806094169616699)
[2025-02-13 03:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16][root][INFO] - Training Epoch: 1/2, step 4967/7134 completed (loss: 0.060665447264909744, acc: 0.986328125)
[2025-02-13 03:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:17][root][INFO] - Training Epoch: 1/2, step 4968/7134 completed (loss: 0.09518173336982727, acc: 0.9715302586555481)
[2025-02-13 03:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:17][root][INFO] - Training Epoch: 1/2, step 4969/7134 completed (loss: 0.09784699976444244, acc: 0.9776632189750671)
[2025-02-13 03:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18][root][INFO] - Training Epoch: 1/2, step 4970/7134 completed (loss: 0.03321082517504692, acc: 0.9899665713310242)
[2025-02-13 03:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18][root][INFO] - Training Epoch: 1/2, step 4971/7134 completed (loss: 0.02266988344490528, acc: 0.991525411605835)
[2025-02-13 03:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18][root][INFO] - Training Epoch: 1/2, step 4972/7134 completed (loss: 0.017837537452578545, acc: 0.995768666267395)
[2025-02-13 03:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:19][root][INFO] - Training Epoch: 1/2, step 4973/7134 completed (loss: 0.044348906725645065, acc: 0.9876543283462524)
[2025-02-13 03:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:19][root][INFO] - Training Epoch: 1/2, step 4974/7134 completed (loss: 0.08865807205438614, acc: 0.9759519100189209)
[2025-02-13 03:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20][root][INFO] - Training Epoch: 1/2, step 4975/7134 completed (loss: 0.07790759950876236, acc: 0.9760383367538452)
[2025-02-13 03:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20][root][INFO] - Training Epoch: 1/2, step 4976/7134 completed (loss: 0.05558886379003525, acc: 0.9812646508216858)
[2025-02-13 03:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20][root][INFO] - Training Epoch: 1/2, step 4977/7134 completed (loss: 0.10722928494215012, acc: 0.9720430374145508)
[2025-02-13 03:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:21][root][INFO] - Training Epoch: 1/2, step 4978/7134 completed (loss: 0.037588976323604584, acc: 0.9875776171684265)
[2025-02-13 03:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:21][root][INFO] - Training Epoch: 1/2, step 4979/7134 completed (loss: 0.1340426802635193, acc: 0.9686985015869141)
[2025-02-13 03:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22][root][INFO] - Training Epoch: 1/2, step 4980/7134 completed (loss: 0.06376002728939056, acc: 0.9871175289154053)
[2025-02-13 03:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22][root][INFO] - Training Epoch: 1/2, step 4981/7134 completed (loss: 0.04550256207585335, acc: 0.9840510487556458)
[2025-02-13 03:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22][root][INFO] - Training Epoch: 1/2, step 4982/7134 completed (loss: 0.09631438553333282, acc: 0.9757084846496582)
[2025-02-13 03:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23][root][INFO] - Training Epoch: 1/2, step 4983/7134 completed (loss: 0.05529139190912247, acc: 0.985981285572052)
[2025-02-13 03:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23][root][INFO] - Training Epoch: 1/2, step 4984/7134 completed (loss: 0.08491582423448563, acc: 0.9791666865348816)
[2025-02-13 03:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23][root][INFO] - Training Epoch: 1/2, step 4985/7134 completed (loss: 0.05319037288427353, acc: 0.9877675771713257)
[2025-02-13 03:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:24][root][INFO] - Training Epoch: 1/2, step 4986/7134 completed (loss: 0.09676750004291534, acc: 0.9777117371559143)
[2025-02-13 03:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:24][root][INFO] - Training Epoch: 1/2, step 4987/7134 completed (loss: 0.04923507571220398, acc: 0.9840954542160034)
[2025-02-13 03:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25][root][INFO] - Training Epoch: 1/2, step 4988/7134 completed (loss: 0.0361962765455246, acc: 0.9893993139266968)
[2025-02-13 03:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25][root][INFO] - Training Epoch: 1/2, step 4989/7134 completed (loss: 0.07716315984725952, acc: 0.9801980257034302)
[2025-02-13 03:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25][root][INFO] - Training Epoch: 1/2, step 4990/7134 completed (loss: 0.04467775300145149, acc: 0.9881756901741028)
[2025-02-13 03:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26][root][INFO] - Training Epoch: 1/2, step 4991/7134 completed (loss: 0.03925308212637901, acc: 0.9865996837615967)
[2025-02-13 03:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26][root][INFO] - Training Epoch: 1/2, step 4992/7134 completed (loss: 0.11621799319982529, acc: 0.9759299755096436)
[2025-02-13 03:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26][root][INFO] - Training Epoch: 1/2, step 4993/7134 completed (loss: 0.03382495045661926, acc: 0.9864253401756287)
[2025-02-13 03:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:27][root][INFO] - Training Epoch: 1/2, step 4994/7134 completed (loss: 0.03352140635251999, acc: 0.9900709390640259)
[2025-02-13 03:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:27][root][INFO] - Training Epoch: 1/2, step 4995/7134 completed (loss: 0.05375226214528084, acc: 0.9836734533309937)
[2025-02-13 03:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28][root][INFO] - Training Epoch: 1/2, step 4996/7134 completed (loss: 0.04209039732813835, acc: 0.9870848655700684)
[2025-02-13 03:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28][root][INFO] - Training Epoch: 1/2, step 4997/7134 completed (loss: 0.07336322218179703, acc: 0.9777397513389587)
[2025-02-13 03:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28][root][INFO] - Training Epoch: 1/2, step 4998/7134 completed (loss: 0.12661908566951752, acc: 0.9699453711509705)
[2025-02-13 03:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:29][root][INFO] - Training Epoch: 1/2, step 4999/7134 completed (loss: 0.028845014050602913, acc: 0.9931153059005737)
[2025-02-13 03:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:29][root][INFO] - Training Epoch: 1/2, step 5000/7134 completed (loss: 0.022367455065250397, acc: 0.9930232763290405)
[2025-02-13 03:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30][root][INFO] - Training Epoch: 1/2, step 5001/7134 completed (loss: 0.07264561206102371, acc: 0.9883720874786377)
[2025-02-13 03:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30][root][INFO] - Training Epoch: 1/2, step 5002/7134 completed (loss: 0.05084864795207977, acc: 0.9888476133346558)
[2025-02-13 03:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30][root][INFO] - Training Epoch: 1/2, step 5003/7134 completed (loss: 0.03777849301695824, acc: 0.9904000163078308)
[2025-02-13 03:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31][root][INFO] - Training Epoch: 1/2, step 5004/7134 completed (loss: 0.013131135143339634, acc: 0.9955849647521973)
[2025-02-13 03:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31][root][INFO] - Training Epoch: 1/2, step 5005/7134 completed (loss: 0.06072142347693443, acc: 0.9804878234863281)
[2025-02-13 03:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31][root][INFO] - Training Epoch: 1/2, step 5006/7134 completed (loss: 0.04369393363595009, acc: 0.98531574010849)
[2025-02-13 03:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:32][root][INFO] - Training Epoch: 1/2, step 5007/7134 completed (loss: 0.030182067304849625, acc: 0.9971751570701599)
[2025-02-13 03:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:32][root][INFO] - Training Epoch: 1/2, step 5008/7134 completed (loss: 0.06419922411441803, acc: 0.9763779640197754)
[2025-02-13 03:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33][root][INFO] - Training Epoch: 1/2, step 5009/7134 completed (loss: 0.07667943090200424, acc: 0.9925558567047119)
[2025-02-13 03:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33][root][INFO] - Training Epoch: 1/2, step 5010/7134 completed (loss: 0.05956059694290161, acc: 0.9858299493789673)
[2025-02-13 03:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33][root][INFO] - Training Epoch: 1/2, step 5011/7134 completed (loss: 0.07492092996835709, acc: 0.9821138381958008)
[2025-02-13 03:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:34][root][INFO] - Training Epoch: 1/2, step 5012/7134 completed (loss: 0.10159412771463394, acc: 0.9741379022598267)
[2025-02-13 03:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:34][root][INFO] - Training Epoch: 1/2, step 5013/7134 completed (loss: 0.026741018518805504, acc: 0.9879759550094604)
[2025-02-13 03:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35][root][INFO] - Training Epoch: 1/2, step 5014/7134 completed (loss: 0.049216363579034805, acc: 0.9897540807723999)
[2025-02-13 03:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35][root][INFO] - Training Epoch: 1/2, step 5015/7134 completed (loss: 0.09437780827283859, acc: 0.9793103337287903)
[2025-02-13 03:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35][root][INFO] - Training Epoch: 1/2, step 5016/7134 completed (loss: 0.02302035316824913, acc: 0.994575023651123)
[2025-02-13 03:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:36][root][INFO] - Training Epoch: 1/2, step 5017/7134 completed (loss: 0.028426621109247208, acc: 0.9963964223861694)
[2025-02-13 03:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:36][root][INFO] - Training Epoch: 1/2, step 5018/7134 completed (loss: 0.03912635147571564, acc: 0.98758864402771)
[2025-02-13 03:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37][root][INFO] - Training Epoch: 1/2, step 5019/7134 completed (loss: 0.031676217913627625, acc: 0.9900285005569458)
[2025-02-13 03:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37][root][INFO] - Training Epoch: 1/2, step 5020/7134 completed (loss: 0.04981829226016998, acc: 0.9862499833106995)
[2025-02-13 03:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37][root][INFO] - Training Epoch: 1/2, step 5021/7134 completed (loss: 0.06003324314951897, acc: 0.9816933870315552)
[2025-02-13 03:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:38][root][INFO] - Training Epoch: 1/2, step 5022/7134 completed (loss: 0.04869934916496277, acc: 0.9891975522041321)
[2025-02-13 03:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:38][root][INFO] - Training Epoch: 1/2, step 5023/7134 completed (loss: 0.05332252383232117, acc: 0.9863221645355225)
[2025-02-13 03:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:39][root][INFO] - Training Epoch: 1/2, step 5024/7134 completed (loss: 0.03149908035993576, acc: 0.993565022945404)
[2025-02-13 03:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:39][root][INFO] - Training Epoch: 1/2, step 5025/7134 completed (loss: 0.030197879299521446, acc: 0.9927536249160767)
[2025-02-13 03:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40][root][INFO] - Training Epoch: 1/2, step 5026/7134 completed (loss: 0.030583219602704048, acc: 0.9912434220314026)
[2025-02-13 03:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40][root][INFO] - Training Epoch: 1/2, step 5027/7134 completed (loss: 0.032337628304958344, acc: 0.9908925294876099)
[2025-02-13 03:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40][root][INFO] - Training Epoch: 1/2, step 5028/7134 completed (loss: 0.03195848688483238, acc: 0.9890282154083252)
[2025-02-13 03:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:41][root][INFO] - Training Epoch: 1/2, step 5029/7134 completed (loss: 0.025221293792128563, acc: 0.9886363744735718)
[2025-02-13 03:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:41][root][INFO] - Training Epoch: 1/2, step 5030/7134 completed (loss: 0.026580406352877617, acc: 0.9945429563522339)
[2025-02-13 03:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42][root][INFO] - Training Epoch: 1/2, step 5031/7134 completed (loss: 0.07335314154624939, acc: 0.9914966225624084)
[2025-02-13 03:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42][root][INFO] - Training Epoch: 1/2, step 5032/7134 completed (loss: 0.02870262786746025, acc: 0.9951298832893372)
[2025-02-13 03:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42][root][INFO] - Training Epoch: 1/2, step 5033/7134 completed (loss: 0.019682474434375763, acc: 0.9935897588729858)
[2025-02-13 03:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:43][root][INFO] - Training Epoch: 1/2, step 5034/7134 completed (loss: 0.022615553811192513, acc: 0.991134762763977)
[2025-02-13 03:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:43][root][INFO] - Training Epoch: 1/2, step 5035/7134 completed (loss: 0.005169674288481474, acc: 1.0)
[2025-02-13 03:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44][root][INFO] - Training Epoch: 1/2, step 5036/7134 completed (loss: 0.07284574955701828, acc: 0.984375)
[2025-02-13 03:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44][root][INFO] - Training Epoch: 1/2, step 5037/7134 completed (loss: 0.0693264752626419, acc: 0.989276111125946)
[2025-02-13 03:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44][root][INFO] - Training Epoch: 1/2, step 5038/7134 completed (loss: 0.05205905809998512, acc: 0.9866369962692261)
[2025-02-13 03:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:45][root][INFO] - Training Epoch: 1/2, step 5039/7134 completed (loss: 0.05014779046177864, acc: 0.9857594966888428)
[2025-02-13 03:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:45][root][INFO] - Training Epoch: 1/2, step 5040/7134 completed (loss: 0.020709063857793808, acc: 0.9923518300056458)
[2025-02-13 03:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46][root][INFO] - Training Epoch: 1/2, step 5041/7134 completed (loss: 0.01002438087016344, acc: 0.9973474740982056)
[2025-02-13 03:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46][root][INFO] - Training Epoch: 1/2, step 5042/7134 completed (loss: 0.06213720887899399, acc: 0.9860140085220337)
[2025-02-13 03:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46][root][INFO] - Training Epoch: 1/2, step 5043/7134 completed (loss: 0.04123824089765549, acc: 0.9881423115730286)
[2025-02-13 03:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47][root][INFO] - Training Epoch: 1/2, step 5044/7134 completed (loss: 0.021699104458093643, acc: 0.9956140518188477)
[2025-02-13 03:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47][root][INFO] - Training Epoch: 1/2, step 5045/7134 completed (loss: 0.06041383370757103, acc: 0.983849287033081)
[2025-02-13 03:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47][root][INFO] - Training Epoch: 1/2, step 5046/7134 completed (loss: 0.09474658966064453, acc: 0.9754385948181152)
[2025-02-13 03:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:48][root][INFO] - Training Epoch: 1/2, step 5047/7134 completed (loss: 0.07646464556455612, acc: 0.9801653027534485)
[2025-02-13 03:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:48][root][INFO] - Training Epoch: 1/2, step 5048/7134 completed (loss: 0.05572211742401123, acc: 0.9832572340965271)
[2025-02-13 03:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49][root][INFO] - Training Epoch: 1/2, step 5049/7134 completed (loss: 0.06932740658521652, acc: 0.9768392443656921)
[2025-02-13 03:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49][root][INFO] - Training Epoch: 1/2, step 5050/7134 completed (loss: 0.06380511820316315, acc: 0.9831223487854004)
[2025-02-13 03:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49][root][INFO] - Training Epoch: 1/2, step 5051/7134 completed (loss: 0.0792786255478859, acc: 0.97826087474823)
[2025-02-13 03:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:50][root][INFO] - Training Epoch: 1/2, step 5052/7134 completed (loss: 0.04172701761126518, acc: 0.9863636493682861)
[2025-02-13 03:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:50][root][INFO] - Training Epoch: 1/2, step 5053/7134 completed (loss: 0.11387301981449127, acc: 0.9716981053352356)
[2025-02-13 03:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51][root][INFO] - Training Epoch: 1/2, step 5054/7134 completed (loss: 0.0893019512295723, acc: 0.9763407111167908)
[2025-02-13 03:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51][root][INFO] - Training Epoch: 1/2, step 5055/7134 completed (loss: 0.03165677189826965, acc: 0.9857142567634583)
[2025-02-13 03:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51][root][INFO] - Training Epoch: 1/2, step 5056/7134 completed (loss: 0.1110544204711914, acc: 0.9777777791023254)
[2025-02-13 03:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:52][root][INFO] - Training Epoch: 1/2, step 5057/7134 completed (loss: 0.09730959683656693, acc: 0.9829303026199341)
[2025-02-13 03:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:52][root][INFO] - Training Epoch: 1/2, step 5058/7134 completed (loss: 0.0841788500547409, acc: 0.9826086759567261)
[2025-02-13 03:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:53][root][INFO] - Training Epoch: 1/2, step 5059/7134 completed (loss: 0.04504449665546417, acc: 0.9842725992202759)
[2025-02-13 03:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:53][root][INFO] - Training Epoch: 1/2, step 5060/7134 completed (loss: 0.036077044904232025, acc: 0.9910846948623657)
[2025-02-13 03:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54][root][INFO] - Training Epoch: 1/2, step 5061/7134 completed (loss: 0.0667409673333168, acc: 0.9784017205238342)
[2025-02-13 03:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54][root][INFO] - Training Epoch: 1/2, step 5062/7134 completed (loss: 0.09855833649635315, acc: 0.9742709994316101)
[2025-02-13 03:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54][root][INFO] - Training Epoch: 1/2, step 5063/7134 completed (loss: 0.0680665597319603, acc: 0.9849749803543091)
[2025-02-13 03:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:55][root][INFO] - Training Epoch: 1/2, step 5064/7134 completed (loss: 0.059214506298303604, acc: 0.9897260069847107)
[2025-02-13 03:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:55][root][INFO] - Training Epoch: 1/2, step 5065/7134 completed (loss: 0.0592992790043354, acc: 0.9873060584068298)
[2025-02-13 03:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56][root][INFO] - Training Epoch: 1/2, step 5066/7134 completed (loss: 0.07571963220834732, acc: 0.9803328514099121)
[2025-02-13 03:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56][root][INFO] - Training Epoch: 1/2, step 5067/7134 completed (loss: 0.034901052713394165, acc: 0.9922027587890625)
[2025-02-13 03:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56][root][INFO] - Training Epoch: 1/2, step 5068/7134 completed (loss: 0.060076143592596054, acc: 0.9873617887496948)
[2025-02-13 03:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:57][root][INFO] - Training Epoch: 1/2, step 5069/7134 completed (loss: 0.03402387350797653, acc: 0.9921875)
[2025-02-13 03:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:57][root][INFO] - Training Epoch: 1/2, step 5070/7134 completed (loss: 0.0345599502325058, acc: 0.9885057210922241)
[2025-02-13 03:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58][root][INFO] - Training Epoch: 1/2, step 5071/7134 completed (loss: 0.08832073956727982, acc: 0.9761570692062378)
[2025-02-13 03:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58][root][INFO] - Training Epoch: 1/2, step 5072/7134 completed (loss: 0.0653316080570221, acc: 0.981179416179657)
[2025-02-13 03:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58][root][INFO] - Training Epoch: 1/2, step 5073/7134 completed (loss: 0.0698469877243042, acc: 0.9729207158088684)
[2025-02-13 03:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:59][root][INFO] - Training Epoch: 1/2, step 5074/7134 completed (loss: 0.07491637766361237, acc: 0.9850746393203735)
[2025-02-13 03:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:59][root][INFO] - Training Epoch: 1/2, step 5075/7134 completed (loss: 0.05577247589826584, acc: 0.9883720874786377)
[2025-02-13 03:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:00][root][INFO] - Training Epoch: 1/2, step 5076/7134 completed (loss: 0.07598130404949188, acc: 0.9844497442245483)
[2025-02-13 03:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:00][root][INFO] - Training Epoch: 1/2, step 5077/7134 completed (loss: 0.06263256818056107, acc: 0.9794303774833679)
[2025-02-13 03:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01][root][INFO] - Training Epoch: 1/2, step 5078/7134 completed (loss: 0.09377128630876541, acc: 0.9748954176902771)
[2025-02-13 03:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01][root][INFO] - Training Epoch: 1/2, step 5079/7134 completed (loss: 0.030952494591474533, acc: 0.9904761910438538)
[2025-02-13 03:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01][root][INFO] - Training Epoch: 1/2, step 5080/7134 completed (loss: 0.028021493926644325, acc: 0.991725742816925)
[2025-02-13 03:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:02][root][INFO] - Training Epoch: 1/2, step 5081/7134 completed (loss: 0.024199698120355606, acc: 0.9945651888847351)
[2025-02-13 03:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:02][root][INFO] - Training Epoch: 1/2, step 5082/7134 completed (loss: 0.032668337225914, acc: 0.9931972622871399)
[2025-02-13 03:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:03][root][INFO] - Training Epoch: 1/2, step 5083/7134 completed (loss: 0.04643533378839493, acc: 0.9887482523918152)
[2025-02-13 03:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:03][root][INFO] - Training Epoch: 1/2, step 5084/7134 completed (loss: 0.01945948787033558, acc: 0.9965277910232544)
[2025-02-13 03:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:04][root][INFO] - Training Epoch: 1/2, step 5085/7134 completed (loss: 0.04403200000524521, acc: 0.9869822263717651)
[2025-02-13 03:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:04][root][INFO] - Training Epoch: 1/2, step 5086/7134 completed (loss: 0.05037233233451843, acc: 0.987293541431427)
[2025-02-13 03:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05][root][INFO] - Training Epoch: 1/2, step 5087/7134 completed (loss: 0.017330344766378403, acc: 0.9919678568840027)
[2025-02-13 03:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05][root][INFO] - Training Epoch: 1/2, step 5088/7134 completed (loss: 0.010525534860789776, acc: 0.9956076145172119)
[2025-02-13 03:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05][root][INFO] - Training Epoch: 1/2, step 5089/7134 completed (loss: 0.03025970235466957, acc: 0.9912717938423157)
[2025-02-13 03:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:06][root][INFO] - Training Epoch: 1/2, step 5090/7134 completed (loss: 0.017450496554374695, acc: 0.9953325390815735)
[2025-02-13 03:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:06][root][INFO] - Training Epoch: 1/2, step 5091/7134 completed (loss: 0.022210288792848587, acc: 0.9934123754501343)
[2025-02-13 03:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07][root][INFO] - Training Epoch: 1/2, step 5092/7134 completed (loss: 0.05432221293449402, acc: 0.9861687421798706)
[2025-02-13 03:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07][root][INFO] - Training Epoch: 1/2, step 5093/7134 completed (loss: 0.03581441938877106, acc: 0.9903314709663391)
[2025-02-13 03:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07][root][INFO] - Training Epoch: 1/2, step 5094/7134 completed (loss: 0.04525431990623474, acc: 0.9909090995788574)
[2025-02-13 03:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:08][root][INFO] - Training Epoch: 1/2, step 5095/7134 completed (loss: 0.13103577494621277, acc: 0.9703587889671326)
[2025-02-13 03:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:08][root][INFO] - Training Epoch: 1/2, step 5096/7134 completed (loss: 0.04472382739186287, acc: 0.9838150143623352)
[2025-02-13 03:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:09][root][INFO] - Training Epoch: 1/2, step 5097/7134 completed (loss: 0.05168162286281586, acc: 0.9865689873695374)
[2025-02-13 03:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:09][root][INFO] - Training Epoch: 1/2, step 5098/7134 completed (loss: 0.04618830978870392, acc: 0.9853768348693848)
[2025-02-13 03:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:10][root][INFO] - Training Epoch: 1/2, step 5099/7134 completed (loss: 0.050797682255506516, acc: 0.982679009437561)
[2025-02-13 03:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:10][root][INFO] - Training Epoch: 1/2, step 5100/7134 completed (loss: 0.04820980131626129, acc: 0.9878854751586914)
[2025-02-13 03:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11][root][INFO] - Training Epoch: 1/2, step 5101/7134 completed (loss: 0.020616406574845314, acc: 0.9953380227088928)
[2025-02-13 03:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11][root][INFO] - Training Epoch: 1/2, step 5102/7134 completed (loss: 0.060689836740493774, acc: 0.9836289286613464)
[2025-02-13 03:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11][root][INFO] - Training Epoch: 1/2, step 5103/7134 completed (loss: 0.040570586919784546, acc: 0.9896907210350037)
[2025-02-13 03:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:12][root][INFO] - Training Epoch: 1/2, step 5104/7134 completed (loss: 0.02043238840997219, acc: 0.9955703020095825)
[2025-02-13 03:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:12][root][INFO] - Training Epoch: 1/2, step 5105/7134 completed (loss: 0.027660537511110306, acc: 0.9895678162574768)
[2025-02-13 03:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:13][root][INFO] - Training Epoch: 1/2, step 5106/7134 completed (loss: 0.03245852142572403, acc: 0.9940740466117859)
[2025-02-13 03:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:13][root][INFO] - Training Epoch: 1/2, step 5107/7134 completed (loss: 0.10025224089622498, acc: 0.9825673699378967)
[2025-02-13 03:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14][root][INFO] - Training Epoch: 1/2, step 5108/7134 completed (loss: 0.09348156303167343, acc: 0.9721254110336304)
[2025-02-13 03:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14][root][INFO] - Training Epoch: 1/2, step 5109/7134 completed (loss: 0.08323810249567032, acc: 0.9767699241638184)
[2025-02-13 03:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14][root][INFO] - Training Epoch: 1/2, step 5110/7134 completed (loss: 0.08493854105472565, acc: 0.9743589758872986)
[2025-02-13 03:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:15][root][INFO] - Training Epoch: 1/2, step 5111/7134 completed (loss: 0.04690629243850708, acc: 0.9863013625144958)
[2025-02-13 03:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:15][root][INFO] - Training Epoch: 1/2, step 5112/7134 completed (loss: 0.0598393939435482, acc: 0.9850560426712036)
[2025-02-13 03:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:16][root][INFO] - Training Epoch: 1/2, step 5113/7134 completed (loss: 0.0602780245244503, acc: 0.9850746393203735)
[2025-02-13 03:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:16][root][INFO] - Training Epoch: 1/2, step 5114/7134 completed (loss: 0.031776461750268936, acc: 0.9944367408752441)
[2025-02-13 03:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17][root][INFO] - Training Epoch: 1/2, step 5115/7134 completed (loss: 0.0335753858089447, acc: 0.9922580718994141)
[2025-02-13 03:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17][root][INFO] - Training Epoch: 1/2, step 5116/7134 completed (loss: 0.03570893406867981, acc: 0.9909090995788574)
[2025-02-13 03:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17][root][INFO] - Training Epoch: 1/2, step 5117/7134 completed (loss: 0.0404859334230423, acc: 0.9888424277305603)
[2025-02-13 03:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:18][root][INFO] - Training Epoch: 1/2, step 5118/7134 completed (loss: 0.07681868225336075, acc: 0.9848254919052124)
[2025-02-13 03:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:18][root][INFO] - Training Epoch: 1/2, step 5119/7134 completed (loss: 0.04564300924539566, acc: 0.993122398853302)
[2025-02-13 03:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19][root][INFO] - Training Epoch: 1/2, step 5120/7134 completed (loss: 0.08616537600755692, acc: 0.981675386428833)
[2025-02-13 03:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19][root][INFO] - Training Epoch: 1/2, step 5121/7134 completed (loss: 0.028732018545269966, acc: 0.9916782379150391)
[2025-02-13 03:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19][root][INFO] - Training Epoch: 1/2, step 5122/7134 completed (loss: 0.05223682150244713, acc: 0.9831223487854004)
[2025-02-13 03:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:20][root][INFO] - Training Epoch: 1/2, step 5123/7134 completed (loss: 0.03189238905906677, acc: 0.9915561079978943)
[2025-02-13 03:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:20][root][INFO] - Training Epoch: 1/2, step 5124/7134 completed (loss: 0.052092019468545914, acc: 0.9801242351531982)
[2025-02-13 03:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:21][root][INFO] - Training Epoch: 1/2, step 5125/7134 completed (loss: 0.03387776389718056, acc: 0.9921011328697205)
[2025-02-13 03:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:21][root][INFO] - Training Epoch: 1/2, step 5126/7134 completed (loss: 0.024417990818619728, acc: 0.9948717951774597)
[2025-02-13 03:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22][root][INFO] - Training Epoch: 1/2, step 5127/7134 completed (loss: 0.04652936011552811, acc: 0.988811194896698)
[2025-02-13 03:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22][root][INFO] - Training Epoch: 1/2, step 5128/7134 completed (loss: 0.02298855595290661, acc: 0.9945054650306702)
[2025-02-13 03:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22][root][INFO] - Training Epoch: 1/2, step 5129/7134 completed (loss: 0.08017094433307648, acc: 0.9725490212440491)
[2025-02-13 03:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:23][root][INFO] - Training Epoch: 1/2, step 5130/7134 completed (loss: 0.02702053263783455, acc: 0.9906396269798279)
[2025-02-13 03:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:23][root][INFO] - Training Epoch: 1/2, step 5131/7134 completed (loss: 0.06837774068117142, acc: 0.9870129823684692)
[2025-02-13 03:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:24][root][INFO] - Training Epoch: 1/2, step 5132/7134 completed (loss: 0.024096792563796043, acc: 0.9881188273429871)
[2025-02-13 03:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:24][root][INFO] - Training Epoch: 1/2, step 5133/7134 completed (loss: 0.06168774142861366, acc: 0.9798136353492737)
[2025-02-13 03:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25][root][INFO] - Training Epoch: 1/2, step 5134/7134 completed (loss: 0.02498818375170231, acc: 0.9924952983856201)
[2025-02-13 03:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25][root][INFO] - Training Epoch: 1/2, step 5135/7134 completed (loss: 0.06442933529615402, acc: 0.9871428608894348)
[2025-02-13 03:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25][root][INFO] - Training Epoch: 1/2, step 5136/7134 completed (loss: 0.03683322295546532, acc: 0.9877049326896667)
[2025-02-13 03:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:26][root][INFO] - Training Epoch: 1/2, step 5137/7134 completed (loss: 0.03319631144404411, acc: 0.9963302612304688)
[2025-02-13 03:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:26][root][INFO] - Training Epoch: 1/2, step 5138/7134 completed (loss: 0.02314729243516922, acc: 0.9937499761581421)
[2025-02-13 03:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27][root][INFO] - Training Epoch: 1/2, step 5139/7134 completed (loss: 0.041878677904605865, acc: 0.9842632412910461)
[2025-02-13 03:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27][root][INFO] - Training Epoch: 1/2, step 5140/7134 completed (loss: 0.024113576859235764, acc: 0.9957582354545593)
[2025-02-13 03:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27][root][INFO] - Training Epoch: 1/2, step 5141/7134 completed (loss: 0.059923842549324036, acc: 0.992277979850769)
[2025-02-13 03:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:28][root][INFO] - Training Epoch: 1/2, step 5142/7134 completed (loss: 0.046271663159132004, acc: 0.9870588183403015)
[2025-02-13 03:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:28][root][INFO] - Training Epoch: 1/2, step 5143/7134 completed (loss: 0.0476611852645874, acc: 0.9835025668144226)
[2025-02-13 03:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:29][root][INFO] - Training Epoch: 1/2, step 5144/7134 completed (loss: 0.02283528447151184, acc: 0.9941657185554504)
[2025-02-13 03:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:29][root][INFO] - Training Epoch: 1/2, step 5145/7134 completed (loss: 0.027790473774075508, acc: 0.9918116927146912)
[2025-02-13 03:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30][root][INFO] - Training Epoch: 1/2, step 5146/7134 completed (loss: 0.0634305402636528, acc: 0.9832776188850403)
[2025-02-13 03:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30][root][INFO] - Training Epoch: 1/2, step 5147/7134 completed (loss: 0.026715056970715523, acc: 0.994535505771637)
[2025-02-13 03:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30][root][INFO] - Training Epoch: 1/2, step 5148/7134 completed (loss: 0.09934290498495102, acc: 0.9800266027450562)
[2025-02-13 03:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:31][root][INFO] - Training Epoch: 1/2, step 5149/7134 completed (loss: 0.06377877295017242, acc: 0.9810945391654968)
[2025-02-13 03:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:31][root][INFO] - Training Epoch: 1/2, step 5150/7134 completed (loss: 0.09859472513198853, acc: 0.9726206064224243)
[2025-02-13 03:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:32][root][INFO] - Training Epoch: 1/2, step 5151/7134 completed (loss: 0.041646670550107956, acc: 0.9888613820075989)
[2025-02-13 03:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:32][root][INFO] - Training Epoch: 1/2, step 5152/7134 completed (loss: 0.03566821292042732, acc: 0.9906166195869446)
[2025-02-13 03:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33][root][INFO] - Training Epoch: 1/2, step 5153/7134 completed (loss: 0.029493074864149094, acc: 0.991769552230835)
[2025-02-13 03:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33][root][INFO] - Training Epoch: 1/2, step 5154/7134 completed (loss: 0.04695545509457588, acc: 0.9886234402656555)
[2025-02-13 03:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33][root][INFO] - Training Epoch: 1/2, step 5155/7134 completed (loss: 0.043630629777908325, acc: 0.9924699068069458)
[2025-02-13 03:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:34][root][INFO] - Training Epoch: 1/2, step 5156/7134 completed (loss: 0.025474417954683304, acc: 0.991062581539154)
[2025-02-13 03:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:34][root][INFO] - Training Epoch: 1/2, step 5157/7134 completed (loss: 0.09356046468019485, acc: 0.9706601500511169)
[2025-02-13 03:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:35][root][INFO] - Training Epoch: 1/2, step 5158/7134 completed (loss: 0.08651795983314514, acc: 0.9777448177337646)
[2025-02-13 03:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:35][root][INFO] - Training Epoch: 1/2, step 5159/7134 completed (loss: 0.01903166063129902, acc: 0.9934980273246765)
[2025-02-13 03:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36][root][INFO] - Training Epoch: 1/2, step 5160/7134 completed (loss: 0.06599634140729904, acc: 0.9783861637115479)
[2025-02-13 03:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36][root][INFO] - Training Epoch: 1/2, step 5161/7134 completed (loss: 0.0910334587097168, acc: 0.9706293940544128)
[2025-02-13 03:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36][root][INFO] - Training Epoch: 1/2, step 5162/7134 completed (loss: 0.04342217370867729, acc: 0.9886547923088074)
[2025-02-13 03:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:37][root][INFO] - Training Epoch: 1/2, step 5163/7134 completed (loss: 0.02797645702958107, acc: 0.9929178357124329)
[2025-02-13 03:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:37][root][INFO] - Training Epoch: 1/2, step 5164/7134 completed (loss: 0.029023874551057816, acc: 0.9932773113250732)
[2025-02-13 03:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38][root][INFO] - Training Epoch: 1/2, step 5165/7134 completed (loss: 0.050345879048109055, acc: 0.9894578456878662)
[2025-02-13 03:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38][root][INFO] - Training Epoch: 1/2, step 5166/7134 completed (loss: 0.028475528582930565, acc: 0.990867555141449)
[2025-02-13 03:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39][root][INFO] - Training Epoch: 1/2, step 5167/7134 completed (loss: 0.07950670272111893, acc: 0.9679487347602844)
[2025-02-13 03:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39][root][INFO] - Training Epoch: 1/2, step 5168/7134 completed (loss: 0.05320032313466072, acc: 0.9829192757606506)
[2025-02-13 03:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39][root][INFO] - Training Epoch: 1/2, step 5169/7134 completed (loss: 0.08345454186201096, acc: 0.9750778675079346)
[2025-02-13 03:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:40][root][INFO] - Training Epoch: 1/2, step 5170/7134 completed (loss: 0.051223840564489365, acc: 0.9841059446334839)
[2025-02-13 03:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:40][root][INFO] - Training Epoch: 1/2, step 5171/7134 completed (loss: 0.049465786665678024, acc: 0.9829059839248657)
[2025-02-13 03:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41][root][INFO] - Training Epoch: 1/2, step 5172/7134 completed (loss: 0.09852622449398041, acc: 0.9761499166488647)
[2025-02-13 03:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41][root][INFO] - Training Epoch: 1/2, step 5173/7134 completed (loss: 0.048529595136642456, acc: 0.9858267903327942)
[2025-02-13 03:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41][root][INFO] - Training Epoch: 1/2, step 5174/7134 completed (loss: 0.01925254985690117, acc: 0.9961685538291931)
[2025-02-13 03:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:42][root][INFO] - Training Epoch: 1/2, step 5175/7134 completed (loss: 0.022742966189980507, acc: 0.9944751262664795)
[2025-02-13 03:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:42][root][INFO] - Training Epoch: 1/2, step 5176/7134 completed (loss: 0.019439492374658585, acc: 0.9908952713012695)
[2025-02-13 03:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:43][root][INFO] - Training Epoch: 1/2, step 5177/7134 completed (loss: 0.09278633445501328, acc: 0.9669564962387085)
[2025-02-13 03:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:43][root][INFO] - Training Epoch: 1/2, step 5178/7134 completed (loss: 0.14008717238903046, acc: 0.9686411023139954)
[2025-02-13 03:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44][root][INFO] - Training Epoch: 1/2, step 5179/7134 completed (loss: 0.03062834031879902, acc: 0.9881656765937805)
[2025-02-13 03:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44][root][INFO] - Training Epoch: 1/2, step 5180/7134 completed (loss: 0.07017725706100464, acc: 0.9756097793579102)
[2025-02-13 03:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44][root][INFO] - Training Epoch: 1/2, step 5181/7134 completed (loss: 0.09458782523870468, acc: 0.9760638475418091)
[2025-02-13 03:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45][root][INFO] - Training Epoch: 1/2, step 5182/7134 completed (loss: 0.16895900666713715, acc: 0.9520833492279053)
[2025-02-13 03:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45][root][INFO] - Training Epoch: 1/2, step 5183/7134 completed (loss: 0.06524574756622314, acc: 0.9834983348846436)
[2025-02-13 03:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45][root][INFO] - Training Epoch: 1/2, step 5184/7134 completed (loss: 0.09372851252555847, acc: 0.9779950976371765)
[2025-02-13 03:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:46][root][INFO] - Training Epoch: 1/2, step 5185/7134 completed (loss: 0.051878657191991806, acc: 0.9903614521026611)
[2025-02-13 03:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:46][root][INFO] - Training Epoch: 1/2, step 5186/7134 completed (loss: 0.08869780600070953, acc: 0.981670081615448)
[2025-02-13 03:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47][root][INFO] - Training Epoch: 1/2, step 5187/7134 completed (loss: 0.021518291905522346, acc: 0.9942857027053833)
[2025-02-13 03:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47][root][INFO] - Training Epoch: 1/2, step 5188/7134 completed (loss: 0.2060764878988266, acc: 0.9490908980369568)
[2025-02-13 03:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47][root][INFO] - Training Epoch: 1/2, step 5189/7134 completed (loss: 0.1637423038482666, acc: 0.9591836929321289)
[2025-02-13 03:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:48][root][INFO] - Training Epoch: 1/2, step 5190/7134 completed (loss: 0.15265385806560516, acc: 0.9643705487251282)
[2025-02-13 03:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:48][root][INFO] - Training Epoch: 1/2, step 5191/7134 completed (loss: 0.1343957483768463, acc: 0.9604685306549072)
[2025-02-13 03:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49][root][INFO] - Training Epoch: 1/2, step 5192/7134 completed (loss: 0.08563463389873505, acc: 0.9881094098091125)
[2025-02-13 03:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49][root][INFO] - Training Epoch: 1/2, step 5193/7134 completed (loss: 0.06126916781067848, acc: 0.9803149700164795)
[2025-02-13 03:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49][root][INFO] - Training Epoch: 1/2, step 5194/7134 completed (loss: 0.13283610343933105, acc: 0.9564356207847595)
[2025-02-13 03:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50][root][INFO] - Training Epoch: 1/2, step 5195/7134 completed (loss: 0.07355091720819473, acc: 0.984649121761322)
[2025-02-13 03:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50][root][INFO] - Training Epoch: 1/2, step 5196/7134 completed (loss: 0.11064345389604568, acc: 0.9688995480537415)
[2025-02-13 03:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50][root][INFO] - Training Epoch: 1/2, step 5197/7134 completed (loss: 0.046502240002155304, acc: 0.9855967164039612)
[2025-02-13 03:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:51][root][INFO] - Training Epoch: 1/2, step 5198/7134 completed (loss: 0.06702852994203568, acc: 0.9792099595069885)
[2025-02-13 03:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:51][root][INFO] - Training Epoch: 1/2, step 5199/7134 completed (loss: 0.1531665325164795, acc: 0.9607558250427246)
[2025-02-13 03:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:52][root][INFO] - Training Epoch: 1/2, step 5200/7134 completed (loss: 0.07180201262235641, acc: 0.9764243364334106)
[2025-02-13 03:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:52][root][INFO] - Training Epoch: 1/2, step 5201/7134 completed (loss: 0.08473163098096848, acc: 0.9770808219909668)
[2025-02-13 03:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53][root][INFO] - Training Epoch: 1/2, step 5202/7134 completed (loss: 0.09330102801322937, acc: 0.9752747416496277)
[2025-02-13 03:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53][root][INFO] - Training Epoch: 1/2, step 5203/7134 completed (loss: 0.09296637028455734, acc: 0.9768707752227783)
[2025-02-13 03:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53][root][INFO] - Training Epoch: 1/2, step 5204/7134 completed (loss: 0.052017029374837875, acc: 0.983505129814148)
[2025-02-13 03:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54][root][INFO] - Training Epoch: 1/2, step 5205/7134 completed (loss: 0.0684298574924469, acc: 0.9737827777862549)
[2025-02-13 03:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54][root][INFO] - Training Epoch: 1/2, step 5206/7134 completed (loss: 0.07891305536031723, acc: 0.9807383418083191)
[2025-02-13 03:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54][root][INFO] - Training Epoch: 1/2, step 5207/7134 completed (loss: 0.1206442341208458, acc: 0.9758551120758057)
[2025-02-13 03:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55][root][INFO] - Training Epoch: 1/2, step 5208/7134 completed (loss: 0.04310743883252144, acc: 0.9881266355514526)
[2025-02-13 03:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55][root][INFO] - Training Epoch: 1/2, step 5209/7134 completed (loss: 0.05720585212111473, acc: 0.9839357137680054)
[2025-02-13 03:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:56][root][INFO] - Training Epoch: 1/2, step 5210/7134 completed (loss: 0.03990863636136055, acc: 0.9893190860748291)
[2025-02-13 03:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:56][root][INFO] - Training Epoch: 1/2, step 5211/7134 completed (loss: 0.06004977226257324, acc: 0.9791666865348816)
[2025-02-13 03:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57][root][INFO] - Training Epoch: 1/2, step 5212/7134 completed (loss: 0.09215439856052399, acc: 0.9624573588371277)
[2025-02-13 03:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57][root][INFO] - Training Epoch: 1/2, step 5213/7134 completed (loss: 0.06568817794322968, acc: 0.9820144176483154)
[2025-02-13 03:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57][root][INFO] - Training Epoch: 1/2, step 5214/7134 completed (loss: 0.1392015814781189, acc: 0.959227442741394)
[2025-02-13 03:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58][root][INFO] - Training Epoch: 1/2, step 5215/7134 completed (loss: 0.10247857123613358, acc: 0.9744572043418884)
[2025-02-13 03:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58][root][INFO] - Training Epoch: 1/2, step 5216/7134 completed (loss: 0.10890233516693115, acc: 0.9695817232131958)
[2025-02-13 03:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58][root][INFO] - Training Epoch: 1/2, step 5217/7134 completed (loss: 0.0819530189037323, acc: 0.9817073345184326)
[2025-02-13 03:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59][root][INFO] - Training Epoch: 1/2, step 5218/7134 completed (loss: 0.032507579773664474, acc: 0.9887640476226807)
[2025-02-13 03:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59][root][INFO] - Training Epoch: 1/2, step 5219/7134 completed (loss: 0.06292469799518585, acc: 0.9878934621810913)
[2025-02-13 03:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59][root][INFO] - Training Epoch: 1/2, step 5220/7134 completed (loss: 0.10700830817222595, acc: 0.971377432346344)
[2025-02-13 03:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00][root][INFO] - Training Epoch: 1/2, step 5221/7134 completed (loss: 0.08630296587944031, acc: 0.9750778675079346)
[2025-02-13 03:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00][root][INFO] - Training Epoch: 1/2, step 5222/7134 completed (loss: 0.07784688472747803, acc: 0.9813084006309509)
[2025-02-13 03:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00][root][INFO] - Training Epoch: 1/2, step 5223/7134 completed (loss: 0.041387125849723816, acc: 0.9907833933830261)
[2025-02-13 03:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:01][root][INFO] - Training Epoch: 1/2, step 5224/7134 completed (loss: 0.0759059488773346, acc: 0.9789103865623474)
[2025-02-13 03:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:01][root][INFO] - Training Epoch: 1/2, step 5225/7134 completed (loss: 0.07838793098926544, acc: 0.9822294116020203)
[2025-02-13 03:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:02][root][INFO] - Training Epoch: 1/2, step 5226/7134 completed (loss: 0.058302707970142365, acc: 0.9818781018257141)
[2025-02-13 03:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:02][root][INFO] - Training Epoch: 1/2, step 5227/7134 completed (loss: 0.050813887268304825, acc: 0.9896103739738464)
[2025-02-13 03:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03][root][INFO] - Training Epoch: 1/2, step 5228/7134 completed (loss: 0.04498513042926788, acc: 0.9883720874786377)
[2025-02-13 03:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03][root][INFO] - Training Epoch: 1/2, step 5229/7134 completed (loss: 0.03787296265363693, acc: 0.9851411581039429)
[2025-02-13 03:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03][root][INFO] - Training Epoch: 1/2, step 5230/7134 completed (loss: 0.03907718136906624, acc: 0.9892183542251587)
[2025-02-13 03:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:04][root][INFO] - Training Epoch: 1/2, step 5231/7134 completed (loss: 0.04086776450276375, acc: 0.9886075854301453)
[2025-02-13 03:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:04][root][INFO] - Training Epoch: 1/2, step 5232/7134 completed (loss: 0.03823469951748848, acc: 0.9895366430282593)
[2025-02-13 03:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:05][root][INFO] - Training Epoch: 1/2, step 5233/7134 completed (loss: 0.03962622582912445, acc: 0.9909909963607788)
[2025-02-13 03:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:05][root][INFO] - Training Epoch: 1/2, step 5234/7134 completed (loss: 0.04566855728626251, acc: 0.990208089351654)
[2025-02-13 03:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06][root][INFO] - Training Epoch: 1/2, step 5235/7134 completed (loss: 0.03304144740104675, acc: 0.9882352948188782)
[2025-02-13 03:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06][root][INFO] - Training Epoch: 1/2, step 5236/7134 completed (loss: 0.02370111271739006, acc: 0.9930555820465088)
[2025-02-13 03:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06][root][INFO] - Training Epoch: 1/2, step 5237/7134 completed (loss: 0.011194052174687386, acc: 0.9982078671455383)
[2025-02-13 03:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:07][root][INFO] - Training Epoch: 1/2, step 5238/7134 completed (loss: 0.018278542906045914, acc: 0.9929577708244324)
[2025-02-13 03:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:07][root][INFO] - Training Epoch: 1/2, step 5239/7134 completed (loss: 0.03084396943449974, acc: 0.992277979850769)
[2025-02-13 03:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08][root][INFO] - Training Epoch: 1/2, step 5240/7134 completed (loss: 0.032426755875349045, acc: 0.9903181195259094)
[2025-02-13 03:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08][root][INFO] - Training Epoch: 1/2, step 5241/7134 completed (loss: 0.06242797523736954, acc: 0.9858934283256531)
[2025-02-13 03:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08][root][INFO] - Training Epoch: 1/2, step 5242/7134 completed (loss: 0.03037862293422222, acc: 0.9933884143829346)
[2025-02-13 03:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:09][root][INFO] - Training Epoch: 1/2, step 5243/7134 completed (loss: 0.04304252937436104, acc: 0.9913793206214905)
[2025-02-13 03:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:09][root][INFO] - Training Epoch: 1/2, step 5244/7134 completed (loss: 0.03371405228972435, acc: 0.9917241334915161)
[2025-02-13 03:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:10][root][INFO] - Training Epoch: 1/2, step 5245/7134 completed (loss: 0.030381597578525543, acc: 0.9895287752151489)
[2025-02-13 03:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:10][root][INFO] - Training Epoch: 1/2, step 5246/7134 completed (loss: 0.04184461385011673, acc: 0.987730085849762)
[2025-02-13 03:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11][root][INFO] - Training Epoch: 1/2, step 5247/7134 completed (loss: 0.02428097277879715, acc: 0.9925925731658936)
[2025-02-13 03:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11][root][INFO] - Training Epoch: 1/2, step 5248/7134 completed (loss: 0.07520163804292679, acc: 0.9794007539749146)
[2025-02-13 03:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11][root][INFO] - Training Epoch: 1/2, step 5249/7134 completed (loss: 0.025298738852143288, acc: 0.9874213933944702)
[2025-02-13 03:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:12][root][INFO] - Training Epoch: 1/2, step 5250/7134 completed (loss: 0.02841389738023281, acc: 0.9924337863922119)
[2025-02-13 03:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:12][root][INFO] - Training Epoch: 1/2, step 5251/7134 completed (loss: 0.029788464307785034, acc: 0.9890909194946289)
[2025-02-13 03:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13][root][INFO] - Training Epoch: 1/2, step 5252/7134 completed (loss: 0.059804823249578476, acc: 0.9836448431015015)
[2025-02-13 03:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13][root][INFO] - Training Epoch: 1/2, step 5253/7134 completed (loss: 0.017133096233010292, acc: 0.9950980544090271)
[2025-02-13 03:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13][root][INFO] - Training Epoch: 1/2, step 5254/7134 completed (loss: 0.05850980058312416, acc: 0.9813242554664612)
[2025-02-13 03:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:14][root][INFO] - Training Epoch: 1/2, step 5255/7134 completed (loss: 0.05285549908876419, acc: 0.9863013625144958)
[2025-02-13 03:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:14][root][INFO] - Training Epoch: 1/2, step 5256/7134 completed (loss: 0.0791516825556755, acc: 0.984544038772583)
[2025-02-13 03:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:15][root][INFO] - Training Epoch: 1/2, step 5257/7134 completed (loss: 0.06285418570041656, acc: 0.9847792983055115)
[2025-02-13 03:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:15][root][INFO] - Training Epoch: 1/2, step 5258/7134 completed (loss: 0.052312035113573074, acc: 0.9885203838348389)
[2025-02-13 03:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16][root][INFO] - Training Epoch: 1/2, step 5259/7134 completed (loss: 0.07909497618675232, acc: 0.9841772317886353)
[2025-02-13 03:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16][root][INFO] - Training Epoch: 1/2, step 5260/7134 completed (loss: 0.03008229099214077, acc: 0.9902439117431641)
[2025-02-13 03:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16][root][INFO] - Training Epoch: 1/2, step 5261/7134 completed (loss: 0.0790930837392807, acc: 0.9769230484962463)
[2025-02-13 03:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:17][root][INFO] - Training Epoch: 1/2, step 5262/7134 completed (loss: 0.039894457906484604, acc: 0.9875862002372742)
[2025-02-13 03:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:17][root][INFO] - Training Epoch: 1/2, step 5263/7134 completed (loss: 0.054368868470191956, acc: 0.9877192974090576)
[2025-02-13 03:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:18][root][INFO] - Training Epoch: 1/2, step 5264/7134 completed (loss: 0.03294786065816879, acc: 0.9878493547439575)
[2025-02-13 03:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:18][root][INFO] - Training Epoch: 1/2, step 5265/7134 completed (loss: 0.04916146770119667, acc: 0.9885641932487488)
[2025-02-13 03:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19][root][INFO] - Training Epoch: 1/2, step 5266/7134 completed (loss: 0.04418601095676422, acc: 0.9849315285682678)
[2025-02-13 03:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19][root][INFO] - Training Epoch: 1/2, step 5267/7134 completed (loss: 0.08683977276086807, acc: 0.9720062017440796)
[2025-02-13 03:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19][root][INFO] - Training Epoch: 1/2, step 5268/7134 completed (loss: 0.08658981323242188, acc: 0.9692898392677307)
[2025-02-13 03:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:20][root][INFO] - Training Epoch: 1/2, step 5269/7134 completed (loss: 0.09682849794626236, acc: 0.971377432346344)
[2025-02-13 03:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:20][root][INFO] - Training Epoch: 1/2, step 5270/7134 completed (loss: 0.040767163038253784, acc: 0.9866310358047485)
[2025-02-13 03:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21][root][INFO] - Training Epoch: 1/2, step 5271/7134 completed (loss: 0.09747002273797989, acc: 0.9805996417999268)
[2025-02-13 03:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21][root][INFO] - Training Epoch: 1/2, step 5272/7134 completed (loss: 0.03528205305337906, acc: 0.9897360801696777)
[2025-02-13 03:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21][root][INFO] - Training Epoch: 1/2, step 5273/7134 completed (loss: 0.02832620032131672, acc: 0.9949066042900085)
[2025-02-13 03:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:22][root][INFO] - Training Epoch: 1/2, step 5274/7134 completed (loss: 0.07658360153436661, acc: 0.9731343388557434)
[2025-02-13 03:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:22][root][INFO] - Training Epoch: 1/2, step 5275/7134 completed (loss: 0.052686482667922974, acc: 0.9890310764312744)
[2025-02-13 03:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:23][root][INFO] - Training Epoch: 1/2, step 5276/7134 completed (loss: 0.048408765345811844, acc: 0.980567991733551)
[2025-02-13 03:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:23][root][INFO] - Training Epoch: 1/2, step 5277/7134 completed (loss: 0.041486576199531555, acc: 0.9841498732566833)
[2025-02-13 03:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24][root][INFO] - Training Epoch: 1/2, step 5278/7134 completed (loss: 0.07877462357282639, acc: 0.9801223278045654)
[2025-02-13 03:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24][root][INFO] - Training Epoch: 1/2, step 5279/7134 completed (loss: 0.03167137876152992, acc: 0.9922178983688354)
[2025-02-13 03:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24][root][INFO] - Training Epoch: 1/2, step 5280/7134 completed (loss: 0.08827922493219376, acc: 0.9800994992256165)
[2025-02-13 03:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:25][root][INFO] - Training Epoch: 1/2, step 5281/7134 completed (loss: 0.02808503620326519, acc: 0.9916805028915405)
[2025-02-13 03:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:25][root][INFO] - Training Epoch: 1/2, step 5282/7134 completed (loss: 0.04412456601858139, acc: 0.9871612191200256)
[2025-02-13 03:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26][root][INFO] - Training Epoch: 1/2, step 5283/7134 completed (loss: 0.03928540274500847, acc: 0.9917920827865601)
[2025-02-13 03:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26][root][INFO] - Training Epoch: 1/2, step 5284/7134 completed (loss: 0.03617784380912781, acc: 0.9930843710899353)
[2025-02-13 03:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26][root][INFO] - Training Epoch: 1/2, step 5285/7134 completed (loss: 0.04486817121505737, acc: 0.9893491268157959)
[2025-02-13 03:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:27][root][INFO] - Training Epoch: 1/2, step 5286/7134 completed (loss: 0.04923953860998154, acc: 0.9867647290229797)
[2025-02-13 03:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:27][root][INFO] - Training Epoch: 1/2, step 5287/7134 completed (loss: 0.08897189050912857, acc: 0.9756097793579102)
[2025-02-13 03:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:28][root][INFO] - Training Epoch: 1/2, step 5288/7134 completed (loss: 0.03251528739929199, acc: 0.9861963391304016)
[2025-02-13 03:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:28][root][INFO] - Training Epoch: 1/2, step 5289/7134 completed (loss: 0.04296010360121727, acc: 0.9815863966941833)
[2025-02-13 03:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29][root][INFO] - Training Epoch: 1/2, step 5290/7134 completed (loss: 0.052270274609327316, acc: 0.9797468185424805)
[2025-02-13 03:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29][root][INFO] - Training Epoch: 1/2, step 5291/7134 completed (loss: 0.03280755877494812, acc: 0.9927272796630859)
[2025-02-13 03:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29][root][INFO] - Training Epoch: 1/2, step 5292/7134 completed (loss: 0.025894178077578545, acc: 0.9923664331436157)
[2025-02-13 03:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:30][root][INFO] - Training Epoch: 1/2, step 5293/7134 completed (loss: 0.04811243340373039, acc: 0.9839416146278381)
[2025-02-13 03:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:30][root][INFO] - Training Epoch: 1/2, step 5294/7134 completed (loss: 0.040741026401519775, acc: 0.9902371168136597)
[2025-02-13 03:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:31][root][INFO] - Training Epoch: 1/2, step 5295/7134 completed (loss: 0.02933337725698948, acc: 0.9926470518112183)
[2025-02-13 03:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:31][root][INFO] - Training Epoch: 1/2, step 5296/7134 completed (loss: 0.059811823070049286, acc: 0.9819193482398987)
[2025-02-13 03:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32][root][INFO] - Training Epoch: 1/2, step 5297/7134 completed (loss: 0.041153643280267715, acc: 0.9841954112052917)
[2025-02-13 03:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32][root][INFO] - Training Epoch: 1/2, step 5298/7134 completed (loss: 0.042575180530548096, acc: 0.9829059839248657)
[2025-02-13 03:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32][root][INFO] - Training Epoch: 1/2, step 5299/7134 completed (loss: 0.03703804314136505, acc: 0.9887640476226807)
[2025-02-13 03:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:33][root][INFO] - Training Epoch: 1/2, step 5300/7134 completed (loss: 0.047677818685770035, acc: 0.988252580165863)
[2025-02-13 03:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:33][root][INFO] - Training Epoch: 1/2, step 5301/7134 completed (loss: 0.040640030056238174, acc: 0.9838472604751587)
[2025-02-13 03:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:34][root][INFO] - Training Epoch: 1/2, step 5302/7134 completed (loss: 0.07832395285367966, acc: 0.978723406791687)
[2025-02-13 03:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:34][root][INFO] - Training Epoch: 1/2, step 5303/7134 completed (loss: 0.07792491465806961, acc: 0.982300877571106)
[2025-02-13 03:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35][root][INFO] - Training Epoch: 1/2, step 5304/7134 completed (loss: 0.08979900926351547, acc: 0.9783693552017212)
[2025-02-13 03:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35][root][INFO] - Training Epoch: 1/2, step 5305/7134 completed (loss: 0.08266837149858475, acc: 0.9751824736595154)
[2025-02-13 03:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35][root][INFO] - Training Epoch: 1/2, step 5306/7134 completed (loss: 0.04859625920653343, acc: 0.987500011920929)
[2025-02-13 03:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:36][root][INFO] - Training Epoch: 1/2, step 5307/7134 completed (loss: 0.05986476317048073, acc: 0.9876543283462524)
[2025-02-13 03:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:36][root][INFO] - Training Epoch: 1/2, step 5308/7134 completed (loss: 0.06952600926160812, acc: 0.9833837151527405)
[2025-02-13 03:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37][root][INFO] - Training Epoch: 1/2, step 5309/7134 completed (loss: 0.12639759480953217, acc: 0.969588577747345)
[2025-02-13 03:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37][root][INFO] - Training Epoch: 1/2, step 5310/7134 completed (loss: 0.08651559054851532, acc: 0.9776847958564758)
[2025-02-13 03:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37][root][INFO] - Training Epoch: 1/2, step 5311/7134 completed (loss: 0.06989631056785583, acc: 0.9855538010597229)
[2025-02-13 03:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:38][root][INFO] - Training Epoch: 1/2, step 5312/7134 completed (loss: 0.03491605073213577, acc: 0.9887429475784302)
[2025-02-13 03:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:38][root][INFO] - Training Epoch: 1/2, step 5313/7134 completed (loss: 0.04845486208796501, acc: 0.9836065769195557)
[2025-02-13 03:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39][root][INFO] - Training Epoch: 1/2, step 5314/7134 completed (loss: 0.04551371932029724, acc: 0.9843205809593201)
[2025-02-13 03:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39][root][INFO] - Training Epoch: 1/2, step 5315/7134 completed (loss: 0.03159841522574425, acc: 0.9918166995048523)
[2025-02-13 03:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39][root][INFO] - Training Epoch: 1/2, step 5316/7134 completed (loss: 0.05080735683441162, acc: 0.9882352948188782)
[2025-02-13 03:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:40][root][INFO] - Training Epoch: 1/2, step 5317/7134 completed (loss: 0.03209155052900314, acc: 0.9892473220825195)
[2025-02-13 03:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:40][root][INFO] - Training Epoch: 1/2, step 5318/7134 completed (loss: 0.02554086223244667, acc: 0.9879102110862732)
[2025-02-13 03:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41][root][INFO] - Training Epoch: 1/2, step 5319/7134 completed (loss: 0.07341174781322479, acc: 0.9835766553878784)
[2025-02-13 03:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41][root][INFO] - Training Epoch: 1/2, step 5320/7134 completed (loss: 0.04128293693065643, acc: 0.989130437374115)
[2025-02-13 03:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41][root][INFO] - Training Epoch: 1/2, step 5321/7134 completed (loss: 0.037310902029275894, acc: 0.9860464930534363)
[2025-02-13 03:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:42][root][INFO] - Training Epoch: 1/2, step 5322/7134 completed (loss: 0.02464469149708748, acc: 0.9924585223197937)
[2025-02-13 03:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:42][root][INFO] - Training Epoch: 1/2, step 5323/7134 completed (loss: 0.039158593863248825, acc: 0.9909090995788574)
[2025-02-13 03:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43][root][INFO] - Training Epoch: 1/2, step 5324/7134 completed (loss: 0.02635372057557106, acc: 0.9938176274299622)
[2025-02-13 03:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43][root][INFO] - Training Epoch: 1/2, step 5325/7134 completed (loss: 0.03062460385262966, acc: 0.991482138633728)
[2025-02-13 03:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43][root][INFO] - Training Epoch: 1/2, step 5326/7134 completed (loss: 0.038921382278203964, acc: 0.9887096881866455)
[2025-02-13 03:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:44][root][INFO] - Training Epoch: 1/2, step 5327/7134 completed (loss: 0.0452539287507534, acc: 0.9898403286933899)
[2025-02-13 03:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:44][root][INFO] - Training Epoch: 1/2, step 5328/7134 completed (loss: 0.027855023741722107, acc: 0.9926605224609375)
[2025-02-13 03:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45][root][INFO] - Training Epoch: 1/2, step 5329/7134 completed (loss: 0.058162350207567215, acc: 0.9854133129119873)
[2025-02-13 03:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45][root][INFO] - Training Epoch: 1/2, step 5330/7134 completed (loss: 0.055573970079422, acc: 0.981249988079071)
[2025-02-13 03:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45][root][INFO] - Training Epoch: 1/2, step 5331/7134 completed (loss: 0.05490001291036606, acc: 0.9877408146858215)
[2025-02-13 03:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:46][root][INFO] - Training Epoch: 1/2, step 5332/7134 completed (loss: 0.039225246757268906, acc: 0.9902439117431641)
[2025-02-13 03:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:46][root][INFO] - Training Epoch: 1/2, step 5333/7134 completed (loss: 0.023915573954582214, acc: 0.9890965819358826)
[2025-02-13 03:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47][root][INFO] - Training Epoch: 1/2, step 5334/7134 completed (loss: 0.0909116342663765, acc: 0.9831546545028687)
[2025-02-13 03:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47][root][INFO] - Training Epoch: 1/2, step 5335/7134 completed (loss: 0.04599366337060928, acc: 0.9888178706169128)
[2025-02-13 03:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47][root][INFO] - Training Epoch: 1/2, step 5336/7134 completed (loss: 0.042860426008701324, acc: 0.9906250238418579)
[2025-02-13 03:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:48][root][INFO] - Training Epoch: 1/2, step 5337/7134 completed (loss: 0.0545516312122345, acc: 0.9799554347991943)
[2025-02-13 03:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:48][root][INFO] - Training Epoch: 1/2, step 5338/7134 completed (loss: 0.03383174166083336, acc: 0.9897260069847107)
[2025-02-13 03:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:49][root][INFO] - Training Epoch: 1/2, step 5339/7134 completed (loss: 0.060298919677734375, acc: 0.9831606149673462)
[2025-02-13 03:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:49][root][INFO] - Training Epoch: 1/2, step 5340/7134 completed (loss: 0.07786841690540314, acc: 0.9774696826934814)
[2025-02-13 03:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:50][root][INFO] - Training Epoch: 1/2, step 5341/7134 completed (loss: 0.022734778001904488, acc: 0.994350254535675)
[2025-02-13 03:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:50][root][INFO] - Training Epoch: 1/2, step 5342/7134 completed (loss: 0.07193952053785324, acc: 0.9773635268211365)
[2025-02-13 03:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51][root][INFO] - Training Epoch: 1/2, step 5343/7134 completed (loss: 0.06003258004784584, acc: 0.9828495979309082)
[2025-02-13 03:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51][root][INFO] - Training Epoch: 1/2, step 5344/7134 completed (loss: 0.049310680478811264, acc: 0.9831578731536865)
[2025-02-13 03:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51][root][INFO] - Training Epoch: 1/2, step 5345/7134 completed (loss: 0.042640168219804764, acc: 0.9896551966667175)
[2025-02-13 03:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:52][root][INFO] - Training Epoch: 1/2, step 5346/7134 completed (loss: 0.05333290621638298, acc: 0.9874081611633301)
[2025-02-13 03:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:52][root][INFO] - Training Epoch: 1/2, step 5347/7134 completed (loss: 0.03382708877325058, acc: 0.988063633441925)
[2025-02-13 03:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:53][root][INFO] - Training Epoch: 1/2, step 5348/7134 completed (loss: 0.04682745411992073, acc: 0.9847645163536072)
[2025-02-13 03:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:54][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0617, device='cuda:0') eval_epoch_loss=tensor(0.0598, device='cuda:0') eval_epoch_acc=tensor(0.9835, device='cuda:0')
[2025-02-13 03:13:54][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:13:54][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:13:54][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_5349_loss_0.05983130633831024/model.pt
[2025-02-13 03:13:54][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:13:54][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.05983130633831024
[2025-02-13 03:13:54][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9834863543510437
[2025-02-13 03:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:55][root][INFO] - Training Epoch: 1/2, step 5349/7134 completed (loss: 0.06833534687757492, acc: 0.9819121360778809)
[2025-02-13 03:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:55][root][INFO] - Training Epoch: 1/2, step 5350/7134 completed (loss: 0.06135551631450653, acc: 0.9828269481658936)
[2025-02-13 03:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56][root][INFO] - Training Epoch: 1/2, step 5351/7134 completed (loss: 0.05789721757173538, acc: 0.981389582157135)
[2025-02-13 03:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56][root][INFO] - Training Epoch: 1/2, step 5352/7134 completed (loss: 0.04182356223464012, acc: 0.9870634078979492)
[2025-02-13 03:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57][root][INFO] - Training Epoch: 1/2, step 5353/7134 completed (loss: 0.019985675811767578, acc: 0.9954904317855835)
[2025-02-13 03:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57][root][INFO] - Training Epoch: 1/2, step 5354/7134 completed (loss: 0.059323396533727646, acc: 0.9823788404464722)
[2025-02-13 03:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57][root][INFO] - Training Epoch: 1/2, step 5355/7134 completed (loss: 0.08187074959278107, acc: 0.9739952683448792)
[2025-02-13 03:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58][root][INFO] - Training Epoch: 1/2, step 5356/7134 completed (loss: 0.07467152923345566, acc: 0.9788293838500977)
[2025-02-13 03:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58][root][INFO] - Training Epoch: 1/2, step 5357/7134 completed (loss: 0.04636408016085625, acc: 0.9878048896789551)
[2025-02-13 03:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59][root][INFO] - Training Epoch: 1/2, step 5358/7134 completed (loss: 0.056774456053972244, acc: 0.9903448224067688)
[2025-02-13 03:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59][root][INFO] - Training Epoch: 1/2, step 5359/7134 completed (loss: 0.023143816739320755, acc: 0.9941725134849548)
[2025-02-13 03:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00][root][INFO] - Training Epoch: 1/2, step 5360/7134 completed (loss: 0.012672425247728825, acc: 0.9958791136741638)
[2025-02-13 03:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00][root][INFO] - Training Epoch: 1/2, step 5361/7134 completed (loss: 0.07407347112894058, acc: 0.9832335114479065)
[2025-02-13 03:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00][root][INFO] - Training Epoch: 1/2, step 5362/7134 completed (loss: 0.03657710924744606, acc: 0.9850746393203735)
[2025-02-13 03:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01][root][INFO] - Training Epoch: 1/2, step 5363/7134 completed (loss: 0.07219216227531433, acc: 0.9752907156944275)
[2025-02-13 03:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01][root][INFO] - Training Epoch: 1/2, step 5364/7134 completed (loss: 0.03246385604143143, acc: 0.9922077655792236)
[2025-02-13 03:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:02][root][INFO] - Training Epoch: 1/2, step 5365/7134 completed (loss: 0.03421295806765556, acc: 0.9876543283462524)
[2025-02-13 03:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:02][root][INFO] - Training Epoch: 1/2, step 5366/7134 completed (loss: 0.07299938052892685, acc: 0.9861111044883728)
[2025-02-13 03:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03][root][INFO] - Training Epoch: 1/2, step 5367/7134 completed (loss: 0.03981078788638115, acc: 0.9886934757232666)
[2025-02-13 03:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03][root][INFO] - Training Epoch: 1/2, step 5368/7134 completed (loss: 0.040977876633405685, acc: 0.9816053509712219)
[2025-02-13 03:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03][root][INFO] - Training Epoch: 1/2, step 5369/7134 completed (loss: 0.029348298907279968, acc: 0.9893842935562134)
[2025-02-13 03:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04][root][INFO] - Training Epoch: 1/2, step 5370/7134 completed (loss: 0.06670865416526794, acc: 0.9811617136001587)
[2025-02-13 03:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04][root][INFO] - Training Epoch: 1/2, step 5371/7134 completed (loss: 0.09084978699684143, acc: 0.9717608094215393)
[2025-02-13 03:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05][root][INFO] - Training Epoch: 1/2, step 5372/7134 completed (loss: 0.016216157004237175, acc: 0.9929873943328857)
[2025-02-13 03:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05][root][INFO] - Training Epoch: 1/2, step 5373/7134 completed (loss: 0.09579166769981384, acc: 0.9707112908363342)
[2025-02-13 03:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05][root][INFO] - Training Epoch: 1/2, step 5374/7134 completed (loss: 0.0544964037835598, acc: 0.9807074069976807)
[2025-02-13 03:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06][root][INFO] - Training Epoch: 1/2, step 5375/7134 completed (loss: 0.06599722057580948, acc: 0.9808428883552551)
[2025-02-13 03:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06][root][INFO] - Training Epoch: 1/2, step 5376/7134 completed (loss: 0.024283403530716896, acc: 0.9912891983985901)
[2025-02-13 03:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:07][root][INFO] - Training Epoch: 1/2, step 5377/7134 completed (loss: 0.05341729149222374, acc: 0.9908952713012695)
[2025-02-13 03:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:07][root][INFO] - Training Epoch: 1/2, step 5378/7134 completed (loss: 0.02026139199733734, acc: 0.9939320683479309)
[2025-02-13 03:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08][root][INFO] - Training Epoch: 1/2, step 5379/7134 completed (loss: 0.04591881111264229, acc: 0.9883268475532532)
[2025-02-13 03:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08][root][INFO] - Training Epoch: 1/2, step 5380/7134 completed (loss: 0.04644300043582916, acc: 0.9876543283462524)
[2025-02-13 03:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08][root][INFO] - Training Epoch: 1/2, step 5381/7134 completed (loss: 0.03450676053762436, acc: 0.9930264949798584)
[2025-02-13 03:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09][root][INFO] - Training Epoch: 1/2, step 5382/7134 completed (loss: 0.019138241186738014, acc: 0.9931740760803223)
[2025-02-13 03:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09][root][INFO] - Training Epoch: 1/2, step 5383/7134 completed (loss: 0.02771126851439476, acc: 0.9886363744735718)
[2025-02-13 03:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10][root][INFO] - Training Epoch: 1/2, step 5384/7134 completed (loss: 0.03210233896970749, acc: 0.9905213117599487)
[2025-02-13 03:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10][root][INFO] - Training Epoch: 1/2, step 5385/7134 completed (loss: 0.039983898401260376, acc: 0.9867899417877197)
[2025-02-13 03:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10][root][INFO] - Training Epoch: 1/2, step 5386/7134 completed (loss: 0.020204419270157814, acc: 0.9943820238113403)
[2025-02-13 03:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:11][root][INFO] - Training Epoch: 1/2, step 5387/7134 completed (loss: 0.016393564641475677, acc: 0.9980952143669128)
[2025-02-13 03:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:11][root][INFO] - Training Epoch: 1/2, step 5388/7134 completed (loss: 0.04849807173013687, acc: 0.9898989796638489)
[2025-02-13 03:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:12][root][INFO] - Training Epoch: 1/2, step 5389/7134 completed (loss: 0.027699416503310204, acc: 0.9936102032661438)
[2025-02-13 03:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:12][root][INFO] - Training Epoch: 1/2, step 5390/7134 completed (loss: 0.04535363242030144, acc: 0.9857369065284729)
[2025-02-13 03:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13][root][INFO] - Training Epoch: 1/2, step 5391/7134 completed (loss: 0.07569263130426407, acc: 0.9845626354217529)
[2025-02-13 03:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13][root][INFO] - Training Epoch: 1/2, step 5392/7134 completed (loss: 0.05611489340662956, acc: 0.9832317233085632)
[2025-02-13 03:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13][root][INFO] - Training Epoch: 1/2, step 5393/7134 completed (loss: 0.04066856577992439, acc: 0.98046875)
[2025-02-13 03:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:14][root][INFO] - Training Epoch: 1/2, step 5394/7134 completed (loss: 0.07944436371326447, acc: 0.97817462682724)
[2025-02-13 03:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:14][root][INFO] - Training Epoch: 1/2, step 5395/7134 completed (loss: 0.044900067150592804, acc: 0.9879336357116699)
[2025-02-13 03:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15][root][INFO] - Training Epoch: 1/2, step 5396/7134 completed (loss: 0.04799528419971466, acc: 0.9864661693572998)
[2025-02-13 03:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15][root][INFO] - Training Epoch: 1/2, step 5397/7134 completed (loss: 0.04716888442635536, acc: 0.9864636063575745)
[2025-02-13 03:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15][root][INFO] - Training Epoch: 1/2, step 5398/7134 completed (loss: 0.027945684269070625, acc: 0.9896755218505859)
[2025-02-13 03:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:16][root][INFO] - Training Epoch: 1/2, step 5399/7134 completed (loss: 0.03426240384578705, acc: 0.9917920827865601)
[2025-02-13 03:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:16][root][INFO] - Training Epoch: 1/2, step 5400/7134 completed (loss: 0.13593949377536774, acc: 0.9686520099639893)
[2025-02-13 03:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17][root][INFO] - Training Epoch: 1/2, step 5401/7134 completed (loss: 0.032497555017471313, acc: 0.9919742941856384)
[2025-02-13 03:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17][root][INFO] - Training Epoch: 1/2, step 5402/7134 completed (loss: 0.046266525983810425, acc: 0.9867841601371765)
[2025-02-13 03:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17][root][INFO] - Training Epoch: 1/2, step 5403/7134 completed (loss: 0.0798405259847641, acc: 0.9787928462028503)
[2025-02-13 03:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18][root][INFO] - Training Epoch: 1/2, step 5404/7134 completed (loss: 0.12264741212129593, acc: 0.9731543660163879)
[2025-02-13 03:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18][root][INFO] - Training Epoch: 1/2, step 5405/7134 completed (loss: 0.051951318979263306, acc: 0.9842932224273682)
[2025-02-13 03:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:19][root][INFO] - Training Epoch: 1/2, step 5406/7134 completed (loss: 0.043291762471199036, acc: 0.988304078578949)
[2025-02-13 03:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:19][root][INFO] - Training Epoch: 1/2, step 5407/7134 completed (loss: 0.04421553015708923, acc: 0.9830917716026306)
[2025-02-13 03:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20][root][INFO] - Training Epoch: 1/2, step 5408/7134 completed (loss: 0.09648928046226501, acc: 0.9812792539596558)
[2025-02-13 03:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20][root][INFO] - Training Epoch: 1/2, step 5409/7134 completed (loss: 0.10627356171607971, acc: 0.9803328514099121)
[2025-02-13 03:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20][root][INFO] - Training Epoch: 1/2, step 5410/7134 completed (loss: 0.06832987070083618, acc: 0.9776902794837952)
[2025-02-13 03:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21][root][INFO] - Training Epoch: 1/2, step 5411/7134 completed (loss: 0.0973084419965744, acc: 0.9709724187850952)
[2025-02-13 03:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21][root][INFO] - Training Epoch: 1/2, step 5412/7134 completed (loss: 0.09728024899959564, acc: 0.9730769395828247)
[2025-02-13 03:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:22][root][INFO] - Training Epoch: 1/2, step 5413/7134 completed (loss: 0.0940970629453659, acc: 0.9671428799629211)
[2025-02-13 03:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:22][root][INFO] - Training Epoch: 1/2, step 5414/7134 completed (loss: 0.1471567451953888, acc: 0.9601542353630066)
[2025-02-13 03:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23][root][INFO] - Training Epoch: 1/2, step 5415/7134 completed (loss: 0.11225584149360657, acc: 0.9699140191078186)
[2025-02-13 03:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23][root][INFO] - Training Epoch: 1/2, step 5416/7134 completed (loss: 0.06969722360372543, acc: 0.9779326319694519)
[2025-02-13 03:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23][root][INFO] - Training Epoch: 1/2, step 5417/7134 completed (loss: 0.04696778208017349, acc: 0.9912609457969666)
[2025-02-13 03:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24][root][INFO] - Training Epoch: 1/2, step 5418/7134 completed (loss: 0.054497599601745605, acc: 0.9778434038162231)
[2025-02-13 03:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24][root][INFO] - Training Epoch: 1/2, step 5419/7134 completed (loss: 0.12517592310905457, acc: 0.9694915413856506)
[2025-02-13 03:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:25][root][INFO] - Training Epoch: 1/2, step 5420/7134 completed (loss: 0.1285346895456314, acc: 0.9685314893722534)
[2025-02-13 03:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:25][root][INFO] - Training Epoch: 1/2, step 5421/7134 completed (loss: 0.10992652177810669, acc: 0.9672386646270752)
[2025-02-13 03:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26][root][INFO] - Training Epoch: 1/2, step 5422/7134 completed (loss: 0.06924981623888016, acc: 0.9837278127670288)
[2025-02-13 03:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26][root][INFO] - Training Epoch: 1/2, step 5423/7134 completed (loss: 0.08371312916278839, acc: 0.9824047088623047)
[2025-02-13 03:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26][root][INFO] - Training Epoch: 1/2, step 5424/7134 completed (loss: 0.0409664586186409, acc: 0.9911971688270569)
[2025-02-13 03:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27][root][INFO] - Training Epoch: 1/2, step 5425/7134 completed (loss: 0.05679326876997948, acc: 0.9883494973182678)
[2025-02-13 03:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27][root][INFO] - Training Epoch: 1/2, step 5426/7134 completed (loss: 0.07625525444746017, acc: 0.9879518151283264)
[2025-02-13 03:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27][root][INFO] - Training Epoch: 1/2, step 5427/7134 completed (loss: 0.0339956134557724, acc: 0.9873617887496948)
[2025-02-13 03:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28][root][INFO] - Training Epoch: 1/2, step 5428/7134 completed (loss: 0.05349056050181389, acc: 0.9846677780151367)
[2025-02-13 03:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28][root][INFO] - Training Epoch: 1/2, step 5429/7134 completed (loss: 0.08430367708206177, acc: 0.9801443815231323)
[2025-02-13 03:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29][root][INFO] - Training Epoch: 1/2, step 5430/7134 completed (loss: 0.03246914967894554, acc: 0.9890282154083252)
[2025-02-13 03:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29][root][INFO] - Training Epoch: 1/2, step 5431/7134 completed (loss: 0.0339280404150486, acc: 0.992438554763794)
[2025-02-13 03:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29][root][INFO] - Training Epoch: 1/2, step 5432/7134 completed (loss: 0.031801071017980576, acc: 0.9905837774276733)
[2025-02-13 03:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30][root][INFO] - Training Epoch: 1/2, step 5433/7134 completed (loss: 0.048019926995038986, acc: 0.991919219493866)
[2025-02-13 03:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30][root][INFO] - Training Epoch: 1/2, step 5434/7134 completed (loss: 0.0417078360915184, acc: 0.9899396300315857)
[2025-02-13 03:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30][root][INFO] - Training Epoch: 1/2, step 5435/7134 completed (loss: 0.031966954469680786, acc: 0.991631805896759)
[2025-02-13 03:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:31][root][INFO] - Training Epoch: 1/2, step 5436/7134 completed (loss: 0.034993670880794525, acc: 0.9925816059112549)
[2025-02-13 03:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:31][root][INFO] - Training Epoch: 1/2, step 5437/7134 completed (loss: 0.013461358845233917, acc: 0.9965096116065979)
[2025-02-13 03:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:32][root][INFO] - Training Epoch: 1/2, step 5438/7134 completed (loss: 0.02869165875017643, acc: 0.9909560680389404)
[2025-02-13 03:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:32][root][INFO] - Training Epoch: 1/2, step 5439/7134 completed (loss: 0.05657519772648811, acc: 0.9825581312179565)
[2025-02-13 03:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33][root][INFO] - Training Epoch: 1/2, step 5440/7134 completed (loss: 0.06670626252889633, acc: 0.9821200370788574)
[2025-02-13 03:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33][root][INFO] - Training Epoch: 1/2, step 5441/7134 completed (loss: 0.04296652227640152, acc: 0.990208089351654)
[2025-02-13 03:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33][root][INFO] - Training Epoch: 1/2, step 5442/7134 completed (loss: 0.025064684450626373, acc: 0.9945873022079468)
[2025-02-13 03:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34][root][INFO] - Training Epoch: 1/2, step 5443/7134 completed (loss: 0.042578015476465225, acc: 0.9857954382896423)
[2025-02-13 03:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34][root][INFO] - Training Epoch: 1/2, step 5444/7134 completed (loss: 0.039172232151031494, acc: 0.985401451587677)
[2025-02-13 03:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35][root][INFO] - Training Epoch: 1/2, step 5445/7134 completed (loss: 0.05817578360438347, acc: 0.9853747487068176)
[2025-02-13 03:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35][root][INFO] - Training Epoch: 1/2, step 5446/7134 completed (loss: 0.027557896450161934, acc: 0.994397759437561)
[2025-02-13 03:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35][root][INFO] - Training Epoch: 1/2, step 5447/7134 completed (loss: 0.0318492092192173, acc: 0.990338146686554)
[2025-02-13 03:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36][root][INFO] - Training Epoch: 1/2, step 5448/7134 completed (loss: 0.03945550322532654, acc: 0.9860334992408752)
[2025-02-13 03:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36][root][INFO] - Training Epoch: 1/2, step 5449/7134 completed (loss: 0.018372971564531326, acc: 0.9929824471473694)
[2025-02-13 03:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:37][root][INFO] - Training Epoch: 1/2, step 5450/7134 completed (loss: 0.023069174960255623, acc: 0.992277979850769)
[2025-02-13 03:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:37][root][INFO] - Training Epoch: 1/2, step 5451/7134 completed (loss: 0.046478744596242905, acc: 0.9882044792175293)
[2025-02-13 03:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38][root][INFO] - Training Epoch: 1/2, step 5452/7134 completed (loss: 0.06243627518415451, acc: 0.9844961166381836)
[2025-02-13 03:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38][root][INFO] - Training Epoch: 1/2, step 5453/7134 completed (loss: 0.05556776374578476, acc: 0.9865030646324158)
[2025-02-13 03:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38][root][INFO] - Training Epoch: 1/2, step 5454/7134 completed (loss: 0.037705082446336746, acc: 0.9882659912109375)
[2025-02-13 03:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:39][root][INFO] - Training Epoch: 1/2, step 5455/7134 completed (loss: 0.10104551911354065, acc: 0.9760100841522217)
[2025-02-13 03:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:39][root][INFO] - Training Epoch: 1/2, step 5456/7134 completed (loss: 0.04895509034395218, acc: 0.986146092414856)
[2025-02-13 03:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40][root][INFO] - Training Epoch: 1/2, step 5457/7134 completed (loss: 0.06958891451358795, acc: 0.9798657894134521)
[2025-02-13 03:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40][root][INFO] - Training Epoch: 1/2, step 5458/7134 completed (loss: 0.04655904695391655, acc: 0.987261176109314)
[2025-02-13 03:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41][root][INFO] - Training Epoch: 1/2, step 5459/7134 completed (loss: 0.021051274612545967, acc: 0.9910581111907959)
[2025-02-13 03:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41][root][INFO] - Training Epoch: 1/2, step 5460/7134 completed (loss: 0.038569651544094086, acc: 0.9944367408752441)
[2025-02-13 03:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41][root][INFO] - Training Epoch: 1/2, step 5461/7134 completed (loss: 0.081375852227211, acc: 0.9690949320793152)
[2025-02-13 03:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42][root][INFO] - Training Epoch: 1/2, step 5462/7134 completed (loss: 0.01971255987882614, acc: 0.9928469061851501)
[2025-02-13 03:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42][root][INFO] - Training Epoch: 1/2, step 5463/7134 completed (loss: 0.029087185859680176, acc: 0.9923664331436157)
[2025-02-13 03:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:43][root][INFO] - Training Epoch: 1/2, step 5464/7134 completed (loss: 0.033940404653549194, acc: 0.9881109595298767)
[2025-02-13 03:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:43][root][INFO] - Training Epoch: 1/2, step 5465/7134 completed (loss: 0.06933515518903732, acc: 0.9865471124649048)
[2025-02-13 03:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:44][root][INFO] - Training Epoch: 1/2, step 5466/7134 completed (loss: 0.024573028087615967, acc: 0.9930955171585083)
[2025-02-13 03:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:44][root][INFO] - Training Epoch: 1/2, step 5467/7134 completed (loss: 0.03995279222726822, acc: 0.98591548204422)
[2025-02-13 03:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45][root][INFO] - Training Epoch: 1/2, step 5468/7134 completed (loss: 0.041931264102458954, acc: 0.9895591735839844)
[2025-02-13 03:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45][root][INFO] - Training Epoch: 1/2, step 5469/7134 completed (loss: 0.029909683391451836, acc: 0.9926380515098572)
[2025-02-13 03:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45][root][INFO] - Training Epoch: 1/2, step 5470/7134 completed (loss: 0.02691982500255108, acc: 0.9927710890769958)
[2025-02-13 03:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46][root][INFO] - Training Epoch: 1/2, step 5471/7134 completed (loss: 0.025062888860702515, acc: 0.9932432174682617)
[2025-02-13 03:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46][root][INFO] - Training Epoch: 1/2, step 5472/7134 completed (loss: 0.031201021745800972, acc: 0.9950124621391296)
[2025-02-13 03:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:47][root][INFO] - Training Epoch: 1/2, step 5473/7134 completed (loss: 0.024500641971826553, acc: 0.991411030292511)
[2025-02-13 03:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:47][root][INFO] - Training Epoch: 1/2, step 5474/7134 completed (loss: 0.03986670449376106, acc: 0.9885714054107666)
[2025-02-13 03:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48][root][INFO] - Training Epoch: 1/2, step 5475/7134 completed (loss: 0.027690382674336433, acc: 0.9943100810050964)
[2025-02-13 03:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48][root][INFO] - Training Epoch: 1/2, step 5476/7134 completed (loss: 0.019860420376062393, acc: 0.9942113161087036)
[2025-02-13 03:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48][root][INFO] - Training Epoch: 1/2, step 5477/7134 completed (loss: 0.01688903570175171, acc: 0.9950739145278931)
[2025-02-13 03:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:49][root][INFO] - Training Epoch: 1/2, step 5478/7134 completed (loss: 0.06050023436546326, acc: 0.9845626354217529)
[2025-02-13 03:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:49][root][INFO] - Training Epoch: 1/2, step 5479/7134 completed (loss: 0.05292317271232605, acc: 0.98097825050354)
[2025-02-13 03:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:50][root][INFO] - Training Epoch: 1/2, step 5480/7134 completed (loss: 0.06172274425625801, acc: 0.9828947186470032)
[2025-02-13 03:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:50][root][INFO] - Training Epoch: 1/2, step 5481/7134 completed (loss: 0.03181665763258934, acc: 0.9858585596084595)
[2025-02-13 03:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:51][root][INFO] - Training Epoch: 1/2, step 5482/7134 completed (loss: 0.03699488565325737, acc: 0.990326464176178)
[2025-02-13 03:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:51][root][INFO] - Training Epoch: 1/2, step 5483/7134 completed (loss: 0.05903045833110809, acc: 0.9832869172096252)
[2025-02-13 03:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52][root][INFO] - Training Epoch: 1/2, step 5484/7134 completed (loss: 0.057204049080610275, acc: 0.9805285334587097)
[2025-02-13 03:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52][root][INFO] - Training Epoch: 1/2, step 5485/7134 completed (loss: 0.07805377244949341, acc: 0.9727767705917358)
[2025-02-13 03:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53][root][INFO] - Training Epoch: 1/2, step 5486/7134 completed (loss: 0.08568073064088821, acc: 0.9740484356880188)
[2025-02-13 03:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53][root][INFO] - Training Epoch: 1/2, step 5487/7134 completed (loss: 0.025871170684695244, acc: 0.9918699264526367)
[2025-02-13 03:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:54][root][INFO] - Training Epoch: 1/2, step 5488/7134 completed (loss: 0.08758504688739777, acc: 0.9671052694320679)
[2025-02-13 03:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:54][root][INFO] - Training Epoch: 1/2, step 5489/7134 completed (loss: 0.05005012825131416, acc: 0.9860681295394897)
[2025-02-13 03:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:54][root][INFO] - Training Epoch: 1/2, step 5490/7134 completed (loss: 0.032173532992601395, acc: 0.9908883571624756)
[2025-02-13 03:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55][root][INFO] - Training Epoch: 1/2, step 5491/7134 completed (loss: 0.032393403351306915, acc: 0.9912023544311523)
[2025-02-13 03:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55][root][INFO] - Training Epoch: 1/2, step 5492/7134 completed (loss: 0.05050869658589363, acc: 0.9847715497016907)
[2025-02-13 03:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55][root][INFO] - Training Epoch: 1/2, step 5493/7134 completed (loss: 0.05954255163669586, acc: 0.98097825050354)
[2025-02-13 03:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:56][root][INFO] - Training Epoch: 1/2, step 5494/7134 completed (loss: 0.017955012619495392, acc: 0.9971550703048706)
[2025-02-13 03:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:56][root][INFO] - Training Epoch: 1/2, step 5495/7134 completed (loss: 0.029961111024022102, acc: 0.9892473220825195)
[2025-02-13 03:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57][root][INFO] - Training Epoch: 1/2, step 5496/7134 completed (loss: 0.07988264411687851, acc: 0.9727626442909241)
[2025-02-13 03:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57][root][INFO] - Training Epoch: 1/2, step 5497/7134 completed (loss: 0.08589046448469162, acc: 0.9780380725860596)
[2025-02-13 03:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57][root][INFO] - Training Epoch: 1/2, step 5498/7134 completed (loss: 0.040187153965234756, acc: 0.982758641242981)
[2025-02-13 03:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:58][root][INFO] - Training Epoch: 1/2, step 5499/7134 completed (loss: 0.0573100745677948, acc: 0.9773869514465332)
[2025-02-13 03:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:58][root][INFO] - Training Epoch: 1/2, step 5500/7134 completed (loss: 0.04199967533349991, acc: 0.9849931597709656)
[2025-02-13 03:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:59][root][INFO] - Training Epoch: 1/2, step 5501/7134 completed (loss: 0.035743243992328644, acc: 0.990123450756073)
[2025-02-13 03:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:59][root][INFO] - Training Epoch: 1/2, step 5502/7134 completed (loss: 0.03787694126367569, acc: 0.9892037510871887)
[2025-02-13 03:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00][root][INFO] - Training Epoch: 1/2, step 5503/7134 completed (loss: 0.05639348179101944, acc: 0.9840954542160034)
[2025-02-13 03:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00][root][INFO] - Training Epoch: 1/2, step 5504/7134 completed (loss: 0.04461843892931938, acc: 0.9878934621810913)
[2025-02-13 03:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00][root][INFO] - Training Epoch: 1/2, step 5505/7134 completed (loss: 0.054093483835458755, acc: 0.980141818523407)
[2025-02-13 03:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:01][root][INFO] - Training Epoch: 1/2, step 5506/7134 completed (loss: 0.05968594178557396, acc: 0.9838523864746094)
[2025-02-13 03:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:01][root][INFO] - Training Epoch: 1/2, step 5507/7134 completed (loss: 0.05566934123635292, acc: 0.9831932783126831)
[2025-02-13 03:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02][root][INFO] - Training Epoch: 1/2, step 5508/7134 completed (loss: 0.05704896152019501, acc: 0.9848130941390991)
[2025-02-13 03:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02][root][INFO] - Training Epoch: 1/2, step 5509/7134 completed (loss: 0.04031374305486679, acc: 0.9906432628631592)
[2025-02-13 03:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03][root][INFO] - Training Epoch: 1/2, step 5510/7134 completed (loss: 0.05555858090519905, acc: 0.9845758080482483)
[2025-02-13 03:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03][root][INFO] - Training Epoch: 1/2, step 5511/7134 completed (loss: 0.06592514365911484, acc: 0.983031690120697)
[2025-02-13 03:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03][root][INFO] - Training Epoch: 1/2, step 5512/7134 completed (loss: 0.03479555994272232, acc: 0.9894598126411438)
[2025-02-13 03:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04][root][INFO] - Training Epoch: 1/2, step 5513/7134 completed (loss: 0.06729581952095032, acc: 0.979619562625885)
[2025-02-13 03:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04][root][INFO] - Training Epoch: 1/2, step 5514/7134 completed (loss: 0.15131229162216187, acc: 0.9626865386962891)
[2025-02-13 03:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05][root][INFO] - Training Epoch: 1/2, step 5515/7134 completed (loss: 0.04774726554751396, acc: 0.984308123588562)
[2025-02-13 03:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05][root][INFO] - Training Epoch: 1/2, step 5516/7134 completed (loss: 0.05037468671798706, acc: 0.9915611743927002)
[2025-02-13 03:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06][root][INFO] - Training Epoch: 1/2, step 5517/7134 completed (loss: 0.037416763603687286, acc: 0.9867021441459656)
[2025-02-13 03:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06][root][INFO] - Training Epoch: 1/2, step 5518/7134 completed (loss: 0.058447401970624924, acc: 0.9850543737411499)
[2025-02-13 03:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06][root][INFO] - Training Epoch: 1/2, step 5519/7134 completed (loss: 0.026143832132220268, acc: 0.9903448224067688)
[2025-02-13 03:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:07][root][INFO] - Training Epoch: 1/2, step 5520/7134 completed (loss: 0.025520620867609978, acc: 0.9943374991416931)
[2025-02-13 03:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:07][root][INFO] - Training Epoch: 1/2, step 5521/7134 completed (loss: 0.057704608887434006, acc: 0.9849246144294739)
[2025-02-13 03:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:08][root][INFO] - Training Epoch: 1/2, step 5522/7134 completed (loss: 0.07182985544204712, acc: 0.9816993474960327)
[2025-02-13 03:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:08][root][INFO] - Training Epoch: 1/2, step 5523/7134 completed (loss: 0.05287300422787666, acc: 0.9859550595283508)
[2025-02-13 03:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09][root][INFO] - Training Epoch: 1/2, step 5524/7134 completed (loss: 0.06151283159852028, acc: 0.9814241528511047)
[2025-02-13 03:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09][root][INFO] - Training Epoch: 1/2, step 5525/7134 completed (loss: 0.047304004430770874, acc: 0.9824304580688477)
[2025-02-13 03:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09][root][INFO] - Training Epoch: 1/2, step 5526/7134 completed (loss: 0.047825887799263, acc: 0.9896907210350037)
[2025-02-13 03:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:10][root][INFO] - Training Epoch: 1/2, step 5527/7134 completed (loss: 0.0517469197511673, acc: 0.9882352948188782)
[2025-02-13 03:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:10][root][INFO] - Training Epoch: 1/2, step 5528/7134 completed (loss: 0.040042515844106674, acc: 0.9857988357543945)
[2025-02-13 03:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:11][root][INFO] - Training Epoch: 1/2, step 5529/7134 completed (loss: 0.06876998394727707, acc: 0.978723406791687)
[2025-02-13 03:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:11][root][INFO] - Training Epoch: 1/2, step 5530/7134 completed (loss: 0.04925193265080452, acc: 0.9910581111907959)
[2025-02-13 03:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12][root][INFO] - Training Epoch: 1/2, step 5531/7134 completed (loss: 0.05345294252038002, acc: 0.9850560426712036)
[2025-02-13 03:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12][root][INFO] - Training Epoch: 1/2, step 5532/7134 completed (loss: 0.07537327706813812, acc: 0.9765142202377319)
[2025-02-13 03:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12][root][INFO] - Training Epoch: 1/2, step 5533/7134 completed (loss: 0.0187364649027586, acc: 0.9943310618400574)
[2025-02-13 03:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13][root][INFO] - Training Epoch: 1/2, step 5534/7134 completed (loss: 0.06250325590372086, acc: 0.9836448431015015)
[2025-02-13 03:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13][root][INFO] - Training Epoch: 1/2, step 5535/7134 completed (loss: 0.03693088889122009, acc: 0.9902067184448242)
[2025-02-13 03:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:14][root][INFO] - Training Epoch: 1/2, step 5536/7134 completed (loss: 0.035754721611738205, acc: 0.9883720874786377)
[2025-02-13 03:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:14][root][INFO] - Training Epoch: 1/2, step 5537/7134 completed (loss: 0.060254763811826706, acc: 0.97782963514328)
[2025-02-13 03:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:15][root][INFO] - Training Epoch: 1/2, step 5538/7134 completed (loss: 0.04014922305941582, acc: 0.9852104783058167)
[2025-02-13 03:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:15][root][INFO] - Training Epoch: 1/2, step 5539/7134 completed (loss: 0.045665547251701355, acc: 0.9853603839874268)
[2025-02-13 03:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:16][root][INFO] - Training Epoch: 1/2, step 5540/7134 completed (loss: 0.05266785994172096, acc: 0.9907894730567932)
[2025-02-13 03:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:16][root][INFO] - Training Epoch: 1/2, step 5541/7134 completed (loss: 0.039150603115558624, acc: 0.9893617033958435)
[2025-02-13 03:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:16][root][INFO] - Training Epoch: 1/2, step 5542/7134 completed (loss: 0.029759345576167107, acc: 0.9914407730102539)
[2025-02-13 03:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17][root][INFO] - Training Epoch: 1/2, step 5543/7134 completed (loss: 0.04004382714629173, acc: 0.9878048896789551)
[2025-02-13 03:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17][root][INFO] - Training Epoch: 1/2, step 5544/7134 completed (loss: 0.02488691732287407, acc: 0.9924731254577637)
[2025-02-13 03:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18][root][INFO] - Training Epoch: 1/2, step 5545/7134 completed (loss: 0.04783092066645622, acc: 0.980327844619751)
[2025-02-13 03:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18][root][INFO] - Training Epoch: 1/2, step 5546/7134 completed (loss: 0.04356606304645538, acc: 0.9856035709381104)
[2025-02-13 03:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:19][root][INFO] - Training Epoch: 1/2, step 5547/7134 completed (loss: 0.035502463579177856, acc: 0.9919354915618896)
[2025-02-13 03:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:19][root][INFO] - Training Epoch: 1/2, step 5548/7134 completed (loss: 0.033598851412534714, acc: 0.9906976819038391)
[2025-02-13 03:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20][root][INFO] - Training Epoch: 1/2, step 5549/7134 completed (loss: 0.0536048598587513, acc: 0.9871428608894348)
[2025-02-13 03:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20][root][INFO] - Training Epoch: 1/2, step 5550/7134 completed (loss: 0.017599230632185936, acc: 0.9941245317459106)
[2025-02-13 03:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20][root][INFO] - Training Epoch: 1/2, step 5551/7134 completed (loss: 0.07058893889188766, acc: 0.981055498123169)
[2025-02-13 03:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:21][root][INFO] - Training Epoch: 1/2, step 5552/7134 completed (loss: 0.0274509247392416, acc: 0.9940546751022339)
[2025-02-13 03:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:21][root][INFO] - Training Epoch: 1/2, step 5553/7134 completed (loss: 0.05004805326461792, acc: 0.9864029884338379)
[2025-02-13 03:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22][root][INFO] - Training Epoch: 1/2, step 5554/7134 completed (loss: 0.022147057577967644, acc: 0.9926650524139404)
[2025-02-13 03:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22][root][INFO] - Training Epoch: 1/2, step 5555/7134 completed (loss: 0.04572644829750061, acc: 0.9839572310447693)
[2025-02-13 03:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22][root][INFO] - Training Epoch: 1/2, step 5556/7134 completed (loss: 0.0660613402724266, acc: 0.9792477488517761)
[2025-02-13 03:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23][root][INFO] - Training Epoch: 1/2, step 5557/7134 completed (loss: 0.02568679489195347, acc: 0.994397759437561)
[2025-02-13 03:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23][root][INFO] - Training Epoch: 1/2, step 5558/7134 completed (loss: 0.035502128303050995, acc: 0.9888198971748352)
[2025-02-13 03:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24][root][INFO] - Training Epoch: 1/2, step 5559/7134 completed (loss: 0.04067851975560188, acc: 0.992546558380127)
[2025-02-13 03:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24][root][INFO] - Training Epoch: 1/2, step 5560/7134 completed (loss: 0.06352151185274124, acc: 0.9831606149673462)
[2025-02-13 03:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25][root][INFO] - Training Epoch: 1/2, step 5561/7134 completed (loss: 0.043514467775821686, acc: 0.9943289160728455)
[2025-02-13 03:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25][root][INFO] - Training Epoch: 1/2, step 5562/7134 completed (loss: 0.04667261987924576, acc: 0.9948805570602417)
[2025-02-13 03:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25][root][INFO] - Training Epoch: 1/2, step 5563/7134 completed (loss: 0.05166417360305786, acc: 0.9894894957542419)
[2025-02-13 03:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:26][root][INFO] - Training Epoch: 1/2, step 5564/7134 completed (loss: 0.03097505122423172, acc: 0.9944211840629578)
[2025-02-13 03:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:26][root][INFO] - Training Epoch: 1/2, step 5565/7134 completed (loss: 0.016524523496627808, acc: 0.996927797794342)
[2025-02-13 03:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27][root][INFO] - Training Epoch: 1/2, step 5566/7134 completed (loss: 0.022370127961039543, acc: 0.991963267326355)
[2025-02-13 03:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27][root][INFO] - Training Epoch: 1/2, step 5567/7134 completed (loss: 0.03758471459150314, acc: 0.990867555141449)
[2025-02-13 03:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28][root][INFO] - Training Epoch: 1/2, step 5568/7134 completed (loss: 0.03582989424467087, acc: 0.9909793734550476)
[2025-02-13 03:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28][root][INFO] - Training Epoch: 1/2, step 5569/7134 completed (loss: 0.021755684167146683, acc: 0.9954338073730469)
[2025-02-13 03:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28][root][INFO] - Training Epoch: 1/2, step 5570/7134 completed (loss: 0.0285347830504179, acc: 0.9937888383865356)
[2025-02-13 03:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:29][root][INFO] - Training Epoch: 1/2, step 5571/7134 completed (loss: 0.047648318111896515, acc: 0.9866270422935486)
[2025-02-13 03:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:29][root][INFO] - Training Epoch: 1/2, step 5572/7134 completed (loss: 0.06901133060455322, acc: 0.9862637519836426)
[2025-02-13 03:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:30][root][INFO] - Training Epoch: 1/2, step 5573/7134 completed (loss: 0.041546814143657684, acc: 0.9868995547294617)
[2025-02-13 03:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:30][root][INFO] - Training Epoch: 1/2, step 5574/7134 completed (loss: 0.01263359747827053, acc: 0.9968253970146179)
[2025-02-13 03:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31][root][INFO] - Training Epoch: 1/2, step 5575/7134 completed (loss: 0.035075947642326355, acc: 0.9888357520103455)
[2025-02-13 03:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31][root][INFO] - Training Epoch: 1/2, step 5576/7134 completed (loss: 0.03238435834646225, acc: 0.9813753366470337)
[2025-02-13 03:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31][root][INFO] - Training Epoch: 1/2, step 5577/7134 completed (loss: 0.06745421886444092, acc: 0.9808027744293213)
[2025-02-13 03:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:32][root][INFO] - Training Epoch: 1/2, step 5578/7134 completed (loss: 0.03228657320141792, acc: 0.991391658782959)
[2025-02-13 03:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:32][root][INFO] - Training Epoch: 1/2, step 5579/7134 completed (loss: 0.043771661818027496, acc: 0.9834024906158447)
[2025-02-13 03:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33][root][INFO] - Training Epoch: 1/2, step 5580/7134 completed (loss: 0.02131475694477558, acc: 0.9945429563522339)
[2025-02-13 03:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33][root][INFO] - Training Epoch: 1/2, step 5581/7134 completed (loss: 0.02646680362522602, acc: 0.9880319237709045)
[2025-02-13 03:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34][root][INFO] - Training Epoch: 1/2, step 5582/7134 completed (loss: 0.026193803176283836, acc: 0.9869621992111206)
[2025-02-13 03:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34][root][INFO] - Training Epoch: 1/2, step 5583/7134 completed (loss: 0.02539125457406044, acc: 0.99370276927948)
[2025-02-13 03:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34][root][INFO] - Training Epoch: 1/2, step 5584/7134 completed (loss: 0.027193354442715645, acc: 0.9926035404205322)
[2025-02-13 03:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:35][root][INFO] - Training Epoch: 1/2, step 5585/7134 completed (loss: 0.04520151764154434, acc: 0.9890909194946289)
[2025-02-13 03:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:35][root][INFO] - Training Epoch: 1/2, step 5586/7134 completed (loss: 0.03271278738975525, acc: 0.9893428087234497)
[2025-02-13 03:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:36][root][INFO] - Training Epoch: 1/2, step 5587/7134 completed (loss: 0.028514862060546875, acc: 0.9930651783943176)
[2025-02-13 03:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:36][root][INFO] - Training Epoch: 1/2, step 5588/7134 completed (loss: 0.01653692126274109, acc: 0.9954614043235779)
[2025-02-13 03:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37][root][INFO] - Training Epoch: 1/2, step 5589/7134 completed (loss: 0.03765686973929405, acc: 0.9936908483505249)
[2025-02-13 03:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37][root][INFO] - Training Epoch: 1/2, step 5590/7134 completed (loss: 0.051552921533584595, acc: 0.9873239398002625)
[2025-02-13 03:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37][root][INFO] - Training Epoch: 1/2, step 5591/7134 completed (loss: 0.027501951903104782, acc: 0.9908376932144165)
[2025-02-13 03:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38][root][INFO] - Training Epoch: 1/2, step 5592/7134 completed (loss: 0.02476510964334011, acc: 0.9931412935256958)
[2025-02-13 03:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38][root][INFO] - Training Epoch: 1/2, step 5593/7134 completed (loss: 0.03873717784881592, acc: 0.9883419871330261)
[2025-02-13 03:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39][root][INFO] - Training Epoch: 1/2, step 5594/7134 completed (loss: 0.015600881539285183, acc: 0.9946091771125793)
[2025-02-13 03:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39][root][INFO] - Training Epoch: 1/2, step 5595/7134 completed (loss: 0.05966505780816078, acc: 0.9839141964912415)
[2025-02-13 03:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39][root][INFO] - Training Epoch: 1/2, step 5596/7134 completed (loss: 0.019684089347720146, acc: 0.9932795763015747)
[2025-02-13 03:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:40][root][INFO] - Training Epoch: 1/2, step 5597/7134 completed (loss: 0.08476928621530533, acc: 0.9807162284851074)
[2025-02-13 03:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:40][root][INFO] - Training Epoch: 1/2, step 5598/7134 completed (loss: 0.03855103626847267, acc: 0.9874301552772522)
[2025-02-13 03:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41][root][INFO] - Training Epoch: 1/2, step 5599/7134 completed (loss: 0.07399967312812805, acc: 0.9822404384613037)
[2025-02-13 03:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41][root][INFO] - Training Epoch: 1/2, step 5600/7134 completed (loss: 0.02311078831553459, acc: 0.9942693114280701)
[2025-02-13 03:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42][root][INFO] - Training Epoch: 1/2, step 5601/7134 completed (loss: 0.07448031008243561, acc: 0.9826086759567261)
[2025-02-13 03:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42][root][INFO] - Training Epoch: 1/2, step 5602/7134 completed (loss: 0.07345705479383469, acc: 0.979066014289856)
[2025-02-13 03:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42][root][INFO] - Training Epoch: 1/2, step 5603/7134 completed (loss: 0.016007399186491966, acc: 0.9934533834457397)
[2025-02-13 03:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43][root][INFO] - Training Epoch: 1/2, step 5604/7134 completed (loss: 0.08585356175899506, acc: 0.9662379622459412)
[2025-02-13 03:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43][root][INFO] - Training Epoch: 1/2, step 5605/7134 completed (loss: 0.04025721177458763, acc: 0.9864176511764526)
[2025-02-13 03:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43][root][INFO] - Training Epoch: 1/2, step 5606/7134 completed (loss: 0.04570229351520538, acc: 0.9871794581413269)
[2025-02-13 03:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:44][root][INFO] - Training Epoch: 1/2, step 5607/7134 completed (loss: 0.029368428513407707, acc: 0.995488703250885)
[2025-02-13 03:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:44][root][INFO] - Training Epoch: 1/2, step 5608/7134 completed (loss: 0.037126634269952774, acc: 0.9885057210922241)
[2025-02-13 03:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45][root][INFO] - Training Epoch: 1/2, step 5609/7134 completed (loss: 0.04736611992120743, acc: 0.9756521582603455)
[2025-02-13 03:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45][root][INFO] - Training Epoch: 1/2, step 5610/7134 completed (loss: 0.033216893672943115, acc: 0.9899396300315857)
[2025-02-13 03:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45][root][INFO] - Training Epoch: 1/2, step 5611/7134 completed (loss: 0.046430740505456924, acc: 0.982594907283783)
[2025-02-13 03:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:46][root][INFO] - Training Epoch: 1/2, step 5612/7134 completed (loss: 0.05322014540433884, acc: 0.9861963391304016)
[2025-02-13 03:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:46][root][INFO] - Training Epoch: 1/2, step 5613/7134 completed (loss: 0.0421728678047657, acc: 0.9874776601791382)
[2025-02-13 03:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47][root][INFO] - Training Epoch: 1/2, step 5614/7134 completed (loss: 0.06061585620045662, acc: 0.9842857122421265)
[2025-02-13 03:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47][root][INFO] - Training Epoch: 1/2, step 5615/7134 completed (loss: 0.06328731775283813, acc: 0.975095808506012)
[2025-02-13 03:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47][root][INFO] - Training Epoch: 1/2, step 5616/7134 completed (loss: 0.03424181789159775, acc: 0.9881129264831543)
[2025-02-13 03:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:48][root][INFO] - Training Epoch: 1/2, step 5617/7134 completed (loss: 0.021078823134303093, acc: 0.9951298832893372)
[2025-02-13 03:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:48][root][INFO] - Training Epoch: 1/2, step 5618/7134 completed (loss: 0.06569557636976242, acc: 0.9788618087768555)
[2025-02-13 03:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49][root][INFO] - Training Epoch: 1/2, step 5619/7134 completed (loss: 0.05345538631081581, acc: 0.9797160029411316)
[2025-02-13 03:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49][root][INFO] - Training Epoch: 1/2, step 5620/7134 completed (loss: 0.040013544261455536, acc: 0.9831775426864624)
[2025-02-13 03:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50][root][INFO] - Training Epoch: 1/2, step 5621/7134 completed (loss: 0.027096685022115707, acc: 0.9923760890960693)
[2025-02-13 03:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50][root][INFO] - Training Epoch: 1/2, step 5622/7134 completed (loss: 0.06691373884677887, acc: 0.9850136041641235)
[2025-02-13 03:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50][root][INFO] - Training Epoch: 1/2, step 5623/7134 completed (loss: 0.044958777725696564, acc: 0.9860627055168152)
[2025-02-13 03:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:51][root][INFO] - Training Epoch: 1/2, step 5624/7134 completed (loss: 0.04536012187600136, acc: 0.9885433912277222)
[2025-02-13 03:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:51][root][INFO] - Training Epoch: 1/2, step 5625/7134 completed (loss: 0.013924231752753258, acc: 0.9952940940856934)
[2025-02-13 03:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52][root][INFO] - Training Epoch: 1/2, step 5626/7134 completed (loss: 0.027408141642808914, acc: 0.989847719669342)
[2025-02-13 03:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52][root][INFO] - Training Epoch: 1/2, step 5627/7134 completed (loss: 0.13172197341918945, acc: 0.9684600830078125)
[2025-02-13 03:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52][root][INFO] - Training Epoch: 1/2, step 5628/7134 completed (loss: 0.057168543338775635, acc: 0.9817850589752197)
[2025-02-13 03:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53][root][INFO] - Training Epoch: 1/2, step 5629/7134 completed (loss: 0.06236174702644348, acc: 0.9789103865623474)
[2025-02-13 03:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53][root][INFO] - Training Epoch: 1/2, step 5630/7134 completed (loss: 0.09733377397060394, acc: 0.9733059406280518)
[2025-02-13 03:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54][root][INFO] - Training Epoch: 1/2, step 5631/7134 completed (loss: 0.08662474155426025, acc: 0.9734659790992737)
[2025-02-13 03:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54][root][INFO] - Training Epoch: 1/2, step 5632/7134 completed (loss: 0.04347128048539162, acc: 0.9872958064079285)
[2025-02-13 03:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54][root][INFO] - Training Epoch: 1/2, step 5633/7134 completed (loss: 0.0816681757569313, acc: 0.9690553545951843)
[2025-02-13 03:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:55][root][INFO] - Training Epoch: 1/2, step 5634/7134 completed (loss: 0.08755796402692795, acc: 0.9860383868217468)
[2025-02-13 03:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:55][root][INFO] - Training Epoch: 1/2, step 5635/7134 completed (loss: 0.02088673785328865, acc: 0.9966216087341309)
[2025-02-13 03:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56][root][INFO] - Training Epoch: 1/2, step 5636/7134 completed (loss: 0.048006631433963776, acc: 0.9856230020523071)
[2025-02-13 03:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56][root][INFO] - Training Epoch: 1/2, step 5637/7134 completed (loss: 0.06011838838458061, acc: 0.9814502596855164)
[2025-02-13 03:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56][root][INFO] - Training Epoch: 1/2, step 5638/7134 completed (loss: 0.04487059265375137, acc: 0.990234375)
[2025-02-13 03:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57][root][INFO] - Training Epoch: 1/2, step 5639/7134 completed (loss: 0.052129749208688736, acc: 0.9829059839248657)
[2025-02-13 03:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57][root][INFO] - Training Epoch: 1/2, step 5640/7134 completed (loss: 0.06561967730522156, acc: 0.9837398529052734)
[2025-02-13 03:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58][root][INFO] - Training Epoch: 1/2, step 5641/7134 completed (loss: 0.0682549700140953, acc: 0.9873737096786499)
[2025-02-13 03:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58][root][INFO] - Training Epoch: 1/2, step 5642/7134 completed (loss: 0.0794534906744957, acc: 0.9803030490875244)
[2025-02-13 03:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58][root][INFO] - Training Epoch: 1/2, step 5643/7134 completed (loss: 0.08157959580421448, acc: 0.9778761267662048)
[2025-02-13 03:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59][root][INFO] - Training Epoch: 1/2, step 5644/7134 completed (loss: 0.05090692639350891, acc: 0.9857723712921143)
[2025-02-13 03:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59][root][INFO] - Training Epoch: 1/2, step 5645/7134 completed (loss: 0.03882475569844246, acc: 0.9823269248008728)
[2025-02-13 03:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59][root][INFO] - Training Epoch: 1/2, step 5646/7134 completed (loss: 0.04998347535729408, acc: 0.9824841022491455)
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][root][INFO] - Training Epoch: 1/2, step 5647/7134 completed (loss: 0.05266878008842468, acc: 0.9844961166381836)
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][root][INFO] - Training Epoch: 1/2, step 5648/7134 completed (loss: 0.108863465487957, acc: 0.9746268391609192)
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01][root][INFO] - Training Epoch: 1/2, step 5649/7134 completed (loss: 0.07535839825868607, acc: 0.9816513657569885)
[2025-02-13 03:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01][root][INFO] - Training Epoch: 1/2, step 5650/7134 completed (loss: 0.03943170607089996, acc: 0.9862778782844543)
[2025-02-13 03:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02][root][INFO] - Training Epoch: 1/2, step 5651/7134 completed (loss: 0.029031815007328987, acc: 0.9926035404205322)
[2025-02-13 03:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02][root][INFO] - Training Epoch: 1/2, step 5652/7134 completed (loss: 0.0456540547311306, acc: 0.9855769276618958)
[2025-02-13 03:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02][root][INFO] - Training Epoch: 1/2, step 5653/7134 completed (loss: 0.08831027150154114, acc: 0.9765625)
[2025-02-13 03:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:03][root][INFO] - Training Epoch: 1/2, step 5654/7134 completed (loss: 0.03627414256334305, acc: 0.9924924969673157)
[2025-02-13 03:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:03][root][INFO] - Training Epoch: 1/2, step 5655/7134 completed (loss: 0.07230450958013535, acc: 0.9795918464660645)
[2025-02-13 03:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04][root][INFO] - Training Epoch: 1/2, step 5656/7134 completed (loss: 0.026586180552840233, acc: 0.9923809766769409)
[2025-02-13 03:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04][root][INFO] - Training Epoch: 1/2, step 5657/7134 completed (loss: 0.05356570705771446, acc: 0.9926793575286865)
[2025-02-13 03:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04][root][INFO] - Training Epoch: 1/2, step 5658/7134 completed (loss: 0.060344357043504715, acc: 0.980663001537323)
[2025-02-13 03:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05][root][INFO] - Training Epoch: 1/2, step 5659/7134 completed (loss: 0.05963582172989845, acc: 0.9886524677276611)
[2025-02-13 03:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05][root][INFO] - Training Epoch: 1/2, step 5660/7134 completed (loss: 0.029199881479144096, acc: 0.9917355179786682)
[2025-02-13 03:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06][root][INFO] - Training Epoch: 1/2, step 5661/7134 completed (loss: 0.015115189366042614, acc: 0.9963503479957581)
[2025-02-13 03:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06][root][INFO] - Training Epoch: 1/2, step 5662/7134 completed (loss: 0.028929948806762695, acc: 0.9967948794364929)
[2025-02-13 03:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06][root][INFO] - Training Epoch: 1/2, step 5663/7134 completed (loss: 0.025647014379501343, acc: 0.990755021572113)
[2025-02-13 03:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:07][root][INFO] - Training Epoch: 1/2, step 5664/7134 completed (loss: 0.08936601877212524, acc: 0.9821693897247314)
[2025-02-13 03:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:07][root][INFO] - Training Epoch: 1/2, step 5665/7134 completed (loss: 0.027772244065999985, acc: 0.9900142550468445)
[2025-02-13 03:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08][root][INFO] - Training Epoch: 1/2, step 5666/7134 completed (loss: 0.015597554855048656, acc: 0.9944827556610107)
[2025-02-13 03:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08][root][INFO] - Training Epoch: 1/2, step 5667/7134 completed (loss: 0.03130721300840378, acc: 0.996363639831543)
[2025-02-13 03:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08][root][INFO] - Training Epoch: 1/2, step 5668/7134 completed (loss: 0.06521718204021454, acc: 0.9817276000976562)
[2025-02-13 03:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:09][root][INFO] - Training Epoch: 1/2, step 5669/7134 completed (loss: 0.04216456785798073, acc: 0.9912023544311523)
[2025-02-13 03:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:09][root][INFO] - Training Epoch: 1/2, step 5670/7134 completed (loss: 0.0332488976418972, acc: 0.9866666793823242)
[2025-02-13 03:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10][root][INFO] - Training Epoch: 1/2, step 5671/7134 completed (loss: 0.017859626561403275, acc: 0.9938499331474304)
[2025-02-13 03:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10][root][INFO] - Training Epoch: 1/2, step 5672/7134 completed (loss: 0.01961752213537693, acc: 0.9905362725257874)
[2025-02-13 03:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11][root][INFO] - Training Epoch: 1/2, step 5673/7134 completed (loss: 0.019332120195031166, acc: 0.9940915703773499)
[2025-02-13 03:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11][root][INFO] - Training Epoch: 1/2, step 5674/7134 completed (loss: 0.021913740783929825, acc: 0.9923547506332397)
[2025-02-13 03:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11][root][INFO] - Training Epoch: 1/2, step 5675/7134 completed (loss: 0.04634382203221321, acc: 0.9834162592887878)
[2025-02-13 03:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:12][root][INFO] - Training Epoch: 1/2, step 5676/7134 completed (loss: 0.09396666288375854, acc: 0.976331353187561)
[2025-02-13 03:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:12][root][INFO] - Training Epoch: 1/2, step 5677/7134 completed (loss: 0.053510989993810654, acc: 0.9857954382896423)
[2025-02-13 03:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13][root][INFO] - Training Epoch: 1/2, step 5678/7134 completed (loss: 0.04074101522564888, acc: 0.9897058606147766)
[2025-02-13 03:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13][root][INFO] - Training Epoch: 1/2, step 5679/7134 completed (loss: 0.09764644503593445, acc: 0.9721815586090088)
[2025-02-13 03:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13][root][INFO] - Training Epoch: 1/2, step 5680/7134 completed (loss: 0.08729352802038193, acc: 0.9687034487724304)
[2025-02-13 03:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14][root][INFO] - Training Epoch: 1/2, step 5681/7134 completed (loss: 0.058449722826480865, acc: 0.9758672714233398)
[2025-02-13 03:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14][root][INFO] - Training Epoch: 1/2, step 5682/7134 completed (loss: 0.03987948223948479, acc: 0.9855700135231018)
[2025-02-13 03:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:15][root][INFO] - Training Epoch: 1/2, step 5683/7134 completed (loss: 0.0715254619717598, acc: 0.9765739440917969)
[2025-02-13 03:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:15][root][INFO] - Training Epoch: 1/2, step 5684/7134 completed (loss: 0.07022014260292053, acc: 0.9831365942955017)
[2025-02-13 03:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16][root][INFO] - Training Epoch: 1/2, step 5685/7134 completed (loss: 0.07359760999679565, acc: 0.9797794222831726)
[2025-02-13 03:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16][root][INFO] - Training Epoch: 1/2, step 5686/7134 completed (loss: 0.07500676065683365, acc: 0.9746192693710327)
[2025-02-13 03:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16][root][INFO] - Training Epoch: 1/2, step 5687/7134 completed (loss: 0.06503962725400925, acc: 0.9798534512519836)
[2025-02-13 03:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:17][root][INFO] - Training Epoch: 1/2, step 5688/7134 completed (loss: 0.09491255134344101, acc: 0.9773030877113342)
[2025-02-13 03:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:17][root][INFO] - Training Epoch: 1/2, step 5689/7134 completed (loss: 0.07814296334981918, acc: 0.9817351698875427)
[2025-02-13 03:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18][root][INFO] - Training Epoch: 1/2, step 5690/7134 completed (loss: 0.04425618052482605, acc: 0.9905660152435303)
[2025-02-13 03:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18][root][INFO] - Training Epoch: 1/2, step 5691/7134 completed (loss: 0.09236442297697067, acc: 0.9763975143432617)
[2025-02-13 03:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19][root][INFO] - Training Epoch: 1/2, step 5692/7134 completed (loss: 0.059691671282052994, acc: 0.9865951538085938)
[2025-02-13 03:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19][root][INFO] - Training Epoch: 1/2, step 5693/7134 completed (loss: 0.02454272098839283, acc: 0.9895052313804626)
[2025-02-13 03:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19][root][INFO] - Training Epoch: 1/2, step 5694/7134 completed (loss: 0.05241921916604042, acc: 0.986975371837616)
[2025-02-13 03:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:20][root][INFO] - Training Epoch: 1/2, step 5695/7134 completed (loss: 0.04229829087853432, acc: 0.9885386824607849)
[2025-02-13 03:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:20][root][INFO] - Training Epoch: 1/2, step 5696/7134 completed (loss: 0.039947863668203354, acc: 0.9882352948188782)
[2025-02-13 03:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21][root][INFO] - Training Epoch: 1/2, step 5697/7134 completed (loss: 0.058601830154657364, acc: 0.9887164831161499)
[2025-02-13 03:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21][root][INFO] - Training Epoch: 1/2, step 5698/7134 completed (loss: 0.046508025377988815, acc: 0.9904761910438538)
[2025-02-13 03:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21][root][INFO] - Training Epoch: 1/2, step 5699/7134 completed (loss: 0.0451960414648056, acc: 0.987261176109314)
[2025-02-13 03:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22][root][INFO] - Training Epoch: 1/2, step 5700/7134 completed (loss: 0.031219355762004852, acc: 0.9872340559959412)
[2025-02-13 03:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22][root][INFO] - Training Epoch: 1/2, step 5701/7134 completed (loss: 0.058158788830041885, acc: 0.9899874925613403)
[2025-02-13 03:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23][root][INFO] - Training Epoch: 1/2, step 5702/7134 completed (loss: 0.054920315742492676, acc: 0.9851149916648865)
[2025-02-13 03:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23][root][INFO] - Training Epoch: 1/2, step 5703/7134 completed (loss: 0.026405174285173416, acc: 0.9887482523918152)
[2025-02-13 03:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24][root][INFO] - Training Epoch: 1/2, step 5704/7134 completed (loss: 0.06759967654943466, acc: 0.980028510093689)
[2025-02-13 03:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24][root][INFO] - Training Epoch: 1/2, step 5705/7134 completed (loss: 0.0186091847717762, acc: 0.9928571581840515)
[2025-02-13 03:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24][root][INFO] - Training Epoch: 1/2, step 5706/7134 completed (loss: 0.02394142374396324, acc: 0.9932885766029358)
[2025-02-13 03:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25][root][INFO] - Training Epoch: 1/2, step 5707/7134 completed (loss: 0.06128757819533348, acc: 0.9806337952613831)
[2025-02-13 03:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25][root][INFO] - Training Epoch: 1/2, step 5708/7134 completed (loss: 0.03291155397891998, acc: 0.9911110997200012)
[2025-02-13 03:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26][root][INFO] - Training Epoch: 1/2, step 5709/7134 completed (loss: 0.06822030991315842, acc: 0.9863201379776001)
[2025-02-13 03:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26][root][INFO] - Training Epoch: 1/2, step 5710/7134 completed (loss: 0.044149354100227356, acc: 0.9872773289680481)
[2025-02-13 03:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26][root][INFO] - Training Epoch: 1/2, step 5711/7134 completed (loss: 0.05118197947740555, acc: 0.9842932224273682)
[2025-02-13 03:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:27][root][INFO] - Training Epoch: 1/2, step 5712/7134 completed (loss: 0.057617198675870895, acc: 0.984000027179718)
[2025-02-13 03:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:27][root][INFO] - Training Epoch: 1/2, step 5713/7134 completed (loss: 0.026406824588775635, acc: 0.9936467409133911)
[2025-02-13 03:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28][root][INFO] - Training Epoch: 1/2, step 5714/7134 completed (loss: 0.028497306630015373, acc: 0.993773341178894)
[2025-02-13 03:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28][root][INFO] - Training Epoch: 1/2, step 5715/7134 completed (loss: 0.07808144390583038, acc: 0.9832402467727661)
[2025-02-13 03:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29][root][INFO] - Training Epoch: 1/2, step 5716/7134 completed (loss: 0.13734374940395355, acc: 0.9751332402229309)
[2025-02-13 03:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29][root][INFO] - Training Epoch: 1/2, step 5717/7134 completed (loss: 0.11453460901975632, acc: 0.9751166701316833)
[2025-02-13 03:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29][root][INFO] - Training Epoch: 1/2, step 5718/7134 completed (loss: 0.06796237081289291, acc: 0.9826498627662659)
[2025-02-13 03:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:30][root][INFO] - Training Epoch: 1/2, step 5719/7134 completed (loss: 0.03869813680648804, acc: 0.9895052313804626)
[2025-02-13 03:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:30][root][INFO] - Training Epoch: 1/2, step 5720/7134 completed (loss: 0.034441571682691574, acc: 0.9879102110862732)
[2025-02-13 03:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31][root][INFO] - Training Epoch: 1/2, step 5721/7134 completed (loss: 0.043478645384311676, acc: 0.9855263233184814)
[2025-02-13 03:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31][root][INFO] - Training Epoch: 1/2, step 5722/7134 completed (loss: 0.04963354766368866, acc: 0.9866962432861328)
[2025-02-13 03:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31][root][INFO] - Training Epoch: 1/2, step 5723/7134 completed (loss: 0.06489971280097961, acc: 0.97579425573349)
[2025-02-13 03:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:32][root][INFO] - Training Epoch: 1/2, step 5724/7134 completed (loss: 0.03090803511440754, acc: 0.9872000217437744)
[2025-02-13 03:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:32][root][INFO] - Training Epoch: 1/2, step 5725/7134 completed (loss: 0.03590681776404381, acc: 0.9928571581840515)
[2025-02-13 03:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33][root][INFO] - Training Epoch: 1/2, step 5726/7134 completed (loss: 0.06048860028386116, acc: 0.9847095012664795)
[2025-02-13 03:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33][root][INFO] - Training Epoch: 1/2, step 5727/7134 completed (loss: 0.020838715136051178, acc: 0.9946523904800415)
[2025-02-13 03:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33][root][INFO] - Training Epoch: 1/2, step 5728/7134 completed (loss: 0.014778384938836098, acc: 1.0)
[2025-02-13 03:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34][root][INFO] - Training Epoch: 1/2, step 5729/7134 completed (loss: 0.03375595062971115, acc: 0.9883333444595337)
[2025-02-13 03:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34][root][INFO] - Training Epoch: 1/2, step 5730/7134 completed (loss: 0.08259841799736023, acc: 0.9797979593276978)
[2025-02-13 03:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35][root][INFO] - Training Epoch: 1/2, step 5731/7134 completed (loss: 0.056846391409635544, acc: 0.9833887219429016)
[2025-02-13 03:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35][root][INFO] - Training Epoch: 1/2, step 5732/7134 completed (loss: 0.1456104964017868, acc: 0.9533980488777161)
[2025-02-13 03:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35][root][INFO] - Training Epoch: 1/2, step 5733/7134 completed (loss: 0.03390178829431534, acc: 0.9890710115432739)
[2025-02-13 03:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:36][root][INFO] - Training Epoch: 1/2, step 5734/7134 completed (loss: 0.01675134338438511, acc: 0.9950000047683716)
[2025-02-13 03:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:36][root][INFO] - Training Epoch: 1/2, step 5735/7134 completed (loss: 0.0648961216211319, acc: 0.9784946441650391)
[2025-02-13 03:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37][root][INFO] - Training Epoch: 1/2, step 5736/7134 completed (loss: 0.07847356051206589, acc: 0.9774436354637146)
[2025-02-13 03:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37][root][INFO] - Training Epoch: 1/2, step 5737/7134 completed (loss: 0.05433934926986694, acc: 0.981873095035553)
[2025-02-13 03:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37][root][INFO] - Training Epoch: 1/2, step 5738/7134 completed (loss: 0.055567216128110886, acc: 0.9794437885284424)
[2025-02-13 03:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:38][root][INFO] - Training Epoch: 1/2, step 5739/7134 completed (loss: 0.08369651436805725, acc: 0.9684210419654846)
[2025-02-13 03:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:38][root][INFO] - Training Epoch: 1/2, step 5740/7134 completed (loss: 0.037069790065288544, acc: 0.9846547245979309)
[2025-02-13 03:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39][root][INFO] - Training Epoch: 1/2, step 5741/7134 completed (loss: 0.049839507788419724, acc: 0.9884467124938965)
[2025-02-13 03:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39][root][INFO] - Training Epoch: 1/2, step 5742/7134 completed (loss: 0.05329141020774841, acc: 0.9829171895980835)
[2025-02-13 03:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40][root][INFO] - Training Epoch: 1/2, step 5743/7134 completed (loss: 0.03701939806342125, acc: 0.9877451062202454)
[2025-02-13 03:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40][root][INFO] - Training Epoch: 1/2, step 5744/7134 completed (loss: 0.028885092586278915, acc: 0.9895397424697876)
[2025-02-13 03:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40][root][INFO] - Training Epoch: 1/2, step 5745/7134 completed (loss: 0.035010453313589096, acc: 0.9870466589927673)
[2025-02-13 03:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41][root][INFO] - Training Epoch: 1/2, step 5746/7134 completed (loss: 0.06543245166540146, acc: 0.9897058606147766)
[2025-02-13 03:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41][root][INFO] - Training Epoch: 1/2, step 5747/7134 completed (loss: 0.026996657252311707, acc: 0.990304708480835)
[2025-02-13 03:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42][root][INFO] - Training Epoch: 1/2, step 5748/7134 completed (loss: 0.036013081669807434, acc: 0.9872029423713684)
[2025-02-13 03:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42][root][INFO] - Training Epoch: 1/2, step 5749/7134 completed (loss: 0.03283362463116646, acc: 0.9908376932144165)
[2025-02-13 03:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42][root][INFO] - Training Epoch: 1/2, step 5750/7134 completed (loss: 0.06423307955265045, acc: 0.9777227640151978)
[2025-02-13 03:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:43][root][INFO] - Training Epoch: 1/2, step 5751/7134 completed (loss: 0.06135676056146622, acc: 0.9807445406913757)
[2025-02-13 03:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:43][root][INFO] - Training Epoch: 1/2, step 5752/7134 completed (loss: 0.075043685734272, acc: 0.9837586879730225)
[2025-02-13 03:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:44][root][INFO] - Training Epoch: 1/2, step 5753/7134 completed (loss: 0.08984556049108505, acc: 0.971875011920929)
[2025-02-13 03:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:44][root][INFO] - Training Epoch: 1/2, step 5754/7134 completed (loss: 0.06154189258813858, acc: 0.9821656346321106)
[2025-02-13 03:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45][root][INFO] - Training Epoch: 1/2, step 5755/7134 completed (loss: 0.055811021476984024, acc: 0.985358715057373)
[2025-02-13 03:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45][root][INFO] - Training Epoch: 1/2, step 5756/7134 completed (loss: 0.05907580628991127, acc: 0.9827127456665039)
[2025-02-13 03:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45][root][INFO] - Training Epoch: 1/2, step 5757/7134 completed (loss: 0.030821330845355988, acc: 0.9910600185394287)
[2025-02-13 03:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:46][root][INFO] - Training Epoch: 1/2, step 5758/7134 completed (loss: 0.0381610244512558, acc: 0.9867647290229797)
[2025-02-13 03:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:46][root][INFO] - Training Epoch: 1/2, step 5759/7134 completed (loss: 0.05749265104532242, acc: 0.9811320900917053)
[2025-02-13 03:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47][root][INFO] - Training Epoch: 1/2, step 5760/7134 completed (loss: 0.0359334722161293, acc: 0.9923664331436157)
[2025-02-13 03:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47][root][INFO] - Training Epoch: 1/2, step 5761/7134 completed (loss: 0.04294514283537865, acc: 0.9816849827766418)
[2025-02-13 03:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47][root][INFO] - Training Epoch: 1/2, step 5762/7134 completed (loss: 0.0540316142141819, acc: 0.985401451587677)
[2025-02-13 03:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:48][root][INFO] - Training Epoch: 1/2, step 5763/7134 completed (loss: 0.04318348318338394, acc: 0.9887076616287231)
[2025-02-13 03:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:48][root][INFO] - Training Epoch: 1/2, step 5764/7134 completed (loss: 0.037753764539957047, acc: 0.9897435903549194)
[2025-02-13 03:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49][root][INFO] - Training Epoch: 1/2, step 5765/7134 completed (loss: 0.020440639927983284, acc: 0.9957982897758484)
[2025-02-13 03:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49][root][INFO] - Training Epoch: 1/2, step 5766/7134 completed (loss: 0.03470991179347038, acc: 0.9955157041549683)
[2025-02-13 03:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49][root][INFO] - Training Epoch: 1/2, step 5767/7134 completed (loss: 0.01270454004406929, acc: 0.9972183704376221)
[2025-02-13 03:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:50][root][INFO] - Training Epoch: 1/2, step 5768/7134 completed (loss: 0.02325417473912239, acc: 0.9910179376602173)
[2025-02-13 03:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:50][root][INFO] - Training Epoch: 1/2, step 5769/7134 completed (loss: 0.009211462922394276, acc: 0.9967266917228699)
[2025-02-13 03:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51][root][INFO] - Training Epoch: 1/2, step 5770/7134 completed (loss: 0.04854113236069679, acc: 0.9877601265907288)
[2025-02-13 03:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51][root][INFO] - Training Epoch: 1/2, step 5771/7134 completed (loss: 0.0514361597597599, acc: 0.98777174949646)
[2025-02-13 03:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52][root][INFO] - Training Epoch: 1/2, step 5772/7134 completed (loss: 0.02184358984231949, acc: 0.991304337978363)
[2025-02-13 03:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52][root][INFO] - Training Epoch: 1/2, step 5773/7134 completed (loss: 0.01634409837424755, acc: 0.9933110475540161)
[2025-02-13 03:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52][root][INFO] - Training Epoch: 1/2, step 5774/7134 completed (loss: 0.036877017468214035, acc: 0.9836795330047607)
[2025-02-13 03:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53][root][INFO] - Training Epoch: 1/2, step 5775/7134 completed (loss: 0.028423164039850235, acc: 0.9917627573013306)
[2025-02-13 03:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53][root][INFO] - Training Epoch: 1/2, step 5776/7134 completed (loss: 0.04839107394218445, acc: 0.983146071434021)
[2025-02-13 03:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54][root][INFO] - Training Epoch: 1/2, step 5777/7134 completed (loss: 0.07679414004087448, acc: 0.9814356565475464)
[2025-02-13 03:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54][root][INFO] - Training Epoch: 1/2, step 5778/7134 completed (loss: 0.08293698728084564, acc: 0.9759615659713745)
[2025-02-13 03:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55][root][INFO] - Training Epoch: 1/2, step 5779/7134 completed (loss: 0.058846935629844666, acc: 0.9889958500862122)
[2025-02-13 03:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55][root][INFO] - Training Epoch: 1/2, step 5780/7134 completed (loss: 0.03263293579220772, acc: 0.9904534816741943)
[2025-02-13 03:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55][root][INFO] - Training Epoch: 1/2, step 5781/7134 completed (loss: 0.05632070079445839, acc: 0.9820442199707031)
[2025-02-13 03:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56][root][INFO] - Training Epoch: 1/2, step 5782/7134 completed (loss: 0.015925345942378044, acc: 0.9935232996940613)
[2025-02-13 03:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56][root][INFO] - Training Epoch: 1/2, step 5783/7134 completed (loss: 0.045185014605522156, acc: 0.9893758296966553)
[2025-02-13 03:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:57][root][INFO] - Training Epoch: 1/2, step 5784/7134 completed (loss: 0.04190885275602341, acc: 0.9878869652748108)
[2025-02-13 03:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:57][root][INFO] - Training Epoch: 1/2, step 5785/7134 completed (loss: 0.05000438913702965, acc: 0.986522912979126)
[2025-02-13 03:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58][root][INFO] - Training Epoch: 1/2, step 5786/7134 completed (loss: 0.06914598494768143, acc: 0.9754098653793335)
[2025-02-13 03:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58][root][INFO] - Training Epoch: 1/2, step 5787/7134 completed (loss: 0.03307976573705673, acc: 0.9888059496879578)
[2025-02-13 03:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58][root][INFO] - Training Epoch: 1/2, step 5788/7134 completed (loss: 0.01758950762450695, acc: 0.9953846335411072)
[2025-02-13 03:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59][root][INFO] - Training Epoch: 1/2, step 5789/7134 completed (loss: 0.05815199390053749, acc: 0.9826302528381348)
[2025-02-13 03:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59][root][INFO] - Training Epoch: 1/2, step 5790/7134 completed (loss: 0.040296100080013275, acc: 0.9848901033401489)
[2025-02-13 03:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:00][root][INFO] - Training Epoch: 1/2, step 5791/7134 completed (loss: 0.0643114373087883, acc: 0.9793814420700073)
[2025-02-13 03:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:00][root][INFO] - Training Epoch: 1/2, step 5792/7134 completed (loss: 0.06422362476587296, acc: 0.9813242554664612)
[2025-02-13 03:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01][root][INFO] - Training Epoch: 1/2, step 5793/7134 completed (loss: 0.06975609064102173, acc: 0.9811320900917053)
[2025-02-13 03:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01][root][INFO] - Training Epoch: 1/2, step 5794/7134 completed (loss: 0.06239014118909836, acc: 0.9818181991577148)
[2025-02-13 03:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01][root][INFO] - Training Epoch: 1/2, step 5795/7134 completed (loss: 0.1095464900135994, acc: 0.9726890921592712)
[2025-02-13 03:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:02][root][INFO] - Training Epoch: 1/2, step 5796/7134 completed (loss: 0.04175999015569687, acc: 0.9864636063575745)
[2025-02-13 03:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:02][root][INFO] - Training Epoch: 1/2, step 5797/7134 completed (loss: 0.047442201524972916, acc: 0.9824000000953674)
[2025-02-13 03:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03][root][INFO] - Training Epoch: 1/2, step 5798/7134 completed (loss: 0.17979350686073303, acc: 0.9617391228675842)
[2025-02-13 03:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03][root][INFO] - Training Epoch: 1/2, step 5799/7134 completed (loss: 0.0866844654083252, acc: 0.9723865985870361)
[2025-02-13 03:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03][root][INFO] - Training Epoch: 1/2, step 5800/7134 completed (loss: 0.07994382083415985, acc: 0.9724264740943909)
[2025-02-13 03:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:04][root][INFO] - Training Epoch: 1/2, step 5801/7134 completed (loss: 0.06707712262868881, acc: 0.9817073345184326)
[2025-02-13 03:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:04][root][INFO] - Training Epoch: 1/2, step 5802/7134 completed (loss: 0.042758382856845856, acc: 0.9907578825950623)
[2025-02-13 03:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05][root][INFO] - Training Epoch: 1/2, step 5803/7134 completed (loss: 0.0277805645018816, acc: 0.9936224222183228)
[2025-02-13 03:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05][root][INFO] - Training Epoch: 1/2, step 5804/7134 completed (loss: 0.10378985106945038, acc: 0.9675236940383911)
[2025-02-13 03:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05][root][INFO] - Training Epoch: 1/2, step 5805/7134 completed (loss: 0.05254806578159332, acc: 0.9856114983558655)
[2025-02-13 03:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:06][root][INFO] - Training Epoch: 1/2, step 5806/7134 completed (loss: 0.05395353212952614, acc: 0.9877049326896667)
[2025-02-13 03:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:06][root][INFO] - Training Epoch: 1/2, step 5807/7134 completed (loss: 0.07451792806386948, acc: 0.97579425573349)
[2025-02-13 03:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:07][root][INFO] - Training Epoch: 1/2, step 5808/7134 completed (loss: 0.038019560277462006, acc: 0.9887482523918152)
[2025-02-13 03:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:07][root][INFO] - Training Epoch: 1/2, step 5809/7134 completed (loss: 0.049552179872989655, acc: 0.9912280440330505)
[2025-02-13 03:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08][root][INFO] - Training Epoch: 1/2, step 5810/7134 completed (loss: 0.04089798033237457, acc: 0.9898580312728882)
[2025-02-13 03:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08][root][INFO] - Training Epoch: 1/2, step 5811/7134 completed (loss: 0.05174550786614418, acc: 0.983132541179657)
[2025-02-13 03:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08][root][INFO] - Training Epoch: 1/2, step 5812/7134 completed (loss: 0.014681403525173664, acc: 0.9966996908187866)
[2025-02-13 03:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:09][root][INFO] - Training Epoch: 1/2, step 5813/7134 completed (loss: 0.027491046115756035, acc: 0.99245285987854)
[2025-02-13 03:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:09][root][INFO] - Training Epoch: 1/2, step 5814/7134 completed (loss: 0.028039926663041115, acc: 0.9932318329811096)
[2025-02-13 03:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:09][root][INFO] - Training Epoch: 1/2, step 5815/7134 completed (loss: 0.04710446298122406, acc: 0.9849340915679932)
[2025-02-13 03:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10][root][INFO] - Training Epoch: 1/2, step 5816/7134 completed (loss: 0.01155104674398899, acc: 0.9950124621391296)
[2025-02-13 03:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10][root][INFO] - Training Epoch: 1/2, step 5817/7134 completed (loss: 0.0787765234708786, acc: 0.984886646270752)
[2025-02-13 03:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][root][INFO] - Training Epoch: 1/2, step 5818/7134 completed (loss: 0.045546483248472214, acc: 0.9851852059364319)
[2025-02-13 03:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][root][INFO] - Training Epoch: 1/2, step 5819/7134 completed (loss: 0.04969332367181778, acc: 0.9844827651977539)
[2025-02-13 03:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][root][INFO] - Training Epoch: 1/2, step 5820/7134 completed (loss: 0.05822475627064705, acc: 0.9876033067703247)
[2025-02-13 03:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:12][root][INFO] - Training Epoch: 1/2, step 5821/7134 completed (loss: 0.0118101816624403, acc: 0.9969183206558228)
[2025-02-13 03:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:12][root][INFO] - Training Epoch: 1/2, step 5822/7134 completed (loss: 0.035143349319696426, acc: 0.9887096881866455)
[2025-02-13 03:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13][root][INFO] - Training Epoch: 1/2, step 5823/7134 completed (loss: 0.030087558552622795, acc: 0.9922239780426025)
[2025-02-13 03:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13][root][INFO] - Training Epoch: 1/2, step 5824/7134 completed (loss: 0.023914175108075142, acc: 0.996889591217041)
[2025-02-13 03:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13][root][INFO] - Training Epoch: 1/2, step 5825/7134 completed (loss: 0.04748491942882538, acc: 0.9861591458320618)
[2025-02-13 03:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14][root][INFO] - Training Epoch: 1/2, step 5826/7134 completed (loss: 0.021228469908237457, acc: 0.9946808218955994)
[2025-02-13 03:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14][root][INFO] - Training Epoch: 1/2, step 5827/7134 completed (loss: 0.025775332003831863, acc: 0.9923076629638672)
[2025-02-13 03:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15][root][INFO] - Training Epoch: 1/2, step 5828/7134 completed (loss: 0.04810234159231186, acc: 0.9851411581039429)
[2025-02-13 03:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15][root][INFO] - Training Epoch: 1/2, step 5829/7134 completed (loss: 0.04615858569741249, acc: 0.9894598126411438)
[2025-02-13 03:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15][root][INFO] - Training Epoch: 1/2, step 5830/7134 completed (loss: 0.03765286132693291, acc: 0.9867674708366394)
[2025-02-13 03:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16][root][INFO] - Training Epoch: 1/2, step 5831/7134 completed (loss: 0.019000336527824402, acc: 0.9953488111495972)
[2025-02-13 03:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16][root][INFO] - Training Epoch: 1/2, step 5832/7134 completed (loss: 0.052531324326992035, acc: 0.9881756901741028)
[2025-02-13 03:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17][root][INFO] - Training Epoch: 1/2, step 5833/7134 completed (loss: 0.04125778004527092, acc: 0.9874551892280579)
[2025-02-13 03:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17][root][INFO] - Training Epoch: 1/2, step 5834/7134 completed (loss: 0.029246190562844276, acc: 0.9958333373069763)
[2025-02-13 03:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17][root][INFO] - Training Epoch: 1/2, step 5835/7134 completed (loss: 0.012854553759098053, acc: 0.9956647157669067)
[2025-02-13 03:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18][root][INFO] - Training Epoch: 1/2, step 5836/7134 completed (loss: 0.040493205189704895, acc: 0.9899135231971741)
[2025-02-13 03:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18][root][INFO] - Training Epoch: 1/2, step 5837/7134 completed (loss: 0.08402708917856216, acc: 0.9814189076423645)
[2025-02-13 03:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:19][root][INFO] - Training Epoch: 1/2, step 5838/7134 completed (loss: 0.05234117433428764, acc: 0.9917627573013306)
[2025-02-13 03:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:19][root][INFO] - Training Epoch: 1/2, step 5839/7134 completed (loss: 0.02216644398868084, acc: 0.9946428537368774)
[2025-02-13 03:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20][root][INFO] - Training Epoch: 1/2, step 5840/7134 completed (loss: 0.023638855665922165, acc: 0.9920508861541748)
[2025-02-13 03:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20][root][INFO] - Training Epoch: 1/2, step 5841/7134 completed (loss: 0.05780065804719925, acc: 0.9872408509254456)
[2025-02-13 03:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20][root][INFO] - Training Epoch: 1/2, step 5842/7134 completed (loss: 0.044682055711746216, acc: 0.9855072498321533)
[2025-02-13 03:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21][root][INFO] - Training Epoch: 1/2, step 5843/7134 completed (loss: 0.049118030816316605, acc: 0.9866130948066711)
[2025-02-13 03:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21][root][INFO] - Training Epoch: 1/2, step 5844/7134 completed (loss: 0.024092890322208405, acc: 0.9943100810050964)
[2025-02-13 03:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22][root][INFO] - Training Epoch: 1/2, step 5845/7134 completed (loss: 0.030460333451628685, acc: 0.9900285005569458)
[2025-02-13 03:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22][root][INFO] - Training Epoch: 1/2, step 5846/7134 completed (loss: 0.04186365753412247, acc: 0.9904240965843201)
[2025-02-13 03:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22][root][INFO] - Training Epoch: 1/2, step 5847/7134 completed (loss: 0.02160804532468319, acc: 0.9931318759918213)
[2025-02-13 03:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:23][root][INFO] - Training Epoch: 1/2, step 5848/7134 completed (loss: 0.04776785522699356, acc: 0.9866071343421936)
[2025-02-13 03:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:23][root][INFO] - Training Epoch: 1/2, step 5849/7134 completed (loss: 0.06064719334244728, acc: 0.9808259606361389)
[2025-02-13 03:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24][root][INFO] - Training Epoch: 1/2, step 5850/7134 completed (loss: 0.020639345049858093, acc: 0.9938931465148926)
[2025-02-13 03:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24][root][INFO] - Training Epoch: 1/2, step 5851/7134 completed (loss: 0.024795372039079666, acc: 0.9934210777282715)
[2025-02-13 03:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24][root][INFO] - Training Epoch: 1/2, step 5852/7134 completed (loss: 0.02208709716796875, acc: 0.9927007555961609)
[2025-02-13 03:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:25][root][INFO] - Training Epoch: 1/2, step 5853/7134 completed (loss: 0.049628809094429016, acc: 0.9878214001655579)
[2025-02-13 03:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:25][root][INFO] - Training Epoch: 1/2, step 5854/7134 completed (loss: 0.026666395366191864, acc: 0.9876106381416321)
[2025-02-13 03:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:26][root][INFO] - Training Epoch: 1/2, step 5855/7134 completed (loss: 0.05190252512693405, acc: 0.9792746305465698)
[2025-02-13 03:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:26][root][INFO] - Training Epoch: 1/2, step 5856/7134 completed (loss: 0.0955071672797203, acc: 0.9773414134979248)
[2025-02-13 03:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:26][root][INFO] - Training Epoch: 1/2, step 5857/7134 completed (loss: 0.036549679934978485, acc: 0.9873617887496948)
[2025-02-13 03:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:27][root][INFO] - Training Epoch: 1/2, step 5858/7134 completed (loss: 0.031685736030340195, acc: 0.9889094233512878)
[2025-02-13 03:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:27][root][INFO] - Training Epoch: 1/2, step 5859/7134 completed (loss: 0.050008200109004974, acc: 0.986328125)
[2025-02-13 03:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28][root][INFO] - Training Epoch: 1/2, step 5860/7134 completed (loss: 0.04160579666495323, acc: 0.9752066135406494)
[2025-02-13 03:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28][root][INFO] - Training Epoch: 1/2, step 5861/7134 completed (loss: 0.04993375763297081, acc: 0.9858012199401855)
[2025-02-13 03:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28][root][INFO] - Training Epoch: 1/2, step 5862/7134 completed (loss: 0.0351986363530159, acc: 0.9871244430541992)
[2025-02-13 03:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:29][root][INFO] - Training Epoch: 1/2, step 5863/7134 completed (loss: 0.05469711869955063, acc: 0.9823232293128967)
[2025-02-13 03:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:29][root][INFO] - Training Epoch: 1/2, step 5864/7134 completed (loss: 0.07688780874013901, acc: 0.9820359349250793)
[2025-02-13 03:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30][root][INFO] - Training Epoch: 1/2, step 5865/7134 completed (loss: 0.05244216322898865, acc: 0.9873015880584717)
[2025-02-13 03:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30][root][INFO] - Training Epoch: 1/2, step 5866/7134 completed (loss: 0.03471066430211067, acc: 0.9920634627342224)
[2025-02-13 03:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30][root][INFO] - Training Epoch: 1/2, step 5867/7134 completed (loss: 0.03050536848604679, acc: 0.9900826215744019)
[2025-02-13 03:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:31][root][INFO] - Training Epoch: 1/2, step 5868/7134 completed (loss: 0.037147458642721176, acc: 0.9896551966667175)
[2025-02-13 03:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:31][root][INFO] - Training Epoch: 1/2, step 5869/7134 completed (loss: 0.05208384990692139, acc: 0.983582079410553)
[2025-02-13 03:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32][root][INFO] - Training Epoch: 1/2, step 5870/7134 completed (loss: 0.06382488459348679, acc: 0.9790794849395752)
[2025-02-13 03:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32][root][INFO] - Training Epoch: 1/2, step 5871/7134 completed (loss: 0.025438902899622917, acc: 0.9830148816108704)
[2025-02-13 03:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32][root][INFO] - Training Epoch: 1/2, step 5872/7134 completed (loss: 0.05509205535054207, acc: 0.9878234267234802)
[2025-02-13 03:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:33][root][INFO] - Training Epoch: 1/2, step 5873/7134 completed (loss: 0.04145682603120804, acc: 0.9906666874885559)
[2025-02-13 03:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:33][root][INFO] - Training Epoch: 1/2, step 5874/7134 completed (loss: 0.01917683146893978, acc: 0.9939758777618408)
[2025-02-13 03:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34][root][INFO] - Training Epoch: 1/2, step 5875/7134 completed (loss: 0.07192208617925644, acc: 0.9805309772491455)
[2025-02-13 03:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34][root][INFO] - Training Epoch: 1/2, step 5876/7134 completed (loss: 0.02626822516322136, acc: 0.9927184581756592)
[2025-02-13 03:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34][root][INFO] - Training Epoch: 1/2, step 5877/7134 completed (loss: 0.06680358201265335, acc: 0.9847715497016907)
[2025-02-13 03:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35][root][INFO] - Training Epoch: 1/2, step 5878/7134 completed (loss: 0.028370197862386703, acc: 0.9912280440330505)
[2025-02-13 03:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35][root][INFO] - Training Epoch: 1/2, step 5879/7134 completed (loss: 0.01943807676434517, acc: 1.0)
[2025-02-13 03:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35][root][INFO] - Training Epoch: 1/2, step 5880/7134 completed (loss: 0.0209701806306839, acc: 0.995555579662323)
[2025-02-13 03:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36][root][INFO] - Training Epoch: 1/2, step 5881/7134 completed (loss: 0.018812159076333046, acc: 0.9942418336868286)
[2025-02-13 03:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36][root][INFO] - Training Epoch: 1/2, step 5882/7134 completed (loss: 0.026468483731150627, acc: 0.9905882477760315)
[2025-02-13 03:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36][root][INFO] - Training Epoch: 1/2, step 5883/7134 completed (loss: 0.040373820811510086, acc: 0.9868913888931274)
[2025-02-13 03:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:37][root][INFO] - Training Epoch: 1/2, step 5884/7134 completed (loss: 0.09931284934282303, acc: 0.9671533107757568)
[2025-02-13 03:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:37][root][INFO] - Training Epoch: 1/2, step 5885/7134 completed (loss: 0.09460872411727905, acc: 0.9755434989929199)
[2025-02-13 03:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38][root][INFO] - Training Epoch: 1/2, step 5886/7134 completed (loss: 0.02306382916867733, acc: 0.9942857027053833)
[2025-02-13 03:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38][root][INFO] - Training Epoch: 1/2, step 5887/7134 completed (loss: 0.043592698872089386, acc: 0.9893842935562134)
[2025-02-13 03:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38][root][INFO] - Training Epoch: 1/2, step 5888/7134 completed (loss: 0.05467570945620537, acc: 0.9831081032752991)
[2025-02-13 03:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39][root][INFO] - Training Epoch: 1/2, step 5889/7134 completed (loss: 0.013195200823247433, acc: 1.0)
[2025-02-13 03:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39][root][INFO] - Training Epoch: 1/2, step 5890/7134 completed (loss: 0.019630244001746178, acc: 0.9947552680969238)
[2025-02-13 03:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39][root][INFO] - Training Epoch: 1/2, step 5891/7134 completed (loss: 0.033624038100242615, acc: 0.988950252532959)
[2025-02-13 03:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40][root][INFO] - Training Epoch: 1/2, step 5892/7134 completed (loss: 0.0780731812119484, acc: 0.9839650392532349)
[2025-02-13 03:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40][root][INFO] - Training Epoch: 1/2, step 5893/7134 completed (loss: 0.05596895515918732, acc: 0.9811023473739624)
[2025-02-13 03:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41][root][INFO] - Training Epoch: 1/2, step 5894/7134 completed (loss: 0.015537142753601074, acc: 0.9967213273048401)
[2025-02-13 03:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41][root][INFO] - Training Epoch: 1/2, step 5895/7134 completed (loss: 0.015662243589758873, acc: 0.9904761910438538)
[2025-02-13 03:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41][root][INFO] - Training Epoch: 1/2, step 5896/7134 completed (loss: 0.057347070425748825, acc: 0.9897959232330322)
[2025-02-13 03:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:42][root][INFO] - Training Epoch: 1/2, step 5897/7134 completed (loss: 0.021530980244278908, acc: 0.994490385055542)
[2025-02-13 03:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:42][root][INFO] - Training Epoch: 1/2, step 5898/7134 completed (loss: 0.02603023126721382, acc: 0.9956896305084229)
[2025-02-13 03:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43][root][INFO] - Training Epoch: 1/2, step 5899/7134 completed (loss: 0.021456679329276085, acc: 0.9940476417541504)
[2025-02-13 03:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43][root][INFO] - Training Epoch: 1/2, step 5900/7134 completed (loss: 0.027607068419456482, acc: 0.988095223903656)
[2025-02-13 03:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43][root][INFO] - Training Epoch: 1/2, step 5901/7134 completed (loss: 0.0760471373796463, acc: 0.9850000143051147)
[2025-02-13 03:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:44][root][INFO] - Training Epoch: 1/2, step 5902/7134 completed (loss: 0.10640289634466171, acc: 0.9664948582649231)
[2025-02-13 03:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:44][root][INFO] - Training Epoch: 1/2, step 5903/7134 completed (loss: 0.07222141325473785, acc: 0.9781022071838379)
[2025-02-13 03:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:44][root][INFO] - Training Epoch: 1/2, step 5904/7134 completed (loss: 0.05490509420633316, acc: 0.9808743000030518)
[2025-02-13 03:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:45][root][INFO] - Training Epoch: 1/2, step 5905/7134 completed (loss: 0.05681062489748001, acc: 0.9805285334587097)
[2025-02-13 03:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:45][root][INFO] - Training Epoch: 1/2, step 5906/7134 completed (loss: 0.0812530517578125, acc: 0.977886974811554)
[2025-02-13 03:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:46][root][INFO] - Training Epoch: 1/2, step 5907/7134 completed (loss: 0.054842278361320496, acc: 0.9793014526367188)
[2025-02-13 03:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:46][root][INFO] - Training Epoch: 1/2, step 5908/7134 completed (loss: 0.043439287692308426, acc: 0.9833759665489197)
[2025-02-13 03:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:47][root][INFO] - Training Epoch: 1/2, step 5909/7134 completed (loss: 0.05377711355686188, acc: 0.9855907559394836)
[2025-02-13 03:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:47][root][INFO] - Training Epoch: 1/2, step 5910/7134 completed (loss: 0.05452181026339531, acc: 0.9750346541404724)
[2025-02-13 03:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48][root][INFO] - Training Epoch: 1/2, step 5911/7134 completed (loss: 0.05411645397543907, acc: 0.9834710955619812)
[2025-02-13 03:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48][root][INFO] - Training Epoch: 1/2, step 5912/7134 completed (loss: 0.03650058060884476, acc: 0.9817159175872803)
[2025-02-13 03:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48][root][INFO] - Training Epoch: 1/2, step 5913/7134 completed (loss: 0.11183584481477737, acc: 0.9728260636329651)
[2025-02-13 03:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:49][root][INFO] - Training Epoch: 1/2, step 5914/7134 completed (loss: 0.028208259493112564, acc: 0.9903692007064819)
[2025-02-13 03:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:49][root][INFO] - Training Epoch: 1/2, step 5915/7134 completed (loss: 0.04841659963130951, acc: 0.9813953638076782)
[2025-02-13 03:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:50][root][INFO] - Training Epoch: 1/2, step 5916/7134 completed (loss: 0.050170741975307465, acc: 0.9868938326835632)
[2025-02-13 03:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:50][root][INFO] - Training Epoch: 1/2, step 5917/7134 completed (loss: 0.04022844135761261, acc: 0.9896013736724854)
[2025-02-13 03:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51][root][INFO] - Training Epoch: 1/2, step 5918/7134 completed (loss: 0.08813855051994324, acc: 0.9811320900917053)
[2025-02-13 03:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51][root][INFO] - Training Epoch: 1/2, step 5919/7134 completed (loss: 0.041805390268564224, acc: 0.9878419637680054)
[2025-02-13 03:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51][root][INFO] - Training Epoch: 1/2, step 5920/7134 completed (loss: 0.04871992766857147, acc: 0.9833610653877258)
[2025-02-13 03:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:52][root][INFO] - Training Epoch: 1/2, step 5921/7134 completed (loss: 0.07353387773036957, acc: 0.9749478101730347)
[2025-02-13 03:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:52][root][INFO] - Training Epoch: 1/2, step 5922/7134 completed (loss: 0.19056393206119537, acc: 0.9556737542152405)
[2025-02-13 03:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53][root][INFO] - Training Epoch: 1/2, step 5923/7134 completed (loss: 0.10916457325220108, acc: 0.966325044631958)
[2025-02-13 03:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53][root][INFO] - Training Epoch: 1/2, step 5924/7134 completed (loss: 0.05887037143111229, acc: 0.9809069037437439)
[2025-02-13 03:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53][root][INFO] - Training Epoch: 1/2, step 5925/7134 completed (loss: 0.24058064818382263, acc: 0.9530791640281677)
[2025-02-13 03:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:54][root][INFO] - Training Epoch: 1/2, step 5926/7134 completed (loss: 0.0986877903342247, acc: 0.9733333587646484)
[2025-02-13 03:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:54][root][INFO] - Training Epoch: 1/2, step 5927/7134 completed (loss: 0.10150529444217682, acc: 0.9710467457771301)
[2025-02-13 03:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55][root][INFO] - Training Epoch: 1/2, step 5928/7134 completed (loss: 0.20998252928256989, acc: 0.9589040875434875)
[2025-02-13 03:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55][root][INFO] - Training Epoch: 1/2, step 5929/7134 completed (loss: 0.04610249027609825, acc: 0.983208954334259)
[2025-02-13 03:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55][root][INFO] - Training Epoch: 1/2, step 5930/7134 completed (loss: 0.10243664681911469, acc: 0.9755011200904846)
[2025-02-13 03:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56][root][INFO] - Training Epoch: 1/2, step 5931/7134 completed (loss: 0.05890501290559769, acc: 0.9827213883399963)
[2025-02-13 03:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56][root][INFO] - Training Epoch: 1/2, step 5932/7134 completed (loss: 0.11181957274675369, acc: 0.9685184955596924)
[2025-02-13 03:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56][root][INFO] - Training Epoch: 1/2, step 5933/7134 completed (loss: 0.1155354455113411, acc: 0.9685534834861755)
[2025-02-13 03:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:57][root][INFO] - Training Epoch: 1/2, step 5934/7134 completed (loss: 0.05296330153942108, acc: 0.9822379946708679)
[2025-02-13 03:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:57][root][INFO] - Training Epoch: 1/2, step 5935/7134 completed (loss: 0.04710937663912773, acc: 0.9902912378311157)
[2025-02-13 03:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58][root][INFO] - Training Epoch: 1/2, step 5936/7134 completed (loss: 0.07703300565481186, acc: 0.9831546545028687)
[2025-02-13 03:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58][root][INFO] - Training Epoch: 1/2, step 5937/7134 completed (loss: 0.030179036781191826, acc: 0.9920254945755005)
[2025-02-13 03:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58][root][INFO] - Training Epoch: 1/2, step 5938/7134 completed (loss: 0.0946609377861023, acc: 0.9732313752174377)
[2025-02-13 03:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:59][root][INFO] - Training Epoch: 1/2, step 5939/7134 completed (loss: 0.19571809470653534, acc: 0.9479637742042542)
[2025-02-13 03:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:59][root][INFO] - Training Epoch: 1/2, step 5940/7134 completed (loss: 0.06993896514177322, acc: 0.9737609624862671)
[2025-02-13 03:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00][root][INFO] - Training Epoch: 1/2, step 5941/7134 completed (loss: 0.10896965116262436, acc: 0.9750000238418579)
[2025-02-13 03:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00][root][INFO] - Training Epoch: 1/2, step 5942/7134 completed (loss: 0.021552955731749535, acc: 0.9942638874053955)
[2025-02-13 03:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00][root][INFO] - Training Epoch: 1/2, step 5943/7134 completed (loss: 0.022495200857520103, acc: 0.995708167552948)
[2025-02-13 03:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:01][root][INFO] - Training Epoch: 1/2, step 5944/7134 completed (loss: 0.0727180540561676, acc: 0.9790576100349426)
[2025-02-13 03:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:01][root][INFO] - Training Epoch: 1/2, step 5945/7134 completed (loss: 0.031656015664339066, acc: 0.9894894957542419)
[2025-02-13 03:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:02][root][INFO] - Training Epoch: 1/2, step 5946/7134 completed (loss: 0.04008402302861214, acc: 0.9873188138008118)
[2025-02-13 03:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:02][root][INFO] - Training Epoch: 1/2, step 5947/7134 completed (loss: 0.04533974826335907, acc: 0.9851301312446594)
[2025-02-13 03:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03][root][INFO] - Training Epoch: 1/2, step 5948/7134 completed (loss: 0.07065517455339432, acc: 0.972000002861023)
[2025-02-13 03:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03][root][INFO] - Training Epoch: 1/2, step 5949/7134 completed (loss: 0.033948127180337906, acc: 0.9899623394012451)
[2025-02-13 03:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03][root][INFO] - Training Epoch: 1/2, step 5950/7134 completed (loss: 0.12594252824783325, acc: 0.9667458534240723)
[2025-02-13 03:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:04][root][INFO] - Training Epoch: 1/2, step 5951/7134 completed (loss: 0.07515978068113327, acc: 0.9784792065620422)
[2025-02-13 03:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:04][root][INFO] - Training Epoch: 1/2, step 5952/7134 completed (loss: 0.1975766122341156, acc: 0.9671875238418579)
[2025-02-13 03:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05][root][INFO] - Training Epoch: 1/2, step 5953/7134 completed (loss: 0.06340589374303818, acc: 0.9779614210128784)
[2025-02-13 03:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05][root][INFO] - Training Epoch: 1/2, step 5954/7134 completed (loss: 0.04473887011408806, acc: 0.9822335243225098)
[2025-02-13 03:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05][root][INFO] - Training Epoch: 1/2, step 5955/7134 completed (loss: 0.06552975624799728, acc: 0.9857142567634583)
[2025-02-13 03:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06][root][INFO] - Training Epoch: 1/2, step 5956/7134 completed (loss: 0.09142211079597473, acc: 0.9745097756385803)
[2025-02-13 03:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06][root][INFO] - Training Epoch: 1/2, step 5957/7134 completed (loss: 0.05368597432971001, acc: 0.9850075244903564)
[2025-02-13 03:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06][root][INFO] - Training Epoch: 1/2, step 5958/7134 completed (loss: 0.037716902792453766, acc: 0.9879194498062134)
[2025-02-13 03:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:07][root][INFO] - Training Epoch: 1/2, step 5959/7134 completed (loss: 0.045910995453596115, acc: 0.9879840016365051)
[2025-02-13 03:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:07][root][INFO] - Training Epoch: 1/2, step 5960/7134 completed (loss: 0.010938319377601147, acc: 0.9985954761505127)
[2025-02-13 03:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08][root][INFO] - Training Epoch: 1/2, step 5961/7134 completed (loss: 0.05487177520990372, acc: 0.9815573692321777)
[2025-02-13 03:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08][root][INFO] - Training Epoch: 1/2, step 5962/7134 completed (loss: 0.018223712220788002, acc: 0.9934959411621094)
[2025-02-13 03:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08][root][INFO] - Training Epoch: 1/2, step 5963/7134 completed (loss: 0.026634667068719864, acc: 0.9902234673500061)
[2025-02-13 03:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09][root][INFO] - Training Epoch: 1/2, step 5964/7134 completed (loss: 0.030433837324380875, acc: 0.9907578825950623)
[2025-02-13 03:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09][root][INFO] - Training Epoch: 1/2, step 5965/7134 completed (loss: 0.06818373501300812, acc: 0.9865067601203918)
[2025-02-13 03:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:10][root][INFO] - Training Epoch: 1/2, step 5966/7134 completed (loss: 0.04956996813416481, acc: 0.9861303567886353)
[2025-02-13 03:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:10][root][INFO] - Training Epoch: 1/2, step 5967/7134 completed (loss: 0.037979062646627426, acc: 0.9940298795700073)
[2025-02-13 03:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11][root][INFO] - Training Epoch: 1/2, step 5968/7134 completed (loss: 0.05304441973567009, acc: 0.9840182662010193)
[2025-02-13 03:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11][root][INFO] - Training Epoch: 1/2, step 5969/7134 completed (loss: 0.05540788173675537, acc: 0.9845201373100281)
[2025-02-13 03:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11][root][INFO] - Training Epoch: 1/2, step 5970/7134 completed (loss: 0.06311696022748947, acc: 0.9856557250022888)
[2025-02-13 03:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:12][root][INFO] - Training Epoch: 1/2, step 5971/7134 completed (loss: 0.026827072724699974, acc: 0.9924812316894531)
[2025-02-13 03:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:12][root][INFO] - Training Epoch: 1/2, step 5972/7134 completed (loss: 0.0896340012550354, acc: 0.9867841601371765)
[2025-02-13 03:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13][root][INFO] - Training Epoch: 1/2, step 5973/7134 completed (loss: 0.09177598357200623, acc: 0.971222996711731)
[2025-02-13 03:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13][root][INFO] - Training Epoch: 1/2, step 5974/7134 completed (loss: 0.01541249267756939, acc: 0.9948006868362427)
[2025-02-13 03:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14][root][INFO] - Training Epoch: 1/2, step 5975/7134 completed (loss: 0.024643555283546448, acc: 0.9882006049156189)
[2025-02-13 03:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14][root][INFO] - Training Epoch: 1/2, step 5976/7134 completed (loss: 0.05245649814605713, acc: 0.9861963391304016)
[2025-02-13 03:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14][root][INFO] - Training Epoch: 1/2, step 5977/7134 completed (loss: 0.019457831978797913, acc: 0.9962825179100037)
[2025-02-13 03:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:15][root][INFO] - Training Epoch: 1/2, step 5978/7134 completed (loss: 0.034223396331071854, acc: 0.9903846383094788)
[2025-02-13 03:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:15][root][INFO] - Training Epoch: 1/2, step 5979/7134 completed (loss: 0.055140525102615356, acc: 0.9851852059364319)
[2025-02-13 03:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16][root][INFO] - Training Epoch: 1/2, step 5980/7134 completed (loss: 0.03241628035902977, acc: 0.9923175573348999)
[2025-02-13 03:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16][root][INFO] - Training Epoch: 1/2, step 5981/7134 completed (loss: 0.03345077857375145, acc: 0.989595353603363)
[2025-02-13 03:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16][root][INFO] - Training Epoch: 1/2, step 5982/7134 completed (loss: 0.052588801831007004, acc: 0.9920477271080017)
[2025-02-13 03:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:17][root][INFO] - Training Epoch: 1/2, step 5983/7134 completed (loss: 0.05466378107666969, acc: 0.9847221970558167)
[2025-02-13 03:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:17][root][INFO] - Training Epoch: 1/2, step 5984/7134 completed (loss: 0.03244517743587494, acc: 0.9914893507957458)
[2025-02-13 03:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:18][root][INFO] - Training Epoch: 1/2, step 5985/7134 completed (loss: 0.04063039273023605, acc: 0.9899874925613403)
[2025-02-13 03:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:18][root][INFO] - Training Epoch: 1/2, step 5986/7134 completed (loss: 0.04893174022436142, acc: 0.987261176109314)
[2025-02-13 03:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19][root][INFO] - Training Epoch: 1/2, step 5987/7134 completed (loss: 0.05834061652421951, acc: 0.9836829900741577)
[2025-02-13 03:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19][root][INFO] - Training Epoch: 1/2, step 5988/7134 completed (loss: 0.020834747701883316, acc: 0.9959999918937683)
[2025-02-13 03:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19][root][INFO] - Training Epoch: 1/2, step 5989/7134 completed (loss: 0.0652213767170906, acc: 0.9870848655700684)
[2025-02-13 03:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:20][root][INFO] - Training Epoch: 1/2, step 5990/7134 completed (loss: 0.04654807597398758, acc: 0.9849315285682678)
[2025-02-13 03:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:20][root][INFO] - Training Epoch: 1/2, step 5991/7134 completed (loss: 0.11476398259401321, acc: 0.9748603105545044)
[2025-02-13 03:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21][root][INFO] - Training Epoch: 1/2, step 5992/7134 completed (loss: 0.03753437474370003, acc: 0.9907084703445435)
[2025-02-13 03:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21][root][INFO] - Training Epoch: 1/2, step 5993/7134 completed (loss: 0.03197064623236656, acc: 0.9902912378311157)
[2025-02-13 03:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21][root][INFO] - Training Epoch: 1/2, step 5994/7134 completed (loss: 0.026490993797779083, acc: 0.9923954606056213)
[2025-02-13 03:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:22][root][INFO] - Training Epoch: 1/2, step 5995/7134 completed (loss: 0.02030065655708313, acc: 0.9929577708244324)
[2025-02-13 03:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:22][root][INFO] - Training Epoch: 1/2, step 5996/7134 completed (loss: 0.06504695117473602, acc: 0.9812206625938416)
[2025-02-13 03:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:23][root][INFO] - Training Epoch: 1/2, step 5997/7134 completed (loss: 0.04462498426437378, acc: 0.9933244585990906)
[2025-02-13 03:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:23][root][INFO] - Training Epoch: 1/2, step 5998/7134 completed (loss: 0.045253753662109375, acc: 0.9862843155860901)
[2025-02-13 03:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24][root][INFO] - Training Epoch: 1/2, step 5999/7134 completed (loss: 0.028097985312342644, acc: 0.9938042163848877)
[2025-02-13 03:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24][root][INFO] - Training Epoch: 1/2, step 6000/7134 completed (loss: 0.06104649230837822, acc: 0.981697142124176)
[2025-02-13 03:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24][root][INFO] - Training Epoch: 1/2, step 6001/7134 completed (loss: 0.026476986706256866, acc: 0.9910485744476318)
[2025-02-13 03:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:25][root][INFO] - Training Epoch: 1/2, step 6002/7134 completed (loss: 0.05522306635975838, acc: 0.9844054579734802)
[2025-02-13 03:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:25][root][INFO] - Training Epoch: 1/2, step 6003/7134 completed (loss: 0.042573247104883194, acc: 0.9838472604751587)
[2025-02-13 03:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:26][root][INFO] - Training Epoch: 1/2, step 6004/7134 completed (loss: 0.03495146706700325, acc: 0.9916550517082214)
[2025-02-13 03:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:26][root][INFO] - Training Epoch: 1/2, step 6005/7134 completed (loss: 0.04498047009110451, acc: 0.9893454909324646)
[2025-02-13 03:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27][root][INFO] - Training Epoch: 1/2, step 6006/7134 completed (loss: 0.0668354481458664, acc: 0.978723406791687)
[2025-02-13 03:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27][root][INFO] - Training Epoch: 1/2, step 6007/7134 completed (loss: 0.02411944605410099, acc: 0.9966611266136169)
[2025-02-13 03:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27][root][INFO] - Training Epoch: 1/2, step 6008/7134 completed (loss: 0.049189139157533646, acc: 0.9836795330047607)
[2025-02-13 03:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:28][root][INFO] - Training Epoch: 1/2, step 6009/7134 completed (loss: 0.07569190114736557, acc: 0.9798136353492737)
[2025-02-13 03:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:28][root][INFO] - Training Epoch: 1/2, step 6010/7134 completed (loss: 0.07734495401382446, acc: 0.9742547273635864)
[2025-02-13 03:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:29][root][INFO] - Training Epoch: 1/2, step 6011/7134 completed (loss: 0.06410753726959229, acc: 0.9805699586868286)
[2025-02-13 03:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:29][root][INFO] - Training Epoch: 1/2, step 6012/7134 completed (loss: 0.05839085951447487, acc: 0.9791044592857361)
[2025-02-13 03:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30][root][INFO] - Training Epoch: 1/2, step 6013/7134 completed (loss: 0.054086245596408844, acc: 0.98128342628479)
[2025-02-13 03:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30][root][INFO] - Training Epoch: 1/2, step 6014/7134 completed (loss: 0.03575949743390083, acc: 0.9836660623550415)
[2025-02-13 03:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30][root][INFO] - Training Epoch: 1/2, step 6015/7134 completed (loss: 0.07457392662763596, acc: 0.9752066135406494)
[2025-02-13 03:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31][root][INFO] - Training Epoch: 1/2, step 6016/7134 completed (loss: 0.02235545590519905, acc: 0.9892473220825195)
[2025-02-13 03:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31][root][INFO] - Training Epoch: 1/2, step 6017/7134 completed (loss: 0.10010503977537155, acc: 0.9851484894752502)
[2025-02-13 03:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31][root][INFO] - Training Epoch: 1/2, step 6018/7134 completed (loss: 0.022185515612363815, acc: 0.9894737005233765)
[2025-02-13 03:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31][root][INFO] - Training Epoch: 1/2, step 6019/7134 completed (loss: 0.09223760664463043, acc: 0.9741379022598267)
[2025-02-13 03:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32][root][INFO] - Training Epoch: 1/2, step 6020/7134 completed (loss: 0.006587598938494921, acc: 1.0)
[2025-02-13 03:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32][root][INFO] - Training Epoch: 1/2, step 6021/7134 completed (loss: 0.04400286078453064, acc: 0.9934640526771545)
[2025-02-13 03:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32][root][INFO] - Training Epoch: 1/2, step 6022/7134 completed (loss: 0.031621791422367096, acc: 0.9889298677444458)
[2025-02-13 03:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33][root][INFO] - Training Epoch: 1/2, step 6023/7134 completed (loss: 0.019118521362543106, acc: 0.9945799708366394)
[2025-02-13 03:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33][root][INFO] - Training Epoch: 1/2, step 6024/7134 completed (loss: 0.010225960984826088, acc: 1.0)
[2025-02-13 03:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33][root][INFO] - Training Epoch: 1/2, step 6025/7134 completed (loss: 0.025041233748197556, acc: 0.991631805896759)
[2025-02-13 03:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33][root][INFO] - Training Epoch: 1/2, step 6026/7134 completed (loss: 0.050476085394620895, acc: 0.9942857027053833)
[2025-02-13 03:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34][root][INFO] - Training Epoch: 1/2, step 6027/7134 completed (loss: 0.029009265825152397, acc: 0.9948586225509644)
[2025-02-13 03:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34][root][INFO] - Training Epoch: 1/2, step 6028/7134 completed (loss: 0.024604691192507744, acc: 0.9911110997200012)
[2025-02-13 03:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34][root][INFO] - Training Epoch: 1/2, step 6029/7134 completed (loss: 0.05433639883995056, acc: 0.9819494485855103)
[2025-02-13 03:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34][root][INFO] - Training Epoch: 1/2, step 6030/7134 completed (loss: 0.043917495757341385, acc: 0.992337167263031)
[2025-02-13 03:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:35][root][INFO] - Training Epoch: 1/2, step 6031/7134 completed (loss: 0.033966224640607834, acc: 0.9912280440330505)
[2025-02-13 03:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:35][root][INFO] - Training Epoch: 1/2, step 6032/7134 completed (loss: 0.038835570216178894, acc: 0.9898734092712402)
[2025-02-13 03:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36][root][INFO] - Training Epoch: 1/2, step 6033/7134 completed (loss: 0.09376977384090424, acc: 0.9754098653793335)
[2025-02-13 03:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36][root][INFO] - Training Epoch: 1/2, step 6034/7134 completed (loss: 0.0495988093316555, acc: 0.9918032884597778)
[2025-02-13 03:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36][root][INFO] - Training Epoch: 1/2, step 6035/7134 completed (loss: 0.10554835200309753, acc: 0.973099410533905)
[2025-02-13 03:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:37][root][INFO] - Training Epoch: 1/2, step 6036/7134 completed (loss: 0.08123096823692322, acc: 0.9748252034187317)
[2025-02-13 03:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:37][root][INFO] - Training Epoch: 1/2, step 6037/7134 completed (loss: 0.040657561272382736, acc: 0.9830508232116699)
[2025-02-13 03:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:38][root][INFO] - Training Epoch: 1/2, step 6038/7134 completed (loss: 0.06269894540309906, acc: 0.9854586124420166)
[2025-02-13 03:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:38][root][INFO] - Training Epoch: 1/2, step 6039/7134 completed (loss: 0.04201897978782654, acc: 0.990510106086731)
[2025-02-13 03:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39][root][INFO] - Training Epoch: 1/2, step 6040/7134 completed (loss: 0.06733038276433945, acc: 0.9844074845314026)
[2025-02-13 03:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39][root][INFO] - Training Epoch: 1/2, step 6041/7134 completed (loss: 0.05028560012578964, acc: 0.9882044792175293)
[2025-02-13 03:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39][root][INFO] - Training Epoch: 1/2, step 6042/7134 completed (loss: 0.028935354202985764, acc: 0.9909999966621399)
[2025-02-13 03:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:40][root][INFO] - Training Epoch: 1/2, step 6043/7134 completed (loss: 0.03986922279000282, acc: 0.9892086386680603)
[2025-02-13 03:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:40][root][INFO] - Training Epoch: 1/2, step 6044/7134 completed (loss: 0.04729853570461273, acc: 0.9851852059364319)
[2025-02-13 03:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:41][root][INFO] - Training Epoch: 1/2, step 6045/7134 completed (loss: 0.0335160531103611, acc: 0.9917159676551819)
[2025-02-13 03:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:41][root][INFO] - Training Epoch: 1/2, step 6046/7134 completed (loss: 0.05488741025328636, acc: 0.9826689958572388)
[2025-02-13 03:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42][root][INFO] - Training Epoch: 1/2, step 6047/7134 completed (loss: 0.10194078832864761, acc: 0.9713321924209595)
[2025-02-13 03:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42][root][INFO] - Training Epoch: 1/2, step 6048/7134 completed (loss: 0.09715848416090012, acc: 0.9699398875236511)
[2025-02-13 03:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42][root][INFO] - Training Epoch: 1/2, step 6049/7134 completed (loss: 0.023908475413918495, acc: 0.9948805570602417)
[2025-02-13 03:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:43][root][INFO] - Training Epoch: 1/2, step 6050/7134 completed (loss: 0.08945970982313156, acc: 0.9740082025527954)
[2025-02-13 03:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:43][root][INFO] - Training Epoch: 1/2, step 6051/7134 completed (loss: 0.10152725875377655, acc: 0.9660266041755676)
[2025-02-13 03:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:44][root][INFO] - Training Epoch: 1/2, step 6052/7134 completed (loss: 0.07957928627729416, acc: 0.9811617136001587)
[2025-02-13 03:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:44][root][INFO] - Training Epoch: 1/2, step 6053/7134 completed (loss: 0.06375957280397415, acc: 0.9802631735801697)
[2025-02-13 03:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45][root][INFO] - Training Epoch: 1/2, step 6054/7134 completed (loss: 0.048362474888563156, acc: 0.9818840622901917)
[2025-02-13 03:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45][root][INFO] - Training Epoch: 1/2, step 6055/7134 completed (loss: 0.03482048958539963, acc: 0.9921011328697205)
[2025-02-13 03:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45][root][INFO] - Training Epoch: 1/2, step 6056/7134 completed (loss: 0.07551681250333786, acc: 0.9769585132598877)
[2025-02-13 03:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46][root][INFO] - Training Epoch: 1/2, step 6057/7134 completed (loss: 0.06247960776090622, acc: 0.9802631735801697)
[2025-02-13 03:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46][root][INFO] - Training Epoch: 1/2, step 6058/7134 completed (loss: 0.014019888825714588, acc: 0.9954954981803894)
[2025-02-13 03:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46][root][INFO] - Training Epoch: 1/2, step 6059/7134 completed (loss: 0.05036759003996849, acc: 0.9787985682487488)
[2025-02-13 03:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:47][root][INFO] - Training Epoch: 1/2, step 6060/7134 completed (loss: 0.03818028047680855, acc: 0.9879699349403381)
[2025-02-13 03:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:47][root][INFO] - Training Epoch: 1/2, step 6061/7134 completed (loss: 0.0797652006149292, acc: 0.9784283638000488)
[2025-02-13 03:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48][root][INFO] - Training Epoch: 1/2, step 6062/7134 completed (loss: 0.10369221121072769, acc: 0.9702842235565186)
[2025-02-13 03:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48][root][INFO] - Training Epoch: 1/2, step 6063/7134 completed (loss: 0.11480405181646347, acc: 0.9728752374649048)
[2025-02-13 03:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48][root][INFO] - Training Epoch: 1/2, step 6064/7134 completed (loss: 0.0754823312163353, acc: 0.9782923460006714)
[2025-02-13 03:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49][root][INFO] - Training Epoch: 1/2, step 6065/7134 completed (loss: 0.06670405715703964, acc: 0.9769137501716614)
[2025-02-13 03:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49][root][INFO] - Training Epoch: 1/2, step 6066/7134 completed (loss: 0.09607487916946411, acc: 0.9654036164283752)
[2025-02-13 03:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:50][root][INFO] - Training Epoch: 1/2, step 6067/7134 completed (loss: 0.09034222364425659, acc: 0.9754689931869507)
[2025-02-13 03:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:50][root][INFO] - Training Epoch: 1/2, step 6068/7134 completed (loss: 0.10152621567249298, acc: 0.9687108993530273)
[2025-02-13 03:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51][root][INFO] - Training Epoch: 1/2, step 6069/7134 completed (loss: 0.09020274132490158, acc: 0.9693430662155151)
[2025-02-13 03:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51][root][INFO] - Training Epoch: 1/2, step 6070/7134 completed (loss: 0.09309038519859314, acc: 0.9773635268211365)
[2025-02-13 03:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51][root][INFO] - Training Epoch: 1/2, step 6071/7134 completed (loss: 0.0665372759103775, acc: 0.9784656763076782)
[2025-02-13 03:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:52][root][INFO] - Training Epoch: 1/2, step 6072/7134 completed (loss: 0.11656351387500763, acc: 0.9684873819351196)
[2025-02-13 03:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:52][root][INFO] - Training Epoch: 1/2, step 6073/7134 completed (loss: 0.06972222775220871, acc: 0.9846153855323792)
[2025-02-13 03:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53][root][INFO] - Training Epoch: 1/2, step 6074/7134 completed (loss: 0.07383111864328384, acc: 0.9823529124259949)
[2025-02-13 03:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53][root][INFO] - Training Epoch: 1/2, step 6075/7134 completed (loss: 0.06173872947692871, acc: 0.9844124913215637)
[2025-02-13 03:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53][root][INFO] - Training Epoch: 1/2, step 6076/7134 completed (loss: 0.1000983938574791, acc: 0.9727011322975159)
[2025-02-13 03:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:54][root][INFO] - Training Epoch: 1/2, step 6077/7134 completed (loss: 0.08567433804273605, acc: 0.97444087266922)
[2025-02-13 03:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:54][root][INFO] - Training Epoch: 1/2, step 6078/7134 completed (loss: 0.10053058713674545, acc: 0.9746478796005249)
[2025-02-13 03:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:55][root][INFO] - Training Epoch: 1/2, step 6079/7134 completed (loss: 0.049714069813489914, acc: 0.9817517995834351)
[2025-02-13 03:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:55][root][INFO] - Training Epoch: 1/2, step 6080/7134 completed (loss: 0.04586681351065636, acc: 0.9886845946311951)
[2025-02-13 03:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56][root][INFO] - Training Epoch: 1/2, step 6081/7134 completed (loss: 0.04369182512164116, acc: 0.9847328066825867)
[2025-02-13 03:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56][root][INFO] - Training Epoch: 1/2, step 6082/7134 completed (loss: 0.04286998137831688, acc: 0.9920381903648376)
[2025-02-13 03:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56][root][INFO] - Training Epoch: 1/2, step 6083/7134 completed (loss: 0.020016316324472427, acc: 0.9950860142707825)
[2025-02-13 03:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:57][root][INFO] - Training Epoch: 1/2, step 6084/7134 completed (loss: 0.07338433712720871, acc: 0.9827044010162354)
[2025-02-13 03:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:57][root][INFO] - Training Epoch: 1/2, step 6085/7134 completed (loss: 0.036667127162218094, acc: 0.989266574382782)
[2025-02-13 03:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:58][root][INFO] - Training Epoch: 1/2, step 6086/7134 completed (loss: 0.03743481636047363, acc: 0.9901685118675232)
[2025-02-13 03:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:58][root][INFO] - Training Epoch: 1/2, step 6087/7134 completed (loss: 0.05475161224603653, acc: 0.9798741936683655)
[2025-02-13 03:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59][root][INFO] - Training Epoch: 1/2, step 6088/7134 completed (loss: 0.039022646844387054, acc: 0.9839141964912415)
[2025-02-13 03:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59][root][INFO] - Training Epoch: 1/2, step 6089/7134 completed (loss: 0.048827193677425385, acc: 0.9875862002372742)
[2025-02-13 03:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59][root][INFO] - Training Epoch: 1/2, step 6090/7134 completed (loss: 0.02991883084177971, acc: 0.9903978109359741)
[2025-02-13 03:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00][root][INFO] - Training Epoch: 1/2, step 6091/7134 completed (loss: 0.026936346665024757, acc: 0.9904458522796631)
[2025-02-13 03:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00][root][INFO] - Training Epoch: 1/2, step 6092/7134 completed (loss: 0.05609815567731857, acc: 0.9873015880584717)
[2025-02-13 03:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01][root][INFO] - Training Epoch: 1/2, step 6093/7134 completed (loss: 0.07196380943059921, acc: 0.9814471006393433)
[2025-02-13 03:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01][root][INFO] - Training Epoch: 1/2, step 6094/7134 completed (loss: 0.03791035711765289, acc: 0.9873949289321899)
[2025-02-13 03:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01][root][INFO] - Training Epoch: 1/2, step 6095/7134 completed (loss: 0.02981940098106861, acc: 0.9913793206214905)
[2025-02-13 03:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:02][root][INFO] - Training Epoch: 1/2, step 6096/7134 completed (loss: 0.05492468178272247, acc: 0.9892601370811462)
[2025-02-13 03:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:02][root][INFO] - Training Epoch: 1/2, step 6097/7134 completed (loss: 0.06152701377868652, acc: 0.9861496090888977)
[2025-02-13 03:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03][root][INFO] - Training Epoch: 1/2, step 6098/7134 completed (loss: 0.08745326846837997, acc: 0.9768339991569519)
[2025-02-13 03:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03][root][INFO] - Training Epoch: 1/2, step 6099/7134 completed (loss: 0.09459362924098969, acc: 0.9847095012664795)
[2025-02-13 03:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03][root][INFO] - Training Epoch: 1/2, step 6100/7134 completed (loss: 0.11789680272340775, acc: 0.9786585569381714)
[2025-02-13 03:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:04][root][INFO] - Training Epoch: 1/2, step 6101/7134 completed (loss: 0.03847847878932953, acc: 0.9904153347015381)
[2025-02-13 03:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:04][root][INFO] - Training Epoch: 1/2, step 6102/7134 completed (loss: 0.058185677975416183, acc: 0.98591548204422)
[2025-02-13 03:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05][root][INFO] - Training Epoch: 1/2, step 6103/7134 completed (loss: 0.038057420402765274, acc: 0.9888888597488403)
[2025-02-13 03:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05][root][INFO] - Training Epoch: 1/2, step 6104/7134 completed (loss: 0.08726708590984344, acc: 0.9733333587646484)
[2025-02-13 03:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05][root][INFO] - Training Epoch: 1/2, step 6105/7134 completed (loss: 0.06679854542016983, acc: 0.9791666865348816)
[2025-02-13 03:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:06][root][INFO] - Training Epoch: 1/2, step 6106/7134 completed (loss: 0.06071477010846138, acc: 0.9838998317718506)
[2025-02-13 03:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:06][root][INFO] - Training Epoch: 1/2, step 6107/7134 completed (loss: 0.0723431259393692, acc: 0.9831932783126831)
[2025-02-13 03:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07][root][INFO] - Training Epoch: 1/2, step 6108/7134 completed (loss: 0.05768469721078873, acc: 0.9836065769195557)
[2025-02-13 03:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07][root][INFO] - Training Epoch: 1/2, step 6109/7134 completed (loss: 0.03432012349367142, acc: 0.9872881174087524)
[2025-02-13 03:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07][root][INFO] - Training Epoch: 1/2, step 6110/7134 completed (loss: 0.10898831486701965, acc: 0.9636752009391785)
[2025-02-13 03:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:08][root][INFO] - Training Epoch: 1/2, step 6111/7134 completed (loss: 0.09779975563287735, acc: 0.9758713245391846)
[2025-02-13 03:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:08][root][INFO] - Training Epoch: 1/2, step 6112/7134 completed (loss: 0.05487498641014099, acc: 0.97826087474823)
[2025-02-13 03:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09][root][INFO] - Training Epoch: 1/2, step 6113/7134 completed (loss: 0.022196821868419647, acc: 0.9908952713012695)
[2025-02-13 03:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09][root][INFO] - Training Epoch: 1/2, step 6114/7134 completed (loss: 0.09015173465013504, acc: 0.9735537171363831)
[2025-02-13 03:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09][root][INFO] - Training Epoch: 1/2, step 6115/7134 completed (loss: 0.06473679095506668, acc: 0.9885057210922241)
[2025-02-13 03:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:10][root][INFO] - Training Epoch: 1/2, step 6116/7134 completed (loss: 0.09411657601594925, acc: 0.9802371263504028)
[2025-02-13 03:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:10][root][INFO] - Training Epoch: 1/2, step 6117/7134 completed (loss: 0.08697802573442459, acc: 0.9863387942314148)
[2025-02-13 03:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11][root][INFO] - Training Epoch: 1/2, step 6118/7134 completed (loss: 0.08850263804197311, acc: 0.9742646813392639)
[2025-02-13 03:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11][root][INFO] - Training Epoch: 1/2, step 6119/7134 completed (loss: 0.04452840983867645, acc: 0.9831932783126831)
[2025-02-13 03:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11][root][INFO] - Training Epoch: 1/2, step 6120/7134 completed (loss: 0.05107531324028969, acc: 0.9874776601791382)
[2025-02-13 03:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:12][root][INFO] - Training Epoch: 1/2, step 6121/7134 completed (loss: 0.0680139809846878, acc: 0.9820144176483154)
[2025-02-13 03:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:12][root][INFO] - Training Epoch: 1/2, step 6122/7134 completed (loss: 0.05262962728738785, acc: 0.9811320900917053)
[2025-02-13 03:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13][root][INFO] - Training Epoch: 1/2, step 6123/7134 completed (loss: 0.09892179071903229, acc: 0.9792099595069885)
[2025-02-13 03:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13][root][INFO] - Training Epoch: 1/2, step 6124/7134 completed (loss: 0.03622458875179291, acc: 0.9865384697914124)
[2025-02-13 03:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13][root][INFO] - Training Epoch: 1/2, step 6125/7134 completed (loss: 0.13564345240592957, acc: 0.9710843563079834)
[2025-02-13 03:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:14][root][INFO] - Training Epoch: 1/2, step 6126/7134 completed (loss: 0.07276149094104767, acc: 0.9843260049819946)
[2025-02-13 03:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:14][root][INFO] - Training Epoch: 1/2, step 6127/7134 completed (loss: 0.030805028975009918, acc: 0.9904761910438538)
[2025-02-13 03:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15][root][INFO] - Training Epoch: 1/2, step 6128/7134 completed (loss: 0.04635975509881973, acc: 0.9838056564331055)
[2025-02-13 03:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15][root][INFO] - Training Epoch: 1/2, step 6129/7134 completed (loss: 0.06499681621789932, acc: 0.9789029359817505)
[2025-02-13 03:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15][root][INFO] - Training Epoch: 1/2, step 6130/7134 completed (loss: 0.04752109944820404, acc: 0.9874551892280579)
[2025-02-13 03:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:16][root][INFO] - Training Epoch: 1/2, step 6131/7134 completed (loss: 0.06426301598548889, acc: 0.9796407222747803)
[2025-02-13 03:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:16][root][INFO] - Training Epoch: 1/2, step 6132/7134 completed (loss: 0.0660616010427475, acc: 0.981249988079071)
[2025-02-13 03:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17][root][INFO] - Training Epoch: 1/2, step 6133/7134 completed (loss: 0.05567508935928345, acc: 0.9890776872634888)
[2025-02-13 03:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17][root][INFO] - Training Epoch: 1/2, step 6134/7134 completed (loss: 0.09544533491134644, acc: 0.9754977226257324)
[2025-02-13 03:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17][root][INFO] - Training Epoch: 1/2, step 6135/7134 completed (loss: 0.02235548384487629, acc: 0.996515691280365)
[2025-02-13 03:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18][root][INFO] - Training Epoch: 1/2, step 6136/7134 completed (loss: 0.08148534595966339, acc: 0.981792688369751)
[2025-02-13 03:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18][root][INFO] - Training Epoch: 1/2, step 6137/7134 completed (loss: 0.0923045203089714, acc: 0.9734789133071899)
[2025-02-13 03:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19][root][INFO] - Training Epoch: 1/2, step 6138/7134 completed (loss: 0.07243850827217102, acc: 0.9846389889717102)
[2025-02-13 03:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19][root][INFO] - Training Epoch: 1/2, step 6139/7134 completed (loss: 0.06536936014890671, acc: 0.979629635810852)
[2025-02-13 03:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19][root][INFO] - Training Epoch: 1/2, step 6140/7134 completed (loss: 0.05818267911672592, acc: 0.9876325130462646)
[2025-02-13 03:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:20][root][INFO] - Training Epoch: 1/2, step 6141/7134 completed (loss: 0.11194021254777908, acc: 0.9877675771713257)
[2025-02-13 03:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:20][root][INFO] - Training Epoch: 1/2, step 6142/7134 completed (loss: 0.045429032295942307, acc: 0.9870503544807434)
[2025-02-13 03:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21][root][INFO] - Training Epoch: 1/2, step 6143/7134 completed (loss: 0.05491891875863075, acc: 0.9878378510475159)
[2025-02-13 03:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21][root][INFO] - Training Epoch: 1/2, step 6144/7134 completed (loss: 0.03678370639681816, acc: 0.9915966391563416)
[2025-02-13 03:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21][root][INFO] - Training Epoch: 1/2, step 6145/7134 completed (loss: 0.07783753424882889, acc: 0.9815059304237366)
[2025-02-13 03:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:22][root][INFO] - Training Epoch: 1/2, step 6146/7134 completed (loss: 0.06921838223934174, acc: 0.9834087491035461)
[2025-02-13 03:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:22][root][INFO] - Training Epoch: 1/2, step 6147/7134 completed (loss: 0.05817068740725517, acc: 0.9821428656578064)
[2025-02-13 03:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23][root][INFO] - Training Epoch: 1/2, step 6148/7134 completed (loss: 0.06942915171384811, acc: 0.979899525642395)
[2025-02-13 03:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23][root][INFO] - Training Epoch: 1/2, step 6149/7134 completed (loss: 0.08462017774581909, acc: 0.9742765426635742)
[2025-02-13 03:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23][root][INFO] - Training Epoch: 1/2, step 6150/7134 completed (loss: 0.08065260201692581, acc: 0.9789473414421082)
[2025-02-13 03:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:24][root][INFO] - Training Epoch: 1/2, step 6151/7134 completed (loss: 0.03944820165634155, acc: 0.9858712553977966)
[2025-02-13 03:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:24][root][INFO] - Training Epoch: 1/2, step 6152/7134 completed (loss: 0.03866082802414894, acc: 0.9879336357116699)
[2025-02-13 03:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25][root][INFO] - Training Epoch: 1/2, step 6153/7134 completed (loss: 0.03125399351119995, acc: 0.9884225726127625)
[2025-02-13 03:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25][root][INFO] - Training Epoch: 1/2, step 6154/7134 completed (loss: 0.053576815873384476, acc: 0.9832636117935181)
[2025-02-13 03:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26][root][INFO] - Training Epoch: 1/2, step 6155/7134 completed (loss: 0.08199375122785568, acc: 0.9807121753692627)
[2025-02-13 03:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26][root][INFO] - Training Epoch: 1/2, step 6156/7134 completed (loss: 0.03502897545695305, acc: 0.9873217344284058)
[2025-02-13 03:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26][root][INFO] - Training Epoch: 1/2, step 6157/7134 completed (loss: 0.047391440719366074, acc: 0.9858934283256531)
[2025-02-13 03:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:27][root][INFO] - Training Epoch: 1/2, step 6158/7134 completed (loss: 0.08512981235980988, acc: 0.9837251305580139)
[2025-02-13 03:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:27][root][INFO] - Training Epoch: 1/2, step 6159/7134 completed (loss: 0.0819016546010971, acc: 0.9746682643890381)
[2025-02-13 03:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:28][root][INFO] - Training Epoch: 1/2, step 6160/7134 completed (loss: 0.11993040889501572, acc: 0.9681817889213562)
[2025-02-13 03:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:28][root][INFO] - Training Epoch: 1/2, step 6161/7134 completed (loss: 0.0716472715139389, acc: 0.982425332069397)
[2025-02-13 03:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29][root][INFO] - Training Epoch: 1/2, step 6162/7134 completed (loss: 0.057885732501745224, acc: 0.9852700233459473)
[2025-02-13 03:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29][root][INFO] - Training Epoch: 1/2, step 6163/7134 completed (loss: 0.10299640893936157, acc: 0.9767857193946838)
[2025-02-13 03:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29][root][INFO] - Training Epoch: 1/2, step 6164/7134 completed (loss: 0.11464188992977142, acc: 0.9716714024543762)
[2025-02-13 03:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:30][root][INFO] - Training Epoch: 1/2, step 6165/7134 completed (loss: 0.07657046616077423, acc: 0.9778597950935364)
[2025-02-13 03:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:30][root][INFO] - Training Epoch: 1/2, step 6166/7134 completed (loss: 0.0471525639295578, acc: 0.9929701089859009)
[2025-02-13 03:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31][root][INFO] - Training Epoch: 1/2, step 6167/7134 completed (loss: 0.05250091105699539, acc: 0.9929278492927551)
[2025-02-13 03:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31][root][INFO] - Training Epoch: 1/2, step 6168/7134 completed (loss: 0.061188697814941406, acc: 0.9858430027961731)
[2025-02-13 03:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31][root][INFO] - Training Epoch: 1/2, step 6169/7134 completed (loss: 0.07930953800678253, acc: 0.9839650392532349)
[2025-02-13 03:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32][root][INFO] - Training Epoch: 1/2, step 6170/7134 completed (loss: 0.04023119807243347, acc: 0.9886578321456909)
[2025-02-13 03:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32][root][INFO] - Training Epoch: 1/2, step 6171/7134 completed (loss: 0.02370387502014637, acc: 0.9934297204017639)
[2025-02-13 03:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33][root][INFO] - Training Epoch: 1/2, step 6172/7134 completed (loss: 0.04611414670944214, acc: 0.9839679598808289)
[2025-02-13 03:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33][root][INFO] - Training Epoch: 1/2, step 6173/7134 completed (loss: 0.019601469859480858, acc: 0.9941089749336243)
[2025-02-13 03:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33][root][INFO] - Training Epoch: 1/2, step 6174/7134 completed (loss: 0.013250576332211494, acc: 0.9956011772155762)
[2025-02-13 03:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:34][root][INFO] - Training Epoch: 1/2, step 6175/7134 completed (loss: 0.053342305123806, acc: 0.9837278127670288)
[2025-02-13 03:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:34][root][INFO] - Training Epoch: 1/2, step 6176/7134 completed (loss: 0.039387159049510956, acc: 0.986975371837616)
[2025-02-13 03:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35][root][INFO] - Training Epoch: 1/2, step 6177/7134 completed (loss: 0.03310936316847801, acc: 0.9884910583496094)
[2025-02-13 03:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35][root][INFO] - Training Epoch: 1/2, step 6178/7134 completed (loss: 0.02379651553928852, acc: 0.9955752491950989)
[2025-02-13 03:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35][root][INFO] - Training Epoch: 1/2, step 6179/7134 completed (loss: 0.061496149748563766, acc: 0.9875776171684265)
[2025-02-13 03:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:36][root][INFO] - Training Epoch: 1/2, step 6180/7134 completed (loss: 0.16035428643226624, acc: 0.9522293210029602)
[2025-02-13 03:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:36][root][INFO] - Training Epoch: 1/2, step 6181/7134 completed (loss: 0.2818085551261902, acc: 0.9297520518302917)
[2025-02-13 03:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37][root][INFO] - Training Epoch: 1/2, step 6182/7134 completed (loss: 0.060128845274448395, acc: 0.9777365326881409)
[2025-02-13 03:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37][root][INFO] - Training Epoch: 1/2, step 6183/7134 completed (loss: 0.0695202499628067, acc: 0.9754977226257324)
[2025-02-13 03:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37][root][INFO] - Training Epoch: 1/2, step 6184/7134 completed (loss: 0.16864849627017975, acc: 0.9495967626571655)
[2025-02-13 03:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:38][root][INFO] - Training Epoch: 1/2, step 6185/7134 completed (loss: 0.05487678572535515, acc: 0.9816176295280457)
[2025-02-13 03:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:38][root][INFO] - Training Epoch: 1/2, step 6186/7134 completed (loss: 0.045521195977926254, acc: 0.9807692170143127)
[2025-02-13 03:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39][root][INFO] - Training Epoch: 1/2, step 6187/7134 completed (loss: 0.03846089541912079, acc: 0.9907407164573669)
[2025-02-13 03:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39][root][INFO] - Training Epoch: 1/2, step 6188/7134 completed (loss: 0.10133003443479538, acc: 0.9732016921043396)
[2025-02-13 03:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39][root][INFO] - Training Epoch: 1/2, step 6189/7134 completed (loss: 0.119381383061409, acc: 0.9610389471054077)
[2025-02-13 03:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:40][root][INFO] - Training Epoch: 1/2, step 6190/7134 completed (loss: 0.134160578250885, acc: 0.9651162624359131)
[2025-02-13 03:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:40][root][INFO] - Training Epoch: 1/2, step 6191/7134 completed (loss: 0.08845870196819305, acc: 0.9702796936035156)
[2025-02-13 03:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:41][root][INFO] - Training Epoch: 1/2, step 6192/7134 completed (loss: 0.05948266759514809, acc: 0.9796355962753296)
[2025-02-13 03:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:41][root][INFO] - Training Epoch: 1/2, step 6193/7134 completed (loss: 0.08413219451904297, acc: 0.9774078726768494)
[2025-02-13 03:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42][root][INFO] - Training Epoch: 1/2, step 6194/7134 completed (loss: 0.09487537294626236, acc: 0.966926097869873)
[2025-02-13 03:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42][root][INFO] - Training Epoch: 1/2, step 6195/7134 completed (loss: 0.1061587929725647, acc: 0.9688995480537415)
[2025-02-13 03:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42][root][INFO] - Training Epoch: 1/2, step 6196/7134 completed (loss: 0.0667325034737587, acc: 0.9779614210128784)
[2025-02-13 03:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:43][root][INFO] - Training Epoch: 1/2, step 6197/7134 completed (loss: 0.12202004343271255, acc: 0.9710344672203064)
[2025-02-13 03:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:43][root][INFO] - Training Epoch: 1/2, step 6198/7134 completed (loss: 0.046234529465436935, acc: 0.9875518679618835)
[2025-02-13 03:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:44][root][INFO] - Training Epoch: 1/2, step 6199/7134 completed (loss: 0.06756523996591568, acc: 0.982598602771759)
[2025-02-13 03:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:44][root][INFO] - Training Epoch: 1/2, step 6200/7134 completed (loss: 0.060314811766147614, acc: 0.9792683124542236)
[2025-02-13 03:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45][root][INFO] - Training Epoch: 1/2, step 6201/7134 completed (loss: 0.03621067851781845, acc: 0.9933110475540161)
[2025-02-13 03:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45][root][INFO] - Training Epoch: 1/2, step 6202/7134 completed (loss: 0.05972318723797798, acc: 0.9848155975341797)
[2025-02-13 03:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45][root][INFO] - Training Epoch: 1/2, step 6203/7134 completed (loss: 0.03457516059279442, acc: 0.9872449040412903)
[2025-02-13 03:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:46][root][INFO] - Training Epoch: 1/2, step 6204/7134 completed (loss: 0.03871876746416092, acc: 0.9960707426071167)
[2025-02-13 03:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:46][root][INFO] - Training Epoch: 1/2, step 6205/7134 completed (loss: 0.038054775446653366, acc: 0.988054633140564)
[2025-02-13 03:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47][root][INFO] - Training Epoch: 1/2, step 6206/7134 completed (loss: 0.04671812430024147, acc: 0.9898697733879089)
[2025-02-13 03:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47][root][INFO] - Training Epoch: 1/2, step 6207/7134 completed (loss: 0.15410982072353363, acc: 0.9666221737861633)
[2025-02-13 03:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47][root][INFO] - Training Epoch: 1/2, step 6208/7134 completed (loss: 0.05218261107802391, acc: 0.9849537014961243)
[2025-02-13 03:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:48][root][INFO] - Training Epoch: 1/2, step 6209/7134 completed (loss: 0.09855400025844574, acc: 0.9754204154014587)
[2025-02-13 03:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:48][root][INFO] - Training Epoch: 1/2, step 6210/7134 completed (loss: 0.06292535364627838, acc: 0.9791208505630493)
[2025-02-13 03:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:49][root][INFO] - Training Epoch: 1/2, step 6211/7134 completed (loss: 0.061025261878967285, acc: 0.981055498123169)
[2025-02-13 03:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:49][root][INFO] - Training Epoch: 1/2, step 6212/7134 completed (loss: 0.07903482019901276, acc: 0.9754385948181152)
[2025-02-13 03:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50][root][INFO] - Training Epoch: 1/2, step 6213/7134 completed (loss: 0.031688135117292404, acc: 0.9896050095558167)
[2025-02-13 03:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50][root][INFO] - Training Epoch: 1/2, step 6214/7134 completed (loss: 0.04546304792165756, acc: 0.9871630072593689)
[2025-02-13 03:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50][root][INFO] - Training Epoch: 1/2, step 6215/7134 completed (loss: 0.07206286489963531, acc: 0.9746543765068054)
[2025-02-13 03:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:51][root][INFO] - Training Epoch: 1/2, step 6216/7134 completed (loss: 0.05404208227992058, acc: 0.9808510541915894)
[2025-02-13 03:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:51][root][INFO] - Training Epoch: 1/2, step 6217/7134 completed (loss: 0.036299560219049454, acc: 0.9879194498062134)
[2025-02-13 03:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52][root][INFO] - Training Epoch: 1/2, step 6218/7134 completed (loss: 0.05675689876079559, acc: 0.9832317233085632)
[2025-02-13 03:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52][root][INFO] - Training Epoch: 1/2, step 6219/7134 completed (loss: 0.03526921570301056, acc: 0.9887482523918152)
[2025-02-13 03:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52][root][INFO] - Training Epoch: 1/2, step 6220/7134 completed (loss: 0.08354273438453674, acc: 0.9714285731315613)
[2025-02-13 03:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:53][root][INFO] - Training Epoch: 1/2, step 6221/7134 completed (loss: 0.0295892171561718, acc: 0.9894737005233765)
[2025-02-13 03:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:53][root][INFO] - Training Epoch: 1/2, step 6222/7134 completed (loss: 0.06671632081270218, acc: 0.9822404384613037)
[2025-02-13 03:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:54][root][INFO] - Training Epoch: 1/2, step 6223/7134 completed (loss: 0.056142497807741165, acc: 0.9841269850730896)
[2025-02-13 03:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:54][root][INFO] - Training Epoch: 1/2, step 6224/7134 completed (loss: 0.02653229609131813, acc: 0.994194507598877)
[2025-02-13 03:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55][root][INFO] - Training Epoch: 1/2, step 6225/7134 completed (loss: 0.03420761227607727, acc: 0.9885203838348389)
[2025-02-13 03:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55][root][INFO] - Training Epoch: 1/2, step 6226/7134 completed (loss: 0.05214591324329376, acc: 0.9829620122909546)
[2025-02-13 03:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55][root][INFO] - Training Epoch: 1/2, step 6227/7134 completed (loss: 0.023127472028136253, acc: 0.9892904758453369)
[2025-02-13 03:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:56][root][INFO] - Training Epoch: 1/2, step 6228/7134 completed (loss: 0.0346941202878952, acc: 0.9864864945411682)
[2025-02-13 03:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:56][root][INFO] - Training Epoch: 1/2, step 6229/7134 completed (loss: 0.07211777567863464, acc: 0.9814385175704956)
[2025-02-13 03:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:57][root][INFO] - Training Epoch: 1/2, step 6230/7134 completed (loss: 0.04261186718940735, acc: 0.9875311851501465)
[2025-02-13 03:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:57][root][INFO] - Training Epoch: 1/2, step 6231/7134 completed (loss: 0.03863675147294998, acc: 0.985788106918335)
[2025-02-13 03:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58][root][INFO] - Training Epoch: 1/2, step 6232/7134 completed (loss: 0.026825878769159317, acc: 0.9873737096786499)
[2025-02-13 03:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58][root][INFO] - Training Epoch: 1/2, step 6233/7134 completed (loss: 0.025939151644706726, acc: 0.9919571280479431)
[2025-02-13 03:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58][root][INFO] - Training Epoch: 1/2, step 6234/7134 completed (loss: 0.06221694126725197, acc: 0.9826202988624573)
[2025-02-13 03:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:59][root][INFO] - Training Epoch: 1/2, step 6235/7134 completed (loss: 0.03404872119426727, acc: 0.9960886836051941)
[2025-02-13 03:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:59][root][INFO] - Training Epoch: 1/2, step 6236/7134 completed (loss: 0.046154603362083435, acc: 0.9863574504852295)
[2025-02-13 03:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00][root][INFO] - Training Epoch: 1/2, step 6237/7134 completed (loss: 0.037815771996974945, acc: 0.9862385392189026)
[2025-02-13 03:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00][root][INFO] - Training Epoch: 1/2, step 6238/7134 completed (loss: 0.015931041911244392, acc: 0.9951534867286682)
[2025-02-13 03:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00][root][INFO] - Training Epoch: 1/2, step 6239/7134 completed (loss: 0.03949430212378502, acc: 0.9916368126869202)
[2025-02-13 03:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:01][root][INFO] - Training Epoch: 1/2, step 6240/7134 completed (loss: 0.006794403772801161, acc: 1.0)
[2025-02-13 03:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:01][root][INFO] - Training Epoch: 1/2, step 6241/7134 completed (loss: 0.015274539589881897, acc: 0.9936708807945251)
[2025-02-13 03:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:02][root][INFO] - Training Epoch: 1/2, step 6242/7134 completed (loss: 0.023085201159119606, acc: 0.9914966225624084)
[2025-02-13 03:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:02][root][INFO] - Training Epoch: 1/2, step 6243/7134 completed (loss: 0.024640515446662903, acc: 0.9904458522796631)
[2025-02-13 03:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03][root][INFO] - Training Epoch: 1/2, step 6244/7134 completed (loss: 0.026033639907836914, acc: 0.991769552230835)
[2025-02-13 03:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03][root][INFO] - Training Epoch: 1/2, step 6245/7134 completed (loss: 0.022767435759305954, acc: 0.9931600689888)
[2025-02-13 03:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03][root][INFO] - Training Epoch: 1/2, step 6246/7134 completed (loss: 0.0406171977519989, acc: 0.9828721880912781)
[2025-02-13 03:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:04][root][INFO] - Training Epoch: 1/2, step 6247/7134 completed (loss: 0.05062751844525337, acc: 0.9807229042053223)
[2025-02-13 03:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:04][root][INFO] - Training Epoch: 1/2, step 6248/7134 completed (loss: 0.04881823807954788, acc: 0.9844236969947815)
[2025-02-13 03:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05][root][INFO] - Training Epoch: 1/2, step 6249/7134 completed (loss: 0.007548232562839985, acc: 0.9968553185462952)
[2025-02-13 03:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05][root][INFO] - Training Epoch: 1/2, step 6250/7134 completed (loss: 0.019420385360717773, acc: 0.9965986609458923)
[2025-02-13 03:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05][root][INFO] - Training Epoch: 1/2, step 6251/7134 completed (loss: 0.024176672101020813, acc: 0.9923312664031982)
[2025-02-13 03:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06][root][INFO] - Training Epoch: 1/2, step 6252/7134 completed (loss: 0.027033518999814987, acc: 0.9935275316238403)
[2025-02-13 03:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06][root][INFO] - Training Epoch: 1/2, step 6253/7134 completed (loss: 0.037889327853918076, acc: 0.9913793206214905)
[2025-02-13 03:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07][root][INFO] - Training Epoch: 1/2, step 6254/7134 completed (loss: 0.028393445536494255, acc: 0.991349458694458)
[2025-02-13 03:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07][root][INFO] - Training Epoch: 1/2, step 6255/7134 completed (loss: 0.02730954997241497, acc: 0.9905149340629578)
[2025-02-13 03:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07][root][INFO] - Training Epoch: 1/2, step 6256/7134 completed (loss: 0.01695621758699417, acc: 0.9900709390640259)
[2025-02-13 03:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:08][root][INFO] - Training Epoch: 1/2, step 6257/7134 completed (loss: 0.025300798937678337, acc: 0.9899280667304993)
[2025-02-13 03:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:08][root][INFO] - Training Epoch: 1/2, step 6258/7134 completed (loss: 0.018076526001095772, acc: 0.995199978351593)
[2025-02-13 03:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09][root][INFO] - Training Epoch: 1/2, step 6259/7134 completed (loss: 0.019766399636864662, acc: 0.9894894957542419)
[2025-02-13 03:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09][root][INFO] - Training Epoch: 1/2, step 6260/7134 completed (loss: 0.032894667237997055, acc: 0.9916107654571533)
[2025-02-13 03:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09][root][INFO] - Training Epoch: 1/2, step 6261/7134 completed (loss: 0.029905682429671288, acc: 0.9908536672592163)
[2025-02-13 03:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:10][root][INFO] - Training Epoch: 1/2, step 6262/7134 completed (loss: 0.007888301275670528, acc: 0.9985652565956116)
[2025-02-13 03:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:10][root][INFO] - Training Epoch: 1/2, step 6263/7134 completed (loss: 0.013601507991552353, acc: 0.9970717430114746)
[2025-02-13 03:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:11][root][INFO] - Training Epoch: 1/2, step 6264/7134 completed (loss: 0.013020969927310944, acc: 0.9967159032821655)
[2025-02-13 03:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:11][root][INFO] - Training Epoch: 1/2, step 6265/7134 completed (loss: 0.017740923911333084, acc: 0.9926035404205322)
[2025-02-13 03:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12][root][INFO] - Training Epoch: 1/2, step 6266/7134 completed (loss: 0.011551178060472012, acc: 0.996503472328186)
[2025-02-13 03:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12][root][INFO] - Training Epoch: 1/2, step 6267/7134 completed (loss: 0.02005779929459095, acc: 0.9983713626861572)
[2025-02-13 03:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12][root][INFO] - Training Epoch: 1/2, step 6268/7134 completed (loss: 0.025428153574466705, acc: 0.9970238208770752)
[2025-02-13 03:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13][root][INFO] - Training Epoch: 1/2, step 6269/7134 completed (loss: 0.02974855527281761, acc: 0.9957355856895447)
[2025-02-13 03:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13][root][INFO] - Training Epoch: 1/2, step 6270/7134 completed (loss: 0.020511282607913017, acc: 0.995398759841919)
[2025-02-13 03:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14][root][INFO] - Training Epoch: 1/2, step 6271/7134 completed (loss: 0.021786917001008987, acc: 0.9959893226623535)
[2025-02-13 03:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14][root][INFO] - Training Epoch: 1/2, step 6272/7134 completed (loss: 0.0054664914496243, acc: 0.9985315799713135)
[2025-02-13 03:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14][root][INFO] - Training Epoch: 1/2, step 6273/7134 completed (loss: 0.011281386949121952, acc: 0.99863201379776)
[2025-02-13 03:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:15][root][INFO] - Training Epoch: 1/2, step 6274/7134 completed (loss: 0.029839323833584785, acc: 0.9914407730102539)
[2025-02-13 03:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:15][root][INFO] - Training Epoch: 1/2, step 6275/7134 completed (loss: 0.03514917939901352, acc: 0.9923896789550781)
[2025-02-13 03:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16][root][INFO] - Training Epoch: 1/2, step 6276/7134 completed (loss: 0.031110549345612526, acc: 0.9877408146858215)
[2025-02-13 03:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16][root][INFO] - Training Epoch: 1/2, step 6277/7134 completed (loss: 0.10583309084177017, acc: 0.9728506803512573)
[2025-02-13 03:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16][root][INFO] - Training Epoch: 1/2, step 6278/7134 completed (loss: 0.018535234034061432, acc: 0.9934210777282715)
[2025-02-13 03:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:17][root][INFO] - Training Epoch: 1/2, step 6279/7134 completed (loss: 0.010073427110910416, acc: 0.9982876777648926)
[2025-02-13 03:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:17][root][INFO] - Training Epoch: 1/2, step 6280/7134 completed (loss: 0.03140955790877342, acc: 0.9930192232131958)
[2025-02-13 03:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18][root][INFO] - Training Epoch: 1/2, step 6281/7134 completed (loss: 0.03377159684896469, acc: 0.9930675625801086)
[2025-02-13 03:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18][root][INFO] - Training Epoch: 1/2, step 6282/7134 completed (loss: 0.014261601492762566, acc: 0.9937106966972351)
[2025-02-13 03:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18][root][INFO] - Training Epoch: 1/2, step 6283/7134 completed (loss: 0.0404045470058918, acc: 0.9890829920768738)
[2025-02-13 03:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:19][root][INFO] - Training Epoch: 1/2, step 6284/7134 completed (loss: 0.05287062004208565, acc: 0.9803063273429871)
[2025-02-13 03:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:19][root][INFO] - Training Epoch: 1/2, step 6285/7134 completed (loss: 0.022702930495142937, acc: 0.9947506785392761)
[2025-02-13 03:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20][root][INFO] - Training Epoch: 1/2, step 6286/7134 completed (loss: 0.020335514098405838, acc: 0.9950658082962036)
[2025-02-13 03:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20][root][INFO] - Training Epoch: 1/2, step 6287/7134 completed (loss: 0.04405542463064194, acc: 0.9852941036224365)
[2025-02-13 03:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20][root][INFO] - Training Epoch: 1/2, step 6288/7134 completed (loss: 0.0130022456869483, acc: 0.9967319965362549)
[2025-02-13 03:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21][root][INFO] - Training Epoch: 1/2, step 6289/7134 completed (loss: 0.029069598764181137, acc: 0.9927927851676941)
[2025-02-13 03:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21][root][INFO] - Training Epoch: 1/2, step 6290/7134 completed (loss: 0.03251604735851288, acc: 0.9904000163078308)
[2025-02-13 03:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21][root][INFO] - Training Epoch: 1/2, step 6291/7134 completed (loss: 0.0151021433994174, acc: 0.996363639831543)
[2025-02-13 03:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:22][root][INFO] - Training Epoch: 1/2, step 6292/7134 completed (loss: 0.052238985896110535, acc: 0.984054684638977)
[2025-02-13 03:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:22][root][INFO] - Training Epoch: 1/2, step 6293/7134 completed (loss: 0.06116533651947975, acc: 0.9812286496162415)
[2025-02-13 03:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23][root][INFO] - Training Epoch: 1/2, step 6294/7134 completed (loss: 0.03156230226159096, acc: 0.990275502204895)
[2025-02-13 03:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23][root][INFO] - Training Epoch: 1/2, step 6295/7134 completed (loss: 0.029413430020213127, acc: 0.9891501069068909)
[2025-02-13 03:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23][root][INFO] - Training Epoch: 1/2, step 6296/7134 completed (loss: 0.05623539164662361, acc: 0.9842519760131836)
[2025-02-13 03:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:24][root][INFO] - Training Epoch: 1/2, step 6297/7134 completed (loss: 0.03866797313094139, acc: 0.9819672107696533)
[2025-02-13 03:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:24][root][INFO] - Training Epoch: 1/2, step 6298/7134 completed (loss: 0.04740414023399353, acc: 0.9793814420700073)
[2025-02-13 03:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25][root][INFO] - Training Epoch: 1/2, step 6299/7134 completed (loss: 0.07097704708576202, acc: 0.9808306694030762)
[2025-02-13 03:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25][root][INFO] - Training Epoch: 1/2, step 6300/7134 completed (loss: 0.049933768808841705, acc: 0.9832496047019958)
[2025-02-13 03:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25][root][INFO] - Training Epoch: 1/2, step 6301/7134 completed (loss: 0.011961931362748146, acc: 0.9977011680603027)
[2025-02-13 03:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:26][root][INFO] - Training Epoch: 1/2, step 6302/7134 completed (loss: 0.021525098010897636, acc: 0.9917126893997192)
[2025-02-13 03:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:26][root][INFO] - Training Epoch: 1/2, step 6303/7134 completed (loss: 0.016976431012153625, acc: 0.9957401752471924)
[2025-02-13 03:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27][root][INFO] - Training Epoch: 1/2, step 6304/7134 completed (loss: 0.023474812507629395, acc: 0.9918919205665588)
[2025-02-13 03:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27][root][INFO] - Training Epoch: 1/2, step 6305/7134 completed (loss: 0.022929396480321884, acc: 0.9934533834457397)
[2025-02-13 03:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:28][root][INFO] - Training Epoch: 1/2, step 6306/7134 completed (loss: 0.05575481802225113, acc: 0.9879952073097229)
[2025-02-13 03:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:28][root][INFO] - Training Epoch: 1/2, step 6307/7134 completed (loss: 0.07119479775428772, acc: 0.9841269850730896)
[2025-02-13 03:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29][root][INFO] - Training Epoch: 1/2, step 6308/7134 completed (loss: 0.021861238405108452, acc: 0.9950860142707825)
[2025-02-13 03:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29][root][INFO] - Training Epoch: 1/2, step 6309/7134 completed (loss: 0.016826853156089783, acc: 0.9936000108718872)
[2025-02-13 03:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29][root][INFO] - Training Epoch: 1/2, step 6310/7134 completed (loss: 0.0956989973783493, acc: 0.9780219793319702)
[2025-02-13 03:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:30][root][INFO] - Training Epoch: 1/2, step 6311/7134 completed (loss: 0.046927716583013535, acc: 0.9864698648452759)
[2025-02-13 03:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:30][root][INFO] - Training Epoch: 1/2, step 6312/7134 completed (loss: 0.04885612800717354, acc: 0.9842767119407654)
[2025-02-13 03:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31][root][INFO] - Training Epoch: 1/2, step 6313/7134 completed (loss: 0.016911234706640244, acc: 0.9926380515098572)
[2025-02-13 03:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31][root][INFO] - Training Epoch: 1/2, step 6314/7134 completed (loss: 0.014752059243619442, acc: 0.996216893196106)
[2025-02-13 03:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31][root][INFO] - Training Epoch: 1/2, step 6315/7134 completed (loss: 0.039567649364471436, acc: 0.9934425950050354)
[2025-02-13 03:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:32][root][INFO] - Training Epoch: 1/2, step 6316/7134 completed (loss: 0.03440926596522331, acc: 0.990275502204895)
[2025-02-13 03:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:32][root][INFO] - Training Epoch: 1/2, step 6317/7134 completed (loss: 0.018143244087696075, acc: 0.993537962436676)
[2025-02-13 03:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:33][root][INFO] - Training Epoch: 1/2, step 6318/7134 completed (loss: 0.017504174262285233, acc: 0.9920508861541748)
[2025-02-13 03:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:33][root][INFO] - Training Epoch: 1/2, step 6319/7134 completed (loss: 0.013060935772955418, acc: 0.9969135522842407)
[2025-02-13 03:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34][root][INFO] - Training Epoch: 1/2, step 6320/7134 completed (loss: 0.022063735872507095, acc: 0.9921011328697205)
[2025-02-13 03:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34][root][INFO] - Training Epoch: 1/2, step 6321/7134 completed (loss: 0.012462952174246311, acc: 0.9933035969734192)
[2025-02-13 03:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34][root][INFO] - Training Epoch: 1/2, step 6322/7134 completed (loss: 0.03750242665410042, acc: 0.9941860437393188)
[2025-02-13 03:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:35][root][INFO] - Training Epoch: 1/2, step 6323/7134 completed (loss: 0.033157236874103546, acc: 0.9894894957542419)
[2025-02-13 03:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:35][root][INFO] - Training Epoch: 1/2, step 6324/7134 completed (loss: 0.021609097719192505, acc: 0.9946737885475159)
[2025-02-13 03:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:36][root][INFO] - Training Epoch: 1/2, step 6325/7134 completed (loss: 0.04117852821946144, acc: 0.9886792302131653)
[2025-02-13 03:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:36][root][INFO] - Training Epoch: 1/2, step 6326/7134 completed (loss: 0.061494115740060806, acc: 0.9839486479759216)
[2025-02-13 03:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37][root][INFO] - Training Epoch: 1/2, step 6327/7134 completed (loss: 0.0358537994325161, acc: 0.9894459247589111)
[2025-02-13 03:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37][root][INFO] - Training Epoch: 1/2, step 6328/7134 completed (loss: 0.026665782555937767, acc: 0.9904305934906006)
[2025-02-13 03:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37][root][INFO] - Training Epoch: 1/2, step 6329/7134 completed (loss: 0.03952183946967125, acc: 0.9878787994384766)
[2025-02-13 03:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:38][root][INFO] - Training Epoch: 1/2, step 6330/7134 completed (loss: 0.02704259566962719, acc: 0.9925558567047119)
[2025-02-13 03:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:38][root][INFO] - Training Epoch: 1/2, step 6331/7134 completed (loss: 0.017998114228248596, acc: 0.9946996569633484)
[2025-02-13 03:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39][root][INFO] - Training Epoch: 1/2, step 6332/7134 completed (loss: 0.09530661255121231, acc: 0.9776951670646667)
[2025-02-13 03:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39][root][INFO] - Training Epoch: 1/2, step 6333/7134 completed (loss: 0.04901667311787605, acc: 0.9809523820877075)
[2025-02-13 03:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39][root][INFO] - Training Epoch: 1/2, step 6334/7134 completed (loss: 0.03809390589594841, acc: 0.9810606241226196)
[2025-02-13 03:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:40][root][INFO] - Training Epoch: 1/2, step 6335/7134 completed (loss: 0.014907331205904484, acc: 0.9932885766029358)
[2025-02-13 03:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:40][root][INFO] - Training Epoch: 1/2, step 6336/7134 completed (loss: 0.03766947612166405, acc: 0.9885321259498596)
[2025-02-13 03:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41][root][INFO] - Training Epoch: 1/2, step 6337/7134 completed (loss: 0.015743765980005264, acc: 0.9963503479957581)
[2025-02-13 03:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41][root][INFO] - Training Epoch: 1/2, step 6338/7134 completed (loss: 0.050488926470279694, acc: 0.9842022061347961)
[2025-02-13 03:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41][root][INFO] - Training Epoch: 1/2, step 6339/7134 completed (loss: 0.04841439425945282, acc: 0.9844290614128113)
[2025-02-13 03:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42][root][INFO] - Training Epoch: 1/2, step 6340/7134 completed (loss: 0.016641858965158463, acc: 0.9941349029541016)
[2025-02-13 03:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42][root][INFO] - Training Epoch: 1/2, step 6341/7134 completed (loss: 0.027851860970258713, acc: 0.992409884929657)
[2025-02-13 03:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:43][root][INFO] - Training Epoch: 1/2, step 6342/7134 completed (loss: 0.025998881086707115, acc: 0.9932705163955688)
[2025-02-13 03:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:43][root][INFO] - Training Epoch: 1/2, step 6343/7134 completed (loss: 0.06458888202905655, acc: 0.9862306118011475)
[2025-02-13 03:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44][root][INFO] - Training Epoch: 1/2, step 6344/7134 completed (loss: 0.02811416983604431, acc: 0.9890561103820801)
[2025-02-13 03:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44][root][INFO] - Training Epoch: 1/2, step 6345/7134 completed (loss: 0.04155557602643967, acc: 0.9849726557731628)
[2025-02-13 03:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44][root][INFO] - Training Epoch: 1/2, step 6346/7134 completed (loss: 0.06945563107728958, acc: 0.9798761606216431)
[2025-02-13 03:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:45][root][INFO] - Training Epoch: 1/2, step 6347/7134 completed (loss: 0.047833893448114395, acc: 0.9848484992980957)
[2025-02-13 03:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:45][root][INFO] - Training Epoch: 1/2, step 6348/7134 completed (loss: 0.09251044690608978, acc: 0.9756944179534912)
[2025-02-13 03:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46][root][INFO] - Training Epoch: 1/2, step 6349/7134 completed (loss: 0.03965909406542778, acc: 0.9854280352592468)
[2025-02-13 03:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46][root][INFO] - Training Epoch: 1/2, step 6350/7134 completed (loss: 0.05158056691288948, acc: 0.9809523820877075)
[2025-02-13 03:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46][root][INFO] - Training Epoch: 1/2, step 6351/7134 completed (loss: 0.061703797429800034, acc: 0.9805970191955566)
[2025-02-13 03:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:47][root][INFO] - Training Epoch: 1/2, step 6352/7134 completed (loss: 0.1270093023777008, acc: 0.9673105478286743)
[2025-02-13 03:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:47][root][INFO] - Training Epoch: 1/2, step 6353/7134 completed (loss: 0.0363713763654232, acc: 0.9912152290344238)
[2025-02-13 03:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48][root][INFO] - Training Epoch: 1/2, step 6354/7134 completed (loss: 0.03149622678756714, acc: 0.9930747747421265)
[2025-02-13 03:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48][root][INFO] - Training Epoch: 1/2, step 6355/7134 completed (loss: 0.02956247702240944, acc: 0.9902912378311157)
[2025-02-13 03:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48][root][INFO] - Training Epoch: 1/2, step 6356/7134 completed (loss: 0.04111455753445625, acc: 0.9910314083099365)
[2025-02-13 03:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:49][root][INFO] - Training Epoch: 1/2, step 6357/7134 completed (loss: 0.014643675647675991, acc: 0.9958333373069763)
[2025-02-13 03:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:49][root][INFO] - Training Epoch: 1/2, step 6358/7134 completed (loss: 0.03684113547205925, acc: 0.9943661689758301)
[2025-02-13 03:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:50][root][INFO] - Training Epoch: 1/2, step 6359/7134 completed (loss: 0.06275931745767593, acc: 0.9845938086509705)
[2025-02-13 03:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:50][root][INFO] - Training Epoch: 1/2, step 6360/7134 completed (loss: 0.033830687403678894, acc: 0.9920381903648376)
[2025-02-13 03:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51][root][INFO] - Training Epoch: 1/2, step 6361/7134 completed (loss: 0.06116902455687523, acc: 0.9817517995834351)
[2025-02-13 03:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51][root][INFO] - Training Epoch: 1/2, step 6362/7134 completed (loss: 0.042302455753088, acc: 0.9893292784690857)
[2025-02-13 03:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51][root][INFO] - Training Epoch: 1/2, step 6363/7134 completed (loss: 0.032668232917785645, acc: 0.9882698059082031)
[2025-02-13 03:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:52][root][INFO] - Training Epoch: 1/2, step 6364/7134 completed (loss: 0.020803608000278473, acc: 0.991416335105896)
[2025-02-13 03:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:52][root][INFO] - Training Epoch: 1/2, step 6365/7134 completed (loss: 0.02918563410639763, acc: 0.9898374080657959)
[2025-02-13 03:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53][root][INFO] - Training Epoch: 1/2, step 6366/7134 completed (loss: 0.04783470556139946, acc: 0.9841479659080505)
[2025-02-13 03:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53][root][INFO] - Training Epoch: 1/2, step 6367/7134 completed (loss: 0.06770482659339905, acc: 0.9848771095275879)
[2025-02-13 03:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53][root][INFO] - Training Epoch: 1/2, step 6368/7134 completed (loss: 0.0653441846370697, acc: 0.9872881174087524)
[2025-02-13 03:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:54][root][INFO] - Training Epoch: 1/2, step 6369/7134 completed (loss: 0.06043703109025955, acc: 0.9855453372001648)
[2025-02-13 03:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:54][root][INFO] - Training Epoch: 1/2, step 6370/7134 completed (loss: 0.13725627958774567, acc: 0.9696570038795471)
[2025-02-13 03:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:55][root][INFO] - Training Epoch: 1/2, step 6371/7134 completed (loss: 0.07189898192882538, acc: 0.9736841917037964)
[2025-02-13 03:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:55][root][INFO] - Training Epoch: 1/2, step 6372/7134 completed (loss: 0.141977459192276, acc: 0.9558541178703308)
[2025-02-13 03:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56][root][INFO] - Training Epoch: 1/2, step 6373/7134 completed (loss: 0.07822506129741669, acc: 0.9763205647468567)
[2025-02-13 03:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56][root][INFO] - Training Epoch: 1/2, step 6374/7134 completed (loss: 0.02166343480348587, acc: 0.9958506226539612)
[2025-02-13 03:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56][root][INFO] - Training Epoch: 1/2, step 6375/7134 completed (loss: 0.05425841361284256, acc: 0.9833119511604309)
[2025-02-13 03:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:57][root][INFO] - Training Epoch: 1/2, step 6376/7134 completed (loss: 0.026218686252832413, acc: 0.9963099360466003)
[2025-02-13 03:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:57][root][INFO] - Training Epoch: 1/2, step 6377/7134 completed (loss: 0.03699469193816185, acc: 0.9853479862213135)
[2025-02-13 03:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58][root][INFO] - Training Epoch: 1/2, step 6378/7134 completed (loss: 0.06794348359107971, acc: 0.9785637855529785)
[2025-02-13 03:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58][root][INFO] - Training Epoch: 1/2, step 6379/7134 completed (loss: 0.05592462047934532, acc: 0.9835025668144226)
[2025-02-13 03:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58][root][INFO] - Training Epoch: 1/2, step 6380/7134 completed (loss: 0.06093517318367958, acc: 0.9865269660949707)
[2025-02-13 03:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59][root][INFO] - Training Epoch: 1/2, step 6381/7134 completed (loss: 0.03505502641201019, acc: 0.9908397197723389)
[2025-02-13 03:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59][root][INFO] - Training Epoch: 1/2, step 6382/7134 completed (loss: 0.01879735477268696, acc: 0.9947437644004822)
[2025-02-13 03:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:00][root][INFO] - Training Epoch: 1/2, step 6383/7134 completed (loss: 0.06233728677034378, acc: 0.9891451597213745)
[2025-02-13 03:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:00][root][INFO] - Training Epoch: 1/2, step 6384/7134 completed (loss: 0.05676836147904396, acc: 0.988041877746582)
[2025-02-13 03:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:01][root][INFO] - Training Epoch: 1/2, step 6385/7134 completed (loss: 0.07448849081993103, acc: 0.9892215728759766)
[2025-02-13 03:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:01][root][INFO] - Training Epoch: 1/2, step 6386/7134 completed (loss: 0.024410635232925415, acc: 0.9951159954071045)
[2025-02-13 03:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02][root][INFO] - Training Epoch: 1/2, step 6387/7134 completed (loss: 0.07169509679079056, acc: 0.9822580814361572)
[2025-02-13 03:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02][root][INFO] - Training Epoch: 1/2, step 6388/7134 completed (loss: 0.06829185038805008, acc: 0.985029935836792)
[2025-02-13 03:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02][root][INFO] - Training Epoch: 1/2, step 6389/7134 completed (loss: 0.06409425288438797, acc: 0.9841269850730896)
[2025-02-13 03:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:03][root][INFO] - Training Epoch: 1/2, step 6390/7134 completed (loss: 0.06867679953575134, acc: 0.98886638879776)
[2025-02-13 03:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:03][root][INFO] - Training Epoch: 1/2, step 6391/7134 completed (loss: 0.07317861169576645, acc: 0.984635055065155)
[2025-02-13 03:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:04][root][INFO] - Training Epoch: 1/2, step 6392/7134 completed (loss: 0.07318375259637833, acc: 0.983589768409729)
[2025-02-13 03:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:04][root][INFO] - Training Epoch: 1/2, step 6393/7134 completed (loss: 0.03449565917253494, acc: 0.9886234402656555)
[2025-02-13 03:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:05][root][INFO] - Training Epoch: 1/2, step 6394/7134 completed (loss: 0.07464035600423813, acc: 0.9805285334587097)
[2025-02-13 03:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:05][root][INFO] - Training Epoch: 1/2, step 6395/7134 completed (loss: 0.04199066385626793, acc: 0.9913899302482605)
[2025-02-13 03:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06][root][INFO] - Training Epoch: 1/2, step 6396/7134 completed (loss: 0.08024221658706665, acc: 0.9873853325843811)
[2025-02-13 03:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06][root][INFO] - Training Epoch: 1/2, step 6397/7134 completed (loss: 0.032907839864492416, acc: 0.9925558567047119)
[2025-02-13 03:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06][root][INFO] - Training Epoch: 1/2, step 6398/7134 completed (loss: 0.043590348213911057, acc: 0.9915611743927002)
[2025-02-13 03:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:07][root][INFO] - Training Epoch: 1/2, step 6399/7134 completed (loss: 0.05879124999046326, acc: 0.9862155318260193)
[2025-02-13 03:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:07][root][INFO] - Training Epoch: 1/2, step 6400/7134 completed (loss: 0.027111291885375977, acc: 0.9928774833679199)
[2025-02-13 03:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:08][root][INFO] - Training Epoch: 1/2, step 6401/7134 completed (loss: 0.05194953456521034, acc: 0.988056480884552)
[2025-02-13 03:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:08][root][INFO] - Training Epoch: 1/2, step 6402/7134 completed (loss: 0.04300141707062721, acc: 0.9909194111824036)
[2025-02-13 03:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:09][root][INFO] - Training Epoch: 1/2, step 6403/7134 completed (loss: 0.033251985907554626, acc: 0.9897959232330322)
[2025-02-13 03:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:09][root][INFO] - Training Epoch: 1/2, step 6404/7134 completed (loss: 0.019118666648864746, acc: 0.9944367408752441)
[2025-02-13 03:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10][root][INFO] - Training Epoch: 1/2, step 6405/7134 completed (loss: 0.024950530380010605, acc: 0.9935483932495117)
[2025-02-13 03:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10][root][INFO] - Training Epoch: 1/2, step 6406/7134 completed (loss: 0.03725910186767578, acc: 0.9931585192680359)
[2025-02-13 03:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10][root][INFO] - Training Epoch: 1/2, step 6407/7134 completed (loss: 0.019948197528719902, acc: 0.9947437644004822)
[2025-02-13 03:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:11][root][INFO] - Training Epoch: 1/2, step 6408/7134 completed (loss: 0.05707354471087456, acc: 0.985228955745697)
[2025-02-13 03:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:11][root][INFO] - Training Epoch: 1/2, step 6409/7134 completed (loss: 0.03515855595469475, acc: 0.996052622795105)
[2025-02-13 03:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12][root][INFO] - Training Epoch: 1/2, step 6410/7134 completed (loss: 0.024148399010300636, acc: 0.9926470518112183)
[2025-02-13 03:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12][root][INFO] - Training Epoch: 1/2, step 6411/7134 completed (loss: 0.04296620562672615, acc: 0.9899497628211975)
[2025-02-13 03:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12][root][INFO] - Training Epoch: 1/2, step 6412/7134 completed (loss: 0.012580715119838715, acc: 0.9973333477973938)
[2025-02-13 03:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:13][root][INFO] - Training Epoch: 1/2, step 6413/7134 completed (loss: 0.04192835092544556, acc: 0.9880136847496033)
[2025-02-13 03:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:13][root][INFO] - Training Epoch: 1/2, step 6414/7134 completed (loss: 0.019316181540489197, acc: 0.9939576983451843)
[2025-02-13 03:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:14][root][INFO] - Training Epoch: 1/2, step 6415/7134 completed (loss: 0.028660736978054047, acc: 0.9910141229629517)
[2025-02-13 03:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:14][root][INFO] - Training Epoch: 1/2, step 6416/7134 completed (loss: 0.06451208144426346, acc: 0.9791271090507507)
[2025-02-13 03:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15][root][INFO] - Training Epoch: 1/2, step 6417/7134 completed (loss: 0.08642090857028961, acc: 0.9841827750205994)
[2025-02-13 03:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15][root][INFO] - Training Epoch: 1/2, step 6418/7134 completed (loss: 0.1309068500995636, acc: 0.9705055952072144)
[2025-02-13 03:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15][root][INFO] - Training Epoch: 1/2, step 6419/7134 completed (loss: 0.07527570426464081, acc: 0.9787928462028503)
[2025-02-13 03:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:16][root][INFO] - Training Epoch: 1/2, step 6420/7134 completed (loss: 0.04514420032501221, acc: 0.9884726405143738)
[2025-02-13 03:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:16][root][INFO] - Training Epoch: 1/2, step 6421/7134 completed (loss: 0.057638268917798996, acc: 0.9866270422935486)
[2025-02-13 03:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17][root][INFO] - Training Epoch: 1/2, step 6422/7134 completed (loss: 0.04843197017908096, acc: 0.9822161197662354)
[2025-02-13 03:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17][root][INFO] - Training Epoch: 1/2, step 6423/7134 completed (loss: 0.07207944244146347, acc: 0.9809384346008301)
[2025-02-13 03:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17][root][INFO] - Training Epoch: 1/2, step 6424/7134 completed (loss: 0.057136159390211105, acc: 0.9848675727844238)
[2025-02-13 03:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:18][root][INFO] - Training Epoch: 1/2, step 6425/7134 completed (loss: 0.04608508571982384, acc: 0.9843546152114868)
[2025-02-13 03:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:18][root][INFO] - Training Epoch: 1/2, step 6426/7134 completed (loss: 0.043178658932447433, acc: 0.9868074059486389)
[2025-02-13 03:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:19][root][INFO] - Training Epoch: 1/2, step 6427/7134 completed (loss: 0.05076764523983002, acc: 0.9886363744735718)
[2025-02-13 03:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:19][root][INFO] - Training Epoch: 1/2, step 6428/7134 completed (loss: 0.024664271622896194, acc: 0.9916201233863831)
[2025-02-13 03:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:20][root][INFO] - Training Epoch: 1/2, step 6429/7134 completed (loss: 0.024639522656798363, acc: 0.9909090995788574)
[2025-02-13 03:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:20][root][INFO] - Training Epoch: 1/2, step 6430/7134 completed (loss: 0.05991942435503006, acc: 0.9856114983558655)
[2025-02-13 03:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21][root][INFO] - Training Epoch: 1/2, step 6431/7134 completed (loss: 0.06579920649528503, acc: 0.9842105507850647)
[2025-02-13 03:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21][root][INFO] - Training Epoch: 1/2, step 6432/7134 completed (loss: 0.19195912778377533, acc: 0.9545454382896423)
[2025-02-13 03:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21][root][INFO] - Training Epoch: 1/2, step 6433/7134 completed (loss: 0.05334258824586868, acc: 0.9849726557731628)
[2025-02-13 03:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:22][root][INFO] - Training Epoch: 1/2, step 6434/7134 completed (loss: 0.01918976940214634, acc: 0.9933422207832336)
[2025-02-13 03:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:22][root][INFO] - Training Epoch: 1/2, step 6435/7134 completed (loss: 0.043369632214307785, acc: 0.9878683090209961)
[2025-02-13 03:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23][root][INFO] - Training Epoch: 1/2, step 6436/7134 completed (loss: 0.017045538872480392, acc: 0.9969087839126587)
[2025-02-13 03:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23][root][INFO] - Training Epoch: 1/2, step 6437/7134 completed (loss: 0.042049556970596313, acc: 0.9916666746139526)
[2025-02-13 03:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23][root][INFO] - Training Epoch: 1/2, step 6438/7134 completed (loss: 0.043733835220336914, acc: 0.9876390695571899)
[2025-02-13 03:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:24][root][INFO] - Training Epoch: 1/2, step 6439/7134 completed (loss: 0.05161057040095329, acc: 0.9811023473739624)
[2025-02-13 03:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:24][root][INFO] - Training Epoch: 1/2, step 6440/7134 completed (loss: 0.030744636431336403, acc: 0.9880525469779968)
[2025-02-13 03:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:25][root][INFO] - Training Epoch: 1/2, step 6441/7134 completed (loss: 0.027089044451713562, acc: 0.9911373853683472)
[2025-02-13 03:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:25][root][INFO] - Training Epoch: 1/2, step 6442/7134 completed (loss: 0.036055129021406174, acc: 0.9954596757888794)
[2025-02-13 03:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:26][root][INFO] - Training Epoch: 1/2, step 6443/7134 completed (loss: 0.018321232870221138, acc: 0.9938119053840637)
[2025-02-13 03:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:26][root][INFO] - Training Epoch: 1/2, step 6444/7134 completed (loss: 0.03434792160987854, acc: 0.9902234673500061)
[2025-02-13 03:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:27][root][INFO] - Training Epoch: 1/2, step 6445/7134 completed (loss: 0.05442936718463898, acc: 0.9876695275306702)
[2025-02-13 03:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:27][root][INFO] - Training Epoch: 1/2, step 6446/7134 completed (loss: 0.028448546305298805, acc: 0.9952324032783508)
[2025-02-13 03:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28][root][INFO] - Training Epoch: 1/2, step 6447/7134 completed (loss: 0.04141214117407799, acc: 0.9884925484657288)
[2025-02-13 03:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28][root][INFO] - Training Epoch: 1/2, step 6448/7134 completed (loss: 0.09138359874486923, acc: 0.9750778675079346)
[2025-02-13 03:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28][root][INFO] - Training Epoch: 1/2, step 6449/7134 completed (loss: 0.02547888271510601, acc: 0.9927536249160767)
[2025-02-13 03:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:29][root][INFO] - Training Epoch: 1/2, step 6450/7134 completed (loss: 0.02845328487455845, acc: 0.9910714030265808)
[2025-02-13 03:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:29][root][INFO] - Training Epoch: 1/2, step 6451/7134 completed (loss: 0.06003614887595177, acc: 0.9853085279464722)
[2025-02-13 03:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:30][root][INFO] - Training Epoch: 1/2, step 6452/7134 completed (loss: 0.012773496098816395, acc: 0.994425892829895)
[2025-02-13 03:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:30][root][INFO] - Training Epoch: 1/2, step 6453/7134 completed (loss: 0.0341302789747715, acc: 0.9905914068222046)
[2025-02-13 03:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:31][root][INFO] - Training Epoch: 1/2, step 6454/7134 completed (loss: 0.041785828769207, acc: 0.9917550086975098)
[2025-02-13 03:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:31][root][INFO] - Training Epoch: 1/2, step 6455/7134 completed (loss: 0.014363839291036129, acc: 0.9932088255882263)
[2025-02-13 03:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32][root][INFO] - Training Epoch: 1/2, step 6456/7134 completed (loss: 0.0391780324280262, acc: 0.9923312664031982)
[2025-02-13 03:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32][root][INFO] - Training Epoch: 1/2, step 6457/7134 completed (loss: 0.01698223315179348, acc: 0.9955947399139404)
[2025-02-13 03:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32][root][INFO] - Training Epoch: 1/2, step 6458/7134 completed (loss: 0.04124285280704498, acc: 0.978151261806488)
[2025-02-13 03:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:33][root][INFO] - Training Epoch: 1/2, step 6459/7134 completed (loss: 0.003383180359378457, acc: 1.0)
[2025-02-13 03:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:33][root][INFO] - Training Epoch: 1/2, step 6460/7134 completed (loss: 0.011149896308779716, acc: 0.996458113193512)
[2025-02-13 03:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:34][root][INFO] - Training Epoch: 1/2, step 6461/7134 completed (loss: 0.01897428371012211, acc: 0.9947437644004822)
[2025-02-13 03:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:34][root][INFO] - Training Epoch: 1/2, step 6462/7134 completed (loss: 0.02419506013393402, acc: 0.9901599287986755)
[2025-02-13 03:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35][root][INFO] - Training Epoch: 1/2, step 6463/7134 completed (loss: 0.029400499537587166, acc: 0.9908883571624756)
[2025-02-13 03:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35][root][INFO] - Training Epoch: 1/2, step 6464/7134 completed (loss: 0.02403183840215206, acc: 0.9934980273246765)
[2025-02-13 03:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35][root][INFO] - Training Epoch: 1/2, step 6465/7134 completed (loss: 0.018143631517887115, acc: 0.9948586225509644)
[2025-02-13 03:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:36][root][INFO] - Training Epoch: 1/2, step 6466/7134 completed (loss: 0.026302078738808632, acc: 0.9907529950141907)
[2025-02-13 03:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:36][root][INFO] - Training Epoch: 1/2, step 6467/7134 completed (loss: 0.045736346393823624, acc: 0.9877862334251404)
[2025-02-13 03:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37][root][INFO] - Training Epoch: 1/2, step 6468/7134 completed (loss: 0.046303343027830124, acc: 0.9887429475784302)
[2025-02-13 03:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37][root][INFO] - Training Epoch: 1/2, step 6469/7134 completed (loss: 0.054002486169338226, acc: 0.986601710319519)
[2025-02-13 03:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37][root][INFO] - Training Epoch: 1/2, step 6470/7134 completed (loss: 0.045577362179756165, acc: 0.9902200698852539)
[2025-02-13 03:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:38][root][INFO] - Training Epoch: 1/2, step 6471/7134 completed (loss: 0.008478878997266293, acc: 0.9983713626861572)
[2025-02-13 03:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:38][root][INFO] - Training Epoch: 1/2, step 6472/7134 completed (loss: 0.07388059049844742, acc: 0.9858155846595764)
[2025-02-13 03:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:39][root][INFO] - Training Epoch: 1/2, step 6473/7134 completed (loss: 0.017034079879522324, acc: 0.9973190426826477)
[2025-02-13 03:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:39][root][INFO] - Training Epoch: 1/2, step 6474/7134 completed (loss: 0.024238863959908485, acc: 0.9959623217582703)
[2025-02-13 03:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40][root][INFO] - Training Epoch: 1/2, step 6475/7134 completed (loss: 0.07143955677747726, acc: 0.9833333492279053)
[2025-02-13 03:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40][root][INFO] - Training Epoch: 1/2, step 6476/7134 completed (loss: 0.07797020673751831, acc: 0.9834983348846436)
[2025-02-13 03:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40][root][INFO] - Training Epoch: 1/2, step 6477/7134 completed (loss: 0.030488591641187668, acc: 0.9936440587043762)
[2025-02-13 03:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:41][root][INFO] - Training Epoch: 1/2, step 6478/7134 completed (loss: 0.03014550358057022, acc: 0.9889298677444458)
[2025-02-13 03:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:41][root][INFO] - Training Epoch: 1/2, step 6479/7134 completed (loss: 0.031276099383831024, acc: 0.9882746934890747)
[2025-02-13 03:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42][root][INFO] - Training Epoch: 1/2, step 6480/7134 completed (loss: 0.06324087083339691, acc: 0.9845361113548279)
[2025-02-13 03:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42][root][INFO] - Training Epoch: 1/2, step 6481/7134 completed (loss: 0.01896866038441658, acc: 0.9914966225624084)
[2025-02-13 03:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42][root][INFO] - Training Epoch: 1/2, step 6482/7134 completed (loss: 0.031009037047624588, acc: 0.9925000071525574)
[2025-02-13 03:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43][root][INFO] - Training Epoch: 1/2, step 6483/7134 completed (loss: 0.08875814825296402, acc: 0.9855595827102661)
[2025-02-13 03:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43][root][INFO] - Training Epoch: 1/2, step 6484/7134 completed (loss: 0.025920817628502846, acc: 0.9912023544311523)
[2025-02-13 03:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44][root][INFO] - Training Epoch: 1/2, step 6485/7134 completed (loss: 0.029120763763785362, acc: 0.992094874382019)
[2025-02-13 03:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44][root][INFO] - Training Epoch: 1/2, step 6486/7134 completed (loss: 0.13097617030143738, acc: 0.9727272987365723)
[2025-02-13 03:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44][root][INFO] - Training Epoch: 1/2, step 6487/7134 completed (loss: 0.16406473517417908, acc: 0.9574899077415466)
[2025-02-13 03:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:45][root][INFO] - Training Epoch: 1/2, step 6488/7134 completed (loss: 0.0841219574213028, acc: 0.9698188900947571)
[2025-02-13 03:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:45][root][INFO] - Training Epoch: 1/2, step 6489/7134 completed (loss: 0.12477873265743256, acc: 0.9738805890083313)
[2025-02-13 03:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46][root][INFO] - Training Epoch: 1/2, step 6490/7134 completed (loss: 0.059317514300346375, acc: 0.9914236664772034)
[2025-02-13 03:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46][root][INFO] - Training Epoch: 1/2, step 6491/7134 completed (loss: 0.08961495012044907, acc: 0.9811066389083862)
[2025-02-13 03:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46][root][INFO] - Training Epoch: 1/2, step 6492/7134 completed (loss: 0.03891437128186226, acc: 0.9870634078979492)
[2025-02-13 03:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:47][root][INFO] - Training Epoch: 1/2, step 6493/7134 completed (loss: 0.08234242349863052, acc: 0.9796437621116638)
[2025-02-13 03:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:47][root][INFO] - Training Epoch: 1/2, step 6494/7134 completed (loss: 0.0456218458712101, acc: 0.9852398633956909)
[2025-02-13 03:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:48][root][INFO] - Training Epoch: 1/2, step 6495/7134 completed (loss: 0.0651463121175766, acc: 0.9817671775817871)
[2025-02-13 03:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:48][root][INFO] - Training Epoch: 1/2, step 6496/7134 completed (loss: 0.050900865346193314, acc: 0.983627200126648)
[2025-02-13 03:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49][root][INFO] - Training Epoch: 1/2, step 6497/7134 completed (loss: 0.1225910559296608, acc: 0.9751166701316833)
[2025-02-13 03:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49][root][INFO] - Training Epoch: 1/2, step 6498/7134 completed (loss: 0.02074163407087326, acc: 0.995121955871582)
[2025-02-13 03:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49][root][INFO] - Training Epoch: 1/2, step 6499/7134 completed (loss: 0.04806634038686752, acc: 0.9813200235366821)
[2025-02-13 03:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:50][root][INFO] - Training Epoch: 1/2, step 6500/7134 completed (loss: 0.08297231793403625, acc: 0.9719887971878052)
[2025-02-13 03:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:50][root][INFO] - Training Epoch: 1/2, step 6501/7134 completed (loss: 0.11838037520647049, acc: 0.9667519330978394)
[2025-02-13 03:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:51][root][INFO] - Training Epoch: 1/2, step 6502/7134 completed (loss: 0.03009927086532116, acc: 0.9904988408088684)
[2025-02-13 03:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:51][root][INFO] - Training Epoch: 1/2, step 6503/7134 completed (loss: 0.05403834208846092, acc: 0.9853372573852539)
[2025-02-13 03:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52][root][INFO] - Training Epoch: 1/2, step 6504/7134 completed (loss: 0.06721170991659164, acc: 0.9810426831245422)
[2025-02-13 03:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52][root][INFO] - Training Epoch: 1/2, step 6505/7134 completed (loss: 0.08473736047744751, acc: 0.9797101616859436)
[2025-02-13 03:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52][root][INFO] - Training Epoch: 1/2, step 6506/7134 completed (loss: 0.0856720581650734, acc: 0.9771987199783325)
[2025-02-13 03:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:53][root][INFO] - Training Epoch: 1/2, step 6507/7134 completed (loss: 0.10004544258117676, acc: 0.9727685451507568)
[2025-02-13 03:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:53][root][INFO] - Training Epoch: 1/2, step 6508/7134 completed (loss: 0.06760382652282715, acc: 0.9894366264343262)
[2025-02-13 03:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:54][root][INFO] - Training Epoch: 1/2, step 6509/7134 completed (loss: 0.04202549159526825, acc: 0.9900332093238831)
[2025-02-13 03:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:54][root][INFO] - Training Epoch: 1/2, step 6510/7134 completed (loss: 0.05336529389023781, acc: 0.9873149991035461)
[2025-02-13 03:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55][root][INFO] - Training Epoch: 1/2, step 6511/7134 completed (loss: 0.07709565758705139, acc: 0.9834586381912231)
[2025-02-13 03:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55][root][INFO] - Training Epoch: 1/2, step 6512/7134 completed (loss: 0.03560752794146538, acc: 0.9885931611061096)
[2025-02-13 03:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55][root][INFO] - Training Epoch: 1/2, step 6513/7134 completed (loss: 0.05489219352602959, acc: 0.9845678806304932)
[2025-02-13 03:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:56][root][INFO] - Training Epoch: 1/2, step 6514/7134 completed (loss: 0.05679669231176376, acc: 0.984375)
[2025-02-13 03:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:56][root][INFO] - Training Epoch: 1/2, step 6515/7134 completed (loss: 0.055721621960401535, acc: 0.9860405921936035)
[2025-02-13 03:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:57][root][INFO] - Training Epoch: 1/2, step 6516/7134 completed (loss: 0.09850224107503891, acc: 0.9721485376358032)
[2025-02-13 03:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:57][root][INFO] - Training Epoch: 1/2, step 6517/7134 completed (loss: 0.028280021622776985, acc: 0.9907894730567932)
[2025-02-13 03:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58][root][INFO] - Training Epoch: 1/2, step 6518/7134 completed (loss: 0.034319598227739334, acc: 0.9875862002372742)
[2025-02-13 03:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58][root][INFO] - Training Epoch: 1/2, step 6519/7134 completed (loss: 0.043904975056648254, acc: 0.9893617033958435)
[2025-02-13 03:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58][root][INFO] - Training Epoch: 1/2, step 6520/7134 completed (loss: 0.036854781210422516, acc: 0.9875583052635193)
[2025-02-13 03:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59][root][INFO] - Training Epoch: 1/2, step 6521/7134 completed (loss: 0.03489858657121658, acc: 0.9869791865348816)
[2025-02-13 03:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59][root][INFO] - Training Epoch: 1/2, step 6522/7134 completed (loss: 0.05060987547039986, acc: 0.9811320900917053)
[2025-02-13 03:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00][root][INFO] - Training Epoch: 1/2, step 6523/7134 completed (loss: 0.10133565217256546, acc: 0.97096186876297)
[2025-02-13 03:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00][root][INFO] - Training Epoch: 1/2, step 6524/7134 completed (loss: 0.08985867351293564, acc: 0.9845758080482483)
[2025-02-13 03:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00][root][INFO] - Training Epoch: 1/2, step 6525/7134 completed (loss: 0.02311559021472931, acc: 0.9919678568840027)
[2025-02-13 03:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:01][root][INFO] - Training Epoch: 1/2, step 6526/7134 completed (loss: 0.08992396295070648, acc: 0.9820742607116699)
[2025-02-13 03:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:01][root][INFO] - Training Epoch: 1/2, step 6527/7134 completed (loss: 0.0510987751185894, acc: 0.9896142482757568)
[2025-02-13 03:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:02][root][INFO] - Training Epoch: 1/2, step 6528/7134 completed (loss: 0.06741945445537567, acc: 0.9855595827102661)
[2025-02-13 03:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:02][root][INFO] - Training Epoch: 1/2, step 6529/7134 completed (loss: 0.09716938436031342, acc: 0.9793233275413513)
[2025-02-13 03:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03][root][INFO] - Training Epoch: 1/2, step 6530/7134 completed (loss: 0.04216577112674713, acc: 0.9871612191200256)
[2025-02-13 03:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03][root][INFO] - Training Epoch: 1/2, step 6531/7134 completed (loss: 0.030170267447829247, acc: 0.9913294911384583)
[2025-02-13 03:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03][root][INFO] - Training Epoch: 1/2, step 6532/7134 completed (loss: 0.06412573158740997, acc: 0.9862328171730042)
[2025-02-13 03:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:04][root][INFO] - Training Epoch: 1/2, step 6533/7134 completed (loss: 0.024378405883908272, acc: 0.9925373196601868)
[2025-02-13 03:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:04][root][INFO] - Training Epoch: 1/2, step 6534/7134 completed (loss: 0.0568380169570446, acc: 0.9906166195869446)
[2025-02-13 03:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05][root][INFO] - Training Epoch: 1/2, step 6535/7134 completed (loss: 0.02977265790104866, acc: 0.9906322956085205)
[2025-02-13 03:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05][root][INFO] - Training Epoch: 1/2, step 6536/7134 completed (loss: 0.06244169920682907, acc: 0.9821428656578064)
[2025-02-13 03:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05][root][INFO] - Training Epoch: 1/2, step 6537/7134 completed (loss: 0.07945997267961502, acc: 0.9690265655517578)
[2025-02-13 03:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06][root][INFO] - Training Epoch: 1/2, step 6538/7134 completed (loss: 0.0807403102517128, acc: 0.9786856174468994)
[2025-02-13 03:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06][root][INFO] - Training Epoch: 1/2, step 6539/7134 completed (loss: 0.01225089468061924, acc: 0.9969879388809204)
[2025-02-13 03:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06][root][INFO] - Training Epoch: 1/2, step 6540/7134 completed (loss: 0.290516197681427, acc: 0.9398906826972961)
[2025-02-13 03:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:07][root][INFO] - Training Epoch: 1/2, step 6541/7134 completed (loss: 0.06787167489528656, acc: 0.9803921580314636)
[2025-02-13 03:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:07][root][INFO] - Training Epoch: 1/2, step 6542/7134 completed (loss: 0.06008094549179077, acc: 0.9796609878540039)
[2025-02-13 03:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:08][root][INFO] - Training Epoch: 1/2, step 6543/7134 completed (loss: 0.04848864674568176, acc: 0.9853249192237854)
[2025-02-13 03:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:08][root][INFO] - Training Epoch: 1/2, step 6544/7134 completed (loss: 0.05122829228639603, acc: 0.9900596141815186)
[2025-02-13 03:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09][root][INFO] - Training Epoch: 1/2, step 6545/7134 completed (loss: 0.036698322743177414, acc: 0.9925650358200073)
[2025-02-13 03:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09][root][INFO] - Training Epoch: 1/2, step 6546/7134 completed (loss: 0.05920618772506714, acc: 0.9885993599891663)
[2025-02-13 03:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09][root][INFO] - Training Epoch: 1/2, step 6547/7134 completed (loss: 0.12770266830921173, acc: 0.9778534770011902)
[2025-02-13 03:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10][root][INFO] - Training Epoch: 1/2, step 6548/7134 completed (loss: 0.02699464187026024, acc: 0.9920634627342224)
[2025-02-13 03:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10][root][INFO] - Training Epoch: 1/2, step 6549/7134 completed (loss: 0.0971817746758461, acc: 0.9766990542411804)
[2025-02-13 03:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10][root][INFO] - Training Epoch: 1/2, step 6550/7134 completed (loss: 0.09218588471412659, acc: 0.9784615635871887)
[2025-02-13 03:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:11][root][INFO] - Training Epoch: 1/2, step 6551/7134 completed (loss: 0.02179507166147232, acc: 0.9954441785812378)
[2025-02-13 03:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:11][root][INFO] - Training Epoch: 1/2, step 6552/7134 completed (loss: 0.04069502279162407, acc: 0.9930278658866882)
[2025-02-13 03:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12][root][INFO] - Training Epoch: 1/2, step 6553/7134 completed (loss: 0.03936411067843437, acc: 0.9872773289680481)
[2025-02-13 03:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12][root][INFO] - Training Epoch: 1/2, step 6554/7134 completed (loss: 0.04473140835762024, acc: 0.9913513660430908)
[2025-02-13 03:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:13][root][INFO] - Training Epoch: 1/2, step 6555/7134 completed (loss: 0.07351808995008469, acc: 0.9768611788749695)
[2025-02-13 03:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:13][root][INFO] - Training Epoch: 1/2, step 6556/7134 completed (loss: 0.045677781105041504, acc: 0.9873684048652649)
[2025-02-13 03:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:14][root][INFO] - Training Epoch: 1/2, step 6557/7134 completed (loss: 0.04277072474360466, acc: 0.9889112710952759)
[2025-02-13 03:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:14][root][INFO] - Training Epoch: 1/2, step 6558/7134 completed (loss: 0.06414534896612167, acc: 0.9826086759567261)
[2025-02-13 03:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:15][root][INFO] - Training Epoch: 1/2, step 6559/7134 completed (loss: 0.042410511523485184, acc: 0.9861303567886353)
[2025-02-13 03:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:15][root][INFO] - Training Epoch: 1/2, step 6560/7134 completed (loss: 0.0367206372320652, acc: 0.9916765689849854)
[2025-02-13 03:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16][root][INFO] - Training Epoch: 1/2, step 6561/7134 completed (loss: 0.028590399771928787, acc: 0.9906432628631592)
[2025-02-13 03:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16][root][INFO] - Training Epoch: 1/2, step 6562/7134 completed (loss: 0.06349960714578629, acc: 0.9790732264518738)
[2025-02-13 03:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16][root][INFO] - Training Epoch: 1/2, step 6563/7134 completed (loss: 0.04424972087144852, acc: 0.9883585572242737)
[2025-02-13 03:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:17][root][INFO] - Training Epoch: 1/2, step 6564/7134 completed (loss: 0.061799269169569016, acc: 0.9851552248001099)
[2025-02-13 03:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:17][root][INFO] - Training Epoch: 1/2, step 6565/7134 completed (loss: 0.04099797084927559, acc: 0.9873149991035461)
[2025-02-13 03:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:18][root][INFO] - Training Epoch: 1/2, step 6566/7134 completed (loss: 0.03563854098320007, acc: 0.9829059839248657)
[2025-02-13 03:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:18][root][INFO] - Training Epoch: 1/2, step 6567/7134 completed (loss: 0.02063876949250698, acc: 0.9939209818840027)
[2025-02-13 03:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:19][root][INFO] - Training Epoch: 1/2, step 6568/7134 completed (loss: 0.028528675436973572, acc: 0.9936948418617249)
[2025-02-13 03:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:19][root][INFO] - Training Epoch: 1/2, step 6569/7134 completed (loss: 0.01941157877445221, acc: 0.9967032670974731)
[2025-02-13 03:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20][root][INFO] - Training Epoch: 1/2, step 6570/7134 completed (loss: 0.02997172437608242, acc: 0.99303138256073)
[2025-02-13 03:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20][root][INFO] - Training Epoch: 1/2, step 6571/7134 completed (loss: 0.026817116886377335, acc: 0.9934554696083069)
[2025-02-13 03:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20][root][INFO] - Training Epoch: 1/2, step 6572/7134 completed (loss: 0.0430481843650341, acc: 0.9889135360717773)
[2025-02-13 03:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:21][root][INFO] - Training Epoch: 1/2, step 6573/7134 completed (loss: 0.03559443727135658, acc: 0.9909706711769104)
[2025-02-13 03:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:21][root][INFO] - Training Epoch: 1/2, step 6574/7134 completed (loss: 0.020392142236232758, acc: 0.9928425550460815)
[2025-02-13 03:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:22][root][INFO] - Training Epoch: 1/2, step 6575/7134 completed (loss: 0.042802512645721436, acc: 0.9845303893089294)
[2025-02-13 03:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:22][root][INFO] - Training Epoch: 1/2, step 6576/7134 completed (loss: 0.02106330730021, acc: 0.9947478771209717)
[2025-02-13 03:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:23][root][INFO] - Training Epoch: 1/2, step 6577/7134 completed (loss: 0.0640605017542839, acc: 0.981792688369751)
[2025-02-13 03:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:23][root][INFO] - Training Epoch: 1/2, step 6578/7134 completed (loss: 0.027726784348487854, acc: 0.9911032319068909)
[2025-02-13 03:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24][root][INFO] - Training Epoch: 1/2, step 6579/7134 completed (loss: 0.039780814200639725, acc: 0.9872958064079285)
[2025-02-13 03:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24][root][INFO] - Training Epoch: 1/2, step 6580/7134 completed (loss: 0.029087724164128304, acc: 0.9905882477760315)
[2025-02-13 03:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:25][root][INFO] - Training Epoch: 1/2, step 6581/7134 completed (loss: 0.036459024995565414, acc: 0.9889298677444458)
[2025-02-13 03:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:25][root][INFO] - Training Epoch: 1/2, step 6582/7134 completed (loss: 0.05696261301636696, acc: 0.9827337861061096)
[2025-02-13 03:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26][root][INFO] - Training Epoch: 1/2, step 6583/7134 completed (loss: 0.01905648224055767, acc: 0.9942792057991028)
[2025-02-13 03:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26][root][INFO] - Training Epoch: 1/2, step 6584/7134 completed (loss: 0.051391344517469406, acc: 0.9865360856056213)
[2025-02-13 03:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26][root][INFO] - Training Epoch: 1/2, step 6585/7134 completed (loss: 0.04492368549108505, acc: 0.9819079041481018)
[2025-02-13 03:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:27][root][INFO] - Training Epoch: 1/2, step 6586/7134 completed (loss: 0.03463881090283394, acc: 0.9909326434135437)
[2025-02-13 03:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:27][root][INFO] - Training Epoch: 1/2, step 6587/7134 completed (loss: 0.07286817580461502, acc: 0.9811320900917053)
[2025-02-13 03:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28][root][INFO] - Training Epoch: 1/2, step 6588/7134 completed (loss: 0.05144646018743515, acc: 0.9899371266365051)
[2025-02-13 03:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28][root][INFO] - Training Epoch: 1/2, step 6589/7134 completed (loss: 0.04349077120423317, acc: 0.9889349937438965)
[2025-02-13 03:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29][root][INFO] - Training Epoch: 1/2, step 6590/7134 completed (loss: 0.0510953813791275, acc: 0.9825378060340881)
[2025-02-13 03:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29][root][INFO] - Training Epoch: 1/2, step 6591/7134 completed (loss: 0.04582047089934349, acc: 0.986369252204895)
[2025-02-13 03:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29][root][INFO] - Training Epoch: 1/2, step 6592/7134 completed (loss: 0.07153208553791046, acc: 0.9852440357208252)
[2025-02-13 03:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:30][root][INFO] - Training Epoch: 1/2, step 6593/7134 completed (loss: 0.045919738709926605, acc: 0.9878234267234802)
[2025-02-13 03:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:30][root][INFO] - Training Epoch: 1/2, step 6594/7134 completed (loss: 0.03440569341182709, acc: 0.9906103014945984)
[2025-02-13 03:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:31][root][INFO] - Training Epoch: 1/2, step 6595/7134 completed (loss: 0.056952882558107376, acc: 0.9881305694580078)
[2025-02-13 03:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:31][root][INFO] - Training Epoch: 1/2, step 6596/7134 completed (loss: 0.03990121930837631, acc: 0.9908952713012695)
[2025-02-13 03:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32][root][INFO] - Training Epoch: 1/2, step 6597/7134 completed (loss: 0.05044301226735115, acc: 0.989386796951294)
[2025-02-13 03:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32][root][INFO] - Training Epoch: 1/2, step 6598/7134 completed (loss: 0.046254437416791916, acc: 0.9879194498062134)
[2025-02-13 03:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32][root][INFO] - Training Epoch: 1/2, step 6599/7134 completed (loss: 0.059476833790540695, acc: 0.9867149591445923)
[2025-02-13 03:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:33][root][INFO] - Training Epoch: 1/2, step 6600/7134 completed (loss: 0.053234685212373734, acc: 0.9825673699378967)
[2025-02-13 03:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:33][root][INFO] - Training Epoch: 1/2, step 6601/7134 completed (loss: 0.06516904383897781, acc: 0.9823943376541138)
[2025-02-13 03:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:34][root][INFO] - Training Epoch: 1/2, step 6602/7134 completed (loss: 0.03932959586381912, acc: 0.9915764331817627)
[2025-02-13 03:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:34][root][INFO] - Training Epoch: 1/2, step 6603/7134 completed (loss: 0.04105145111680031, acc: 0.9859747290611267)
[2025-02-13 03:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35][root][INFO] - Training Epoch: 1/2, step 6604/7134 completed (loss: 0.05965065583586693, acc: 0.9798850417137146)
[2025-02-13 03:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35][root][INFO] - Training Epoch: 1/2, step 6605/7134 completed (loss: 0.09881748259067535, acc: 0.9709543585777283)
[2025-02-13 03:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35][root][INFO] - Training Epoch: 1/2, step 6606/7134 completed (loss: 0.039674311876297, acc: 0.9898989796638489)
[2025-02-13 03:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:36][root][INFO] - Training Epoch: 1/2, step 6607/7134 completed (loss: 0.09560273587703705, acc: 0.978723406791687)
[2025-02-13 03:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:36][root][INFO] - Training Epoch: 1/2, step 6608/7134 completed (loss: 0.03248735889792442, acc: 0.9920318722724915)
[2025-02-13 03:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:37][root][INFO] - Training Epoch: 1/2, step 6609/7134 completed (loss: 0.052462149411439896, acc: 0.9853300452232361)
[2025-02-13 03:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:37][root][INFO] - Training Epoch: 1/2, step 6610/7134 completed (loss: 0.09235725551843643, acc: 0.9784366488456726)
[2025-02-13 03:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38][root][INFO] - Training Epoch: 1/2, step 6611/7134 completed (loss: 0.14193759858608246, acc: 0.9661538600921631)
[2025-02-13 03:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38][root][INFO] - Training Epoch: 1/2, step 6612/7134 completed (loss: 0.06877297908067703, acc: 0.9824970960617065)
[2025-02-13 03:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38][root][INFO] - Training Epoch: 1/2, step 6613/7134 completed (loss: 0.03835620358586311, acc: 0.9895833134651184)
[2025-02-13 03:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:39][root][INFO] - Training Epoch: 1/2, step 6614/7134 completed (loss: 0.07351738214492798, acc: 0.9845984578132629)
[2025-02-13 03:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:39][root][INFO] - Training Epoch: 1/2, step 6615/7134 completed (loss: 0.05126853287220001, acc: 0.9830268621444702)
[2025-02-13 03:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:40][root][INFO] - Training Epoch: 1/2, step 6616/7134 completed (loss: 0.04390104115009308, acc: 0.9894366264343262)
[2025-02-13 03:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:40][root][INFO] - Training Epoch: 1/2, step 6617/7134 completed (loss: 0.060556862503290176, acc: 0.9827089309692383)
[2025-02-13 03:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:41][root][INFO] - Training Epoch: 1/2, step 6618/7134 completed (loss: 0.035591427236795425, acc: 0.9881129264831543)
[2025-02-13 03:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:41][root][INFO] - Training Epoch: 1/2, step 6619/7134 completed (loss: 0.04021277278661728, acc: 0.9876681566238403)
[2025-02-13 03:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42][root][INFO] - Training Epoch: 1/2, step 6620/7134 completed (loss: 0.05020403861999512, acc: 0.9834938049316406)
[2025-02-13 03:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42][root][INFO] - Training Epoch: 1/2, step 6621/7134 completed (loss: 0.03571772202849388, acc: 0.989595353603363)
[2025-02-13 03:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42][root][INFO] - Training Epoch: 1/2, step 6622/7134 completed (loss: 0.06485089659690857, acc: 0.9822404384613037)
[2025-02-13 03:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:43][root][INFO] - Training Epoch: 1/2, step 6623/7134 completed (loss: 0.02743517793715, acc: 0.9899371266365051)
[2025-02-13 03:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:43][root][INFO] - Training Epoch: 1/2, step 6624/7134 completed (loss: 0.03115890920162201, acc: 0.9856770634651184)
[2025-02-13 03:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:44][root][INFO] - Training Epoch: 1/2, step 6625/7134 completed (loss: 0.023420017212629318, acc: 0.9903714060783386)
[2025-02-13 03:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:44][root][INFO] - Training Epoch: 1/2, step 6626/7134 completed (loss: 0.019205987453460693, acc: 0.9951515197753906)
[2025-02-13 03:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45][root][INFO] - Training Epoch: 1/2, step 6627/7134 completed (loss: 0.03128823637962341, acc: 0.9881080985069275)
[2025-02-13 03:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45][root][INFO] - Training Epoch: 1/2, step 6628/7134 completed (loss: 0.06453536450862885, acc: 0.9822485446929932)
[2025-02-13 03:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45][root][INFO] - Training Epoch: 1/2, step 6629/7134 completed (loss: 0.06583660840988159, acc: 0.9884467124938965)
[2025-02-13 03:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:46][root][INFO] - Training Epoch: 1/2, step 6630/7134 completed (loss: 0.058146312832832336, acc: 0.9843924045562744)
[2025-02-13 03:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:46][root][INFO] - Training Epoch: 1/2, step 6631/7134 completed (loss: 0.04787677899003029, acc: 0.9855832457542419)
[2025-02-13 03:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:47][root][INFO] - Training Epoch: 1/2, step 6632/7134 completed (loss: 0.04141195863485336, acc: 0.9876265525817871)
[2025-02-13 03:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:47][root][INFO] - Training Epoch: 1/2, step 6633/7134 completed (loss: 0.0491056852042675, acc: 0.9890244007110596)
[2025-02-13 03:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48][root][INFO] - Training Epoch: 1/2, step 6634/7134 completed (loss: 0.05887427181005478, acc: 0.9838998317718506)
[2025-02-13 03:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48][root][INFO] - Training Epoch: 1/2, step 6635/7134 completed (loss: 0.12433820217847824, acc: 0.971867024898529)
[2025-02-13 03:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48][root][INFO] - Training Epoch: 1/2, step 6636/7134 completed (loss: 0.05217595770955086, acc: 0.982876718044281)
[2025-02-13 03:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:49][root][INFO] - Training Epoch: 1/2, step 6637/7134 completed (loss: 0.044784605503082275, acc: 0.9898107647895813)
[2025-02-13 03:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:49][root][INFO] - Training Epoch: 1/2, step 6638/7134 completed (loss: 0.028258932754397392, acc: 0.9923760890960693)
[2025-02-13 03:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:50][root][INFO] - Training Epoch: 1/2, step 6639/7134 completed (loss: 0.03206288442015648, acc: 0.9910581111907959)
[2025-02-13 03:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:50][root][INFO] - Training Epoch: 1/2, step 6640/7134 completed (loss: 0.022887863218784332, acc: 0.9908116459846497)
[2025-02-13 03:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:51][root][INFO] - Training Epoch: 1/2, step 6641/7134 completed (loss: 0.09377200156450272, acc: 0.9765533208847046)
[2025-02-13 03:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:51][root][INFO] - Training Epoch: 1/2, step 6642/7134 completed (loss: 0.049567677080631256, acc: 0.983433723449707)
[2025-02-13 03:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52][root][INFO] - Training Epoch: 1/2, step 6643/7134 completed (loss: 0.0384051650762558, acc: 0.9907529950141907)
[2025-02-13 03:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52][root][INFO] - Training Epoch: 1/2, step 6644/7134 completed (loss: 0.06540659815073013, acc: 0.9847406148910522)
[2025-02-13 03:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52][root][INFO] - Training Epoch: 1/2, step 6645/7134 completed (loss: 0.029731862246990204, acc: 0.9926289916038513)
[2025-02-13 03:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:53][root][INFO] - Training Epoch: 1/2, step 6646/7134 completed (loss: 0.04250229895114899, acc: 0.9878854751586914)
[2025-02-13 03:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:53][root][INFO] - Training Epoch: 1/2, step 6647/7134 completed (loss: 0.02560625970363617, acc: 0.9932065010070801)
[2025-02-13 03:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:54][root][INFO] - Training Epoch: 1/2, step 6648/7134 completed (loss: 0.09770096838474274, acc: 0.9760000109672546)
[2025-02-13 03:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:54][root][INFO] - Training Epoch: 1/2, step 6649/7134 completed (loss: 0.057511523365974426, acc: 0.9834710955619812)
[2025-02-13 03:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55][root][INFO] - Training Epoch: 1/2, step 6650/7134 completed (loss: 0.026994401589035988, acc: 0.9921875)
[2025-02-13 03:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55][root][INFO] - Training Epoch: 1/2, step 6651/7134 completed (loss: 0.02784392610192299, acc: 0.9931787252426147)
[2025-02-13 03:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:56][root][INFO] - Training Epoch: 1/2, step 6652/7134 completed (loss: 0.045621249824762344, acc: 0.9867149591445923)
[2025-02-13 03:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:56][root][INFO] - Training Epoch: 1/2, step 6653/7134 completed (loss: 0.044551149010658264, acc: 0.9835706353187561)
[2025-02-13 03:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57][root][INFO] - Training Epoch: 1/2, step 6654/7134 completed (loss: 0.08658469468355179, acc: 0.9798449873924255)
[2025-02-13 03:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57][root][INFO] - Training Epoch: 1/2, step 6655/7134 completed (loss: 0.05669471621513367, acc: 0.9826302528381348)
[2025-02-13 03:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57][root][INFO] - Training Epoch: 1/2, step 6656/7134 completed (loss: 0.0660855770111084, acc: 0.983460545539856)
[2025-02-13 03:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58][root][INFO] - Training Epoch: 1/2, step 6657/7134 completed (loss: 0.05477107688784599, acc: 0.9837398529052734)
[2025-02-13 03:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58][root][INFO] - Training Epoch: 1/2, step 6658/7134 completed (loss: 0.01841777190566063, acc: 0.9922580718994141)
[2025-02-13 03:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:59][root][INFO] - Training Epoch: 1/2, step 6659/7134 completed (loss: 0.05138453468680382, acc: 0.9869791865348816)
[2025-02-13 03:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:59][root][INFO] - Training Epoch: 1/2, step 6660/7134 completed (loss: 0.029594892635941505, acc: 0.9920634627342224)
[2025-02-13 03:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:00][root][INFO] - Training Epoch: 1/2, step 6661/7134 completed (loss: 0.03238046541810036, acc: 0.9863861203193665)
[2025-02-13 03:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:00][root][INFO] - Training Epoch: 1/2, step 6662/7134 completed (loss: 0.0646599531173706, acc: 0.9785407781600952)
[2025-02-13 03:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01][root][INFO] - Training Epoch: 1/2, step 6663/7134 completed (loss: 0.03527991846203804, acc: 0.9890710115432739)
[2025-02-13 03:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01][root][INFO] - Training Epoch: 1/2, step 6664/7134 completed (loss: 0.12478387355804443, acc: 0.9612545967102051)
[2025-02-13 03:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01][root][INFO] - Training Epoch: 1/2, step 6665/7134 completed (loss: 0.1015033945441246, acc: 0.9626623392105103)
[2025-02-13 03:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:02][root][INFO] - Training Epoch: 1/2, step 6666/7134 completed (loss: 0.032294049859046936, acc: 0.989847719669342)
[2025-02-13 03:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:02][root][INFO] - Training Epoch: 1/2, step 6667/7134 completed (loss: 0.0504625178873539, acc: 0.982332170009613)
[2025-02-13 03:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03][root][INFO] - Training Epoch: 1/2, step 6668/7134 completed (loss: 0.01584608294069767, acc: 0.9930675625801086)
[2025-02-13 03:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03][root][INFO] - Training Epoch: 1/2, step 6669/7134 completed (loss: 0.03864308074116707, acc: 0.9882550239562988)
[2025-02-13 03:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03][root][INFO] - Training Epoch: 1/2, step 6670/7134 completed (loss: 0.032885827124118805, acc: 0.9916387796401978)
[2025-02-13 03:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:04][root][INFO] - Training Epoch: 1/2, step 6671/7134 completed (loss: 0.031191306188702583, acc: 0.9897360801696777)
[2025-02-13 03:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:04][root][INFO] - Training Epoch: 1/2, step 6672/7134 completed (loss: 0.02572740986943245, acc: 0.9887429475784302)
[2025-02-13 03:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05][root][INFO] - Training Epoch: 1/2, step 6673/7134 completed (loss: 0.02646365761756897, acc: 0.9897210001945496)
[2025-02-13 03:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05][root][INFO] - Training Epoch: 1/2, step 6674/7134 completed (loss: 0.019127901643514633, acc: 0.9941973090171814)
[2025-02-13 03:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05][root][INFO] - Training Epoch: 1/2, step 6675/7134 completed (loss: 0.03886038437485695, acc: 0.9950617551803589)
[2025-02-13 03:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:06][root][INFO] - Training Epoch: 1/2, step 6676/7134 completed (loss: 0.03457066789269447, acc: 0.9880668520927429)
[2025-02-13 03:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:06][root][INFO] - Training Epoch: 1/2, step 6677/7134 completed (loss: 0.07139384746551514, acc: 0.9809160232543945)
[2025-02-13 03:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07][root][INFO] - Training Epoch: 1/2, step 6678/7134 completed (loss: 0.017350945621728897, acc: 0.9964285492897034)
[2025-02-13 03:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07][root][INFO] - Training Epoch: 1/2, step 6679/7134 completed (loss: 0.014092200435698032, acc: 0.9956331849098206)
[2025-02-13 03:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07][root][INFO] - Training Epoch: 1/2, step 6680/7134 completed (loss: 0.011904614977538586, acc: 0.9972972869873047)
[2025-02-13 03:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08][root][INFO] - Training Epoch: 1/2, step 6681/7134 completed (loss: 0.010988127440214157, acc: 0.9958419799804688)
[2025-02-13 03:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08][root][INFO] - Training Epoch: 1/2, step 6682/7134 completed (loss: 0.04695175215601921, acc: 0.9906542301177979)
[2025-02-13 03:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08][root][INFO] - Training Epoch: 1/2, step 6683/7134 completed (loss: 0.025457199662923813, acc: 0.9890109896659851)
[2025-02-13 03:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:09][root][INFO] - Training Epoch: 1/2, step 6684/7134 completed (loss: 0.039180219173431396, acc: 0.987500011920929)
[2025-02-13 03:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:09][root][INFO] - Training Epoch: 1/2, step 6685/7134 completed (loss: 0.03275972977280617, acc: 0.9923664331436157)
[2025-02-13 03:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10][root][INFO] - Training Epoch: 1/2, step 6686/7134 completed (loss: 0.0875590518116951, acc: 0.9793388247489929)
[2025-02-13 03:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10][root][INFO] - Training Epoch: 1/2, step 6687/7134 completed (loss: 0.08751935511827469, acc: 0.9691358208656311)
[2025-02-13 03:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10][root][INFO] - Training Epoch: 1/2, step 6688/7134 completed (loss: 0.1082226037979126, acc: 0.975944995880127)
[2025-02-13 03:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:11][root][INFO] - Training Epoch: 1/2, step 6689/7134 completed (loss: 0.07142925262451172, acc: 0.9841583967208862)
[2025-02-13 03:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:11][root][INFO] - Training Epoch: 1/2, step 6690/7134 completed (loss: 0.15189312398433685, acc: 0.9539951682090759)
[2025-02-13 03:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12][root][INFO] - Training Epoch: 1/2, step 6691/7134 completed (loss: 0.030218513682484627, acc: 0.9934533834457397)
[2025-02-13 03:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12][root][INFO] - Training Epoch: 1/2, step 6692/7134 completed (loss: 0.03894323855638504, acc: 0.9851694703102112)
[2025-02-13 03:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12][root][INFO] - Training Epoch: 1/2, step 6693/7134 completed (loss: 0.043462276458740234, acc: 0.9818481802940369)
[2025-02-13 03:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:13][root][INFO] - Training Epoch: 1/2, step 6694/7134 completed (loss: 0.07792405784130096, acc: 0.9785932898521423)
[2025-02-13 03:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:13][root][INFO] - Training Epoch: 1/2, step 6695/7134 completed (loss: 0.053213778883218765, acc: 0.9876881241798401)
[2025-02-13 03:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14][root][INFO] - Training Epoch: 1/2, step 6696/7134 completed (loss: 0.08793505281209946, acc: 0.9845201373100281)
[2025-02-13 03:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14][root][INFO] - Training Epoch: 1/2, step 6697/7134 completed (loss: 0.0945705696940422, acc: 0.9688524603843689)
[2025-02-13 03:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14][root][INFO] - Training Epoch: 1/2, step 6698/7134 completed (loss: 0.028600484132766724, acc: 0.9896142482757568)
[2025-02-13 03:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:15][root][INFO] - Training Epoch: 1/2, step 6699/7134 completed (loss: 0.05296703800559044, acc: 0.9886105060577393)
[2025-02-13 03:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:15][root][INFO] - Training Epoch: 1/2, step 6700/7134 completed (loss: 0.07999330759048462, acc: 0.9779411554336548)
[2025-02-13 03:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16][root][INFO] - Training Epoch: 1/2, step 6701/7134 completed (loss: 0.12761983275413513, acc: 0.9682539701461792)
[2025-02-13 03:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16][root][INFO] - Training Epoch: 1/2, step 6702/7134 completed (loss: 0.05984361842274666, acc: 0.9783890247344971)
[2025-02-13 03:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16][root][INFO] - Training Epoch: 1/2, step 6703/7134 completed (loss: 0.11753363162279129, acc: 0.9671052694320679)
[2025-02-13 03:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:17][root][INFO] - Training Epoch: 1/2, step 6704/7134 completed (loss: 0.1401250958442688, acc: 0.963777482509613)
[2025-02-13 03:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:17][root][INFO] - Training Epoch: 1/2, step 6705/7134 completed (loss: 0.06640070676803589, acc: 0.9869961142539978)
[2025-02-13 03:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:18][root][INFO] - Training Epoch: 1/2, step 6706/7134 completed (loss: 0.05342848598957062, acc: 0.9834710955619812)
[2025-02-13 03:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:18][root][INFO] - Training Epoch: 1/2, step 6707/7134 completed (loss: 0.07739346474409103, acc: 0.9783037304878235)
[2025-02-13 03:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19][root][INFO] - Training Epoch: 1/2, step 6708/7134 completed (loss: 0.059147465974092484, acc: 0.9834983348846436)
[2025-02-13 03:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19][root][INFO] - Training Epoch: 1/2, step 6709/7134 completed (loss: 0.03880373015999794, acc: 0.9895209670066833)
[2025-02-13 03:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19][root][INFO] - Training Epoch: 1/2, step 6710/7134 completed (loss: 0.10314718633890152, acc: 0.97074955701828)
[2025-02-13 03:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:20][root][INFO] - Training Epoch: 1/2, step 6711/7134 completed (loss: 0.043386273086071014, acc: 0.9936507940292358)
[2025-02-13 03:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:20][root][INFO] - Training Epoch: 1/2, step 6712/7134 completed (loss: 0.06812155246734619, acc: 0.9818456768989563)
[2025-02-13 03:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:21][root][INFO] - Training Epoch: 1/2, step 6713/7134 completed (loss: 0.06055489182472229, acc: 0.9826338887214661)
[2025-02-13 03:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:21][root][INFO] - Training Epoch: 1/2, step 6714/7134 completed (loss: 0.04502253606915474, acc: 0.9821693897247314)
[2025-02-13 03:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22][root][INFO] - Training Epoch: 1/2, step 6715/7134 completed (loss: 0.0740635097026825, acc: 0.9748822450637817)
[2025-02-13 03:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22][root][INFO] - Training Epoch: 1/2, step 6716/7134 completed (loss: 0.01933511719107628, acc: 0.9940119981765747)
[2025-02-13 03:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22][root][INFO] - Training Epoch: 1/2, step 6717/7134 completed (loss: 0.038884349167346954, acc: 0.988727867603302)
[2025-02-13 03:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:23][root][INFO] - Training Epoch: 1/2, step 6718/7134 completed (loss: 0.0765087679028511, acc: 0.978787899017334)
[2025-02-13 03:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:23][root][INFO] - Training Epoch: 1/2, step 6719/7134 completed (loss: 0.03557443991303444, acc: 0.9901960492134094)
[2025-02-13 03:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24][root][INFO] - Training Epoch: 1/2, step 6720/7134 completed (loss: 0.08443547040224075, acc: 0.9832402467727661)
[2025-02-13 03:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24][root][INFO] - Training Epoch: 1/2, step 6721/7134 completed (loss: 0.06623189151287079, acc: 0.9843993782997131)
[2025-02-13 03:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24][root][INFO] - Training Epoch: 1/2, step 6722/7134 completed (loss: 0.1072683185338974, acc: 0.9718804955482483)
[2025-02-13 03:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:25][root][INFO] - Training Epoch: 1/2, step 6723/7134 completed (loss: 0.028633343055844307, acc: 0.9888888597488403)
[2025-02-13 03:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:25][root][INFO] - Training Epoch: 1/2, step 6724/7134 completed (loss: 0.051985740661621094, acc: 0.9838945865631104)
[2025-02-13 03:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26][root][INFO] - Training Epoch: 1/2, step 6725/7134 completed (loss: 0.03915558010339737, acc: 0.9863481521606445)
[2025-02-13 03:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26][root][INFO] - Training Epoch: 1/2, step 6726/7134 completed (loss: 0.08817669004201889, acc: 0.9799196720123291)
[2025-02-13 03:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26][root][INFO] - Training Epoch: 1/2, step 6727/7134 completed (loss: 0.07043122500181198, acc: 0.9841549396514893)
[2025-02-13 03:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27][root][INFO] - Training Epoch: 1/2, step 6728/7134 completed (loss: 0.06064993143081665, acc: 0.9892473220825195)
[2025-02-13 03:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27][root][INFO] - Training Epoch: 1/2, step 6729/7134 completed (loss: 0.039298415184020996, acc: 0.99048912525177)
[2025-02-13 03:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28][root][INFO] - Training Epoch: 1/2, step 6730/7134 completed (loss: 0.039280809462070465, acc: 0.9921507239341736)
[2025-02-13 03:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28][root][INFO] - Training Epoch: 1/2, step 6731/7134 completed (loss: 0.04457844793796539, acc: 0.9864253401756287)
[2025-02-13 03:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28][root][INFO] - Training Epoch: 1/2, step 6732/7134 completed (loss: 0.05772901326417923, acc: 0.9858490824699402)
[2025-02-13 03:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:29][root][INFO] - Training Epoch: 1/2, step 6733/7134 completed (loss: 0.046377185732126236, acc: 0.9886147975921631)
[2025-02-13 03:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:29][root][INFO] - Training Epoch: 1/2, step 6734/7134 completed (loss: 0.06825718283653259, acc: 0.9869158864021301)
[2025-02-13 03:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30][root][INFO] - Training Epoch: 1/2, step 6735/7134 completed (loss: 0.07440482079982758, acc: 0.981566846370697)
[2025-02-13 03:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30][root][INFO] - Training Epoch: 1/2, step 6736/7134 completed (loss: 0.05363829433917999, acc: 0.9874652028083801)
[2025-02-13 03:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30][root][INFO] - Training Epoch: 1/2, step 6737/7134 completed (loss: 0.06474632769823074, acc: 0.9821717739105225)
[2025-02-13 03:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31][root][INFO] - Training Epoch: 1/2, step 6738/7134 completed (loss: 0.050370484590530396, acc: 0.986940324306488)
[2025-02-13 03:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31][root][INFO] - Training Epoch: 1/2, step 6739/7134 completed (loss: 0.03966331109404564, acc: 0.9871323704719543)
[2025-02-13 03:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32][root][INFO] - Training Epoch: 1/2, step 6740/7134 completed (loss: 0.049440376460552216, acc: 0.9881154298782349)
[2025-02-13 03:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32][root][INFO] - Training Epoch: 1/2, step 6741/7134 completed (loss: 0.03650674596428871, acc: 0.9898403286933899)
[2025-02-13 03:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32][root][INFO] - Training Epoch: 1/2, step 6742/7134 completed (loss: 0.028566401451826096, acc: 0.9927849769592285)
[2025-02-13 03:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:33][root][INFO] - Training Epoch: 1/2, step 6743/7134 completed (loss: 0.01878226175904274, acc: 0.9959127902984619)
[2025-02-13 03:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:33][root][INFO] - Training Epoch: 1/2, step 6744/7134 completed (loss: 0.02775096893310547, acc: 0.9935897588729858)
[2025-02-13 03:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34][root][INFO] - Training Epoch: 1/2, step 6745/7134 completed (loss: 0.009363234974443913, acc: 0.9972489476203918)
[2025-02-13 03:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34][root][INFO] - Training Epoch: 1/2, step 6746/7134 completed (loss: 0.023617558181285858, acc: 0.9894179701805115)
[2025-02-13 03:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34][root][INFO] - Training Epoch: 1/2, step 6747/7134 completed (loss: 0.03038671240210533, acc: 0.992546558380127)
[2025-02-13 03:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:35][root][INFO] - Training Epoch: 1/2, step 6748/7134 completed (loss: 0.026094451546669006, acc: 0.9954904317855835)
[2025-02-13 03:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:35][root][INFO] - Training Epoch: 1/2, step 6749/7134 completed (loss: 0.04819527640938759, acc: 0.9891067743301392)
[2025-02-13 03:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:36][root][INFO] - Training Epoch: 1/2, step 6750/7134 completed (loss: 0.019885988906025887, acc: 0.9965397715568542)
[2025-02-13 03:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:36][root][INFO] - Training Epoch: 1/2, step 6751/7134 completed (loss: 0.017166469246149063, acc: 0.9954338073730469)
[2025-02-13 03:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37][root][INFO] - Training Epoch: 1/2, step 6752/7134 completed (loss: 0.020496508106589317, acc: 0.9898403286933899)
[2025-02-13 03:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37][root][INFO] - Training Epoch: 1/2, step 6753/7134 completed (loss: 0.01924951747059822, acc: 0.9965596199035645)
[2025-02-13 03:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38][root][INFO] - Training Epoch: 1/2, step 6754/7134 completed (loss: 0.0071080648340284824, acc: 0.9975903630256653)
[2025-02-13 03:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38][root][INFO] - Training Epoch: 1/2, step 6755/7134 completed (loss: 0.013677412644028664, acc: 0.9963054060935974)
[2025-02-13 03:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38][root][INFO] - Training Epoch: 1/2, step 6756/7134 completed (loss: 0.007837309502065182, acc: 0.9986594915390015)
[2025-02-13 03:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:39][root][INFO] - Training Epoch: 1/2, step 6757/7134 completed (loss: 0.023327501490712166, acc: 0.9939024448394775)
[2025-02-13 03:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:39][root][INFO] - Training Epoch: 1/2, step 6758/7134 completed (loss: 0.01817391999065876, acc: 0.9961190223693848)
[2025-02-13 03:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40][root][INFO] - Training Epoch: 1/2, step 6759/7134 completed (loss: 0.04567903280258179, acc: 0.9876957535743713)
[2025-02-13 03:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40][root][INFO] - Training Epoch: 1/2, step 6760/7134 completed (loss: 0.05004842206835747, acc: 0.9855642914772034)
[2025-02-13 03:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:41][root][INFO] - Training Epoch: 1/2, step 6761/7134 completed (loss: 0.03512478619813919, acc: 0.9903614521026611)
[2025-02-13 03:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:41][root][INFO] - Training Epoch: 1/2, step 6762/7134 completed (loss: 0.05150426924228668, acc: 0.9901960492134094)
[2025-02-13 03:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42][root][INFO] - Training Epoch: 1/2, step 6763/7134 completed (loss: 0.017336314544081688, acc: 0.9949811697006226)
[2025-02-13 03:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42][root][INFO] - Training Epoch: 1/2, step 6764/7134 completed (loss: 0.029835863038897514, acc: 0.9928160905838013)
[2025-02-13 03:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42][root][INFO] - Training Epoch: 1/2, step 6765/7134 completed (loss: 0.025219090282917023, acc: 0.9880159497261047)
[2025-02-13 03:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:43][root][INFO] - Training Epoch: 1/2, step 6766/7134 completed (loss: 0.020995846018195152, acc: 0.9917241334915161)
[2025-02-13 03:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:43][root][INFO] - Training Epoch: 1/2, step 6767/7134 completed (loss: 0.06581172347068787, acc: 0.9887387156486511)
[2025-02-13 03:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44][root][INFO] - Training Epoch: 1/2, step 6768/7134 completed (loss: 0.026928644627332687, acc: 0.9904943108558655)
[2025-02-13 03:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44][root][INFO] - Training Epoch: 1/2, step 6769/7134 completed (loss: 0.018838031217455864, acc: 0.9922839403152466)
[2025-02-13 03:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44][root][INFO] - Training Epoch: 1/2, step 6770/7134 completed (loss: 0.04666215926408768, acc: 0.9812080264091492)
[2025-02-13 03:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:45][root][INFO] - Training Epoch: 1/2, step 6771/7134 completed (loss: 0.024922169744968414, acc: 0.9936908483505249)
[2025-02-13 03:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:45][root][INFO] - Training Epoch: 1/2, step 6772/7134 completed (loss: 0.09798463433980942, acc: 0.9786096215248108)
[2025-02-13 03:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46][root][INFO] - Training Epoch: 1/2, step 6773/7134 completed (loss: 0.0491274893283844, acc: 0.9857594966888428)
[2025-02-13 03:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46][root][INFO] - Training Epoch: 1/2, step 6774/7134 completed (loss: 0.0619531087577343, acc: 0.987261176109314)
[2025-02-13 03:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46][root][INFO] - Training Epoch: 1/2, step 6775/7134 completed (loss: 0.020432496443390846, acc: 0.9943289160728455)
[2025-02-13 03:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47][root][INFO] - Training Epoch: 1/2, step 6776/7134 completed (loss: 0.04265149310231209, acc: 0.9851064085960388)
[2025-02-13 03:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47][root][INFO] - Training Epoch: 1/2, step 6777/7134 completed (loss: 0.03328949213027954, acc: 0.991946280002594)
[2025-02-13 03:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:48][root][INFO] - Training Epoch: 1/2, step 6778/7134 completed (loss: 0.06456760317087173, acc: 0.9836552739143372)
[2025-02-13 03:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:48][root][INFO] - Training Epoch: 1/2, step 6779/7134 completed (loss: 0.031042039394378662, acc: 0.9880596995353699)
[2025-02-13 03:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49][root][INFO] - Training Epoch: 1/2, step 6780/7134 completed (loss: 0.021471073850989342, acc: 0.9925261735916138)
[2025-02-13 03:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49][root][INFO] - Training Epoch: 1/2, step 6781/7134 completed (loss: 0.031411148607730865, acc: 0.9910179376602173)
[2025-02-13 03:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49][root][INFO] - Training Epoch: 1/2, step 6782/7134 completed (loss: 0.01942262053489685, acc: 0.9982638955116272)
[2025-02-13 03:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:50][root][INFO] - Training Epoch: 1/2, step 6783/7134 completed (loss: 0.05120869725942612, acc: 0.9904030561447144)
[2025-02-13 03:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:50][root][INFO] - Training Epoch: 1/2, step 6784/7134 completed (loss: 0.035311684012413025, acc: 0.9916805028915405)
[2025-02-13 03:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51][root][INFO] - Training Epoch: 1/2, step 6785/7134 completed (loss: 0.024478338658809662, acc: 0.9889937043190002)
[2025-02-13 03:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51][root][INFO] - Training Epoch: 1/2, step 6786/7134 completed (loss: 0.010927128605544567, acc: 0.9956834316253662)
[2025-02-13 03:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51][root][INFO] - Training Epoch: 1/2, step 6787/7134 completed (loss: 0.0300610288977623, acc: 0.9894737005233765)
[2025-02-13 03:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:52][root][INFO] - Training Epoch: 1/2, step 6788/7134 completed (loss: 0.01979290135204792, acc: 0.9964028596878052)
[2025-02-13 03:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:52][root][INFO] - Training Epoch: 1/2, step 6789/7134 completed (loss: 0.02125932276248932, acc: 0.994163453578949)
[2025-02-13 03:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:53][root][INFO] - Training Epoch: 1/2, step 6790/7134 completed (loss: 0.061446502804756165, acc: 0.9870689511299133)
[2025-02-13 03:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:53][root][INFO] - Training Epoch: 1/2, step 6791/7134 completed (loss: 0.024864671751856804, acc: 0.9931856989860535)
[2025-02-13 03:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54][root][INFO] - Training Epoch: 1/2, step 6792/7134 completed (loss: 0.02546541765332222, acc: 0.9908257126808167)
[2025-02-13 03:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54][root][INFO] - Training Epoch: 1/2, step 6793/7134 completed (loss: 0.028570784255862236, acc: 0.9894514679908752)
[2025-02-13 03:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54][root][INFO] - Training Epoch: 1/2, step 6794/7134 completed (loss: 0.028639184311032295, acc: 0.9967159032821655)
[2025-02-13 03:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:55][root][INFO] - Training Epoch: 1/2, step 6795/7134 completed (loss: 0.01371240708976984, acc: 0.9961038827896118)
[2025-02-13 03:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:55][root][INFO] - Training Epoch: 1/2, step 6796/7134 completed (loss: 0.015209193341434002, acc: 0.9956011772155762)
[2025-02-13 03:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56][root][INFO] - Training Epoch: 1/2, step 6797/7134 completed (loss: 0.07959665358066559, acc: 0.9796215295791626)
[2025-02-13 03:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56][root][INFO] - Training Epoch: 1/2, step 6798/7134 completed (loss: 0.04100082442164421, acc: 0.9834710955619812)
[2025-02-13 03:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56][root][INFO] - Training Epoch: 1/2, step 6799/7134 completed (loss: 0.030056195333600044, acc: 0.9904240965843201)
[2025-02-13 03:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:57][root][INFO] - Training Epoch: 1/2, step 6800/7134 completed (loss: 0.03870441019535065, acc: 0.9875690340995789)
[2025-02-13 03:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:57][root][INFO] - Training Epoch: 1/2, step 6801/7134 completed (loss: 0.03557492047548294, acc: 0.9905511736869812)
[2025-02-13 03:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:58][root][INFO] - Training Epoch: 1/2, step 6802/7134 completed (loss: 0.027947066351771355, acc: 0.9912152290344238)
[2025-02-13 03:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:58][root][INFO] - Training Epoch: 1/2, step 6803/7134 completed (loss: 0.01884339191019535, acc: 0.9937106966972351)
[2025-02-13 03:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59][root][INFO] - Training Epoch: 1/2, step 6804/7134 completed (loss: 0.03016108274459839, acc: 0.9921875)
[2025-02-13 03:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59][root][INFO] - Training Epoch: 1/2, step 6805/7134 completed (loss: 0.056796930730342865, acc: 0.9855769276618958)
[2025-02-13 03:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59][root][INFO] - Training Epoch: 1/2, step 6806/7134 completed (loss: 0.054380372166633606, acc: 0.9898734092712402)
[2025-02-13 03:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:00][root][INFO] - Training Epoch: 1/2, step 6807/7134 completed (loss: 0.01964714005589485, acc: 0.9956896305084229)
[2025-02-13 03:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:00][root][INFO] - Training Epoch: 1/2, step 6808/7134 completed (loss: 0.03715858235955238, acc: 0.9904305934906006)
[2025-02-13 03:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:01][root][INFO] - Training Epoch: 1/2, step 6809/7134 completed (loss: 0.0523807592689991, acc: 0.989159882068634)
[2025-02-13 03:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:01][root][INFO] - Training Epoch: 1/2, step 6810/7134 completed (loss: 0.025969669222831726, acc: 0.9947368502616882)
[2025-02-13 03:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02][root][INFO] - Training Epoch: 1/2, step 6811/7134 completed (loss: 0.05286980792880058, acc: 0.9868938326835632)
[2025-02-13 03:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02][root][INFO] - Training Epoch: 1/2, step 6812/7134 completed (loss: 0.036973197013139725, acc: 0.990275502204895)
[2025-02-13 03:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02][root][INFO] - Training Epoch: 1/2, step 6813/7134 completed (loss: 0.011859326623380184, acc: 0.9985915422439575)
[2025-02-13 03:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:03][root][INFO] - Training Epoch: 1/2, step 6814/7134 completed (loss: 0.03405684605240822, acc: 0.9905914068222046)
[2025-02-13 03:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:03][root][INFO] - Training Epoch: 1/2, step 6815/7134 completed (loss: 0.04192885383963585, acc: 0.9881734848022461)
[2025-02-13 03:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:04][root][INFO] - Training Epoch: 1/2, step 6816/7134 completed (loss: 0.03635149821639061, acc: 0.9933110475540161)
[2025-02-13 03:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:04][root][INFO] - Training Epoch: 1/2, step 6817/7134 completed (loss: 0.05935417115688324, acc: 0.9863247871398926)
[2025-02-13 03:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05][root][INFO] - Training Epoch: 1/2, step 6818/7134 completed (loss: 0.021195001900196075, acc: 0.9910045266151428)
[2025-02-13 03:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05][root][INFO] - Training Epoch: 1/2, step 6819/7134 completed (loss: 0.054145876318216324, acc: 0.9820144176483154)
[2025-02-13 03:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05][root][INFO] - Training Epoch: 1/2, step 6820/7134 completed (loss: 0.032240431755781174, acc: 0.9927927851676941)
[2025-02-13 03:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:06][root][INFO] - Training Epoch: 1/2, step 6821/7134 completed (loss: 0.02578376792371273, acc: 0.9939393997192383)
[2025-02-13 03:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:06][root][INFO] - Training Epoch: 1/2, step 6822/7134 completed (loss: 0.05036180466413498, acc: 0.9895287752151489)
[2025-02-13 03:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07][root][INFO] - Training Epoch: 1/2, step 6823/7134 completed (loss: 0.03659866750240326, acc: 0.9928469061851501)
[2025-02-13 03:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07][root][INFO] - Training Epoch: 1/2, step 6824/7134 completed (loss: 0.07947912812232971, acc: 0.9737903475761414)
[2025-02-13 03:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07][root][INFO] - Training Epoch: 1/2, step 6825/7134 completed (loss: 0.025292444974184036, acc: 0.992682933807373)
[2025-02-13 03:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08][root][INFO] - Training Epoch: 1/2, step 6826/7134 completed (loss: 0.05156208574771881, acc: 0.9865319728851318)
[2025-02-13 03:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08][root][INFO] - Training Epoch: 1/2, step 6827/7134 completed (loss: 0.13411006331443787, acc: 0.9664948582649231)
[2025-02-13 03:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08][root][INFO] - Training Epoch: 1/2, step 6828/7134 completed (loss: 0.054234642535448074, acc: 0.9835766553878784)
[2025-02-13 03:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:09][root][INFO] - Training Epoch: 1/2, step 6829/7134 completed (loss: 0.06326358020305634, acc: 0.9821958541870117)
[2025-02-13 03:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:09][root][INFO] - Training Epoch: 1/2, step 6830/7134 completed (loss: 0.039203524589538574, acc: 0.9851729869842529)
[2025-02-13 03:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:10][root][INFO] - Training Epoch: 1/2, step 6831/7134 completed (loss: 0.047635845839977264, acc: 0.9865591526031494)
[2025-02-13 03:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:10][root][INFO] - Training Epoch: 1/2, step 6832/7134 completed (loss: 0.061749618500471115, acc: 0.9815157055854797)
[2025-02-13 03:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11][root][INFO] - Training Epoch: 1/2, step 6833/7134 completed (loss: 0.04764460772275925, acc: 0.9903537034988403)
[2025-02-13 03:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11][root][INFO] - Training Epoch: 1/2, step 6834/7134 completed (loss: 0.052260495722293854, acc: 0.9846677780151367)
[2025-02-13 03:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11][root][INFO] - Training Epoch: 1/2, step 6835/7134 completed (loss: 0.03757098317146301, acc: 0.9886363744735718)
[2025-02-13 03:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:12][root][INFO] - Training Epoch: 1/2, step 6836/7134 completed (loss: 0.0561908483505249, acc: 0.9811557531356812)
[2025-02-13 03:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:12][root][INFO] - Training Epoch: 1/2, step 6837/7134 completed (loss: 0.04369721561670303, acc: 0.9901840686798096)
[2025-02-13 03:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13][root][INFO] - Training Epoch: 1/2, step 6838/7134 completed (loss: 0.07935424894094467, acc: 0.9757834672927856)
[2025-02-13 03:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13][root][INFO] - Training Epoch: 1/2, step 6839/7134 completed (loss: 0.103289894759655, acc: 0.9636963605880737)
[2025-02-13 03:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13][root][INFO] - Training Epoch: 1/2, step 6840/7134 completed (loss: 0.08407150954008102, acc: 0.9785100221633911)
[2025-02-13 03:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:14][root][INFO] - Training Epoch: 1/2, step 6841/7134 completed (loss: 0.06103816255927086, acc: 0.9797979593276978)
[2025-02-13 03:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:14][root][INFO] - Training Epoch: 1/2, step 6842/7134 completed (loss: 0.04376330226659775, acc: 0.9849624037742615)
[2025-02-13 03:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15][root][INFO] - Training Epoch: 1/2, step 6843/7134 completed (loss: 0.07121838629245758, acc: 0.9790382385253906)
[2025-02-13 03:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15][root][INFO] - Training Epoch: 1/2, step 6844/7134 completed (loss: 0.040909431874752045, acc: 0.9879102110862732)
[2025-02-13 03:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15][root][INFO] - Training Epoch: 1/2, step 6845/7134 completed (loss: 0.03818380460143089, acc: 0.9897959232330322)
[2025-02-13 03:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:16][root][INFO] - Training Epoch: 1/2, step 6846/7134 completed (loss: 0.050985872745513916, acc: 0.984240710735321)
[2025-02-13 03:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:16][root][INFO] - Training Epoch: 1/2, step 6847/7134 completed (loss: 0.053466491401195526, acc: 0.9833333492279053)
[2025-02-13 03:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17][root][INFO] - Training Epoch: 1/2, step 6848/7134 completed (loss: 0.023544836789369583, acc: 0.9945945739746094)
[2025-02-13 03:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17][root][INFO] - Training Epoch: 1/2, step 6849/7134 completed (loss: 0.04307379201054573, acc: 0.9862542748451233)
[2025-02-13 03:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17][root][INFO] - Training Epoch: 1/2, step 6850/7134 completed (loss: 0.03221212327480316, acc: 0.9908466935157776)
[2025-02-13 03:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:18][root][INFO] - Training Epoch: 1/2, step 6851/7134 completed (loss: 0.06269001215696335, acc: 0.9788434505462646)
[2025-02-13 03:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:18][root][INFO] - Training Epoch: 1/2, step 6852/7134 completed (loss: 0.030863719061017036, acc: 0.9930070042610168)
[2025-02-13 03:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:19][root][INFO] - Training Epoch: 1/2, step 6853/7134 completed (loss: 0.04340299218893051, acc: 0.9889349937438965)
[2025-02-13 03:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:19][root][INFO] - Training Epoch: 1/2, step 6854/7134 completed (loss: 0.0399387888610363, acc: 0.9934924244880676)
[2025-02-13 03:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20][root][INFO] - Training Epoch: 1/2, step 6855/7134 completed (loss: 0.012662398628890514, acc: 0.9952830076217651)
[2025-02-13 03:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20][root][INFO] - Training Epoch: 1/2, step 6856/7134 completed (loss: 0.011695623397827148, acc: 0.995708167552948)
[2025-02-13 03:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20][root][INFO] - Training Epoch: 1/2, step 6857/7134 completed (loss: 0.03503291681408882, acc: 0.9897360801696777)
[2025-02-13 03:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21][root][INFO] - Training Epoch: 1/2, step 6858/7134 completed (loss: 0.0099186385050416, acc: 0.9963099360466003)
[2025-02-13 03:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21][root][INFO] - Training Epoch: 1/2, step 6859/7134 completed (loss: 0.013085144571959972, acc: 0.9974392056465149)
[2025-02-13 03:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22][root][INFO] - Training Epoch: 1/2, step 6860/7134 completed (loss: 0.020892778411507607, acc: 0.9929078221321106)
[2025-02-13 03:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22][root][INFO] - Training Epoch: 1/2, step 6861/7134 completed (loss: 0.020050374791026115, acc: 0.9982699155807495)
[2025-02-13 03:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22][root][INFO] - Training Epoch: 1/2, step 6862/7134 completed (loss: 0.04206164553761482, acc: 0.9885714054107666)
[2025-02-13 03:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:23][root][INFO] - Training Epoch: 1/2, step 6863/7134 completed (loss: 0.0151585778221488, acc: 0.9957627058029175)
[2025-02-13 03:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:23][root][INFO] - Training Epoch: 1/2, step 6864/7134 completed (loss: 0.01834150403738022, acc: 0.9933444261550903)
[2025-02-13 03:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24][root][INFO] - Training Epoch: 1/2, step 6865/7134 completed (loss: 0.0481986440718174, acc: 0.983505129814148)
[2025-02-13 03:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24][root][INFO] - Training Epoch: 1/2, step 6866/7134 completed (loss: 0.007662036921828985, acc: 0.9984802603721619)
[2025-02-13 03:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24][root][INFO] - Training Epoch: 1/2, step 6867/7134 completed (loss: 0.008273014798760414, acc: 0.9962962865829468)
[2025-02-13 03:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:25][root][INFO] - Training Epoch: 1/2, step 6868/7134 completed (loss: 0.003515287535265088, acc: 1.0)
[2025-02-13 03:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:25][root][INFO] - Training Epoch: 1/2, step 6869/7134 completed (loss: 0.008780078962445259, acc: 1.0)
[2025-02-13 03:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26][root][INFO] - Training Epoch: 1/2, step 6870/7134 completed (loss: 0.017000460997223854, acc: 0.9951140284538269)
[2025-02-13 03:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26][root][INFO] - Training Epoch: 1/2, step 6871/7134 completed (loss: 0.005921522155404091, acc: 0.9981273412704468)
[2025-02-13 03:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26][root][INFO] - Training Epoch: 1/2, step 6872/7134 completed (loss: 0.009767013601958752, acc: 0.9944598078727722)
[2025-02-13 03:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27][root][INFO] - Training Epoch: 1/2, step 6873/7134 completed (loss: 0.01069968193769455, acc: 0.9971988797187805)
[2025-02-13 03:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27][root][INFO] - Training Epoch: 1/2, step 6874/7134 completed (loss: 0.02097279392182827, acc: 0.9946808218955994)
[2025-02-13 03:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28][root][INFO] - Training Epoch: 1/2, step 6875/7134 completed (loss: 0.01837964914739132, acc: 0.9938176274299622)
[2025-02-13 03:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28][root][INFO] - Training Epoch: 1/2, step 6876/7134 completed (loss: 0.031843941658735275, acc: 0.989847719669342)
[2025-02-13 03:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28][root][INFO] - Training Epoch: 1/2, step 6877/7134 completed (loss: 0.020529454573988914, acc: 0.992977499961853)
[2025-02-13 03:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:29][root][INFO] - Training Epoch: 1/2, step 6878/7134 completed (loss: 0.018484093248844147, acc: 0.9940029978752136)
[2025-02-13 03:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:29][root][INFO] - Training Epoch: 1/2, step 6879/7134 completed (loss: 0.028848011046648026, acc: 0.9895397424697876)
[2025-02-13 03:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30][root][INFO] - Training Epoch: 1/2, step 6880/7134 completed (loss: 0.021540183573961258, acc: 0.9941792488098145)
[2025-02-13 03:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30][root][INFO] - Training Epoch: 1/2, step 6881/7134 completed (loss: 0.08121339231729507, acc: 0.9854192137718201)
[2025-02-13 03:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30][root][INFO] - Training Epoch: 1/2, step 6882/7134 completed (loss: 0.044002655893564224, acc: 0.9849624037742615)
[2025-02-13 03:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:31][root][INFO] - Training Epoch: 1/2, step 6883/7134 completed (loss: 0.03198227658867836, acc: 0.9950494766235352)
[2025-02-13 03:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:31][root][INFO] - Training Epoch: 1/2, step 6884/7134 completed (loss: 0.06565587967634201, acc: 0.989924430847168)
[2025-02-13 03:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32][root][INFO] - Training Epoch: 1/2, step 6885/7134 completed (loss: 0.07353239506483078, acc: 0.9888392686843872)
[2025-02-13 03:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32][root][INFO] - Training Epoch: 1/2, step 6886/7134 completed (loss: 0.029902217909693718, acc: 0.9857369065284729)
[2025-02-13 03:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32][root][INFO] - Training Epoch: 1/2, step 6887/7134 completed (loss: 0.05529329180717468, acc: 0.9856262803077698)
[2025-02-13 03:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33][root][INFO] - Training Epoch: 1/2, step 6888/7134 completed (loss: 0.02765258587896824, acc: 0.9916943311691284)
[2025-02-13 03:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33][root][INFO] - Training Epoch: 1/2, step 6889/7134 completed (loss: 0.018930718302726746, acc: 0.9954057931900024)
[2025-02-13 03:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:34][root][INFO] - Training Epoch: 1/2, step 6890/7134 completed (loss: 0.03066015988588333, acc: 0.9907529950141907)
[2025-02-13 03:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:34][root][INFO] - Training Epoch: 1/2, step 6891/7134 completed (loss: 0.028159363195300102, acc: 0.9911110997200012)
[2025-02-13 03:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:35][root][INFO] - Training Epoch: 1/2, step 6892/7134 completed (loss: 0.028129061684012413, acc: 0.9908854365348816)
[2025-02-13 03:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:35][root][INFO] - Training Epoch: 1/2, step 6893/7134 completed (loss: 0.05748501420021057, acc: 0.980140209197998)
[2025-02-13 03:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36][root][INFO] - Training Epoch: 1/2, step 6894/7134 completed (loss: 0.054187845438718796, acc: 0.9855263233184814)
[2025-02-13 03:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36][root][INFO] - Training Epoch: 1/2, step 6895/7134 completed (loss: 0.04908636212348938, acc: 0.9820144176483154)
[2025-02-13 03:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36][root][INFO] - Training Epoch: 1/2, step 6896/7134 completed (loss: 0.025482501834630966, acc: 0.9898550510406494)
[2025-02-13 03:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:37][root][INFO] - Training Epoch: 1/2, step 6897/7134 completed (loss: 0.02159108966588974, acc: 0.9938650131225586)
[2025-02-13 03:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:37][root][INFO] - Training Epoch: 1/2, step 6898/7134 completed (loss: 0.021011438220739365, acc: 0.9945054650306702)
[2025-02-13 03:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38][root][INFO] - Training Epoch: 1/2, step 6899/7134 completed (loss: 0.029258498921990395, acc: 0.9899497628211975)
[2025-02-13 03:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38][root][INFO] - Training Epoch: 1/2, step 6900/7134 completed (loss: 0.047105442732572556, acc: 0.9801980257034302)
[2025-02-13 03:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38][root][INFO] - Training Epoch: 1/2, step 6901/7134 completed (loss: 0.03339057043194771, acc: 0.9920739531517029)
[2025-02-13 03:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:39][root][INFO] - Training Epoch: 1/2, step 6902/7134 completed (loss: 0.03882024809718132, acc: 0.9861303567886353)
[2025-02-13 03:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:39][root][INFO] - Training Epoch: 1/2, step 6903/7134 completed (loss: 0.028439445421099663, acc: 0.9936628937721252)
[2025-02-13 03:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:40][root][INFO] - Training Epoch: 1/2, step 6904/7134 completed (loss: 0.0143580948933959, acc: 0.996268630027771)
[2025-02-13 03:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:40][root][INFO] - Training Epoch: 1/2, step 6905/7134 completed (loss: 0.02867697924375534, acc: 0.9910485744476318)
[2025-02-13 03:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:41][root][INFO] - Training Epoch: 1/2, step 6906/7134 completed (loss: 0.0399249903857708, acc: 0.9928143620491028)
[2025-02-13 03:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:41][root][INFO] - Training Epoch: 1/2, step 6907/7134 completed (loss: 0.03760330379009247, acc: 0.9913151264190674)
[2025-02-13 03:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42][root][INFO] - Training Epoch: 1/2, step 6908/7134 completed (loss: 0.029449544847011566, acc: 0.9922279715538025)
[2025-02-13 03:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42][root][INFO] - Training Epoch: 1/2, step 6909/7134 completed (loss: 0.02024216204881668, acc: 0.996052622795105)
[2025-02-13 03:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42][root][INFO] - Training Epoch: 1/2, step 6910/7134 completed (loss: 0.016308236867189407, acc: 0.9961783289909363)
[2025-02-13 03:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:43][root][INFO] - Training Epoch: 1/2, step 6911/7134 completed (loss: 0.05535618215799332, acc: 0.9883720874786377)
[2025-02-13 03:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:43][root][INFO] - Training Epoch: 1/2, step 6912/7134 completed (loss: 0.028966201469302177, acc: 0.9962546825408936)
[2025-02-13 03:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:44][root][INFO] - Training Epoch: 1/2, step 6913/7134 completed (loss: 0.036127906292676926, acc: 0.986601710319519)
[2025-02-13 03:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:44][root][INFO] - Training Epoch: 1/2, step 6914/7134 completed (loss: 0.069719597697258, acc: 0.9865319728851318)
[2025-02-13 03:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:45][root][INFO] - Training Epoch: 1/2, step 6915/7134 completed (loss: 0.03514675050973892, acc: 0.984455943107605)
[2025-02-13 03:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:45][root][INFO] - Training Epoch: 1/2, step 6916/7134 completed (loss: 0.029915178194642067, acc: 0.9938650131225586)
[2025-02-13 03:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46][root][INFO] - Training Epoch: 1/2, step 6917/7134 completed (loss: 0.0674506351351738, acc: 0.9848484992980957)
[2025-02-13 03:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46][root][INFO] - Training Epoch: 1/2, step 6918/7134 completed (loss: 0.04192609712481499, acc: 0.9859485030174255)
[2025-02-13 03:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46][root][INFO] - Training Epoch: 1/2, step 6919/7134 completed (loss: 0.04388239234685898, acc: 0.9896193742752075)
[2025-02-13 03:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:47][root][INFO] - Training Epoch: 1/2, step 6920/7134 completed (loss: 0.020917804911732674, acc: 0.9914529919624329)
[2025-02-13 03:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:47][root][INFO] - Training Epoch: 1/2, step 6921/7134 completed (loss: 0.07356608659029007, acc: 0.9815242290496826)
[2025-02-13 03:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48][root][INFO] - Training Epoch: 1/2, step 6922/7134 completed (loss: 0.06638554483652115, acc: 0.9859594106674194)
[2025-02-13 03:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48][root][INFO] - Training Epoch: 1/2, step 6923/7134 completed (loss: 0.05884441360831261, acc: 0.9883268475532532)
[2025-02-13 03:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48][root][INFO] - Training Epoch: 1/2, step 6924/7134 completed (loss: 0.02824668027460575, acc: 0.9863760471343994)
[2025-02-13 03:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:49][root][INFO] - Training Epoch: 1/2, step 6925/7134 completed (loss: 0.031163794919848442, acc: 0.9895833134651184)
[2025-02-13 03:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:49][root][INFO] - Training Epoch: 1/2, step 6926/7134 completed (loss: 0.05633604899048805, acc: 0.978300154209137)
[2025-02-13 03:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:50][root][INFO] - Training Epoch: 1/2, step 6927/7134 completed (loss: 0.04323718324303627, acc: 0.986810564994812)
[2025-02-13 03:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:50][root][INFO] - Training Epoch: 1/2, step 6928/7134 completed (loss: 0.07134377211332321, acc: 0.9823874831199646)
[2025-02-13 03:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:51][root][INFO] - Training Epoch: 1/2, step 6929/7134 completed (loss: 0.06077529489994049, acc: 0.9830508232116699)
[2025-02-13 03:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:51][root][INFO] - Training Epoch: 1/2, step 6930/7134 completed (loss: 0.0773928314447403, acc: 0.9826302528381348)
[2025-02-13 03:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52][root][INFO] - Training Epoch: 1/2, step 6931/7134 completed (loss: 0.04628423973917961, acc: 0.9853658676147461)
[2025-02-13 03:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52][root][INFO] - Training Epoch: 1/2, step 6932/7134 completed (loss: 0.055856846272945404, acc: 0.9779411554336548)
[2025-02-13 03:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52][root][INFO] - Training Epoch: 1/2, step 6933/7134 completed (loss: 0.08944631367921829, acc: 0.9793014526367188)
[2025-02-13 03:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:53][root][INFO] - Training Epoch: 1/2, step 6934/7134 completed (loss: 0.05369286239147186, acc: 0.9807383418083191)
[2025-02-13 03:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:53][root][INFO] - Training Epoch: 1/2, step 6935/7134 completed (loss: 0.059145908802747726, acc: 0.985602080821991)
[2025-02-13 03:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:54][root][INFO] - Training Epoch: 1/2, step 6936/7134 completed (loss: 0.03691798448562622, acc: 0.9866666793823242)
[2025-02-13 03:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:54][root][INFO] - Training Epoch: 1/2, step 6937/7134 completed (loss: 0.11340128630399704, acc: 0.9719789624214172)
[2025-02-13 03:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55][root][INFO] - Training Epoch: 1/2, step 6938/7134 completed (loss: 0.132112056016922, acc: 0.96875)
[2025-02-13 03:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55][root][INFO] - Training Epoch: 1/2, step 6939/7134 completed (loss: 0.09593262523412704, acc: 0.9791377186775208)
[2025-02-13 03:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55][root][INFO] - Training Epoch: 1/2, step 6940/7134 completed (loss: 0.08449599146842957, acc: 0.9811643958091736)
[2025-02-13 03:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56][root][INFO] - Training Epoch: 1/2, step 6941/7134 completed (loss: 0.03961716964840889, acc: 0.9913194179534912)
[2025-02-13 03:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56][root][INFO] - Training Epoch: 1/2, step 6942/7134 completed (loss: 0.0758485347032547, acc: 0.9767123460769653)
[2025-02-13 03:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56][root][INFO] - Training Epoch: 1/2, step 6943/7134 completed (loss: 0.0437711663544178, acc: 0.991631805896759)
[2025-02-13 03:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57][root][INFO] - Training Epoch: 1/2, step 6944/7134 completed (loss: 0.040628354996442795, acc: 0.9920381903648376)
[2025-02-13 03:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57][root][INFO] - Training Epoch: 1/2, step 6945/7134 completed (loss: 0.0773029550909996, acc: 0.9842932224273682)
[2025-02-13 03:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58][root][INFO] - Training Epoch: 1/2, step 6946/7134 completed (loss: 0.050170738250017166, acc: 0.982807993888855)
[2025-02-13 03:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58][root][INFO] - Training Epoch: 1/2, step 6947/7134 completed (loss: 0.044261906296014786, acc: 0.9826086759567261)
[2025-02-13 03:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58][root][INFO] - Training Epoch: 1/2, step 6948/7134 completed (loss: 0.0679415613412857, acc: 0.9782244563102722)
[2025-02-13 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:59][root][INFO] - Training Epoch: 1/2, step 6949/7134 completed (loss: 0.06044052168726921, acc: 0.9850427508354187)
[2025-02-13 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:59][root][INFO] - Training Epoch: 1/2, step 6950/7134 completed (loss: 0.027303006500005722, acc: 0.9974489808082581)
[2025-02-13 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00][root][INFO] - Training Epoch: 1/2, step 6951/7134 completed (loss: 0.030801966786384583, acc: 0.991525411605835)
[2025-02-13 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00][root][INFO] - Training Epoch: 1/2, step 6952/7134 completed (loss: 0.02816663309931755, acc: 0.9929906725883484)
[2025-02-13 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00][root][INFO] - Training Epoch: 1/2, step 6953/7134 completed (loss: 0.01642315462231636, acc: 0.9934425950050354)
[2025-02-13 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01][root][INFO] - Training Epoch: 1/2, step 6954/7134 completed (loss: 0.09322132915258408, acc: 0.9829424023628235)
[2025-02-13 03:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01][root][INFO] - Training Epoch: 1/2, step 6955/7134 completed (loss: 0.07086435705423355, acc: 0.9817073345184326)
[2025-02-13 03:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01][root][INFO] - Training Epoch: 1/2, step 6956/7134 completed (loss: 0.0555659644305706, acc: 0.9875665903091431)
[2025-02-13 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:02][root][INFO] - Training Epoch: 1/2, step 6957/7134 completed (loss: 0.024346405640244484, acc: 0.9941291809082031)
[2025-02-13 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:02][root][INFO] - Training Epoch: 1/2, step 6958/7134 completed (loss: 0.07169225811958313, acc: 0.9766082167625427)
[2025-02-13 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03][root][INFO] - Training Epoch: 1/2, step 6959/7134 completed (loss: 0.05534903332591057, acc: 0.9822616577148438)
[2025-02-13 03:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03][root][INFO] - Training Epoch: 1/2, step 6960/7134 completed (loss: 0.040215667337179184, acc: 0.9902439117431641)
[2025-02-13 03:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03][root][INFO] - Training Epoch: 1/2, step 6961/7134 completed (loss: 0.04189314693212509, acc: 0.9898989796638489)
[2025-02-13 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04][root][INFO] - Training Epoch: 1/2, step 6962/7134 completed (loss: 0.048091936856508255, acc: 0.9878261089324951)
[2025-02-13 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04][root][INFO] - Training Epoch: 1/2, step 6963/7134 completed (loss: 0.024786287918686867, acc: 0.9932659864425659)
[2025-02-13 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05][root][INFO] - Training Epoch: 1/2, step 6964/7134 completed (loss: 0.033171456307172775, acc: 0.9891696572303772)
[2025-02-13 03:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05][root][INFO] - Training Epoch: 1/2, step 6965/7134 completed (loss: 0.02342708595097065, acc: 0.9945054650306702)
[2025-02-13 03:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05][root][INFO] - Training Epoch: 1/2, step 6966/7134 completed (loss: 0.05497385561466217, acc: 0.9800498485565186)
[2025-02-13 03:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:06][root][INFO] - Training Epoch: 1/2, step 6967/7134 completed (loss: 0.025187896564602852, acc: 0.993966817855835)
[2025-02-13 03:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:06][root][INFO] - Training Epoch: 1/2, step 6968/7134 completed (loss: 0.03774368762969971, acc: 0.9896551966667175)
[2025-02-13 03:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07][root][INFO] - Training Epoch: 1/2, step 6969/7134 completed (loss: 0.08493503928184509, acc: 0.9812889695167542)
[2025-02-13 03:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07][root][INFO] - Training Epoch: 1/2, step 6970/7134 completed (loss: 0.08464133739471436, acc: 0.9789473414421082)
[2025-02-13 03:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07][root][INFO] - Training Epoch: 1/2, step 6971/7134 completed (loss: 0.09514512866735458, acc: 0.9775474667549133)
[2025-02-13 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:08][root][INFO] - Training Epoch: 1/2, step 6972/7134 completed (loss: 0.020846758037805557, acc: 0.9910873174667358)
[2025-02-13 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:08][root][INFO] - Training Epoch: 1/2, step 6973/7134 completed (loss: 0.04445188120007515, acc: 0.9911894202232361)
[2025-02-13 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09][root][INFO] - Training Epoch: 1/2, step 6974/7134 completed (loss: 0.04466203972697258, acc: 0.9896551966667175)
[2025-02-13 03:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09][root][INFO] - Training Epoch: 1/2, step 6975/7134 completed (loss: 0.03711613640189171, acc: 0.9934924244880676)
[2025-02-13 03:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09][root][INFO] - Training Epoch: 1/2, step 6976/7134 completed (loss: 0.051979582756757736, acc: 0.984402060508728)
[2025-02-13 03:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10][root][INFO] - Training Epoch: 1/2, step 6977/7134 completed (loss: 0.03916141390800476, acc: 0.9887217879295349)
[2025-02-13 03:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10][root][INFO] - Training Epoch: 1/2, step 6978/7134 completed (loss: 0.029806090518832207, acc: 0.9886178970336914)
[2025-02-13 03:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:11][root][INFO] - Training Epoch: 1/2, step 6979/7134 completed (loss: 0.08395162969827652, acc: 0.9664633870124817)
[2025-02-13 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:11][root][INFO] - Training Epoch: 1/2, step 6980/7134 completed (loss: 0.1203179582953453, acc: 0.9672955870628357)
[2025-02-13 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12][root][INFO] - Training Epoch: 1/2, step 6981/7134 completed (loss: 0.11538383364677429, acc: 0.9680715203285217)
[2025-02-13 03:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12][root][INFO] - Training Epoch: 1/2, step 6982/7134 completed (loss: 0.12646161019802094, acc: 0.9658952355384827)
[2025-02-13 03:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12][root][INFO] - Training Epoch: 1/2, step 6983/7134 completed (loss: 0.1093255802989006, acc: 0.9777448177337646)
[2025-02-13 03:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:13][root][INFO] - Training Epoch: 1/2, step 6984/7134 completed (loss: 0.04897637665271759, acc: 0.986066460609436)
[2025-02-13 03:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:13][root][INFO] - Training Epoch: 1/2, step 6985/7134 completed (loss: 0.1416824907064438, acc: 0.9682051539421082)
[2025-02-13 03:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14][root][INFO] - Training Epoch: 1/2, step 6986/7134 completed (loss: 0.05039633437991142, acc: 0.98959881067276)
[2025-02-13 03:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14][root][INFO] - Training Epoch: 1/2, step 6987/7134 completed (loss: 0.07783760130405426, acc: 0.9767676591873169)
[2025-02-13 03:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15][root][INFO] - Training Epoch: 1/2, step 6988/7134 completed (loss: 0.0568818524479866, acc: 0.9840954542160034)
[2025-02-13 03:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15][root][INFO] - Training Epoch: 1/2, step 6989/7134 completed (loss: 0.10332031548023224, acc: 0.9672726988792419)
[2025-02-13 03:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15][root][INFO] - Training Epoch: 1/2, step 6990/7134 completed (loss: 0.02915826067328453, acc: 0.9910714030265808)
[2025-02-13 03:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16][root][INFO] - Training Epoch: 1/2, step 6991/7134 completed (loss: 0.089738629758358, acc: 0.9774436354637146)
[2025-02-13 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16][root][INFO] - Training Epoch: 1/2, step 6992/7134 completed (loss: 0.10114405304193497, acc: 0.9768421053886414)
[2025-02-13 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16][root][INFO] - Training Epoch: 1/2, step 6993/7134 completed (loss: 0.11810063570737839, acc: 0.9698340892791748)
[2025-02-13 03:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:17][root][INFO] - Training Epoch: 1/2, step 6994/7134 completed (loss: 0.14550116658210754, acc: 0.971107542514801)
[2025-02-13 03:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:17][root][INFO] - Training Epoch: 1/2, step 6995/7134 completed (loss: 0.04511309787631035, acc: 0.9908376932144165)
[2025-02-13 03:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18][root][INFO] - Training Epoch: 1/2, step 6996/7134 completed (loss: 0.04301682859659195, acc: 0.9886363744735718)
[2025-02-13 03:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18][root][INFO] - Training Epoch: 1/2, step 6997/7134 completed (loss: 0.05626215413212776, acc: 0.985981285572052)
[2025-02-13 03:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18][root][INFO] - Training Epoch: 1/2, step 6998/7134 completed (loss: 0.11899139732122421, acc: 0.9656862616539001)
[2025-02-13 03:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:19][root][INFO] - Training Epoch: 1/2, step 6999/7134 completed (loss: 0.044225335121154785, acc: 0.98562091588974)
[2025-02-13 03:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:19][root][INFO] - Training Epoch: 1/2, step 7000/7134 completed (loss: 0.04172423854470253, acc: 0.9894067645072937)
[2025-02-13 03:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20][root][INFO] - Training Epoch: 1/2, step 7001/7134 completed (loss: 0.09498748183250427, acc: 0.9789156913757324)
[2025-02-13 03:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20][root][INFO] - Training Epoch: 1/2, step 7002/7134 completed (loss: 0.02015233412384987, acc: 0.9918864369392395)
[2025-02-13 03:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21][root][INFO] - Training Epoch: 1/2, step 7003/7134 completed (loss: 0.03383147716522217, acc: 0.9868565201759338)
[2025-02-13 03:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21][root][INFO] - Training Epoch: 1/2, step 7004/7134 completed (loss: 0.09669820219278336, acc: 0.9763779640197754)
[2025-02-13 03:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21][root][INFO] - Training Epoch: 1/2, step 7005/7134 completed (loss: 0.07427658140659332, acc: 0.9811066389083862)
[2025-02-13 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:22][root][INFO] - Training Epoch: 1/2, step 7006/7134 completed (loss: 0.04249953478574753, acc: 0.9835841059684753)
[2025-02-13 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:22][root][INFO] - Training Epoch: 1/2, step 7007/7134 completed (loss: 0.05076479911804199, acc: 0.985029935836792)
[2025-02-13 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:23][root][INFO] - Training Epoch: 1/2, step 7008/7134 completed (loss: 0.03737280145287514, acc: 0.9913686513900757)
[2025-02-13 03:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:23][root][INFO] - Training Epoch: 1/2, step 7009/7134 completed (loss: 0.049071334302425385, acc: 0.9849537014961243)
[2025-02-13 03:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24][root][INFO] - Training Epoch: 1/2, step 7010/7134 completed (loss: 0.019041290506720543, acc: 0.9953774809837341)
[2025-02-13 03:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24][root][INFO] - Training Epoch: 1/2, step 7011/7134 completed (loss: 0.02558700554072857, acc: 0.9879807829856873)
[2025-02-13 03:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24][root][INFO] - Training Epoch: 1/2, step 7012/7134 completed (loss: 0.10220424830913544, acc: 0.9795918464660645)
[2025-02-13 03:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:25][root][INFO] - Training Epoch: 1/2, step 7013/7134 completed (loss: 0.01499919407069683, acc: 0.9927954077720642)
[2025-02-13 03:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:25][root][INFO] - Training Epoch: 1/2, step 7014/7134 completed (loss: 0.034462325274944305, acc: 0.988727867603302)
[2025-02-13 03:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26][root][INFO] - Training Epoch: 1/2, step 7015/7134 completed (loss: 0.042079824954271317, acc: 0.9767441749572754)
[2025-02-13 03:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26][root][INFO] - Training Epoch: 1/2, step 7016/7134 completed (loss: 0.04881833493709564, acc: 0.9797688126564026)
[2025-02-13 03:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26][root][INFO] - Training Epoch: 1/2, step 7017/7134 completed (loss: 0.06576398760080338, acc: 0.9854771494865417)
[2025-02-13 03:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:27][root][INFO] - Training Epoch: 1/2, step 7018/7134 completed (loss: 0.046695757657289505, acc: 0.987596869468689)
[2025-02-13 03:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:27][root][INFO] - Training Epoch: 1/2, step 7019/7134 completed (loss: 0.1044786274433136, acc: 0.9722222089767456)
[2025-02-13 03:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28][root][INFO] - Training Epoch: 1/2, step 7020/7134 completed (loss: 0.025344090536236763, acc: 0.9915373921394348)
[2025-02-13 03:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28][root][INFO] - Training Epoch: 1/2, step 7021/7134 completed (loss: 0.029210031032562256, acc: 0.9931856989860535)
[2025-02-13 03:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28][root][INFO] - Training Epoch: 1/2, step 7022/7134 completed (loss: 0.1225133091211319, acc: 0.9629629850387573)
[2025-02-13 03:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:29][root][INFO] - Training Epoch: 1/2, step 7023/7134 completed (loss: 0.09057708084583282, acc: 0.9761336445808411)
[2025-02-13 03:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:29][root][INFO] - Training Epoch: 1/2, step 7024/7134 completed (loss: 0.05208953469991684, acc: 0.987261176109314)
[2025-02-13 03:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30][root][INFO] - Training Epoch: 1/2, step 7025/7134 completed (loss: 0.17984654009342194, acc: 0.954023003578186)
[2025-02-13 03:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30][root][INFO] - Training Epoch: 1/2, step 7026/7134 completed (loss: 0.1314241886138916, acc: 0.9498069286346436)
[2025-02-13 03:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30][root][INFO] - Training Epoch: 1/2, step 7027/7134 completed (loss: 0.04689987003803253, acc: 0.9878048896789551)
[2025-02-13 03:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31][root][INFO] - Training Epoch: 1/2, step 7028/7134 completed (loss: 0.02918163686990738, acc: 0.990314781665802)
[2025-02-13 03:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31][root][INFO] - Training Epoch: 1/2, step 7029/7134 completed (loss: 0.04707261174917221, acc: 0.9814385175704956)
[2025-02-13 03:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31][root][INFO] - Training Epoch: 1/2, step 7030/7134 completed (loss: 0.13758619129657745, acc: 0.9668246507644653)
[2025-02-13 03:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32][root][INFO] - Training Epoch: 1/2, step 7031/7134 completed (loss: 0.10440172255039215, acc: 0.9644669890403748)
[2025-02-13 03:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32][root][INFO] - Training Epoch: 1/2, step 7032/7134 completed (loss: 0.10366260260343552, acc: 0.9593495726585388)
[2025-02-13 03:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32][root][INFO] - Training Epoch: 1/2, step 7033/7134 completed (loss: 0.07674578577280045, acc: 0.973607063293457)
[2025-02-13 03:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33][root][INFO] - Training Epoch: 1/2, step 7034/7134 completed (loss: 0.11890354752540588, acc: 0.971731424331665)
[2025-02-13 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33][root][INFO] - Training Epoch: 1/2, step 7035/7134 completed (loss: 0.1218995526432991, acc: 0.9603174328804016)
[2025-02-13 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33][root][INFO] - Training Epoch: 1/2, step 7036/7134 completed (loss: 0.06812556087970734, acc: 0.9794238805770874)
[2025-02-13 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34][root][INFO] - Training Epoch: 1/2, step 7037/7134 completed (loss: 0.041633300483226776, acc: 0.984375)
[2025-02-13 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34][root][INFO] - Training Epoch: 1/2, step 7038/7134 completed (loss: 0.13686464726924896, acc: 0.960422158241272)
[2025-02-13 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34][root][INFO] - Training Epoch: 1/2, step 7039/7134 completed (loss: 0.0700349435210228, acc: 0.9806763529777527)
[2025-02-13 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35][root][INFO] - Training Epoch: 1/2, step 7040/7134 completed (loss: 0.17490865290164948, acc: 0.9367815852165222)
[2025-02-13 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35][root][INFO] - Training Epoch: 1/2, step 7041/7134 completed (loss: 0.1364622414112091, acc: 0.9654088020324707)
[2025-02-13 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35][root][INFO] - Training Epoch: 1/2, step 7042/7134 completed (loss: 0.11065638810396194, acc: 0.9569230675697327)
[2025-02-13 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36][root][INFO] - Training Epoch: 1/2, step 7043/7134 completed (loss: 0.10097178816795349, acc: 0.9679999947547913)
[2025-02-13 03:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36][root][INFO] - Training Epoch: 1/2, step 7044/7134 completed (loss: 0.1315459907054901, acc: 0.963302731513977)
[2025-02-13 03:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36][root][INFO] - Training Epoch: 1/2, step 7045/7134 completed (loss: 0.045154083520174026, acc: 0.9897959232330322)
[2025-02-13 03:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:37][root][INFO] - Training Epoch: 1/2, step 7046/7134 completed (loss: 0.09231454133987427, acc: 0.9754098653793335)
[2025-02-13 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:37][root][INFO] - Training Epoch: 1/2, step 7047/7134 completed (loss: 0.08691863715648651, acc: 0.9785714149475098)
[2025-02-13 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38][root][INFO] - Training Epoch: 1/2, step 7048/7134 completed (loss: 0.01129382848739624, acc: 0.9976498484611511)
[2025-02-13 03:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38][root][INFO] - Training Epoch: 1/2, step 7049/7134 completed (loss: 0.036608874797821045, acc: 0.9893048405647278)
[2025-02-13 03:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38][root][INFO] - Training Epoch: 1/2, step 7050/7134 completed (loss: 0.03546921908855438, acc: 0.9909326434135437)
[2025-02-13 03:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:39][root][INFO] - Training Epoch: 1/2, step 7051/7134 completed (loss: 0.057573117315769196, acc: 0.983565092086792)
[2025-02-13 03:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:39][root][INFO] - Training Epoch: 1/2, step 7052/7134 completed (loss: 0.048070330172777176, acc: 0.9880159497261047)
[2025-02-13 03:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:40][root][INFO] - Training Epoch: 1/2, step 7053/7134 completed (loss: 0.032124266028404236, acc: 0.9914737939834595)
[2025-02-13 03:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:40][root][INFO] - Training Epoch: 1/2, step 7054/7134 completed (loss: 0.08771105855703354, acc: 0.9778481125831604)
[2025-02-13 03:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:41][root][INFO] - Training Epoch: 1/2, step 7055/7134 completed (loss: 0.044775377959012985, acc: 0.9872449040412903)
[2025-02-13 03:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:41][root][INFO] - Training Epoch: 1/2, step 7056/7134 completed (loss: 0.06145242974162102, acc: 0.9843937754631042)
[2025-02-13 03:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42][root][INFO] - Training Epoch: 1/2, step 7057/7134 completed (loss: 0.13159100711345673, acc: 0.9712328910827637)
[2025-02-13 03:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42][root][INFO] - Training Epoch: 1/2, step 7058/7134 completed (loss: 0.06495559215545654, acc: 0.9803149700164795)
[2025-02-13 03:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42][root][INFO] - Training Epoch: 1/2, step 7059/7134 completed (loss: 0.03523564711213112, acc: 0.9888268113136292)
[2025-02-13 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:43][root][INFO] - Training Epoch: 1/2, step 7060/7134 completed (loss: 0.08192411810159683, acc: 0.9790322780609131)
[2025-02-13 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:43][root][INFO] - Training Epoch: 1/2, step 7061/7134 completed (loss: 0.05105955898761749, acc: 0.98828125)
[2025-02-13 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:44][root][INFO] - Training Epoch: 1/2, step 7062/7134 completed (loss: 0.15164488554000854, acc: 0.9666666388511658)
[2025-02-13 03:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:44][root][INFO] - Training Epoch: 1/2, step 7063/7134 completed (loss: 0.07574871927499771, acc: 0.9803921580314636)
[2025-02-13 03:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45][root][INFO] - Training Epoch: 1/2, step 7064/7134 completed (loss: 0.0308048315346241, acc: 0.9940652847290039)
[2025-02-13 03:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45][root][INFO] - Training Epoch: 1/2, step 7065/7134 completed (loss: 0.02672816999256611, acc: 0.9918887615203857)
[2025-02-13 03:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45][root][INFO] - Training Epoch: 1/2, step 7066/7134 completed (loss: 0.03429984673857689, acc: 0.9928656220436096)
[2025-02-13 03:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:46][root][INFO] - Training Epoch: 1/2, step 7067/7134 completed (loss: 0.034330323338508606, acc: 0.9885350465774536)
[2025-02-13 03:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:46][root][INFO] - Training Epoch: 1/2, step 7068/7134 completed (loss: 0.028974398970603943, acc: 0.9891435503959656)
[2025-02-13 03:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:47][root][INFO] - Training Epoch: 1/2, step 7069/7134 completed (loss: 0.02535691112279892, acc: 0.993096649646759)
[2025-02-13 03:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:47][root][INFO] - Training Epoch: 1/2, step 7070/7134 completed (loss: 0.029899071902036667, acc: 0.991769552230835)
[2025-02-13 03:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:48][root][INFO] - Training Epoch: 1/2, step 7071/7134 completed (loss: 0.0692097395658493, acc: 0.9868420958518982)
[2025-02-13 03:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:48][root][INFO] - Training Epoch: 1/2, step 7072/7134 completed (loss: 0.04225030541419983, acc: 0.9878787994384766)
[2025-02-13 03:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49][root][INFO] - Training Epoch: 1/2, step 7073/7134 completed (loss: 0.040146034210920334, acc: 0.9863481521606445)
[2025-02-13 03:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49][root][INFO] - Training Epoch: 1/2, step 7074/7134 completed (loss: 0.03772987052798271, acc: 0.9923518300056458)
[2025-02-13 03:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49][root][INFO] - Training Epoch: 1/2, step 7075/7134 completed (loss: 0.03666295111179352, acc: 0.988041877746582)
[2025-02-13 03:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:50][root][INFO] - Training Epoch: 1/2, step 7076/7134 completed (loss: 0.06083162873983383, acc: 0.9857549667358398)
[2025-02-13 03:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:50][root][INFO] - Training Epoch: 1/2, step 7077/7134 completed (loss: 0.06417039781808853, acc: 0.9819672107696533)
[2025-02-13 03:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51][root][INFO] - Training Epoch: 1/2, step 7078/7134 completed (loss: 0.05029568076133728, acc: 0.9791666865348816)
[2025-02-13 03:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51][root][INFO] - Training Epoch: 1/2, step 7079/7134 completed (loss: 0.04181531071662903, acc: 0.9819375872612)
[2025-02-13 03:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51][root][INFO] - Training Epoch: 1/2, step 7080/7134 completed (loss: 0.03708332031965256, acc: 0.9867109656333923)
[2025-02-13 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:52][root][INFO] - Training Epoch: 1/2, step 7081/7134 completed (loss: 0.05150168389081955, acc: 0.9831697344779968)
[2025-02-13 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:52][root][INFO] - Training Epoch: 1/2, step 7082/7134 completed (loss: 0.05014876648783684, acc: 0.987075924873352)
[2025-02-13 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53][root][INFO] - Training Epoch: 1/2, step 7083/7134 completed (loss: 0.022711722180247307, acc: 0.9949832558631897)
[2025-02-13 03:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53][root][INFO] - Training Epoch: 1/2, step 7084/7134 completed (loss: 0.04754791036248207, acc: 0.9840348362922668)
[2025-02-13 03:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53][root][INFO] - Training Epoch: 1/2, step 7085/7134 completed (loss: 0.04814852774143219, acc: 0.9861111044883728)
[2025-02-13 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54][root][INFO] - Training Epoch: 1/2, step 7086/7134 completed (loss: 0.036400794982910156, acc: 0.9863201379776001)
[2025-02-13 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54][root][INFO] - Training Epoch: 1/2, step 7087/7134 completed (loss: 0.03740634396672249, acc: 0.9926062822341919)
[2025-02-13 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55][root][INFO] - Training Epoch: 1/2, step 7088/7134 completed (loss: 0.05935453623533249, acc: 0.9850993156433105)
[2025-02-13 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55][root][INFO] - Training Epoch: 1/2, step 7089/7134 completed (loss: 0.11454108357429504, acc: 0.977707028388977)
[2025-02-13 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55][root][INFO] - Training Epoch: 1/2, step 7090/7134 completed (loss: 0.0414377897977829, acc: 0.989159882068634)
[2025-02-13 03:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56][root][INFO] - Training Epoch: 1/2, step 7091/7134 completed (loss: 0.00857964064925909, acc: 0.9986072182655334)
[2025-02-13 03:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56][root][INFO] - Training Epoch: 1/2, step 7092/7134 completed (loss: 0.0345517136156559, acc: 0.9879931211471558)
[2025-02-13 03:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57][root][INFO] - Training Epoch: 1/2, step 7093/7134 completed (loss: 0.054462987929582596, acc: 0.988095223903656)
[2025-02-13 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57][root][INFO] - Training Epoch: 1/2, step 7094/7134 completed (loss: 0.027306845411658287, acc: 0.9930875301361084)
[2025-02-13 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57][root][INFO] - Training Epoch: 1/2, step 7095/7134 completed (loss: 0.04234476387500763, acc: 0.9896694421768188)
[2025-02-13 03:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:58][root][INFO] - Training Epoch: 1/2, step 7096/7134 completed (loss: 0.06382394582033157, acc: 0.9815126061439514)
[2025-02-13 03:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:58][root][INFO] - Training Epoch: 1/2, step 7097/7134 completed (loss: 0.06874129176139832, acc: 0.9841498732566833)
[2025-02-13 03:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59][root][INFO] - Training Epoch: 1/2, step 7098/7134 completed (loss: 0.04280555248260498, acc: 0.9871588945388794)
[2025-02-13 03:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59][root][INFO] - Training Epoch: 1/2, step 7099/7134 completed (loss: 0.04396839812397957, acc: 0.9829059839248657)
[2025-02-13 03:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59][root][INFO] - Training Epoch: 1/2, step 7100/7134 completed (loss: 0.07543382793664932, acc: 0.9751937985420227)
[2025-02-13 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:00][root][INFO] - Training Epoch: 1/2, step 7101/7134 completed (loss: 0.09514867514371872, acc: 0.9821109175682068)
[2025-02-13 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:00][root][INFO] - Training Epoch: 1/2, step 7102/7134 completed (loss: 0.015459121204912663, acc: 0.998062014579773)
[2025-02-13 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01][root][INFO] - Training Epoch: 1/2, step 7103/7134 completed (loss: 0.04035104438662529, acc: 0.9879032373428345)
[2025-02-13 03:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01][root][INFO] - Training Epoch: 1/2, step 7104/7134 completed (loss: 0.028575098142027855, acc: 0.9958847761154175)
[2025-02-13 03:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01][root][INFO] - Training Epoch: 1/2, step 7105/7134 completed (loss: 0.0424564965069294, acc: 0.9900000095367432)
[2025-02-13 03:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02][root][INFO] - Training Epoch: 1/2, step 7106/7134 completed (loss: 0.029069703072309494, acc: 0.989983320236206)
[2025-02-13 03:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02][root][INFO] - Training Epoch: 1/2, step 7107/7134 completed (loss: 0.03627563267946243, acc: 0.989130437374115)
[2025-02-13 03:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02][root][INFO] - Training Epoch: 1/2, step 7108/7134 completed (loss: 0.0396156869828701, acc: 0.9900000095367432)
[2025-02-13 03:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:03][root][INFO] - Training Epoch: 1/2, step 7109/7134 completed (loss: 0.07336898893117905, acc: 0.9699812531471252)
[2025-02-13 03:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:03][root][INFO] - Training Epoch: 1/2, step 7110/7134 completed (loss: 0.04109783098101616, acc: 0.9830508232116699)
[2025-02-13 03:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04][root][INFO] - Training Epoch: 1/2, step 7111/7134 completed (loss: 0.07932276278734207, acc: 0.9733924865722656)
[2025-02-13 03:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04][root][INFO] - Training Epoch: 1/2, step 7112/7134 completed (loss: 0.049626048654317856, acc: 0.9876543283462524)
[2025-02-13 03:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04][root][INFO] - Training Epoch: 1/2, step 7113/7134 completed (loss: 0.09991098940372467, acc: 0.965299665927887)
[2025-02-13 03:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:05][root][INFO] - Training Epoch: 1/2, step 7114/7134 completed (loss: 0.04189632087945938, acc: 0.9885277152061462)
[2025-02-13 03:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:05][root][INFO] - Training Epoch: 1/2, step 7115/7134 completed (loss: 0.034489769488573074, acc: 0.9894179701805115)
[2025-02-13 03:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06][root][INFO] - Training Epoch: 1/2, step 7116/7134 completed (loss: 0.030152607709169388, acc: 0.9894551634788513)
[2025-02-13 03:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06][root][INFO] - Training Epoch: 1/2, step 7117/7134 completed (loss: 0.0184169989079237, acc: 0.9950000047683716)
[2025-02-13 03:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06][root][INFO] - Training Epoch: 1/2, step 7118/7134 completed (loss: 0.054606009274721146, acc: 0.9883720874786377)
[2025-02-13 03:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:07][root][INFO] - Training Epoch: 1/2, step 7119/7134 completed (loss: 0.02778710424900055, acc: 0.9918166995048523)
[2025-02-13 03:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:07][root][INFO] - Training Epoch: 1/2, step 7120/7134 completed (loss: 0.052865345031023026, acc: 0.9836448431015015)
[2025-02-13 03:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08][root][INFO] - Training Epoch: 1/2, step 7121/7134 completed (loss: 0.029206719249486923, acc: 0.9894366264343262)
[2025-02-13 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08][root][INFO] - Training Epoch: 1/2, step 7122/7134 completed (loss: 0.044041190296411514, acc: 0.9839572310447693)
[2025-02-13 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08][root][INFO] - Training Epoch: 1/2, step 7123/7134 completed (loss: 0.03073006309568882, acc: 0.9853658676147461)
[2025-02-13 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09][root][INFO] - Training Epoch: 1/2, step 7124/7134 completed (loss: 0.03370477631688118, acc: 0.9906191229820251)
[2025-02-13 03:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09][root][INFO] - Training Epoch: 1/2, step 7125/7134 completed (loss: 0.02816867083311081, acc: 0.9867330193519592)
[2025-02-13 03:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09][root][INFO] - Training Epoch: 1/2, step 7126/7134 completed (loss: 0.09244280308485031, acc: 0.9713321924209595)
[2025-02-13 03:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:10][root][INFO] - Training Epoch: 1/2, step 7127/7134 completed (loss: 0.04220599681138992, acc: 0.9840764403343201)
[2025-02-13 03:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:10][root][INFO] - Training Epoch: 1/2, step 7128/7134 completed (loss: 0.04504822939634323, acc: 0.9869918823242188)
[2025-02-13 03:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11][root][INFO] - Training Epoch: 1/2, step 7129/7134 completed (loss: 0.08041968941688538, acc: 0.9753915071487427)
[2025-02-13 03:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11][root][INFO] - Training Epoch: 1/2, step 7130/7134 completed (loss: 0.11743500828742981, acc: 0.9624999761581421)
[2025-02-13 03:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11][root][INFO] - Training Epoch: 1/2, step 7131/7134 completed (loss: 0.021331047639250755, acc: 0.9907192587852478)
[2025-02-13 03:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:17][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0590, device='cuda:0') eval_epoch_loss=tensor(0.0573, device='cuda:0') eval_epoch_acc=tensor(0.9842, device='cuda:0')
[2025-02-13 03:30:17][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:30:17][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:30:17][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_7132_loss_0.057319674640893936/model.pt
[2025-02-13 03:30:17][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:30:17][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.057319674640893936
[2025-02-13 03:30:17][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9841892719268799
[2025-02-13 03:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:17][root][INFO] - Training Epoch: 1/2, step 7132/7134 completed (loss: 0.03515179455280304, acc: 0.9898989796638489)
[2025-02-13 03:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:18][root][INFO] - Training Epoch: 1/2, step 7133/7134 completed (loss: 0.021539965644478798, acc: 0.9907578825950623)
[2025-02-13 03:30:18][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=1.1209, train_epoch_loss=0.1141, epoch time 3969.342839989811s
[2025-02-13 03:30:18][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2025-02-13 03:30:18][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 30 GB
[2025-02-13 03:30:18][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2025-02-13 03:30:18][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-02-13 03:30:18][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-13 03:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:19][root][INFO] - Training Epoch: 2/2, step 0/7134 completed (loss: 0.045406240969896317, acc: 0.9881129264831543)
[2025-02-13 03:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:20][root][INFO] - Training Epoch: 2/2, step 1/7134 completed (loss: 0.03915921598672867, acc: 0.9888424277305603)
[2025-02-13 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:20][root][INFO] - Training Epoch: 2/2, step 2/7134 completed (loss: 0.02636781893670559, acc: 0.993306577205658)
[2025-02-13 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21][root][INFO] - Training Epoch: 2/2, step 3/7134 completed (loss: 0.0075323437340557575, acc: 1.0)
[2025-02-13 03:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21][root][INFO] - Training Epoch: 2/2, step 4/7134 completed (loss: 0.0397120825946331, acc: 0.9836289286613464)
[2025-02-13 03:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21][root][INFO] - Training Epoch: 2/2, step 5/7134 completed (loss: 0.012827659957110882, acc: 0.9949238300323486)
[2025-02-13 03:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:22][root][INFO] - Training Epoch: 2/2, step 6/7134 completed (loss: 0.010694368742406368, acc: 0.9983948469161987)
[2025-02-13 03:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:22][root][INFO] - Training Epoch: 2/2, step 7/7134 completed (loss: 0.06005934253334999, acc: 0.9838107228279114)
[2025-02-13 03:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:23][root][INFO] - Training Epoch: 2/2, step 8/7134 completed (loss: 0.060551974922418594, acc: 0.9793672561645508)
[2025-02-13 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:23][root][INFO] - Training Epoch: 2/2, step 9/7134 completed (loss: 0.013843339867889881, acc: 0.9968992471694946)
[2025-02-13 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24][root][INFO] - Training Epoch: 2/2, step 10/7134 completed (loss: 0.03660716861486435, acc: 0.9886202216148376)
[2025-02-13 03:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24][root][INFO] - Training Epoch: 2/2, step 11/7134 completed (loss: 0.010963240638375282, acc: 0.998123824596405)
[2025-02-13 03:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24][root][INFO] - Training Epoch: 2/2, step 12/7134 completed (loss: 0.011802202090620995, acc: 0.9959623217582703)
[2025-02-13 03:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:25][root][INFO] - Training Epoch: 2/2, step 13/7134 completed (loss: 0.03324964642524719, acc: 0.9903714060783386)
[2025-02-13 03:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:25][root][INFO] - Training Epoch: 2/2, step 14/7134 completed (loss: 0.021706953644752502, acc: 0.9949579834938049)
[2025-02-13 03:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:26][root][INFO] - Training Epoch: 2/2, step 15/7134 completed (loss: 0.027570901438593864, acc: 0.9948365092277527)
[2025-02-13 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:26][root][INFO] - Training Epoch: 2/2, step 16/7134 completed (loss: 0.004779847338795662, acc: 0.9979674816131592)
[2025-02-13 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27][root][INFO] - Training Epoch: 2/2, step 17/7134 completed (loss: 0.08610471338033676, acc: 0.9783783555030823)
[2025-02-13 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27][root][INFO] - Training Epoch: 2/2, step 18/7134 completed (loss: 0.00880026537925005, acc: 0.9985795617103577)
[2025-02-13 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27][root][INFO] - Training Epoch: 2/2, step 19/7134 completed (loss: 0.03432606905698776, acc: 0.9903448224067688)
[2025-02-13 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:28][root][INFO] - Training Epoch: 2/2, step 20/7134 completed (loss: 0.016632677987217903, acc: 0.9934210777282715)
[2025-02-13 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:28][root][INFO] - Training Epoch: 2/2, step 21/7134 completed (loss: 0.010164964944124222, acc: 0.9974226951599121)
[2025-02-13 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29][root][INFO] - Training Epoch: 2/2, step 22/7134 completed (loss: 0.0032361072953790426, acc: 0.9985590577125549)
[2025-02-13 03:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29][root][INFO] - Training Epoch: 2/2, step 23/7134 completed (loss: 0.013985848054289818, acc: 0.9959239363670349)
[2025-02-13 03:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29][root][INFO] - Training Epoch: 2/2, step 24/7134 completed (loss: 0.012730710208415985, acc: 0.9944289922714233)
[2025-02-13 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30][root][INFO] - Training Epoch: 2/2, step 25/7134 completed (loss: 0.026217812672257423, acc: 0.9900568127632141)
[2025-02-13 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30][root][INFO] - Training Epoch: 2/2, step 26/7134 completed (loss: 0.0310908742249012, acc: 0.991150438785553)
[2025-02-13 03:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:31][root][INFO] - Training Epoch: 2/2, step 27/7134 completed (loss: 0.011237021535634995, acc: 0.9985228776931763)
[2025-02-13 03:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:31][root][INFO] - Training Epoch: 2/2, step 28/7134 completed (loss: 0.06181444227695465, acc: 0.9855421781539917)
[2025-02-13 03:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32][root][INFO] - Training Epoch: 2/2, step 29/7134 completed (loss: 0.05302714556455612, acc: 0.9872521162033081)
[2025-02-13 03:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32][root][INFO] - Training Epoch: 2/2, step 30/7134 completed (loss: 0.02060745283961296, acc: 0.993220329284668)
[2025-02-13 03:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33][root][INFO] - Training Epoch: 2/2, step 31/7134 completed (loss: 0.04150086268782616, acc: 0.9894067645072937)
[2025-02-13 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33][root][INFO] - Training Epoch: 2/2, step 32/7134 completed (loss: 0.046375054866075516, acc: 0.9843953251838684)
[2025-02-13 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33][root][INFO] - Training Epoch: 2/2, step 33/7134 completed (loss: 0.12499725073575974, acc: 0.9780876636505127)
[2025-02-13 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34][root][INFO] - Training Epoch: 2/2, step 34/7134 completed (loss: 0.054045628756284714, acc: 0.9847596883773804)
[2025-02-13 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34][root][INFO] - Training Epoch: 2/2, step 35/7134 completed (loss: 0.06184206157922745, acc: 0.9858623743057251)
[2025-02-13 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:35][root][INFO] - Training Epoch: 2/2, step 36/7134 completed (loss: 0.04960031807422638, acc: 0.9956569075584412)
[2025-02-13 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:35][root][INFO] - Training Epoch: 2/2, step 37/7134 completed (loss: 0.027438241988420486, acc: 0.9887359142303467)
[2025-02-13 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:36][root][INFO] - Training Epoch: 2/2, step 38/7134 completed (loss: 0.031120700761675835, acc: 0.992559552192688)
[2025-02-13 03:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:36][root][INFO] - Training Epoch: 2/2, step 39/7134 completed (loss: 0.009719743393361568, acc: 0.9959404468536377)
[2025-02-13 03:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37][root][INFO] - Training Epoch: 2/2, step 40/7134 completed (loss: 0.01725410483777523, acc: 0.9932735562324524)
[2025-02-13 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37][root][INFO] - Training Epoch: 2/2, step 41/7134 completed (loss: 0.031039826571941376, acc: 0.9902912378311157)
[2025-02-13 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37][root][INFO] - Training Epoch: 2/2, step 42/7134 completed (loss: 0.05332082882523537, acc: 0.9873772859573364)
[2025-02-13 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:38][root][INFO] - Training Epoch: 2/2, step 43/7134 completed (loss: 0.013415739871561527, acc: 0.9935483932495117)
[2025-02-13 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:38][root][INFO] - Training Epoch: 2/2, step 44/7134 completed (loss: 0.04476235806941986, acc: 0.9910827875137329)
[2025-02-13 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:39][root][INFO] - Training Epoch: 2/2, step 45/7134 completed (loss: 0.04215957969427109, acc: 0.9934994578361511)
[2025-02-13 03:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:39][root][INFO] - Training Epoch: 2/2, step 46/7134 completed (loss: 0.04550428315997124, acc: 0.9906666874885559)
[2025-02-13 03:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:40][root][INFO] - Training Epoch: 2/2, step 47/7134 completed (loss: 0.046524811536073685, acc: 0.9855700135231018)
[2025-02-13 03:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:40][root][INFO] - Training Epoch: 2/2, step 48/7134 completed (loss: 0.0252989474684, acc: 0.9921348094940186)
[2025-02-13 03:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41][root][INFO] - Training Epoch: 2/2, step 49/7134 completed (loss: 0.01794058084487915, acc: 0.9972565174102783)
[2025-02-13 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41][root][INFO] - Training Epoch: 2/2, step 50/7134 completed (loss: 0.014388250187039375, acc: 0.9965397715568542)
[2025-02-13 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41][root][INFO] - Training Epoch: 2/2, step 51/7134 completed (loss: 0.015058689750730991, acc: 0.9961928725242615)
[2025-02-13 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:42][root][INFO] - Training Epoch: 2/2, step 52/7134 completed (loss: 0.014993681572377682, acc: 0.995121955871582)
[2025-02-13 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:42][root][INFO] - Training Epoch: 2/2, step 53/7134 completed (loss: 0.011222289875149727, acc: 0.9960629940032959)
[2025-02-13 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43][root][INFO] - Training Epoch: 2/2, step 54/7134 completed (loss: 0.007867462933063507, acc: 0.9976984858512878)
[2025-02-13 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43][root][INFO] - Training Epoch: 2/2, step 55/7134 completed (loss: 0.020137380808591843, acc: 0.9949238300323486)
[2025-02-13 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44][root][INFO] - Training Epoch: 2/2, step 56/7134 completed (loss: 0.0150730200111866, acc: 0.9928571581840515)
[2025-02-13 03:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44][root][INFO] - Training Epoch: 2/2, step 57/7134 completed (loss: 0.05916672199964523, acc: 0.9863353967666626)
[2025-02-13 03:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44][root][INFO] - Training Epoch: 2/2, step 58/7134 completed (loss: 0.060001764446496964, acc: 0.984635055065155)
[2025-02-13 03:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45][root][INFO] - Training Epoch: 2/2, step 59/7134 completed (loss: 0.021308831870555878, acc: 0.9940898418426514)
[2025-02-13 03:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45][root][INFO] - Training Epoch: 2/2, step 60/7134 completed (loss: 0.03686276450753212, acc: 0.9900568127632141)
[2025-02-13 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:46][root][INFO] - Training Epoch: 2/2, step 61/7134 completed (loss: 0.03636525943875313, acc: 0.9889841079711914)
[2025-02-13 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:46][root][INFO] - Training Epoch: 2/2, step 62/7134 completed (loss: 0.027943959459662437, acc: 0.9898348450660706)
[2025-02-13 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47][root][INFO] - Training Epoch: 2/2, step 63/7134 completed (loss: 0.06235090270638466, acc: 0.9863013625144958)
[2025-02-13 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47][root][INFO] - Training Epoch: 2/2, step 64/7134 completed (loss: 0.019643748179078102, acc: 0.9905914068222046)
[2025-02-13 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48][root][INFO] - Training Epoch: 2/2, step 65/7134 completed (loss: 0.05616579204797745, acc: 0.9815725088119507)
[2025-02-13 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48][root][INFO] - Training Epoch: 2/2, step 66/7134 completed (loss: 0.06766272336244583, acc: 0.9820788502693176)
[2025-02-13 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49][root][INFO] - Training Epoch: 2/2, step 67/7134 completed (loss: 0.05247388407588005, acc: 0.9803921580314636)
[2025-02-13 03:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49][root][INFO] - Training Epoch: 2/2, step 68/7134 completed (loss: 0.053054485470056534, acc: 0.9793548583984375)
[2025-02-13 03:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49][root][INFO] - Training Epoch: 2/2, step 69/7134 completed (loss: 0.07003898918628693, acc: 0.9788029789924622)
[2025-02-13 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50][root][INFO] - Training Epoch: 2/2, step 70/7134 completed (loss: 0.0620480440557003, acc: 0.9824561476707458)
[2025-02-13 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50][root][INFO] - Training Epoch: 2/2, step 71/7134 completed (loss: 0.0292219165712595, acc: 0.9915966391563416)
[2025-02-13 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:51][root][INFO] - Training Epoch: 2/2, step 72/7134 completed (loss: 0.03989759087562561, acc: 0.9872123003005981)
[2025-02-13 03:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:51][root][INFO] - Training Epoch: 2/2, step 73/7134 completed (loss: 0.0340987928211689, acc: 0.99048912525177)
[2025-02-13 03:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52][root][INFO] - Training Epoch: 2/2, step 74/7134 completed (loss: 0.044155996292829514, acc: 0.9818941354751587)
[2025-02-13 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52][root][INFO] - Training Epoch: 2/2, step 75/7134 completed (loss: 0.08819402009248734, acc: 0.980567991733551)
[2025-02-13 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53][root][INFO] - Training Epoch: 2/2, step 76/7134 completed (loss: 0.025925053283572197, acc: 0.99370276927948)
[2025-02-13 03:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53][root][INFO] - Training Epoch: 2/2, step 77/7134 completed (loss: 0.03354911133646965, acc: 0.9924812316894531)
[2025-02-13 03:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53][root][INFO] - Training Epoch: 2/2, step 78/7134 completed (loss: 0.0540960431098938, acc: 0.9858490824699402)
[2025-02-13 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:54][root][INFO] - Training Epoch: 2/2, step 79/7134 completed (loss: 0.04782555252313614, acc: 0.9768518805503845)
[2025-02-13 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:54][root][INFO] - Training Epoch: 2/2, step 80/7134 completed (loss: 0.03316250443458557, acc: 0.9947916865348816)
[2025-02-13 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55][root][INFO] - Training Epoch: 2/2, step 81/7134 completed (loss: 0.029527226462960243, acc: 0.9921466112136841)
[2025-02-13 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55][root][INFO] - Training Epoch: 2/2, step 82/7134 completed (loss: 0.03592268377542496, acc: 0.9915134310722351)
[2025-02-13 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:56][root][INFO] - Training Epoch: 2/2, step 83/7134 completed (loss: 0.027208419516682625, acc: 0.9910025596618652)
[2025-02-13 03:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:56][root][INFO] - Training Epoch: 2/2, step 84/7134 completed (loss: 0.0320933498442173, acc: 0.9896373152732849)
[2025-02-13 03:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57][root][INFO] - Training Epoch: 2/2, step 85/7134 completed (loss: 0.048670444637537, acc: 0.9856801629066467)
[2025-02-13 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57][root][INFO] - Training Epoch: 2/2, step 86/7134 completed (loss: 0.029201118275523186, acc: 0.9862385392189026)
[2025-02-13 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57][root][INFO] - Training Epoch: 2/2, step 87/7134 completed (loss: 0.12345194816589355, acc: 0.9706314206123352)
[2025-02-13 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58][root][INFO] - Training Epoch: 2/2, step 88/7134 completed (loss: 0.11389254778623581, acc: 0.9723374843597412)
[2025-02-13 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58][root][INFO] - Training Epoch: 2/2, step 89/7134 completed (loss: 0.036344002932310104, acc: 0.9897304177284241)
[2025-02-13 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:59][root][INFO] - Training Epoch: 2/2, step 90/7134 completed (loss: 0.044959139078855515, acc: 0.9877622127532959)
[2025-02-13 03:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:59][root][INFO] - Training Epoch: 2/2, step 91/7134 completed (loss: 0.04110541567206383, acc: 0.9877551198005676)
[2025-02-13 03:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00][root][INFO] - Training Epoch: 2/2, step 92/7134 completed (loss: 0.04640350863337517, acc: 0.9845722317695618)
[2025-02-13 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00][root][INFO] - Training Epoch: 2/2, step 93/7134 completed (loss: 0.027605228126049042, acc: 0.9917355179786682)
[2025-02-13 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00][root][INFO] - Training Epoch: 2/2, step 94/7134 completed (loss: 0.08318024128675461, acc: 0.9767981171607971)
[2025-02-13 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01][root][INFO] - Training Epoch: 2/2, step 95/7134 completed (loss: 0.03882487118244171, acc: 0.9861809015274048)
[2025-02-13 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01][root][INFO] - Training Epoch: 2/2, step 96/7134 completed (loss: 0.03377338498830795, acc: 0.9888268113136292)
[2025-02-13 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01][root][INFO] - Training Epoch: 2/2, step 97/7134 completed (loss: 0.20738935470581055, acc: 0.9638554453849792)
[2025-02-13 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02][root][INFO] - Training Epoch: 2/2, step 98/7134 completed (loss: 0.06131628155708313, acc: 0.9773463010787964)
[2025-02-13 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02][root][INFO] - Training Epoch: 2/2, step 99/7134 completed (loss: 0.09118222445249557, acc: 0.9779816269874573)
[2025-02-13 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03][root][INFO] - Training Epoch: 2/2, step 100/7134 completed (loss: 0.03383580222725868, acc: 0.9923076629638672)
[2025-02-13 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03][root][INFO] - Training Epoch: 2/2, step 101/7134 completed (loss: 0.04433946684002876, acc: 0.9819079041481018)
[2025-02-13 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:04][root][INFO] - Training Epoch: 2/2, step 102/7134 completed (loss: 0.029584215953946114, acc: 0.9914529919624329)
[2025-02-13 03:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:04][root][INFO] - Training Epoch: 2/2, step 103/7134 completed (loss: 0.04324805736541748, acc: 0.9833333492279053)
[2025-02-13 03:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05][root][INFO] - Training Epoch: 2/2, step 104/7134 completed (loss: 0.0329052209854126, acc: 0.9848942756652832)
[2025-02-13 03:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05][root][INFO] - Training Epoch: 2/2, step 105/7134 completed (loss: 0.030187904834747314, acc: 0.990212082862854)
[2025-02-13 03:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05][root][INFO] - Training Epoch: 2/2, step 106/7134 completed (loss: 0.05814220383763313, acc: 0.9786700010299683)
[2025-02-13 03:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06][root][INFO] - Training Epoch: 2/2, step 107/7134 completed (loss: 0.15148484706878662, acc: 0.9527220726013184)
[2025-02-13 03:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06][root][INFO] - Training Epoch: 2/2, step 108/7134 completed (loss: 0.05824341997504234, acc: 0.9832134246826172)
[2025-02-13 03:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07][root][INFO] - Training Epoch: 2/2, step 109/7134 completed (loss: 0.048015620559453964, acc: 0.9871323704719543)
[2025-02-13 03:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07][root][INFO] - Training Epoch: 2/2, step 110/7134 completed (loss: 0.05396975576877594, acc: 0.9858989715576172)
[2025-02-13 03:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08][root][INFO] - Training Epoch: 2/2, step 111/7134 completed (loss: 0.04231254383921623, acc: 0.9895470142364502)
[2025-02-13 03:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08][root][INFO] - Training Epoch: 2/2, step 112/7134 completed (loss: 0.033028505742549896, acc: 0.990755021572113)
[2025-02-13 03:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08][root][INFO] - Training Epoch: 2/2, step 113/7134 completed (loss: 0.023800106719136238, acc: 0.9962025284767151)
[2025-02-13 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:09][root][INFO] - Training Epoch: 2/2, step 114/7134 completed (loss: 0.03328961506485939, acc: 0.9854111671447754)
[2025-02-13 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:09][root][INFO] - Training Epoch: 2/2, step 115/7134 completed (loss: 0.08550114929676056, acc: 0.9685039520263672)
[2025-02-13 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:10][root][INFO] - Training Epoch: 2/2, step 116/7134 completed (loss: 0.052512649446725845, acc: 0.9799764156341553)
[2025-02-13 03:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:10][root][INFO] - Training Epoch: 2/2, step 117/7134 completed (loss: 0.04365520551800728, acc: 0.9861496090888977)
[2025-02-13 03:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11][root][INFO] - Training Epoch: 2/2, step 118/7134 completed (loss: 0.03171174228191376, acc: 0.9885203838348389)
[2025-02-13 03:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11][root][INFO] - Training Epoch: 2/2, step 119/7134 completed (loss: 0.04915420711040497, acc: 0.9842615127563477)
[2025-02-13 03:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11][root][INFO] - Training Epoch: 2/2, step 120/7134 completed (loss: 0.01495580654591322, acc: 0.9960681796073914)
[2025-02-13 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12][root][INFO] - Training Epoch: 2/2, step 121/7134 completed (loss: 0.03734811022877693, acc: 0.9894319772720337)
[2025-02-13 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12][root][INFO] - Training Epoch: 2/2, step 122/7134 completed (loss: 0.019261522218585014, acc: 0.990728497505188)
[2025-02-13 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13][root][INFO] - Training Epoch: 2/2, step 123/7134 completed (loss: 0.014490279369056225, acc: 0.9954954981803894)
[2025-02-13 03:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13][root][INFO] - Training Epoch: 2/2, step 124/7134 completed (loss: 0.02434147708117962, acc: 0.9879310131072998)
[2025-02-13 03:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14][root][INFO] - Training Epoch: 2/2, step 125/7134 completed (loss: 0.03768559545278549, acc: 0.9856194853782654)
[2025-02-13 03:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14][root][INFO] - Training Epoch: 2/2, step 126/7134 completed (loss: 0.029335027560591698, acc: 0.9906666874885559)
[2025-02-13 03:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14][root][INFO] - Training Epoch: 2/2, step 127/7134 completed (loss: 0.01827738620340824, acc: 0.9925925731658936)
[2025-02-13 03:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15][root][INFO] - Training Epoch: 2/2, step 128/7134 completed (loss: 0.025360215455293655, acc: 0.9942330121994019)
[2025-02-13 03:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15][root][INFO] - Training Epoch: 2/2, step 129/7134 completed (loss: 0.04475283622741699, acc: 0.9914215803146362)
[2025-02-13 03:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:16][root][INFO] - Training Epoch: 2/2, step 130/7134 completed (loss: 0.02385902591049671, acc: 0.9894737005233765)
[2025-02-13 03:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:16][root][INFO] - Training Epoch: 2/2, step 131/7134 completed (loss: 0.019804587587714195, acc: 0.9934210777282715)
[2025-02-13 03:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17][root][INFO] - Training Epoch: 2/2, step 132/7134 completed (loss: 0.02432582527399063, acc: 0.994194507598877)
[2025-02-13 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17][root][INFO] - Training Epoch: 2/2, step 133/7134 completed (loss: 0.009361855685710907, acc: 0.9983360767364502)
[2025-02-13 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18][root][INFO] - Training Epoch: 2/2, step 134/7134 completed (loss: 0.025623919442296028, acc: 0.9946902394294739)
[2025-02-13 03:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18][root][INFO] - Training Epoch: 2/2, step 135/7134 completed (loss: 0.025846276432275772, acc: 0.996666669845581)
[2025-02-13 03:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18][root][INFO] - Training Epoch: 2/2, step 136/7134 completed (loss: 0.023417530581355095, acc: 0.9952210187911987)
[2025-02-13 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19][root][INFO] - Training Epoch: 2/2, step 137/7134 completed (loss: 0.049425940960645676, acc: 0.9876998662948608)
[2025-02-13 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19][root][INFO] - Training Epoch: 2/2, step 138/7134 completed (loss: 0.029982810840010643, acc: 0.9898862242698669)
[2025-02-13 03:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:20][root][INFO] - Training Epoch: 2/2, step 139/7134 completed (loss: 0.051633693277835846, acc: 0.9897698163986206)
[2025-02-13 03:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:20][root][INFO] - Training Epoch: 2/2, step 140/7134 completed (loss: 0.016494497656822205, acc: 0.9940298795700073)
[2025-02-13 03:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21][root][INFO] - Training Epoch: 2/2, step 141/7134 completed (loss: 0.010626337490975857, acc: 0.9964622855186462)
[2025-02-13 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21][root][INFO] - Training Epoch: 2/2, step 142/7134 completed (loss: 0.06690718233585358, acc: 0.9847009778022766)
[2025-02-13 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22][root][INFO] - Training Epoch: 2/2, step 143/7134 completed (loss: 0.009297914803028107, acc: 0.9973924160003662)
[2025-02-13 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22][root][INFO] - Training Epoch: 2/2, step 144/7134 completed (loss: 0.08702848851680756, acc: 0.9782971739768982)
[2025-02-13 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23][root][INFO] - Training Epoch: 2/2, step 145/7134 completed (loss: 0.1392877846956253, acc: 0.9619718194007874)
[2025-02-13 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23][root][INFO] - Training Epoch: 2/2, step 146/7134 completed (loss: 0.0568661168217659, acc: 0.9800570011138916)
[2025-02-13 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23][root][INFO] - Training Epoch: 2/2, step 147/7134 completed (loss: 0.06682217866182327, acc: 0.9792027473449707)
[2025-02-13 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24][root][INFO] - Training Epoch: 2/2, step 148/7134 completed (loss: 0.13246126472949982, acc: 0.9605568647384644)
[2025-02-13 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24][root][INFO] - Training Epoch: 2/2, step 149/7134 completed (loss: 0.10567290335893631, acc: 0.9767742156982422)
[2025-02-13 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25][root][INFO] - Training Epoch: 2/2, step 150/7134 completed (loss: 0.06439327448606491, acc: 0.9745989441871643)
[2025-02-13 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25][root][INFO] - Training Epoch: 2/2, step 151/7134 completed (loss: 0.07353543490171432, acc: 0.9773269891738892)
[2025-02-13 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26][root][INFO] - Training Epoch: 2/2, step 152/7134 completed (loss: 0.059845320880413055, acc: 0.9794988632202148)
[2025-02-13 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26][root][INFO] - Training Epoch: 2/2, step 153/7134 completed (loss: 0.09351722151041031, acc: 0.9790502786636353)
[2025-02-13 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26][root][INFO] - Training Epoch: 2/2, step 154/7134 completed (loss: 0.05917574465274811, acc: 0.9878419637680054)
[2025-02-13 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:27][root][INFO] - Training Epoch: 2/2, step 155/7134 completed (loss: 0.03897577151656151, acc: 0.9883117079734802)
[2025-02-13 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:27][root][INFO] - Training Epoch: 2/2, step 156/7134 completed (loss: 0.044651344418525696, acc: 0.9859976768493652)
[2025-02-13 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28][root][INFO] - Training Epoch: 2/2, step 157/7134 completed (loss: 0.08260651677846909, acc: 0.9795180559158325)
[2025-02-13 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28][root][INFO] - Training Epoch: 2/2, step 158/7134 completed (loss: 0.024835513904690742, acc: 0.9907529950141907)
[2025-02-13 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29][root][INFO] - Training Epoch: 2/2, step 159/7134 completed (loss: 0.059159498661756516, acc: 0.9826839566230774)
[2025-02-13 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29][root][INFO] - Training Epoch: 2/2, step 160/7134 completed (loss: 0.06900715082883835, acc: 0.97773277759552)
[2025-02-13 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29][root][INFO] - Training Epoch: 2/2, step 161/7134 completed (loss: 0.022901810705661774, acc: 0.9912790656089783)
[2025-02-13 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:30][root][INFO] - Training Epoch: 2/2, step 162/7134 completed (loss: 0.05009962245821953, acc: 0.9795396327972412)
[2025-02-13 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:30][root][INFO] - Training Epoch: 2/2, step 163/7134 completed (loss: 0.07421790808439255, acc: 0.985228955745697)
[2025-02-13 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31][root][INFO] - Training Epoch: 2/2, step 164/7134 completed (loss: 0.03679496422410011, acc: 0.9936169981956482)
[2025-02-13 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31][root][INFO] - Training Epoch: 2/2, step 165/7134 completed (loss: 0.07463311403989792, acc: 0.9832535982131958)
[2025-02-13 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31][root][INFO] - Training Epoch: 2/2, step 166/7134 completed (loss: 0.04542700573801994, acc: 0.9910447597503662)
[2025-02-13 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32][root][INFO] - Training Epoch: 2/2, step 167/7134 completed (loss: 0.4776803255081177, acc: 0.9126559495925903)
[2025-02-13 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32][root][INFO] - Training Epoch: 2/2, step 168/7134 completed (loss: 1.4301611185073853, acc: 0.7137255072593689)
[2025-02-13 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32][root][INFO] - Training Epoch: 2/2, step 169/7134 completed (loss: 0.9119888544082642, acc: 0.7759103775024414)
[2025-02-13 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33][root][INFO] - Training Epoch: 2/2, step 170/7134 completed (loss: 0.3584889769554138, acc: 0.9032257795333862)
[2025-02-13 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33][root][INFO] - Training Epoch: 2/2, step 171/7134 completed (loss: 0.4941166043281555, acc: 0.8831169009208679)
[2025-02-13 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33][root][INFO] - Training Epoch: 2/2, step 172/7134 completed (loss: 0.29718711972236633, acc: 0.90220046043396)
[2025-02-13 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:34][root][INFO] - Training Epoch: 2/2, step 173/7134 completed (loss: 0.23048901557922363, acc: 0.9469026327133179)
[2025-02-13 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:34][root][INFO] - Training Epoch: 2/2, step 174/7134 completed (loss: 0.22167877852916718, acc: 0.9436619877815247)
[2025-02-13 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35][root][INFO] - Training Epoch: 2/2, step 175/7134 completed (loss: 0.1692209541797638, acc: 0.9563636183738708)
[2025-02-13 03:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35][root][INFO] - Training Epoch: 2/2, step 176/7134 completed (loss: 0.13427191972732544, acc: 0.9594202637672424)
[2025-02-13 03:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35][root][INFO] - Training Epoch: 2/2, step 177/7134 completed (loss: 0.17703202366828918, acc: 0.9462962746620178)
[2025-02-13 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36][root][INFO] - Training Epoch: 2/2, step 178/7134 completed (loss: 0.1215878427028656, acc: 0.9534883499145508)
[2025-02-13 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36][root][INFO] - Training Epoch: 2/2, step 179/7134 completed (loss: 0.11624351143836975, acc: 0.9613152742385864)
[2025-02-13 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:37][root][INFO] - Training Epoch: 2/2, step 180/7134 completed (loss: 0.0852876603603363, acc: 0.9794344305992126)
[2025-02-13 03:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:37][root][INFO] - Training Epoch: 2/2, step 181/7134 completed (loss: 0.04204728081822395, acc: 0.9881423115730286)
[2025-02-13 03:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38][root][INFO] - Training Epoch: 2/2, step 182/7134 completed (loss: 0.04527869448065758, acc: 0.9853479862213135)
[2025-02-13 03:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38][root][INFO] - Training Epoch: 2/2, step 183/7134 completed (loss: 0.057861510664224625, acc: 0.9877384305000305)
[2025-02-13 03:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38][root][INFO] - Training Epoch: 2/2, step 184/7134 completed (loss: 0.09251923859119415, acc: 0.9769585132598877)
[2025-02-13 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39][root][INFO] - Training Epoch: 2/2, step 185/7134 completed (loss: 0.023136373609304428, acc: 0.9928469061851501)
[2025-02-13 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39][root][INFO] - Training Epoch: 2/2, step 186/7134 completed (loss: 0.06576066464185715, acc: 0.9775640964508057)
[2025-02-13 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40][root][INFO] - Training Epoch: 2/2, step 187/7134 completed (loss: 0.06963463872671127, acc: 0.9832402467727661)
[2025-02-13 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40][root][INFO] - Training Epoch: 2/2, step 188/7134 completed (loss: 0.13864880800247192, acc: 0.9624999761581421)
[2025-02-13 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40][root][INFO] - Training Epoch: 2/2, step 189/7134 completed (loss: 0.08812740445137024, acc: 0.9807692170143127)
[2025-02-13 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41][root][INFO] - Training Epoch: 2/2, step 190/7134 completed (loss: 0.17014901340007782, acc: 0.9687987565994263)
[2025-02-13 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41][root][INFO] - Training Epoch: 2/2, step 191/7134 completed (loss: 0.054154712706804276, acc: 0.9829059839248657)
[2025-02-13 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:42][root][INFO] - Training Epoch: 2/2, step 192/7134 completed (loss: 0.017595624551177025, acc: 0.993127167224884)
[2025-02-13 03:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:42][root][INFO] - Training Epoch: 2/2, step 193/7134 completed (loss: 0.06302240490913391, acc: 0.9798657894134521)
[2025-02-13 03:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:43][root][INFO] - Training Epoch: 2/2, step 194/7134 completed (loss: 0.057990849018096924, acc: 0.9799138903617859)
[2025-02-13 03:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:43][root][INFO] - Training Epoch: 2/2, step 195/7134 completed (loss: 0.06184057518839836, acc: 0.9798319339752197)
[2025-02-13 03:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44][root][INFO] - Training Epoch: 2/2, step 196/7134 completed (loss: 0.07755731046199799, acc: 0.9799465537071228)
[2025-02-13 03:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44][root][INFO] - Training Epoch: 2/2, step 197/7134 completed (loss: 0.06719831377267838, acc: 0.9886040091514587)
[2025-02-13 03:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44][root][INFO] - Training Epoch: 2/2, step 198/7134 completed (loss: 0.04347969591617584, acc: 0.98591548204422)
[2025-02-13 03:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:45][root][INFO] - Training Epoch: 2/2, step 199/7134 completed (loss: 0.08551310002803802, acc: 0.9704433679580688)
[2025-02-13 03:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:45][root][INFO] - Training Epoch: 2/2, step 200/7134 completed (loss: 0.02989836037158966, acc: 0.990604043006897)
[2025-02-13 03:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:46][root][INFO] - Training Epoch: 2/2, step 201/7134 completed (loss: 0.05334971845149994, acc: 0.9873595237731934)
[2025-02-13 03:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:46][root][INFO] - Training Epoch: 2/2, step 202/7134 completed (loss: 0.05075808987021446, acc: 0.9820689558982849)
[2025-02-13 03:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47][root][INFO] - Training Epoch: 2/2, step 203/7134 completed (loss: 0.061030369251966476, acc: 0.9793388247489929)
[2025-02-13 03:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47][root][INFO] - Training Epoch: 2/2, step 204/7134 completed (loss: 0.05402592942118645, acc: 0.9802431464195251)
[2025-02-13 03:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47][root][INFO] - Training Epoch: 2/2, step 205/7134 completed (loss: 0.0967726781964302, acc: 0.9767140746116638)
[2025-02-13 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48][root][INFO] - Training Epoch: 2/2, step 206/7134 completed (loss: 0.0930686891078949, acc: 0.9727126955986023)
[2025-02-13 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48][root][INFO] - Training Epoch: 2/2, step 207/7134 completed (loss: 0.07118921726942062, acc: 0.979522168636322)
[2025-02-13 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:49][root][INFO] - Training Epoch: 2/2, step 208/7134 completed (loss: 0.12249704450368881, acc: 0.9666666388511658)
[2025-02-13 03:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:49][root][INFO] - Training Epoch: 2/2, step 209/7134 completed (loss: 0.12637442350387573, acc: 0.9676165580749512)
[2025-02-13 03:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50][root][INFO] - Training Epoch: 2/2, step 210/7134 completed (loss: 0.17523130774497986, acc: 0.9627851247787476)
[2025-02-13 03:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50][root][INFO] - Training Epoch: 2/2, step 211/7134 completed (loss: 0.1443396508693695, acc: 0.9640522599220276)
[2025-02-13 03:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50][root][INFO] - Training Epoch: 2/2, step 212/7134 completed (loss: 0.2299790233373642, acc: 0.9566115736961365)
[2025-02-13 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:51][root][INFO] - Training Epoch: 2/2, step 213/7134 completed (loss: 0.07853934168815613, acc: 0.9773463010787964)
[2025-02-13 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:51][root][INFO] - Training Epoch: 2/2, step 214/7134 completed (loss: 0.06305010616779327, acc: 0.9850427508354187)
[2025-02-13 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:52][root][INFO] - Training Epoch: 2/2, step 215/7134 completed (loss: 0.08552813529968262, acc: 0.9826388955116272)
[2025-02-13 03:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:52][root][INFO] - Training Epoch: 2/2, step 216/7134 completed (loss: 0.06017214432358742, acc: 0.9861111044883728)
[2025-02-13 03:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53][root][INFO] - Training Epoch: 2/2, step 217/7134 completed (loss: 0.06416132301092148, acc: 0.9866666793823242)
[2025-02-13 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53][root][INFO] - Training Epoch: 2/2, step 218/7134 completed (loss: 0.023874733597040176, acc: 0.9961832165718079)
[2025-02-13 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53][root][INFO] - Training Epoch: 2/2, step 219/7134 completed (loss: 0.05253429338335991, acc: 0.9863547682762146)
[2025-02-13 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:54][root][INFO] - Training Epoch: 2/2, step 220/7134 completed (loss: 0.04925986006855965, acc: 0.9895397424697876)
[2025-02-13 03:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:54][root][INFO] - Training Epoch: 2/2, step 221/7134 completed (loss: 0.03476673737168312, acc: 0.9925373196601868)
[2025-02-13 03:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55][root][INFO] - Training Epoch: 2/2, step 222/7134 completed (loss: 0.058899473398923874, acc: 0.9869109988212585)
[2025-02-13 03:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55][root][INFO] - Training Epoch: 2/2, step 223/7134 completed (loss: 0.07983721047639847, acc: 0.9781771302223206)
[2025-02-13 03:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56][root][INFO] - Training Epoch: 2/2, step 224/7134 completed (loss: 0.11241743713617325, acc: 0.9675745964050293)
[2025-02-13 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56][root][INFO] - Training Epoch: 2/2, step 225/7134 completed (loss: 0.1092434972524643, acc: 0.9704749584197998)
[2025-02-13 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56][root][INFO] - Training Epoch: 2/2, step 226/7134 completed (loss: 0.04891923442482948, acc: 0.9902200698852539)
[2025-02-13 03:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:57][root][INFO] - Training Epoch: 2/2, step 227/7134 completed (loss: 0.16202397644519806, acc: 0.9652605652809143)
[2025-02-13 03:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:57][root][INFO] - Training Epoch: 2/2, step 228/7134 completed (loss: 0.06991103291511536, acc: 0.9797394871711731)
[2025-02-13 03:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:58][root][INFO] - Training Epoch: 2/2, step 229/7134 completed (loss: 0.10611969977617264, acc: 0.9721518754959106)
[2025-02-13 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:58][root][INFO] - Training Epoch: 2/2, step 230/7134 completed (loss: 0.06225300952792168, acc: 0.9766584634780884)
[2025-02-13 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][root][INFO] - Training Epoch: 2/2, step 231/7134 completed (loss: 0.06271762400865555, acc: 0.9792843461036682)
[2025-02-13 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][root][INFO] - Training Epoch: 2/2, step 232/7134 completed (loss: 0.04669648036360741, acc: 0.9835255146026611)
[2025-02-13 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][root][INFO] - Training Epoch: 2/2, step 233/7134 completed (loss: 0.06552741676568985, acc: 0.9857369065284729)
[2025-02-13 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00][root][INFO] - Training Epoch: 2/2, step 234/7134 completed (loss: 0.058399781584739685, acc: 0.9847972989082336)
[2025-02-13 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00][root][INFO] - Training Epoch: 2/2, step 235/7134 completed (loss: 0.10416820645332336, acc: 0.9800570011138916)
[2025-02-13 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01][root][INFO] - Training Epoch: 2/2, step 236/7134 completed (loss: 0.02923683449625969, acc: 0.989062488079071)
[2025-02-13 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01][root][INFO] - Training Epoch: 2/2, step 237/7134 completed (loss: 0.08067839592695236, acc: 0.9761549830436707)
[2025-02-13 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02][root][INFO] - Training Epoch: 2/2, step 238/7134 completed (loss: 0.10512956231832504, acc: 0.97947758436203)
[2025-02-13 03:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02][root][INFO] - Training Epoch: 2/2, step 239/7134 completed (loss: 0.023679303005337715, acc: 0.993697464466095)
[2025-02-13 03:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02][root][INFO] - Training Epoch: 2/2, step 240/7134 completed (loss: 0.0830456092953682, acc: 0.9788135886192322)
[2025-02-13 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03][root][INFO] - Training Epoch: 2/2, step 241/7134 completed (loss: 0.1115509644150734, acc: 0.979066014289856)
[2025-02-13 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03][root][INFO] - Training Epoch: 2/2, step 242/7134 completed (loss: 0.09387052059173584, acc: 0.9653379321098328)
[2025-02-13 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04][root][INFO] - Training Epoch: 2/2, step 243/7134 completed (loss: 0.10373543947935104, acc: 0.9727891087532043)
[2025-02-13 03:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04][root][INFO] - Training Epoch: 2/2, step 244/7134 completed (loss: 0.035062652081251144, acc: 0.9879336357116699)
[2025-02-13 03:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04][root][INFO] - Training Epoch: 2/2, step 245/7134 completed (loss: 0.09947285801172256, acc: 0.9772329330444336)
[2025-02-13 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05][root][INFO] - Training Epoch: 2/2, step 246/7134 completed (loss: 0.06077226251363754, acc: 0.9858044385910034)
[2025-02-13 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05][root][INFO] - Training Epoch: 2/2, step 247/7134 completed (loss: 0.07390972226858139, acc: 0.9764065146446228)
[2025-02-13 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06][root][INFO] - Training Epoch: 2/2, step 248/7134 completed (loss: 0.04600709304213524, acc: 0.9825581312179565)
[2025-02-13 03:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06][root][INFO] - Training Epoch: 2/2, step 249/7134 completed (loss: 0.05134829506278038, acc: 0.9812332391738892)
[2025-02-13 03:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06][root][INFO] - Training Epoch: 2/2, step 250/7134 completed (loss: 0.06078118830919266, acc: 0.982594907283783)
[2025-02-13 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07][root][INFO] - Training Epoch: 2/2, step 251/7134 completed (loss: 0.05181245878338814, acc: 0.9846153855323792)
[2025-02-13 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07][root][INFO] - Training Epoch: 2/2, step 252/7134 completed (loss: 0.030666276812553406, acc: 0.9901639223098755)
[2025-02-13 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08][root][INFO] - Training Epoch: 2/2, step 253/7134 completed (loss: 0.03604297339916229, acc: 0.987522304058075)
[2025-02-13 03:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08][root][INFO] - Training Epoch: 2/2, step 254/7134 completed (loss: 0.03305890038609505, acc: 0.9892857074737549)
[2025-02-13 03:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08][root][INFO] - Training Epoch: 2/2, step 255/7134 completed (loss: 0.04423712566494942, acc: 0.9824561476707458)
[2025-02-13 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:09][root][INFO] - Training Epoch: 2/2, step 256/7134 completed (loss: 0.0823865681886673, acc: 0.9742646813392639)
[2025-02-13 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:09][root][INFO] - Training Epoch: 2/2, step 257/7134 completed (loss: 0.055660635232925415, acc: 0.9846625924110413)
[2025-02-13 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:10][root][INFO] - Training Epoch: 2/2, step 258/7134 completed (loss: 0.039907343685626984, acc: 0.9851239919662476)
[2025-02-13 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:10][root][INFO] - Training Epoch: 2/2, step 259/7134 completed (loss: 0.048370618373155594, acc: 0.9837925434112549)
[2025-02-13 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11][root][INFO] - Training Epoch: 2/2, step 260/7134 completed (loss: 0.03365834429860115, acc: 0.9915397763252258)
[2025-02-13 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11][root][INFO] - Training Epoch: 2/2, step 261/7134 completed (loss: 0.04638369381427765, acc: 0.987730085849762)
[2025-02-13 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11][root][INFO] - Training Epoch: 2/2, step 262/7134 completed (loss: 0.020258795469999313, acc: 0.9916805028915405)
[2025-02-13 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:12][root][INFO] - Training Epoch: 2/2, step 263/7134 completed (loss: 0.05456375703215599, acc: 0.9872286319732666)
[2025-02-13 03:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:12][root][INFO] - Training Epoch: 2/2, step 264/7134 completed (loss: 0.04169755056500435, acc: 0.9871495366096497)
[2025-02-13 03:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:13][root][INFO] - Training Epoch: 2/2, step 265/7134 completed (loss: 0.06046314910054207, acc: 0.9832473993301392)
[2025-02-13 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:13][root][INFO] - Training Epoch: 2/2, step 266/7134 completed (loss: 0.03770475089550018, acc: 0.9885321259498596)
[2025-02-13 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:14][root][INFO] - Training Epoch: 2/2, step 267/7134 completed (loss: 0.05005352571606636, acc: 0.9793103337287903)
[2025-02-13 03:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:14][root][INFO] - Training Epoch: 2/2, step 268/7134 completed (loss: 0.0642537996172905, acc: 0.9844357967376709)
[2025-02-13 03:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15][root][INFO] - Training Epoch: 2/2, step 269/7134 completed (loss: 0.050396353006362915, acc: 0.9833101630210876)
[2025-02-13 03:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15][root][INFO] - Training Epoch: 2/2, step 270/7134 completed (loss: 0.04465114697813988, acc: 0.9880596995353699)
[2025-02-13 03:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15][root][INFO] - Training Epoch: 2/2, step 271/7134 completed (loss: 0.04836604744195938, acc: 0.9836065769195557)
[2025-02-13 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:16][root][INFO] - Training Epoch: 2/2, step 272/7134 completed (loss: 0.032547805458307266, acc: 0.9885495901107788)
[2025-02-13 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:16][root][INFO] - Training Epoch: 2/2, step 273/7134 completed (loss: 0.02413284219801426, acc: 0.9880239367485046)
[2025-02-13 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:17][root][INFO] - Training Epoch: 2/2, step 274/7134 completed (loss: 0.03294408321380615, acc: 0.993122398853302)
[2025-02-13 03:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:17][root][INFO] - Training Epoch: 2/2, step 275/7134 completed (loss: 0.05249736085534096, acc: 0.988399088382721)
[2025-02-13 03:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18][root][INFO] - Training Epoch: 2/2, step 276/7134 completed (loss: 0.025084584951400757, acc: 0.9902557730674744)
[2025-02-13 03:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18][root][INFO] - Training Epoch: 2/2, step 277/7134 completed (loss: 0.05388482287526131, acc: 0.9871299862861633)
[2025-02-13 03:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18][root][INFO] - Training Epoch: 2/2, step 278/7134 completed (loss: 0.03085296042263508, acc: 0.992337167263031)
[2025-02-13 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19][root][INFO] - Training Epoch: 2/2, step 279/7134 completed (loss: 0.03918430209159851, acc: 0.990813672542572)
[2025-02-13 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19][root][INFO] - Training Epoch: 2/2, step 280/7134 completed (loss: 0.03763937950134277, acc: 0.9856630563735962)
[2025-02-13 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20][root][INFO] - Training Epoch: 2/2, step 281/7134 completed (loss: 0.056152403354644775, acc: 0.9819999933242798)
[2025-02-13 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20][root][INFO] - Training Epoch: 2/2, step 282/7134 completed (loss: 0.07008504867553711, acc: 0.970992386341095)
[2025-02-13 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:21][root][INFO] - Training Epoch: 2/2, step 283/7134 completed (loss: 0.06590774655342102, acc: 0.9847198724746704)
[2025-02-13 03:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:21][root][INFO] - Training Epoch: 2/2, step 284/7134 completed (loss: 0.036939993500709534, acc: 0.9815436005592346)
[2025-02-13 03:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:21][root][INFO] - Training Epoch: 2/2, step 285/7134 completed (loss: 0.03908521682024002, acc: 0.9823633432388306)
[2025-02-13 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:22][root][INFO] - Training Epoch: 2/2, step 286/7134 completed (loss: 0.06287239491939545, acc: 0.9876106381416321)
[2025-02-13 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:22][root][INFO] - Training Epoch: 2/2, step 287/7134 completed (loss: 0.04726213216781616, acc: 0.9839486479759216)
[2025-02-13 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:23][root][INFO] - Training Epoch: 2/2, step 288/7134 completed (loss: 0.06380289047956467, acc: 0.9834254384040833)
[2025-02-13 03:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:23][root][INFO] - Training Epoch: 2/2, step 289/7134 completed (loss: 0.04172321781516075, acc: 0.9819168448448181)
[2025-02-13 03:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24][root][INFO] - Training Epoch: 2/2, step 290/7134 completed (loss: 0.02772989496588707, acc: 0.9908814430236816)
[2025-02-13 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24][root][INFO] - Training Epoch: 2/2, step 291/7134 completed (loss: 0.08228569477796555, acc: 0.9769737124443054)
[2025-02-13 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24][root][INFO] - Training Epoch: 2/2, step 292/7134 completed (loss: 0.09079008549451828, acc: 0.9723926186561584)
[2025-02-13 03:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:25][root][INFO] - Training Epoch: 2/2, step 293/7134 completed (loss: 0.06624671816825867, acc: 0.9839357137680054)
[2025-02-13 03:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:25][root][INFO] - Training Epoch: 2/2, step 294/7134 completed (loss: 0.048584721982479095, acc: 0.9844054579734802)
[2025-02-13 03:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:26][root][INFO] - Training Epoch: 2/2, step 295/7134 completed (loss: 0.07016812264919281, acc: 0.9812286496162415)
[2025-02-13 03:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:26][root][INFO] - Training Epoch: 2/2, step 296/7134 completed (loss: 0.031003180891275406, acc: 0.9906976819038391)
[2025-02-13 03:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27][root][INFO] - Training Epoch: 2/2, step 297/7134 completed (loss: 0.024848993867635727, acc: 0.9928951859474182)
[2025-02-13 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27][root][INFO] - Training Epoch: 2/2, step 298/7134 completed (loss: 0.020885618403553963, acc: 0.9919999837875366)
[2025-02-13 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27][root][INFO] - Training Epoch: 2/2, step 299/7134 completed (loss: 0.05127059295773506, acc: 0.9819548726081848)
[2025-02-13 03:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:28][root][INFO] - Training Epoch: 2/2, step 300/7134 completed (loss: 0.029756629839539528, acc: 0.9886731505393982)
[2025-02-13 03:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:28][root][INFO] - Training Epoch: 2/2, step 301/7134 completed (loss: 0.028377829119563103, acc: 0.9886845946311951)
[2025-02-13 03:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29][root][INFO] - Training Epoch: 2/2, step 302/7134 completed (loss: 0.0675991028547287, acc: 0.9824561476707458)
[2025-02-13 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29][root][INFO] - Training Epoch: 2/2, step 303/7134 completed (loss: 0.04718217998743057, acc: 0.9864077568054199)
[2025-02-13 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29][root][INFO] - Training Epoch: 2/2, step 304/7134 completed (loss: 0.026668647304177284, acc: 0.9870370626449585)
[2025-02-13 03:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:30][root][INFO] - Training Epoch: 2/2, step 305/7134 completed (loss: 0.03676334768533707, acc: 0.9920508861541748)
[2025-02-13 03:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:30][root][INFO] - Training Epoch: 2/2, step 306/7134 completed (loss: 0.011011441238224506, acc: 0.9981883764266968)
[2025-02-13 03:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31][root][INFO] - Training Epoch: 2/2, step 307/7134 completed (loss: 0.0624142661690712, acc: 0.9786477088928223)
[2025-02-13 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31][root][INFO] - Training Epoch: 2/2, step 308/7134 completed (loss: 0.01889495924115181, acc: 0.9950658082962036)
[2025-02-13 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:32][root][INFO] - Training Epoch: 2/2, step 309/7134 completed (loss: 0.0406540110707283, acc: 0.982758641242981)
[2025-02-13 03:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:32][root][INFO] - Training Epoch: 2/2, step 310/7134 completed (loss: 0.01957976073026657, acc: 0.9956140518188477)
[2025-02-13 03:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:32][root][INFO] - Training Epoch: 2/2, step 311/7134 completed (loss: 0.022926095873117447, acc: 0.993779182434082)
[2025-02-13 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33][root][INFO] - Training Epoch: 2/2, step 312/7134 completed (loss: 0.015259025618433952, acc: 0.998266875743866)
[2025-02-13 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33][root][INFO] - Training Epoch: 2/2, step 313/7134 completed (loss: 0.02289828658103943, acc: 0.991304337978363)
[2025-02-13 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34][root][INFO] - Training Epoch: 2/2, step 314/7134 completed (loss: 0.030601542443037033, acc: 0.9906250238418579)
[2025-02-13 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34][root][INFO] - Training Epoch: 2/2, step 315/7134 completed (loss: 0.03558119386434555, acc: 0.9918032884597778)
[2025-02-13 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34][root][INFO] - Training Epoch: 2/2, step 316/7134 completed (loss: 0.046063050627708435, acc: 0.987364649772644)
[2025-02-13 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35][root][INFO] - Training Epoch: 2/2, step 317/7134 completed (loss: 0.007921010255813599, acc: 0.9972936511039734)
[2025-02-13 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35][root][INFO] - Training Epoch: 2/2, step 318/7134 completed (loss: 0.07755123823881149, acc: 0.9829059839248657)
[2025-02-13 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:36][root][INFO] - Training Epoch: 2/2, step 319/7134 completed (loss: 0.06071984022855759, acc: 0.9859747290611267)
[2025-02-13 03:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:36][root][INFO] - Training Epoch: 2/2, step 320/7134 completed (loss: 0.038354869931936264, acc: 0.9914407730102539)
[2025-02-13 03:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37][root][INFO] - Training Epoch: 2/2, step 321/7134 completed (loss: 0.10063279420137405, acc: 0.977011501789093)
[2025-02-13 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37][root][INFO] - Training Epoch: 2/2, step 322/7134 completed (loss: 0.05943665653467178, acc: 0.9804804921150208)
[2025-02-13 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][root][INFO] - Training Epoch: 2/2, step 323/7134 completed (loss: 0.07259650528430939, acc: 0.9833119511604309)
[2025-02-13 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][root][INFO] - Training Epoch: 2/2, step 324/7134 completed (loss: 0.05973422899842262, acc: 0.9850402474403381)
[2025-02-13 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][root][INFO] - Training Epoch: 2/2, step 325/7134 completed (loss: 0.02198910340666771, acc: 0.9884318709373474)
[2025-02-13 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39][root][INFO] - Training Epoch: 2/2, step 326/7134 completed (loss: 0.03095098026096821, acc: 0.9939098954200745)
[2025-02-13 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39][root][INFO] - Training Epoch: 2/2, step 327/7134 completed (loss: 0.05359429121017456, acc: 0.9887387156486511)
[2025-02-13 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:40][root][INFO] - Training Epoch: 2/2, step 328/7134 completed (loss: 0.040599431842565536, acc: 0.9896296262741089)
[2025-02-13 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:40][root][INFO] - Training Epoch: 2/2, step 329/7134 completed (loss: 0.013211648911237717, acc: 0.9964747428894043)
[2025-02-13 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41][root][INFO] - Training Epoch: 2/2, step 330/7134 completed (loss: 0.03305327519774437, acc: 0.9879153966903687)
[2025-02-13 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41][root][INFO] - Training Epoch: 2/2, step 331/7134 completed (loss: 0.030801139771938324, acc: 0.9931585192680359)
[2025-02-13 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:42][root][INFO] - Training Epoch: 2/2, step 332/7134 completed (loss: 0.014453609474003315, acc: 0.9940968155860901)
[2025-02-13 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:42][root][INFO] - Training Epoch: 2/2, step 333/7134 completed (loss: 0.03955025225877762, acc: 0.989051103591919)
[2025-02-13 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43][root][INFO] - Training Epoch: 2/2, step 334/7134 completed (loss: 0.01870078593492508, acc: 0.9923954606056213)
[2025-02-13 03:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43][root][INFO] - Training Epoch: 2/2, step 335/7134 completed (loss: 0.019154733046889305, acc: 0.9956331849098206)
[2025-02-13 03:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43][root][INFO] - Training Epoch: 2/2, step 336/7134 completed (loss: 0.05004477500915527, acc: 0.9856287240982056)
[2025-02-13 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44][root][INFO] - Training Epoch: 2/2, step 337/7134 completed (loss: 0.022230803966522217, acc: 0.9923312664031982)
[2025-02-13 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44][root][INFO] - Training Epoch: 2/2, step 338/7134 completed (loss: 0.05628285929560661, acc: 0.98740553855896)
[2025-02-13 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:45][root][INFO] - Training Epoch: 2/2, step 339/7134 completed (loss: 0.02331489883363247, acc: 0.9887499809265137)
[2025-02-13 03:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:45][root][INFO] - Training Epoch: 2/2, step 340/7134 completed (loss: 0.04108607396483421, acc: 0.9847328066825867)
[2025-02-13 03:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46][root][INFO] - Training Epoch: 2/2, step 341/7134 completed (loss: 0.01865045167505741, acc: 0.9936948418617249)
[2025-02-13 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46][root][INFO] - Training Epoch: 2/2, step 342/7134 completed (loss: 0.013159731402993202, acc: 0.9954545497894287)
[2025-02-13 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46][root][INFO] - Training Epoch: 2/2, step 343/7134 completed (loss: 0.04106978327035904, acc: 0.9829867482185364)
[2025-02-13 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:47][root][INFO] - Training Epoch: 2/2, step 344/7134 completed (loss: 0.0121779665350914, acc: 0.9945255517959595)
[2025-02-13 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:47][root][INFO] - Training Epoch: 2/2, step 345/7134 completed (loss: 0.028038425371050835, acc: 0.9946428537368774)
[2025-02-13 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48][root][INFO] - Training Epoch: 2/2, step 346/7134 completed (loss: 0.02786163240671158, acc: 0.9904610514640808)
[2025-02-13 03:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48][root][INFO] - Training Epoch: 2/2, step 347/7134 completed (loss: 0.026649443432688713, acc: 0.9929873943328857)
[2025-02-13 03:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49][root][INFO] - Training Epoch: 2/2, step 348/7134 completed (loss: 0.011603281833231449, acc: 0.998643159866333)
[2025-02-13 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49][root][INFO] - Training Epoch: 2/2, step 349/7134 completed (loss: 0.02871258556842804, acc: 0.990604043006897)
[2025-02-13 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50][root][INFO] - Training Epoch: 2/2, step 350/7134 completed (loss: 0.04411445930600166, acc: 0.9891696572303772)
[2025-02-13 03:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50][root][INFO] - Training Epoch: 2/2, step 351/7134 completed (loss: 0.03390469774603844, acc: 0.9949109554290771)
[2025-02-13 03:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50][root][INFO] - Training Epoch: 2/2, step 352/7134 completed (loss: 0.02054019458591938, acc: 0.9945651888847351)
[2025-02-13 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51][root][INFO] - Training Epoch: 2/2, step 353/7134 completed (loss: 0.030594991520047188, acc: 0.9906914830207825)
[2025-02-13 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51][root][INFO] - Training Epoch: 2/2, step 354/7134 completed (loss: 0.017502550035715103, acc: 0.9965075850486755)
[2025-02-13 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:52][root][INFO] - Training Epoch: 2/2, step 355/7134 completed (loss: 0.05765102803707123, acc: 0.9888268113136292)
[2025-02-13 03:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:52][root][INFO] - Training Epoch: 2/2, step 356/7134 completed (loss: 0.020997168496251106, acc: 0.9920634627342224)
[2025-02-13 03:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:53][root][INFO] - Training Epoch: 2/2, step 357/7134 completed (loss: 0.019498025998473167, acc: 0.9936386942863464)
[2025-02-13 03:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:53][root][INFO] - Training Epoch: 2/2, step 358/7134 completed (loss: 0.029090436175465584, acc: 0.9890859723091125)
[2025-02-13 03:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54][root][INFO] - Training Epoch: 2/2, step 359/7134 completed (loss: 0.017218081280589104, acc: 0.9932432174682617)
[2025-02-13 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54][root][INFO] - Training Epoch: 2/2, step 360/7134 completed (loss: 0.02486223727464676, acc: 0.9939758777618408)
[2025-02-13 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54][root][INFO] - Training Epoch: 2/2, step 361/7134 completed (loss: 0.03557400405406952, acc: 0.9897210001945496)
[2025-02-13 03:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:55][root][INFO] - Training Epoch: 2/2, step 362/7134 completed (loss: 0.04094746708869934, acc: 0.9880239367485046)
[2025-02-13 03:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:55][root][INFO] - Training Epoch: 2/2, step 363/7134 completed (loss: 0.04334339499473572, acc: 0.9895833134651184)
[2025-02-13 03:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56][root][INFO] - Training Epoch: 2/2, step 364/7134 completed (loss: 0.020208686590194702, acc: 0.9953271150588989)
[2025-02-13 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56][root][INFO] - Training Epoch: 2/2, step 365/7134 completed (loss: 0.025027792900800705, acc: 0.9917355179786682)
[2025-02-13 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:57][root][INFO] - Training Epoch: 2/2, step 366/7134 completed (loss: 0.03317423164844513, acc: 0.9874607920646667)
[2025-02-13 03:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:57][root][INFO] - Training Epoch: 2/2, step 367/7134 completed (loss: 0.02891821227967739, acc: 0.9912499785423279)
[2025-02-13 03:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58][root][INFO] - Training Epoch: 2/2, step 368/7134 completed (loss: 0.013460952788591385, acc: 0.9955423474311829)
[2025-02-13 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58][root][INFO] - Training Epoch: 2/2, step 369/7134 completed (loss: 0.02352137304842472, acc: 0.9938744306564331)
[2025-02-13 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58][root][INFO] - Training Epoch: 2/2, step 370/7134 completed (loss: 0.04992973431944847, acc: 0.9868995547294617)
[2025-02-13 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:59][root][INFO] - Training Epoch: 2/2, step 371/7134 completed (loss: 0.025832396000623703, acc: 0.9909774661064148)
[2025-02-13 03:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:59][root][INFO] - Training Epoch: 2/2, step 372/7134 completed (loss: 0.031210042536258698, acc: 0.9910846948623657)
[2025-02-13 03:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00][root][INFO] - Training Epoch: 2/2, step 373/7134 completed (loss: 0.017328551039099693, acc: 0.994020938873291)
[2025-02-13 03:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00][root][INFO] - Training Epoch: 2/2, step 374/7134 completed (loss: 0.031848255544900894, acc: 0.9903537034988403)
[2025-02-13 03:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00][root][INFO] - Training Epoch: 2/2, step 375/7134 completed (loss: 0.04561690241098404, acc: 0.9906291961669922)
[2025-02-13 03:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:01][root][INFO] - Training Epoch: 2/2, step 376/7134 completed (loss: 0.042128901928663254, acc: 0.9862068891525269)
[2025-02-13 03:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:01][root][INFO] - Training Epoch: 2/2, step 377/7134 completed (loss: 0.03155864030122757, acc: 0.9876203536987305)
[2025-02-13 03:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02][root][INFO] - Training Epoch: 2/2, step 378/7134 completed (loss: 0.02424785867333412, acc: 0.9904631972312927)
[2025-02-13 03:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02][root][INFO] - Training Epoch: 2/2, step 379/7134 completed (loss: 0.011452523060142994, acc: 0.9962311387062073)
[2025-02-13 03:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:03][root][INFO] - Training Epoch: 2/2, step 380/7134 completed (loss: 0.05053617060184479, acc: 0.9829171895980835)
[2025-02-13 03:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:03][root][INFO] - Training Epoch: 2/2, step 381/7134 completed (loss: 0.04388302564620972, acc: 0.9922480583190918)
[2025-02-13 03:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04][root][INFO] - Training Epoch: 2/2, step 382/7134 completed (loss: 0.022612279281020164, acc: 0.9922978281974792)
[2025-02-13 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04][root][INFO] - Training Epoch: 2/2, step 383/7134 completed (loss: 0.03724776580929756, acc: 0.9905481934547424)
[2025-02-13 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04][root][INFO] - Training Epoch: 2/2, step 384/7134 completed (loss: 0.02239326946437359, acc: 0.9959946870803833)
[2025-02-13 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:05][root][INFO] - Training Epoch: 2/2, step 385/7134 completed (loss: 0.024555891752243042, acc: 0.9952903985977173)
[2025-02-13 03:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:05][root][INFO] - Training Epoch: 2/2, step 386/7134 completed (loss: 0.013717032968997955, acc: 0.9959100484848022)
[2025-02-13 03:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06][root][INFO] - Training Epoch: 2/2, step 387/7134 completed (loss: 0.033570967614650726, acc: 0.9854227304458618)
[2025-02-13 03:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06][root][INFO] - Training Epoch: 2/2, step 388/7134 completed (loss: 0.022932270541787148, acc: 0.9941314458847046)
[2025-02-13 03:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06][root][INFO] - Training Epoch: 2/2, step 389/7134 completed (loss: 0.05789143964648247, acc: 0.9820936918258667)
[2025-02-13 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:07][root][INFO] - Training Epoch: 2/2, step 390/7134 completed (loss: 0.025596624240279198, acc: 0.9967637658119202)
[2025-02-13 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:07][root][INFO] - Training Epoch: 2/2, step 391/7134 completed (loss: 0.03546725958585739, acc: 0.988095223903656)
[2025-02-13 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:08][root][INFO] - Training Epoch: 2/2, step 392/7134 completed (loss: 0.034190740436315536, acc: 0.9866666793823242)
[2025-02-13 03:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:08][root][INFO] - Training Epoch: 2/2, step 393/7134 completed (loss: 0.026768827810883522, acc: 0.9861111044883728)
[2025-02-13 03:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09][root][INFO] - Training Epoch: 2/2, step 394/7134 completed (loss: 0.029209483414888382, acc: 0.9902724027633667)
[2025-02-13 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09][root][INFO] - Training Epoch: 2/2, step 395/7134 completed (loss: 0.06787849217653275, acc: 0.9876543283462524)
[2025-02-13 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09][root][INFO] - Training Epoch: 2/2, step 396/7134 completed (loss: 0.016015473753213882, acc: 0.9978494644165039)
[2025-02-13 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:10][root][INFO] - Training Epoch: 2/2, step 397/7134 completed (loss: 0.06396616250276566, acc: 0.9907407164573669)
[2025-02-13 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:10][root][INFO] - Training Epoch: 2/2, step 398/7134 completed (loss: 0.028569430112838745, acc: 0.9935566782951355)
[2025-02-13 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:11][root][INFO] - Training Epoch: 2/2, step 399/7134 completed (loss: 0.05915694683790207, acc: 0.9873217344284058)
[2025-02-13 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:11][root][INFO] - Training Epoch: 2/2, step 400/7134 completed (loss: 0.0946555957198143, acc: 0.9746268391609192)
[2025-02-13 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12][root][INFO] - Training Epoch: 2/2, step 401/7134 completed (loss: 0.06245962902903557, acc: 0.9840686321258545)
[2025-02-13 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12][root][INFO] - Training Epoch: 2/2, step 402/7134 completed (loss: 0.024075187742710114, acc: 0.9949302673339844)
[2025-02-13 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13][root][INFO] - Training Epoch: 2/2, step 403/7134 completed (loss: 0.031256258487701416, acc: 0.9924405813217163)
[2025-02-13 03:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13][root][INFO] - Training Epoch: 2/2, step 404/7134 completed (loss: 0.02885323390364647, acc: 0.9884615540504456)
[2025-02-13 03:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13][root][INFO] - Training Epoch: 2/2, step 405/7134 completed (loss: 0.103118397295475, acc: 0.9797507524490356)
[2025-02-13 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:14][root][INFO] - Training Epoch: 2/2, step 406/7134 completed (loss: 0.058311574161052704, acc: 0.9898989796638489)
[2025-02-13 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:14][root][INFO] - Training Epoch: 2/2, step 407/7134 completed (loss: 0.12536142766475677, acc: 0.9711864590644836)
[2025-02-13 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15][root][INFO] - Training Epoch: 2/2, step 408/7134 completed (loss: 0.10125106573104858, acc: 0.9766803979873657)
[2025-02-13 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15][root][INFO] - Training Epoch: 2/2, step 409/7134 completed (loss: 0.052455734461545944, acc: 0.9823899269104004)
[2025-02-13 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16][root][INFO] - Training Epoch: 2/2, step 410/7134 completed (loss: 0.06409304589033127, acc: 0.9807121753692627)
[2025-02-13 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16][root][INFO] - Training Epoch: 2/2, step 411/7134 completed (loss: 0.04326144978404045, acc: 0.9896432757377625)
[2025-02-13 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17][root][INFO] - Training Epoch: 2/2, step 412/7134 completed (loss: 0.033951614052057266, acc: 0.991990864276886)
[2025-02-13 03:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17][root][INFO] - Training Epoch: 2/2, step 413/7134 completed (loss: 0.04181920737028122, acc: 0.9866488575935364)
[2025-02-13 03:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17][root][INFO] - Training Epoch: 2/2, step 414/7134 completed (loss: 0.035535670816898346, acc: 0.9866468906402588)
[2025-02-13 03:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18][root][INFO] - Training Epoch: 2/2, step 415/7134 completed (loss: 0.03882945328950882, acc: 0.9942129850387573)
[2025-02-13 03:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18][root][INFO] - Training Epoch: 2/2, step 416/7134 completed (loss: 0.025468388572335243, acc: 0.9929161667823792)
[2025-02-13 03:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19][root][INFO] - Training Epoch: 2/2, step 417/7134 completed (loss: 0.08752515912055969, acc: 0.979619562625885)
[2025-02-13 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19][root][INFO] - Training Epoch: 2/2, step 418/7134 completed (loss: 0.03172691911458969, acc: 0.9862227439880371)
[2025-02-13 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:20][root][INFO] - Training Epoch: 2/2, step 419/7134 completed (loss: 0.03854825720191002, acc: 0.9888337254524231)
[2025-02-13 03:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:20][root][INFO] - Training Epoch: 2/2, step 420/7134 completed (loss: 0.029130296781659126, acc: 0.9881556630134583)
[2025-02-13 03:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21][root][INFO] - Training Epoch: 2/2, step 421/7134 completed (loss: 0.029759222641587257, acc: 0.987261176109314)
[2025-02-13 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21][root][INFO] - Training Epoch: 2/2, step 422/7134 completed (loss: 0.037823379039764404, acc: 0.9852941036224365)
[2025-02-13 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21][root][INFO] - Training Epoch: 2/2, step 423/7134 completed (loss: 0.015764307230710983, acc: 0.9942857027053833)
[2025-02-13 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:22][root][INFO] - Training Epoch: 2/2, step 424/7134 completed (loss: 0.03188243508338928, acc: 0.9857594966888428)
[2025-02-13 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:22][root][INFO] - Training Epoch: 2/2, step 425/7134 completed (loss: 0.012950550764799118, acc: 0.9970930218696594)
[2025-02-13 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:23][root][INFO] - Training Epoch: 2/2, step 426/7134 completed (loss: 0.013262358494102955, acc: 0.9972144961357117)
[2025-02-13 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:23][root][INFO] - Training Epoch: 2/2, step 427/7134 completed (loss: 0.019311940297484398, acc: 0.9939302206039429)
[2025-02-13 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24][root][INFO] - Training Epoch: 2/2, step 428/7134 completed (loss: 0.019579820334911346, acc: 0.9969879388809204)
[2025-02-13 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24][root][INFO] - Training Epoch: 2/2, step 429/7134 completed (loss: 0.03707974776625633, acc: 0.9872813820838928)
[2025-02-13 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24][root][INFO] - Training Epoch: 2/2, step 430/7134 completed (loss: 0.020801037549972534, acc: 0.9928264021873474)
[2025-02-13 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:25][root][INFO] - Training Epoch: 2/2, step 431/7134 completed (loss: 0.024072496220469475, acc: 0.9942362904548645)
[2025-02-13 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:25][root][INFO] - Training Epoch: 2/2, step 432/7134 completed (loss: 0.013960449956357479, acc: 0.9945155382156372)
[2025-02-13 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:26][root][INFO] - Training Epoch: 2/2, step 433/7134 completed (loss: 0.026314008980989456, acc: 0.991847813129425)
[2025-02-13 03:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:26][root][INFO] - Training Epoch: 2/2, step 434/7134 completed (loss: 0.02017050050199032, acc: 0.9912152290344238)
[2025-02-13 03:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27][root][INFO] - Training Epoch: 2/2, step 435/7134 completed (loss: 0.024848945438861847, acc: 0.9943262338638306)
[2025-02-13 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27][root][INFO] - Training Epoch: 2/2, step 436/7134 completed (loss: 0.032963722944259644, acc: 0.9921259880065918)
[2025-02-13 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27][root][INFO] - Training Epoch: 2/2, step 437/7134 completed (loss: 0.04816192016005516, acc: 0.9893993139266968)
[2025-02-13 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:28][root][INFO] - Training Epoch: 2/2, step 438/7134 completed (loss: 0.027617117390036583, acc: 0.9865384697914124)
[2025-02-13 03:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:28][root][INFO] - Training Epoch: 2/2, step 439/7134 completed (loss: 0.033142101019620895, acc: 0.9921466112136841)
[2025-02-13 03:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29][root][INFO] - Training Epoch: 2/2, step 440/7134 completed (loss: 0.025334244593977928, acc: 0.9878683090209961)
[2025-02-13 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29][root][INFO] - Training Epoch: 2/2, step 441/7134 completed (loss: 0.010576550848782063, acc: 0.995502233505249)
[2025-02-13 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29][root][INFO] - Training Epoch: 2/2, step 442/7134 completed (loss: 0.014087740331888199, acc: 0.9981343150138855)
[2025-02-13 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30][root][INFO] - Training Epoch: 2/2, step 443/7134 completed (loss: 0.0035977966617792845, acc: 1.0)
[2025-02-13 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30][root][INFO] - Training Epoch: 2/2, step 444/7134 completed (loss: 0.0077897352166473866, acc: 0.9983333349227905)
[2025-02-13 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:31][root][INFO] - Training Epoch: 2/2, step 445/7134 completed (loss: 0.029776273295283318, acc: 0.991253674030304)
[2025-02-13 03:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:31][root][INFO] - Training Epoch: 2/2, step 446/7134 completed (loss: 0.00750237051397562, acc: 0.9958217144012451)
[2025-02-13 03:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][root][INFO] - Training Epoch: 2/2, step 447/7134 completed (loss: 0.016238607466220856, acc: 0.9958391189575195)
[2025-02-13 03:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][root][INFO] - Training Epoch: 2/2, step 448/7134 completed (loss: 0.05638102442026138, acc: 0.9846547245979309)
[2025-02-13 03:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][root][INFO] - Training Epoch: 2/2, step 449/7134 completed (loss: 0.11975826323032379, acc: 0.9751309156417847)
[2025-02-13 03:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:33][root][INFO] - Training Epoch: 2/2, step 450/7134 completed (loss: 0.051115456968545914, acc: 0.9883227348327637)
[2025-02-13 03:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:33][root][INFO] - Training Epoch: 2/2, step 451/7134 completed (loss: 0.08891734480857849, acc: 0.9762187600135803)
[2025-02-13 03:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:34][root][INFO] - Training Epoch: 2/2, step 452/7134 completed (loss: 0.08167976140975952, acc: 0.9684931635856628)
[2025-02-13 03:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:34][root][INFO] - Training Epoch: 2/2, step 453/7134 completed (loss: 0.05729798227548599, acc: 0.9816513657569885)
[2025-02-13 03:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:35][root][INFO] - Training Epoch: 2/2, step 454/7134 completed (loss: 0.07738173753023148, acc: 0.9784714579582214)
[2025-02-13 03:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:35][root][INFO] - Training Epoch: 2/2, step 455/7134 completed (loss: 0.09485767781734467, acc: 0.9764705896377563)
[2025-02-13 03:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36][root][INFO] - Training Epoch: 2/2, step 456/7134 completed (loss: 0.019842540845274925, acc: 0.9929178357124329)
[2025-02-13 03:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36][root][INFO] - Training Epoch: 2/2, step 457/7134 completed (loss: 0.028804145753383636, acc: 0.994966447353363)
[2025-02-13 03:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36][root][INFO] - Training Epoch: 2/2, step 458/7134 completed (loss: 0.027014102786779404, acc: 0.9940357804298401)
[2025-02-13 03:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:37][root][INFO] - Training Epoch: 2/2, step 459/7134 completed (loss: 0.07925298810005188, acc: 0.9777777791023254)
[2025-02-13 03:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:37][root][INFO] - Training Epoch: 2/2, step 460/7134 completed (loss: 0.07329651713371277, acc: 0.9842519760131836)
[2025-02-13 03:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:38][root][INFO] - Training Epoch: 2/2, step 461/7134 completed (loss: 0.036824412643909454, acc: 0.9873772859573364)
[2025-02-13 03:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:38][root][INFO] - Training Epoch: 2/2, step 462/7134 completed (loss: 0.0759044960141182, acc: 0.9778037667274475)
[2025-02-13 03:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:39][root][INFO] - Training Epoch: 2/2, step 463/7134 completed (loss: 0.0644615888595581, acc: 0.9836888313293457)
[2025-02-13 03:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:39][root][INFO] - Training Epoch: 2/2, step 464/7134 completed (loss: 0.04905308410525322, acc: 0.9830028414726257)
[2025-02-13 03:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40][root][INFO] - Training Epoch: 2/2, step 465/7134 completed (loss: 0.07931706309318542, acc: 0.9791122674942017)
[2025-02-13 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40][root][INFO] - Training Epoch: 2/2, step 466/7134 completed (loss: 0.10082109272480011, acc: 0.9729729890823364)
[2025-02-13 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41][root][INFO] - Training Epoch: 2/2, step 467/7134 completed (loss: 0.09457121789455414, acc: 0.9716874361038208)
[2025-02-13 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41][root][INFO] - Training Epoch: 2/2, step 468/7134 completed (loss: 0.16600826382637024, acc: 0.9628610610961914)
[2025-02-13 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41][root][INFO] - Training Epoch: 2/2, step 469/7134 completed (loss: 0.08317822217941284, acc: 0.9769452214241028)
[2025-02-13 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42][root][INFO] - Training Epoch: 2/2, step 470/7134 completed (loss: 0.08923619985580444, acc: 0.9795918464660645)
[2025-02-13 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42][root][INFO] - Training Epoch: 2/2, step 471/7134 completed (loss: 0.07380697131156921, acc: 0.9798816442489624)
[2025-02-13 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43][root][INFO] - Training Epoch: 2/2, step 472/7134 completed (loss: 0.09103626012802124, acc: 0.9780058860778809)
[2025-02-13 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43][root][INFO] - Training Epoch: 2/2, step 473/7134 completed (loss: 0.04765850678086281, acc: 0.9830247163772583)
[2025-02-13 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43][root][INFO] - Training Epoch: 2/2, step 474/7134 completed (loss: 0.07864192128181458, acc: 0.9815546870231628)
[2025-02-13 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:44][root][INFO] - Training Epoch: 2/2, step 475/7134 completed (loss: 0.11260538548231125, acc: 0.9735293984413147)
[2025-02-13 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:44][root][INFO] - Training Epoch: 2/2, step 476/7134 completed (loss: 0.07530330121517181, acc: 0.9699140191078186)
[2025-02-13 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:45][root][INFO] - Training Epoch: 2/2, step 477/7134 completed (loss: 0.05668538063764572, acc: 0.9828850626945496)
[2025-02-13 03:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:45][root][INFO] - Training Epoch: 2/2, step 478/7134 completed (loss: 0.12124019116163254, acc: 0.957716703414917)
[2025-02-13 03:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46][root][INFO] - Training Epoch: 2/2, step 479/7134 completed (loss: 0.03337594121694565, acc: 0.9885222315788269)
[2025-02-13 03:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46][root][INFO] - Training Epoch: 2/2, step 480/7134 completed (loss: 0.025321893393993378, acc: 0.9941605925559998)
[2025-02-13 03:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46][root][INFO] - Training Epoch: 2/2, step 481/7134 completed (loss: 0.04322640970349312, acc: 0.9856770634651184)
[2025-02-13 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47][root][INFO] - Training Epoch: 2/2, step 482/7134 completed (loss: 0.018789811059832573, acc: 0.9928571581840515)
[2025-02-13 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47][root][INFO] - Training Epoch: 2/2, step 483/7134 completed (loss: 0.011747286655008793, acc: 0.9975429773330688)
[2025-02-13 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:48][root][INFO] - Training Epoch: 2/2, step 484/7134 completed (loss: 0.06815481185913086, acc: 0.9852941036224365)
[2025-02-13 03:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:48][root][INFO] - Training Epoch: 2/2, step 485/7134 completed (loss: 0.08262410759925842, acc: 0.9848713874816895)
[2025-02-13 03:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49][root][INFO] - Training Epoch: 2/2, step 486/7134 completed (loss: 0.04371137171983719, acc: 0.9901315569877625)
[2025-02-13 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49][root][INFO] - Training Epoch: 2/2, step 487/7134 completed (loss: 0.048156049102544785, acc: 0.988252580165863)
[2025-02-13 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49][root][INFO] - Training Epoch: 2/2, step 488/7134 completed (loss: 0.09776278585195541, acc: 0.9728260636329651)
[2025-02-13 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50][root][INFO] - Training Epoch: 2/2, step 489/7134 completed (loss: 0.08221370726823807, acc: 0.978151261806488)
[2025-02-13 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50][root][INFO] - Training Epoch: 2/2, step 490/7134 completed (loss: 0.04303315281867981, acc: 0.9959183931350708)
[2025-02-13 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:51][root][INFO] - Training Epoch: 2/2, step 491/7134 completed (loss: 0.03915630280971527, acc: 0.9901315569877625)
[2025-02-13 03:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:51][root][INFO] - Training Epoch: 2/2, step 492/7134 completed (loss: 0.009700891561806202, acc: 0.998487114906311)
[2025-02-13 03:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52][root][INFO] - Training Epoch: 2/2, step 493/7134 completed (loss: 0.0591462142765522, acc: 0.9785100221633911)
[2025-02-13 03:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52][root][INFO] - Training Epoch: 2/2, step 494/7134 completed (loss: 0.02152843028306961, acc: 0.9921875)
[2025-02-13 03:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52][root][INFO] - Training Epoch: 2/2, step 495/7134 completed (loss: 0.029485376551747322, acc: 0.9882352948188782)
[2025-02-13 03:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:53][root][INFO] - Training Epoch: 2/2, step 496/7134 completed (loss: 0.048671040683984756, acc: 0.989159882068634)
[2025-02-13 03:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:53][root][INFO] - Training Epoch: 2/2, step 497/7134 completed (loss: 0.06759557873010635, acc: 0.9806201457977295)
[2025-02-13 03:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54][root][INFO] - Training Epoch: 2/2, step 498/7134 completed (loss: 0.028349079191684723, acc: 0.9935897588729858)
[2025-02-13 03:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54][root][INFO] - Training Epoch: 2/2, step 499/7134 completed (loss: 0.014953713864088058, acc: 0.9958275556564331)
[2025-02-13 03:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54][root][INFO] - Training Epoch: 2/2, step 500/7134 completed (loss: 0.03187984600663185, acc: 0.9919999837875366)
[2025-02-13 03:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:55][root][INFO] - Training Epoch: 2/2, step 501/7134 completed (loss: 0.014879134483635426, acc: 0.9958449006080627)
[2025-02-13 03:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:55][root][INFO] - Training Epoch: 2/2, step 502/7134 completed (loss: 0.016780924052000046, acc: 0.9954128265380859)
[2025-02-13 03:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:56][root][INFO] - Training Epoch: 2/2, step 503/7134 completed (loss: 0.03825782611966133, acc: 0.9865771532058716)
[2025-02-13 03:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:56][root][INFO] - Training Epoch: 2/2, step 504/7134 completed (loss: 0.017574846744537354, acc: 0.994413435459137)
[2025-02-13 03:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57][root][INFO] - Training Epoch: 2/2, step 505/7134 completed (loss: 0.01614905707538128, acc: 0.9947643876075745)
[2025-02-13 03:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57][root][INFO] - Training Epoch: 2/2, step 506/7134 completed (loss: 0.028114868327975273, acc: 0.9937810897827148)
[2025-02-13 03:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57][root][INFO] - Training Epoch: 2/2, step 507/7134 completed (loss: 0.046176727861166, acc: 0.99210524559021)
[2025-02-13 03:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:58][root][INFO] - Training Epoch: 2/2, step 508/7134 completed (loss: 0.03963407129049301, acc: 0.9846368432044983)
[2025-02-13 03:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:58][root][INFO] - Training Epoch: 2/2, step 509/7134 completed (loss: 0.04165024310350418, acc: 0.9829984307289124)
[2025-02-13 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59][root][INFO] - Training Epoch: 2/2, step 510/7134 completed (loss: 0.0981653556227684, acc: 0.9792099595069885)
[2025-02-13 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59][root][INFO] - Training Epoch: 2/2, step 511/7134 completed (loss: 0.056001391261816025, acc: 0.9853300452232361)
[2025-02-13 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00][root][INFO] - Training Epoch: 2/2, step 512/7134 completed (loss: 0.08038253337144852, acc: 0.9859719276428223)
[2025-02-13 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00][root][INFO] - Training Epoch: 2/2, step 513/7134 completed (loss: 0.07588287442922592, acc: 0.9810671210289001)
[2025-02-13 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00][root][INFO] - Training Epoch: 2/2, step 514/7134 completed (loss: 0.0797024518251419, acc: 0.9748743772506714)
[2025-02-13 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01][root][INFO] - Training Epoch: 2/2, step 515/7134 completed (loss: 0.029312096536159515, acc: 0.9915134310722351)
[2025-02-13 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01][root][INFO] - Training Epoch: 2/2, step 516/7134 completed (loss: 0.03572140634059906, acc: 0.9931350350379944)
[2025-02-13 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02][root][INFO] - Training Epoch: 2/2, step 517/7134 completed (loss: 0.09153177589178085, acc: 0.9767025113105774)
[2025-02-13 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02][root][INFO] - Training Epoch: 2/2, step 518/7134 completed (loss: 0.040072306990623474, acc: 0.9925037622451782)
[2025-02-13 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02][root][INFO] - Training Epoch: 2/2, step 519/7134 completed (loss: 0.03218566253781319, acc: 0.9896907210350037)
[2025-02-13 03:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:03][root][INFO] - Training Epoch: 2/2, step 520/7134 completed (loss: 0.01640027016401291, acc: 0.9952531456947327)
[2025-02-13 03:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:03][root][INFO] - Training Epoch: 2/2, step 521/7134 completed (loss: 0.05391871556639671, acc: 0.9893190860748291)
[2025-02-13 03:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04][root][INFO] - Training Epoch: 2/2, step 522/7134 completed (loss: 0.029946910217404366, acc: 0.9932975769042969)
[2025-02-13 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04][root][INFO] - Training Epoch: 2/2, step 523/7134 completed (loss: 0.017750797793269157, acc: 0.9956140518188477)
[2025-02-13 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05][root][INFO] - Training Epoch: 2/2, step 524/7134 completed (loss: 0.0413946732878685, acc: 0.9892473220825195)
[2025-02-13 03:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05][root][INFO] - Training Epoch: 2/2, step 525/7134 completed (loss: 0.07242047786712646, acc: 0.9814528822898865)
[2025-02-13 03:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05][root][INFO] - Training Epoch: 2/2, step 526/7134 completed (loss: 0.03930612653493881, acc: 0.9890410900115967)
[2025-02-13 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:06][root][INFO] - Training Epoch: 2/2, step 527/7134 completed (loss: 0.040365103632211685, acc: 0.9925373196601868)
[2025-02-13 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:06][root][INFO] - Training Epoch: 2/2, step 528/7134 completed (loss: 0.004292580299079418, acc: 1.0)
[2025-02-13 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:07][root][INFO] - Training Epoch: 2/2, step 529/7134 completed (loss: 0.05676183104515076, acc: 0.9873015880584717)
[2025-02-13 03:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:07][root][INFO] - Training Epoch: 2/2, step 530/7134 completed (loss: 0.026645081117749214, acc: 0.9931389093399048)
[2025-02-13 03:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08][root][INFO] - Training Epoch: 2/2, step 531/7134 completed (loss: 0.08703017234802246, acc: 0.9829059839248657)
[2025-02-13 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08][root][INFO] - Training Epoch: 2/2, step 532/7134 completed (loss: 0.01840978115797043, acc: 0.9958419799804688)
[2025-02-13 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08][root][INFO] - Training Epoch: 2/2, step 533/7134 completed (loss: 0.08162444084882736, acc: 0.9793814420700073)
[2025-02-13 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09][root][INFO] - Training Epoch: 2/2, step 534/7134 completed (loss: 0.018826641142368317, acc: 0.9958791136741638)
[2025-02-13 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09][root][INFO] - Training Epoch: 2/2, step 535/7134 completed (loss: 0.034611139446496964, acc: 0.9929701089859009)
[2025-02-13 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10][root][INFO] - Training Epoch: 2/2, step 536/7134 completed (loss: 0.02971666119992733, acc: 0.9912408590316772)
[2025-02-13 03:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10][root][INFO] - Training Epoch: 2/2, step 537/7134 completed (loss: 0.03643748536705971, acc: 0.9867452383041382)
[2025-02-13 03:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11][root][INFO] - Training Epoch: 2/2, step 538/7134 completed (loss: 0.03329523652791977, acc: 0.9896142482757568)
[2025-02-13 03:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11][root][INFO] - Training Epoch: 2/2, step 539/7134 completed (loss: 0.03683081269264221, acc: 0.9896193742752075)
[2025-02-13 03:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11][root][INFO] - Training Epoch: 2/2, step 540/7134 completed (loss: 0.027099544182419777, acc: 0.9878787994384766)
[2025-02-13 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12][root][INFO] - Training Epoch: 2/2, step 541/7134 completed (loss: 0.05935361608862877, acc: 0.9853723645210266)
[2025-02-13 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12][root][INFO] - Training Epoch: 2/2, step 542/7134 completed (loss: 0.07632039487361908, acc: 0.9835680723190308)
[2025-02-13 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13][root][INFO] - Training Epoch: 2/2, step 543/7134 completed (loss: 0.08607140183448792, acc: 0.9710668921470642)
[2025-02-13 03:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13][root][INFO] - Training Epoch: 2/2, step 544/7134 completed (loss: 0.06644739210605621, acc: 0.9794952869415283)
[2025-02-13 03:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13][root][INFO] - Training Epoch: 2/2, step 545/7134 completed (loss: 0.04831133037805557, acc: 0.9855072498321533)
[2025-02-13 03:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:14][root][INFO] - Training Epoch: 2/2, step 546/7134 completed (loss: 0.11134849488735199, acc: 0.9692671298980713)
[2025-02-13 03:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:14][root][INFO] - Training Epoch: 2/2, step 547/7134 completed (loss: 0.0523449145257473, acc: 0.9883889555931091)
[2025-02-13 03:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:15][root][INFO] - Training Epoch: 2/2, step 548/7134 completed (loss: 0.031879130750894547, acc: 0.9934810996055603)
[2025-02-13 03:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:15][root][INFO] - Training Epoch: 2/2, step 549/7134 completed (loss: 0.06282762438058853, acc: 0.9736024737358093)
[2025-02-13 03:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:16][root][INFO] - Training Epoch: 2/2, step 550/7134 completed (loss: 0.05548287183046341, acc: 0.9846416115760803)
[2025-02-13 03:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:16][root][INFO] - Training Epoch: 2/2, step 551/7134 completed (loss: 0.04711895063519478, acc: 0.9852670431137085)
[2025-02-13 03:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:16][root][INFO] - Training Epoch: 2/2, step 552/7134 completed (loss: 0.044388607144355774, acc: 0.9829059839248657)
[2025-02-13 03:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:17][root][INFO] - Training Epoch: 2/2, step 553/7134 completed (loss: 0.042987484484910965, acc: 0.984649121761322)
[2025-02-13 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:17][root][INFO] - Training Epoch: 2/2, step 554/7134 completed (loss: 0.0749991312623024, acc: 0.980028510093689)
[2025-02-13 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18][root][INFO] - Training Epoch: 2/2, step 555/7134 completed (loss: 0.055235959589481354, acc: 0.9810426831245422)
[2025-02-13 03:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18][root][INFO] - Training Epoch: 2/2, step 556/7134 completed (loss: 0.03763619810342789, acc: 0.9900990128517151)
[2025-02-13 03:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18][root][INFO] - Training Epoch: 2/2, step 557/7134 completed (loss: 0.054055992513895035, acc: 0.9830795526504517)
[2025-02-13 03:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:19][root][INFO] - Training Epoch: 2/2, step 558/7134 completed (loss: 0.022900806739926338, acc: 0.9943262338638306)
[2025-02-13 03:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:19][root][INFO] - Training Epoch: 2/2, step 559/7134 completed (loss: 0.09003561735153198, acc: 0.9672364592552185)
[2025-02-13 03:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:20][root][INFO] - Training Epoch: 2/2, step 560/7134 completed (loss: 0.03587809577584267, acc: 0.9912126660346985)
[2025-02-13 03:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:20][root][INFO] - Training Epoch: 2/2, step 561/7134 completed (loss: 0.0630553737282753, acc: 0.9795657992362976)
[2025-02-13 03:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21][root][INFO] - Training Epoch: 2/2, step 562/7134 completed (loss: 0.03390049561858177, acc: 0.9854304790496826)
[2025-02-13 03:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21][root][INFO] - Training Epoch: 2/2, step 563/7134 completed (loss: 0.07652287185192108, acc: 0.9762658476829529)
[2025-02-13 03:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21][root][INFO] - Training Epoch: 2/2, step 564/7134 completed (loss: 0.04510171338915825, acc: 0.9876390695571899)
[2025-02-13 03:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:22][root][INFO] - Training Epoch: 2/2, step 565/7134 completed (loss: 0.05771384388208389, acc: 0.9765493869781494)
[2025-02-13 03:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:22][root][INFO] - Training Epoch: 2/2, step 566/7134 completed (loss: 0.08301568776369095, acc: 0.9744779467582703)
[2025-02-13 03:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23][root][INFO] - Training Epoch: 2/2, step 567/7134 completed (loss: 0.04928358271718025, acc: 0.9901823401451111)
[2025-02-13 03:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23][root][INFO] - Training Epoch: 2/2, step 568/7134 completed (loss: 0.06073050945997238, acc: 0.98562091588974)
[2025-02-13 03:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24][root][INFO] - Training Epoch: 2/2, step 569/7134 completed (loss: 0.028295787051320076, acc: 0.9925925731658936)
[2025-02-13 03:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24][root][INFO] - Training Epoch: 2/2, step 570/7134 completed (loss: 0.03054729476571083, acc: 0.9894242286682129)
[2025-02-13 03:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24][root][INFO] - Training Epoch: 2/2, step 571/7134 completed (loss: 0.02258746325969696, acc: 0.992443323135376)
[2025-02-13 03:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:25][root][INFO] - Training Epoch: 2/2, step 572/7134 completed (loss: 0.02464328519999981, acc: 0.9897040128707886)
[2025-02-13 03:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:25][root][INFO] - Training Epoch: 2/2, step 573/7134 completed (loss: 0.013110755942761898, acc: 0.9948849081993103)
[2025-02-13 03:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:26][root][INFO] - Training Epoch: 2/2, step 574/7134 completed (loss: 0.027145229279994965, acc: 0.9934123754501343)
[2025-02-13 03:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:26][root][INFO] - Training Epoch: 2/2, step 575/7134 completed (loss: 0.02394784800708294, acc: 0.9922879338264465)
[2025-02-13 03:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27][root][INFO] - Training Epoch: 2/2, step 576/7134 completed (loss: 0.024889670312404633, acc: 0.9962871074676514)
[2025-02-13 03:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27][root][INFO] - Training Epoch: 2/2, step 577/7134 completed (loss: 0.027952423319220543, acc: 0.9881423115730286)
[2025-02-13 03:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28][root][INFO] - Training Epoch: 2/2, step 578/7134 completed (loss: 0.0372721366584301, acc: 0.988399088382721)
[2025-02-13 03:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28][root][INFO] - Training Epoch: 2/2, step 579/7134 completed (loss: 0.060059044510126114, acc: 0.9845956563949585)
[2025-02-13 03:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28][root][INFO] - Training Epoch: 2/2, step 580/7134 completed (loss: 0.05847373977303505, acc: 0.9804941415786743)
[2025-02-13 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:29][root][INFO] - Training Epoch: 2/2, step 581/7134 completed (loss: 0.029294490814208984, acc: 0.991631805896759)
[2025-02-13 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:29][root][INFO] - Training Epoch: 2/2, step 582/7134 completed (loss: 0.02829386293888092, acc: 0.9892215728759766)
[2025-02-13 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:30][root][INFO] - Training Epoch: 2/2, step 583/7134 completed (loss: 0.028225457295775414, acc: 0.9909090995788574)
[2025-02-13 03:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:30][root][INFO] - Training Epoch: 2/2, step 584/7134 completed (loss: 0.014007709920406342, acc: 0.9964115023612976)
[2025-02-13 03:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31][root][INFO] - Training Epoch: 2/2, step 585/7134 completed (loss: 0.026305517181754112, acc: 0.9939613342285156)
[2025-02-13 03:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31][root][INFO] - Training Epoch: 2/2, step 586/7134 completed (loss: 0.03064758889377117, acc: 0.9895697236061096)
[2025-02-13 03:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31][root][INFO] - Training Epoch: 2/2, step 587/7134 completed (loss: 0.03938274458050728, acc: 0.9898348450660706)
[2025-02-13 03:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32][root][INFO] - Training Epoch: 2/2, step 588/7134 completed (loss: 0.025089148432016373, acc: 0.9937205910682678)
[2025-02-13 03:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32][root][INFO] - Training Epoch: 2/2, step 589/7134 completed (loss: 0.027493562549352646, acc: 0.9897210001945496)
[2025-02-13 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:33][root][INFO] - Training Epoch: 2/2, step 590/7134 completed (loss: 0.02279851585626602, acc: 0.995488703250885)
[2025-02-13 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:33][root][INFO] - Training Epoch: 2/2, step 591/7134 completed (loss: 0.028536517173051834, acc: 0.9927361011505127)
[2025-02-13 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34][root][INFO] - Training Epoch: 2/2, step 592/7134 completed (loss: 0.02515161596238613, acc: 0.9923858046531677)
[2025-02-13 03:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34][root][INFO] - Training Epoch: 2/2, step 593/7134 completed (loss: 0.04440474137663841, acc: 0.9868766665458679)
[2025-02-13 03:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35][root][INFO] - Training Epoch: 2/2, step 594/7134 completed (loss: 0.05644404888153076, acc: 0.9874213933944702)
[2025-02-13 03:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35][root][INFO] - Training Epoch: 2/2, step 595/7134 completed (loss: 0.03242028132081032, acc: 0.9898862242698669)
[2025-02-13 03:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35][root][INFO] - Training Epoch: 2/2, step 596/7134 completed (loss: 0.03731195628643036, acc: 0.9909793734550476)
[2025-02-13 03:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:36][root][INFO] - Training Epoch: 2/2, step 597/7134 completed (loss: 0.12228653579950333, acc: 0.9710144996643066)
[2025-02-13 03:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:36][root][INFO] - Training Epoch: 2/2, step 598/7134 completed (loss: 0.05887996032834053, acc: 0.9824086427688599)
[2025-02-13 03:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:37][root][INFO] - Training Epoch: 2/2, step 599/7134 completed (loss: 0.03071601316332817, acc: 0.9874686598777771)
[2025-02-13 03:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:37][root][INFO] - Training Epoch: 2/2, step 600/7134 completed (loss: 0.03964259847998619, acc: 0.9927797913551331)
[2025-02-13 03:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38][root][INFO] - Training Epoch: 2/2, step 601/7134 completed (loss: 0.04044584557414055, acc: 0.9861687421798706)
[2025-02-13 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38][root][INFO] - Training Epoch: 2/2, step 602/7134 completed (loss: 0.042732059955596924, acc: 0.9907192587852478)
[2025-02-13 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39][root][INFO] - Training Epoch: 2/2, step 603/7134 completed (loss: 0.03693193942308426, acc: 0.9884318709373474)
[2025-02-13 03:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39][root][INFO] - Training Epoch: 2/2, step 604/7134 completed (loss: 0.041293978691101074, acc: 0.985358715057373)
[2025-02-13 03:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39][root][INFO] - Training Epoch: 2/2, step 605/7134 completed (loss: 0.02264806069433689, acc: 0.9948186278343201)
[2025-02-13 03:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:40][root][INFO] - Training Epoch: 2/2, step 606/7134 completed (loss: 0.09797104448080063, acc: 0.9802225232124329)
[2025-02-13 03:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:40][root][INFO] - Training Epoch: 2/2, step 607/7134 completed (loss: 0.03211596980690956, acc: 0.9906542301177979)
[2025-02-13 03:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:41][root][INFO] - Training Epoch: 2/2, step 608/7134 completed (loss: 0.027997374534606934, acc: 0.9922279715538025)
[2025-02-13 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:41][root][INFO] - Training Epoch: 2/2, step 609/7134 completed (loss: 0.039140112698078156, acc: 0.9871194362640381)
[2025-02-13 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:42][root][INFO] - Training Epoch: 2/2, step 610/7134 completed (loss: 0.05469126254320145, acc: 0.9810366630554199)
[2025-02-13 03:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:42][root][INFO] - Training Epoch: 2/2, step 611/7134 completed (loss: 0.023274021223187447, acc: 0.9939024448394775)
[2025-02-13 03:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:43][root][INFO] - Training Epoch: 2/2, step 612/7134 completed (loss: 0.025831911712884903, acc: 0.9907651543617249)
[2025-02-13 03:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:43][root][INFO] - Training Epoch: 2/2, step 613/7134 completed (loss: 0.03359520062804222, acc: 0.9882766604423523)
[2025-02-13 03:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:43][root][INFO] - Training Epoch: 2/2, step 614/7134 completed (loss: 0.022413037717342377, acc: 0.9909399747848511)
[2025-02-13 03:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:44][root][INFO] - Training Epoch: 2/2, step 615/7134 completed (loss: 0.010045534931123257, acc: 0.9975460171699524)
[2025-02-13 03:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:44][root][INFO] - Training Epoch: 2/2, step 616/7134 completed (loss: 0.03043098747730255, acc: 0.9920454621315002)
[2025-02-13 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45][root][INFO] - Training Epoch: 2/2, step 617/7134 completed (loss: 0.03525456041097641, acc: 0.9897494316101074)
[2025-02-13 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45][root][INFO] - Training Epoch: 2/2, step 618/7134 completed (loss: 0.0239097960293293, acc: 0.9933155179023743)
[2025-02-13 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:46][root][INFO] - Training Epoch: 2/2, step 619/7134 completed (loss: 0.03780030459165573, acc: 0.9919028282165527)
[2025-02-13 03:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:46][root][INFO] - Training Epoch: 2/2, step 620/7134 completed (loss: 0.07163502275943756, acc: 0.987261176109314)
[2025-02-13 03:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47][root][INFO] - Training Epoch: 2/2, step 621/7134 completed (loss: 0.037278663367033005, acc: 0.9878234267234802)
[2025-02-13 03:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47][root][INFO] - Training Epoch: 2/2, step 622/7134 completed (loss: 0.04184810444712639, acc: 0.9885222315788269)
[2025-02-13 03:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47][root][INFO] - Training Epoch: 2/2, step 623/7134 completed (loss: 0.07227524369955063, acc: 0.9868804812431335)
[2025-02-13 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:48][root][INFO] - Training Epoch: 2/2, step 624/7134 completed (loss: 0.021243643015623093, acc: 0.9972972869873047)
[2025-02-13 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:48][root][INFO] - Training Epoch: 2/2, step 625/7134 completed (loss: 0.060684144496917725, acc: 0.9854881167411804)
[2025-02-13 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:49][root][INFO] - Training Epoch: 2/2, step 626/7134 completed (loss: 0.07770666480064392, acc: 0.9828178882598877)
[2025-02-13 03:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:49][root][INFO] - Training Epoch: 2/2, step 627/7134 completed (loss: 0.0340435728430748, acc: 0.9896755218505859)
[2025-02-13 03:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50][root][INFO] - Training Epoch: 2/2, step 628/7134 completed (loss: 0.03392983227968216, acc: 0.9916782379150391)
[2025-02-13 03:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50][root][INFO] - Training Epoch: 2/2, step 629/7134 completed (loss: 0.06335291266441345, acc: 0.9871244430541992)
[2025-02-13 03:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50][root][INFO] - Training Epoch: 2/2, step 630/7134 completed (loss: 0.03735021874308586, acc: 0.9818652868270874)
[2025-02-13 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:51][root][INFO] - Training Epoch: 2/2, step 631/7134 completed (loss: 0.030669212341308594, acc: 0.99210524559021)
[2025-02-13 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:51][root][INFO] - Training Epoch: 2/2, step 632/7134 completed (loss: 0.021042924374341965, acc: 0.9935275316238403)
[2025-02-13 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:52][root][INFO] - Training Epoch: 2/2, step 633/7134 completed (loss: 0.030764171853661537, acc: 0.9927431344985962)
[2025-02-13 03:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:52][root][INFO] - Training Epoch: 2/2, step 634/7134 completed (loss: 0.018213991075754166, acc: 0.9936708807945251)
[2025-02-13 03:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53][root][INFO] - Training Epoch: 2/2, step 635/7134 completed (loss: 0.02607508935034275, acc: 0.9923664331436157)
[2025-02-13 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53][root][INFO] - Training Epoch: 2/2, step 636/7134 completed (loss: 0.08340677618980408, acc: 0.9820689558982849)
[2025-02-13 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53][root][INFO] - Training Epoch: 2/2, step 637/7134 completed (loss: 0.03755141794681549, acc: 0.9888268113136292)
[2025-02-13 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:54][root][INFO] - Training Epoch: 2/2, step 638/7134 completed (loss: 0.05142725631594658, acc: 0.9875195026397705)
[2025-02-13 03:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:54][root][INFO] - Training Epoch: 2/2, step 639/7134 completed (loss: 0.018761882558465004, acc: 0.9933333396911621)
[2025-02-13 03:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55][root][INFO] - Training Epoch: 2/2, step 640/7134 completed (loss: 0.06118053197860718, acc: 0.9843478202819824)
[2025-02-13 03:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55][root][INFO] - Training Epoch: 2/2, step 641/7134 completed (loss: 0.050655853003263474, acc: 0.9869067072868347)
[2025-02-13 03:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55][root][INFO] - Training Epoch: 2/2, step 642/7134 completed (loss: 0.023849494755268097, acc: 0.9931972622871399)
[2025-02-13 03:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:56][root][INFO] - Training Epoch: 2/2, step 643/7134 completed (loss: 0.012971386313438416, acc: 0.9961488842964172)
[2025-02-13 03:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:56][root][INFO] - Training Epoch: 2/2, step 644/7134 completed (loss: 0.025404106825590134, acc: 0.9903475046157837)
[2025-02-13 03:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:57][root][INFO] - Training Epoch: 2/2, step 645/7134 completed (loss: 0.03186791390180588, acc: 0.9932659864425659)
[2025-02-13 03:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:57][root][INFO] - Training Epoch: 2/2, step 646/7134 completed (loss: 0.03620316460728645, acc: 0.9908257126808167)
[2025-02-13 03:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58][root][INFO] - Training Epoch: 2/2, step 647/7134 completed (loss: 0.006733396556228399, acc: 0.9983079433441162)
[2025-02-13 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58][root][INFO] - Training Epoch: 2/2, step 648/7134 completed (loss: 0.08008512109518051, acc: 0.9790576100349426)
[2025-02-13 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58][root][INFO] - Training Epoch: 2/2, step 649/7134 completed (loss: 0.02850140631198883, acc: 0.9897435903549194)
[2025-02-13 03:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:59][root][INFO] - Training Epoch: 2/2, step 650/7134 completed (loss: 0.060160644352436066, acc: 0.9827315807342529)
[2025-02-13 03:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:59][root][INFO] - Training Epoch: 2/2, step 651/7134 completed (loss: 0.1006908267736435, acc: 0.9740061163902283)
[2025-02-13 03:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:00][root][INFO] - Training Epoch: 2/2, step 652/7134 completed (loss: 0.02964860014617443, acc: 0.993127167224884)
[2025-02-13 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:00][root][INFO] - Training Epoch: 2/2, step 653/7134 completed (loss: 0.027910679578781128, acc: 0.9886524677276611)
[2025-02-13 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01][root][INFO] - Training Epoch: 2/2, step 654/7134 completed (loss: 0.10703890770673752, acc: 0.9697624444961548)
[2025-02-13 03:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01][root][INFO] - Training Epoch: 2/2, step 655/7134 completed (loss: 0.053209539502859116, acc: 0.9858871102333069)
[2025-02-13 03:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01][root][INFO] - Training Epoch: 2/2, step 656/7134 completed (loss: 0.0626126155257225, acc: 0.9841688871383667)
[2025-02-13 03:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:02][root][INFO] - Training Epoch: 2/2, step 657/7134 completed (loss: 0.04240786284208298, acc: 0.9887640476226807)
[2025-02-13 03:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:02][root][INFO] - Training Epoch: 2/2, step 658/7134 completed (loss: 0.0357138030230999, acc: 0.990777313709259)
[2025-02-13 03:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:03][root][INFO] - Training Epoch: 2/2, step 659/7134 completed (loss: 0.019342996180057526, acc: 0.992668628692627)
[2025-02-13 03:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:03][root][INFO] - Training Epoch: 2/2, step 660/7134 completed (loss: 0.0594274066388607, acc: 0.9853801131248474)
[2025-02-13 03:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:04][root][INFO] - Training Epoch: 2/2, step 661/7134 completed (loss: 0.09550163149833679, acc: 0.98221755027771)
[2025-02-13 03:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:04][root][INFO] - Training Epoch: 2/2, step 662/7134 completed (loss: 0.031526245176792145, acc: 0.9906166195869446)
[2025-02-13 03:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:05][root][INFO] - Training Epoch: 2/2, step 663/7134 completed (loss: 0.06265813857316971, acc: 0.9831649661064148)
[2025-02-13 03:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:05][root][INFO] - Training Epoch: 2/2, step 664/7134 completed (loss: 0.05194266140460968, acc: 0.9900000095367432)
[2025-02-13 03:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:06][root][INFO] - Training Epoch: 2/2, step 665/7134 completed (loss: 0.05616694688796997, acc: 0.9878869652748108)
[2025-02-13 03:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:06][root][INFO] - Training Epoch: 2/2, step 666/7134 completed (loss: 0.07521944493055344, acc: 0.9807692170143127)
[2025-02-13 03:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:06][root][INFO] - Training Epoch: 2/2, step 667/7134 completed (loss: 0.07000666856765747, acc: 0.9837996959686279)
[2025-02-13 03:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:07][root][INFO] - Training Epoch: 2/2, step 668/7134 completed (loss: 0.03527076169848442, acc: 0.9898605942726135)
[2025-02-13 03:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:07][root][INFO] - Training Epoch: 2/2, step 669/7134 completed (loss: 0.031746845692396164, acc: 0.9931787252426147)
[2025-02-13 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:08][root][INFO] - Training Epoch: 2/2, step 670/7134 completed (loss: 0.05155444145202637, acc: 0.9856262803077698)
[2025-02-13 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:08][root][INFO] - Training Epoch: 2/2, step 671/7134 completed (loss: 0.04764318838715553, acc: 0.9882100820541382)
[2025-02-13 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:09][root][INFO] - Training Epoch: 2/2, step 672/7134 completed (loss: 0.06853343546390533, acc: 0.9880775213241577)
[2025-02-13 03:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:09][root][INFO] - Training Epoch: 2/2, step 673/7134 completed (loss: 0.04731201380491257, acc: 0.9877150058746338)
[2025-02-13 03:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:10][root][INFO] - Training Epoch: 2/2, step 674/7134 completed (loss: 0.05473305284976959, acc: 0.9819004535675049)
[2025-02-13 03:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:10][root][INFO] - Training Epoch: 2/2, step 675/7134 completed (loss: 0.07133138179779053, acc: 0.9843546152114868)
[2025-02-13 03:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:11][root][INFO] - Training Epoch: 2/2, step 676/7134 completed (loss: 0.05259448289871216, acc: 0.9869822263717651)
[2025-02-13 03:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:11][root][INFO] - Training Epoch: 2/2, step 677/7134 completed (loss: 0.014513755217194557, acc: 0.9963008761405945)
[2025-02-13 03:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12][root][INFO] - Training Epoch: 2/2, step 678/7134 completed (loss: 0.032123811542987823, acc: 0.9892328381538391)
[2025-02-13 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12][root][INFO] - Training Epoch: 2/2, step 679/7134 completed (loss: 0.020660409703850746, acc: 0.9943289160728455)
[2025-02-13 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12][root][INFO] - Training Epoch: 2/2, step 680/7134 completed (loss: 0.07807028293609619, acc: 0.9770889282226562)
[2025-02-13 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:13][root][INFO] - Training Epoch: 2/2, step 681/7134 completed (loss: 0.046656377613544464, acc: 0.9865030646324158)
[2025-02-13 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:13][root][INFO] - Training Epoch: 2/2, step 682/7134 completed (loss: 0.03281966596841812, acc: 0.9884726405143738)
[2025-02-13 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:14][root][INFO] - Training Epoch: 2/2, step 683/7134 completed (loss: 0.050453707575798035, acc: 0.9835391044616699)
[2025-02-13 03:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:14][root][INFO] - Training Epoch: 2/2, step 684/7134 completed (loss: 0.029424654319882393, acc: 0.9916897416114807)
[2025-02-13 03:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:15][root][INFO] - Training Epoch: 2/2, step 685/7134 completed (loss: 0.024778762832283974, acc: 0.9937655925750732)
[2025-02-13 03:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:15][root][INFO] - Training Epoch: 2/2, step 686/7134 completed (loss: 0.017849020659923553, acc: 0.9933949708938599)
[2025-02-13 03:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16][root][INFO] - Training Epoch: 2/2, step 687/7134 completed (loss: 0.02127787470817566, acc: 0.9941973090171814)
[2025-02-13 03:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16][root][INFO] - Training Epoch: 2/2, step 688/7134 completed (loss: 0.013206671923398972, acc: 0.9970414042472839)
[2025-02-13 03:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16][root][INFO] - Training Epoch: 2/2, step 689/7134 completed (loss: 0.02529001608490944, acc: 0.9931623935699463)
[2025-02-13 03:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:17][root][INFO] - Training Epoch: 2/2, step 690/7134 completed (loss: 0.016446903347969055, acc: 0.995230495929718)
[2025-02-13 03:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:17][root][INFO] - Training Epoch: 2/2, step 691/7134 completed (loss: 0.055446743965148926, acc: 0.9856321811676025)
[2025-02-13 03:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:18][root][INFO] - Training Epoch: 2/2, step 692/7134 completed (loss: 0.06154501438140869, acc: 0.9853333234786987)
[2025-02-13 03:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:18][root][INFO] - Training Epoch: 2/2, step 693/7134 completed (loss: 0.04037553071975708, acc: 0.9838449358940125)
[2025-02-13 03:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19][root][INFO] - Training Epoch: 2/2, step 694/7134 completed (loss: 0.061316125094890594, acc: 0.9868612885475159)
[2025-02-13 03:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19][root][INFO] - Training Epoch: 2/2, step 695/7134 completed (loss: 0.08140284568071365, acc: 0.9810671210289001)
[2025-02-13 03:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19][root][INFO] - Training Epoch: 2/2, step 696/7134 completed (loss: 0.07438436150550842, acc: 0.9805825352668762)
[2025-02-13 03:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:20][root][INFO] - Training Epoch: 2/2, step 697/7134 completed (loss: 0.033908210694789886, acc: 0.9925558567047119)
[2025-02-13 03:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:20][root][INFO] - Training Epoch: 2/2, step 698/7134 completed (loss: 0.01390839833766222, acc: 0.9967426657676697)
[2025-02-13 03:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:21][root][INFO] - Training Epoch: 2/2, step 699/7134 completed (loss: 0.02358333207666874, acc: 0.9906542301177979)
[2025-02-13 03:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:21][root][INFO] - Training Epoch: 2/2, step 700/7134 completed (loss: 0.0492129884660244, acc: 0.9906716346740723)
[2025-02-13 03:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22][root][INFO] - Training Epoch: 2/2, step 701/7134 completed (loss: 0.0386168509721756, acc: 0.9840425252914429)
[2025-02-13 03:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22][root][INFO] - Training Epoch: 2/2, step 702/7134 completed (loss: 0.02860119752585888, acc: 0.9918830990791321)
[2025-02-13 03:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22][root][INFO] - Training Epoch: 2/2, step 703/7134 completed (loss: 0.026835525408387184, acc: 0.9903069734573364)
[2025-02-13 03:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:23][root][INFO] - Training Epoch: 2/2, step 704/7134 completed (loss: 0.03353692963719368, acc: 0.9878419637680054)
[2025-02-13 03:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:23][root][INFO] - Training Epoch: 2/2, step 705/7134 completed (loss: 0.05541061982512474, acc: 0.9798882603645325)
[2025-02-13 03:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:24][root][INFO] - Training Epoch: 2/2, step 706/7134 completed (loss: 0.020273515954613686, acc: 0.9934354424476624)
[2025-02-13 03:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:24][root][INFO] - Training Epoch: 2/2, step 707/7134 completed (loss: 0.03201720118522644, acc: 0.9880095720291138)
[2025-02-13 03:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:25][root][INFO] - Training Epoch: 2/2, step 708/7134 completed (loss: 0.024290839210152626, acc: 0.9913259148597717)
[2025-02-13 03:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:25][root][INFO] - Training Epoch: 2/2, step 709/7134 completed (loss: 0.026339182630181313, acc: 0.9940333962440491)
[2025-02-13 03:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26][root][INFO] - Training Epoch: 2/2, step 710/7134 completed (loss: 0.021815361455082893, acc: 0.9939467310905457)
[2025-02-13 03:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26][root][INFO] - Training Epoch: 2/2, step 711/7134 completed (loss: 0.03458483889698982, acc: 0.990554928779602)
[2025-02-13 03:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26][root][INFO] - Training Epoch: 2/2, step 712/7134 completed (loss: 0.02325942926108837, acc: 0.9926739931106567)
[2025-02-13 03:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:27][root][INFO] - Training Epoch: 2/2, step 713/7134 completed (loss: 0.014729401096701622, acc: 0.9965397715568542)
[2025-02-13 03:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:27][root][INFO] - Training Epoch: 2/2, step 714/7134 completed (loss: 0.03017454408109188, acc: 0.990338146686554)
[2025-02-13 03:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:28][root][INFO] - Training Epoch: 2/2, step 715/7134 completed (loss: 0.022353487089276314, acc: 0.9900568127632141)
[2025-02-13 03:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:28][root][INFO] - Training Epoch: 2/2, step 716/7134 completed (loss: 0.013995620422065258, acc: 0.9954493641853333)
[2025-02-13 03:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29][root][INFO] - Training Epoch: 2/2, step 717/7134 completed (loss: 0.04625925049185753, acc: 0.9962073564529419)
[2025-02-13 03:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29][root][INFO] - Training Epoch: 2/2, step 718/7134 completed (loss: 0.013776648789644241, acc: 0.9962406158447266)
[2025-02-13 03:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29][root][INFO] - Training Epoch: 2/2, step 719/7134 completed (loss: 0.011903186328709126, acc: 0.998275876045227)
[2025-02-13 03:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:30][root][INFO] - Training Epoch: 2/2, step 720/7134 completed (loss: 0.020047627389431, acc: 0.9962406158447266)
[2025-02-13 03:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:30][root][INFO] - Training Epoch: 2/2, step 721/7134 completed (loss: 0.030125848948955536, acc: 0.98975670337677)
[2025-02-13 03:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31][root][INFO] - Training Epoch: 2/2, step 722/7134 completed (loss: 0.01847808063030243, acc: 0.9929988384246826)
[2025-02-13 03:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31][root][INFO] - Training Epoch: 2/2, step 723/7134 completed (loss: 0.013428638689219952, acc: 0.9963680505752563)
[2025-02-13 03:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:32][root][INFO] - Training Epoch: 2/2, step 724/7134 completed (loss: 0.02610923908650875, acc: 0.9942775368690491)
[2025-02-13 03:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:32][root][INFO] - Training Epoch: 2/2, step 725/7134 completed (loss: 0.01390074286609888, acc: 0.9942113161087036)
[2025-02-13 03:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:33][root][INFO] - Training Epoch: 2/2, step 726/7134 completed (loss: 0.02082790806889534, acc: 0.991631805896759)
[2025-02-13 03:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:33][root][INFO] - Training Epoch: 2/2, step 727/7134 completed (loss: 0.00737247709184885, acc: 0.9971140027046204)
[2025-02-13 03:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34][root][INFO] - Training Epoch: 2/2, step 728/7134 completed (loss: 0.019132327288389206, acc: 0.9931129217147827)
[2025-02-13 03:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34][root][INFO] - Training Epoch: 2/2, step 729/7134 completed (loss: 0.011675924994051456, acc: 0.9944367408752441)
[2025-02-13 03:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34][root][INFO] - Training Epoch: 2/2, step 730/7134 completed (loss: 0.024080727249383926, acc: 0.993514895439148)
[2025-02-13 03:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:35][root][INFO] - Training Epoch: 2/2, step 731/7134 completed (loss: 0.02100907452404499, acc: 0.9946666955947876)
[2025-02-13 03:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:35][root][INFO] - Training Epoch: 2/2, step 732/7134 completed (loss: 0.05314519256353378, acc: 0.9792453050613403)
[2025-02-13 03:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36][root][INFO] - Training Epoch: 2/2, step 733/7134 completed (loss: 0.03991156816482544, acc: 0.980461835861206)
[2025-02-13 03:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36][root][INFO] - Training Epoch: 2/2, step 734/7134 completed (loss: 0.0198830496519804, acc: 0.9923195242881775)
[2025-02-13 03:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36][root][INFO] - Training Epoch: 2/2, step 735/7134 completed (loss: 0.018067331984639168, acc: 0.9903537034988403)
[2025-02-13 03:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:37][root][INFO] - Training Epoch: 2/2, step 736/7134 completed (loss: 0.044290587306022644, acc: 0.9824175834655762)
[2025-02-13 03:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:37][root][INFO] - Training Epoch: 2/2, step 737/7134 completed (loss: 0.11414705961942673, acc: 0.9776875972747803)
[2025-02-13 03:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:38][root][INFO] - Training Epoch: 2/2, step 738/7134 completed (loss: 0.010342366993427277, acc: 0.9961538314819336)
[2025-02-13 03:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:38][root][INFO] - Training Epoch: 2/2, step 739/7134 completed (loss: 0.017923176288604736, acc: 0.9963302612304688)
[2025-02-13 03:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39][root][INFO] - Training Epoch: 2/2, step 740/7134 completed (loss: 0.006322748959064484, acc: 0.9984126687049866)
[2025-02-13 03:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39][root][INFO] - Training Epoch: 2/2, step 741/7134 completed (loss: 0.029354805126786232, acc: 0.9905362725257874)
[2025-02-13 03:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39][root][INFO] - Training Epoch: 2/2, step 742/7134 completed (loss: 0.0389217771589756, acc: 0.9874213933944702)
[2025-02-13 03:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:40][root][INFO] - Training Epoch: 2/2, step 743/7134 completed (loss: 0.04252813756465912, acc: 0.9864457845687866)
[2025-02-13 03:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:40][root][INFO] - Training Epoch: 2/2, step 744/7134 completed (loss: 0.02427351288497448, acc: 0.9920634627342224)
[2025-02-13 03:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41][root][INFO] - Training Epoch: 2/2, step 745/7134 completed (loss: 0.03175194188952446, acc: 0.9939576983451843)
[2025-02-13 03:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41][root][INFO] - Training Epoch: 2/2, step 746/7134 completed (loss: 0.04506206512451172, acc: 0.9901477694511414)
[2025-02-13 03:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41][root][INFO] - Training Epoch: 2/2, step 747/7134 completed (loss: 0.1131022647023201, acc: 0.97508305311203)
[2025-02-13 03:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:42][root][INFO] - Training Epoch: 2/2, step 748/7134 completed (loss: 0.024419687688350677, acc: 0.9924812316894531)
[2025-02-13 03:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:42][root][INFO] - Training Epoch: 2/2, step 749/7134 completed (loss: 0.018428748473525047, acc: 0.9956772327423096)
[2025-02-13 03:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:43][root][INFO] - Training Epoch: 2/2, step 750/7134 completed (loss: 0.025761665776371956, acc: 0.9945945739746094)
[2025-02-13 03:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:43][root][INFO] - Training Epoch: 2/2, step 751/7134 completed (loss: 0.06194089725613594, acc: 0.9829642176628113)
[2025-02-13 03:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44][root][INFO] - Training Epoch: 2/2, step 752/7134 completed (loss: 0.08401037007570267, acc: 0.9849849939346313)
[2025-02-13 03:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44][root][INFO] - Training Epoch: 2/2, step 753/7134 completed (loss: 0.0430217944085598, acc: 0.9881756901741028)
[2025-02-13 03:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44][root][INFO] - Training Epoch: 2/2, step 754/7134 completed (loss: 0.014192501083016396, acc: 0.9969834089279175)
[2025-02-13 03:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:45][root][INFO] - Training Epoch: 2/2, step 755/7134 completed (loss: 0.08758829534053802, acc: 0.9768977165222168)
[2025-02-13 03:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:45][root][INFO] - Training Epoch: 2/2, step 756/7134 completed (loss: 0.034417226910591125, acc: 0.9906542301177979)
[2025-02-13 03:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46][root][INFO] - Training Epoch: 2/2, step 757/7134 completed (loss: 0.021448709070682526, acc: 0.9961685538291931)
[2025-02-13 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46][root][INFO] - Training Epoch: 2/2, step 758/7134 completed (loss: 0.042333438992500305, acc: 0.9913941621780396)
[2025-02-13 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46][root][INFO] - Training Epoch: 2/2, step 759/7134 completed (loss: 0.03157778084278107, acc: 0.9864341020584106)
[2025-02-13 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:47][root][INFO] - Training Epoch: 2/2, step 760/7134 completed (loss: 0.013293695636093616, acc: 0.9965338110923767)
[2025-02-13 03:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:47][root][INFO] - Training Epoch: 2/2, step 761/7134 completed (loss: 0.03215266391634941, acc: 0.9906103014945984)
[2025-02-13 03:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48][root][INFO] - Training Epoch: 2/2, step 762/7134 completed (loss: 0.01942857727408409, acc: 0.9928315281867981)
[2025-02-13 03:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48][root][INFO] - Training Epoch: 2/2, step 763/7134 completed (loss: 0.055797677487134933, acc: 0.9820846915245056)
[2025-02-13 03:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48][root][INFO] - Training Epoch: 2/2, step 764/7134 completed (loss: 0.02582201361656189, acc: 0.991391658782959)
[2025-02-13 03:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:49][root][INFO] - Training Epoch: 2/2, step 765/7134 completed (loss: 0.015053686685860157, acc: 0.9956076145172119)
[2025-02-13 03:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:49][root][INFO] - Training Epoch: 2/2, step 766/7134 completed (loss: 0.012789744883775711, acc: 0.997032642364502)
[2025-02-13 03:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:50][root][INFO] - Training Epoch: 2/2, step 767/7134 completed (loss: 0.02037067525088787, acc: 0.9970717430114746)
[2025-02-13 03:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:50][root][INFO] - Training Epoch: 2/2, step 768/7134 completed (loss: 0.03917010873556137, acc: 0.9915013909339905)
[2025-02-13 03:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51][root][INFO] - Training Epoch: 2/2, step 769/7134 completed (loss: 0.0126083604991436, acc: 0.9971346855163574)
[2025-02-13 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51][root][INFO] - Training Epoch: 2/2, step 770/7134 completed (loss: 0.028219208121299744, acc: 0.9892802238464355)
[2025-02-13 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51][root][INFO] - Training Epoch: 2/2, step 771/7134 completed (loss: 0.02289782650768757, acc: 0.9920791983604431)
[2025-02-13 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:52][root][INFO] - Training Epoch: 2/2, step 772/7134 completed (loss: 0.03531237319111824, acc: 0.9882746934890747)
[2025-02-13 03:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:52][root][INFO] - Training Epoch: 2/2, step 773/7134 completed (loss: 0.036117780953645706, acc: 0.9910025596618652)
[2025-02-13 03:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:53][root][INFO] - Training Epoch: 2/2, step 774/7134 completed (loss: 0.014076589606702328, acc: 0.9950166344642639)
[2025-02-13 03:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:53][root][INFO] - Training Epoch: 2/2, step 775/7134 completed (loss: 0.019348643720149994, acc: 0.9946737885475159)
[2025-02-13 03:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54][root][INFO] - Training Epoch: 2/2, step 776/7134 completed (loss: 0.028345772996544838, acc: 0.9926035404205322)
[2025-02-13 03:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54][root][INFO] - Training Epoch: 2/2, step 777/7134 completed (loss: 0.02123047597706318, acc: 0.9935794472694397)
[2025-02-13 03:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54][root][INFO] - Training Epoch: 2/2, step 778/7134 completed (loss: 0.04017772898077965, acc: 0.9935897588729858)
[2025-02-13 03:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:55][root][INFO] - Training Epoch: 2/2, step 779/7134 completed (loss: 0.041339095681905746, acc: 0.9868612885475159)
[2025-02-13 03:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:55][root][INFO] - Training Epoch: 2/2, step 780/7134 completed (loss: 0.039479173719882965, acc: 0.9923954606056213)
[2025-02-13 03:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:56][root][INFO] - Training Epoch: 2/2, step 781/7134 completed (loss: 0.03865040838718414, acc: 0.9890109896659851)
[2025-02-13 03:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:56][root][INFO] - Training Epoch: 2/2, step 782/7134 completed (loss: 0.032692648470401764, acc: 0.9889240264892578)
[2025-02-13 03:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57][root][INFO] - Training Epoch: 2/2, step 783/7134 completed (loss: 0.030274344608187675, acc: 0.9910827875137329)
[2025-02-13 03:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57][root][INFO] - Training Epoch: 2/2, step 784/7134 completed (loss: 0.05087855085730553, acc: 0.986601710319519)
[2025-02-13 03:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57][root][INFO] - Training Epoch: 2/2, step 785/7134 completed (loss: 0.037300631403923035, acc: 0.9866220951080322)
[2025-02-13 03:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:58][root][INFO] - Training Epoch: 2/2, step 786/7134 completed (loss: 0.03962535038590431, acc: 0.9907063245773315)
[2025-02-13 03:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:58][root][INFO] - Training Epoch: 2/2, step 787/7134 completed (loss: 0.005842133890837431, acc: 1.0)
[2025-02-13 03:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59][root][INFO] - Training Epoch: 2/2, step 788/7134 completed (loss: 0.05867435783147812, acc: 0.98531574010849)
[2025-02-13 03:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59][root][INFO] - Training Epoch: 2/2, step 789/7134 completed (loss: 0.03213941678404808, acc: 0.989313006401062)
[2025-02-13 03:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59][root][INFO] - Training Epoch: 2/2, step 790/7134 completed (loss: 0.06420356780290604, acc: 0.9852070808410645)
[2025-02-13 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:00][root][INFO] - Training Epoch: 2/2, step 791/7134 completed (loss: 0.059529103338718414, acc: 0.9861660003662109)
[2025-02-13 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:00][root][INFO] - Training Epoch: 2/2, step 792/7134 completed (loss: 0.043533679097890854, acc: 0.989180862903595)
[2025-02-13 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01][root][INFO] - Training Epoch: 2/2, step 793/7134 completed (loss: 0.016633938997983932, acc: 0.9940828680992126)
[2025-02-13 03:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01][root][INFO] - Training Epoch: 2/2, step 794/7134 completed (loss: 0.032430898398160934, acc: 0.9860140085220337)
[2025-02-13 03:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01][root][INFO] - Training Epoch: 2/2, step 795/7134 completed (loss: 0.06920687854290009, acc: 0.9813084006309509)
[2025-02-13 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:02][root][INFO] - Training Epoch: 2/2, step 796/7134 completed (loss: 0.06306169182062149, acc: 0.9772727489471436)
[2025-02-13 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:02][root][INFO] - Training Epoch: 2/2, step 797/7134 completed (loss: 0.04133208096027374, acc: 0.9839357137680054)
[2025-02-13 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03][root][INFO] - Training Epoch: 2/2, step 798/7134 completed (loss: 0.02340603806078434, acc: 0.9920634627342224)
[2025-02-13 03:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03][root][INFO] - Training Epoch: 2/2, step 799/7134 completed (loss: 0.03302498161792755, acc: 0.9870967864990234)
[2025-02-13 03:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03][root][INFO] - Training Epoch: 2/2, step 800/7134 completed (loss: 0.05730534344911575, acc: 0.9869646430015564)
[2025-02-13 03:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:04][root][INFO] - Training Epoch: 2/2, step 801/7134 completed (loss: 0.05329205468297005, acc: 0.9855999946594238)
[2025-02-13 03:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:04][root][INFO] - Training Epoch: 2/2, step 802/7134 completed (loss: 0.05227931588888168, acc: 0.9855072498321533)
[2025-02-13 03:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05][root][INFO] - Training Epoch: 2/2, step 803/7134 completed (loss: 0.022109797224402428, acc: 0.9928571581840515)
[2025-02-13 03:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05][root][INFO] - Training Epoch: 2/2, step 804/7134 completed (loss: 0.02718810923397541, acc: 0.9979633688926697)
[2025-02-13 03:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05][root][INFO] - Training Epoch: 2/2, step 805/7134 completed (loss: 0.04523638263344765, acc: 0.9756554365158081)
[2025-02-13 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:06][root][INFO] - Training Epoch: 2/2, step 806/7134 completed (loss: 0.015580526553094387, acc: 0.9972066879272461)
[2025-02-13 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:06][root][INFO] - Training Epoch: 2/2, step 807/7134 completed (loss: 0.03747699409723282, acc: 0.9902676343917847)
[2025-02-13 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:07][root][INFO] - Training Epoch: 2/2, step 808/7134 completed (loss: 0.016731232404708862, acc: 0.9976047873497009)
[2025-02-13 03:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:07][root][INFO] - Training Epoch: 2/2, step 809/7134 completed (loss: 0.014426372945308685, acc: 0.9970282316207886)
[2025-02-13 03:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08][root][INFO] - Training Epoch: 2/2, step 810/7134 completed (loss: 0.03517621010541916, acc: 0.9905533194541931)
[2025-02-13 03:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08][root][INFO] - Training Epoch: 2/2, step 811/7134 completed (loss: 0.08234729617834091, acc: 0.982425332069397)
[2025-02-13 03:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08][root][INFO] - Training Epoch: 2/2, step 812/7134 completed (loss: 0.0270827729254961, acc: 0.9924337863922119)
[2025-02-13 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:09][root][INFO] - Training Epoch: 2/2, step 813/7134 completed (loss: 0.02532220631837845, acc: 0.9927431344985962)
[2025-02-13 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:09][root][INFO] - Training Epoch: 2/2, step 814/7134 completed (loss: 0.04156599938869476, acc: 0.9896774291992188)
[2025-02-13 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:10][root][INFO] - Training Epoch: 2/2, step 815/7134 completed (loss: 0.04386665299534798, acc: 0.984544038772583)
[2025-02-13 03:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:10][root][INFO] - Training Epoch: 2/2, step 816/7134 completed (loss: 0.05091056227684021, acc: 0.9779661297798157)
[2025-02-13 03:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11][root][INFO] - Training Epoch: 2/2, step 817/7134 completed (loss: 0.07621719688177109, acc: 0.9780621528625488)
[2025-02-13 03:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11][root][INFO] - Training Epoch: 2/2, step 818/7134 completed (loss: 0.05391952022910118, acc: 0.975944995880127)
[2025-02-13 03:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11][root][INFO] - Training Epoch: 2/2, step 819/7134 completed (loss: 0.05003030225634575, acc: 0.9904000163078308)
[2025-02-13 03:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:12][root][INFO] - Training Epoch: 2/2, step 820/7134 completed (loss: 0.04243452101945877, acc: 0.9907264113426208)
[2025-02-13 03:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:12][root][INFO] - Training Epoch: 2/2, step 821/7134 completed (loss: 0.026200678199529648, acc: 0.9959785342216492)
[2025-02-13 03:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:13][root][INFO] - Training Epoch: 2/2, step 822/7134 completed (loss: 0.03172895684838295, acc: 0.9905808568000793)
[2025-02-13 03:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:13][root][INFO] - Training Epoch: 2/2, step 823/7134 completed (loss: 0.012766114436089993, acc: 0.9943714737892151)
[2025-02-13 03:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:14][root][INFO] - Training Epoch: 2/2, step 824/7134 completed (loss: 0.06823554635047913, acc: 0.9770318269729614)
[2025-02-13 03:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:14][root][INFO] - Training Epoch: 2/2, step 825/7134 completed (loss: 0.033383794128894806, acc: 0.990777313709259)
[2025-02-13 03:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15][root][INFO] - Training Epoch: 2/2, step 826/7134 completed (loss: 0.02102658525109291, acc: 0.9918256402015686)
[2025-02-13 03:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15][root][INFO] - Training Epoch: 2/2, step 827/7134 completed (loss: 0.03563141077756882, acc: 0.9895012974739075)
[2025-02-13 03:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15][root][INFO] - Training Epoch: 2/2, step 828/7134 completed (loss: 0.03839730843901634, acc: 0.9906687140464783)
[2025-02-13 03:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:16][root][INFO] - Training Epoch: 2/2, step 829/7134 completed (loss: 0.03009582869708538, acc: 0.9927184581756592)
[2025-02-13 03:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:16][root][INFO] - Training Epoch: 2/2, step 830/7134 completed (loss: 0.02516915835440159, acc: 0.9900332093238831)
[2025-02-13 03:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:17][root][INFO] - Training Epoch: 2/2, step 831/7134 completed (loss: 0.010909264907240868, acc: 0.9972602725028992)
[2025-02-13 03:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:17][root][INFO] - Training Epoch: 2/2, step 832/7134 completed (loss: 0.07936134934425354, acc: 0.9812286496162415)
[2025-02-13 03:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18][root][INFO] - Training Epoch: 2/2, step 833/7134 completed (loss: 0.027910016477108, acc: 0.9901960492134094)
[2025-02-13 03:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18][root][INFO] - Training Epoch: 2/2, step 834/7134 completed (loss: 0.02208678424358368, acc: 0.9930070042610168)
[2025-02-13 03:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18][root][INFO] - Training Epoch: 2/2, step 835/7134 completed (loss: 0.038994308561086655, acc: 0.9902098178863525)
[2025-02-13 03:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:19][root][INFO] - Training Epoch: 2/2, step 836/7134 completed (loss: 0.025384197011590004, acc: 0.9907192587852478)
[2025-02-13 03:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:19][root][INFO] - Training Epoch: 2/2, step 837/7134 completed (loss: 0.0333118736743927, acc: 0.9918256402015686)
[2025-02-13 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:20][root][INFO] - Training Epoch: 2/2, step 838/7134 completed (loss: 0.02019497938454151, acc: 0.995230495929718)
[2025-02-13 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:20][root][INFO] - Training Epoch: 2/2, step 839/7134 completed (loss: 0.01963002234697342, acc: 0.9904305934906006)
[2025-02-13 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:21][root][INFO] - Training Epoch: 2/2, step 840/7134 completed (loss: 0.017280234023928642, acc: 0.9972106218338013)
[2025-02-13 03:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:21][root][INFO] - Training Epoch: 2/2, step 841/7134 completed (loss: 0.036530882120132446, acc: 0.9875518679618835)
[2025-02-13 03:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22][root][INFO] - Training Epoch: 2/2, step 842/7134 completed (loss: 0.026858652010560036, acc: 0.9919137358665466)
[2025-02-13 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22][root][INFO] - Training Epoch: 2/2, step 843/7134 completed (loss: 0.028064366430044174, acc: 0.9930459260940552)
[2025-02-13 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22][root][INFO] - Training Epoch: 2/2, step 844/7134 completed (loss: 0.026732617989182472, acc: 0.9873617887496948)
[2025-02-13 03:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:23][root][INFO] - Training Epoch: 2/2, step 845/7134 completed (loss: 0.035616423934698105, acc: 0.9876106381416321)
[2025-02-13 03:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:23][root][INFO] - Training Epoch: 2/2, step 846/7134 completed (loss: 0.0260205939412117, acc: 0.9910979270935059)
[2025-02-13 03:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:24][root][INFO] - Training Epoch: 2/2, step 847/7134 completed (loss: 0.00851873867213726, acc: 1.0)
[2025-02-13 03:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:24][root][INFO] - Training Epoch: 2/2, step 848/7134 completed (loss: 0.036234356462955475, acc: 0.990111231803894)
[2025-02-13 03:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:25][root][INFO] - Training Epoch: 2/2, step 849/7134 completed (loss: 0.018982641398906708, acc: 0.9927448630332947)
[2025-02-13 03:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:25][root][INFO] - Training Epoch: 2/2, step 850/7134 completed (loss: 0.01601550541818142, acc: 0.9932885766029358)
[2025-02-13 03:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26][root][INFO] - Training Epoch: 2/2, step 851/7134 completed (loss: 0.03415626287460327, acc: 0.9894875288009644)
[2025-02-13 03:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26][root][INFO] - Training Epoch: 2/2, step 852/7134 completed (loss: 0.020200662314891815, acc: 0.9938949942588806)
[2025-02-13 03:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26][root][INFO] - Training Epoch: 2/2, step 853/7134 completed (loss: 0.014579549431800842, acc: 0.9955947399139404)
[2025-02-13 03:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27][root][INFO] - Training Epoch: 2/2, step 854/7134 completed (loss: 0.03230850026011467, acc: 0.9925373196601868)
[2025-02-13 03:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27][root][INFO] - Training Epoch: 2/2, step 855/7134 completed (loss: 0.03083362616598606, acc: 0.9941452145576477)
[2025-02-13 03:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:28][root][INFO] - Training Epoch: 2/2, step 856/7134 completed (loss: 0.017432693392038345, acc: 0.9939467310905457)
[2025-02-13 03:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:28][root][INFO] - Training Epoch: 2/2, step 857/7134 completed (loss: 0.03125249966979027, acc: 0.9911894202232361)
[2025-02-13 03:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:29][root][INFO] - Training Epoch: 2/2, step 858/7134 completed (loss: 0.04435412958264351, acc: 0.9863184094429016)
[2025-02-13 03:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:29][root][INFO] - Training Epoch: 2/2, step 859/7134 completed (loss: 0.04825166240334511, acc: 0.9845971465110779)
[2025-02-13 03:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30][root][INFO] - Training Epoch: 2/2, step 860/7134 completed (loss: 0.03703542426228523, acc: 0.9898862242698669)
[2025-02-13 03:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30][root][INFO] - Training Epoch: 2/2, step 861/7134 completed (loss: 0.07632147520780563, acc: 0.9794149398803711)
[2025-02-13 03:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30][root][INFO] - Training Epoch: 2/2, step 862/7134 completed (loss: 0.03159362077713013, acc: 0.9915764331817627)
[2025-02-13 03:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:31][root][INFO] - Training Epoch: 2/2, step 863/7134 completed (loss: 0.025369392707943916, acc: 0.99068683385849)
[2025-02-13 03:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:31][root][INFO] - Training Epoch: 2/2, step 864/7134 completed (loss: 0.07349648326635361, acc: 0.9805309772491455)
[2025-02-13 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:32][root][INFO] - Training Epoch: 2/2, step 865/7134 completed (loss: 0.0495164580643177, acc: 0.9805352687835693)
[2025-02-13 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:32][root][INFO] - Training Epoch: 2/2, step 866/7134 completed (loss: 0.041450370103120804, acc: 0.9851852059364319)
[2025-02-13 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:33][root][INFO] - Training Epoch: 2/2, step 867/7134 completed (loss: 0.06352724879980087, acc: 0.9774718284606934)
[2025-02-13 03:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:33][root][INFO] - Training Epoch: 2/2, step 868/7134 completed (loss: 0.06627209484577179, acc: 0.985602080821991)
[2025-02-13 03:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34][root][INFO] - Training Epoch: 2/2, step 869/7134 completed (loss: 0.05234100669622421, acc: 0.977707028388977)
[2025-02-13 03:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34][root][INFO] - Training Epoch: 2/2, step 870/7134 completed (loss: 0.02855795808136463, acc: 0.9878706336021423)
[2025-02-13 03:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34][root][INFO] - Training Epoch: 2/2, step 871/7134 completed (loss: 0.0567096509039402, acc: 0.9885993599891663)
[2025-02-13 03:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:35][root][INFO] - Training Epoch: 2/2, step 872/7134 completed (loss: 0.051138196140527725, acc: 0.9874411225318909)
[2025-02-13 03:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:35][root][INFO] - Training Epoch: 2/2, step 873/7134 completed (loss: 0.05368250608444214, acc: 0.9873239398002625)
[2025-02-13 03:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:36][root][INFO] - Training Epoch: 2/2, step 874/7134 completed (loss: 0.024529188871383667, acc: 0.9922958612442017)
[2025-02-13 03:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:36][root][INFO] - Training Epoch: 2/2, step 875/7134 completed (loss: 0.029680032283067703, acc: 0.9897511005401611)
[2025-02-13 03:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37][root][INFO] - Training Epoch: 2/2, step 876/7134 completed (loss: 0.028153447434306145, acc: 0.9897660613059998)
[2025-02-13 03:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37][root][INFO] - Training Epoch: 2/2, step 877/7134 completed (loss: 0.0654841959476471, acc: 0.9841059446334839)
[2025-02-13 03:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37][root][INFO] - Training Epoch: 2/2, step 878/7134 completed (loss: 0.04658065736293793, acc: 0.982758641242981)
[2025-02-13 03:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:38][root][INFO] - Training Epoch: 2/2, step 879/7134 completed (loss: 0.07132898271083832, acc: 0.9854439496994019)
[2025-02-13 03:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:38][root][INFO] - Training Epoch: 2/2, step 880/7134 completed (loss: 0.011445454321801662, acc: 0.9970887899398804)
[2025-02-13 03:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39][root][INFO] - Training Epoch: 2/2, step 881/7134 completed (loss: 0.013988422229886055, acc: 0.9959920048713684)
[2025-02-13 03:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39][root][INFO] - Training Epoch: 2/2, step 882/7134 completed (loss: 0.02185714803636074, acc: 0.9928057789802551)
[2025-02-13 03:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39][root][INFO] - Training Epoch: 2/2, step 883/7134 completed (loss: 0.05501866713166237, acc: 0.9831804037094116)
[2025-02-13 03:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:40][root][INFO] - Training Epoch: 2/2, step 884/7134 completed (loss: 0.03145608305931091, acc: 0.9865951538085938)
[2025-02-13 03:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:40][root][INFO] - Training Epoch: 2/2, step 885/7134 completed (loss: 0.005731227807700634, acc: 1.0)
[2025-02-13 03:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:41][root][INFO] - Training Epoch: 2/2, step 886/7134 completed (loss: 0.06901172548532486, acc: 0.9844054579734802)
[2025-02-13 03:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:41][root][INFO] - Training Epoch: 2/2, step 887/7134 completed (loss: 0.03277965635061264, acc: 0.9946902394294739)
[2025-02-13 03:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42][root][INFO] - Training Epoch: 2/2, step 888/7134 completed (loss: 0.06196267157793045, acc: 0.9863429665565491)
[2025-02-13 03:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42][root][INFO] - Training Epoch: 2/2, step 889/7134 completed (loss: 0.016781147569417953, acc: 0.9933333396911621)
[2025-02-13 03:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42][root][INFO] - Training Epoch: 2/2, step 890/7134 completed (loss: 0.04889175668358803, acc: 0.9898132681846619)
[2025-02-13 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:43][root][INFO] - Training Epoch: 2/2, step 891/7134 completed (loss: 0.06303753703832626, acc: 0.9860917925834656)
[2025-02-13 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:43][root][INFO] - Training Epoch: 2/2, step 892/7134 completed (loss: 0.055811963975429535, acc: 0.9883720874786377)
[2025-02-13 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44][root][INFO] - Training Epoch: 2/2, step 893/7134 completed (loss: 0.04110446199774742, acc: 0.9891641139984131)
[2025-02-13 03:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44][root][INFO] - Training Epoch: 2/2, step 894/7134 completed (loss: 0.03759131580591202, acc: 0.9888268113136292)
[2025-02-13 03:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44][root][INFO] - Training Epoch: 2/2, step 895/7134 completed (loss: 0.016663148999214172, acc: 0.9931623935699463)
[2025-02-13 03:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:45][root][INFO] - Training Epoch: 2/2, step 896/7134 completed (loss: 0.0286689605563879, acc: 0.9940000176429749)
[2025-02-13 03:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:45][root][INFO] - Training Epoch: 2/2, step 897/7134 completed (loss: 0.0317748598754406, acc: 0.9902507066726685)
[2025-02-13 03:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:46][root][INFO] - Training Epoch: 2/2, step 898/7134 completed (loss: 0.012008204124867916, acc: 0.9987546801567078)
[2025-02-13 03:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:46][root][INFO] - Training Epoch: 2/2, step 899/7134 completed (loss: 0.02996193617582321, acc: 0.9937810897827148)
[2025-02-13 03:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47][root][INFO] - Training Epoch: 2/2, step 900/7134 completed (loss: 0.01806342974305153, acc: 0.9929676651954651)
[2025-02-13 03:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47][root][INFO] - Training Epoch: 2/2, step 901/7134 completed (loss: 0.020946867763996124, acc: 0.9946666955947876)
[2025-02-13 03:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47][root][INFO] - Training Epoch: 2/2, step 902/7134 completed (loss: 0.012692985124886036, acc: 0.9971305727958679)
[2025-02-13 03:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:48][root][INFO] - Training Epoch: 2/2, step 903/7134 completed (loss: 0.004018757957965136, acc: 1.0)
[2025-02-13 03:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:48][root][INFO] - Training Epoch: 2/2, step 904/7134 completed (loss: 0.014788302592933178, acc: 0.99609375)
[2025-02-13 03:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:49][root][INFO] - Training Epoch: 2/2, step 905/7134 completed (loss: 0.011875213123857975, acc: 0.9973045587539673)
[2025-02-13 03:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:49][root][INFO] - Training Epoch: 2/2, step 906/7134 completed (loss: 0.022348899394273758, acc: 0.9919571280479431)
[2025-02-13 03:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:50][root][INFO] - Training Epoch: 2/2, step 907/7134 completed (loss: 0.014210107736289501, acc: 0.9973683953285217)
[2025-02-13 03:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:50][root][INFO] - Training Epoch: 2/2, step 908/7134 completed (loss: 0.01647527515888214, acc: 0.9934980273246765)
[2025-02-13 03:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:51][root][INFO] - Training Epoch: 2/2, step 909/7134 completed (loss: 0.04064194858074188, acc: 0.9864864945411682)
[2025-02-13 03:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:51][root][INFO] - Training Epoch: 2/2, step 910/7134 completed (loss: 0.0313776433467865, acc: 0.9906542301177979)
[2025-02-13 03:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52][root][INFO] - Training Epoch: 2/2, step 911/7134 completed (loss: 0.012842066586017609, acc: 0.9958041906356812)
[2025-02-13 03:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52][root][INFO] - Training Epoch: 2/2, step 912/7134 completed (loss: 0.03550272434949875, acc: 0.9899117350578308)
[2025-02-13 03:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52][root][INFO] - Training Epoch: 2/2, step 913/7134 completed (loss: 0.017554273828864098, acc: 0.9937185645103455)
[2025-02-13 03:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53][root][INFO] - Training Epoch: 2/2, step 914/7134 completed (loss: 0.030385131016373634, acc: 0.9910714030265808)
[2025-02-13 03:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53][root][INFO] - Training Epoch: 2/2, step 915/7134 completed (loss: 0.06947382539510727, acc: 0.9870550036430359)
[2025-02-13 03:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54][root][INFO] - Training Epoch: 2/2, step 916/7134 completed (loss: 0.021709397435188293, acc: 0.9947643876075745)
[2025-02-13 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54][root][INFO] - Training Epoch: 2/2, step 917/7134 completed (loss: 0.046138517558574677, acc: 0.9864253401756287)
[2025-02-13 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54][root][INFO] - Training Epoch: 2/2, step 918/7134 completed (loss: 0.01671508327126503, acc: 0.9954057931900024)
[2025-02-13 03:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:55][root][INFO] - Training Epoch: 2/2, step 919/7134 completed (loss: 0.010542322881519794, acc: 0.9942280054092407)
[2025-02-13 03:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:55][root][INFO] - Training Epoch: 2/2, step 920/7134 completed (loss: 0.023962002247571945, acc: 0.9915966391563416)
[2025-02-13 03:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:56][root][INFO] - Training Epoch: 2/2, step 921/7134 completed (loss: 0.01041228137910366, acc: 0.9972899556159973)
[2025-02-13 03:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:56][root][INFO] - Training Epoch: 2/2, step 922/7134 completed (loss: 0.029823003336787224, acc: 0.9885246157646179)
[2025-02-13 03:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:57][root][INFO] - Training Epoch: 2/2, step 923/7134 completed (loss: 0.05836017057299614, acc: 0.9841269850730896)
[2025-02-13 03:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:57][root][INFO] - Training Epoch: 2/2, step 924/7134 completed (loss: 0.013484451919794083, acc: 0.9939613342285156)
[2025-02-13 03:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58][root][INFO] - Training Epoch: 2/2, step 925/7134 completed (loss: 0.029432397335767746, acc: 0.9911894202232361)
[2025-02-13 03:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58][root][INFO] - Training Epoch: 2/2, step 926/7134 completed (loss: 0.040311217308044434, acc: 0.9925093650817871)
[2025-02-13 03:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58][root][INFO] - Training Epoch: 2/2, step 927/7134 completed (loss: 0.057232312858104706, acc: 0.9934640526771545)
[2025-02-13 03:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:59][root][INFO] - Training Epoch: 2/2, step 928/7134 completed (loss: 0.03235439583659172, acc: 0.990138053894043)
[2025-02-13 03:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:59][root][INFO] - Training Epoch: 2/2, step 929/7134 completed (loss: 0.01688254624605179, acc: 0.9952606558799744)
[2025-02-13 03:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00][root][INFO] - Training Epoch: 2/2, step 930/7134 completed (loss: 0.04337277263402939, acc: 0.9858871102333069)
[2025-02-13 03:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00][root][INFO] - Training Epoch: 2/2, step 931/7134 completed (loss: 0.04948988929390907, acc: 0.9876543283462524)
[2025-02-13 03:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00][root][INFO] - Training Epoch: 2/2, step 932/7134 completed (loss: 0.03735174611210823, acc: 0.9885386824607849)
[2025-02-13 03:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:01][root][INFO] - Training Epoch: 2/2, step 933/7134 completed (loss: 0.04165099561214447, acc: 0.9901269674301147)
[2025-02-13 03:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:01][root][INFO] - Training Epoch: 2/2, step 934/7134 completed (loss: 0.01872580125927925, acc: 0.9934554696083069)
[2025-02-13 03:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:02][root][INFO] - Training Epoch: 2/2, step 935/7134 completed (loss: 0.024705184623599052, acc: 0.989983320236206)
[2025-02-13 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:02][root][INFO] - Training Epoch: 2/2, step 936/7134 completed (loss: 0.029316242784261703, acc: 0.9929701089859009)
[2025-02-13 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03][root][INFO] - Training Epoch: 2/2, step 937/7134 completed (loss: 0.02491791546344757, acc: 0.9929577708244324)
[2025-02-13 03:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03][root][INFO] - Training Epoch: 2/2, step 938/7134 completed (loss: 0.027341414242982864, acc: 0.9885386824607849)
[2025-02-13 03:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03][root][INFO] - Training Epoch: 2/2, step 939/7134 completed (loss: 0.03532220795750618, acc: 0.9875389337539673)
[2025-02-13 03:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:04][root][INFO] - Training Epoch: 2/2, step 940/7134 completed (loss: 0.03595936670899391, acc: 0.9856938719749451)
[2025-02-13 03:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:04][root][INFO] - Training Epoch: 2/2, step 941/7134 completed (loss: 0.018295347690582275, acc: 0.9944827556610107)
[2025-02-13 03:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:05][root][INFO] - Training Epoch: 2/2, step 942/7134 completed (loss: 0.037271227687597275, acc: 0.994194507598877)
[2025-02-13 03:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:05][root][INFO] - Training Epoch: 2/2, step 943/7134 completed (loss: 0.019336778670549393, acc: 0.9939117431640625)
[2025-02-13 03:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06][root][INFO] - Training Epoch: 2/2, step 944/7134 completed (loss: 0.028818398714065552, acc: 0.9910045266151428)
[2025-02-13 03:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06][root][INFO] - Training Epoch: 2/2, step 945/7134 completed (loss: 0.04115032032132149, acc: 0.9944055676460266)
[2025-02-13 03:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06][root][INFO] - Training Epoch: 2/2, step 946/7134 completed (loss: 0.007037682458758354, acc: 0.9978586435317993)
[2025-02-13 03:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:07][root][INFO] - Training Epoch: 2/2, step 947/7134 completed (loss: 0.03053184226155281, acc: 0.9931153059005737)
[2025-02-13 03:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:07][root][INFO] - Training Epoch: 2/2, step 948/7134 completed (loss: 0.02780703641474247, acc: 0.9916550517082214)
[2025-02-13 03:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08][root][INFO] - Training Epoch: 2/2, step 949/7134 completed (loss: 0.02206527441740036, acc: 0.9929873943328857)
[2025-02-13 03:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08][root][INFO] - Training Epoch: 2/2, step 950/7134 completed (loss: 0.02268357202410698, acc: 0.9891975522041321)
[2025-02-13 03:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08][root][INFO] - Training Epoch: 2/2, step 951/7134 completed (loss: 0.01846453733742237, acc: 0.9947183132171631)
[2025-02-13 03:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:09][root][INFO] - Training Epoch: 2/2, step 952/7134 completed (loss: 0.04958983510732651, acc: 0.9850249290466309)
[2025-02-13 03:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:09][root][INFO] - Training Epoch: 2/2, step 953/7134 completed (loss: 0.05331635847687721, acc: 0.9852761030197144)
[2025-02-13 03:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:10][root][INFO] - Training Epoch: 2/2, step 954/7134 completed (loss: 0.04579336196184158, acc: 0.988252580165863)
[2025-02-13 03:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:10][root][INFO] - Training Epoch: 2/2, step 955/7134 completed (loss: 0.034613236784935, acc: 0.9906291961669922)
[2025-02-13 03:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:11][root][INFO] - Training Epoch: 2/2, step 956/7134 completed (loss: 0.018316609784960747, acc: 0.9939024448394775)
[2025-02-13 03:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:11][root][INFO] - Training Epoch: 2/2, step 957/7134 completed (loss: 0.019129682332277298, acc: 0.9920364022254944)
[2025-02-13 03:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12][root][INFO] - Training Epoch: 2/2, step 958/7134 completed (loss: 0.04020240157842636, acc: 0.9908088445663452)
[2025-02-13 03:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12][root][INFO] - Training Epoch: 2/2, step 959/7134 completed (loss: 0.07832034677267075, acc: 0.98128342628479)
[2025-02-13 03:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12][root][INFO] - Training Epoch: 2/2, step 960/7134 completed (loss: 0.053928472101688385, acc: 0.9792060256004333)
[2025-02-13 03:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:13][root][INFO] - Training Epoch: 2/2, step 961/7134 completed (loss: 0.04698353260755539, acc: 0.986522912979126)
[2025-02-13 03:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:13][root][INFO] - Training Epoch: 2/2, step 962/7134 completed (loss: 0.09922639280557632, acc: 0.973793089389801)
[2025-02-13 03:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:14][root][INFO] - Training Epoch: 2/2, step 963/7134 completed (loss: 0.050938647240400314, acc: 0.9832317233085632)
[2025-02-13 03:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:14][root][INFO] - Training Epoch: 2/2, step 964/7134 completed (loss: 0.11872564256191254, acc: 0.9685157537460327)
[2025-02-13 03:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15][root][INFO] - Training Epoch: 2/2, step 965/7134 completed (loss: 0.05641978234052658, acc: 0.9813486337661743)
[2025-02-13 03:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15][root][INFO] - Training Epoch: 2/2, step 966/7134 completed (loss: 0.040294963866472244, acc: 0.9901639223098755)
[2025-02-13 03:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15][root][INFO] - Training Epoch: 2/2, step 967/7134 completed (loss: 0.03546769171953201, acc: 0.9918144345283508)
[2025-02-13 03:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:16][root][INFO] - Training Epoch: 2/2, step 968/7134 completed (loss: 0.0971217155456543, acc: 0.9721670150756836)
[2025-02-13 03:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:16][root][INFO] - Training Epoch: 2/2, step 969/7134 completed (loss: 0.03464440256357193, acc: 0.9864253401756287)
[2025-02-13 03:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:17][root][INFO] - Training Epoch: 2/2, step 970/7134 completed (loss: 0.059473808854818344, acc: 0.9847328066825867)
[2025-02-13 03:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:17][root][INFO] - Training Epoch: 2/2, step 971/7134 completed (loss: 0.02824375219643116, acc: 0.9865030646324158)
[2025-02-13 03:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18][root][INFO] - Training Epoch: 2/2, step 972/7134 completed (loss: 0.046313732862472534, acc: 0.9875466823577881)
[2025-02-13 03:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18][root][INFO] - Training Epoch: 2/2, step 973/7134 completed (loss: 0.09837660938501358, acc: 0.971794843673706)
[2025-02-13 03:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18][root][INFO] - Training Epoch: 2/2, step 974/7134 completed (loss: 0.05411474034190178, acc: 0.9851767420768738)
[2025-02-13 03:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:19][root][INFO] - Training Epoch: 2/2, step 975/7134 completed (loss: 0.07111090421676636, acc: 0.9868420958518982)
[2025-02-13 03:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:19][root][INFO] - Training Epoch: 2/2, step 976/7134 completed (loss: 0.050187673419713974, acc: 0.9844236969947815)
[2025-02-13 03:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:20][root][INFO] - Training Epoch: 2/2, step 977/7134 completed (loss: 0.049691151827573776, acc: 0.9870129823684692)
[2025-02-13 03:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:20][root][INFO] - Training Epoch: 2/2, step 978/7134 completed (loss: 0.02698422037065029, acc: 0.9927797913551331)
[2025-02-13 03:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21][root][INFO] - Training Epoch: 2/2, step 979/7134 completed (loss: 0.03714434802532196, acc: 0.990641713142395)
[2025-02-13 03:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21][root][INFO] - Training Epoch: 2/2, step 980/7134 completed (loss: 0.06277070939540863, acc: 0.987500011920929)
[2025-02-13 03:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21][root][INFO] - Training Epoch: 2/2, step 981/7134 completed (loss: 0.10594689100980759, acc: 0.9823151230812073)
[2025-02-13 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:22][root][INFO] - Training Epoch: 2/2, step 982/7134 completed (loss: 0.03178911656141281, acc: 0.9859550595283508)
[2025-02-13 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:22][root][INFO] - Training Epoch: 2/2, step 983/7134 completed (loss: 0.055270470678806305, acc: 0.9880136847496033)
[2025-02-13 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:23][root][INFO] - Training Epoch: 2/2, step 984/7134 completed (loss: 0.04203382134437561, acc: 0.9908088445663452)
[2025-02-13 03:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:23][root][INFO] - Training Epoch: 2/2, step 985/7134 completed (loss: 0.035503000020980835, acc: 0.9897959232330322)
[2025-02-13 03:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24][root][INFO] - Training Epoch: 2/2, step 986/7134 completed (loss: 0.016129085794091225, acc: 0.9943342804908752)
[2025-02-13 03:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24][root][INFO] - Training Epoch: 2/2, step 987/7134 completed (loss: 0.02891525812447071, acc: 0.9919614195823669)
[2025-02-13 03:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24][root][INFO] - Training Epoch: 2/2, step 988/7134 completed (loss: 0.09455118328332901, acc: 0.9750346541404724)
[2025-02-13 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:25][root][INFO] - Training Epoch: 2/2, step 989/7134 completed (loss: 0.06547053158283234, acc: 0.977746844291687)
[2025-02-13 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:25][root][INFO] - Training Epoch: 2/2, step 990/7134 completed (loss: 0.0451645590364933, acc: 0.9873577952384949)
[2025-02-13 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:26][root][INFO] - Training Epoch: 2/2, step 991/7134 completed (loss: 0.05517531558871269, acc: 0.9796556830406189)
[2025-02-13 03:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:26][root][INFO] - Training Epoch: 2/2, step 992/7134 completed (loss: 0.04065493494272232, acc: 0.9828947186470032)
[2025-02-13 03:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:27][root][INFO] - Training Epoch: 2/2, step 993/7134 completed (loss: 0.05051526054739952, acc: 0.9836333990097046)
[2025-02-13 03:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:27][root][INFO] - Training Epoch: 2/2, step 994/7134 completed (loss: 0.04387778043746948, acc: 0.9864864945411682)
[2025-02-13 03:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28][root][INFO] - Training Epoch: 2/2, step 995/7134 completed (loss: 0.034526266157627106, acc: 0.9845678806304932)
[2025-02-13 03:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28][root][INFO] - Training Epoch: 2/2, step 996/7134 completed (loss: 0.041040677577257156, acc: 0.9846368432044983)
[2025-02-13 03:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28][root][INFO] - Training Epoch: 2/2, step 997/7134 completed (loss: 0.0318673774600029, acc: 0.9885386824607849)
[2025-02-13 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:29][root][INFO] - Training Epoch: 2/2, step 998/7134 completed (loss: 0.033985886722803116, acc: 0.9943661689758301)
[2025-02-13 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:29][root][INFO] - Training Epoch: 2/2, step 999/7134 completed (loss: 0.018111132085323334, acc: 0.9892473220825195)
[2025-02-13 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30][root][INFO] - Training Epoch: 2/2, step 1000/7134 completed (loss: 0.028663231059908867, acc: 0.9897260069847107)
[2025-02-13 03:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30][root][INFO] - Training Epoch: 2/2, step 1001/7134 completed (loss: 0.03766569122672081, acc: 0.991919219493866)
[2025-02-13 03:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30][root][INFO] - Training Epoch: 2/2, step 1002/7134 completed (loss: 0.027702581137418747, acc: 0.9928673505783081)
[2025-02-13 03:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:31][root][INFO] - Training Epoch: 2/2, step 1003/7134 completed (loss: 0.03701256960630417, acc: 0.9910846948623657)
[2025-02-13 03:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:31][root][INFO] - Training Epoch: 2/2, step 1004/7134 completed (loss: 0.06447908282279968, acc: 0.983505129814148)
[2025-02-13 03:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:32][root][INFO] - Training Epoch: 2/2, step 1005/7134 completed (loss: 0.08409103006124496, acc: 0.9834437370300293)
[2025-02-13 03:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:32][root][INFO] - Training Epoch: 2/2, step 1006/7134 completed (loss: 0.0506618358194828, acc: 0.9860724210739136)
[2025-02-13 03:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33][root][INFO] - Training Epoch: 2/2, step 1007/7134 completed (loss: 0.06666632741689682, acc: 0.9780219793319702)
[2025-02-13 03:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33][root][INFO] - Training Epoch: 2/2, step 1008/7134 completed (loss: 0.049495890736579895, acc: 0.9872340559959412)
[2025-02-13 03:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33][root][INFO] - Training Epoch: 2/2, step 1009/7134 completed (loss: 0.049437399953603745, acc: 0.9911190271377563)
[2025-02-13 03:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:34][root][INFO] - Training Epoch: 2/2, step 1010/7134 completed (loss: 0.03234114870429039, acc: 0.9861660003662109)
[2025-02-13 03:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:34][root][INFO] - Training Epoch: 2/2, step 1011/7134 completed (loss: 0.03751440718770027, acc: 0.9889349937438965)
[2025-02-13 03:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:35][root][INFO] - Training Epoch: 2/2, step 1012/7134 completed (loss: 0.04459477961063385, acc: 0.9872521162033081)
[2025-02-13 03:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:35][root][INFO] - Training Epoch: 2/2, step 1013/7134 completed (loss: 0.04867666959762573, acc: 0.984240710735321)
[2025-02-13 03:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36][root][INFO] - Training Epoch: 2/2, step 1014/7134 completed (loss: 0.037223897874355316, acc: 0.9885057210922241)
[2025-02-13 03:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36][root][INFO] - Training Epoch: 2/2, step 1015/7134 completed (loss: 0.043345384299755096, acc: 0.9879518151283264)
[2025-02-13 03:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36][root][INFO] - Training Epoch: 2/2, step 1016/7134 completed (loss: 0.04538557305932045, acc: 0.990439772605896)
[2025-02-13 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:37][root][INFO] - Training Epoch: 2/2, step 1017/7134 completed (loss: 0.013569551520049572, acc: 0.9946308732032776)
[2025-02-13 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:37][root][INFO] - Training Epoch: 2/2, step 1018/7134 completed (loss: 0.054602254182100296, acc: 0.9868228435516357)
[2025-02-13 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:38][root][INFO] - Training Epoch: 2/2, step 1019/7134 completed (loss: 0.03790940344333649, acc: 0.9919871687889099)
[2025-02-13 03:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:38][root][INFO] - Training Epoch: 2/2, step 1020/7134 completed (loss: 0.00918743945658207, acc: 0.9985074400901794)
[2025-02-13 03:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39][root][INFO] - Training Epoch: 2/2, step 1021/7134 completed (loss: 0.018551845103502274, acc: 0.9968152642250061)
[2025-02-13 03:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39][root][INFO] - Training Epoch: 2/2, step 1022/7134 completed (loss: 0.02782982401549816, acc: 0.992094874382019)
[2025-02-13 03:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39][root][INFO] - Training Epoch: 2/2, step 1023/7134 completed (loss: 0.043247777968645096, acc: 0.9909909963607788)
[2025-02-13 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:40][root][INFO] - Training Epoch: 2/2, step 1024/7134 completed (loss: 0.051763519644737244, acc: 0.985637366771698)
[2025-02-13 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:40][root][INFO] - Training Epoch: 2/2, step 1025/7134 completed (loss: 0.014937041327357292, acc: 0.9964413046836853)
[2025-02-13 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41][root][INFO] - Training Epoch: 2/2, step 1026/7134 completed (loss: 0.048655763268470764, acc: 0.9873684048652649)
[2025-02-13 03:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41][root][INFO] - Training Epoch: 2/2, step 1027/7134 completed (loss: 0.011790833435952663, acc: 0.9986467957496643)
[2025-02-13 03:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41][root][INFO] - Training Epoch: 2/2, step 1028/7134 completed (loss: 0.020697511732578278, acc: 0.994020938873291)
[2025-02-13 03:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:42][root][INFO] - Training Epoch: 2/2, step 1029/7134 completed (loss: 0.006962564308196306, acc: 1.0)
[2025-02-13 03:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:42][root][INFO] - Training Epoch: 2/2, step 1030/7134 completed (loss: 0.011160509660840034, acc: 0.9973045587539673)
[2025-02-13 03:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:43][root][INFO] - Training Epoch: 2/2, step 1031/7134 completed (loss: 0.03762761130928993, acc: 0.9878048896789551)
[2025-02-13 03:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:43][root][INFO] - Training Epoch: 2/2, step 1032/7134 completed (loss: 0.03208182379603386, acc: 0.9942396283149719)
[2025-02-13 03:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44][root][INFO] - Training Epoch: 2/2, step 1033/7134 completed (loss: 0.025153599679470062, acc: 0.9928264021873474)
[2025-02-13 03:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44][root][INFO] - Training Epoch: 2/2, step 1034/7134 completed (loss: 0.032802991569042206, acc: 0.9935170412063599)
[2025-02-13 03:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44][root][INFO] - Training Epoch: 2/2, step 1035/7134 completed (loss: 0.1343768835067749, acc: 0.9666666388511658)
[2025-02-13 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:45][root][INFO] - Training Epoch: 2/2, step 1036/7134 completed (loss: 0.12524768710136414, acc: 0.9733333587646484)
[2025-02-13 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:45][root][INFO] - Training Epoch: 2/2, step 1037/7134 completed (loss: 0.016238898038864136, acc: 0.9952380657196045)
[2025-02-13 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:46][root][INFO] - Training Epoch: 2/2, step 1038/7134 completed (loss: 0.0157737135887146, acc: 0.9966443181037903)
[2025-02-13 03:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:46][root][INFO] - Training Epoch: 2/2, step 1039/7134 completed (loss: 0.018137196078896523, acc: 0.9958592057228088)
[2025-02-13 03:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47][root][INFO] - Training Epoch: 2/2, step 1040/7134 completed (loss: 0.00766978831961751, acc: 0.9982876777648926)
[2025-02-13 03:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47][root][INFO] - Training Epoch: 2/2, step 1041/7134 completed (loss: 0.031585726886987686, acc: 0.9922118186950684)
[2025-02-13 03:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47][root][INFO] - Training Epoch: 2/2, step 1042/7134 completed (loss: 0.025415871292352676, acc: 0.9884615540504456)
[2025-02-13 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:48][root][INFO] - Training Epoch: 2/2, step 1043/7134 completed (loss: 0.02201097458600998, acc: 0.9915764331817627)
[2025-02-13 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:48][root][INFO] - Training Epoch: 2/2, step 1044/7134 completed (loss: 0.030987830832600594, acc: 0.9945205450057983)
[2025-02-13 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:49][root][INFO] - Training Epoch: 2/2, step 1045/7134 completed (loss: 0.084372878074646, acc: 0.9806138873100281)
[2025-02-13 03:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:49][root][INFO] - Training Epoch: 2/2, step 1046/7134 completed (loss: 0.03008667379617691, acc: 0.992277979850769)
[2025-02-13 03:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:50][root][INFO] - Training Epoch: 2/2, step 1047/7134 completed (loss: 0.058524180203676224, acc: 0.9792592525482178)
[2025-02-13 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:50][root][INFO] - Training Epoch: 2/2, step 1048/7134 completed (loss: 0.009436517022550106, acc: 0.9956011772155762)
[2025-02-13 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51][root][INFO] - Training Epoch: 2/2, step 1049/7134 completed (loss: 0.028409374877810478, acc: 0.9912152290344238)
[2025-02-13 03:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51][root][INFO] - Training Epoch: 2/2, step 1050/7134 completed (loss: 0.03366740047931671, acc: 0.9863387942314148)
[2025-02-13 03:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51][root][INFO] - Training Epoch: 2/2, step 1051/7134 completed (loss: 0.01892613060772419, acc: 0.9917808175086975)
[2025-02-13 03:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:52][root][INFO] - Training Epoch: 2/2, step 1052/7134 completed (loss: 0.04832196235656738, acc: 0.9822294116020203)
[2025-02-13 03:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:52][root][INFO] - Training Epoch: 2/2, step 1053/7134 completed (loss: 0.019440582022070885, acc: 0.9915493130683899)
[2025-02-13 03:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53][root][INFO] - Training Epoch: 2/2, step 1054/7134 completed (loss: 0.021465469151735306, acc: 0.9933775067329407)
[2025-02-13 03:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53][root][INFO] - Training Epoch: 2/2, step 1055/7134 completed (loss: 0.07951897382736206, acc: 0.9733924865722656)
[2025-02-13 03:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53][root][INFO] - Training Epoch: 2/2, step 1056/7134 completed (loss: 0.05608176067471504, acc: 0.981574535369873)
[2025-02-13 03:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:54][root][INFO] - Training Epoch: 2/2, step 1057/7134 completed (loss: 0.03121795319020748, acc: 0.9884393215179443)
[2025-02-13 03:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:54][root][INFO] - Training Epoch: 2/2, step 1058/7134 completed (loss: 0.029677720740437508, acc: 0.9956521987915039)
[2025-02-13 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:55][root][INFO] - Training Epoch: 2/2, step 1059/7134 completed (loss: 0.04139632731676102, acc: 0.9896774291992188)
[2025-02-13 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:55][root][INFO] - Training Epoch: 2/2, step 1060/7134 completed (loss: 0.03769170120358467, acc: 0.9905362725257874)
[2025-02-13 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:56][root][INFO] - Training Epoch: 2/2, step 1061/7134 completed (loss: 0.0253320150077343, acc: 0.992732584476471)
[2025-02-13 03:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:56][root][INFO] - Training Epoch: 2/2, step 1062/7134 completed (loss: 0.037586014717817307, acc: 0.9847457408905029)
[2025-02-13 03:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57][root][INFO] - Training Epoch: 2/2, step 1063/7134 completed (loss: 0.008429802022874355, acc: 0.9983792304992676)
[2025-02-13 03:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57][root][INFO] - Training Epoch: 2/2, step 1064/7134 completed (loss: 0.02903641015291214, acc: 0.9907407164573669)
[2025-02-13 03:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57][root][INFO] - Training Epoch: 2/2, step 1065/7134 completed (loss: 0.016960347071290016, acc: 0.9945799708366394)
[2025-02-13 03:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:58][root][INFO] - Training Epoch: 2/2, step 1066/7134 completed (loss: 0.048169322311878204, acc: 0.9832214713096619)
[2025-02-13 03:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:58][root][INFO] - Training Epoch: 2/2, step 1067/7134 completed (loss: 0.0412011593580246, acc: 0.9892141819000244)
[2025-02-13 03:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:59][root][INFO] - Training Epoch: 2/2, step 1068/7134 completed (loss: 0.03647078573703766, acc: 0.9868667721748352)
[2025-02-13 03:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:59][root][INFO] - Training Epoch: 2/2, step 1069/7134 completed (loss: 0.03947441652417183, acc: 0.9849749803543091)
[2025-02-13 03:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00][root][INFO] - Training Epoch: 2/2, step 1070/7134 completed (loss: 0.02688111551105976, acc: 0.9901477694511414)
[2025-02-13 03:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00][root][INFO] - Training Epoch: 2/2, step 1071/7134 completed (loss: 0.012468220666050911, acc: 0.9946452379226685)
[2025-02-13 03:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00][root][INFO] - Training Epoch: 2/2, step 1072/7134 completed (loss: 0.003257300704717636, acc: 1.0)
[2025-02-13 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:01][root][INFO] - Training Epoch: 2/2, step 1073/7134 completed (loss: 0.011564385145902634, acc: 0.9942362904548645)
[2025-02-13 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:01][root][INFO] - Training Epoch: 2/2, step 1074/7134 completed (loss: 0.05113743990659714, acc: 0.9861830472946167)
[2025-02-13 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:02][root][INFO] - Training Epoch: 2/2, step 1075/7134 completed (loss: 0.0620177760720253, acc: 0.9815837740898132)
[2025-02-13 03:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:02][root][INFO] - Training Epoch: 2/2, step 1076/7134 completed (loss: 0.07453679293394089, acc: 0.9760638475418091)
[2025-02-13 03:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03][root][INFO] - Training Epoch: 2/2, step 1077/7134 completed (loss: 0.06164805218577385, acc: 0.9729729890823364)
[2025-02-13 03:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03][root][INFO] - Training Epoch: 2/2, step 1078/7134 completed (loss: 0.07747118175029755, acc: 0.9822294116020203)
[2025-02-13 03:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03][root][INFO] - Training Epoch: 2/2, step 1079/7134 completed (loss: 0.02238866128027439, acc: 0.9930796027183533)
[2025-02-13 03:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:04][root][INFO] - Training Epoch: 2/2, step 1080/7134 completed (loss: 0.04728802293539047, acc: 0.9943609237670898)
[2025-02-13 03:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:04][root][INFO] - Training Epoch: 2/2, step 1081/7134 completed (loss: 0.01563052460551262, acc: 0.9943898916244507)
[2025-02-13 03:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:05][root][INFO] - Training Epoch: 2/2, step 1082/7134 completed (loss: 0.013025878928601742, acc: 0.9969167709350586)
[2025-02-13 03:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:05][root][INFO] - Training Epoch: 2/2, step 1083/7134 completed (loss: 0.022536197677254677, acc: 0.9909604787826538)
[2025-02-13 03:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:06][root][INFO] - Training Epoch: 2/2, step 1084/7134 completed (loss: 0.02400020882487297, acc: 0.9913259148597717)
[2025-02-13 03:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:06][root][INFO] - Training Epoch: 2/2, step 1085/7134 completed (loss: 0.032204147428274155, acc: 0.9906291961669922)
[2025-02-13 03:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07][root][INFO] - Training Epoch: 2/2, step 1086/7134 completed (loss: 0.04850633442401886, acc: 0.9848771095275879)
[2025-02-13 03:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07][root][INFO] - Training Epoch: 2/2, step 1087/7134 completed (loss: 0.021700482815504074, acc: 0.9914634227752686)
[2025-02-13 03:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07][root][INFO] - Training Epoch: 2/2, step 1088/7134 completed (loss: 0.024823764339089394, acc: 0.99301677942276)
[2025-02-13 03:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:08][root][INFO] - Training Epoch: 2/2, step 1089/7134 completed (loss: 0.04180153086781502, acc: 0.9910581111907959)
[2025-02-13 03:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:08][root][INFO] - Training Epoch: 2/2, step 1090/7134 completed (loss: 0.030902573838829994, acc: 0.9875195026397705)
[2025-02-13 03:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:09][root][INFO] - Training Epoch: 2/2, step 1091/7134 completed (loss: 0.016472969204187393, acc: 0.9954338073730469)
[2025-02-13 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:09][root][INFO] - Training Epoch: 2/2, step 1092/7134 completed (loss: 0.009985954500734806, acc: 0.9971098303794861)
[2025-02-13 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10][root][INFO] - Training Epoch: 2/2, step 1093/7134 completed (loss: 0.09633532166481018, acc: 0.9717868566513062)
[2025-02-13 03:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10][root][INFO] - Training Epoch: 2/2, step 1094/7134 completed (loss: 0.061808645725250244, acc: 0.9832713603973389)
[2025-02-13 03:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10][root][INFO] - Training Epoch: 2/2, step 1095/7134 completed (loss: 0.03862395137548447, acc: 0.9878197312355042)
[2025-02-13 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:11][root][INFO] - Training Epoch: 2/2, step 1096/7134 completed (loss: 0.02478565275669098, acc: 0.9949367046356201)
[2025-02-13 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:11][root][INFO] - Training Epoch: 2/2, step 1097/7134 completed (loss: 0.07369089871644974, acc: 0.9845505356788635)
[2025-02-13 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:12][root][INFO] - Training Epoch: 2/2, step 1098/7134 completed (loss: 0.020538801327347755, acc: 0.9925834536552429)
[2025-02-13 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:12][root][INFO] - Training Epoch: 2/2, step 1099/7134 completed (loss: 0.03185180574655533, acc: 0.9910256266593933)
[2025-02-13 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:13][root][INFO] - Training Epoch: 2/2, step 1100/7134 completed (loss: 0.037125490605831146, acc: 0.98777174949646)
[2025-02-13 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:13][root][INFO] - Training Epoch: 2/2, step 1101/7134 completed (loss: 0.023114535957574844, acc: 0.9961038827896118)
[2025-02-13 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14][root][INFO] - Training Epoch: 2/2, step 1102/7134 completed (loss: 0.04863164573907852, acc: 0.9843444228172302)
[2025-02-13 03:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14][root][INFO] - Training Epoch: 2/2, step 1103/7134 completed (loss: 0.06953328847885132, acc: 0.984000027179718)
[2025-02-13 03:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14][root][INFO] - Training Epoch: 2/2, step 1104/7134 completed (loss: 0.030438348650932312, acc: 0.9927954077720642)
[2025-02-13 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:15][root][INFO] - Training Epoch: 2/2, step 1105/7134 completed (loss: 0.03227178752422333, acc: 0.9905660152435303)
[2025-02-13 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:15][root][INFO] - Training Epoch: 2/2, step 1106/7134 completed (loss: 0.042290907353162766, acc: 0.9848484992980957)
[2025-02-13 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:16][root][INFO] - Training Epoch: 2/2, step 1107/7134 completed (loss: 0.028359107673168182, acc: 0.991793692111969)
[2025-02-13 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:16][root][INFO] - Training Epoch: 2/2, step 1108/7134 completed (loss: 0.012129583396017551, acc: 1.0)
[2025-02-13 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:17][root][INFO] - Training Epoch: 2/2, step 1109/7134 completed (loss: 0.030850427225232124, acc: 0.9916201233863831)
[2025-02-13 03:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:17][root][INFO] - Training Epoch: 2/2, step 1110/7134 completed (loss: 0.04568561539053917, acc: 0.9868938326835632)
[2025-02-13 03:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18][root][INFO] - Training Epoch: 2/2, step 1111/7134 completed (loss: 0.048158615827560425, acc: 0.9834254384040833)
[2025-02-13 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18][root][INFO] - Training Epoch: 2/2, step 1112/7134 completed (loss: 0.034894004464149475, acc: 0.9882352948188782)
[2025-02-13 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18][root][INFO] - Training Epoch: 2/2, step 1113/7134 completed (loss: 0.02530078776180744, acc: 0.9897040128707886)
[2025-02-13 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:19][root][INFO] - Training Epoch: 2/2, step 1114/7134 completed (loss: 0.04225575551390648, acc: 0.9878493547439575)
[2025-02-13 03:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:19][root][INFO] - Training Epoch: 2/2, step 1115/7134 completed (loss: 0.03165791928768158, acc: 0.9921383857727051)
[2025-02-13 03:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20][root][INFO] - Training Epoch: 2/2, step 1116/7134 completed (loss: 0.013863285072147846, acc: 0.996268630027771)
[2025-02-13 03:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20][root][INFO] - Training Epoch: 2/2, step 1117/7134 completed (loss: 0.03824220970273018, acc: 0.9892638325691223)
[2025-02-13 03:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20][root][INFO] - Training Epoch: 2/2, step 1118/7134 completed (loss: 0.015184219926595688, acc: 0.9948275685310364)
[2025-02-13 03:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:21][root][INFO] - Training Epoch: 2/2, step 1119/7134 completed (loss: 0.035532984882593155, acc: 0.9873617887496948)
[2025-02-13 03:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:21][root][INFO] - Training Epoch: 2/2, step 1120/7134 completed (loss: 0.034505560994148254, acc: 0.9827044010162354)
[2025-02-13 03:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22][root][INFO] - Training Epoch: 2/2, step 1121/7134 completed (loss: 0.03941019997000694, acc: 0.9867060780525208)
[2025-02-13 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22][root][INFO] - Training Epoch: 2/2, step 1122/7134 completed (loss: 0.03481651097536087, acc: 0.9883494973182678)
[2025-02-13 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22][root][INFO] - Training Epoch: 2/2, step 1123/7134 completed (loss: 0.016507433727383614, acc: 0.9935794472694397)
[2025-02-13 03:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:23][root][INFO] - Training Epoch: 2/2, step 1124/7134 completed (loss: 0.02182801254093647, acc: 0.9922600388526917)
[2025-02-13 03:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:23][root][INFO] - Training Epoch: 2/2, step 1125/7134 completed (loss: 0.01681225746870041, acc: 0.9932340979576111)
[2025-02-13 03:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:24][root][INFO] - Training Epoch: 2/2, step 1126/7134 completed (loss: 0.028350137174129486, acc: 0.9903314709663391)
[2025-02-13 03:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:24][root][INFO] - Training Epoch: 2/2, step 1127/7134 completed (loss: 0.0205091405659914, acc: 0.9919999837875366)
[2025-02-13 03:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25][root][INFO] - Training Epoch: 2/2, step 1128/7134 completed (loss: 0.030396848917007446, acc: 0.9878472089767456)
[2025-02-13 03:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25][root][INFO] - Training Epoch: 2/2, step 1129/7134 completed (loss: 0.04178471863269806, acc: 0.9865471124649048)
[2025-02-13 03:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25][root][INFO] - Training Epoch: 2/2, step 1130/7134 completed (loss: 0.020491886883974075, acc: 0.993537962436676)
[2025-02-13 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:26][root][INFO] - Training Epoch: 2/2, step 1131/7134 completed (loss: 0.03939121961593628, acc: 0.9882075190544128)
[2025-02-13 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:26][root][INFO] - Training Epoch: 2/2, step 1132/7134 completed (loss: 0.09187186509370804, acc: 0.9785605072975159)
[2025-02-13 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:27][root][INFO] - Training Epoch: 2/2, step 1133/7134 completed (loss: 0.09899061918258667, acc: 0.9771754741668701)
[2025-02-13 03:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:27][root][INFO] - Training Epoch: 2/2, step 1134/7134 completed (loss: 0.06559618562459946, acc: 0.9788029789924622)
[2025-02-13 03:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28][root][INFO] - Training Epoch: 2/2, step 1135/7134 completed (loss: 0.06428676843643188, acc: 0.9824000000953674)
[2025-02-13 03:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28][root][INFO] - Training Epoch: 2/2, step 1136/7134 completed (loss: 0.06477232277393341, acc: 0.9850339889526367)
[2025-02-13 03:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28][root][INFO] - Training Epoch: 2/2, step 1137/7134 completed (loss: 0.04378849267959595, acc: 0.9840909242630005)
[2025-02-13 03:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:29][root][INFO] - Training Epoch: 2/2, step 1138/7134 completed (loss: 0.03796035796403885, acc: 0.9904371500015259)
[2025-02-13 03:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:29][root][INFO] - Training Epoch: 2/2, step 1139/7134 completed (loss: 0.06549844145774841, acc: 0.9780775904655457)
[2025-02-13 03:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:30][root][INFO] - Training Epoch: 2/2, step 1140/7134 completed (loss: 0.02076311782002449, acc: 0.9924812316894531)
[2025-02-13 03:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:30][root][INFO] - Training Epoch: 2/2, step 1141/7134 completed (loss: 0.024343155324459076, acc: 0.9929742217063904)
[2025-02-13 03:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31][root][INFO] - Training Epoch: 2/2, step 1142/7134 completed (loss: 0.04785565659403801, acc: 0.9934895634651184)
[2025-02-13 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31][root][INFO] - Training Epoch: 2/2, step 1143/7134 completed (loss: 0.06648588180541992, acc: 0.9819193482398987)
[2025-02-13 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31][root][INFO] - Training Epoch: 2/2, step 1144/7134 completed (loss: 0.06551013141870499, acc: 0.9825000166893005)
[2025-02-13 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:32][root][INFO] - Training Epoch: 2/2, step 1145/7134 completed (loss: 0.032066550105810165, acc: 0.9952038526535034)
[2025-02-13 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:32][root][INFO] - Training Epoch: 2/2, step 1146/7134 completed (loss: 0.03223951533436775, acc: 0.9909420013427734)
[2025-02-13 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:33][root][INFO] - Training Epoch: 2/2, step 1147/7134 completed (loss: 0.018642153590917587, acc: 0.996259331703186)
[2025-02-13 03:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:33][root][INFO] - Training Epoch: 2/2, step 1148/7134 completed (loss: 0.04956934228539467, acc: 0.9846416115760803)
[2025-02-13 03:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34][root][INFO] - Training Epoch: 2/2, step 1149/7134 completed (loss: 0.07406782358884811, acc: 0.9714285731315613)
[2025-02-13 03:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34][root][INFO] - Training Epoch: 2/2, step 1150/7134 completed (loss: 0.024795029312372208, acc: 0.9968992471694946)
[2025-02-13 03:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34][root][INFO] - Training Epoch: 2/2, step 1151/7134 completed (loss: 0.008932195603847504, acc: 1.0)
[2025-02-13 03:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:35][root][INFO] - Training Epoch: 2/2, step 1152/7134 completed (loss: 0.02313758060336113, acc: 0.9914529919624329)
[2025-02-13 03:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:35][root][INFO] - Training Epoch: 2/2, step 1153/7134 completed (loss: 0.07201778143644333, acc: 0.9822888374328613)
[2025-02-13 03:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:36][root][INFO] - Training Epoch: 2/2, step 1154/7134 completed (loss: 0.04853098466992378, acc: 0.991150438785553)
[2025-02-13 03:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:36][root][INFO] - Training Epoch: 2/2, step 1155/7134 completed (loss: 0.05542125180363655, acc: 0.9758771657943726)
[2025-02-13 03:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37][root][INFO] - Training Epoch: 2/2, step 1156/7134 completed (loss: 0.01286298967897892, acc: 0.996927797794342)
[2025-02-13 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37][root][INFO] - Training Epoch: 2/2, step 1157/7134 completed (loss: 0.04618744179606438, acc: 0.9915110468864441)
[2025-02-13 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37][root][INFO] - Training Epoch: 2/2, step 1158/7134 completed (loss: 0.03485919162631035, acc: 0.9921104311943054)
[2025-02-13 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:38][root][INFO] - Training Epoch: 2/2, step 1159/7134 completed (loss: 0.04984140023589134, acc: 0.9843971729278564)
[2025-02-13 03:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:38][root][INFO] - Training Epoch: 2/2, step 1160/7134 completed (loss: 0.01935507170855999, acc: 0.9955357313156128)
[2025-02-13 03:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39][root][INFO] - Training Epoch: 2/2, step 1161/7134 completed (loss: 0.08600056171417236, acc: 0.9820554852485657)
[2025-02-13 03:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39][root][INFO] - Training Epoch: 2/2, step 1162/7134 completed (loss: 0.023110752925276756, acc: 0.9917355179786682)
[2025-02-13 03:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39][root][INFO] - Training Epoch: 2/2, step 1163/7134 completed (loss: 0.0400364026427269, acc: 0.9914675951004028)
[2025-02-13 03:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:40][root][INFO] - Training Epoch: 2/2, step 1164/7134 completed (loss: 0.045829612761735916, acc: 0.9930459260940552)
[2025-02-13 03:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:40][root][INFO] - Training Epoch: 2/2, step 1165/7134 completed (loss: 0.026440106332302094, acc: 0.9959294199943542)
[2025-02-13 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:41][root][INFO] - Training Epoch: 2/2, step 1166/7134 completed (loss: 0.016894742846488953, acc: 0.9958847761154175)
[2025-02-13 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:41][root][INFO] - Training Epoch: 2/2, step 1167/7134 completed (loss: 0.01201001275330782, acc: 0.9960707426071167)
[2025-02-13 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:42][root][INFO] - Training Epoch: 2/2, step 1168/7134 completed (loss: 0.010218274779617786, acc: 0.996052622795105)
[2025-02-13 03:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:42][root][INFO] - Training Epoch: 2/2, step 1169/7134 completed (loss: 0.03316126763820648, acc: 0.9878378510475159)
[2025-02-13 03:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43][root][INFO] - Training Epoch: 2/2, step 1170/7134 completed (loss: 0.03729948401451111, acc: 0.9915730357170105)
[2025-02-13 03:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43][root][INFO] - Training Epoch: 2/2, step 1171/7134 completed (loss: 0.016777798533439636, acc: 0.9916527271270752)
[2025-02-13 03:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43][root][INFO] - Training Epoch: 2/2, step 1172/7134 completed (loss: 0.03245120123028755, acc: 0.9878970980644226)
[2025-02-13 03:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:44][root][INFO] - Training Epoch: 2/2, step 1173/7134 completed (loss: 0.03399374708533287, acc: 0.9927536249160767)
[2025-02-13 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:44][root][INFO] - Training Epoch: 2/2, step 1174/7134 completed (loss: 0.0145967872813344, acc: 0.9969969987869263)
[2025-02-13 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45][root][INFO] - Training Epoch: 2/2, step 1175/7134 completed (loss: 0.019219033420085907, acc: 0.9942029118537903)
[2025-02-13 03:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45][root][INFO] - Training Epoch: 2/2, step 1176/7134 completed (loss: 0.005010133143514395, acc: 0.9982876777648926)
[2025-02-13 03:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45][root][INFO] - Training Epoch: 2/2, step 1177/7134 completed (loss: 0.010106991045176983, acc: 0.998039186000824)
[2025-02-13 03:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:46][root][INFO] - Training Epoch: 2/2, step 1178/7134 completed (loss: 0.02371852472424507, acc: 0.9929906725883484)
[2025-02-13 03:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:46][root][INFO] - Training Epoch: 2/2, step 1179/7134 completed (loss: 0.017324406653642654, acc: 0.9961758852005005)
[2025-02-13 03:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47][root][INFO] - Training Epoch: 2/2, step 1180/7134 completed (loss: 0.009694371372461319, acc: 0.9963503479957581)
[2025-02-13 03:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47][root][INFO] - Training Epoch: 2/2, step 1181/7134 completed (loss: 0.028525900095701218, acc: 0.9913669228553772)
[2025-02-13 03:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47][root][INFO] - Training Epoch: 2/2, step 1182/7134 completed (loss: 0.003610474057495594, acc: 1.0)
[2025-02-13 03:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:48][root][INFO] - Training Epoch: 2/2, step 1183/7134 completed (loss: 0.01696394942700863, acc: 0.9941262602806091)
[2025-02-13 03:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:48][root][INFO] - Training Epoch: 2/2, step 1184/7134 completed (loss: 0.04241512343287468, acc: 0.991416335105896)
[2025-02-13 03:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:49][root][INFO] - Training Epoch: 2/2, step 1185/7134 completed (loss: 0.007952721789479256, acc: 0.9984962344169617)
[2025-02-13 03:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:49][root][INFO] - Training Epoch: 2/2, step 1186/7134 completed (loss: 0.02103469893336296, acc: 0.9987030029296875)
[2025-02-13 03:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50][root][INFO] - Training Epoch: 2/2, step 1187/7134 completed (loss: 0.02373930811882019, acc: 0.9930747747421265)
[2025-02-13 03:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50][root][INFO] - Training Epoch: 2/2, step 1188/7134 completed (loss: 0.013367262668907642, acc: 0.9973614811897278)
[2025-02-13 03:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50][root][INFO] - Training Epoch: 2/2, step 1189/7134 completed (loss: 0.01150570809841156, acc: 0.9971098303794861)
[2025-02-13 03:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:51][root][INFO] - Training Epoch: 2/2, step 1190/7134 completed (loss: 0.01782311126589775, acc: 0.9949109554290771)
[2025-02-13 03:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:51][root][INFO] - Training Epoch: 2/2, step 1191/7134 completed (loss: 0.03371042013168335, acc: 0.9911373853683472)
[2025-02-13 03:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:52][root][INFO] - Training Epoch: 2/2, step 1192/7134 completed (loss: 0.012506573460996151, acc: 0.9960681796073914)
[2025-02-13 03:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:52][root][INFO] - Training Epoch: 2/2, step 1193/7134 completed (loss: 0.04970019683241844, acc: 0.9904305934906006)
[2025-02-13 03:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:53][root][INFO] - Training Epoch: 2/2, step 1194/7134 completed (loss: 0.03973333537578583, acc: 0.9903846383094788)
[2025-02-13 03:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:53][root][INFO] - Training Epoch: 2/2, step 1195/7134 completed (loss: 0.02958609163761139, acc: 0.9885621070861816)
[2025-02-13 03:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54][root][INFO] - Training Epoch: 2/2, step 1196/7134 completed (loss: 0.056083329021930695, acc: 0.9852458834648132)
[2025-02-13 03:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54][root][INFO] - Training Epoch: 2/2, step 1197/7134 completed (loss: 0.031217074021697044, acc: 0.9897360801696777)
[2025-02-13 03:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54][root][INFO] - Training Epoch: 2/2, step 1198/7134 completed (loss: 0.025891093537211418, acc: 0.9913669228553772)
[2025-02-13 03:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:55][root][INFO] - Training Epoch: 2/2, step 1199/7134 completed (loss: 0.016883675009012222, acc: 0.9955817461013794)
[2025-02-13 03:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:55][root][INFO] - Training Epoch: 2/2, step 1200/7134 completed (loss: 0.03876693174242973, acc: 0.9897330403327942)
[2025-02-13 03:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:56][root][INFO] - Training Epoch: 2/2, step 1201/7134 completed (loss: 0.02903576008975506, acc: 0.9926362037658691)
[2025-02-13 03:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:56][root][INFO] - Training Epoch: 2/2, step 1202/7134 completed (loss: 0.0390111543238163, acc: 0.9892328381538391)
[2025-02-13 03:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57][root][INFO] - Training Epoch: 2/2, step 1203/7134 completed (loss: 0.00833592563867569, acc: 0.9954476356506348)
[2025-02-13 03:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57][root][INFO] - Training Epoch: 2/2, step 1204/7134 completed (loss: 0.030652383342385292, acc: 0.9906716346740723)
[2025-02-13 03:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57][root][INFO] - Training Epoch: 2/2, step 1205/7134 completed (loss: 0.07615005970001221, acc: 0.9816901683807373)
[2025-02-13 03:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:58][root][INFO] - Training Epoch: 2/2, step 1206/7134 completed (loss: 0.02722998894751072, acc: 0.9913151264190674)
[2025-02-13 03:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:58][root][INFO] - Training Epoch: 2/2, step 1207/7134 completed (loss: 0.04015118256211281, acc: 0.9886934757232666)
[2025-02-13 03:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:59][root][INFO] - Training Epoch: 2/2, step 1208/7134 completed (loss: 0.04280718043446541, acc: 0.9850746393203735)
[2025-02-13 03:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:59][root][INFO] - Training Epoch: 2/2, step 1209/7134 completed (loss: 0.09129894524812698, acc: 0.9798902869224548)
[2025-02-13 03:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00][root][INFO] - Training Epoch: 2/2, step 1210/7134 completed (loss: 0.0625331774353981, acc: 0.9776714444160461)
[2025-02-13 03:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00][root][INFO] - Training Epoch: 2/2, step 1211/7134 completed (loss: 0.09822925925254822, acc: 0.9614643454551697)
[2025-02-13 03:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00][root][INFO] - Training Epoch: 2/2, step 1212/7134 completed (loss: 0.08912624418735504, acc: 0.980182945728302)
[2025-02-13 03:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:01][root][INFO] - Training Epoch: 2/2, step 1213/7134 completed (loss: 0.024398986250162125, acc: 0.9900990128517151)
[2025-02-13 03:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:01][root][INFO] - Training Epoch: 2/2, step 1214/7134 completed (loss: 0.05979064851999283, acc: 0.9815384745597839)
[2025-02-13 03:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:02][root][INFO] - Training Epoch: 2/2, step 1215/7134 completed (loss: 0.07696110010147095, acc: 0.9820689558982849)
[2025-02-13 03:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:02][root][INFO] - Training Epoch: 2/2, step 1216/7134 completed (loss: 0.029608970507979393, acc: 0.9941605925559998)
[2025-02-13 03:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:03][root][INFO] - Training Epoch: 2/2, step 1217/7134 completed (loss: 0.04191069304943085, acc: 0.9886363744735718)
[2025-02-13 03:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:03][root][INFO] - Training Epoch: 2/2, step 1218/7134 completed (loss: 0.09341614693403244, acc: 0.9805970191955566)
[2025-02-13 03:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04][root][INFO] - Training Epoch: 2/2, step 1219/7134 completed (loss: 0.07730643451213837, acc: 0.9806678295135498)
[2025-02-13 03:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04][root][INFO] - Training Epoch: 2/2, step 1220/7134 completed (loss: 0.07170513272285461, acc: 0.9835025668144226)
[2025-02-13 03:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04][root][INFO] - Training Epoch: 2/2, step 1221/7134 completed (loss: 0.06546720862388611, acc: 0.9857456088066101)
[2025-02-13 03:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:05][root][INFO] - Training Epoch: 2/2, step 1222/7134 completed (loss: 0.059201065450906754, acc: 0.9856972694396973)
[2025-02-13 03:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:05][root][INFO] - Training Epoch: 2/2, step 1223/7134 completed (loss: 0.1427278071641922, acc: 0.9766803979873657)
[2025-02-13 03:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:06][root][INFO] - Training Epoch: 2/2, step 1224/7134 completed (loss: 0.04757923260331154, acc: 0.9911699891090393)
[2025-02-13 03:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:06][root][INFO] - Training Epoch: 2/2, step 1225/7134 completed (loss: 0.055343665182590485, acc: 0.9841089844703674)
[2025-02-13 03:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:07][root][INFO] - Training Epoch: 2/2, step 1226/7134 completed (loss: 0.10938501358032227, acc: 0.9718309640884399)
[2025-02-13 03:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:07][root][INFO] - Training Epoch: 2/2, step 1227/7134 completed (loss: 0.07500258833169937, acc: 0.9758713245391846)
[2025-02-13 03:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08][root][INFO] - Training Epoch: 2/2, step 1228/7134 completed (loss: 0.08695574849843979, acc: 0.9799714088439941)
[2025-02-13 03:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08][root][INFO] - Training Epoch: 2/2, step 1229/7134 completed (loss: 0.03344034031033516, acc: 0.9939975738525391)
[2025-02-13 03:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:09][root][INFO] - Training Epoch: 2/2, step 1230/7134 completed (loss: 0.08462414890527725, acc: 0.9807909727096558)
[2025-02-13 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:09][root][INFO] - Training Epoch: 2/2, step 1231/7134 completed (loss: 0.051027506589889526, acc: 0.988624632358551)
[2025-02-13 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10][root][INFO] - Training Epoch: 2/2, step 1232/7134 completed (loss: 0.06715678423643112, acc: 0.9850904941558838)
[2025-02-13 03:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10][root][INFO] - Training Epoch: 2/2, step 1233/7134 completed (loss: 0.11197896301746368, acc: 0.9722543358802795)
[2025-02-13 03:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10][root][INFO] - Training Epoch: 2/2, step 1234/7134 completed (loss: 0.0315362885594368, acc: 0.9922360181808472)
[2025-02-13 03:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:11][root][INFO] - Training Epoch: 2/2, step 1235/7134 completed (loss: 0.07212001085281372, acc: 0.9844124913215637)
[2025-02-13 03:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:11][root][INFO] - Training Epoch: 2/2, step 1236/7134 completed (loss: 0.11120437830686569, acc: 0.9736456871032715)
[2025-02-13 03:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12][root][INFO] - Training Epoch: 2/2, step 1237/7134 completed (loss: 0.07100991904735565, acc: 0.97826087474823)
[2025-02-13 03:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12][root][INFO] - Training Epoch: 2/2, step 1238/7134 completed (loss: 0.11920732259750366, acc: 0.9650654792785645)
[2025-02-13 03:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:13][root][INFO] - Training Epoch: 2/2, step 1239/7134 completed (loss: 0.0850968062877655, acc: 0.9713876843452454)
[2025-02-13 03:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:13][root][INFO] - Training Epoch: 2/2, step 1240/7134 completed (loss: 0.06740614026784897, acc: 0.9828042387962341)
[2025-02-13 03:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14][root][INFO] - Training Epoch: 2/2, step 1241/7134 completed (loss: 0.03579946979880333, acc: 0.9839357137680054)
[2025-02-13 03:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14][root][INFO] - Training Epoch: 2/2, step 1242/7134 completed (loss: 0.04353789985179901, acc: 0.9903846383094788)
[2025-02-13 03:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14][root][INFO] - Training Epoch: 2/2, step 1243/7134 completed (loss: 0.056680355221033096, acc: 0.9853556752204895)
[2025-02-13 03:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:15][root][INFO] - Training Epoch: 2/2, step 1244/7134 completed (loss: 0.08832161128520966, acc: 0.9850993156433105)
[2025-02-13 03:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:15][root][INFO] - Training Epoch: 2/2, step 1245/7134 completed (loss: 0.05952765792608261, acc: 0.982367753982544)
[2025-02-13 03:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:16][root][INFO] - Training Epoch: 2/2, step 1246/7134 completed (loss: 0.09187500178813934, acc: 0.9794608354568481)
[2025-02-13 03:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:16][root][INFO] - Training Epoch: 2/2, step 1247/7134 completed (loss: 0.03796780854463577, acc: 0.9884225726127625)
[2025-02-13 03:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17][root][INFO] - Training Epoch: 2/2, step 1248/7134 completed (loss: 0.07653525471687317, acc: 0.9771167039871216)
[2025-02-13 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17][root][INFO] - Training Epoch: 2/2, step 1249/7134 completed (loss: 0.026701858267188072, acc: 0.9946619272232056)
[2025-02-13 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17][root][INFO] - Training Epoch: 2/2, step 1250/7134 completed (loss: 0.02851535938680172, acc: 0.9886685609817505)
[2025-02-13 03:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:18][root][INFO] - Training Epoch: 2/2, step 1251/7134 completed (loss: 0.02312173880636692, acc: 0.991769552230835)
[2025-02-13 03:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:18][root][INFO] - Training Epoch: 2/2, step 1252/7134 completed (loss: 0.03214459493756294, acc: 0.9922839403152466)
[2025-02-13 03:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:19][root][INFO] - Training Epoch: 2/2, step 1253/7134 completed (loss: 0.06327962875366211, acc: 0.9864864945411682)
[2025-02-13 03:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:19][root][INFO] - Training Epoch: 2/2, step 1254/7134 completed (loss: 0.06781377643346786, acc: 0.9772727489471436)
[2025-02-13 03:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20][root][INFO] - Training Epoch: 2/2, step 1255/7134 completed (loss: 0.03818522393703461, acc: 0.991253674030304)
[2025-02-13 03:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20][root][INFO] - Training Epoch: 2/2, step 1256/7134 completed (loss: 0.03657326474785805, acc: 0.9871382713317871)
[2025-02-13 03:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20][root][INFO] - Training Epoch: 2/2, step 1257/7134 completed (loss: 0.025137973949313164, acc: 0.9903069734573364)
[2025-02-13 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:21][root][INFO] - Training Epoch: 2/2, step 1258/7134 completed (loss: 0.05881787836551666, acc: 0.9795022010803223)
[2025-02-13 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:21][root][INFO] - Training Epoch: 2/2, step 1259/7134 completed (loss: 0.05684394761919975, acc: 0.9892703890800476)
[2025-02-13 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22][root][INFO] - Training Epoch: 2/2, step 1260/7134 completed (loss: 0.014382836408913136, acc: 0.9985548853874207)
[2025-02-13 03:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22][root][INFO] - Training Epoch: 2/2, step 1261/7134 completed (loss: 0.040554486215114594, acc: 0.9910979270935059)
[2025-02-13 03:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22][root][INFO] - Training Epoch: 2/2, step 1262/7134 completed (loss: 0.00617817509919405, acc: 0.9981684684753418)
[2025-02-13 03:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:23][root][INFO] - Training Epoch: 2/2, step 1263/7134 completed (loss: 0.011568202637135983, acc: 0.9957143068313599)
[2025-02-13 03:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:23][root][INFO] - Training Epoch: 2/2, step 1264/7134 completed (loss: 0.01652655564248562, acc: 0.9953271150588989)
[2025-02-13 03:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:24][root][INFO] - Training Epoch: 2/2, step 1265/7134 completed (loss: 0.015883296728134155, acc: 0.9937343597412109)
[2025-02-13 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:24][root][INFO] - Training Epoch: 2/2, step 1266/7134 completed (loss: 0.014189101755619049, acc: 0.9965576529502869)
[2025-02-13 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:25][root][INFO] - Training Epoch: 2/2, step 1267/7134 completed (loss: 0.027516407892107964, acc: 0.9931034445762634)
[2025-02-13 03:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:25][root][INFO] - Training Epoch: 2/2, step 1268/7134 completed (loss: 0.030766485258936882, acc: 0.9881831407546997)
[2025-02-13 03:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26][root][INFO] - Training Epoch: 2/2, step 1269/7134 completed (loss: 0.06887272745370865, acc: 0.9793103337287903)
[2025-02-13 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26][root][INFO] - Training Epoch: 2/2, step 1270/7134 completed (loss: 0.020253535360097885, acc: 0.9930264949798584)
[2025-02-13 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26][root][INFO] - Training Epoch: 2/2, step 1271/7134 completed (loss: 0.06575048714876175, acc: 0.9814814925193787)
[2025-02-13 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:27][root][INFO] - Training Epoch: 2/2, step 1272/7134 completed (loss: 0.020376665517687798, acc: 0.9933949708938599)
[2025-02-13 03:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:27][root][INFO] - Training Epoch: 2/2, step 1273/7134 completed (loss: 0.02128979004919529, acc: 0.9951612949371338)
[2025-02-13 03:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28][root][INFO] - Training Epoch: 2/2, step 1274/7134 completed (loss: 0.0232137069106102, acc: 0.9915540814399719)
[2025-02-13 03:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28][root][INFO] - Training Epoch: 2/2, step 1275/7134 completed (loss: 0.028606154024600983, acc: 0.9928186535835266)
[2025-02-13 03:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28][root][INFO] - Training Epoch: 2/2, step 1276/7134 completed (loss: 0.016110321506857872, acc: 0.9951612949371338)
[2025-02-13 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:29][root][INFO] - Training Epoch: 2/2, step 1277/7134 completed (loss: 0.027929527685046196, acc: 0.9903069734573364)
[2025-02-13 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:29][root][INFO] - Training Epoch: 2/2, step 1278/7134 completed (loss: 0.04168394207954407, acc: 0.9844478964805603)
[2025-02-13 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:30][root][INFO] - Training Epoch: 2/2, step 1279/7134 completed (loss: 0.04613463580608368, acc: 0.9888337254524231)
[2025-02-13 03:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:30][root][INFO] - Training Epoch: 2/2, step 1280/7134 completed (loss: 0.05917855724692345, acc: 0.9842312932014465)
[2025-02-13 03:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31][root][INFO] - Training Epoch: 2/2, step 1281/7134 completed (loss: 0.036468930542469025, acc: 0.9913941621780396)
[2025-02-13 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31][root][INFO] - Training Epoch: 2/2, step 1282/7134 completed (loss: 0.10665494948625565, acc: 0.9719251394271851)
[2025-02-13 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31][root][INFO] - Training Epoch: 2/2, step 1283/7134 completed (loss: 0.05896587669849396, acc: 0.9876760840415955)
[2025-02-13 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:32][root][INFO] - Training Epoch: 2/2, step 1284/7134 completed (loss: 0.07021791487932205, acc: 0.9836512207984924)
[2025-02-13 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:32][root][INFO] - Training Epoch: 2/2, step 1285/7134 completed (loss: 0.05089676380157471, acc: 0.9898989796638489)
[2025-02-13 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:33][root][INFO] - Training Epoch: 2/2, step 1286/7134 completed (loss: 0.03579850494861603, acc: 0.9942775368690491)
[2025-02-13 03:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:33][root][INFO] - Training Epoch: 2/2, step 1287/7134 completed (loss: 0.08533782511949539, acc: 0.985049843788147)
[2025-02-13 03:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34][root][INFO] - Training Epoch: 2/2, step 1288/7134 completed (loss: 0.04407896101474762, acc: 0.9860405921936035)
[2025-02-13 03:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34][root][INFO] - Training Epoch: 2/2, step 1289/7134 completed (loss: 0.04637833684682846, acc: 0.9865996837615967)
[2025-02-13 03:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34][root][INFO] - Training Epoch: 2/2, step 1290/7134 completed (loss: 0.04174013063311577, acc: 0.9887323975563049)
[2025-02-13 03:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:35][root][INFO] - Training Epoch: 2/2, step 1291/7134 completed (loss: 0.026853282004594803, acc: 0.9972106218338013)
[2025-02-13 03:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:35][root][INFO] - Training Epoch: 2/2, step 1292/7134 completed (loss: 0.03022667020559311, acc: 0.9935064911842346)
[2025-02-13 03:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:36][root][INFO] - Training Epoch: 2/2, step 1293/7134 completed (loss: 0.037416793406009674, acc: 0.9883585572242737)
[2025-02-13 03:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:36][root][INFO] - Training Epoch: 2/2, step 1294/7134 completed (loss: 0.037639081478118896, acc: 0.9895969033241272)
[2025-02-13 03:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37][root][INFO] - Training Epoch: 2/2, step 1295/7134 completed (loss: 0.03393605723977089, acc: 0.9864176511764526)
[2025-02-13 03:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37][root][INFO] - Training Epoch: 2/2, step 1296/7134 completed (loss: 0.010214217938482761, acc: 0.997787594795227)
[2025-02-13 03:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37][root][INFO] - Training Epoch: 2/2, step 1297/7134 completed (loss: 0.03258915990591049, acc: 0.9854469895362854)
[2025-02-13 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:38][root][INFO] - Training Epoch: 2/2, step 1298/7134 completed (loss: 0.08430884778499603, acc: 0.980215847492218)
[2025-02-13 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:38][root][INFO] - Training Epoch: 2/2, step 1299/7134 completed (loss: 0.012999688275158405, acc: 1.0)
[2025-02-13 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:39][root][INFO] - Training Epoch: 2/2, step 1300/7134 completed (loss: 0.028102805837988853, acc: 0.9902557730674744)
[2025-02-13 03:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:39][root][INFO] - Training Epoch: 2/2, step 1301/7134 completed (loss: 0.008631730452179909, acc: 0.998516321182251)
[2025-02-13 03:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40][root][INFO] - Training Epoch: 2/2, step 1302/7134 completed (loss: 0.04690783470869064, acc: 0.9877232313156128)
[2025-02-13 03:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40][root][INFO] - Training Epoch: 2/2, step 1303/7134 completed (loss: 0.05784905329346657, acc: 0.9879807829856873)
[2025-02-13 03:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40][root][INFO] - Training Epoch: 2/2, step 1304/7134 completed (loss: 0.06738647818565369, acc: 0.9868247509002686)
[2025-02-13 03:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:41][root][INFO] - Training Epoch: 2/2, step 1305/7134 completed (loss: 0.036890335381031036, acc: 0.9923858046531677)
[2025-02-13 03:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:41][root][INFO] - Training Epoch: 2/2, step 1306/7134 completed (loss: 0.05211777612566948, acc: 0.9863429665565491)
[2025-02-13 03:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:42][root][INFO] - Training Epoch: 2/2, step 1307/7134 completed (loss: 0.028971806168556213, acc: 0.9905213117599487)
[2025-02-13 03:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:42][root][INFO] - Training Epoch: 2/2, step 1308/7134 completed (loss: 0.028736410662531853, acc: 0.9924812316894531)
[2025-02-13 03:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:43][root][INFO] - Training Epoch: 2/2, step 1309/7134 completed (loss: 0.0417514331638813, acc: 0.9904305934906006)
[2025-02-13 03:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:43][root][INFO] - Training Epoch: 2/2, step 1310/7134 completed (loss: 0.04553666338324547, acc: 0.9850339889526367)
[2025-02-13 03:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:44][root][INFO] - Training Epoch: 2/2, step 1311/7134 completed (loss: 0.06529166549444199, acc: 0.9819004535675049)
[2025-02-13 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:44][root][INFO] - Training Epoch: 2/2, step 1312/7134 completed (loss: 0.07224602997303009, acc: 0.9838274717330933)
[2025-02-13 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45][root][INFO] - Training Epoch: 2/2, step 1313/7134 completed (loss: 0.06212550401687622, acc: 0.9844192862510681)
[2025-02-13 03:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45][root][INFO] - Training Epoch: 2/2, step 1314/7134 completed (loss: 0.04980406537652016, acc: 0.980701744556427)
[2025-02-13 03:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45][root][INFO] - Training Epoch: 2/2, step 1315/7134 completed (loss: 0.14720141887664795, acc: 0.9666666388511658)
[2025-02-13 03:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:46][root][INFO] - Training Epoch: 2/2, step 1316/7134 completed (loss: 0.03209109976887703, acc: 0.9887005686759949)
[2025-02-13 03:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:46][root][INFO] - Training Epoch: 2/2, step 1317/7134 completed (loss: 0.1198364868760109, acc: 0.9674796462059021)
[2025-02-13 03:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47][root][INFO] - Training Epoch: 2/2, step 1318/7134 completed (loss: 0.05199863761663437, acc: 0.9836512207984924)
[2025-02-13 03:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47][root][INFO] - Training Epoch: 2/2, step 1319/7134 completed (loss: 0.0709492415189743, acc: 0.9885583519935608)
[2025-02-13 03:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47][root][INFO] - Training Epoch: 2/2, step 1320/7134 completed (loss: 0.1012500748038292, acc: 0.9771987199783325)
[2025-02-13 03:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:48][root][INFO] - Training Epoch: 2/2, step 1321/7134 completed (loss: 0.02994498983025551, acc: 0.9914215803146362)
[2025-02-13 03:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:48][root][INFO] - Training Epoch: 2/2, step 1322/7134 completed (loss: 0.0653197318315506, acc: 0.9852700233459473)
[2025-02-13 03:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49][root][INFO] - Training Epoch: 2/2, step 1323/7134 completed (loss: 0.047409363090991974, acc: 0.987500011920929)
[2025-02-13 03:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49][root][INFO] - Training Epoch: 2/2, step 1324/7134 completed (loss: 0.03449476882815361, acc: 0.9934210777282715)
[2025-02-13 03:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49][root][INFO] - Training Epoch: 2/2, step 1325/7134 completed (loss: 0.03130289912223816, acc: 0.9909297227859497)
[2025-02-13 03:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:50][root][INFO] - Training Epoch: 2/2, step 1326/7134 completed (loss: 0.06159958615899086, acc: 0.9868276715278625)
[2025-02-13 03:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:50][root][INFO] - Training Epoch: 2/2, step 1327/7134 completed (loss: 0.05163060128688812, acc: 0.9895052313804626)
[2025-02-13 03:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51][root][INFO] - Training Epoch: 2/2, step 1328/7134 completed (loss: 0.04731116071343422, acc: 0.9906976819038391)
[2025-02-13 03:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51][root][INFO] - Training Epoch: 2/2, step 1329/7134 completed (loss: 0.06280913203954697, acc: 0.9834254384040833)
[2025-02-13 03:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52][root][INFO] - Training Epoch: 2/2, step 1330/7134 completed (loss: 0.02923622913658619, acc: 0.989708423614502)
[2025-02-13 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52][root][INFO] - Training Epoch: 2/2, step 1331/7134 completed (loss: 0.015394456684589386, acc: 0.9951140284538269)
[2025-02-13 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52][root][INFO] - Training Epoch: 2/2, step 1332/7134 completed (loss: 0.009647316299378872, acc: 0.9978166222572327)
[2025-02-13 03:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:53][root][INFO] - Training Epoch: 2/2, step 1333/7134 completed (loss: 0.04995085671544075, acc: 0.9846938848495483)
[2025-02-13 03:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:53][root][INFO] - Training Epoch: 2/2, step 1334/7134 completed (loss: 0.08555653691291809, acc: 0.97826087474823)
[2025-02-13 03:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:54][root][INFO] - Training Epoch: 2/2, step 1335/7134 completed (loss: 0.11264211684465408, acc: 0.9840213060379028)
[2025-02-13 03:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:54][root][INFO] - Training Epoch: 2/2, step 1336/7134 completed (loss: 0.023581305518746376, acc: 0.9932340979576111)
[2025-02-13 03:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55][root][INFO] - Training Epoch: 2/2, step 1337/7134 completed (loss: 0.020198233425617218, acc: 0.9907975196838379)
[2025-02-13 03:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55][root][INFO] - Training Epoch: 2/2, step 1338/7134 completed (loss: 0.05151878297328949, acc: 0.9919678568840027)
[2025-02-13 03:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55][root][INFO] - Training Epoch: 2/2, step 1339/7134 completed (loss: 0.07205414772033691, acc: 0.9747191071510315)
[2025-02-13 03:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:56][root][INFO] - Training Epoch: 2/2, step 1340/7134 completed (loss: 0.05964041128754616, acc: 0.9865871667861938)
[2025-02-13 03:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:56][root][INFO] - Training Epoch: 2/2, step 1341/7134 completed (loss: 0.08097932487726212, acc: 0.9858757257461548)
[2025-02-13 03:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57][root][INFO] - Training Epoch: 2/2, step 1342/7134 completed (loss: 0.007229417096823454, acc: 0.998389720916748)
[2025-02-13 03:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57][root][INFO] - Training Epoch: 2/2, step 1343/7134 completed (loss: 0.0837978720664978, acc: 0.9838998317718506)
[2025-02-13 03:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57][root][INFO] - Training Epoch: 2/2, step 1344/7134 completed (loss: 0.04330205172300339, acc: 0.9864253401756287)
[2025-02-13 03:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:58][root][INFO] - Training Epoch: 2/2, step 1345/7134 completed (loss: 0.023173050954937935, acc: 0.9867374300956726)
[2025-02-13 03:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:58][root][INFO] - Training Epoch: 2/2, step 1346/7134 completed (loss: 0.04868042469024658, acc: 0.9893190860748291)
[2025-02-13 03:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59][root][INFO] - Training Epoch: 2/2, step 1347/7134 completed (loss: 0.05808492749929428, acc: 0.9860627055168152)
[2025-02-13 03:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59][root][INFO] - Training Epoch: 2/2, step 1348/7134 completed (loss: 0.021507233381271362, acc: 0.9960681796073914)
[2025-02-13 03:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59][root][INFO] - Training Epoch: 2/2, step 1349/7134 completed (loss: 0.0512884147465229, acc: 0.9931623935699463)
[2025-02-13 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:00][root][INFO] - Training Epoch: 2/2, step 1350/7134 completed (loss: 0.025366036221385002, acc: 0.9942113161087036)
[2025-02-13 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:00][root][INFO] - Training Epoch: 2/2, step 1351/7134 completed (loss: 0.018798166885972023, acc: 0.994106113910675)
[2025-02-13 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01][root][INFO] - Training Epoch: 2/2, step 1352/7134 completed (loss: 0.03373430669307709, acc: 0.992559552192688)
[2025-02-13 03:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01][root][INFO] - Training Epoch: 2/2, step 1353/7134 completed (loss: 0.04155789315700531, acc: 0.9885057210922241)
[2025-02-13 03:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01][root][INFO] - Training Epoch: 2/2, step 1354/7134 completed (loss: 0.011678923852741718, acc: 0.9951377511024475)
[2025-02-13 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:02][root][INFO] - Training Epoch: 2/2, step 1355/7134 completed (loss: 0.008200973272323608, acc: 0.9984423518180847)
[2025-02-13 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:02][root][INFO] - Training Epoch: 2/2, step 1356/7134 completed (loss: 0.028187982738018036, acc: 0.9947368502616882)
[2025-02-13 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:03][root][INFO] - Training Epoch: 2/2, step 1357/7134 completed (loss: 0.013172515667974949, acc: 0.996216893196106)
[2025-02-13 03:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:03][root][INFO] - Training Epoch: 2/2, step 1358/7134 completed (loss: 0.02772746980190277, acc: 0.9922077655792236)
[2025-02-13 03:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:04][root][INFO] - Training Epoch: 2/2, step 1359/7134 completed (loss: 0.011313366703689098, acc: 0.9979757070541382)
[2025-02-13 03:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:04][root][INFO] - Training Epoch: 2/2, step 1360/7134 completed (loss: 0.011316072195768356, acc: 0.9959677457809448)
[2025-02-13 03:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05][root][INFO] - Training Epoch: 2/2, step 1361/7134 completed (loss: 0.020027240738272667, acc: 0.9956204295158386)
[2025-02-13 03:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05][root][INFO] - Training Epoch: 2/2, step 1362/7134 completed (loss: 0.018436824902892113, acc: 0.9959016442298889)
[2025-02-13 03:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05][root][INFO] - Training Epoch: 2/2, step 1363/7134 completed (loss: 0.01534756738692522, acc: 0.9959785342216492)
[2025-02-13 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:06][root][INFO] - Training Epoch: 2/2, step 1364/7134 completed (loss: 0.015247576870024204, acc: 0.9942857027053833)
[2025-02-13 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:06][root][INFO] - Training Epoch: 2/2, step 1365/7134 completed (loss: 0.003794994903728366, acc: 1.0)
[2025-02-13 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:07][root][INFO] - Training Epoch: 2/2, step 1366/7134 completed (loss: 0.004786430858075619, acc: 1.0)
[2025-02-13 03:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:07][root][INFO] - Training Epoch: 2/2, step 1367/7134 completed (loss: 0.00156811170745641, acc: 1.0)
[2025-02-13 03:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08][root][INFO] - Training Epoch: 2/2, step 1368/7134 completed (loss: 0.023758910596370697, acc: 0.9908536672592163)
[2025-02-13 03:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08][root][INFO] - Training Epoch: 2/2, step 1369/7134 completed (loss: 0.026662195101380348, acc: 0.9912434220314026)
[2025-02-13 03:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08][root][INFO] - Training Epoch: 2/2, step 1370/7134 completed (loss: 0.03689732402563095, acc: 0.993678867816925)
[2025-02-13 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:09][root][INFO] - Training Epoch: 2/2, step 1371/7134 completed (loss: 0.024086078628897667, acc: 0.9956331849098206)
[2025-02-13 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:09][root][INFO] - Training Epoch: 2/2, step 1372/7134 completed (loss: 0.04137784615159035, acc: 0.9906367063522339)
[2025-02-13 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:10][root][INFO] - Training Epoch: 2/2, step 1373/7134 completed (loss: 0.017040997743606567, acc: 0.9940944910049438)
[2025-02-13 03:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:10][root][INFO] - Training Epoch: 2/2, step 1374/7134 completed (loss: 0.026086723431944847, acc: 0.9881188273429871)
[2025-02-13 03:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11][root][INFO] - Training Epoch: 2/2, step 1375/7134 completed (loss: 0.03532135859131813, acc: 0.9831730723381042)
[2025-02-13 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11][root][INFO] - Training Epoch: 2/2, step 1376/7134 completed (loss: 0.05475342646241188, acc: 0.9832985401153564)
[2025-02-13 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11][root][INFO] - Training Epoch: 2/2, step 1377/7134 completed (loss: 0.015410278923809528, acc: 0.9952940940856934)
[2025-02-13 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:12][root][INFO] - Training Epoch: 2/2, step 1378/7134 completed (loss: 0.06030585616827011, acc: 0.9824561476707458)
[2025-02-13 03:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:12][root][INFO] - Training Epoch: 2/2, step 1379/7134 completed (loss: 0.0382017120718956, acc: 0.9870967864990234)
[2025-02-13 03:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13][root][INFO] - Training Epoch: 2/2, step 1380/7134 completed (loss: 0.02600647322833538, acc: 0.9943925142288208)
[2025-02-13 03:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13][root][INFO] - Training Epoch: 2/2, step 1381/7134 completed (loss: 0.043715301901102066, acc: 0.9850746393203735)
[2025-02-13 03:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13][root][INFO] - Training Epoch: 2/2, step 1382/7134 completed (loss: 0.05313384160399437, acc: 0.9857142567634583)
[2025-02-13 03:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:14][root][INFO] - Training Epoch: 2/2, step 1383/7134 completed (loss: 0.025411099195480347, acc: 0.9896729588508606)
[2025-02-13 03:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:14][root][INFO] - Training Epoch: 2/2, step 1384/7134 completed (loss: 0.056251343339681625, acc: 0.9733777046203613)
[2025-02-13 03:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15][root][INFO] - Training Epoch: 2/2, step 1385/7134 completed (loss: 0.04747661203145981, acc: 0.9871559739112854)
[2025-02-13 03:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15][root][INFO] - Training Epoch: 2/2, step 1386/7134 completed (loss: 0.034892305731773376, acc: 0.9896755218505859)
[2025-02-13 03:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15][root][INFO] - Training Epoch: 2/2, step 1387/7134 completed (loss: 0.05066107586026192, acc: 0.9871060252189636)
[2025-02-13 03:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:16][root][INFO] - Training Epoch: 2/2, step 1388/7134 completed (loss: 0.04036765545606613, acc: 0.9880749583244324)
[2025-02-13 03:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:16][root][INFO] - Training Epoch: 2/2, step 1389/7134 completed (loss: 0.028825119137763977, acc: 0.9886547923088074)
[2025-02-13 03:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:17][root][INFO] - Training Epoch: 2/2, step 1390/7134 completed (loss: 0.026934929192066193, acc: 0.9937597513198853)
[2025-02-13 03:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:17][root][INFO] - Training Epoch: 2/2, step 1391/7134 completed (loss: 0.0686384066939354, acc: 0.9771528840065002)
[2025-02-13 03:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18][root][INFO] - Training Epoch: 2/2, step 1392/7134 completed (loss: 0.038405537605285645, acc: 0.9861591458320618)
[2025-02-13 03:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18][root][INFO] - Training Epoch: 2/2, step 1393/7134 completed (loss: 0.043743766844272614, acc: 0.9838709831237793)
[2025-02-13 03:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18][root][INFO] - Training Epoch: 2/2, step 1394/7134 completed (loss: 0.018344860523939133, acc: 0.9920634627342224)
[2025-02-13 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:19][root][INFO] - Training Epoch: 2/2, step 1395/7134 completed (loss: 0.05233307555317879, acc: 0.990138053894043)
[2025-02-13 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:19][root][INFO] - Training Epoch: 2/2, step 1396/7134 completed (loss: 0.03663073480129242, acc: 0.9915540814399719)
[2025-02-13 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20][root][INFO] - Training Epoch: 2/2, step 1397/7134 completed (loss: 0.05863326042890549, acc: 0.9848229289054871)
[2025-02-13 03:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20][root][INFO] - Training Epoch: 2/2, step 1398/7134 completed (loss: 0.023218579590320587, acc: 0.993914783000946)
[2025-02-13 03:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20][root][INFO] - Training Epoch: 2/2, step 1399/7134 completed (loss: 0.04834666848182678, acc: 0.9836956262588501)
[2025-02-13 03:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:21][root][INFO] - Training Epoch: 2/2, step 1400/7134 completed (loss: 0.04069811850786209, acc: 0.9895287752151489)
[2025-02-13 03:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:21][root][INFO] - Training Epoch: 2/2, step 1401/7134 completed (loss: 0.01998576521873474, acc: 0.9917627573013306)
[2025-02-13 03:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:22][root][INFO] - Training Epoch: 2/2, step 1402/7134 completed (loss: 0.020179735496640205, acc: 0.9930915236473083)
[2025-02-13 03:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:22][root][INFO] - Training Epoch: 2/2, step 1403/7134 completed (loss: 0.027749745175242424, acc: 0.9879518151283264)
[2025-02-13 03:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23][root][INFO] - Training Epoch: 2/2, step 1404/7134 completed (loss: 0.029806086793541908, acc: 0.9933686852455139)
[2025-02-13 03:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23][root][INFO] - Training Epoch: 2/2, step 1405/7134 completed (loss: 0.026771824806928635, acc: 0.9871794581413269)
[2025-02-13 03:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23][root][INFO] - Training Epoch: 2/2, step 1406/7134 completed (loss: 0.006547225639224052, acc: 0.9985632300376892)
[2025-02-13 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:24][root][INFO] - Training Epoch: 2/2, step 1407/7134 completed (loss: 0.022393031045794487, acc: 0.9921773076057434)
[2025-02-13 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:24][root][INFO] - Training Epoch: 2/2, step 1408/7134 completed (loss: 0.016026858240365982, acc: 0.9935897588729858)
[2025-02-13 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:25][root][INFO] - Training Epoch: 2/2, step 1409/7134 completed (loss: 0.019024381414055824, acc: 0.9910581111907959)
[2025-02-13 03:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:25][root][INFO] - Training Epoch: 2/2, step 1410/7134 completed (loss: 0.028780996799468994, acc: 0.9947916865348816)
[2025-02-13 03:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:26][root][INFO] - Training Epoch: 2/2, step 1411/7134 completed (loss: 0.011453421786427498, acc: 0.9973404407501221)
[2025-02-13 03:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:26][root][INFO] - Training Epoch: 2/2, step 1412/7134 completed (loss: 0.00850646011531353, acc: 0.9988123774528503)
[2025-02-13 03:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27][root][INFO] - Training Epoch: 2/2, step 1413/7134 completed (loss: 0.04405774176120758, acc: 0.9913669228553772)
[2025-02-13 03:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27][root][INFO] - Training Epoch: 2/2, step 1414/7134 completed (loss: 0.012536418624222279, acc: 0.9968847632408142)
[2025-02-13 03:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27][root][INFO] - Training Epoch: 2/2, step 1415/7134 completed (loss: 0.02061057835817337, acc: 0.9933775067329407)
[2025-02-13 03:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:28][root][INFO] - Training Epoch: 2/2, step 1416/7134 completed (loss: 0.010118345730006695, acc: 0.9985954761505127)
[2025-02-13 03:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:28][root][INFO] - Training Epoch: 2/2, step 1417/7134 completed (loss: 0.013666845858097076, acc: 0.9982993006706238)
[2025-02-13 03:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:29][root][INFO] - Training Epoch: 2/2, step 1418/7134 completed (loss: 0.01770925149321556, acc: 0.9961832165718079)
[2025-02-13 03:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:29][root][INFO] - Training Epoch: 2/2, step 1419/7134 completed (loss: 0.028828570619225502, acc: 0.9946019053459167)
[2025-02-13 03:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30][root][INFO] - Training Epoch: 2/2, step 1420/7134 completed (loss: 0.021053429692983627, acc: 0.993261456489563)
[2025-02-13 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30][root][INFO] - Training Epoch: 2/2, step 1421/7134 completed (loss: 0.017562227323651314, acc: 0.995192289352417)
[2025-02-13 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30][root][INFO] - Training Epoch: 2/2, step 1422/7134 completed (loss: 0.06937705725431442, acc: 0.9814814925193787)
[2025-02-13 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:31][root][INFO] - Training Epoch: 2/2, step 1423/7134 completed (loss: 0.03305661305785179, acc: 0.9938775300979614)
[2025-02-13 03:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:31][root][INFO] - Training Epoch: 2/2, step 1424/7134 completed (loss: 0.06173732131719589, acc: 0.9790874719619751)
[2025-02-13 03:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:32][root][INFO] - Training Epoch: 2/2, step 1425/7134 completed (loss: 0.03106648474931717, acc: 0.9880775213241577)
[2025-02-13 03:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:32][root][INFO] - Training Epoch: 2/2, step 1426/7134 completed (loss: 0.028179755434393883, acc: 0.9897172451019287)
[2025-02-13 03:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33][root][INFO] - Training Epoch: 2/2, step 1427/7134 completed (loss: 0.044412657618522644, acc: 0.9795657992362976)
[2025-02-13 03:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33][root][INFO] - Training Epoch: 2/2, step 1428/7134 completed (loss: 0.21583497524261475, acc: 0.9425981640815735)
[2025-02-13 03:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33][root][INFO] - Training Epoch: 2/2, step 1429/7134 completed (loss: 0.03142913058400154, acc: 0.9937629699707031)
[2025-02-13 03:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:34][root][INFO] - Training Epoch: 2/2, step 1430/7134 completed (loss: 0.05312316119670868, acc: 0.9852398633956909)
[2025-02-13 03:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:34][root][INFO] - Training Epoch: 2/2, step 1431/7134 completed (loss: 0.0203729048371315, acc: 0.9918588995933533)
[2025-02-13 03:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:35][root][INFO] - Training Epoch: 2/2, step 1432/7134 completed (loss: 0.025456896051764488, acc: 0.992277979850769)
[2025-02-13 03:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:35][root][INFO] - Training Epoch: 2/2, step 1433/7134 completed (loss: 0.022443898022174835, acc: 0.9921466112136841)
[2025-02-13 03:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36][root][INFO] - Training Epoch: 2/2, step 1434/7134 completed (loss: 0.04395563155412674, acc: 0.9852744340896606)
[2025-02-13 03:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36][root][INFO] - Training Epoch: 2/2, step 1435/7134 completed (loss: 0.047293733805418015, acc: 0.9780058860778809)
[2025-02-13 03:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36][root][INFO] - Training Epoch: 2/2, step 1436/7134 completed (loss: 0.08409176766872406, acc: 0.9682741165161133)
[2025-02-13 03:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:37][root][INFO] - Training Epoch: 2/2, step 1437/7134 completed (loss: 0.040810227394104004, acc: 0.9860724210739136)
[2025-02-13 03:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:37][root][INFO] - Training Epoch: 2/2, step 1438/7134 completed (loss: 0.030774980783462524, acc: 0.9912408590316772)
[2025-02-13 03:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:38][root][INFO] - Training Epoch: 2/2, step 1439/7134 completed (loss: 0.032451532781124115, acc: 0.9870129823684692)
[2025-02-13 03:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:38][root][INFO] - Training Epoch: 2/2, step 1440/7134 completed (loss: 0.06677669286727905, acc: 0.9724612832069397)
[2025-02-13 03:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39][root][INFO] - Training Epoch: 2/2, step 1441/7134 completed (loss: 0.03114684857428074, acc: 0.9897360801696777)
[2025-02-13 03:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39][root][INFO] - Training Epoch: 2/2, step 1442/7134 completed (loss: 0.025947032496333122, acc: 0.9880383014678955)
[2025-02-13 03:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39][root][INFO] - Training Epoch: 2/2, step 1443/7134 completed (loss: 0.06605580449104309, acc: 0.9806678295135498)
[2025-02-13 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:40][root][INFO] - Training Epoch: 2/2, step 1444/7134 completed (loss: 0.032606545835733414, acc: 0.9922178983688354)
[2025-02-13 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:40][root][INFO] - Training Epoch: 2/2, step 1445/7134 completed (loss: 0.0531008206307888, acc: 0.9833610653877258)
[2025-02-13 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:41][root][INFO] - Training Epoch: 2/2, step 1446/7134 completed (loss: 0.0252060629427433, acc: 0.9928876161575317)
[2025-02-13 03:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:41][root][INFO] - Training Epoch: 2/2, step 1447/7134 completed (loss: 0.032375186681747437, acc: 0.9886524677276611)
[2025-02-13 03:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42][root][INFO] - Training Epoch: 2/2, step 1448/7134 completed (loss: 0.051480766385793686, acc: 0.9830508232116699)
[2025-02-13 03:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42][root][INFO] - Training Epoch: 2/2, step 1449/7134 completed (loss: 0.06537774205207825, acc: 0.9845505356788635)
[2025-02-13 03:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42][root][INFO] - Training Epoch: 2/2, step 1450/7134 completed (loss: 0.023414215072989464, acc: 0.9898403286933899)
[2025-02-13 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:43][root][INFO] - Training Epoch: 2/2, step 1451/7134 completed (loss: 0.021672483533620834, acc: 0.9956076145172119)
[2025-02-13 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:43][root][INFO] - Training Epoch: 2/2, step 1452/7134 completed (loss: 0.06815601140260696, acc: 0.984375)
[2025-02-13 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44][root][INFO] - Training Epoch: 2/2, step 1453/7134 completed (loss: 0.068466417491436, acc: 0.9857369065284729)
[2025-02-13 03:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44][root][INFO] - Training Epoch: 2/2, step 1454/7134 completed (loss: 0.06081327795982361, acc: 0.982503354549408)
[2025-02-13 03:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44][root][INFO] - Training Epoch: 2/2, step 1455/7134 completed (loss: 0.05960312485694885, acc: 0.9816513657569885)
[2025-02-13 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:45][root][INFO] - Training Epoch: 2/2, step 1456/7134 completed (loss: 0.029398329555988312, acc: 0.9978166222572327)
[2025-02-13 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:45][root][INFO] - Training Epoch: 2/2, step 1457/7134 completed (loss: 0.030718322843313217, acc: 0.9891473054885864)
[2025-02-13 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:46][root][INFO] - Training Epoch: 2/2, step 1458/7134 completed (loss: 0.011020882986485958, acc: 0.9985272288322449)
[2025-02-13 03:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:46][root][INFO] - Training Epoch: 2/2, step 1459/7134 completed (loss: 0.03203358128666878, acc: 0.9893428087234497)
[2025-02-13 03:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47][root][INFO] - Training Epoch: 2/2, step 1460/7134 completed (loss: 0.07536271959543228, acc: 0.9802631735801697)
[2025-02-13 03:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47][root][INFO] - Training Epoch: 2/2, step 1461/7134 completed (loss: 0.024173956364393234, acc: 0.9872408509254456)
[2025-02-13 03:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47][root][INFO] - Training Epoch: 2/2, step 1462/7134 completed (loss: 0.08313914388418198, acc: 0.9821428656578064)
[2025-02-13 03:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:48][root][INFO] - Training Epoch: 2/2, step 1463/7134 completed (loss: 0.0699385553598404, acc: 0.9790794849395752)
[2025-02-13 03:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:48][root][INFO] - Training Epoch: 2/2, step 1464/7134 completed (loss: 0.07875644415616989, acc: 0.9849849939346313)
[2025-02-13 03:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:49][root][INFO] - Training Epoch: 2/2, step 1465/7134 completed (loss: 0.046215519309043884, acc: 0.9874607920646667)
[2025-02-13 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:49][root][INFO] - Training Epoch: 2/2, step 1466/7134 completed (loss: 0.016053153201937675, acc: 0.9971671104431152)
[2025-02-13 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50][root][INFO] - Training Epoch: 2/2, step 1467/7134 completed (loss: 0.012705670669674873, acc: 0.9971181750297546)
[2025-02-13 03:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50][root][INFO] - Training Epoch: 2/2, step 1468/7134 completed (loss: 0.02828558161854744, acc: 0.9894319772720337)
[2025-02-13 03:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50][root][INFO] - Training Epoch: 2/2, step 1469/7134 completed (loss: 0.02988572232425213, acc: 0.9932705163955688)
[2025-02-13 03:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:51][root][INFO] - Training Epoch: 2/2, step 1470/7134 completed (loss: 0.026016240939497948, acc: 0.9913294911384583)
[2025-02-13 03:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:51][root][INFO] - Training Epoch: 2/2, step 1471/7134 completed (loss: 0.030270623043179512, acc: 0.9893428087234497)
[2025-02-13 03:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:52][root][INFO] - Training Epoch: 2/2, step 1472/7134 completed (loss: 0.030588777735829353, acc: 0.9911949634552002)
[2025-02-13 03:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:52][root][INFO] - Training Epoch: 2/2, step 1473/7134 completed (loss: 0.027956223115324974, acc: 0.9896142482757568)
[2025-02-13 03:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53][root][INFO] - Training Epoch: 2/2, step 1474/7134 completed (loss: 0.04962329566478729, acc: 0.9798657894134521)
[2025-02-13 03:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53][root][INFO] - Training Epoch: 2/2, step 1475/7134 completed (loss: 0.058303434401750565, acc: 0.9907651543617249)
[2025-02-13 03:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53][root][INFO] - Training Epoch: 2/2, step 1476/7134 completed (loss: 0.03115403652191162, acc: 0.9875776171684265)
[2025-02-13 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:54][root][INFO] - Training Epoch: 2/2, step 1477/7134 completed (loss: 0.037864942103624344, acc: 0.9860383868217468)
[2025-02-13 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:54][root][INFO] - Training Epoch: 2/2, step 1478/7134 completed (loss: 0.04305458441376686, acc: 0.9927007555961609)
[2025-02-13 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:55][root][INFO] - Training Epoch: 2/2, step 1479/7134 completed (loss: 0.02371806465089321, acc: 0.9899425506591797)
[2025-02-13 03:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:55][root][INFO] - Training Epoch: 2/2, step 1480/7134 completed (loss: 0.015793215483427048, acc: 0.9922077655792236)
[2025-02-13 03:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56][root][INFO] - Training Epoch: 2/2, step 1481/7134 completed (loss: 0.032137680798769, acc: 0.9934210777282715)
[2025-02-13 03:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56][root][INFO] - Training Epoch: 2/2, step 1482/7134 completed (loss: 0.041222114115953445, acc: 0.9872159361839294)
[2025-02-13 03:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56][root][INFO] - Training Epoch: 2/2, step 1483/7134 completed (loss: 0.022161390632390976, acc: 0.9921011328697205)
[2025-02-13 03:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:57][root][INFO] - Training Epoch: 2/2, step 1484/7134 completed (loss: 0.0520813949406147, acc: 0.986994206905365)
[2025-02-13 03:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:57][root][INFO] - Training Epoch: 2/2, step 1485/7134 completed (loss: 0.02188032679259777, acc: 0.991150438785553)
[2025-02-13 03:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:58][root][INFO] - Training Epoch: 2/2, step 1486/7134 completed (loss: 0.0424303337931633, acc: 0.9891745448112488)
[2025-02-13 03:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:58][root][INFO] - Training Epoch: 2/2, step 1487/7134 completed (loss: 0.011974249966442585, acc: 0.9960784316062927)
[2025-02-13 03:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:59][root][INFO] - Training Epoch: 2/2, step 1488/7134 completed (loss: 0.03144640102982521, acc: 0.9935829043388367)
[2025-02-13 03:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:59][root][INFO] - Training Epoch: 2/2, step 1489/7134 completed (loss: 0.022741518914699554, acc: 0.9906542301177979)
[2025-02-13 03:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00][root][INFO] - Training Epoch: 2/2, step 1490/7134 completed (loss: 0.012566315941512585, acc: 0.9957671761512756)
[2025-02-13 03:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00][root][INFO] - Training Epoch: 2/2, step 1491/7134 completed (loss: 0.0131556810811162, acc: 0.9965517520904541)
[2025-02-13 03:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00][root][INFO] - Training Epoch: 2/2, step 1492/7134 completed (loss: 0.05374743044376373, acc: 0.9872390031814575)
[2025-02-13 03:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:01][root][INFO] - Training Epoch: 2/2, step 1493/7134 completed (loss: 0.025351503863930702, acc: 0.9965437650680542)
[2025-02-13 03:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:01][root][INFO] - Training Epoch: 2/2, step 1494/7134 completed (loss: 0.013815733604133129, acc: 0.9963503479957581)
[2025-02-13 03:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:02][root][INFO] - Training Epoch: 2/2, step 1495/7134 completed (loss: 0.01753813587129116, acc: 0.9952095746994019)
[2025-02-13 03:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:02][root][INFO] - Training Epoch: 2/2, step 1496/7134 completed (loss: 0.03354354575276375, acc: 0.994301974773407)
[2025-02-13 03:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:03][root][INFO] - Training Epoch: 2/2, step 1497/7134 completed (loss: 0.03240315988659859, acc: 0.9886877536773682)
[2025-02-13 03:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:03][root][INFO] - Training Epoch: 2/2, step 1498/7134 completed (loss: 0.012799742631614208, acc: 0.9986824989318848)
[2025-02-13 03:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04][root][INFO] - Training Epoch: 2/2, step 1499/7134 completed (loss: 0.020734993740916252, acc: 0.9952550530433655)
[2025-02-13 03:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04][root][INFO] - Training Epoch: 2/2, step 1500/7134 completed (loss: 0.010000801645219326, acc: 0.996688723564148)
[2025-02-13 03:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04][root][INFO] - Training Epoch: 2/2, step 1501/7134 completed (loss: 0.03544517233967781, acc: 0.99262535572052)
[2025-02-13 03:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:05][root][INFO] - Training Epoch: 2/2, step 1502/7134 completed (loss: 0.034665413200855255, acc: 0.990980863571167)
[2025-02-13 03:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:05][root][INFO] - Training Epoch: 2/2, step 1503/7134 completed (loss: 0.06518203765153885, acc: 0.9815340638160706)
[2025-02-13 03:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06][root][INFO] - Training Epoch: 2/2, step 1504/7134 completed (loss: 0.011112596839666367, acc: 0.99622642993927)
[2025-02-13 03:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06][root][INFO] - Training Epoch: 2/2, step 1505/7134 completed (loss: 0.03767096996307373, acc: 0.9922118186950684)
[2025-02-13 03:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07][root][INFO] - Training Epoch: 2/2, step 1506/7134 completed (loss: 0.05046549811959267, acc: 0.9832826852798462)
[2025-02-13 03:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07][root][INFO] - Training Epoch: 2/2, step 1507/7134 completed (loss: 0.02162283845245838, acc: 0.9934554696083069)
[2025-02-13 03:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07][root][INFO] - Training Epoch: 2/2, step 1508/7134 completed (loss: 0.018310964107513428, acc: 0.9945295453071594)
[2025-02-13 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:08][root][INFO] - Training Epoch: 2/2, step 1509/7134 completed (loss: 0.013831126503646374, acc: 0.9971988797187805)
[2025-02-13 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:08][root][INFO] - Training Epoch: 2/2, step 1510/7134 completed (loss: 0.04369882121682167, acc: 0.9866666793823242)
[2025-02-13 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:09][root][INFO] - Training Epoch: 2/2, step 1511/7134 completed (loss: 0.06611262261867523, acc: 0.9815602898597717)
[2025-02-13 03:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:09][root][INFO] - Training Epoch: 2/2, step 1512/7134 completed (loss: 0.01917419768869877, acc: 0.990867555141449)
[2025-02-13 03:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:10][root][INFO] - Training Epoch: 2/2, step 1513/7134 completed (loss: 0.016542932018637657, acc: 0.9920634627342224)
[2025-02-13 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:10][root][INFO] - Training Epoch: 2/2, step 1514/7134 completed (loss: 0.03839946910738945, acc: 0.9950310587882996)
[2025-02-13 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11][root][INFO] - Training Epoch: 2/2, step 1515/7134 completed (loss: 0.024646973237395287, acc: 0.985981285572052)
[2025-02-13 03:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11][root][INFO] - Training Epoch: 2/2, step 1516/7134 completed (loss: 0.0418522022664547, acc: 0.9857549667358398)
[2025-02-13 03:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11][root][INFO] - Training Epoch: 2/2, step 1517/7134 completed (loss: 0.040870632976293564, acc: 0.9873949289321899)
[2025-02-13 03:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12][root][INFO] - Training Epoch: 2/2, step 1518/7134 completed (loss: 0.013021816499531269, acc: 0.9975903630256653)
[2025-02-13 03:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12][root][INFO] - Training Epoch: 2/2, step 1519/7134 completed (loss: 0.018469179049134254, acc: 0.9930875301361084)
[2025-02-13 03:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12][root][INFO] - Training Epoch: 2/2, step 1520/7134 completed (loss: 0.02876373939216137, acc: 0.9906396269798279)
[2025-02-13 03:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13][root][INFO] - Training Epoch: 2/2, step 1521/7134 completed (loss: 0.0957033634185791, acc: 0.9720670580863953)
[2025-02-13 03:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13][root][INFO] - Training Epoch: 2/2, step 1522/7134 completed (loss: 0.0650319829583168, acc: 0.9749518036842346)
[2025-02-13 03:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:14][root][INFO] - Training Epoch: 2/2, step 1523/7134 completed (loss: 0.10239953547716141, acc: 0.9700149893760681)
[2025-02-13 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:14][root][INFO] - Training Epoch: 2/2, step 1524/7134 completed (loss: 0.026345329359173775, acc: 0.9966443181037903)
[2025-02-13 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15][root][INFO] - Training Epoch: 2/2, step 1525/7134 completed (loss: 0.02151680365204811, acc: 0.9932126402854919)
[2025-02-13 03:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15][root][INFO] - Training Epoch: 2/2, step 1526/7134 completed (loss: 0.0688697099685669, acc: 0.9825834631919861)
[2025-02-13 03:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15][root][INFO] - Training Epoch: 2/2, step 1527/7134 completed (loss: 0.10455728322267532, acc: 0.9749582409858704)
[2025-02-13 03:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:16][root][INFO] - Training Epoch: 2/2, step 1528/7134 completed (loss: 0.04981271177530289, acc: 0.988727867603302)
[2025-02-13 03:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:16][root][INFO] - Training Epoch: 2/2, step 1529/7134 completed (loss: 0.029422152787446976, acc: 0.9892984628677368)
[2025-02-13 03:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:17][root][INFO] - Training Epoch: 2/2, step 1530/7134 completed (loss: 0.06117209047079086, acc: 0.9885277152061462)
[2025-02-13 03:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:17][root][INFO] - Training Epoch: 2/2, step 1531/7134 completed (loss: 0.16261406242847443, acc: 0.9592198729515076)
[2025-02-13 03:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:18][root][INFO] - Training Epoch: 2/2, step 1532/7134 completed (loss: 0.029829755425453186, acc: 0.989595353603363)
[2025-02-13 03:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:18][root][INFO] - Training Epoch: 2/2, step 1533/7134 completed (loss: 0.026357993483543396, acc: 0.9903181195259094)
[2025-02-13 03:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:19][root][INFO] - Training Epoch: 2/2, step 1534/7134 completed (loss: 0.0484728068113327, acc: 0.9889196753501892)
[2025-02-13 03:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:19][root][INFO] - Training Epoch: 2/2, step 1535/7134 completed (loss: 0.03137923777103424, acc: 0.9904191493988037)
[2025-02-13 03:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20][root][INFO] - Training Epoch: 2/2, step 1536/7134 completed (loss: 0.019297240301966667, acc: 0.9949430823326111)
[2025-02-13 03:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20][root][INFO] - Training Epoch: 2/2, step 1537/7134 completed (loss: 0.049982067197561264, acc: 0.9842932224273682)
[2025-02-13 03:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20][root][INFO] - Training Epoch: 2/2, step 1538/7134 completed (loss: 0.06348052620887756, acc: 0.9884763360023499)
[2025-02-13 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:21][root][INFO] - Training Epoch: 2/2, step 1539/7134 completed (loss: 0.029115445911884308, acc: 0.9894598126411438)
[2025-02-13 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:21][root][INFO] - Training Epoch: 2/2, step 1540/7134 completed (loss: 0.018544618040323257, acc: 0.9946380853652954)
[2025-02-13 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:22][root][INFO] - Training Epoch: 2/2, step 1541/7134 completed (loss: 0.04568730294704437, acc: 0.9811320900917053)
[2025-02-13 03:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:22][root][INFO] - Training Epoch: 2/2, step 1542/7134 completed (loss: 0.020899450406432152, acc: 0.9905660152435303)
[2025-02-13 03:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:23][root][INFO] - Training Epoch: 2/2, step 1543/7134 completed (loss: 0.015536394901573658, acc: 0.994535505771637)
[2025-02-13 03:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:23][root][INFO] - Training Epoch: 2/2, step 1544/7134 completed (loss: 0.01603098027408123, acc: 0.9951456189155579)
[2025-02-13 03:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24][root][INFO] - Training Epoch: 2/2, step 1545/7134 completed (loss: 0.03254437446594238, acc: 0.991391658782959)
[2025-02-13 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24][root][INFO] - Training Epoch: 2/2, step 1546/7134 completed (loss: 0.07727733254432678, acc: 0.9831697344779968)
[2025-02-13 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24][root][INFO] - Training Epoch: 2/2, step 1547/7134 completed (loss: 0.030506057664752007, acc: 0.9895522594451904)
[2025-02-13 03:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:25][root][INFO] - Training Epoch: 2/2, step 1548/7134 completed (loss: 0.030573179945349693, acc: 0.9908987283706665)
[2025-02-13 03:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:25][root][INFO] - Training Epoch: 2/2, step 1549/7134 completed (loss: 0.022304082289338112, acc: 0.9933554530143738)
[2025-02-13 03:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:26][root][INFO] - Training Epoch: 2/2, step 1550/7134 completed (loss: 0.02043292112648487, acc: 0.9911280274391174)
[2025-02-13 03:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:26][root][INFO] - Training Epoch: 2/2, step 1551/7134 completed (loss: 0.02167196199297905, acc: 0.9909090995788574)
[2025-02-13 03:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:27][root][INFO] - Training Epoch: 2/2, step 1552/7134 completed (loss: 0.03640284389257431, acc: 0.9870689511299133)
[2025-02-13 03:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:27][root][INFO] - Training Epoch: 2/2, step 1553/7134 completed (loss: 0.045365918427705765, acc: 0.9908972978591919)
[2025-02-13 03:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:28][root][INFO] - Training Epoch: 2/2, step 1554/7134 completed (loss: 0.029928620904684067, acc: 0.988726019859314)
[2025-02-13 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:28][root][INFO] - Training Epoch: 2/2, step 1555/7134 completed (loss: 0.028945861384272575, acc: 0.9899216294288635)
[2025-02-13 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29][root][INFO] - Training Epoch: 2/2, step 1556/7134 completed (loss: 0.02074330858886242, acc: 0.9917743802070618)
[2025-02-13 03:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29][root][INFO] - Training Epoch: 2/2, step 1557/7134 completed (loss: 0.010130996815860271, acc: 0.9977628588676453)
[2025-02-13 03:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29][root][INFO] - Training Epoch: 2/2, step 1558/7134 completed (loss: 0.027155762538313866, acc: 0.9901546835899353)
[2025-02-13 03:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:30][root][INFO] - Training Epoch: 2/2, step 1559/7134 completed (loss: 0.010744325816631317, acc: 0.9946452379226685)
[2025-02-13 03:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:30][root][INFO] - Training Epoch: 2/2, step 1560/7134 completed (loss: 0.023627663031220436, acc: 0.9923760890960693)
[2025-02-13 03:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:31][root][INFO] - Training Epoch: 2/2, step 1561/7134 completed (loss: 0.08111358433961868, acc: 0.9783315062522888)
[2025-02-13 03:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:31][root][INFO] - Training Epoch: 2/2, step 1562/7134 completed (loss: 0.028353983536362648, acc: 0.9924356937408447)
[2025-02-13 03:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32][root][INFO] - Training Epoch: 2/2, step 1563/7134 completed (loss: 0.045747436583042145, acc: 0.9874285459518433)
[2025-02-13 03:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32][root][INFO] - Training Epoch: 2/2, step 1564/7134 completed (loss: 0.07460323721170425, acc: 0.9752475023269653)
[2025-02-13 03:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32][root][INFO] - Training Epoch: 2/2, step 1565/7134 completed (loss: 0.04294336959719658, acc: 0.9865546226501465)
[2025-02-13 03:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33][root][INFO] - Training Epoch: 2/2, step 1566/7134 completed (loss: 0.022444793954491615, acc: 0.9931600689888)
[2025-02-13 03:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33][root][INFO] - Training Epoch: 2/2, step 1567/7134 completed (loss: 0.038527294993400574, acc: 0.9925187230110168)
[2025-02-13 03:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:34][root][INFO] - Training Epoch: 2/2, step 1568/7134 completed (loss: 0.04701634868979454, acc: 0.9834319353103638)
[2025-02-13 03:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:34][root][INFO] - Training Epoch: 2/2, step 1569/7134 completed (loss: 0.05942041799426079, acc: 0.9853801131248474)
[2025-02-13 03:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:35][root][INFO] - Training Epoch: 2/2, step 1570/7134 completed (loss: 0.08742620795965195, acc: 0.9823594093322754)
[2025-02-13 03:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:35][root][INFO] - Training Epoch: 2/2, step 1571/7134 completed (loss: 0.04349209740757942, acc: 0.9893238544464111)
[2025-02-13 03:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:36][root][INFO] - Training Epoch: 2/2, step 1572/7134 completed (loss: 0.03394880145788193, acc: 0.9905771613121033)
[2025-02-13 03:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:36][root][INFO] - Training Epoch: 2/2, step 1573/7134 completed (loss: 0.01399064902216196, acc: 0.994194507598877)
[2025-02-13 03:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:37][root][INFO] - Training Epoch: 2/2, step 1574/7134 completed (loss: 0.04646158590912819, acc: 0.9901531934738159)
[2025-02-13 03:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:37][root][INFO] - Training Epoch: 2/2, step 1575/7134 completed (loss: 0.047657500952482224, acc: 0.9889135360717773)
[2025-02-13 03:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38][root][INFO] - Training Epoch: 2/2, step 1576/7134 completed (loss: 0.03976988047361374, acc: 0.9918224215507507)
[2025-02-13 03:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38][root][INFO] - Training Epoch: 2/2, step 1577/7134 completed (loss: 0.0441751554608345, acc: 0.9882214665412903)
[2025-02-13 03:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38][root][INFO] - Training Epoch: 2/2, step 1578/7134 completed (loss: 0.033635351806879044, acc: 0.9878214001655579)
[2025-02-13 03:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:39][root][INFO] - Training Epoch: 2/2, step 1579/7134 completed (loss: 0.037882234901189804, acc: 0.9884792566299438)
[2025-02-13 03:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:39][root][INFO] - Training Epoch: 2/2, step 1580/7134 completed (loss: 0.06825641542673111, acc: 0.9801762104034424)
[2025-02-13 03:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:40][root][INFO] - Training Epoch: 2/2, step 1581/7134 completed (loss: 0.05939076095819473, acc: 0.9830247163772583)
[2025-02-13 03:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:40][root][INFO] - Training Epoch: 2/2, step 1582/7134 completed (loss: 0.04105929657816887, acc: 0.9906103014945984)
[2025-02-13 03:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41][root][INFO] - Training Epoch: 2/2, step 1583/7134 completed (loss: 0.021005406975746155, acc: 0.9942693114280701)
[2025-02-13 03:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41][root][INFO] - Training Epoch: 2/2, step 1584/7134 completed (loss: 0.03524433448910713, acc: 0.9903640151023865)
[2025-02-13 03:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41][root][INFO] - Training Epoch: 2/2, step 1585/7134 completed (loss: 0.020621662959456444, acc: 0.9917550086975098)
[2025-02-13 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:42][root][INFO] - Training Epoch: 2/2, step 1586/7134 completed (loss: 0.05284114181995392, acc: 0.9881235361099243)
[2025-02-13 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:42][root][INFO] - Training Epoch: 2/2, step 1587/7134 completed (loss: 0.03245527297258377, acc: 0.9905808568000793)
[2025-02-13 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:43][root][INFO] - Training Epoch: 2/2, step 1588/7134 completed (loss: 0.06662391871213913, acc: 0.9866844415664673)
[2025-02-13 03:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:43][root][INFO] - Training Epoch: 2/2, step 1589/7134 completed (loss: 0.0445992648601532, acc: 0.991183876991272)
[2025-02-13 03:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:44][root][INFO] - Training Epoch: 2/2, step 1590/7134 completed (loss: 0.029598424211144447, acc: 0.9909365773200989)
[2025-02-13 03:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:44][root][INFO] - Training Epoch: 2/2, step 1591/7134 completed (loss: 0.047255150973796844, acc: 0.989847719669342)
[2025-02-13 03:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45][root][INFO] - Training Epoch: 2/2, step 1592/7134 completed (loss: 0.022734425961971283, acc: 0.9928673505783081)
[2025-02-13 03:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45][root][INFO] - Training Epoch: 2/2, step 1593/7134 completed (loss: 0.0423445999622345, acc: 0.9873563051223755)
[2025-02-13 03:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45][root][INFO] - Training Epoch: 2/2, step 1594/7134 completed (loss: 0.023364368826150894, acc: 0.9917355179786682)
[2025-02-13 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:46][root][INFO] - Training Epoch: 2/2, step 1595/7134 completed (loss: 0.019698400050401688, acc: 0.9964157938957214)
[2025-02-13 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:46][root][INFO] - Training Epoch: 2/2, step 1596/7134 completed (loss: 0.05848462134599686, acc: 0.9889763593673706)
[2025-02-13 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:47][root][INFO] - Training Epoch: 2/2, step 1597/7134 completed (loss: 0.01775314286351204, acc: 0.9957020282745361)
[2025-02-13 03:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:47][root][INFO] - Training Epoch: 2/2, step 1598/7134 completed (loss: 0.07787442952394485, acc: 0.9800398945808411)
[2025-02-13 03:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48][root][INFO] - Training Epoch: 2/2, step 1599/7134 completed (loss: 0.09373629838228226, acc: 0.9785124063491821)
[2025-02-13 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48][root][INFO] - Training Epoch: 2/2, step 1600/7134 completed (loss: 0.046635523438453674, acc: 0.9896073937416077)
[2025-02-13 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48][root][INFO] - Training Epoch: 2/2, step 1601/7134 completed (loss: 0.024417735636234283, acc: 0.9917469024658203)
[2025-02-13 03:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:49][root][INFO] - Training Epoch: 2/2, step 1602/7134 completed (loss: 0.04452759400010109, acc: 0.9924585223197937)
[2025-02-13 03:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:49][root][INFO] - Training Epoch: 2/2, step 1603/7134 completed (loss: 0.0454946905374527, acc: 0.9897698163986206)
[2025-02-13 03:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:50][root][INFO] - Training Epoch: 2/2, step 1604/7134 completed (loss: 0.019654862582683563, acc: 0.9960212111473083)
[2025-02-13 03:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:50][root][INFO] - Training Epoch: 2/2, step 1605/7134 completed (loss: 0.037508606910705566, acc: 0.9921773076057434)
[2025-02-13 03:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51][root][INFO] - Training Epoch: 2/2, step 1606/7134 completed (loss: 0.017878275364637375, acc: 0.9954057931900024)
[2025-02-13 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51][root][INFO] - Training Epoch: 2/2, step 1607/7134 completed (loss: 0.016811687499284744, acc: 0.9959266781806946)
[2025-02-13 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51][root][INFO] - Training Epoch: 2/2, step 1608/7134 completed (loss: 0.010929496958851814, acc: 0.9962825179100037)
[2025-02-13 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:52][root][INFO] - Training Epoch: 2/2, step 1609/7134 completed (loss: 0.03495177999138832, acc: 0.9937694668769836)
[2025-02-13 03:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:52][root][INFO] - Training Epoch: 2/2, step 1610/7134 completed (loss: 0.047068413347005844, acc: 0.9933221936225891)
[2025-02-13 03:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53][root][INFO] - Training Epoch: 2/2, step 1611/7134 completed (loss: 0.05906464159488678, acc: 0.9867724776268005)
[2025-02-13 03:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53][root][INFO] - Training Epoch: 2/2, step 1612/7134 completed (loss: 0.06328064203262329, acc: 0.9847328066825867)
[2025-02-13 03:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53][root][INFO] - Training Epoch: 2/2, step 1613/7134 completed (loss: 0.06302111595869064, acc: 0.9841897487640381)
[2025-02-13 03:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:54][root][INFO] - Training Epoch: 2/2, step 1614/7134 completed (loss: 0.039918202906847, acc: 0.9921362996101379)
[2025-02-13 03:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:54][root][INFO] - Training Epoch: 2/2, step 1615/7134 completed (loss: 0.04004542529582977, acc: 0.9865771532058716)
[2025-02-13 03:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:55][root][INFO] - Training Epoch: 2/2, step 1616/7134 completed (loss: 0.021386345848441124, acc: 0.9942280054092407)
[2025-02-13 03:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:55][root][INFO] - Training Epoch: 2/2, step 1617/7134 completed (loss: 0.046527232974767685, acc: 0.9890795350074768)
[2025-02-13 03:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:56][root][INFO] - Training Epoch: 2/2, step 1618/7134 completed (loss: 0.06493864953517914, acc: 0.9842105507850647)
[2025-02-13 03:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:56][root][INFO] - Training Epoch: 2/2, step 1619/7134 completed (loss: 0.03202349320054054, acc: 0.9931880235671997)
[2025-02-13 03:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57][root][INFO] - Training Epoch: 2/2, step 1620/7134 completed (loss: 0.059719644486904144, acc: 0.9803328514099121)
[2025-02-13 03:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57][root][INFO] - Training Epoch: 2/2, step 1621/7134 completed (loss: 0.08021651953458786, acc: 0.9716216325759888)
[2025-02-13 03:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57][root][INFO] - Training Epoch: 2/2, step 1622/7134 completed (loss: 0.04900763928890228, acc: 0.9874652028083801)
[2025-02-13 03:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:58][root][INFO] - Training Epoch: 2/2, step 1623/7134 completed (loss: 0.04391086846590042, acc: 0.9873595237731934)
[2025-02-13 03:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:58][root][INFO] - Training Epoch: 2/2, step 1624/7134 completed (loss: 0.04962419345974922, acc: 0.9897210001945496)
[2025-02-13 03:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:59][root][INFO] - Training Epoch: 2/2, step 1625/7134 completed (loss: 0.02475390024483204, acc: 0.9914529919624329)
[2025-02-13 03:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:59][root][INFO] - Training Epoch: 2/2, step 1626/7134 completed (loss: 0.09376047551631927, acc: 0.9814814925193787)
[2025-02-13 03:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00][root][INFO] - Training Epoch: 2/2, step 1627/7134 completed (loss: 0.022258268669247627, acc: 0.9908758997917175)
[2025-02-13 03:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00][root][INFO] - Training Epoch: 2/2, step 1628/7134 completed (loss: 0.009816402569413185, acc: 0.9970845580101013)
[2025-02-13 03:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00][root][INFO] - Training Epoch: 2/2, step 1629/7134 completed (loss: 0.0304206945002079, acc: 0.9872727394104004)
[2025-02-13 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:01][root][INFO] - Training Epoch: 2/2, step 1630/7134 completed (loss: 0.0229368694126606, acc: 0.9940740466117859)
[2025-02-13 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:01][root][INFO] - Training Epoch: 2/2, step 1631/7134 completed (loss: 0.04235062375664711, acc: 0.9871382713317871)
[2025-02-13 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02][root][INFO] - Training Epoch: 2/2, step 1632/7134 completed (loss: 0.015384756028652191, acc: 0.9966832399368286)
[2025-02-13 03:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02][root][INFO] - Training Epoch: 2/2, step 1633/7134 completed (loss: 0.015664076432585716, acc: 0.9952903985977173)
[2025-02-13 03:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02][root][INFO] - Training Epoch: 2/2, step 1634/7134 completed (loss: 0.013694516383111477, acc: 0.9969372153282166)
[2025-02-13 03:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:03][root][INFO] - Training Epoch: 2/2, step 1635/7134 completed (loss: 0.02188437432050705, acc: 0.9918533563613892)
[2025-02-13 03:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:03][root][INFO] - Training Epoch: 2/2, step 1636/7134 completed (loss: 0.03463929891586304, acc: 0.9885714054107666)
[2025-02-13 03:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:04][root][INFO] - Training Epoch: 2/2, step 1637/7134 completed (loss: 0.019412025809288025, acc: 0.9971751570701599)
[2025-02-13 03:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:04][root][INFO] - Training Epoch: 2/2, step 1638/7134 completed (loss: 0.06208045408129692, acc: 0.9830220937728882)
[2025-02-13 03:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05][root][INFO] - Training Epoch: 2/2, step 1639/7134 completed (loss: 0.11159838736057281, acc: 0.9748892188072205)
[2025-02-13 03:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05][root][INFO] - Training Epoch: 2/2, step 1640/7134 completed (loss: 0.04512782394886017, acc: 0.9862805008888245)
[2025-02-13 03:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05][root][INFO] - Training Epoch: 2/2, step 1641/7134 completed (loss: 0.009742219932377338, acc: 0.9985632300376892)
[2025-02-13 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:06][root][INFO] - Training Epoch: 2/2, step 1642/7134 completed (loss: 0.06183731555938721, acc: 0.9789473414421082)
[2025-02-13 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:06][root][INFO] - Training Epoch: 2/2, step 1643/7134 completed (loss: 0.08921755105257034, acc: 0.9774696826934814)
[2025-02-13 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:07][root][INFO] - Training Epoch: 2/2, step 1644/7134 completed (loss: 0.046107061207294464, acc: 0.9895697236061096)
[2025-02-13 03:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:07][root][INFO] - Training Epoch: 2/2, step 1645/7134 completed (loss: 0.06430716812610626, acc: 0.9841954112052917)
[2025-02-13 03:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:08][root][INFO] - Training Epoch: 2/2, step 1646/7134 completed (loss: 0.06267556548118591, acc: 0.9840255379676819)
[2025-02-13 03:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:08][root][INFO] - Training Epoch: 2/2, step 1647/7134 completed (loss: 0.03033115714788437, acc: 0.9899216294288635)
[2025-02-13 03:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:09][root][INFO] - Training Epoch: 2/2, step 1648/7134 completed (loss: 0.059578683227300644, acc: 0.9840425252914429)
[2025-02-13 03:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:09][root][INFO] - Training Epoch: 2/2, step 1649/7134 completed (loss: 0.027649257332086563, acc: 0.9922308325767517)
[2025-02-13 03:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10][root][INFO] - Training Epoch: 2/2, step 1650/7134 completed (loss: 0.03030642308294773, acc: 0.9871134161949158)
[2025-02-13 03:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10][root][INFO] - Training Epoch: 2/2, step 1651/7134 completed (loss: 0.0540238656103611, acc: 0.9878318309783936)
[2025-02-13 03:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10][root][INFO] - Training Epoch: 2/2, step 1652/7134 completed (loss: 0.027579061686992645, acc: 0.994547426700592)
[2025-02-13 03:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:11][root][INFO] - Training Epoch: 2/2, step 1653/7134 completed (loss: 0.034071460366249084, acc: 0.9875466823577881)
[2025-02-13 03:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:11][root][INFO] - Training Epoch: 2/2, step 1654/7134 completed (loss: 0.04139253869652748, acc: 0.9898062944412231)
[2025-02-13 03:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:12][root][INFO] - Training Epoch: 2/2, step 1655/7134 completed (loss: 0.04861203581094742, acc: 0.9900867342948914)
[2025-02-13 03:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:12][root][INFO] - Training Epoch: 2/2, step 1656/7134 completed (loss: 0.04858371987938881, acc: 0.9887387156486511)
[2025-02-13 03:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:13][root][INFO] - Training Epoch: 2/2, step 1657/7134 completed (loss: 0.0420490987598896, acc: 0.9881278276443481)
[2025-02-13 03:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:13][root][INFO] - Training Epoch: 2/2, step 1658/7134 completed (loss: 0.018275799229741096, acc: 0.9938775300979614)
[2025-02-13 03:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:14][root][INFO] - Training Epoch: 2/2, step 1659/7134 completed (loss: 0.03971817344427109, acc: 0.9857819676399231)
[2025-02-13 03:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:14][root][INFO] - Training Epoch: 2/2, step 1660/7134 completed (loss: 0.031079689040780067, acc: 0.9939613342285156)
[2025-02-13 03:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:15][root][INFO] - Training Epoch: 2/2, step 1661/7134 completed (loss: 0.04625438153743744, acc: 0.9835886359214783)
[2025-02-13 03:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:15][root][INFO] - Training Epoch: 2/2, step 1662/7134 completed (loss: 0.03956087678670883, acc: 0.9915164113044739)
[2025-02-13 03:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:16][root][INFO] - Training Epoch: 2/2, step 1663/7134 completed (loss: 0.042730171233415604, acc: 0.990750253200531)
[2025-02-13 03:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:16][root][INFO] - Training Epoch: 2/2, step 1664/7134 completed (loss: 0.02384342812001705, acc: 0.9916567206382751)
[2025-02-13 03:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:17][root][INFO] - Training Epoch: 2/2, step 1665/7134 completed (loss: 0.027802210301160812, acc: 0.9884892106056213)
[2025-02-13 03:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:17][root][INFO] - Training Epoch: 2/2, step 1666/7134 completed (loss: 0.019027769565582275, acc: 0.9960079789161682)
[2025-02-13 03:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18][root][INFO] - Training Epoch: 2/2, step 1667/7134 completed (loss: 0.03424056991934776, acc: 0.9881235361099243)
[2025-02-13 03:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18][root][INFO] - Training Epoch: 2/2, step 1668/7134 completed (loss: 0.016335057094693184, acc: 0.9947478771209717)
[2025-02-13 03:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:19][root][INFO] - Training Epoch: 2/2, step 1669/7134 completed (loss: 0.02088128589093685, acc: 0.9934086799621582)
[2025-02-13 03:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:19][root][INFO] - Training Epoch: 2/2, step 1670/7134 completed (loss: 0.05672694370150566, acc: 0.9866412281990051)
[2025-02-13 03:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:20][root][INFO] - Training Epoch: 2/2, step 1671/7134 completed (loss: 0.04807589575648308, acc: 0.9880525469779968)
[2025-02-13 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:20][root][INFO] - Training Epoch: 2/2, step 1672/7134 completed (loss: 0.050380632281303406, acc: 0.9867197871208191)
[2025-02-13 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21][root][INFO] - Training Epoch: 2/2, step 1673/7134 completed (loss: 0.04089678078889847, acc: 0.9932795763015747)
[2025-02-13 03:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21][root][INFO] - Training Epoch: 2/2, step 1674/7134 completed (loss: 0.046240370720624924, acc: 0.9902234673500061)
[2025-02-13 03:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21][root][INFO] - Training Epoch: 2/2, step 1675/7134 completed (loss: 0.07986637204885483, acc: 0.9836734533309937)
[2025-02-13 03:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22][root][INFO] - Training Epoch: 2/2, step 1676/7134 completed (loss: 0.022791437804698944, acc: 0.9930939078330994)
[2025-02-13 03:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22][root][INFO] - Training Epoch: 2/2, step 1677/7134 completed (loss: 0.07321200519800186, acc: 0.9819967150688171)
[2025-02-13 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:23][root][INFO] - Training Epoch: 2/2, step 1678/7134 completed (loss: 0.019111761823296547, acc: 0.9931507110595703)
[2025-02-13 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:23][root][INFO] - Training Epoch: 2/2, step 1679/7134 completed (loss: 0.05005159229040146, acc: 0.985358715057373)
[2025-02-13 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:24][root][INFO] - Training Epoch: 2/2, step 1680/7134 completed (loss: 0.035198722034692764, acc: 0.9932975769042969)
[2025-02-13 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:24][root][INFO] - Training Epoch: 2/2, step 1681/7134 completed (loss: 0.03486018627882004, acc: 0.9928977489471436)
[2025-02-13 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25][root][INFO] - Training Epoch: 2/2, step 1682/7134 completed (loss: 0.06817349046468735, acc: 0.9798271059989929)
[2025-02-13 03:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25][root][INFO] - Training Epoch: 2/2, step 1683/7134 completed (loss: 0.027392594143748283, acc: 0.991909384727478)
[2025-02-13 03:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25][root][INFO] - Training Epoch: 2/2, step 1684/7134 completed (loss: 0.038010384887456894, acc: 0.9863013625144958)
[2025-02-13 03:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:26][root][INFO] - Training Epoch: 2/2, step 1685/7134 completed (loss: 0.043910201638936996, acc: 0.9874826073646545)
[2025-02-13 03:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:26][root][INFO] - Training Epoch: 2/2, step 1686/7134 completed (loss: 0.03340110555291176, acc: 0.9881756901741028)
[2025-02-13 03:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27][root][INFO] - Training Epoch: 2/2, step 1687/7134 completed (loss: 0.03142046555876732, acc: 0.9885931611061096)
[2025-02-13 03:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27][root][INFO] - Training Epoch: 2/2, step 1688/7134 completed (loss: 0.04726453125476837, acc: 0.9907264113426208)
[2025-02-13 03:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28][root][INFO] - Training Epoch: 2/2, step 1689/7134 completed (loss: 0.02708420157432556, acc: 0.9927849769592285)
[2025-02-13 03:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28][root][INFO] - Training Epoch: 2/2, step 1690/7134 completed (loss: 0.03103557787835598, acc: 0.9939024448394775)
[2025-02-13 03:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28][root][INFO] - Training Epoch: 2/2, step 1691/7134 completed (loss: 0.0079396553337574, acc: 0.996688723564148)
[2025-02-13 03:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:29][root][INFO] - Training Epoch: 2/2, step 1692/7134 completed (loss: 0.04732547700405121, acc: 0.9831365942955017)
[2025-02-13 03:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:29][root][INFO] - Training Epoch: 2/2, step 1693/7134 completed (loss: 0.031577229499816895, acc: 0.992514967918396)
[2025-02-13 03:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30][root][INFO] - Training Epoch: 2/2, step 1694/7134 completed (loss: 0.033907752484083176, acc: 0.9874804615974426)
[2025-02-13 03:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30][root][INFO] - Training Epoch: 2/2, step 1695/7134 completed (loss: 0.007905997335910797, acc: 1.0)
[2025-02-13 03:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30][root][INFO] - Training Epoch: 2/2, step 1696/7134 completed (loss: 0.04971058666706085, acc: 0.9849246144294739)
[2025-02-13 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:31][root][INFO] - Training Epoch: 2/2, step 1697/7134 completed (loss: 0.03185364603996277, acc: 0.992175281047821)
[2025-02-13 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:31][root][INFO] - Training Epoch: 2/2, step 1698/7134 completed (loss: 0.01875733956694603, acc: 0.9952977895736694)
[2025-02-13 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:32][root][INFO] - Training Epoch: 2/2, step 1699/7134 completed (loss: 0.025730522349476814, acc: 0.987889289855957)
[2025-02-13 03:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:32][root][INFO] - Training Epoch: 2/2, step 1700/7134 completed (loss: 0.03724595531821251, acc: 0.9838998317718506)
[2025-02-13 03:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33][root][INFO] - Training Epoch: 2/2, step 1701/7134 completed (loss: 0.041111186146736145, acc: 0.9847009778022766)
[2025-02-13 03:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33][root][INFO] - Training Epoch: 2/2, step 1702/7134 completed (loss: 0.067762091755867, acc: 0.9838509559631348)
[2025-02-13 03:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33][root][INFO] - Training Epoch: 2/2, step 1703/7134 completed (loss: 0.05679692327976227, acc: 0.980663001537323)
[2025-02-13 03:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:34][root][INFO] - Training Epoch: 2/2, step 1704/7134 completed (loss: 0.09356848150491714, acc: 0.9741848111152649)
[2025-02-13 03:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:34][root][INFO] - Training Epoch: 2/2, step 1705/7134 completed (loss: 0.050371114164590836, acc: 0.9842209219932556)
[2025-02-13 03:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35][root][INFO] - Training Epoch: 2/2, step 1706/7134 completed (loss: 0.03056417778134346, acc: 0.9848484992980957)
[2025-02-13 03:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35][root][INFO] - Training Epoch: 2/2, step 1707/7134 completed (loss: 0.03360632061958313, acc: 0.9957716464996338)
[2025-02-13 03:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35][root][INFO] - Training Epoch: 2/2, step 1708/7134 completed (loss: 0.04054667800664902, acc: 0.9853658676147461)
[2025-02-13 03:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:36][root][INFO] - Training Epoch: 2/2, step 1709/7134 completed (loss: 0.041958682239055634, acc: 0.9810344576835632)
[2025-02-13 03:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:36][root][INFO] - Training Epoch: 2/2, step 1710/7134 completed (loss: 0.046852052211761475, acc: 0.9856321811676025)
[2025-02-13 03:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:37][root][INFO] - Training Epoch: 2/2, step 1711/7134 completed (loss: 0.04381968080997467, acc: 0.9879102110862732)
[2025-02-13 03:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:37][root][INFO] - Training Epoch: 2/2, step 1712/7134 completed (loss: 0.0788818746805191, acc: 0.9719298481941223)
[2025-02-13 03:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38][root][INFO] - Training Epoch: 2/2, step 1713/7134 completed (loss: 0.033683955669403076, acc: 0.9920159578323364)
[2025-02-13 03:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38][root][INFO] - Training Epoch: 2/2, step 1714/7134 completed (loss: 0.057495035231113434, acc: 0.9835526347160339)
[2025-02-13 03:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38][root][INFO] - Training Epoch: 2/2, step 1715/7134 completed (loss: 0.02641269564628601, acc: 0.9959431886672974)
[2025-02-13 03:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:39][root][INFO] - Training Epoch: 2/2, step 1716/7134 completed (loss: 0.06518890708684921, acc: 0.9783333539962769)
[2025-02-13 03:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:39][root][INFO] - Training Epoch: 2/2, step 1717/7134 completed (loss: 0.026664400473237038, acc: 0.9883268475532532)
[2025-02-13 03:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40][root][INFO] - Training Epoch: 2/2, step 1718/7134 completed (loss: 0.057496219873428345, acc: 0.9821826219558716)
[2025-02-13 03:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40][root][INFO] - Training Epoch: 2/2, step 1719/7134 completed (loss: 0.08186682313680649, acc: 0.9876712560653687)
[2025-02-13 03:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40][root][INFO] - Training Epoch: 2/2, step 1720/7134 completed (loss: 0.06280060857534409, acc: 0.9710144996643066)
[2025-02-13 03:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:41][root][INFO] - Training Epoch: 2/2, step 1721/7134 completed (loss: 0.035183876752853394, acc: 0.9865092635154724)
[2025-02-13 03:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:41][root][INFO] - Training Epoch: 2/2, step 1722/7134 completed (loss: 0.014648075215518475, acc: 0.99210524559021)
[2025-02-13 03:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42][root][INFO] - Training Epoch: 2/2, step 1723/7134 completed (loss: 0.08526017516851425, acc: 0.9836065769195557)
[2025-02-13 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42][root][INFO] - Training Epoch: 2/2, step 1724/7134 completed (loss: 0.07416193932294846, acc: 0.9748822450637817)
[2025-02-13 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42][root][INFO] - Training Epoch: 2/2, step 1725/7134 completed (loss: 0.019196931272745132, acc: 0.9935691356658936)
[2025-02-13 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43][root][INFO] - Training Epoch: 2/2, step 1726/7134 completed (loss: 0.036766767501831055, acc: 0.9843049049377441)
[2025-02-13 03:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43][root][INFO] - Training Epoch: 2/2, step 1727/7134 completed (loss: 0.026445560157299042, acc: 0.9927007555961609)
[2025-02-13 03:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43][root][INFO] - Training Epoch: 2/2, step 1728/7134 completed (loss: 0.030799193307757378, acc: 0.9917218685150146)
[2025-02-13 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:44][root][INFO] - Training Epoch: 2/2, step 1729/7134 completed (loss: 0.05111170932650566, acc: 0.9877800345420837)
[2025-02-13 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:44][root][INFO] - Training Epoch: 2/2, step 1730/7134 completed (loss: 0.0209000576287508, acc: 0.9935759902000427)
[2025-02-13 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45][root][INFO] - Training Epoch: 2/2, step 1731/7134 completed (loss: 0.08391152322292328, acc: 0.9884393215179443)
[2025-02-13 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45][root][INFO] - Training Epoch: 2/2, step 1732/7134 completed (loss: 0.04379149153828621, acc: 0.987679660320282)
[2025-02-13 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45][root][INFO] - Training Epoch: 2/2, step 1733/7134 completed (loss: 0.04375796765089035, acc: 0.9835391044616699)
[2025-02-13 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:46][root][INFO] - Training Epoch: 2/2, step 1734/7134 completed (loss: 0.05317457765340805, acc: 0.9877150058746338)
[2025-02-13 03:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:46][root][INFO] - Training Epoch: 2/2, step 1735/7134 completed (loss: 0.016644487157464027, acc: 0.9957924485206604)
[2025-02-13 03:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:47][root][INFO] - Training Epoch: 2/2, step 1736/7134 completed (loss: 0.04072422534227371, acc: 0.9842424392700195)
[2025-02-13 03:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:47][root][INFO] - Training Epoch: 2/2, step 1737/7134 completed (loss: 0.034686412662267685, acc: 0.9943820238113403)
[2025-02-13 03:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48][root][INFO] - Training Epoch: 2/2, step 1738/7134 completed (loss: 0.018287671729922295, acc: 0.9915561079978943)
[2025-02-13 03:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48][root][INFO] - Training Epoch: 2/2, step 1739/7134 completed (loss: 0.03727811947464943, acc: 0.9820359349250793)
[2025-02-13 03:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48][root][INFO] - Training Epoch: 2/2, step 1740/7134 completed (loss: 0.011885382235050201, acc: 0.9970760345458984)
[2025-02-13 03:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:49][root][INFO] - Training Epoch: 2/2, step 1741/7134 completed (loss: 0.04508895426988602, acc: 0.9854497313499451)
[2025-02-13 03:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:49][root][INFO] - Training Epoch: 2/2, step 1742/7134 completed (loss: 0.04169464111328125, acc: 0.9843971729278564)
[2025-02-13 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:50][root][INFO] - Training Epoch: 2/2, step 1743/7134 completed (loss: 0.051650628447532654, acc: 0.9831932783126831)
[2025-02-13 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:50][root][INFO] - Training Epoch: 2/2, step 1744/7134 completed (loss: 0.035386938601732254, acc: 0.9951140284538269)
[2025-02-13 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:51][root][INFO] - Training Epoch: 2/2, step 1745/7134 completed (loss: 0.03392458334565163, acc: 0.9887217879295349)
[2025-02-13 03:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:51][root][INFO] - Training Epoch: 2/2, step 1746/7134 completed (loss: 0.01562188658863306, acc: 0.9956395626068115)
[2025-02-13 03:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52][root][INFO] - Training Epoch: 2/2, step 1747/7134 completed (loss: 0.039947930723428726, acc: 0.987484335899353)
[2025-02-13 03:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52][root][INFO] - Training Epoch: 2/2, step 1748/7134 completed (loss: 0.030635956674814224, acc: 0.9913151264190674)
[2025-02-13 03:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53][root][INFO] - Training Epoch: 2/2, step 1749/7134 completed (loss: 0.03849203884601593, acc: 0.9931895732879639)
[2025-02-13 03:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53][root][INFO] - Training Epoch: 2/2, step 1750/7134 completed (loss: 0.028327416628599167, acc: 0.9928057789802551)
[2025-02-13 03:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53][root][INFO] - Training Epoch: 2/2, step 1751/7134 completed (loss: 0.050774313509464264, acc: 0.9806060791015625)
[2025-02-13 03:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:54][root][INFO] - Training Epoch: 2/2, step 1752/7134 completed (loss: 0.05826118215918541, acc: 0.9819587469100952)
[2025-02-13 03:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:54][root][INFO] - Training Epoch: 2/2, step 1753/7134 completed (loss: 0.049684032797813416, acc: 0.9908972978591919)
[2025-02-13 03:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:55][root][INFO] - Training Epoch: 2/2, step 1754/7134 completed (loss: 0.0474756620824337, acc: 0.9884726405143738)
[2025-02-13 03:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:55][root][INFO] - Training Epoch: 2/2, step 1755/7134 completed (loss: 0.01473518367856741, acc: 0.9947090148925781)
[2025-02-13 03:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:56][root][INFO] - Training Epoch: 2/2, step 1756/7134 completed (loss: 0.045736756175756454, acc: 0.9882965087890625)
[2025-02-13 03:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:56][root][INFO] - Training Epoch: 2/2, step 1757/7134 completed (loss: 0.051392845809459686, acc: 0.9833610653877258)
[2025-02-13 03:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57][root][INFO] - Training Epoch: 2/2, step 1758/7134 completed (loss: 0.02132953703403473, acc: 0.993686854839325)
[2025-02-13 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57][root][INFO] - Training Epoch: 2/2, step 1759/7134 completed (loss: 0.03554091602563858, acc: 0.9888734221458435)
[2025-02-13 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57][root][INFO] - Training Epoch: 2/2, step 1760/7134 completed (loss: 0.025440016761422157, acc: 0.9915966391563416)
[2025-02-13 03:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:58][root][INFO] - Training Epoch: 2/2, step 1761/7134 completed (loss: 0.02181827276945114, acc: 0.9914772510528564)
[2025-02-13 03:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:58][root][INFO] - Training Epoch: 2/2, step 1762/7134 completed (loss: 0.011160042136907578, acc: 1.0)
[2025-02-13 03:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59][root][INFO] - Training Epoch: 2/2, step 1763/7134 completed (loss: 0.03898054361343384, acc: 0.9871794581413269)
[2025-02-13 03:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59][root][INFO] - Training Epoch: 2/2, step 1764/7134 completed (loss: 0.01673811487853527, acc: 0.9925650358200073)
[2025-02-13 03:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59][root][INFO] - Training Epoch: 2/2, step 1765/7134 completed (loss: 0.02087073214352131, acc: 0.9953703880310059)
[2025-02-13 03:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:00][root][INFO] - Training Epoch: 2/2, step 1766/7134 completed (loss: 0.01666657067835331, acc: 0.9956011772155762)
[2025-02-13 03:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:00][root][INFO] - Training Epoch: 2/2, step 1767/7134 completed (loss: 0.051193349063396454, acc: 0.985855758190155)
[2025-02-13 03:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:01][root][INFO] - Training Epoch: 2/2, step 1768/7134 completed (loss: 0.030947713181376457, acc: 0.9884393215179443)
[2025-02-13 03:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:01][root][INFO] - Training Epoch: 2/2, step 1769/7134 completed (loss: 0.032667770981788635, acc: 0.9897058606147766)
[2025-02-13 03:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02][root][INFO] - Training Epoch: 2/2, step 1770/7134 completed (loss: 0.0185813307762146, acc: 0.9908952713012695)
[2025-02-13 03:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02][root][INFO] - Training Epoch: 2/2, step 1771/7134 completed (loss: 0.013376310467720032, acc: 0.998603343963623)
[2025-02-13 03:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02][root][INFO] - Training Epoch: 2/2, step 1772/7134 completed (loss: 0.021920453757047653, acc: 0.9916247725486755)
[2025-02-13 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:03][root][INFO] - Training Epoch: 2/2, step 1773/7134 completed (loss: 0.012239097617566586, acc: 0.9970930218696594)
[2025-02-13 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:03][root][INFO] - Training Epoch: 2/2, step 1774/7134 completed (loss: 0.008219655603170395, acc: 0.9966777563095093)
[2025-02-13 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04][root][INFO] - Training Epoch: 2/2, step 1775/7134 completed (loss: 0.011038237251341343, acc: 0.996874988079071)
[2025-02-13 03:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04][root][INFO] - Training Epoch: 2/2, step 1776/7134 completed (loss: 0.037702515721321106, acc: 0.9895209670066833)
[2025-02-13 03:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04][root][INFO] - Training Epoch: 2/2, step 1777/7134 completed (loss: 0.012831049971282482, acc: 0.9970414042472839)
[2025-02-13 03:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:05][root][INFO] - Training Epoch: 2/2, step 1778/7134 completed (loss: 0.04389689117670059, acc: 0.9926035404205322)
[2025-02-13 03:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:05][root][INFO] - Training Epoch: 2/2, step 1779/7134 completed (loss: 0.0166593249887228, acc: 0.9959595799446106)
[2025-02-13 03:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:06][root][INFO] - Training Epoch: 2/2, step 1780/7134 completed (loss: 0.030036576092243195, acc: 0.9957567453384399)
[2025-02-13 03:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:06][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0528, device='cuda:0') eval_epoch_loss=tensor(0.0515, device='cuda:0') eval_epoch_acc=tensor(0.9865, device='cuda:0')
[2025-02-13 03:47:06][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:47:06][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:47:07][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_1781_loss_0.05146584287285805/model.pt
[2025-02-13 03:47:07][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:47:07][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.05146584287285805
[2025-02-13 03:47:07][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9864792227745056
[2025-02-13 03:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:07][root][INFO] - Training Epoch: 2/2, step 1781/7134 completed (loss: 0.014243446290493011, acc: 0.9938931465148926)
[2025-02-13 03:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:08][root][INFO] - Training Epoch: 2/2, step 1782/7134 completed (loss: 0.012709807604551315, acc: 0.9939117431640625)
[2025-02-13 03:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:08][root][INFO] - Training Epoch: 2/2, step 1783/7134 completed (loss: 0.037407759577035904, acc: 0.9905405640602112)
[2025-02-13 03:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09][root][INFO] - Training Epoch: 2/2, step 1784/7134 completed (loss: 0.0186893492937088, acc: 0.9941349029541016)
[2025-02-13 03:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09][root][INFO] - Training Epoch: 2/2, step 1785/7134 completed (loss: 0.021621059626340866, acc: 0.9944979548454285)
[2025-02-13 03:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09][root][INFO] - Training Epoch: 2/2, step 1786/7134 completed (loss: 0.02803698368370533, acc: 0.9937597513198853)
[2025-02-13 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:10][root][INFO] - Training Epoch: 2/2, step 1787/7134 completed (loss: 0.018795359879732132, acc: 0.9946428537368774)
[2025-02-13 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:10][root][INFO] - Training Epoch: 2/2, step 1788/7134 completed (loss: 0.02645554021000862, acc: 0.9901800155639648)
[2025-02-13 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11][root][INFO] - Training Epoch: 2/2, step 1789/7134 completed (loss: 0.01447168830782175, acc: 0.996927797794342)
[2025-02-13 03:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11][root][INFO] - Training Epoch: 2/2, step 1790/7134 completed (loss: 0.1276054084300995, acc: 0.9822866320610046)
[2025-02-13 03:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11][root][INFO] - Training Epoch: 2/2, step 1791/7134 completed (loss: 0.0989246666431427, acc: 0.9814550876617432)
[2025-02-13 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:12][root][INFO] - Training Epoch: 2/2, step 1792/7134 completed (loss: 0.15070433914661407, acc: 0.9756097793579102)
[2025-02-13 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:12][root][INFO] - Training Epoch: 2/2, step 1793/7134 completed (loss: 0.11601582169532776, acc: 0.9743243455886841)
[2025-02-13 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:13][root][INFO] - Training Epoch: 2/2, step 1794/7134 completed (loss: 0.07688456028699875, acc: 0.9795597195625305)
[2025-02-13 03:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:13][root][INFO] - Training Epoch: 2/2, step 1795/7134 completed (loss: 0.1305561512708664, acc: 0.9680851101875305)
[2025-02-13 03:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14][root][INFO] - Training Epoch: 2/2, step 1796/7134 completed (loss: 0.1082838773727417, acc: 0.9686520099639893)
[2025-02-13 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14][root][INFO] - Training Epoch: 2/2, step 1797/7134 completed (loss: 0.05268704891204834, acc: 0.9831932783126831)
[2025-02-13 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14][root][INFO] - Training Epoch: 2/2, step 1798/7134 completed (loss: 0.0545695424079895, acc: 0.9826254844665527)
[2025-02-13 03:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:15][root][INFO] - Training Epoch: 2/2, step 1799/7134 completed (loss: 0.07209363579750061, acc: 0.9861809015274048)
[2025-02-13 03:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:15][root][INFO] - Training Epoch: 2/2, step 1800/7134 completed (loss: 0.0979931429028511, acc: 0.9684579372406006)
[2025-02-13 03:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:16][root][INFO] - Training Epoch: 2/2, step 1801/7134 completed (loss: 0.06174704059958458, acc: 0.9830508232116699)
[2025-02-13 03:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:16][root][INFO] - Training Epoch: 2/2, step 1802/7134 completed (loss: 0.06588118523359299, acc: 0.9837296605110168)
[2025-02-13 03:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17][root][INFO] - Training Epoch: 2/2, step 1803/7134 completed (loss: 0.05022718384861946, acc: 0.9845916628837585)
[2025-02-13 03:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17][root][INFO] - Training Epoch: 2/2, step 1804/7134 completed (loss: 0.03210698440670967, acc: 0.9901719689369202)
[2025-02-13 03:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17][root][INFO] - Training Epoch: 2/2, step 1805/7134 completed (loss: 0.0631927102804184, acc: 0.9769119620323181)
[2025-02-13 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:18][root][INFO] - Training Epoch: 2/2, step 1806/7134 completed (loss: 0.04274127259850502, acc: 0.9907894730567932)
[2025-02-13 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:18][root][INFO] - Training Epoch: 2/2, step 1807/7134 completed (loss: 0.04769318923354149, acc: 0.9844412803649902)
[2025-02-13 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:19][root][INFO] - Training Epoch: 2/2, step 1808/7134 completed (loss: 0.028193620964884758, acc: 0.9921976327896118)
[2025-02-13 03:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:19][root][INFO] - Training Epoch: 2/2, step 1809/7134 completed (loss: 0.03084683232009411, acc: 0.9932432174682617)
[2025-02-13 03:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20][root][INFO] - Training Epoch: 2/2, step 1810/7134 completed (loss: 0.016946475952863693, acc: 0.9948453903198242)
[2025-02-13 03:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20][root][INFO] - Training Epoch: 2/2, step 1811/7134 completed (loss: 0.0690578743815422, acc: 0.9810040593147278)
[2025-02-13 03:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20][root][INFO] - Training Epoch: 2/2, step 1812/7134 completed (loss: 0.02866649068892002, acc: 0.9910714030265808)
[2025-02-13 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:21][root][INFO] - Training Epoch: 2/2, step 1813/7134 completed (loss: 0.050020892173051834, acc: 0.9880239367485046)
[2025-02-13 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:21][root][INFO] - Training Epoch: 2/2, step 1814/7134 completed (loss: 0.01991497166454792, acc: 0.9941725134849548)
[2025-02-13 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:22][root][INFO] - Training Epoch: 2/2, step 1815/7134 completed (loss: 0.031541455537080765, acc: 0.9913793206214905)
[2025-02-13 03:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:22][root][INFO] - Training Epoch: 2/2, step 1816/7134 completed (loss: 0.04179694503545761, acc: 0.9890453815460205)
[2025-02-13 03:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:23][root][INFO] - Training Epoch: 2/2, step 1817/7134 completed (loss: 0.03949522227048874, acc: 0.9904534816741943)
[2025-02-13 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:23][root][INFO] - Training Epoch: 2/2, step 1818/7134 completed (loss: 0.05294419825077057, acc: 0.9900867342948914)
[2025-02-13 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24][root][INFO] - Training Epoch: 2/2, step 1819/7134 completed (loss: 0.02646307647228241, acc: 0.9937655925750732)
[2025-02-13 03:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24][root][INFO] - Training Epoch: 2/2, step 1820/7134 completed (loss: 0.016758235171437263, acc: 0.9934498071670532)
[2025-02-13 03:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24][root][INFO] - Training Epoch: 2/2, step 1821/7134 completed (loss: 0.023427290841937065, acc: 0.9945295453071594)
[2025-02-13 03:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:25][root][INFO] - Training Epoch: 2/2, step 1822/7134 completed (loss: 0.00998133048415184, acc: 0.9970760345458984)
[2025-02-13 03:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:25][root][INFO] - Training Epoch: 2/2, step 1823/7134 completed (loss: 0.025831498205661774, acc: 0.9931787252426147)
[2025-02-13 03:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:26][root][INFO] - Training Epoch: 2/2, step 1824/7134 completed (loss: 0.01721065305173397, acc: 0.995055615901947)
[2025-02-13 03:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:26][root][INFO] - Training Epoch: 2/2, step 1825/7134 completed (loss: 0.025244783610105515, acc: 0.9943946003913879)
[2025-02-13 03:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:27][root][INFO] - Training Epoch: 2/2, step 1826/7134 completed (loss: 0.01404501497745514, acc: 0.9932705163955688)
[2025-02-13 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:27][root][INFO] - Training Epoch: 2/2, step 1827/7134 completed (loss: 0.04337841644883156, acc: 0.9900771975517273)
[2025-02-13 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28][root][INFO] - Training Epoch: 2/2, step 1828/7134 completed (loss: 0.007677936926484108, acc: 0.9974259734153748)
[2025-02-13 03:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28][root][INFO] - Training Epoch: 2/2, step 1829/7134 completed (loss: 0.0195647943764925, acc: 0.9937888383865356)
[2025-02-13 03:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28][root][INFO] - Training Epoch: 2/2, step 1830/7134 completed (loss: 0.018447550013661385, acc: 0.9939467310905457)
[2025-02-13 03:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:29][root][INFO] - Training Epoch: 2/2, step 1831/7134 completed (loss: 0.00583311403170228, acc: 1.0)
[2025-02-13 03:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:29][root][INFO] - Training Epoch: 2/2, step 1832/7134 completed (loss: 0.06788264960050583, acc: 0.9840989112854004)
[2025-02-13 03:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:30][root][INFO] - Training Epoch: 2/2, step 1833/7134 completed (loss: 0.17824876308441162, acc: 0.9619450569152832)
[2025-02-13 03:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:30][root][INFO] - Training Epoch: 2/2, step 1834/7134 completed (loss: 0.0468447171151638, acc: 0.98591548204422)
[2025-02-13 03:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31][root][INFO] - Training Epoch: 2/2, step 1835/7134 completed (loss: 0.06856828182935715, acc: 0.9840116500854492)
[2025-02-13 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31][root][INFO] - Training Epoch: 2/2, step 1836/7134 completed (loss: 0.025266772136092186, acc: 0.991725742816925)
[2025-02-13 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31][root][INFO] - Training Epoch: 2/2, step 1837/7134 completed (loss: 0.018398882821202278, acc: 0.9901719689369202)
[2025-02-13 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:32][root][INFO] - Training Epoch: 2/2, step 1838/7134 completed (loss: 0.008503790013492107, acc: 0.9979591965675354)
[2025-02-13 03:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:32][root][INFO] - Training Epoch: 2/2, step 1839/7134 completed (loss: 0.02905462123453617, acc: 0.9909326434135437)
[2025-02-13 03:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33][root][INFO] - Training Epoch: 2/2, step 1840/7134 completed (loss: 0.029990633949637413, acc: 0.9930264949798584)
[2025-02-13 03:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33][root][INFO] - Training Epoch: 2/2, step 1841/7134 completed (loss: 0.026331620290875435, acc: 0.9914893507957458)
[2025-02-13 03:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33][root][INFO] - Training Epoch: 2/2, step 1842/7134 completed (loss: 0.06824950128793716, acc: 0.9877551198005676)
[2025-02-13 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:34][root][INFO] - Training Epoch: 2/2, step 1843/7134 completed (loss: 0.05002441257238388, acc: 0.9862499833106995)
[2025-02-13 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:34][root][INFO] - Training Epoch: 2/2, step 1844/7134 completed (loss: 0.03202228248119354, acc: 0.9934959411621094)
[2025-02-13 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:35][root][INFO] - Training Epoch: 2/2, step 1845/7134 completed (loss: 0.036689043045043945, acc: 0.9895615577697754)
[2025-02-13 03:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:35][root][INFO] - Training Epoch: 2/2, step 1846/7134 completed (loss: 0.018730908632278442, acc: 0.995726466178894)
[2025-02-13 03:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:36][root][INFO] - Training Epoch: 2/2, step 1847/7134 completed (loss: 0.021665720269083977, acc: 0.9953917264938354)
[2025-02-13 03:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:36][root][INFO] - Training Epoch: 2/2, step 1848/7134 completed (loss: 0.04339481517672539, acc: 0.991525411605835)
[2025-02-13 03:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37][root][INFO] - Training Epoch: 2/2, step 1849/7134 completed (loss: 0.060956940054893494, acc: 0.982425332069397)
[2025-02-13 03:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37][root][INFO] - Training Epoch: 2/2, step 1850/7134 completed (loss: 0.020785389468073845, acc: 0.9938875436782837)
[2025-02-13 03:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37][root][INFO] - Training Epoch: 2/2, step 1851/7134 completed (loss: 0.04655279964208603, acc: 0.9865689873695374)
[2025-02-13 03:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:38][root][INFO] - Training Epoch: 2/2, step 1852/7134 completed (loss: 0.036595236510038376, acc: 0.9875776171684265)
[2025-02-13 03:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:38][root][INFO] - Training Epoch: 2/2, step 1853/7134 completed (loss: 0.0405159667134285, acc: 0.9916267991065979)
[2025-02-13 03:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:39][root][INFO] - Training Epoch: 2/2, step 1854/7134 completed (loss: 0.05079168081283569, acc: 0.9797859787940979)
[2025-02-13 03:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:39][root][INFO] - Training Epoch: 2/2, step 1855/7134 completed (loss: 0.04897112026810646, acc: 0.9872832298278809)
[2025-02-13 03:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40][root][INFO] - Training Epoch: 2/2, step 1856/7134 completed (loss: 0.041644178330898285, acc: 0.9889298677444458)
[2025-02-13 03:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40][root][INFO] - Training Epoch: 2/2, step 1857/7134 completed (loss: 0.05089164897799492, acc: 0.9864531755447388)
[2025-02-13 03:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40][root][INFO] - Training Epoch: 2/2, step 1858/7134 completed (loss: 0.02344762347638607, acc: 0.9916267991065979)
[2025-02-13 03:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:41][root][INFO] - Training Epoch: 2/2, step 1859/7134 completed (loss: 0.015366882085800171, acc: 0.9965870380401611)
[2025-02-13 03:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:41][root][INFO] - Training Epoch: 2/2, step 1860/7134 completed (loss: 0.015228222124278545, acc: 0.9962025284767151)
[2025-02-13 03:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:42][root][INFO] - Training Epoch: 2/2, step 1861/7134 completed (loss: 0.0510110966861248, acc: 0.9909560680389404)
[2025-02-13 03:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:42][root][INFO] - Training Epoch: 2/2, step 1862/7134 completed (loss: 0.01916705258190632, acc: 0.9945725798606873)
[2025-02-13 03:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43][root][INFO] - Training Epoch: 2/2, step 1863/7134 completed (loss: 0.02293250523507595, acc: 0.9928057789802551)
[2025-02-13 03:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43][root][INFO] - Training Epoch: 2/2, step 1864/7134 completed (loss: 0.12852367758750916, acc: 0.9687075018882751)
[2025-02-13 03:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43][root][INFO] - Training Epoch: 2/2, step 1865/7134 completed (loss: 0.025900287553668022, acc: 0.9881556630134583)
[2025-02-13 03:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:44][root][INFO] - Training Epoch: 2/2, step 1866/7134 completed (loss: 0.01341235265135765, acc: 0.994397759437561)
[2025-02-13 03:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:44][root][INFO] - Training Epoch: 2/2, step 1867/7134 completed (loss: 0.02476687915623188, acc: 0.9911727905273438)
[2025-02-13 03:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:45][root][INFO] - Training Epoch: 2/2, step 1868/7134 completed (loss: 0.06336380541324615, acc: 0.9851484894752502)
[2025-02-13 03:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:45][root][INFO] - Training Epoch: 2/2, step 1869/7134 completed (loss: 0.0391041599214077, acc: 0.9885807633399963)
[2025-02-13 03:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46][root][INFO] - Training Epoch: 2/2, step 1870/7134 completed (loss: 0.010062286630272865, acc: 0.9986979365348816)
[2025-02-13 03:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46][root][INFO] - Training Epoch: 2/2, step 1871/7134 completed (loss: 0.065973661839962, acc: 0.9885807633399963)
[2025-02-13 03:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46][root][INFO] - Training Epoch: 2/2, step 1872/7134 completed (loss: 0.4399150311946869, acc: 0.9171270728111267)
[2025-02-13 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:47][root][INFO] - Training Epoch: 2/2, step 1873/7134 completed (loss: 0.11577360332012177, acc: 0.9691211581230164)
[2025-02-13 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:47][root][INFO] - Training Epoch: 2/2, step 1874/7134 completed (loss: 0.049628060311079025, acc: 0.9842725992202759)
[2025-02-13 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48][root][INFO] - Training Epoch: 2/2, step 1875/7134 completed (loss: 0.028840908780694008, acc: 0.9956331849098206)
[2025-02-13 03:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48][root][INFO] - Training Epoch: 2/2, step 1876/7134 completed (loss: 0.11092624068260193, acc: 0.9660441279411316)
[2025-02-13 03:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48][root][INFO] - Training Epoch: 2/2, step 1877/7134 completed (loss: 0.046212535351514816, acc: 0.989924430847168)
[2025-02-13 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49][root][INFO] - Training Epoch: 2/2, step 1878/7134 completed (loss: 0.0724235400557518, acc: 0.973805844783783)
[2025-02-13 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49][root][INFO] - Training Epoch: 2/2, step 1879/7134 completed (loss: 0.06811896711587906, acc: 0.9771615266799927)
[2025-02-13 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50][root][INFO] - Training Epoch: 2/2, step 1880/7134 completed (loss: 0.054662659764289856, acc: 0.98591548204422)
[2025-02-13 03:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50][root][INFO] - Training Epoch: 2/2, step 1881/7134 completed (loss: 0.06663811206817627, acc: 0.983660101890564)
[2025-02-13 03:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50][root][INFO] - Training Epoch: 2/2, step 1882/7134 completed (loss: 0.046330347657203674, acc: 0.9852216839790344)
[2025-02-13 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:51][root][INFO] - Training Epoch: 2/2, step 1883/7134 completed (loss: 0.059040751308202744, acc: 0.9822834730148315)
[2025-02-13 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:51][root][INFO] - Training Epoch: 2/2, step 1884/7134 completed (loss: 0.07979513704776764, acc: 0.9747235178947449)
[2025-02-13 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52][root][INFO] - Training Epoch: 2/2, step 1885/7134 completed (loss: 0.058656297624111176, acc: 0.9812382459640503)
[2025-02-13 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52][root][INFO] - Training Epoch: 2/2, step 1886/7134 completed (loss: 0.08757927268743515, acc: 0.9749478101730347)
[2025-02-13 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52][root][INFO] - Training Epoch: 2/2, step 1887/7134 completed (loss: 0.06271515786647797, acc: 0.9810246825218201)
[2025-02-13 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53][root][INFO] - Training Epoch: 2/2, step 1888/7134 completed (loss: 0.06725505739450455, acc: 0.984644889831543)
[2025-02-13 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53][root][INFO] - Training Epoch: 2/2, step 1889/7134 completed (loss: 0.03434194624423981, acc: 0.986994206905365)
[2025-02-13 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:54][root][INFO] - Training Epoch: 2/2, step 1890/7134 completed (loss: 0.02771654725074768, acc: 0.9922600388526917)
[2025-02-13 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:54][root][INFO] - Training Epoch: 2/2, step 1891/7134 completed (loss: 0.08968015015125275, acc: 0.9740259647369385)
[2025-02-13 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55][root][INFO] - Training Epoch: 2/2, step 1892/7134 completed (loss: 0.05638352036476135, acc: 0.984544038772583)
[2025-02-13 03:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55][root][INFO] - Training Epoch: 2/2, step 1893/7134 completed (loss: 0.030990520492196083, acc: 0.9875195026397705)
[2025-02-13 03:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55][root][INFO] - Training Epoch: 2/2, step 1894/7134 completed (loss: 0.09074443578720093, acc: 0.9791666865348816)
[2025-02-13 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:56][root][INFO] - Training Epoch: 2/2, step 1895/7134 completed (loss: 0.21449735760688782, acc: 0.9411764740943909)
[2025-02-13 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:56][root][INFO] - Training Epoch: 2/2, step 1896/7134 completed (loss: 0.06749512255191803, acc: 0.9764705896377563)
[2025-02-13 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57][root][INFO] - Training Epoch: 2/2, step 1897/7134 completed (loss: 0.01718321070075035, acc: 0.9944444298744202)
[2025-02-13 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57][root][INFO] - Training Epoch: 2/2, step 1898/7134 completed (loss: 0.03613005205988884, acc: 0.9883913993835449)
[2025-02-13 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57][root][INFO] - Training Epoch: 2/2, step 1899/7134 completed (loss: 0.04900703579187393, acc: 0.9873417615890503)
[2025-02-13 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:58][root][INFO] - Training Epoch: 2/2, step 1900/7134 completed (loss: 0.02196669578552246, acc: 0.9967637658119202)
[2025-02-13 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:58][root][INFO] - Training Epoch: 2/2, step 1901/7134 completed (loss: 0.05785509943962097, acc: 0.9853747487068176)
[2025-02-13 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59][root][INFO] - Training Epoch: 2/2, step 1902/7134 completed (loss: 0.027964185923337936, acc: 0.9929701089859009)
[2025-02-13 03:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59][root][INFO] - Training Epoch: 2/2, step 1903/7134 completed (loss: 0.022621070966124535, acc: 0.9930555820465088)
[2025-02-13 03:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59][root][INFO] - Training Epoch: 2/2, step 1904/7134 completed (loss: 0.020837394520640373, acc: 0.991525411605835)
[2025-02-13 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:00][root][INFO] - Training Epoch: 2/2, step 1905/7134 completed (loss: 0.04067414999008179, acc: 0.9897750616073608)
[2025-02-13 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:00][root][INFO] - Training Epoch: 2/2, step 1906/7134 completed (loss: 0.04555322602391243, acc: 0.9863547682762146)
[2025-02-13 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01][root][INFO] - Training Epoch: 2/2, step 1907/7134 completed (loss: 0.03710397705435753, acc: 0.9932773113250732)
[2025-02-13 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01][root][INFO] - Training Epoch: 2/2, step 1908/7134 completed (loss: 0.03123866766691208, acc: 0.9914070963859558)
[2025-02-13 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01][root][INFO] - Training Epoch: 2/2, step 1909/7134 completed (loss: 0.03477148711681366, acc: 0.9867549538612366)
[2025-02-13 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:02][root][INFO] - Training Epoch: 2/2, step 1910/7134 completed (loss: 0.034846171736717224, acc: 0.9923245906829834)
[2025-02-13 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:02][root][INFO] - Training Epoch: 2/2, step 1911/7134 completed (loss: 0.0359380878508091, acc: 0.9915048480033875)
[2025-02-13 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03][root][INFO] - Training Epoch: 2/2, step 1912/7134 completed (loss: 0.010324111208319664, acc: 0.9960052967071533)
[2025-02-13 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03][root][INFO] - Training Epoch: 2/2, step 1913/7134 completed (loss: 0.06657575815916061, acc: 0.9870129823684692)
[2025-02-13 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04][root][INFO] - Training Epoch: 2/2, step 1914/7134 completed (loss: 0.055372633039951324, acc: 0.9864681959152222)
[2025-02-13 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04][root][INFO] - Training Epoch: 2/2, step 1915/7134 completed (loss: 0.04146203026175499, acc: 0.9869494438171387)
[2025-02-13 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04][root][INFO] - Training Epoch: 2/2, step 1916/7134 completed (loss: 0.03323950991034508, acc: 0.9853836894035339)
[2025-02-13 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:05][root][INFO] - Training Epoch: 2/2, step 1917/7134 completed (loss: 0.036690179258584976, acc: 0.9915966391563416)
[2025-02-13 03:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:05][root][INFO] - Training Epoch: 2/2, step 1918/7134 completed (loss: 0.05184422805905342, acc: 0.9890282154083252)
[2025-02-13 03:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06][root][INFO] - Training Epoch: 2/2, step 1919/7134 completed (loss: 0.041659023612737656, acc: 0.9870874881744385)
[2025-02-13 03:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06][root][INFO] - Training Epoch: 2/2, step 1920/7134 completed (loss: 0.024514909833669662, acc: 0.9969135522842407)
[2025-02-13 03:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06][root][INFO] - Training Epoch: 2/2, step 1921/7134 completed (loss: 0.022625653073191643, acc: 0.9944598078727722)
[2025-02-13 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:07][root][INFO] - Training Epoch: 2/2, step 1922/7134 completed (loss: 0.030292311683297157, acc: 0.9940357804298401)
[2025-02-13 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:07][root][INFO] - Training Epoch: 2/2, step 1923/7134 completed (loss: 0.021763402968645096, acc: 0.9916247725486755)
[2025-02-13 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:08][root][INFO] - Training Epoch: 2/2, step 1924/7134 completed (loss: 0.04321639984846115, acc: 0.9871244430541992)
[2025-02-13 03:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:08][root][INFO] - Training Epoch: 2/2, step 1925/7134 completed (loss: 0.055466178804636, acc: 0.9830687642097473)
[2025-02-13 03:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09][root][INFO] - Training Epoch: 2/2, step 1926/7134 completed (loss: 0.015629291534423828, acc: 0.9948927760124207)
[2025-02-13 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09][root][INFO] - Training Epoch: 2/2, step 1927/7134 completed (loss: 0.05184895545244217, acc: 0.9922651648521423)
[2025-02-13 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10][root][INFO] - Training Epoch: 2/2, step 1928/7134 completed (loss: 0.031155183911323547, acc: 0.9908814430236816)
[2025-02-13 03:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10][root][INFO] - Training Epoch: 2/2, step 1929/7134 completed (loss: 0.021230921149253845, acc: 0.9957627058029175)
[2025-02-13 03:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10][root][INFO] - Training Epoch: 2/2, step 1930/7134 completed (loss: 0.07163448631763458, acc: 0.9803149700164795)
[2025-02-13 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:11][root][INFO] - Training Epoch: 2/2, step 1931/7134 completed (loss: 0.11107523739337921, acc: 0.9772440195083618)
[2025-02-13 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:11][root][INFO] - Training Epoch: 2/2, step 1932/7134 completed (loss: 0.081949882209301, acc: 0.9761193990707397)
[2025-02-13 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:12][root][INFO] - Training Epoch: 2/2, step 1933/7134 completed (loss: 0.12211286276578903, acc: 0.970588207244873)
[2025-02-13 03:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:12][root][INFO] - Training Epoch: 2/2, step 1934/7134 completed (loss: 0.037519071251153946, acc: 0.9971751570701599)
[2025-02-13 03:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13][root][INFO] - Training Epoch: 2/2, step 1935/7134 completed (loss: 0.1158207356929779, acc: 0.9694322943687439)
[2025-02-13 03:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13][root][INFO] - Training Epoch: 2/2, step 1936/7134 completed (loss: 0.019656112417578697, acc: 0.9952606558799744)
[2025-02-13 03:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13][root][INFO] - Training Epoch: 2/2, step 1937/7134 completed (loss: 0.03584633395075798, acc: 0.9886363744735718)
[2025-02-13 03:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:14][root][INFO] - Training Epoch: 2/2, step 1938/7134 completed (loss: 0.06160915270447731, acc: 0.9886040091514587)
[2025-02-13 03:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:14][root][INFO] - Training Epoch: 2/2, step 1939/7134 completed (loss: 0.07257167249917984, acc: 0.9894067645072937)
[2025-02-13 03:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:15][root][INFO] - Training Epoch: 2/2, step 1940/7134 completed (loss: 0.06308242678642273, acc: 0.9795918464660645)
[2025-02-13 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:15][root][INFO] - Training Epoch: 2/2, step 1941/7134 completed (loss: 0.049480851739645004, acc: 0.9826202988624573)
[2025-02-13 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16][root][INFO] - Training Epoch: 2/2, step 1942/7134 completed (loss: 0.04644077271223068, acc: 0.9878048896789551)
[2025-02-13 03:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16][root][INFO] - Training Epoch: 2/2, step 1943/7134 completed (loss: 0.048956118524074554, acc: 0.9841463565826416)
[2025-02-13 03:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16][root][INFO] - Training Epoch: 2/2, step 1944/7134 completed (loss: 0.03199710696935654, acc: 0.9890310764312744)
[2025-02-13 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:17][root][INFO] - Training Epoch: 2/2, step 1945/7134 completed (loss: 0.04480037838220596, acc: 0.9803149700164795)
[2025-02-13 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:17][root][INFO] - Training Epoch: 2/2, step 1946/7134 completed (loss: 0.03809559345245361, acc: 0.9864176511764526)
[2025-02-13 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:18][root][INFO] - Training Epoch: 2/2, step 1947/7134 completed (loss: 0.057058293372392654, acc: 0.9802631735801697)
[2025-02-13 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:18][root][INFO] - Training Epoch: 2/2, step 1948/7134 completed (loss: 0.0830564871430397, acc: 0.9753320813179016)
[2025-02-13 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19][root][INFO] - Training Epoch: 2/2, step 1949/7134 completed (loss: 0.03935914859175682, acc: 0.9866220951080322)
[2025-02-13 03:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19][root][INFO] - Training Epoch: 2/2, step 1950/7134 completed (loss: 0.046518757939338684, acc: 0.985401451587677)
[2025-02-13 03:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19][root][INFO] - Training Epoch: 2/2, step 1951/7134 completed (loss: 0.05511163920164108, acc: 0.9916387796401978)
[2025-02-13 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:20][root][INFO] - Training Epoch: 2/2, step 1952/7134 completed (loss: 0.07777871936559677, acc: 0.980033278465271)
[2025-02-13 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:20][root][INFO] - Training Epoch: 2/2, step 1953/7134 completed (loss: 0.0311735887080431, acc: 0.99071204662323)
[2025-02-13 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21][root][INFO] - Training Epoch: 2/2, step 1954/7134 completed (loss: 0.025181211531162262, acc: 0.995121955871582)
[2025-02-13 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21][root][INFO] - Training Epoch: 2/2, step 1955/7134 completed (loss: 0.03531232103705406, acc: 0.9894551634788513)
[2025-02-13 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21][root][INFO] - Training Epoch: 2/2, step 1956/7134 completed (loss: 0.023090926930308342, acc: 0.9919354915618896)
[2025-02-13 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22][root][INFO] - Training Epoch: 2/2, step 1957/7134 completed (loss: 0.049456000328063965, acc: 0.9851852059364319)
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22][root][INFO] - Training Epoch: 2/2, step 1958/7134 completed (loss: 0.034270476549863815, acc: 0.9942693114280701)
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22][root][INFO] - Training Epoch: 2/2, step 1959/7134 completed (loss: 0.023295694962143898, acc: 0.9936034083366394)
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:23][root][INFO] - Training Epoch: 2/2, step 1960/7134 completed (loss: 0.01299750991165638, acc: 0.9962406158447266)
[2025-02-13 03:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:23][root][INFO] - Training Epoch: 2/2, step 1961/7134 completed (loss: 0.031774021685123444, acc: 0.9966555237770081)
[2025-02-13 03:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24][root][INFO] - Training Epoch: 2/2, step 1962/7134 completed (loss: 0.04340397194027901, acc: 0.9916201233863831)
[2025-02-13 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24][root][INFO] - Training Epoch: 2/2, step 1963/7134 completed (loss: 0.03499350696802139, acc: 0.9913644194602966)
[2025-02-13 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24][root][INFO] - Training Epoch: 2/2, step 1964/7134 completed (loss: 0.04226088523864746, acc: 0.9841269850730896)
[2025-02-13 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25][root][INFO] - Training Epoch: 2/2, step 1965/7134 completed (loss: 0.019471649080514908, acc: 0.9944211840629578)
[2025-02-13 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25][root][INFO] - Training Epoch: 2/2, step 1966/7134 completed (loss: 0.02790069580078125, acc: 0.9908257126808167)
[2025-02-13 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26][root][INFO] - Training Epoch: 2/2, step 1967/7134 completed (loss: 0.06382282823324203, acc: 0.9808917045593262)
[2025-02-13 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26][root][INFO] - Training Epoch: 2/2, step 1968/7134 completed (loss: 0.02030865289270878, acc: 0.9925925731658936)
[2025-02-13 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26][root][INFO] - Training Epoch: 2/2, step 1969/7134 completed (loss: 0.03429699316620827, acc: 0.9906666874885559)
[2025-02-13 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:27][root][INFO] - Training Epoch: 2/2, step 1970/7134 completed (loss: 0.022550512105226517, acc: 0.9923312664031982)
[2025-02-13 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:27][root][INFO] - Training Epoch: 2/2, step 1971/7134 completed (loss: 0.039446476846933365, acc: 0.988063633441925)
[2025-02-13 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28][root][INFO] - Training Epoch: 2/2, step 1972/7134 completed (loss: 0.18329210579395294, acc: 0.9484029412269592)
[2025-02-13 03:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28][root][INFO] - Training Epoch: 2/2, step 1973/7134 completed (loss: 0.09009268879890442, acc: 0.9638095498085022)
[2025-02-13 03:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29][root][INFO] - Training Epoch: 2/2, step 1974/7134 completed (loss: 0.03299691155552864, acc: 0.9886934757232666)
[2025-02-13 03:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29][root][INFO] - Training Epoch: 2/2, step 1975/7134 completed (loss: 0.020221566781401634, acc: 0.9969696998596191)
[2025-02-13 03:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29][root][INFO] - Training Epoch: 2/2, step 1976/7134 completed (loss: 0.04210277646780014, acc: 0.987542450428009)
[2025-02-13 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:30][root][INFO] - Training Epoch: 2/2, step 1977/7134 completed (loss: 0.03366589918732643, acc: 0.9884058237075806)
[2025-02-13 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:30][root][INFO] - Training Epoch: 2/2, step 1978/7134 completed (loss: 0.017225176095962524, acc: 0.9973154067993164)
[2025-02-13 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31][root][INFO] - Training Epoch: 2/2, step 1979/7134 completed (loss: 0.021569812670350075, acc: 0.9934895634651184)
[2025-02-13 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31][root][INFO] - Training Epoch: 2/2, step 1980/7134 completed (loss: 0.09973559528589249, acc: 0.9804432988166809)
[2025-02-13 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32][root][INFO] - Training Epoch: 2/2, step 1981/7134 completed (loss: 0.057105932384729385, acc: 0.9790209531784058)
[2025-02-13 03:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32][root][INFO] - Training Epoch: 2/2, step 1982/7134 completed (loss: 0.03506812825798988, acc: 0.9871299862861633)
[2025-02-13 03:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32][root][INFO] - Training Epoch: 2/2, step 1983/7134 completed (loss: 0.024676548317074776, acc: 0.9949109554290771)
[2025-02-13 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33][root][INFO] - Training Epoch: 2/2, step 1984/7134 completed (loss: 0.013326122425496578, acc: 0.9959623217582703)
[2025-02-13 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33][root][INFO] - Training Epoch: 2/2, step 1985/7134 completed (loss: 0.02512262389063835, acc: 0.9902507066726685)
[2025-02-13 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34][root][INFO] - Training Epoch: 2/2, step 1986/7134 completed (loss: 0.04341619461774826, acc: 0.9884169697761536)
[2025-02-13 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34][root][INFO] - Training Epoch: 2/2, step 1987/7134 completed (loss: 0.0466177836060524, acc: 0.9872521162033081)
[2025-02-13 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][root][INFO] - Training Epoch: 2/2, step 1988/7134 completed (loss: 0.03959212079644203, acc: 0.9871612191200256)
[2025-02-13 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][root][INFO] - Training Epoch: 2/2, step 1989/7134 completed (loss: 0.060391079634428024, acc: 0.9856459498405457)
[2025-02-13 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][root][INFO] - Training Epoch: 2/2, step 1990/7134 completed (loss: 0.031720615923404694, acc: 0.9951456189155579)
[2025-02-13 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:36][root][INFO] - Training Epoch: 2/2, step 1991/7134 completed (loss: 0.022141676396131516, acc: 0.9916943311691284)
[2025-02-13 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:36][root][INFO] - Training Epoch: 2/2, step 1992/7134 completed (loss: 0.04042454808950424, acc: 0.9891473054885864)
[2025-02-13 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37][root][INFO] - Training Epoch: 2/2, step 1993/7134 completed (loss: 0.011849860660731792, acc: 0.9982425570487976)
[2025-02-13 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37][root][INFO] - Training Epoch: 2/2, step 1994/7134 completed (loss: 0.010865557007491589, acc: 0.9965517520904541)
[2025-02-13 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37][root][INFO] - Training Epoch: 2/2, step 1995/7134 completed (loss: 0.026132693514227867, acc: 0.9938176274299622)
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38][root][INFO] - Training Epoch: 2/2, step 1996/7134 completed (loss: 0.036991845816373825, acc: 0.9900826215744019)
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38][root][INFO] - Training Epoch: 2/2, step 1997/7134 completed (loss: 0.02125735394656658, acc: 0.9970887899398804)
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39][root][INFO] - Training Epoch: 2/2, step 1998/7134 completed (loss: 0.021769795566797256, acc: 0.9921507239341736)
[2025-02-13 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39][root][INFO] - Training Epoch: 2/2, step 1999/7134 completed (loss: 0.03700752183794975, acc: 0.9881154298782349)
[2025-02-13 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40][root][INFO] - Training Epoch: 2/2, step 2000/7134 completed (loss: 0.0303315632045269, acc: 0.9921135902404785)
[2025-02-13 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40][root][INFO] - Training Epoch: 2/2, step 2001/7134 completed (loss: 0.01646954007446766, acc: 0.9954545497894287)
[2025-02-13 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40][root][INFO] - Training Epoch: 2/2, step 2002/7134 completed (loss: 0.035353805869817734, acc: 0.991304337978363)
[2025-02-13 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41][root][INFO] - Training Epoch: 2/2, step 2003/7134 completed (loss: 0.014350243844091892, acc: 0.9979166388511658)
[2025-02-13 03:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41][root][INFO] - Training Epoch: 2/2, step 2004/7134 completed (loss: 0.1720397174358368, acc: 0.9578059315681458)
[2025-02-13 03:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41][root][INFO] - Training Epoch: 2/2, step 2005/7134 completed (loss: 0.09475544840097427, acc: 0.9808219075202942)
[2025-02-13 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:42][root][INFO] - Training Epoch: 2/2, step 2006/7134 completed (loss: 0.017549147829413414, acc: 0.9942362904548645)
[2025-02-13 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:42][root][INFO] - Training Epoch: 2/2, step 2007/7134 completed (loss: 0.041286587715148926, acc: 0.9899598360061646)
[2025-02-13 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][root][INFO] - Training Epoch: 2/2, step 2008/7134 completed (loss: 0.051157671958208084, acc: 0.9786477088928223)
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][root][INFO] - Training Epoch: 2/2, step 2009/7134 completed (loss: 0.027591830119490623, acc: 0.9927797913551331)
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][root][INFO] - Training Epoch: 2/2, step 2010/7134 completed (loss: 0.019833305850625038, acc: 0.994854211807251)
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:44][root][INFO] - Training Epoch: 2/2, step 2011/7134 completed (loss: 0.015061506070196629, acc: 0.9960861206054688)
[2025-02-13 03:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:44][root][INFO] - Training Epoch: 2/2, step 2012/7134 completed (loss: 0.027384094893932343, acc: 0.9915433526039124)
[2025-02-13 03:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45][root][INFO] - Training Epoch: 2/2, step 2013/7134 completed (loss: 0.0150441974401474, acc: 0.9980000257492065)
[2025-02-13 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45][root][INFO] - Training Epoch: 2/2, step 2014/7134 completed (loss: 0.025755178183317184, acc: 0.9937008023262024)
[2025-02-13 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45][root][INFO] - Training Epoch: 2/2, step 2015/7134 completed (loss: 0.02755708433687687, acc: 0.9917762875556946)
[2025-02-13 03:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:46][root][INFO] - Training Epoch: 2/2, step 2016/7134 completed (loss: 0.060836996883153915, acc: 0.9846153855323792)
[2025-02-13 03:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:46][root][INFO] - Training Epoch: 2/2, step 2017/7134 completed (loss: 0.06693799793720245, acc: 0.9851351380348206)
[2025-02-13 03:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47][root][INFO] - Training Epoch: 2/2, step 2018/7134 completed (loss: 0.021381502971053123, acc: 0.9935897588729858)
[2025-02-13 03:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47][root][INFO] - Training Epoch: 2/2, step 2019/7134 completed (loss: 0.03841359540820122, acc: 0.9875862002372742)
[2025-02-13 03:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47][root][INFO] - Training Epoch: 2/2, step 2020/7134 completed (loss: 0.01370900496840477, acc: 0.9966158866882324)
[2025-02-13 03:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48][root][INFO] - Training Epoch: 2/2, step 2021/7134 completed (loss: 0.049654800444841385, acc: 0.9870129823684692)
[2025-02-13 03:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48][root][INFO] - Training Epoch: 2/2, step 2022/7134 completed (loss: 0.03607979044318199, acc: 0.9965338110923767)
[2025-02-13 03:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49][root][INFO] - Training Epoch: 2/2, step 2023/7134 completed (loss: 0.03805137425661087, acc: 0.9883211851119995)
[2025-02-13 03:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49][root][INFO] - Training Epoch: 2/2, step 2024/7134 completed (loss: 0.0439174547791481, acc: 0.9902777671813965)
[2025-02-13 03:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][root][INFO] - Training Epoch: 2/2, step 2025/7134 completed (loss: 0.03944594785571098, acc: 0.9919999837875366)
[2025-02-13 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][root][INFO] - Training Epoch: 2/2, step 2026/7134 completed (loss: 0.049913715571165085, acc: 0.9849397540092468)
[2025-02-13 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][root][INFO] - Training Epoch: 2/2, step 2027/7134 completed (loss: 0.026952778920531273, acc: 0.9933554530143738)
[2025-02-13 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:51][root][INFO] - Training Epoch: 2/2, step 2028/7134 completed (loss: 0.03131924942135811, acc: 0.9926900863647461)
[2025-02-13 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:51][root][INFO] - Training Epoch: 2/2, step 2029/7134 completed (loss: 0.038335952907800674, acc: 0.9886363744735718)
[2025-02-13 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52][root][INFO] - Training Epoch: 2/2, step 2030/7134 completed (loss: 0.04706408455967903, acc: 0.9785276055335999)
[2025-02-13 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52][root][INFO] - Training Epoch: 2/2, step 2031/7134 completed (loss: 0.03610654175281525, acc: 0.9889570474624634)
[2025-02-13 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53][root][INFO] - Training Epoch: 2/2, step 2032/7134 completed (loss: 0.041576724499464035, acc: 0.9879032373428345)
[2025-02-13 03:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53][root][INFO] - Training Epoch: 2/2, step 2033/7134 completed (loss: 0.029923515394330025, acc: 0.994854211807251)
[2025-02-13 03:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53][root][INFO] - Training Epoch: 2/2, step 2034/7134 completed (loss: 0.05388893559575081, acc: 0.9877488613128662)
[2025-02-13 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54][root][INFO] - Training Epoch: 2/2, step 2035/7134 completed (loss: 0.014111971482634544, acc: 0.9962639808654785)
[2025-02-13 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54][root][INFO] - Training Epoch: 2/2, step 2036/7134 completed (loss: 0.02239907719194889, acc: 0.9948096871376038)
[2025-02-13 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55][root][INFO] - Training Epoch: 2/2, step 2037/7134 completed (loss: 0.015662122517824173, acc: 0.9940652847290039)
[2025-02-13 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55][root][INFO] - Training Epoch: 2/2, step 2038/7134 completed (loss: 0.020488666370511055, acc: 0.9929078221321106)
[2025-02-13 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56][root][INFO] - Training Epoch: 2/2, step 2039/7134 completed (loss: 0.006883425638079643, acc: 1.0)
[2025-02-13 03:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56][root][INFO] - Training Epoch: 2/2, step 2040/7134 completed (loss: 0.04769469052553177, acc: 0.991769552230835)
[2025-02-13 03:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56][root][INFO] - Training Epoch: 2/2, step 2041/7134 completed (loss: 0.011439265683293343, acc: 0.9958791136741638)
[2025-02-13 03:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57][root][INFO] - Training Epoch: 2/2, step 2042/7134 completed (loss: 0.01160829421132803, acc: 0.9979838728904724)
[2025-02-13 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57][root][INFO] - Training Epoch: 2/2, step 2043/7134 completed (loss: 0.045282550156116486, acc: 0.9906250238418579)
[2025-02-13 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][root][INFO] - Training Epoch: 2/2, step 2044/7134 completed (loss: 0.02254505455493927, acc: 0.9964221715927124)
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][root][INFO] - Training Epoch: 2/2, step 2045/7134 completed (loss: 0.019674580544233322, acc: 0.994575023651123)
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][root][INFO] - Training Epoch: 2/2, step 2046/7134 completed (loss: 0.02478684112429619, acc: 0.9898374080657959)
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:59][root][INFO] - Training Epoch: 2/2, step 2047/7134 completed (loss: 0.026085421442985535, acc: 0.989983320236206)
[2025-02-13 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:59][root][INFO] - Training Epoch: 2/2, step 2048/7134 completed (loss: 0.04702354222536087, acc: 0.9865471124649048)
[2025-02-13 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00][root][INFO] - Training Epoch: 2/2, step 2049/7134 completed (loss: 0.06698883324861526, acc: 0.981176495552063)
[2025-02-13 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00][root][INFO] - Training Epoch: 2/2, step 2050/7134 completed (loss: 0.08544817566871643, acc: 0.9846153855323792)
[2025-02-13 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00][root][INFO] - Training Epoch: 2/2, step 2051/7134 completed (loss: 0.052256252616643906, acc: 0.986328125)
[2025-02-13 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01][root][INFO] - Training Epoch: 2/2, step 2052/7134 completed (loss: 0.06943486630916595, acc: 0.9829351305961609)
[2025-02-13 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01][root][INFO] - Training Epoch: 2/2, step 2053/7134 completed (loss: 0.032710961997509, acc: 0.9919999837875366)
[2025-02-13 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02][root][INFO] - Training Epoch: 2/2, step 2054/7134 completed (loss: 0.1111261174082756, acc: 0.9697452187538147)
[2025-02-13 03:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02][root][INFO] - Training Epoch: 2/2, step 2055/7134 completed (loss: 0.08596962690353394, acc: 0.977952778339386)
[2025-02-13 03:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02][root][INFO] - Training Epoch: 2/2, step 2056/7134 completed (loss: 0.05480145663022995, acc: 0.9831387996673584)
[2025-02-13 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03][root][INFO] - Training Epoch: 2/2, step 2057/7134 completed (loss: 0.06315715610980988, acc: 0.9810040593147278)
[2025-02-13 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03][root][INFO] - Training Epoch: 2/2, step 2058/7134 completed (loss: 0.0999860167503357, acc: 0.9770444631576538)
[2025-02-13 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04][root][INFO] - Training Epoch: 2/2, step 2059/7134 completed (loss: 0.1109282597899437, acc: 0.9757365584373474)
[2025-02-13 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04][root][INFO] - Training Epoch: 2/2, step 2060/7134 completed (loss: 0.03691644221544266, acc: 0.9899857044219971)
[2025-02-13 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05][root][INFO] - Training Epoch: 2/2, step 2061/7134 completed (loss: 0.06538600474596024, acc: 0.980169951915741)
[2025-02-13 03:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05][root][INFO] - Training Epoch: 2/2, step 2062/7134 completed (loss: 0.07324039936065674, acc: 0.9772295951843262)
[2025-02-13 03:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05][root][INFO] - Training Epoch: 2/2, step 2063/7134 completed (loss: 0.075264111161232, acc: 0.97947758436203)
[2025-02-13 03:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06][root][INFO] - Training Epoch: 2/2, step 2064/7134 completed (loss: 0.023289157077670097, acc: 0.9937499761581421)
[2025-02-13 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06][root][INFO] - Training Epoch: 2/2, step 2065/7134 completed (loss: 0.07140569388866425, acc: 0.9842519760131836)
[2025-02-13 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06][root][INFO] - Training Epoch: 2/2, step 2066/7134 completed (loss: 0.0448794960975647, acc: 0.9859719276428223)
[2025-02-13 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07][root][INFO] - Training Epoch: 2/2, step 2067/7134 completed (loss: 0.02927020564675331, acc: 0.992409884929657)
[2025-02-13 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07][root][INFO] - Training Epoch: 2/2, step 2068/7134 completed (loss: 0.05382578819990158, acc: 0.9836065769195557)
[2025-02-13 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:08][root][INFO] - Training Epoch: 2/2, step 2069/7134 completed (loss: 0.04209798946976662, acc: 0.9861538410186768)
[2025-02-13 03:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:08][root][INFO] - Training Epoch: 2/2, step 2070/7134 completed (loss: 0.04153409227728844, acc: 0.9904943108558655)
[2025-02-13 03:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09][root][INFO] - Training Epoch: 2/2, step 2071/7134 completed (loss: 0.016334019601345062, acc: 0.9953161478042603)
[2025-02-13 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09][root][INFO] - Training Epoch: 2/2, step 2072/7134 completed (loss: 0.02129816822707653, acc: 0.9925742745399475)
[2025-02-13 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09][root][INFO] - Training Epoch: 2/2, step 2073/7134 completed (loss: 0.03450147435069084, acc: 0.9903846383094788)
[2025-02-13 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10][root][INFO] - Training Epoch: 2/2, step 2074/7134 completed (loss: 0.03995034843683243, acc: 0.987089216709137)
[2025-02-13 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10][root][INFO] - Training Epoch: 2/2, step 2075/7134 completed (loss: 0.045525021851062775, acc: 0.9816360473632812)
[2025-02-13 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11][root][INFO] - Training Epoch: 2/2, step 2076/7134 completed (loss: 0.028139542788267136, acc: 0.9923076629638672)
[2025-02-13 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11][root][INFO] - Training Epoch: 2/2, step 2077/7134 completed (loss: 0.032433267682790756, acc: 0.9907038807868958)
[2025-02-13 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11][root][INFO] - Training Epoch: 2/2, step 2078/7134 completed (loss: 0.02576242946088314, acc: 0.9906976819038391)
[2025-02-13 03:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:12][root][INFO] - Training Epoch: 2/2, step 2079/7134 completed (loss: 0.01911047473549843, acc: 0.9933862686157227)
[2025-02-13 03:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:12][root][INFO] - Training Epoch: 2/2, step 2080/7134 completed (loss: 0.030144240707159042, acc: 0.9926470518112183)
[2025-02-13 03:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13][root][INFO] - Training Epoch: 2/2, step 2081/7134 completed (loss: 0.010513312183320522, acc: 0.9970972537994385)
[2025-02-13 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13][root][INFO] - Training Epoch: 2/2, step 2082/7134 completed (loss: 0.09732728451490402, acc: 0.9790356159210205)
[2025-02-13 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14][root][INFO] - Training Epoch: 2/2, step 2083/7134 completed (loss: 0.05682152137160301, acc: 0.9925925731658936)
[2025-02-13 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14][root][INFO] - Training Epoch: 2/2, step 2084/7134 completed (loss: 0.02902061678469181, acc: 0.9937888383865356)
[2025-02-13 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14][root][INFO] - Training Epoch: 2/2, step 2085/7134 completed (loss: 0.018687047064304352, acc: 0.996303141117096)
[2025-02-13 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:15][root][INFO] - Training Epoch: 2/2, step 2086/7134 completed (loss: 0.024996649473905563, acc: 0.9885844588279724)
[2025-02-13 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:15][root][INFO] - Training Epoch: 2/2, step 2087/7134 completed (loss: 0.005438740365207195, acc: 1.0)
[2025-02-13 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16][root][INFO] - Training Epoch: 2/2, step 2088/7134 completed (loss: 0.024395788088440895, acc: 0.9927954077720642)
[2025-02-13 03:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16][root][INFO] - Training Epoch: 2/2, step 2089/7134 completed (loss: 0.020807676017284393, acc: 0.9952681660652161)
[2025-02-13 03:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16][root][INFO] - Training Epoch: 2/2, step 2090/7134 completed (loss: 0.010091674514114857, acc: 0.998420238494873)
[2025-02-13 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17][root][INFO] - Training Epoch: 2/2, step 2091/7134 completed (loss: 0.02728448063135147, acc: 0.9949324131011963)
[2025-02-13 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17][root][INFO] - Training Epoch: 2/2, step 2092/7134 completed (loss: 0.012928239069879055, acc: 0.9984227418899536)
[2025-02-13 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18][root][INFO] - Training Epoch: 2/2, step 2093/7134 completed (loss: 0.020728493109345436, acc: 0.9918588995933533)
[2025-02-13 03:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18][root][INFO] - Training Epoch: 2/2, step 2094/7134 completed (loss: 0.04175649955868721, acc: 0.9918864369392395)
[2025-02-13 03:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18][root][INFO] - Training Epoch: 2/2, step 2095/7134 completed (loss: 0.03532987833023071, acc: 0.9934036731719971)
[2025-02-13 03:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:19][root][INFO] - Training Epoch: 2/2, step 2096/7134 completed (loss: 0.021963467821478844, acc: 0.9948275685310364)
[2025-02-13 03:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:19][root][INFO] - Training Epoch: 2/2, step 2097/7134 completed (loss: 0.013267237693071365, acc: 0.9957924485206604)
[2025-02-13 03:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20][root][INFO] - Training Epoch: 2/2, step 2098/7134 completed (loss: 0.020565617829561234, acc: 0.9967845678329468)
[2025-02-13 03:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20][root][INFO] - Training Epoch: 2/2, step 2099/7134 completed (loss: 0.03262832388281822, acc: 0.988990843296051)
[2025-02-13 03:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21][root][INFO] - Training Epoch: 2/2, step 2100/7134 completed (loss: 0.017641279846429825, acc: 0.9946879148483276)
[2025-02-13 03:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21][root][INFO] - Training Epoch: 2/2, step 2101/7134 completed (loss: 0.016900796443223953, acc: 0.992443323135376)
[2025-02-13 03:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21][root][INFO] - Training Epoch: 2/2, step 2102/7134 completed (loss: 0.035631466656923294, acc: 0.9897959232330322)
[2025-02-13 03:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22][root][INFO] - Training Epoch: 2/2, step 2103/7134 completed (loss: 0.02685302309691906, acc: 0.9923312664031982)
[2025-02-13 03:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22][root][INFO] - Training Epoch: 2/2, step 2104/7134 completed (loss: 0.017035139724612236, acc: 0.9974226951599121)
[2025-02-13 03:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23][root][INFO] - Training Epoch: 2/2, step 2105/7134 completed (loss: 0.0561080239713192, acc: 0.9827855825424194)
[2025-02-13 03:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23][root][INFO] - Training Epoch: 2/2, step 2106/7134 completed (loss: 0.04927898198366165, acc: 0.9899425506591797)
[2025-02-13 03:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23][root][INFO] - Training Epoch: 2/2, step 2107/7134 completed (loss: 0.022458387538790703, acc: 0.9932523369789124)
[2025-02-13 03:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:24][root][INFO] - Training Epoch: 2/2, step 2108/7134 completed (loss: 0.024356670677661896, acc: 0.9910314083099365)
[2025-02-13 03:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:24][root][INFO] - Training Epoch: 2/2, step 2109/7134 completed (loss: 0.06456019729375839, acc: 0.9823633432388306)
[2025-02-13 03:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:25][root][INFO] - Training Epoch: 2/2, step 2110/7134 completed (loss: 0.058732397854328156, acc: 0.9825581312179565)
[2025-02-13 03:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:25][root][INFO] - Training Epoch: 2/2, step 2111/7134 completed (loss: 0.048658452928066254, acc: 0.9883268475532532)
[2025-02-13 03:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26][root][INFO] - Training Epoch: 2/2, step 2112/7134 completed (loss: 0.06554901599884033, acc: 0.9792993664741516)
[2025-02-13 03:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26][root][INFO] - Training Epoch: 2/2, step 2113/7134 completed (loss: 0.020724954083561897, acc: 0.992548406124115)
[2025-02-13 03:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26][root][INFO] - Training Epoch: 2/2, step 2114/7134 completed (loss: 0.029100773856043816, acc: 0.9934640526771545)
[2025-02-13 03:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27][root][INFO] - Training Epoch: 2/2, step 2115/7134 completed (loss: 0.030528973788022995, acc: 0.9924905896186829)
[2025-02-13 03:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27][root][INFO] - Training Epoch: 2/2, step 2116/7134 completed (loss: 0.03909905254840851, acc: 0.9923312664031982)
[2025-02-13 03:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28][root][INFO] - Training Epoch: 2/2, step 2117/7134 completed (loss: 0.01800423674285412, acc: 0.9937810897827148)
[2025-02-13 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28][root][INFO] - Training Epoch: 2/2, step 2118/7134 completed (loss: 0.031256623566150665, acc: 0.9931787252426147)
[2025-02-13 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28][root][INFO] - Training Epoch: 2/2, step 2119/7134 completed (loss: 0.03241495043039322, acc: 0.9947506785392761)
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29][root][INFO] - Training Epoch: 2/2, step 2120/7134 completed (loss: 0.03491071239113808, acc: 0.990867555141449)
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29][root][INFO] - Training Epoch: 2/2, step 2121/7134 completed (loss: 0.030972840264439583, acc: 0.9904761910438538)
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30][root][INFO] - Training Epoch: 2/2, step 2122/7134 completed (loss: 0.01942787878215313, acc: 0.9947916865348816)
[2025-02-13 03:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30][root][INFO] - Training Epoch: 2/2, step 2123/7134 completed (loss: 0.022407082840800285, acc: 0.9952830076217651)
[2025-02-13 03:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30][root][INFO] - Training Epoch: 2/2, step 2124/7134 completed (loss: 0.020160777494311333, acc: 0.9906542301177979)
[2025-02-13 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31][root][INFO] - Training Epoch: 2/2, step 2125/7134 completed (loss: 0.011182744987308979, acc: 0.9984848499298096)
[2025-02-13 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31][root][INFO] - Training Epoch: 2/2, step 2126/7134 completed (loss: 0.024402225390076637, acc: 0.9949579834938049)
[2025-02-13 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:32][root][INFO] - Training Epoch: 2/2, step 2127/7134 completed (loss: 0.01721492037177086, acc: 0.9970501661300659)
[2025-02-13 03:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:32][root][INFO] - Training Epoch: 2/2, step 2128/7134 completed (loss: 0.021636618301272392, acc: 0.990813672542572)
[2025-02-13 03:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33][root][INFO] - Training Epoch: 2/2, step 2129/7134 completed (loss: 0.03795270994305611, acc: 0.9864253401756287)
[2025-02-13 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33][root][INFO] - Training Epoch: 2/2, step 2130/7134 completed (loss: 0.03091510385274887, acc: 0.9917491674423218)
[2025-02-13 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33][root][INFO] - Training Epoch: 2/2, step 2131/7134 completed (loss: 0.016443295404314995, acc: 0.9969040155410767)
[2025-02-13 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34][root][INFO] - Training Epoch: 2/2, step 2132/7134 completed (loss: 0.016037223860621452, acc: 0.9946091771125793)
[2025-02-13 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34][root][INFO] - Training Epoch: 2/2, step 2133/7134 completed (loss: 0.023475397378206253, acc: 0.9933949708938599)
[2025-02-13 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35][root][INFO] - Training Epoch: 2/2, step 2134/7134 completed (loss: 0.009645507670938969, acc: 0.9973226189613342)
[2025-02-13 03:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35][root][INFO] - Training Epoch: 2/2, step 2135/7134 completed (loss: 0.02770570106804371, acc: 0.9894578456878662)
[2025-02-13 03:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35][root][INFO] - Training Epoch: 2/2, step 2136/7134 completed (loss: 0.01997959613800049, acc: 0.9959946870803833)
[2025-02-13 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36][root][INFO] - Training Epoch: 2/2, step 2137/7134 completed (loss: 0.014580887742340565, acc: 0.9948387145996094)
[2025-02-13 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36][root][INFO] - Training Epoch: 2/2, step 2138/7134 completed (loss: 0.00787354912608862, acc: 0.9984756112098694)
[2025-02-13 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:37][root][INFO] - Training Epoch: 2/2, step 2139/7134 completed (loss: 0.011077134869992733, acc: 0.9958391189575195)
[2025-02-13 03:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:37][root][INFO] - Training Epoch: 2/2, step 2140/7134 completed (loss: 0.04344230517745018, acc: 0.9862259030342102)
[2025-02-13 03:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38][root][INFO] - Training Epoch: 2/2, step 2141/7134 completed (loss: 0.020668191835284233, acc: 0.9941520690917969)
[2025-02-13 03:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38][root][INFO] - Training Epoch: 2/2, step 2142/7134 completed (loss: 0.04292585328221321, acc: 0.9844192862510681)
[2025-02-13 03:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38][root][INFO] - Training Epoch: 2/2, step 2143/7134 completed (loss: 0.019319932907819748, acc: 0.9947368502616882)
[2025-02-13 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39][root][INFO] - Training Epoch: 2/2, step 2144/7134 completed (loss: 0.004465972073376179, acc: 0.9986720085144043)
[2025-02-13 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39][root][INFO] - Training Epoch: 2/2, step 2145/7134 completed (loss: 0.0377061665058136, acc: 0.9842632412910461)
[2025-02-13 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:40][root][INFO] - Training Epoch: 2/2, step 2146/7134 completed (loss: 0.0200597383081913, acc: 0.9964788556098938)
[2025-02-13 03:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:40][root][INFO] - Training Epoch: 2/2, step 2147/7134 completed (loss: 0.029274219647049904, acc: 0.9887780547142029)
[2025-02-13 03:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41][root][INFO] - Training Epoch: 2/2, step 2148/7134 completed (loss: 0.04272635281085968, acc: 0.9868995547294617)
[2025-02-13 03:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41][root][INFO] - Training Epoch: 2/2, step 2149/7134 completed (loss: 0.007960151880979538, acc: 0.9985915422439575)
[2025-02-13 03:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41][root][INFO] - Training Epoch: 2/2, step 2150/7134 completed (loss: 0.022784560918807983, acc: 0.9906666874885559)
[2025-02-13 03:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42][root][INFO] - Training Epoch: 2/2, step 2151/7134 completed (loss: 0.01885242573916912, acc: 0.9928571581840515)
[2025-02-13 03:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42][root][INFO] - Training Epoch: 2/2, step 2152/7134 completed (loss: 0.06422527134418488, acc: 0.9779411554336548)
[2025-02-13 03:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:43][root][INFO] - Training Epoch: 2/2, step 2153/7134 completed (loss: 0.040611930191516876, acc: 0.9900110960006714)
[2025-02-13 03:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:43][root][INFO] - Training Epoch: 2/2, step 2154/7134 completed (loss: 0.051637161523103714, acc: 0.983565092086792)
[2025-02-13 03:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44][root][INFO] - Training Epoch: 2/2, step 2155/7134 completed (loss: 0.07679739594459534, acc: 0.9780219793319702)
[2025-02-13 03:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44][root][INFO] - Training Epoch: 2/2, step 2156/7134 completed (loss: 0.018348855897784233, acc: 0.9930555820465088)
[2025-02-13 03:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44][root][INFO] - Training Epoch: 2/2, step 2157/7134 completed (loss: 0.024120286107063293, acc: 0.9910614490509033)
[2025-02-13 03:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:45][root][INFO] - Training Epoch: 2/2, step 2158/7134 completed (loss: 0.0629316046833992, acc: 0.9897959232330322)
[2025-02-13 03:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:45][root][INFO] - Training Epoch: 2/2, step 2159/7134 completed (loss: 0.019895361736416817, acc: 0.9961685538291931)
[2025-02-13 03:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46][root][INFO] - Training Epoch: 2/2, step 2160/7134 completed (loss: 0.0242353156208992, acc: 0.9917469024658203)
[2025-02-13 03:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46][root][INFO] - Training Epoch: 2/2, step 2161/7134 completed (loss: 0.040683191269636154, acc: 0.98975670337677)
[2025-02-13 03:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47][root][INFO] - Training Epoch: 2/2, step 2162/7134 completed (loss: 0.03074747510254383, acc: 0.9894099831581116)
[2025-02-13 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47][root][INFO] - Training Epoch: 2/2, step 2163/7134 completed (loss: 0.037099190056324005, acc: 0.9889763593673706)
[2025-02-13 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47][root][INFO] - Training Epoch: 2/2, step 2164/7134 completed (loss: 0.018450425937771797, acc: 0.9931389093399048)
[2025-02-13 03:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:48][root][INFO] - Training Epoch: 2/2, step 2165/7134 completed (loss: 0.024688322097063065, acc: 0.9928264021873474)
[2025-02-13 03:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:48][root][INFO] - Training Epoch: 2/2, step 2166/7134 completed (loss: 0.03221221640706062, acc: 0.9934895634651184)
[2025-02-13 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:49][root][INFO] - Training Epoch: 2/2, step 2167/7134 completed (loss: 0.03785770386457443, acc: 0.9910913109779358)
[2025-02-13 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:49][root][INFO] - Training Epoch: 2/2, step 2168/7134 completed (loss: 0.03104078769683838, acc: 0.9888888597488403)
[2025-02-13 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:50][root][INFO] - Training Epoch: 2/2, step 2169/7134 completed (loss: 0.041630685329437256, acc: 0.9930394291877747)
[2025-02-13 03:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:50][root][INFO] - Training Epoch: 2/2, step 2170/7134 completed (loss: 0.034189797937870026, acc: 0.9912663698196411)
[2025-02-13 03:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51][root][INFO] - Training Epoch: 2/2, step 2171/7134 completed (loss: 0.037014108151197433, acc: 0.9909399747848511)
[2025-02-13 03:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51][root][INFO] - Training Epoch: 2/2, step 2172/7134 completed (loss: 0.024249473586678505, acc: 0.9896694421768188)
[2025-02-13 03:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51][root][INFO] - Training Epoch: 2/2, step 2173/7134 completed (loss: 0.04509498551487923, acc: 0.9909365773200989)
[2025-02-13 03:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:52][root][INFO] - Training Epoch: 2/2, step 2174/7134 completed (loss: 0.013408195227384567, acc: 0.9943262338638306)
[2025-02-13 03:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:52][root][INFO] - Training Epoch: 2/2, step 2175/7134 completed (loss: 0.07472552359104156, acc: 0.9861496090888977)
[2025-02-13 03:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53][root][INFO] - Training Epoch: 2/2, step 2176/7134 completed (loss: 0.03641781583428383, acc: 0.9881516695022583)
[2025-02-13 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53][root][INFO] - Training Epoch: 2/2, step 2177/7134 completed (loss: 0.028362426906824112, acc: 0.9896103739738464)
[2025-02-13 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:54][root][INFO] - Training Epoch: 2/2, step 2178/7134 completed (loss: 0.01648251712322235, acc: 0.9955849647521973)
[2025-02-13 03:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:54][root][INFO] - Training Epoch: 2/2, step 2179/7134 completed (loss: 0.028960095718503, acc: 0.9949367046356201)
[2025-02-13 03:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55][root][INFO] - Training Epoch: 2/2, step 2180/7134 completed (loss: 0.026935070753097534, acc: 0.9905437231063843)
[2025-02-13 03:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55][root][INFO] - Training Epoch: 2/2, step 2181/7134 completed (loss: 0.01889115944504738, acc: 0.995708167552948)
[2025-02-13 03:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55][root][INFO] - Training Epoch: 2/2, step 2182/7134 completed (loss: 0.04283669590950012, acc: 0.9861591458320618)
[2025-02-13 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56][root][INFO] - Training Epoch: 2/2, step 2183/7134 completed (loss: 0.027756545692682266, acc: 0.9941792488098145)
[2025-02-13 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56][root][INFO] - Training Epoch: 2/2, step 2184/7134 completed (loss: 0.024597102776169777, acc: 0.9936102032661438)
[2025-02-13 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57][root][INFO] - Training Epoch: 2/2, step 2185/7134 completed (loss: 0.05272785946726799, acc: 0.9858356714248657)
[2025-02-13 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57][root][INFO] - Training Epoch: 2/2, step 2186/7134 completed (loss: 0.06970827281475067, acc: 0.9854133129119873)
[2025-02-13 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58][root][INFO] - Training Epoch: 2/2, step 2187/7134 completed (loss: 0.08755387365818024, acc: 0.9807999730110168)
[2025-02-13 03:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58][root][INFO] - Training Epoch: 2/2, step 2188/7134 completed (loss: 0.02846536971628666, acc: 0.9890561103820801)
[2025-02-13 03:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59][root][INFO] - Training Epoch: 2/2, step 2189/7134 completed (loss: 0.0456143356859684, acc: 0.9816513657569885)
[2025-02-13 03:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59][root][INFO] - Training Epoch: 2/2, step 2190/7134 completed (loss: 0.046989407390356064, acc: 0.9868074059486389)
[2025-02-13 03:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59][root][INFO] - Training Epoch: 2/2, step 2191/7134 completed (loss: 0.026554645970463753, acc: 0.9945799708366394)
[2025-02-13 03:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00][root][INFO] - Training Epoch: 2/2, step 2192/7134 completed (loss: 0.009986094199120998, acc: 0.9962335228919983)
[2025-02-13 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00][root][INFO] - Training Epoch: 2/2, step 2193/7134 completed (loss: 0.03297512233257294, acc: 0.9933510422706604)
[2025-02-13 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01][root][INFO] - Training Epoch: 2/2, step 2194/7134 completed (loss: 0.03733539581298828, acc: 0.9882746934890747)
[2025-02-13 03:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01][root][INFO] - Training Epoch: 2/2, step 2195/7134 completed (loss: 0.015721958130598068, acc: 0.9936000108718872)
[2025-02-13 03:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01][root][INFO] - Training Epoch: 2/2, step 2196/7134 completed (loss: 0.02617437206208706, acc: 0.9935232996940613)
[2025-02-13 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:02][root][INFO] - Training Epoch: 2/2, step 2197/7134 completed (loss: 0.029447495937347412, acc: 0.9890710115432739)
[2025-02-13 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:02][root][INFO] - Training Epoch: 2/2, step 2198/7134 completed (loss: 0.04186020791530609, acc: 0.9894419312477112)
[2025-02-13 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:03][root][INFO] - Training Epoch: 2/2, step 2199/7134 completed (loss: 0.014552492648363113, acc: 0.995468258857727)
[2025-02-13 03:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:03][root][INFO] - Training Epoch: 2/2, step 2200/7134 completed (loss: 0.02854068949818611, acc: 0.9927667379379272)
[2025-02-13 03:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][root][INFO] - Training Epoch: 2/2, step 2201/7134 completed (loss: 0.015140960924327374, acc: 0.9969696998596191)
[2025-02-13 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][root][INFO] - Training Epoch: 2/2, step 2202/7134 completed (loss: 0.02482445538043976, acc: 0.9916527271270752)
[2025-02-13 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][root][INFO] - Training Epoch: 2/2, step 2203/7134 completed (loss: 0.008162179961800575, acc: 0.9972602725028992)
[2025-02-13 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:05][root][INFO] - Training Epoch: 2/2, step 2204/7134 completed (loss: 0.012940964661538601, acc: 0.9941520690917969)
[2025-02-13 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:05][root][INFO] - Training Epoch: 2/2, step 2205/7134 completed (loss: 0.025953572243452072, acc: 0.9929824471473694)
[2025-02-13 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06][root][INFO] - Training Epoch: 2/2, step 2206/7134 completed (loss: 0.04972166195511818, acc: 0.9859402179718018)
[2025-02-13 03:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06][root][INFO] - Training Epoch: 2/2, step 2207/7134 completed (loss: 0.013853393495082855, acc: 0.9951534867286682)
[2025-02-13 03:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06][root][INFO] - Training Epoch: 2/2, step 2208/7134 completed (loss: 0.041484616696834564, acc: 0.9917355179786682)
[2025-02-13 03:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:07][root][INFO] - Training Epoch: 2/2, step 2209/7134 completed (loss: 0.025483328849077225, acc: 0.9900990128517151)
[2025-02-13 03:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:07][root][INFO] - Training Epoch: 2/2, step 2210/7134 completed (loss: 0.03539443016052246, acc: 0.9893778562545776)
[2025-02-13 03:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08][root][INFO] - Training Epoch: 2/2, step 2211/7134 completed (loss: 0.028799235820770264, acc: 0.9944598078727722)
[2025-02-13 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08][root][INFO] - Training Epoch: 2/2, step 2212/7134 completed (loss: 0.007197273895144463, acc: 0.9986110925674438)
[2025-02-13 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09][root][INFO] - Training Epoch: 2/2, step 2213/7134 completed (loss: 0.021527038887143135, acc: 0.9901130199432373)
[2025-02-13 03:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09][root][INFO] - Training Epoch: 2/2, step 2214/7134 completed (loss: 0.03829691931605339, acc: 0.9880596995353699)
[2025-02-13 03:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09][root][INFO] - Training Epoch: 2/2, step 2215/7134 completed (loss: 0.019400708377361298, acc: 0.992977499961853)
[2025-02-13 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10][root][INFO] - Training Epoch: 2/2, step 2216/7134 completed (loss: 0.018832821398973465, acc: 0.9919893145561218)
[2025-02-13 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10][root][INFO] - Training Epoch: 2/2, step 2217/7134 completed (loss: 0.027284041047096252, acc: 0.9925280213356018)
[2025-02-13 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11][root][INFO] - Training Epoch: 2/2, step 2218/7134 completed (loss: 0.033553171902894974, acc: 0.9928160905838013)
[2025-02-13 03:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11][root][INFO] - Training Epoch: 2/2, step 2219/7134 completed (loss: 0.013016967102885246, acc: 0.9953632354736328)
[2025-02-13 03:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11][root][INFO] - Training Epoch: 2/2, step 2220/7134 completed (loss: 0.009432118386030197, acc: 1.0)
[2025-02-13 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:12][root][INFO] - Training Epoch: 2/2, step 2221/7134 completed (loss: 0.020527807995676994, acc: 0.9961038827896118)
[2025-02-13 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:12][root][INFO] - Training Epoch: 2/2, step 2222/7134 completed (loss: 0.024842670187354088, acc: 0.9927745461463928)
[2025-02-13 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13][root][INFO] - Training Epoch: 2/2, step 2223/7134 completed (loss: 0.021633241325616837, acc: 0.9958158731460571)
[2025-02-13 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13][root][INFO] - Training Epoch: 2/2, step 2224/7134 completed (loss: 0.015395061112940311, acc: 0.9958847761154175)
[2025-02-13 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14][root][INFO] - Training Epoch: 2/2, step 2225/7134 completed (loss: 0.013404754921793938, acc: 0.9956584572792053)
[2025-02-13 03:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14][root][INFO] - Training Epoch: 2/2, step 2226/7134 completed (loss: 0.021002814173698425, acc: 0.9923896789550781)
[2025-02-13 03:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14][root][INFO] - Training Epoch: 2/2, step 2227/7134 completed (loss: 0.014891141094267368, acc: 0.9970717430114746)
[2025-02-13 03:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15][root][INFO] - Training Epoch: 2/2, step 2228/7134 completed (loss: 0.011829793453216553, acc: 0.9967479705810547)
[2025-02-13 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15][root][INFO] - Training Epoch: 2/2, step 2229/7134 completed (loss: 0.01908627338707447, acc: 0.9959999918937683)
[2025-02-13 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16][root][INFO] - Training Epoch: 2/2, step 2230/7134 completed (loss: 0.017911432310938835, acc: 0.9967637658119202)
[2025-02-13 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16][root][INFO] - Training Epoch: 2/2, step 2231/7134 completed (loss: 0.04174073040485382, acc: 0.9840637445449829)
[2025-02-13 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16][root][INFO] - Training Epoch: 2/2, step 2232/7134 completed (loss: 0.012237755581736565, acc: 0.9946523904800415)
[2025-02-13 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:17][root][INFO] - Training Epoch: 2/2, step 2233/7134 completed (loss: 0.031070668250322342, acc: 0.9871976971626282)
[2025-02-13 03:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:17][root][INFO] - Training Epoch: 2/2, step 2234/7134 completed (loss: 0.0030820344109088182, acc: 1.0)
[2025-02-13 03:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18][root][INFO] - Training Epoch: 2/2, step 2235/7134 completed (loss: 0.039188504219055176, acc: 0.9921630024909973)
[2025-02-13 03:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18][root][INFO] - Training Epoch: 2/2, step 2236/7134 completed (loss: 0.020654335618019104, acc: 0.9925037622451782)
[2025-02-13 03:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18][root][INFO] - Training Epoch: 2/2, step 2237/7134 completed (loss: 0.035384222865104675, acc: 0.9915134310722351)
[2025-02-13 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:19][root][INFO] - Training Epoch: 2/2, step 2238/7134 completed (loss: 0.06110963970422745, acc: 0.986975371837616)
[2025-02-13 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:19][root][INFO] - Training Epoch: 2/2, step 2239/7134 completed (loss: 0.08130739629268646, acc: 0.9818181991577148)
[2025-02-13 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20][root][INFO] - Training Epoch: 2/2, step 2240/7134 completed (loss: 0.03528143838047981, acc: 0.9896103739738464)
[2025-02-13 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20][root][INFO] - Training Epoch: 2/2, step 2241/7134 completed (loss: 0.06744009256362915, acc: 0.978723406791687)
[2025-02-13 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21][root][INFO] - Training Epoch: 2/2, step 2242/7134 completed (loss: 0.08648443967103958, acc: 0.978622317314148)
[2025-02-13 03:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21][root][INFO] - Training Epoch: 2/2, step 2243/7134 completed (loss: 0.0842982605099678, acc: 0.9770992398262024)
[2025-02-13 03:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21][root][INFO] - Training Epoch: 2/2, step 2244/7134 completed (loss: 0.02552618831396103, acc: 0.9889937043190002)
[2025-02-13 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:22][root][INFO] - Training Epoch: 2/2, step 2245/7134 completed (loss: 0.06519972532987595, acc: 0.9773463010787964)
[2025-02-13 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:22][root][INFO] - Training Epoch: 2/2, step 2246/7134 completed (loss: 0.09390244632959366, acc: 0.9689189195632935)
[2025-02-13 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:23][root][INFO] - Training Epoch: 2/2, step 2247/7134 completed (loss: 0.07874102145433426, acc: 0.977011501789093)
[2025-02-13 03:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:23][root][INFO] - Training Epoch: 2/2, step 2248/7134 completed (loss: 0.052534665912389755, acc: 0.9875518679618835)
[2025-02-13 03:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24][root][INFO] - Training Epoch: 2/2, step 2249/7134 completed (loss: 0.040846049785614014, acc: 0.9860334992408752)
[2025-02-13 03:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24][root][INFO] - Training Epoch: 2/2, step 2250/7134 completed (loss: 0.0444251224398613, acc: 0.9893048405647278)
[2025-02-13 03:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24][root][INFO] - Training Epoch: 2/2, step 2251/7134 completed (loss: 0.046976517885923386, acc: 0.9909297227859497)
[2025-02-13 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25][root][INFO] - Training Epoch: 2/2, step 2252/7134 completed (loss: 0.04187539964914322, acc: 0.9845938086509705)
[2025-02-13 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25][root][INFO] - Training Epoch: 2/2, step 2253/7134 completed (loss: 0.03387301787734032, acc: 0.9899665713310242)
[2025-02-13 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26][root][INFO] - Training Epoch: 2/2, step 2254/7134 completed (loss: 0.10980558395385742, acc: 0.9681274890899658)
[2025-02-13 03:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26][root][INFO] - Training Epoch: 2/2, step 2255/7134 completed (loss: 0.03537461906671524, acc: 0.9863429665565491)
[2025-02-13 03:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26][root][INFO] - Training Epoch: 2/2, step 2256/7134 completed (loss: 0.07928866893053055, acc: 0.9668141603469849)
[2025-02-13 03:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27][root][INFO] - Training Epoch: 2/2, step 2257/7134 completed (loss: 0.03089986927807331, acc: 0.9909365773200989)
[2025-02-13 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27][root][INFO] - Training Epoch: 2/2, step 2258/7134 completed (loss: 0.034283414483070374, acc: 0.9903846383094788)
[2025-02-13 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28][root][INFO] - Training Epoch: 2/2, step 2259/7134 completed (loss: 0.02821831777691841, acc: 0.9911110997200012)
[2025-02-13 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28][root][INFO] - Training Epoch: 2/2, step 2260/7134 completed (loss: 0.0809440016746521, acc: 0.9772727489471436)
[2025-02-13 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28][root][INFO] - Training Epoch: 2/2, step 2261/7134 completed (loss: 0.07750178873538971, acc: 0.970802903175354)
[2025-02-13 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29][root][INFO] - Training Epoch: 2/2, step 2262/7134 completed (loss: 0.018358509987592697, acc: 0.9944751262664795)
[2025-02-13 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29][root][INFO] - Training Epoch: 2/2, step 2263/7134 completed (loss: 0.037287380546331406, acc: 0.9891892075538635)
[2025-02-13 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29][root][INFO] - Training Epoch: 2/2, step 2264/7134 completed (loss: 0.07173900306224823, acc: 0.9774965047836304)
[2025-02-13 03:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:30][root][INFO] - Training Epoch: 2/2, step 2265/7134 completed (loss: 0.07649768888950348, acc: 0.9810996651649475)
[2025-02-13 03:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:30][root][INFO] - Training Epoch: 2/2, step 2266/7134 completed (loss: 0.051843587309122086, acc: 0.9919571280479431)
[2025-02-13 03:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31][root][INFO] - Training Epoch: 2/2, step 2267/7134 completed (loss: 0.01291667204350233, acc: 0.9979423880577087)
[2025-02-13 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31][root][INFO] - Training Epoch: 2/2, step 2268/7134 completed (loss: 0.025648431852459908, acc: 0.991391658782959)
[2025-02-13 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32][root][INFO] - Training Epoch: 2/2, step 2269/7134 completed (loss: 0.015514837577939034, acc: 0.9937106966972351)
[2025-02-13 03:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32][root][INFO] - Training Epoch: 2/2, step 2270/7134 completed (loss: 0.0698309913277626, acc: 0.9767827391624451)
[2025-02-13 03:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32][root][INFO] - Training Epoch: 2/2, step 2271/7134 completed (loss: 0.04235545173287392, acc: 0.9850402474403381)
[2025-02-13 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:33][root][INFO] - Training Epoch: 2/2, step 2272/7134 completed (loss: 0.030304541811347008, acc: 0.9926650524139404)
[2025-02-13 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:33][root][INFO] - Training Epoch: 2/2, step 2273/7134 completed (loss: 0.015589659102261066, acc: 0.9964072108268738)
[2025-02-13 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34][root][INFO] - Training Epoch: 2/2, step 2274/7134 completed (loss: 0.09328256547451019, acc: 0.9808917045593262)
[2025-02-13 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34][root][INFO] - Training Epoch: 2/2, step 2275/7134 completed (loss: 0.023561207577586174, acc: 0.9941314458847046)
[2025-02-13 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:35][root][INFO] - Training Epoch: 2/2, step 2276/7134 completed (loss: 0.04271586239337921, acc: 0.9905213117599487)
[2025-02-13 03:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:35][root][INFO] - Training Epoch: 2/2, step 2277/7134 completed (loss: 0.025800520554184914, acc: 0.9930716156959534)
[2025-02-13 03:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36][root][INFO] - Training Epoch: 2/2, step 2278/7134 completed (loss: 0.09400322288274765, acc: 0.9792332053184509)
[2025-02-13 03:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36][root][INFO] - Training Epoch: 2/2, step 2279/7134 completed (loss: 0.02789384126663208, acc: 0.9940968155860901)
[2025-02-13 03:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36][root][INFO] - Training Epoch: 2/2, step 2280/7134 completed (loss: 0.03768020495772362, acc: 0.9917355179786682)
[2025-02-13 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:37][root][INFO] - Training Epoch: 2/2, step 2281/7134 completed (loss: 0.09396345168352127, acc: 0.9866666793823242)
[2025-02-13 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:37][root][INFO] - Training Epoch: 2/2, step 2282/7134 completed (loss: 0.012954158708453178, acc: 0.998763918876648)
[2025-02-13 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:38][root][INFO] - Training Epoch: 2/2, step 2283/7134 completed (loss: 0.02722356654703617, acc: 0.9926650524139404)
[2025-02-13 03:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:38][root][INFO] - Training Epoch: 2/2, step 2284/7134 completed (loss: 0.03551178425550461, acc: 0.9911373853683472)
[2025-02-13 03:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:39][root][INFO] - Training Epoch: 2/2, step 2285/7134 completed (loss: 0.015837175771594048, acc: 0.9955406785011292)
[2025-02-13 03:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:39][root][INFO] - Training Epoch: 2/2, step 2286/7134 completed (loss: 0.04048442840576172, acc: 0.9870298504829407)
[2025-02-13 03:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40][root][INFO] - Training Epoch: 2/2, step 2287/7134 completed (loss: 0.024468787014484406, acc: 0.9865951538085938)
[2025-02-13 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40][root][INFO] - Training Epoch: 2/2, step 2288/7134 completed (loss: 0.025273233652114868, acc: 0.9952885508537292)
[2025-02-13 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40][root][INFO] - Training Epoch: 2/2, step 2289/7134 completed (loss: 0.02818901650607586, acc: 0.9910141229629517)
[2025-02-13 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:41][root][INFO] - Training Epoch: 2/2, step 2290/7134 completed (loss: 0.026205165311694145, acc: 0.993630588054657)
[2025-02-13 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:41][root][INFO] - Training Epoch: 2/2, step 2291/7134 completed (loss: 0.03718806430697441, acc: 0.9892037510871887)
[2025-02-13 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:42][root][INFO] - Training Epoch: 2/2, step 2292/7134 completed (loss: 0.06279970705509186, acc: 0.9870967864990234)
[2025-02-13 03:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:42][root][INFO] - Training Epoch: 2/2, step 2293/7134 completed (loss: 0.036469507962465286, acc: 0.9888641238212585)
[2025-02-13 03:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43][root][INFO] - Training Epoch: 2/2, step 2294/7134 completed (loss: 0.03839142993092537, acc: 0.9876265525817871)
[2025-02-13 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43][root][INFO] - Training Epoch: 2/2, step 2295/7134 completed (loss: 0.038692329078912735, acc: 0.9886040091514587)
[2025-02-13 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44][root][INFO] - Training Epoch: 2/2, step 2296/7134 completed (loss: 0.018714796751737595, acc: 0.992553174495697)
[2025-02-13 03:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44][root][INFO] - Training Epoch: 2/2, step 2297/7134 completed (loss: 0.023315206170082092, acc: 0.9928861856460571)
[2025-02-13 03:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44][root][INFO] - Training Epoch: 2/2, step 2298/7134 completed (loss: 0.05956168845295906, acc: 0.9831755757331848)
[2025-02-13 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45][root][INFO] - Training Epoch: 2/2, step 2299/7134 completed (loss: 0.03618483245372772, acc: 0.9874857664108276)
[2025-02-13 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45][root][INFO] - Training Epoch: 2/2, step 2300/7134 completed (loss: 0.009418269619345665, acc: 0.9977452158927917)
[2025-02-13 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46][root][INFO] - Training Epoch: 2/2, step 2301/7134 completed (loss: 0.03555786982178688, acc: 0.9920529723167419)
[2025-02-13 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46][root][INFO] - Training Epoch: 2/2, step 2302/7134 completed (loss: 0.041445791721343994, acc: 0.9889705777168274)
[2025-02-13 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:47][root][INFO] - Training Epoch: 2/2, step 2303/7134 completed (loss: 0.02842698246240616, acc: 0.9886105060577393)
[2025-02-13 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:47][root][INFO] - Training Epoch: 2/2, step 2304/7134 completed (loss: 0.04866914451122284, acc: 0.9856114983558655)
[2025-02-13 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:48][root][INFO] - Training Epoch: 2/2, step 2305/7134 completed (loss: 0.021221941336989403, acc: 0.9946751594543457)
[2025-02-13 03:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:48][root][INFO] - Training Epoch: 2/2, step 2306/7134 completed (loss: 0.01716586761176586, acc: 0.9944567680358887)
[2025-02-13 03:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49][root][INFO] - Training Epoch: 2/2, step 2307/7134 completed (loss: 0.0315471813082695, acc: 0.9927685856819153)
[2025-02-13 03:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49][root][INFO] - Training Epoch: 2/2, step 2308/7134 completed (loss: 0.03463363274931908, acc: 0.9928498268127441)
[2025-02-13 03:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49][root][INFO] - Training Epoch: 2/2, step 2309/7134 completed (loss: 0.016319775953888893, acc: 0.9964028596878052)
[2025-02-13 03:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:50][root][INFO] - Training Epoch: 2/2, step 2310/7134 completed (loss: 0.02532639168202877, acc: 0.993220329284668)
[2025-02-13 03:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:50][root][INFO] - Training Epoch: 2/2, step 2311/7134 completed (loss: 0.034791797399520874, acc: 0.9908257126808167)
[2025-02-13 03:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:51][root][INFO] - Training Epoch: 2/2, step 2312/7134 completed (loss: 0.019232437014579773, acc: 0.9967602491378784)
[2025-02-13 03:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:51][root][INFO] - Training Epoch: 2/2, step 2313/7134 completed (loss: 0.04223589226603508, acc: 0.9863013625144958)
[2025-02-13 03:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:52][root][INFO] - Training Epoch: 2/2, step 2314/7134 completed (loss: 0.02313574217259884, acc: 0.9944506287574768)
[2025-02-13 03:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:52][root][INFO] - Training Epoch: 2/2, step 2315/7134 completed (loss: 0.025002382695674896, acc: 0.9917647242546082)
[2025-02-13 03:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:53][root][INFO] - Training Epoch: 2/2, step 2316/7134 completed (loss: 0.02444497123360634, acc: 0.9930955171585083)
[2025-02-13 03:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:53][root][INFO] - Training Epoch: 2/2, step 2317/7134 completed (loss: 0.027468275278806686, acc: 0.9947090148925781)
[2025-02-13 03:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][root][INFO] - Training Epoch: 2/2, step 2318/7134 completed (loss: 0.012824325822293758, acc: 0.9958158731460571)
[2025-02-13 03:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][root][INFO] - Training Epoch: 2/2, step 2319/7134 completed (loss: 0.015629202127456665, acc: 0.9956236481666565)
[2025-02-13 03:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][root][INFO] - Training Epoch: 2/2, step 2320/7134 completed (loss: 0.016709979623556137, acc: 0.9943181872367859)
[2025-02-13 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:55][root][INFO] - Training Epoch: 2/2, step 2321/7134 completed (loss: 0.023189658299088478, acc: 0.9944444298744202)
[2025-02-13 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:55][root][INFO] - Training Epoch: 2/2, step 2322/7134 completed (loss: 0.02385498397052288, acc: 0.9881481528282166)
[2025-02-13 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:56][root][INFO] - Training Epoch: 2/2, step 2323/7134 completed (loss: 0.032519061118364334, acc: 0.9858044385910034)
[2025-02-13 03:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:56][root][INFO] - Training Epoch: 2/2, step 2324/7134 completed (loss: 0.04654897376894951, acc: 0.9871794581413269)
[2025-02-13 03:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57][root][INFO] - Training Epoch: 2/2, step 2325/7134 completed (loss: 0.039105374366045, acc: 0.988034188747406)
[2025-02-13 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57][root][INFO] - Training Epoch: 2/2, step 2326/7134 completed (loss: 0.012780430726706982, acc: 0.9963235259056091)
[2025-02-13 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57][root][INFO] - Training Epoch: 2/2, step 2327/7134 completed (loss: 0.016902634873986244, acc: 0.9950819611549377)
[2025-02-13 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:58][root][INFO] - Training Epoch: 2/2, step 2328/7134 completed (loss: 0.06879281997680664, acc: 0.9856630563735962)
[2025-02-13 03:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:58][root][INFO] - Training Epoch: 2/2, step 2329/7134 completed (loss: 0.023824529722332954, acc: 0.9984496235847473)
[2025-02-13 03:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59][root][INFO] - Training Epoch: 2/2, step 2330/7134 completed (loss: 0.01969573274254799, acc: 0.9938931465148926)
[2025-02-13 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59][root][INFO] - Training Epoch: 2/2, step 2331/7134 completed (loss: 0.012582209892570972, acc: 0.9969325065612793)
[2025-02-13 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59][root][INFO] - Training Epoch: 2/2, step 2332/7134 completed (loss: 0.03115958720445633, acc: 0.9925705790519714)
[2025-02-13 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:00][root][INFO] - Training Epoch: 2/2, step 2333/7134 completed (loss: 0.012015447951853275, acc: 0.9955357313156128)
[2025-02-13 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:00][root][INFO] - Training Epoch: 2/2, step 2334/7134 completed (loss: 0.020884694531559944, acc: 0.9910447597503662)
[2025-02-13 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01][root][INFO] - Training Epoch: 2/2, step 2335/7134 completed (loss: 0.028615470975637436, acc: 0.9935483932495117)
[2025-02-13 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01][root][INFO] - Training Epoch: 2/2, step 2336/7134 completed (loss: 0.010688281618058681, acc: 0.9953632354736328)
[2025-02-13 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01][root][INFO] - Training Epoch: 2/2, step 2337/7134 completed (loss: 0.002385492669418454, acc: 1.0)
[2025-02-13 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:02][root][INFO] - Training Epoch: 2/2, step 2338/7134 completed (loss: 0.022053584456443787, acc: 0.9953632354736328)
[2025-02-13 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:02][root][INFO] - Training Epoch: 2/2, step 2339/7134 completed (loss: 0.037962689995765686, acc: 0.990212082862854)
[2025-02-13 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03][root][INFO] - Training Epoch: 2/2, step 2340/7134 completed (loss: 0.008879536762833595, acc: 0.9984917044639587)
[2025-02-13 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03][root][INFO] - Training Epoch: 2/2, step 2341/7134 completed (loss: 0.00735132023692131, acc: 0.9981203079223633)
[2025-02-13 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04][root][INFO] - Training Epoch: 2/2, step 2342/7134 completed (loss: 0.01113580446690321, acc: 0.9970458149909973)
[2025-02-13 03:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04][root][INFO] - Training Epoch: 2/2, step 2343/7134 completed (loss: 0.014523252844810486, acc: 0.995199978351593)
[2025-02-13 03:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04][root][INFO] - Training Epoch: 2/2, step 2344/7134 completed (loss: 0.009938064031302929, acc: 0.9968701004981995)
[2025-02-13 03:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:05][root][INFO] - Training Epoch: 2/2, step 2345/7134 completed (loss: 0.013231228105723858, acc: 0.9958041906356812)
[2025-02-13 03:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:05][root][INFO] - Training Epoch: 2/2, step 2346/7134 completed (loss: 0.012688705697655678, acc: 0.9943342804908752)
[2025-02-13 03:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06][root][INFO] - Training Epoch: 2/2, step 2347/7134 completed (loss: 0.059911616146564484, acc: 0.9868420958518982)
[2025-02-13 03:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06][root][INFO] - Training Epoch: 2/2, step 2348/7134 completed (loss: 0.07156429439783096, acc: 0.9785624146461487)
[2025-02-13 03:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06][root][INFO] - Training Epoch: 2/2, step 2349/7134 completed (loss: 0.033315397799015045, acc: 0.9902777671813965)
[2025-02-13 03:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:07][root][INFO] - Training Epoch: 2/2, step 2350/7134 completed (loss: 0.0705408975481987, acc: 0.9832317233085632)
[2025-02-13 03:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:07][root][INFO] - Training Epoch: 2/2, step 2351/7134 completed (loss: 0.047402020543813705, acc: 0.9863387942314148)
[2025-02-13 03:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08][root][INFO] - Training Epoch: 2/2, step 2352/7134 completed (loss: 0.032337792217731476, acc: 0.9919871687889099)
[2025-02-13 03:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08][root][INFO] - Training Epoch: 2/2, step 2353/7134 completed (loss: 0.03856931999325752, acc: 0.9872340559959412)
[2025-02-13 03:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09][root][INFO] - Training Epoch: 2/2, step 2354/7134 completed (loss: 0.025459492579102516, acc: 0.9911392331123352)
[2025-02-13 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09][root][INFO] - Training Epoch: 2/2, step 2355/7134 completed (loss: 0.041825637221336365, acc: 0.990123450756073)
[2025-02-13 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09][root][INFO] - Training Epoch: 2/2, step 2356/7134 completed (loss: 0.05003726854920387, acc: 0.9840989112854004)
[2025-02-13 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10][root][INFO] - Training Epoch: 2/2, step 2357/7134 completed (loss: 0.047609396278858185, acc: 0.9821428656578064)
[2025-02-13 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10][root][INFO] - Training Epoch: 2/2, step 2358/7134 completed (loss: 0.03581194579601288, acc: 0.9903614521026611)
[2025-02-13 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:11][root][INFO] - Training Epoch: 2/2, step 2359/7134 completed (loss: 0.0404214970767498, acc: 0.98531574010849)
[2025-02-13 03:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:11][root][INFO] - Training Epoch: 2/2, step 2360/7134 completed (loss: 0.021200105547904968, acc: 0.9941434860229492)
[2025-02-13 03:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12][root][INFO] - Training Epoch: 2/2, step 2361/7134 completed (loss: 0.040400248020887375, acc: 0.9866864085197449)
[2025-02-13 03:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12][root][INFO] - Training Epoch: 2/2, step 2362/7134 completed (loss: 0.0317828506231308, acc: 0.9910314083099365)
[2025-02-13 03:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12][root][INFO] - Training Epoch: 2/2, step 2363/7134 completed (loss: 0.03807472437620163, acc: 0.9907299876213074)
[2025-02-13 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13][root][INFO] - Training Epoch: 2/2, step 2364/7134 completed (loss: 0.023363763466477394, acc: 0.9878234267234802)
[2025-02-13 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13][root][INFO] - Training Epoch: 2/2, step 2365/7134 completed (loss: 0.016244467347860336, acc: 0.9931129217147827)
[2025-02-13 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:14][root][INFO] - Training Epoch: 2/2, step 2366/7134 completed (loss: 0.012771043926477432, acc: 0.9965277910232544)
[2025-02-13 03:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:14][root][INFO] - Training Epoch: 2/2, step 2367/7134 completed (loss: 0.042134400457143784, acc: 0.9899497628211975)
[2025-02-13 03:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15][root][INFO] - Training Epoch: 2/2, step 2368/7134 completed (loss: 0.027708103880286217, acc: 0.9939393997192383)
[2025-02-13 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15][root][INFO] - Training Epoch: 2/2, step 2369/7134 completed (loss: 0.02000471018254757, acc: 0.9917647242546082)
[2025-02-13 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15][root][INFO] - Training Epoch: 2/2, step 2370/7134 completed (loss: 0.025359656661748886, acc: 0.9938271641731262)
[2025-02-13 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:16][root][INFO] - Training Epoch: 2/2, step 2371/7134 completed (loss: 0.01945401169359684, acc: 0.9956772327423096)
[2025-02-13 03:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:16][root][INFO] - Training Epoch: 2/2, step 2372/7134 completed (loss: 0.02888217754662037, acc: 0.9943310618400574)
[2025-02-13 03:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:17][root][INFO] - Training Epoch: 2/2, step 2373/7134 completed (loss: 0.016975870355963707, acc: 0.9975185990333557)
[2025-02-13 03:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:17][root][INFO] - Training Epoch: 2/2, step 2374/7134 completed (loss: 0.013077049516141415, acc: 0.993306577205658)
[2025-02-13 03:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18][root][INFO] - Training Epoch: 2/2, step 2375/7134 completed (loss: 0.04911130294203758, acc: 0.989393949508667)
[2025-02-13 03:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18][root][INFO] - Training Epoch: 2/2, step 2376/7134 completed (loss: 0.08905702084302902, acc: 0.9761499166488647)
[2025-02-13 03:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18][root][INFO] - Training Epoch: 2/2, step 2377/7134 completed (loss: 0.060167498886585236, acc: 0.9844683408737183)
[2025-02-13 03:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:19][root][INFO] - Training Epoch: 2/2, step 2378/7134 completed (loss: 0.05354752764105797, acc: 0.9894366264343262)
[2025-02-13 03:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:19][root][INFO] - Training Epoch: 2/2, step 2379/7134 completed (loss: 0.0804012343287468, acc: 0.977931022644043)
[2025-02-13 03:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20][root][INFO] - Training Epoch: 2/2, step 2380/7134 completed (loss: 0.03815160691738129, acc: 0.9891172647476196)
[2025-02-13 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20][root][INFO] - Training Epoch: 2/2, step 2381/7134 completed (loss: 0.10748660564422607, acc: 0.9695431590080261)
[2025-02-13 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20][root][INFO] - Training Epoch: 2/2, step 2382/7134 completed (loss: 0.03463078290224075, acc: 0.9892966151237488)
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21][root][INFO] - Training Epoch: 2/2, step 2383/7134 completed (loss: 0.08019237965345383, acc: 0.9803921580314636)
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21][root][INFO] - Training Epoch: 2/2, step 2384/7134 completed (loss: 0.06331668794155121, acc: 0.9843137264251709)
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:22][root][INFO] - Training Epoch: 2/2, step 2385/7134 completed (loss: 0.037700045853853226, acc: 0.9875690340995789)
[2025-02-13 03:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:22][root][INFO] - Training Epoch: 2/2, step 2386/7134 completed (loss: 0.036636706441640854, acc: 0.985602080821991)
[2025-02-13 03:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:23][root][INFO] - Training Epoch: 2/2, step 2387/7134 completed (loss: 0.0394057035446167, acc: 0.9895591735839844)
[2025-02-13 03:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:23][root][INFO] - Training Epoch: 2/2, step 2388/7134 completed (loss: 0.07765141129493713, acc: 0.9785459041595459)
[2025-02-13 03:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24][root][INFO] - Training Epoch: 2/2, step 2389/7134 completed (loss: 0.05321386456489563, acc: 0.9862227439880371)
[2025-02-13 03:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24][root][INFO] - Training Epoch: 2/2, step 2390/7134 completed (loss: 0.06139381229877472, acc: 0.9844683408737183)
[2025-02-13 03:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24][root][INFO] - Training Epoch: 2/2, step 2391/7134 completed (loss: 0.06506166607141495, acc: 0.9796696305274963)
[2025-02-13 03:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25][root][INFO] - Training Epoch: 2/2, step 2392/7134 completed (loss: 0.041486337780952454, acc: 0.9879840016365051)
[2025-02-13 03:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25][root][INFO] - Training Epoch: 2/2, step 2393/7134 completed (loss: 0.05779355764389038, acc: 0.9805447459220886)
[2025-02-13 03:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26][root][INFO] - Training Epoch: 2/2, step 2394/7134 completed (loss: 0.02749331295490265, acc: 0.9915151596069336)
[2025-02-13 03:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26][root][INFO] - Training Epoch: 2/2, step 2395/7134 completed (loss: 0.11951036751270294, acc: 0.9674267172813416)
[2025-02-13 03:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27][root][INFO] - Training Epoch: 2/2, step 2396/7134 completed (loss: 0.06557595729827881, acc: 0.9765840172767639)
[2025-02-13 03:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27][root][INFO] - Training Epoch: 2/2, step 2397/7134 completed (loss: 0.08019386231899261, acc: 0.9737991094589233)
[2025-02-13 03:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27][root][INFO] - Training Epoch: 2/2, step 2398/7134 completed (loss: 0.02902231737971306, acc: 0.9862448573112488)
[2025-02-13 03:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:28][root][INFO] - Training Epoch: 2/2, step 2399/7134 completed (loss: 0.045417629182338715, acc: 0.9842632412910461)
[2025-02-13 03:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:28][root][INFO] - Training Epoch: 2/2, step 2400/7134 completed (loss: 0.022328075021505356, acc: 0.9917241334915161)
[2025-02-13 03:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29][root][INFO] - Training Epoch: 2/2, step 2401/7134 completed (loss: 0.09348804503679276, acc: 0.982594907283783)
[2025-02-13 03:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29][root][INFO] - Training Epoch: 2/2, step 2402/7134 completed (loss: 0.06273537129163742, acc: 0.9760956168174744)
[2025-02-13 03:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30][root][INFO] - Training Epoch: 2/2, step 2403/7134 completed (loss: 0.035536687821149826, acc: 0.9872340559959412)
[2025-02-13 03:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30][root][INFO] - Training Epoch: 2/2, step 2404/7134 completed (loss: 0.026283804327249527, acc: 0.9918032884597778)
[2025-02-13 03:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30][root][INFO] - Training Epoch: 2/2, step 2405/7134 completed (loss: 0.053457848727703094, acc: 0.97826087474823)
[2025-02-13 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31][root][INFO] - Training Epoch: 2/2, step 2406/7134 completed (loss: 0.01219885516911745, acc: 0.9933510422706604)
[2025-02-13 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31][root][INFO] - Training Epoch: 2/2, step 2407/7134 completed (loss: 0.025957783684134483, acc: 0.9893617033958435)
[2025-02-13 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32][root][INFO] - Training Epoch: 2/2, step 2408/7134 completed (loss: 0.11085580289363861, acc: 0.9750000238418579)
[2025-02-13 03:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32][root][INFO] - Training Epoch: 2/2, step 2409/7134 completed (loss: 0.030969807878136635, acc: 0.9877049326896667)
[2025-02-13 03:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32][root][INFO] - Training Epoch: 2/2, step 2410/7134 completed (loss: 0.02339918352663517, acc: 0.991391658782959)
[2025-02-13 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:33][root][INFO] - Training Epoch: 2/2, step 2411/7134 completed (loss: 0.02561367303133011, acc: 0.9922380447387695)
[2025-02-13 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:33][root][INFO] - Training Epoch: 2/2, step 2412/7134 completed (loss: 0.034525662660598755, acc: 0.9884615540504456)
[2025-02-13 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34][root][INFO] - Training Epoch: 2/2, step 2413/7134 completed (loss: 0.012470364570617676, acc: 0.9949173927307129)
[2025-02-13 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34][root][INFO] - Training Epoch: 2/2, step 2414/7134 completed (loss: 0.018778275698423386, acc: 0.9925373196601868)
[2025-02-13 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35][root][INFO] - Training Epoch: 2/2, step 2415/7134 completed (loss: 0.06517036259174347, acc: 0.9821673631668091)
[2025-02-13 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35][root][INFO] - Training Epoch: 2/2, step 2416/7134 completed (loss: 0.025793178007006645, acc: 0.9930843710899353)
[2025-02-13 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36][root][INFO] - Training Epoch: 2/2, step 2417/7134 completed (loss: 0.05975039303302765, acc: 0.9866220951080322)
[2025-02-13 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36][root][INFO] - Training Epoch: 2/2, step 2418/7134 completed (loss: 0.025847937911748886, acc: 0.9929676651954651)
[2025-02-13 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36][root][INFO] - Training Epoch: 2/2, step 2419/7134 completed (loss: 0.026173003017902374, acc: 0.9899857044219971)
[2025-02-13 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:37][root][INFO] - Training Epoch: 2/2, step 2420/7134 completed (loss: 0.0207233726978302, acc: 0.9950248599052429)
[2025-02-13 03:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:37][root][INFO] - Training Epoch: 2/2, step 2421/7134 completed (loss: 0.04431881755590439, acc: 0.9875621795654297)
[2025-02-13 03:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38][root][INFO] - Training Epoch: 2/2, step 2422/7134 completed (loss: 0.014268244616687298, acc: 0.9976359605789185)
[2025-02-13 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38][root][INFO] - Training Epoch: 2/2, step 2423/7134 completed (loss: 0.02583790011703968, acc: 0.9930264949798584)
[2025-02-13 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38][root][INFO] - Training Epoch: 2/2, step 2424/7134 completed (loss: 0.024066871032118797, acc: 0.992977499961853)
[2025-02-13 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:39][root][INFO] - Training Epoch: 2/2, step 2425/7134 completed (loss: 0.03754476457834244, acc: 0.9879999756813049)
[2025-02-13 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:39][root][INFO] - Training Epoch: 2/2, step 2426/7134 completed (loss: 0.01631622016429901, acc: 0.9928656220436096)
[2025-02-13 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:40][root][INFO] - Training Epoch: 2/2, step 2427/7134 completed (loss: 0.010066096670925617, acc: 0.9976798295974731)
[2025-02-13 03:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:40][root][INFO] - Training Epoch: 2/2, step 2428/7134 completed (loss: 0.023867668583989143, acc: 0.9923567175865173)
[2025-02-13 03:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41][root][INFO] - Training Epoch: 2/2, step 2429/7134 completed (loss: 0.03213144838809967, acc: 0.9890965819358826)
[2025-02-13 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41][root][INFO] - Training Epoch: 2/2, step 2430/7134 completed (loss: 0.03132811188697815, acc: 0.9895678162574768)
[2025-02-13 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:42][root][INFO] - Training Epoch: 2/2, step 2431/7134 completed (loss: 0.04466342553496361, acc: 0.9853372573852539)
[2025-02-13 03:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:42][root][INFO] - Training Epoch: 2/2, step 2432/7134 completed (loss: 0.08804994821548462, acc: 0.979626476764679)
[2025-02-13 03:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:42][root][INFO] - Training Epoch: 2/2, step 2433/7134 completed (loss: 0.05152234807610512, acc: 0.9872159361839294)
[2025-02-13 03:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:43][root][INFO] - Training Epoch: 2/2, step 2434/7134 completed (loss: 0.0551602803170681, acc: 0.9844720363616943)
[2025-02-13 03:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:43][root][INFO] - Training Epoch: 2/2, step 2435/7134 completed (loss: 0.025151552632451057, acc: 0.9947299361228943)
[2025-02-13 03:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:44][root][INFO] - Training Epoch: 2/2, step 2436/7134 completed (loss: 0.03450048714876175, acc: 0.9871944189071655)
[2025-02-13 03:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:44][root][INFO] - Training Epoch: 2/2, step 2437/7134 completed (loss: 0.04087267443537712, acc: 0.9894419312477112)
[2025-02-13 03:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45][root][INFO] - Training Epoch: 2/2, step 2438/7134 completed (loss: 0.040289074182510376, acc: 0.9904305934906006)
[2025-02-13 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45][root][INFO] - Training Epoch: 2/2, step 2439/7134 completed (loss: 0.04110444709658623, acc: 0.9849300384521484)
[2025-02-13 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45][root][INFO] - Training Epoch: 2/2, step 2440/7134 completed (loss: 0.035400670021772385, acc: 0.9887892603874207)
[2025-02-13 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:46][root][INFO] - Training Epoch: 2/2, step 2441/7134 completed (loss: 0.12947718799114227, acc: 0.9692898392677307)
[2025-02-13 03:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:46][root][INFO] - Training Epoch: 2/2, step 2442/7134 completed (loss: 0.05141491815447807, acc: 0.9818181991577148)
[2025-02-13 03:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47][root][INFO] - Training Epoch: 2/2, step 2443/7134 completed (loss: 0.0330917090177536, acc: 0.9916550517082214)
[2025-02-13 03:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47][root][INFO] - Training Epoch: 2/2, step 2444/7134 completed (loss: 0.04882417246699333, acc: 0.9791666865348816)
[2025-02-13 03:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47][root][INFO] - Training Epoch: 2/2, step 2445/7134 completed (loss: 0.07287826389074326, acc: 0.9809644818305969)
[2025-02-13 03:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:48][root][INFO] - Training Epoch: 2/2, step 2446/7134 completed (loss: 0.0797530934214592, acc: 0.9791666865348816)
[2025-02-13 03:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:48][root][INFO] - Training Epoch: 2/2, step 2447/7134 completed (loss: 0.038695432245731354, acc: 0.9877913594245911)
[2025-02-13 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:49][root][INFO] - Training Epoch: 2/2, step 2448/7134 completed (loss: 0.04686401039361954, acc: 0.9893048405647278)
[2025-02-13 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:49][root][INFO] - Training Epoch: 2/2, step 2449/7134 completed (loss: 0.03917373716831207, acc: 0.9893898963928223)
[2025-02-13 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:50][root][INFO] - Training Epoch: 2/2, step 2450/7134 completed (loss: 0.02490982972085476, acc: 0.9961880445480347)
[2025-02-13 03:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:50][root][INFO] - Training Epoch: 2/2, step 2451/7134 completed (loss: 0.02472936548292637, acc: 0.9923567175865173)
[2025-02-13 03:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51][root][INFO] - Training Epoch: 2/2, step 2452/7134 completed (loss: 0.021986911073327065, acc: 0.9921976327896118)
[2025-02-13 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51][root][INFO] - Training Epoch: 2/2, step 2453/7134 completed (loss: 0.037118494510650635, acc: 0.9883871078491211)
[2025-02-13 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51][root][INFO] - Training Epoch: 2/2, step 2454/7134 completed (loss: 0.03875722363591194, acc: 0.9816053509712219)
[2025-02-13 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52][root][INFO] - Training Epoch: 2/2, step 2455/7134 completed (loss: 0.020113952457904816, acc: 0.9943342804908752)
[2025-02-13 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52][root][INFO] - Training Epoch: 2/2, step 2456/7134 completed (loss: 0.05059082806110382, acc: 0.9844357967376709)
[2025-02-13 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:53][root][INFO] - Training Epoch: 2/2, step 2457/7134 completed (loss: 0.054492752999067307, acc: 0.981697142124176)
[2025-02-13 03:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:53][root][INFO] - Training Epoch: 2/2, step 2458/7134 completed (loss: 0.06928860396146774, acc: 0.9874213933944702)
[2025-02-13 03:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54][root][INFO] - Training Epoch: 2/2, step 2459/7134 completed (loss: 0.015716521069407463, acc: 0.9955621361732483)
[2025-02-13 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54][root][INFO] - Training Epoch: 2/2, step 2460/7134 completed (loss: 0.026649994775652885, acc: 0.9944674968719482)
[2025-02-13 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54][root][INFO] - Training Epoch: 2/2, step 2461/7134 completed (loss: 0.023615621030330658, acc: 0.9927536249160767)
[2025-02-13 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55][root][INFO] - Training Epoch: 2/2, step 2462/7134 completed (loss: 0.04360174387693405, acc: 0.9889655113220215)
[2025-02-13 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55][root][INFO] - Training Epoch: 2/2, step 2463/7134 completed (loss: 0.017104825004935265, acc: 0.9909228682518005)
[2025-02-13 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56][root][INFO] - Training Epoch: 2/2, step 2464/7134 completed (loss: 0.04302879795432091, acc: 0.9906367063522339)
[2025-02-13 03:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56][root][INFO] - Training Epoch: 2/2, step 2465/7134 completed (loss: 0.037569914013147354, acc: 0.9880794882774353)
[2025-02-13 03:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56][root][INFO] - Training Epoch: 2/2, step 2466/7134 completed (loss: 0.015541122294962406, acc: 0.9947019815444946)
[2025-02-13 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:57][root][INFO] - Training Epoch: 2/2, step 2467/7134 completed (loss: 0.030939655378460884, acc: 0.9897172451019287)
[2025-02-13 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:57][root][INFO] - Training Epoch: 2/2, step 2468/7134 completed (loss: 0.019999558106064796, acc: 0.9944055676460266)
[2025-02-13 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:58][root][INFO] - Training Epoch: 2/2, step 2469/7134 completed (loss: 0.014587722718715668, acc: 0.9959072470664978)
[2025-02-13 03:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:58][root][INFO] - Training Epoch: 2/2, step 2470/7134 completed (loss: 0.02883676439523697, acc: 0.9949495196342468)
[2025-02-13 03:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59][root][INFO] - Training Epoch: 2/2, step 2471/7134 completed (loss: 0.017523285001516342, acc: 0.9933110475540161)
[2025-02-13 03:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59][root][INFO] - Training Epoch: 2/2, step 2472/7134 completed (loss: 0.012983683496713638, acc: 0.9984848499298096)
[2025-02-13 03:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59][root][INFO] - Training Epoch: 2/2, step 2473/7134 completed (loss: 0.018766511231660843, acc: 0.993852436542511)
[2025-02-13 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:00][root][INFO] - Training Epoch: 2/2, step 2474/7134 completed (loss: 0.013746143318712711, acc: 0.9957746267318726)
[2025-02-13 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:00][root][INFO] - Training Epoch: 2/2, step 2475/7134 completed (loss: 0.02125292271375656, acc: 0.989708423614502)
[2025-02-13 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01][root][INFO] - Training Epoch: 2/2, step 2476/7134 completed (loss: 0.05221335589885712, acc: 0.9888579249382019)
[2025-02-13 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01][root][INFO] - Training Epoch: 2/2, step 2477/7134 completed (loss: 0.02773169055581093, acc: 0.9944751262664795)
[2025-02-13 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01][root][INFO] - Training Epoch: 2/2, step 2478/7134 completed (loss: 0.010012106038630009, acc: 0.9953271150588989)
[2025-02-13 03:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:02][root][INFO] - Training Epoch: 2/2, step 2479/7134 completed (loss: 0.07299982011318207, acc: 0.9793322682380676)
[2025-02-13 03:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:02][root][INFO] - Training Epoch: 2/2, step 2480/7134 completed (loss: 0.007221294566988945, acc: 0.9977777600288391)
[2025-02-13 03:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03][root][INFO] - Training Epoch: 2/2, step 2481/7134 completed (loss: 0.03670148178935051, acc: 0.9875621795654297)
[2025-02-13 03:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03][root][INFO] - Training Epoch: 2/2, step 2482/7134 completed (loss: 0.036158353090286255, acc: 0.9839486479759216)
[2025-02-13 03:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03][root][INFO] - Training Epoch: 2/2, step 2483/7134 completed (loss: 0.012941880151629448, acc: 0.9958391189575195)
[2025-02-13 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:04][root][INFO] - Training Epoch: 2/2, step 2484/7134 completed (loss: 0.026481730863451958, acc: 0.9902777671813965)
[2025-02-13 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:04][root][INFO] - Training Epoch: 2/2, step 2485/7134 completed (loss: 0.017184661701321602, acc: 0.9952380657196045)
[2025-02-13 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:05][root][INFO] - Training Epoch: 2/2, step 2486/7134 completed (loss: 0.01953238993883133, acc: 0.9971264600753784)
[2025-02-13 03:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:05][root][INFO] - Training Epoch: 2/2, step 2487/7134 completed (loss: 0.0718914270401001, acc: 0.9858267903327942)
[2025-02-13 03:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06][root][INFO] - Training Epoch: 2/2, step 2488/7134 completed (loss: 0.02801825851202011, acc: 0.9903314709663391)
[2025-02-13 03:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06][root][INFO] - Training Epoch: 2/2, step 2489/7134 completed (loss: 0.017375105991959572, acc: 0.9944289922714233)
[2025-02-13 03:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06][root][INFO] - Training Epoch: 2/2, step 2490/7134 completed (loss: 0.017781497910618782, acc: 0.9942938685417175)
[2025-02-13 03:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07][root][INFO] - Training Epoch: 2/2, step 2491/7134 completed (loss: 0.09588580578565598, acc: 0.9814814925193787)
[2025-02-13 03:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07][root][INFO] - Training Epoch: 2/2, step 2492/7134 completed (loss: 0.040494903922080994, acc: 0.985111653804779)
[2025-02-13 03:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08][root][INFO] - Training Epoch: 2/2, step 2493/7134 completed (loss: 0.046418316662311554, acc: 0.9788732528686523)
[2025-02-13 03:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08][root][INFO] - Training Epoch: 2/2, step 2494/7134 completed (loss: 0.043720535933971405, acc: 0.9849812388420105)
[2025-02-13 03:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08][root][INFO] - Training Epoch: 2/2, step 2495/7134 completed (loss: 0.03734545782208443, acc: 0.9864314794540405)
[2025-02-13 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:09][root][INFO] - Training Epoch: 2/2, step 2496/7134 completed (loss: 0.018677981570363045, acc: 0.9971910119056702)
[2025-02-13 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:09][root][INFO] - Training Epoch: 2/2, step 2497/7134 completed (loss: 0.03784150630235672, acc: 0.989051103591919)
[2025-02-13 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10][root][INFO] - Training Epoch: 2/2, step 2498/7134 completed (loss: 0.031548041850328445, acc: 0.9886234402656555)
[2025-02-13 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10][root][INFO] - Training Epoch: 2/2, step 2499/7134 completed (loss: 0.023723455145955086, acc: 0.9939613342285156)
[2025-02-13 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:11][root][INFO] - Training Epoch: 2/2, step 2500/7134 completed (loss: 0.018398990854620934, acc: 0.9961389899253845)
[2025-02-13 03:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:11][root][INFO] - Training Epoch: 2/2, step 2501/7134 completed (loss: 0.03183293342590332, acc: 0.9885583519935608)
[2025-02-13 03:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12][root][INFO] - Training Epoch: 2/2, step 2502/7134 completed (loss: 0.01625603251159191, acc: 0.9946996569633484)
[2025-02-13 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12][root][INFO] - Training Epoch: 2/2, step 2503/7134 completed (loss: 0.03439490124583244, acc: 0.994301974773407)
[2025-02-13 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12][root][INFO] - Training Epoch: 2/2, step 2504/7134 completed (loss: 0.031794216483831406, acc: 0.9906666874885559)
[2025-02-13 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:13][root][INFO] - Training Epoch: 2/2, step 2505/7134 completed (loss: 0.018988512456417084, acc: 0.9943116903305054)
[2025-02-13 03:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:13][root][INFO] - Training Epoch: 2/2, step 2506/7134 completed (loss: 0.027647843584418297, acc: 0.9919871687889099)
[2025-02-13 03:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:14][root][INFO] - Training Epoch: 2/2, step 2507/7134 completed (loss: 0.008743355050683022, acc: 1.0)
[2025-02-13 03:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:14][root][INFO] - Training Epoch: 2/2, step 2508/7134 completed (loss: 0.03795044496655464, acc: 0.9884792566299438)
[2025-02-13 03:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15][root][INFO] - Training Epoch: 2/2, step 2509/7134 completed (loss: 0.014217322692275047, acc: 0.9963280558586121)
[2025-02-13 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15][root][INFO] - Training Epoch: 2/2, step 2510/7134 completed (loss: 0.01669066958129406, acc: 0.9939302206039429)
[2025-02-13 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15][root][INFO] - Training Epoch: 2/2, step 2511/7134 completed (loss: 0.03647952154278755, acc: 0.9943946003913879)
[2025-02-13 03:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:16][root][INFO] - Training Epoch: 2/2, step 2512/7134 completed (loss: 0.0552070252597332, acc: 0.9824561476707458)
[2025-02-13 03:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:16][root][INFO] - Training Epoch: 2/2, step 2513/7134 completed (loss: 0.02071884088218212, acc: 0.9940758347511292)
[2025-02-13 03:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:17][root][INFO] - Training Epoch: 2/2, step 2514/7134 completed (loss: 0.01867729425430298, acc: 0.9929988384246826)
[2025-02-13 03:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:17][root][INFO] - Training Epoch: 2/2, step 2515/7134 completed (loss: 0.009274076670408249, acc: 0.9986996054649353)
[2025-02-13 03:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18][root][INFO] - Training Epoch: 2/2, step 2516/7134 completed (loss: 0.012010636739432812, acc: 0.9954545497894287)
[2025-02-13 03:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18][root][INFO] - Training Epoch: 2/2, step 2517/7134 completed (loss: 0.036425020545721054, acc: 0.991963267326355)
[2025-02-13 03:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18][root][INFO] - Training Epoch: 2/2, step 2518/7134 completed (loss: 0.019232558086514473, acc: 0.9942775368690491)
[2025-02-13 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:19][root][INFO] - Training Epoch: 2/2, step 2519/7134 completed (loss: 0.0060522169806063175, acc: 0.9986168742179871)
[2025-02-13 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:19][root][INFO] - Training Epoch: 2/2, step 2520/7134 completed (loss: 0.011953478679060936, acc: 0.994301974773407)
[2025-02-13 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:20][root][INFO] - Training Epoch: 2/2, step 2521/7134 completed (loss: 0.05974921956658363, acc: 0.9871299862861633)
[2025-02-13 03:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:20][root][INFO] - Training Epoch: 2/2, step 2522/7134 completed (loss: 0.025601733475923538, acc: 0.9914039969444275)
[2025-02-13 03:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21][root][INFO] - Training Epoch: 2/2, step 2523/7134 completed (loss: 0.038646504282951355, acc: 0.9934554696083069)
[2025-02-13 03:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21][root][INFO] - Training Epoch: 2/2, step 2524/7134 completed (loss: 0.004535383079200983, acc: 1.0)
[2025-02-13 03:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21][root][INFO] - Training Epoch: 2/2, step 2525/7134 completed (loss: 0.028080318123102188, acc: 0.989983320236206)
[2025-02-13 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:22][root][INFO] - Training Epoch: 2/2, step 2526/7134 completed (loss: 0.00342005118727684, acc: 1.0)
[2025-02-13 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:22][root][INFO] - Training Epoch: 2/2, step 2527/7134 completed (loss: 0.02435092255473137, acc: 0.9914634227752686)
[2025-02-13 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23][root][INFO] - Training Epoch: 2/2, step 2528/7134 completed (loss: 0.010266821831464767, acc: 0.99726402759552)
[2025-02-13 03:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23][root][INFO] - Training Epoch: 2/2, step 2529/7134 completed (loss: 0.011385208927094936, acc: 0.998670220375061)
[2025-02-13 03:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23][root][INFO] - Training Epoch: 2/2, step 2530/7134 completed (loss: 0.019136575981974602, acc: 0.9935064911842346)
[2025-02-13 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:24][root][INFO] - Training Epoch: 2/2, step 2531/7134 completed (loss: 0.01228636410087347, acc: 0.9958563446998596)
[2025-02-13 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:24][root][INFO] - Training Epoch: 2/2, step 2532/7134 completed (loss: 0.006845603697001934, acc: 0.9984591603279114)
[2025-02-13 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:25][root][INFO] - Training Epoch: 2/2, step 2533/7134 completed (loss: 0.023272816091775894, acc: 0.9924585223197937)
[2025-02-13 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:25][root][INFO] - Training Epoch: 2/2, step 2534/7134 completed (loss: 0.03743467479944229, acc: 0.9961783289909363)
[2025-02-13 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26][root][INFO] - Training Epoch: 2/2, step 2535/7134 completed (loss: 0.028242867439985275, acc: 0.9922928810119629)
[2025-02-13 03:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26][root][INFO] - Training Epoch: 2/2, step 2536/7134 completed (loss: 0.01955253817141056, acc: 0.9926199316978455)
[2025-02-13 03:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26][root][INFO] - Training Epoch: 2/2, step 2537/7134 completed (loss: 0.05367250367999077, acc: 0.9862385392189026)
[2025-02-13 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:27][root][INFO] - Training Epoch: 2/2, step 2538/7134 completed (loss: 0.04872714728116989, acc: 0.9886685609817505)
[2025-02-13 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:27][root][INFO] - Training Epoch: 2/2, step 2539/7134 completed (loss: 0.017299562692642212, acc: 0.9946428537368774)
[2025-02-13 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28][root][INFO] - Training Epoch: 2/2, step 2540/7134 completed (loss: 0.053550686687231064, acc: 0.9862385392189026)
[2025-02-13 03:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28][root][INFO] - Training Epoch: 2/2, step 2541/7134 completed (loss: 0.01411372795701027, acc: 0.9950330853462219)
[2025-02-13 03:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28][root][INFO] - Training Epoch: 2/2, step 2542/7134 completed (loss: 0.04971715807914734, acc: 0.9881656765937805)
[2025-02-13 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29][root][INFO] - Training Epoch: 2/2, step 2543/7134 completed (loss: 0.06647349894046783, acc: 0.9877049326896667)
[2025-02-13 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29][root][INFO] - Training Epoch: 2/2, step 2544/7134 completed (loss: 0.03138923645019531, acc: 0.9896907210350037)
[2025-02-13 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:30][root][INFO] - Training Epoch: 2/2, step 2545/7134 completed (loss: 0.03397800773382187, acc: 0.9930192232131958)
[2025-02-13 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:30][root][INFO] - Training Epoch: 2/2, step 2546/7134 completed (loss: 0.1053890585899353, acc: 0.9675870537757874)
[2025-02-13 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31][root][INFO] - Training Epoch: 2/2, step 2547/7134 completed (loss: 0.05545768886804581, acc: 0.9838969111442566)
[2025-02-13 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31][root][INFO] - Training Epoch: 2/2, step 2548/7134 completed (loss: 0.02498169057071209, acc: 0.9921259880065918)
[2025-02-13 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31][root][INFO] - Training Epoch: 2/2, step 2549/7134 completed (loss: 0.0488203726708889, acc: 0.9852670431137085)
[2025-02-13 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32][root][INFO] - Training Epoch: 2/2, step 2550/7134 completed (loss: 0.016375645995140076, acc: 0.9967373609542847)
[2025-02-13 03:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32][root][INFO] - Training Epoch: 2/2, step 2551/7134 completed (loss: 0.06294258683919907, acc: 0.9884488582611084)
[2025-02-13 03:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32][root][INFO] - Training Epoch: 2/2, step 2552/7134 completed (loss: 0.011057744733989239, acc: 0.9936908483505249)
[2025-02-13 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:33][root][INFO] - Training Epoch: 2/2, step 2553/7134 completed (loss: 0.02109368145465851, acc: 0.9909365773200989)
[2025-02-13 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:33][root][INFO] - Training Epoch: 2/2, step 2554/7134 completed (loss: 0.05876349285244942, acc: 0.9849246144294739)
[2025-02-13 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34][root][INFO] - Training Epoch: 2/2, step 2555/7134 completed (loss: 0.026470808312296867, acc: 0.9917355179786682)
[2025-02-13 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34][root][INFO] - Training Epoch: 2/2, step 2556/7134 completed (loss: 0.03787612542510033, acc: 0.9832285046577454)
[2025-02-13 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34][root][INFO] - Training Epoch: 2/2, step 2557/7134 completed (loss: 0.03587760031223297, acc: 0.9901639223098755)
[2025-02-13 03:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:35][root][INFO] - Training Epoch: 2/2, step 2558/7134 completed (loss: 0.011056422255933285, acc: 0.9983818531036377)
[2025-02-13 03:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:35][root][INFO] - Training Epoch: 2/2, step 2559/7134 completed (loss: 0.013386407867074013, acc: 0.9972936511039734)
[2025-02-13 03:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:36][root][INFO] - Training Epoch: 2/2, step 2560/7134 completed (loss: 0.03330940380692482, acc: 0.9902439117431641)
[2025-02-13 03:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:36][root][INFO] - Training Epoch: 2/2, step 2561/7134 completed (loss: 0.02369067259132862, acc: 0.9934640526771545)
[2025-02-13 03:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37][root][INFO] - Training Epoch: 2/2, step 2562/7134 completed (loss: 0.041671521961688995, acc: 0.9914529919624329)
[2025-02-13 03:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37][root][INFO] - Training Epoch: 2/2, step 2563/7134 completed (loss: 0.03090851381421089, acc: 0.9956011772155762)
[2025-02-13 03:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37][root][INFO] - Training Epoch: 2/2, step 2564/7134 completed (loss: 0.08469997346401215, acc: 0.9815436005592346)
[2025-02-13 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:38][root][INFO] - Training Epoch: 2/2, step 2565/7134 completed (loss: 0.019984325394034386, acc: 0.995468258857727)
[2025-02-13 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:38][root][INFO] - Training Epoch: 2/2, step 2566/7134 completed (loss: 0.026571085676550865, acc: 0.9887640476226807)
[2025-02-13 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39][root][INFO] - Training Epoch: 2/2, step 2567/7134 completed (loss: 0.024663999676704407, acc: 0.9911816716194153)
[2025-02-13 03:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39][root][INFO] - Training Epoch: 2/2, step 2568/7134 completed (loss: 0.04204928129911423, acc: 0.9855491518974304)
[2025-02-13 03:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39][root][INFO] - Training Epoch: 2/2, step 2569/7134 completed (loss: 0.032704856246709824, acc: 0.990963876247406)
[2025-02-13 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:40][root][INFO] - Training Epoch: 2/2, step 2570/7134 completed (loss: 0.038759760558605194, acc: 0.9910045266151428)
[2025-02-13 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:40][root][INFO] - Training Epoch: 2/2, step 2571/7134 completed (loss: 0.025350866839289665, acc: 0.9912126660346985)
[2025-02-13 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41][root][INFO] - Training Epoch: 2/2, step 2572/7134 completed (loss: 0.024395938962697983, acc: 0.9921135902404785)
[2025-02-13 03:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41][root][INFO] - Training Epoch: 2/2, step 2573/7134 completed (loss: 0.04998307302594185, acc: 0.9852458834648132)
[2025-02-13 03:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41][root][INFO] - Training Epoch: 2/2, step 2574/7134 completed (loss: 0.03013579174876213, acc: 0.9890829920768738)
[2025-02-13 03:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:42][root][INFO] - Training Epoch: 2/2, step 2575/7134 completed (loss: 0.015142548829317093, acc: 0.9944238066673279)
[2025-02-13 03:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:42][root][INFO] - Training Epoch: 2/2, step 2576/7134 completed (loss: 0.02216314896941185, acc: 0.9948275685310364)
[2025-02-13 03:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43][root][INFO] - Training Epoch: 2/2, step 2577/7134 completed (loss: 0.056600913405418396, acc: 0.988034188747406)
[2025-02-13 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43][root][INFO] - Training Epoch: 2/2, step 2578/7134 completed (loss: 0.05182487517595291, acc: 0.9831546545028687)
[2025-02-13 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43][root][INFO] - Training Epoch: 2/2, step 2579/7134 completed (loss: 0.06479693949222565, acc: 0.9778481125831604)
[2025-02-13 03:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:44][root][INFO] - Training Epoch: 2/2, step 2580/7134 completed (loss: 0.04081714525818825, acc: 0.9869451522827148)
[2025-02-13 03:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:44][root][INFO] - Training Epoch: 2/2, step 2581/7134 completed (loss: 0.041093382984399796, acc: 0.9892328381538391)
[2025-02-13 03:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45][root][INFO] - Training Epoch: 2/2, step 2582/7134 completed (loss: 0.042337436228990555, acc: 0.9810298085212708)
[2025-02-13 03:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45][root][INFO] - Training Epoch: 2/2, step 2583/7134 completed (loss: 0.04685825854539871, acc: 0.9843527674674988)
[2025-02-13 03:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45][root][INFO] - Training Epoch: 2/2, step 2584/7134 completed (loss: 0.07222367078065872, acc: 0.9790576100349426)
[2025-02-13 03:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:46][root][INFO] - Training Epoch: 2/2, step 2585/7134 completed (loss: 0.05046684294939041, acc: 0.9913669228553772)
[2025-02-13 03:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:46][root][INFO] - Training Epoch: 2/2, step 2586/7134 completed (loss: 0.04639837145805359, acc: 0.98591548204422)
[2025-02-13 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:47][root][INFO] - Training Epoch: 2/2, step 2587/7134 completed (loss: 0.03643471747636795, acc: 0.9889435172080994)
[2025-02-13 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:47][root][INFO] - Training Epoch: 2/2, step 2588/7134 completed (loss: 0.008200396783649921, acc: 0.9986357688903809)
[2025-02-13 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48][root][INFO] - Training Epoch: 2/2, step 2589/7134 completed (loss: 0.060746390372514725, acc: 0.9893993139266968)
[2025-02-13 03:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48][root][INFO] - Training Epoch: 2/2, step 2590/7134 completed (loss: 0.026344837620854378, acc: 0.9934640526771545)
[2025-02-13 03:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48][root][INFO] - Training Epoch: 2/2, step 2591/7134 completed (loss: 0.06432264298200607, acc: 0.9833759665489197)
[2025-02-13 03:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:49][root][INFO] - Training Epoch: 2/2, step 2592/7134 completed (loss: 0.03749378025531769, acc: 0.9864698648452759)
[2025-02-13 03:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:49][root][INFO] - Training Epoch: 2/2, step 2593/7134 completed (loss: 0.036457717418670654, acc: 0.9921466112136841)
[2025-02-13 03:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:50][root][INFO] - Training Epoch: 2/2, step 2594/7134 completed (loss: 0.036523934453725815, acc: 0.9877675771713257)
[2025-02-13 03:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:50][root][INFO] - Training Epoch: 2/2, step 2595/7134 completed (loss: 0.02063428796827793, acc: 0.9946236610412598)
[2025-02-13 03:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51][root][INFO] - Training Epoch: 2/2, step 2596/7134 completed (loss: 0.03381772339344025, acc: 0.9899749159812927)
[2025-02-13 03:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51][root][INFO] - Training Epoch: 2/2, step 2597/7134 completed (loss: 0.04195452481508255, acc: 0.9891892075538635)
[2025-02-13 03:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52][root][INFO] - Training Epoch: 2/2, step 2598/7134 completed (loss: 0.06761059910058975, acc: 0.9798761606216431)
[2025-02-13 03:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52][root][INFO] - Training Epoch: 2/2, step 2599/7134 completed (loss: 0.012181089259684086, acc: 0.9947229623794556)
[2025-02-13 03:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52][root][INFO] - Training Epoch: 2/2, step 2600/7134 completed (loss: 0.049092553555965424, acc: 0.984635055065155)
[2025-02-13 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:53][root][INFO] - Training Epoch: 2/2, step 2601/7134 completed (loss: 0.05874183773994446, acc: 0.9797022938728333)
[2025-02-13 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:53][root][INFO] - Training Epoch: 2/2, step 2602/7134 completed (loss: 0.038378361612558365, acc: 0.990314781665802)
[2025-02-13 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:54][root][INFO] - Training Epoch: 2/2, step 2603/7134 completed (loss: 0.041407469660043716, acc: 0.9891473054885864)
[2025-02-13 03:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:54][root][INFO] - Training Epoch: 2/2, step 2604/7134 completed (loss: 0.025831347331404686, acc: 0.9912280440330505)
[2025-02-13 03:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55][root][INFO] - Training Epoch: 2/2, step 2605/7134 completed (loss: 0.06687068939208984, acc: 0.9765343070030212)
[2025-02-13 03:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55][root][INFO] - Training Epoch: 2/2, step 2606/7134 completed (loss: 0.02274763584136963, acc: 0.9942445755004883)
[2025-02-13 03:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56][root][INFO] - Training Epoch: 2/2, step 2607/7134 completed (loss: 0.036686547100543976, acc: 0.9835293889045715)
[2025-02-13 03:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56][root][INFO] - Training Epoch: 2/2, step 2608/7134 completed (loss: 0.05484956130385399, acc: 0.9814586043357849)
[2025-02-13 03:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56][root][INFO] - Training Epoch: 2/2, step 2609/7134 completed (loss: 0.037519171833992004, acc: 0.9888579249382019)
[2025-02-13 03:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:57][root][INFO] - Training Epoch: 2/2, step 2610/7134 completed (loss: 0.07015317678451538, acc: 0.9877862334251404)
[2025-02-13 03:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:57][root][INFO] - Training Epoch: 2/2, step 2611/7134 completed (loss: 0.06708666682243347, acc: 0.9804822206497192)
[2025-02-13 03:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:58][root][INFO] - Training Epoch: 2/2, step 2612/7134 completed (loss: 0.04941919445991516, acc: 0.9851936101913452)
[2025-02-13 03:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:58][root][INFO] - Training Epoch: 2/2, step 2613/7134 completed (loss: 0.03160838410258293, acc: 0.9898734092712402)
[2025-02-13 03:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59][root][INFO] - Training Epoch: 2/2, step 2614/7134 completed (loss: 0.06372799724340439, acc: 0.980637788772583)
[2025-02-13 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59][root][INFO] - Training Epoch: 2/2, step 2615/7134 completed (loss: 0.05098959058523178, acc: 0.9832904934883118)
[2025-02-13 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59][root][INFO] - Training Epoch: 2/2, step 2616/7134 completed (loss: 0.062292374670505524, acc: 0.9814814925193787)
[2025-02-13 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:00][root][INFO] - Training Epoch: 2/2, step 2617/7134 completed (loss: 0.04870503395795822, acc: 0.9851552248001099)
[2025-02-13 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:00][root][INFO] - Training Epoch: 2/2, step 2618/7134 completed (loss: 0.029873106628656387, acc: 0.9917864203453064)
[2025-02-13 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:01][root][INFO] - Training Epoch: 2/2, step 2619/7134 completed (loss: 0.04774457961320877, acc: 0.9848993420600891)
[2025-02-13 03:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:01][root][INFO] - Training Epoch: 2/2, step 2620/7134 completed (loss: 0.04895373433828354, acc: 0.986369252204895)
[2025-02-13 03:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02][root][INFO] - Training Epoch: 2/2, step 2621/7134 completed (loss: 0.035083744674921036, acc: 0.9952606558799744)
[2025-02-13 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02][root][INFO] - Training Epoch: 2/2, step 2622/7134 completed (loss: 0.04260455071926117, acc: 0.9885433912277222)
[2025-02-13 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02][root][INFO] - Training Epoch: 2/2, step 2623/7134 completed (loss: 0.045046549290418625, acc: 0.9839080572128296)
[2025-02-13 03:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:03][root][INFO] - Training Epoch: 2/2, step 2624/7134 completed (loss: 0.03837806358933449, acc: 0.9921259880065918)
[2025-02-13 03:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:03][root][INFO] - Training Epoch: 2/2, step 2625/7134 completed (loss: 0.037900689989328384, acc: 0.987500011920929)
[2025-02-13 03:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:04][root][INFO] - Training Epoch: 2/2, step 2626/7134 completed (loss: 0.051934659481048584, acc: 0.9876237511634827)
[2025-02-13 03:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:04][root][INFO] - Training Epoch: 2/2, step 2627/7134 completed (loss: 0.024380525574088097, acc: 0.9918808937072754)
[2025-02-13 03:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:05][root][INFO] - Training Epoch: 2/2, step 2628/7134 completed (loss: 0.04905293136835098, acc: 0.9861111044883728)
[2025-02-13 03:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:05][root][INFO] - Training Epoch: 2/2, step 2629/7134 completed (loss: 0.019601693376898766, acc: 0.993678867816925)
[2025-02-13 03:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06][root][INFO] - Training Epoch: 2/2, step 2630/7134 completed (loss: 0.03566320613026619, acc: 0.9888178706169128)
[2025-02-13 03:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06][root][INFO] - Training Epoch: 2/2, step 2631/7134 completed (loss: 0.02058192901313305, acc: 0.9941520690917969)
[2025-02-13 03:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06][root][INFO] - Training Epoch: 2/2, step 2632/7134 completed (loss: 0.024035265669226646, acc: 0.9921414256095886)
[2025-02-13 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:07][root][INFO] - Training Epoch: 2/2, step 2633/7134 completed (loss: 0.03929492458701134, acc: 0.9900850057601929)
[2025-02-13 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:07][root][INFO] - Training Epoch: 2/2, step 2634/7134 completed (loss: 0.02057974971830845, acc: 0.9940029978752136)
[2025-02-13 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:08][root][INFO] - Training Epoch: 2/2, step 2635/7134 completed (loss: 0.0840117484331131, acc: 0.9819444417953491)
[2025-02-13 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:08][root][INFO] - Training Epoch: 2/2, step 2636/7134 completed (loss: 0.06379249691963196, acc: 0.9843527674674988)
[2025-02-13 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09][root][INFO] - Training Epoch: 2/2, step 2637/7134 completed (loss: 0.14886999130249023, acc: 0.9626485705375671)
[2025-02-13 03:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09][root][INFO] - Training Epoch: 2/2, step 2638/7134 completed (loss: 0.08254951983690262, acc: 0.9752281904220581)
[2025-02-13 03:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09][root][INFO] - Training Epoch: 2/2, step 2639/7134 completed (loss: 0.06616394221782684, acc: 0.9751037359237671)
[2025-02-13 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:10][root][INFO] - Training Epoch: 2/2, step 2640/7134 completed (loss: 0.06469479948282242, acc: 0.9850187301635742)
[2025-02-13 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:10][root][INFO] - Training Epoch: 2/2, step 2641/7134 completed (loss: 0.056428007781505585, acc: 0.9779086709022522)
[2025-02-13 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:11][root][INFO] - Training Epoch: 2/2, step 2642/7134 completed (loss: 0.08322501927614212, acc: 0.9768683314323425)
[2025-02-13 03:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:11][root][INFO] - Training Epoch: 2/2, step 2643/7134 completed (loss: 0.04135487228631973, acc: 0.985049843788147)
[2025-02-13 03:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12][root][INFO] - Training Epoch: 2/2, step 2644/7134 completed (loss: 0.06578819453716278, acc: 0.9854369163513184)
[2025-02-13 03:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12][root][INFO] - Training Epoch: 2/2, step 2645/7134 completed (loss: 0.05118391662836075, acc: 0.9839650392532349)
[2025-02-13 03:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12][root][INFO] - Training Epoch: 2/2, step 2646/7134 completed (loss: 0.12030695378780365, acc: 0.9633758068084717)
[2025-02-13 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:13][root][INFO] - Training Epoch: 2/2, step 2647/7134 completed (loss: 0.03591016307473183, acc: 0.9895969033241272)
[2025-02-13 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:13][root][INFO] - Training Epoch: 2/2, step 2648/7134 completed (loss: 0.03240268677473068, acc: 0.9896774291992188)
[2025-02-13 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:14][root][INFO] - Training Epoch: 2/2, step 2649/7134 completed (loss: 0.10227179527282715, acc: 0.9716840386390686)
[2025-02-13 03:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:14][root][INFO] - Training Epoch: 2/2, step 2650/7134 completed (loss: 0.10258294641971588, acc: 0.974281370639801)
[2025-02-13 03:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15][root][INFO] - Training Epoch: 2/2, step 2651/7134 completed (loss: 0.03509536758065224, acc: 0.9880810379981995)
[2025-02-13 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15][root][INFO] - Training Epoch: 2/2, step 2652/7134 completed (loss: 0.04028738662600517, acc: 0.9899117350578308)
[2025-02-13 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15][root][INFO] - Training Epoch: 2/2, step 2653/7134 completed (loss: 0.03714257851243019, acc: 0.9819079041481018)
[2025-02-13 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:16][root][INFO] - Training Epoch: 2/2, step 2654/7134 completed (loss: 0.0415029302239418, acc: 0.9868852496147156)
[2025-02-13 03:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:16][root][INFO] - Training Epoch: 2/2, step 2655/7134 completed (loss: 0.0224718376994133, acc: 0.9909228682518005)
[2025-02-13 03:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17][root][INFO] - Training Epoch: 2/2, step 2656/7134 completed (loss: 0.0498843714594841, acc: 0.9874826073646545)
[2025-02-13 03:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17][root][INFO] - Training Epoch: 2/2, step 2657/7134 completed (loss: 0.029707729816436768, acc: 0.9935483932495117)
[2025-02-13 03:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17][root][INFO] - Training Epoch: 2/2, step 2658/7134 completed (loss: 0.09922154992818832, acc: 0.9813084006309509)
[2025-02-13 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:18][root][INFO] - Training Epoch: 2/2, step 2659/7134 completed (loss: 0.045142173767089844, acc: 0.9865900278091431)
[2025-02-13 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:18][root][INFO] - Training Epoch: 2/2, step 2660/7134 completed (loss: 0.03503954038023949, acc: 0.9838709831237793)
[2025-02-13 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19][root][INFO] - Training Epoch: 2/2, step 2661/7134 completed (loss: 0.04009915143251419, acc: 0.991584837436676)
[2025-02-13 03:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19][root][INFO] - Training Epoch: 2/2, step 2662/7134 completed (loss: 0.01635659672319889, acc: 0.9943100810050964)
[2025-02-13 03:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19][root][INFO] - Training Epoch: 2/2, step 2663/7134 completed (loss: 0.028860928490757942, acc: 0.9906432628631592)
[2025-02-13 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:20][root][INFO] - Training Epoch: 2/2, step 2664/7134 completed (loss: 0.01187160611152649, acc: 0.9948387145996094)
[2025-02-13 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:20][root][INFO] - Training Epoch: 2/2, step 2665/7134 completed (loss: 0.009833510033786297, acc: 0.9986979365348816)
[2025-02-13 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:21][root][INFO] - Training Epoch: 2/2, step 2666/7134 completed (loss: 0.023386243730783463, acc: 0.9914320707321167)
[2025-02-13 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:21][root][INFO] - Training Epoch: 2/2, step 2667/7134 completed (loss: 0.011493418365716934, acc: 0.9985835552215576)
[2025-02-13 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22][root][INFO] - Training Epoch: 2/2, step 2668/7134 completed (loss: 0.023601850494742393, acc: 0.9959623217582703)
[2025-02-13 03:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22][root][INFO] - Training Epoch: 2/2, step 2669/7134 completed (loss: 0.005919411312788725, acc: 0.9988492727279663)
[2025-02-13 03:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22][root][INFO] - Training Epoch: 2/2, step 2670/7134 completed (loss: 0.0231948122382164, acc: 0.9908257126808167)
[2025-02-13 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:23][root][INFO] - Training Epoch: 2/2, step 2671/7134 completed (loss: 0.0171921756118536, acc: 0.9933155179023743)
[2025-02-13 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:23][root][INFO] - Training Epoch: 2/2, step 2672/7134 completed (loss: 0.012142757885158062, acc: 0.9956268072128296)
[2025-02-13 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:24][root][INFO] - Training Epoch: 2/2, step 2673/7134 completed (loss: 0.006466804537922144, acc: 0.997474730014801)
[2025-02-13 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:24][root][INFO] - Training Epoch: 2/2, step 2674/7134 completed (loss: 0.013957704417407513, acc: 0.9944211840629578)
[2025-02-13 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:25][root][INFO] - Training Epoch: 2/2, step 2675/7134 completed (loss: 0.004389616660773754, acc: 1.0)
[2025-02-13 03:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:25][root][INFO] - Training Epoch: 2/2, step 2676/7134 completed (loss: 0.01685520075261593, acc: 0.9935064911842346)
[2025-02-13 03:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26][root][INFO] - Training Epoch: 2/2, step 2677/7134 completed (loss: 0.012647817842662334, acc: 0.9955357313156128)
[2025-02-13 03:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26][root][INFO] - Training Epoch: 2/2, step 2678/7134 completed (loss: 0.0036447388119995594, acc: 1.0)
[2025-02-13 03:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26][root][INFO] - Training Epoch: 2/2, step 2679/7134 completed (loss: 0.0098335649818182, acc: 0.996632993221283)
[2025-02-13 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:27][root][INFO] - Training Epoch: 2/2, step 2680/7134 completed (loss: 0.01421968825161457, acc: 0.9963008761405945)
[2025-02-13 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:27][root][INFO] - Training Epoch: 2/2, step 2681/7134 completed (loss: 0.008560901507735252, acc: 0.9969651103019714)
[2025-02-13 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28][root][INFO] - Training Epoch: 2/2, step 2682/7134 completed (loss: 0.009803131222724915, acc: 0.9968101978302002)
[2025-02-13 03:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28][root][INFO] - Training Epoch: 2/2, step 2683/7134 completed (loss: 0.004828593228012323, acc: 0.9984825253486633)
[2025-02-13 03:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28][root][INFO] - Training Epoch: 2/2, step 2684/7134 completed (loss: 0.028387097641825676, acc: 0.9928315281867981)
[2025-02-13 03:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:29][root][INFO] - Training Epoch: 2/2, step 2685/7134 completed (loss: 0.019127123057842255, acc: 0.9960988163948059)
[2025-02-13 03:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:29][root][INFO] - Training Epoch: 2/2, step 2686/7134 completed (loss: 0.03212136775255203, acc: 0.9884259104728699)
[2025-02-13 03:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:30][root][INFO] - Training Epoch: 2/2, step 2687/7134 completed (loss: 0.027425289154052734, acc: 0.9911660552024841)
[2025-02-13 03:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:30][root][INFO] - Training Epoch: 2/2, step 2688/7134 completed (loss: 0.03067309968173504, acc: 0.9921383857727051)
[2025-02-13 03:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31][root][INFO] - Training Epoch: 2/2, step 2689/7134 completed (loss: 0.031808022409677505, acc: 0.9927667379379272)
[2025-02-13 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31][root][INFO] - Training Epoch: 2/2, step 2690/7134 completed (loss: 0.061911940574645996, acc: 0.9829984307289124)
[2025-02-13 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31][root][INFO] - Training Epoch: 2/2, step 2691/7134 completed (loss: 0.0678286924958229, acc: 0.98828125)
[2025-02-13 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:32][root][INFO] - Training Epoch: 2/2, step 2692/7134 completed (loss: 0.08071647584438324, acc: 0.9757914543151855)
[2025-02-13 03:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:32][root][INFO] - Training Epoch: 2/2, step 2693/7134 completed (loss: 0.10609471052885056, acc: 0.9730769395828247)
[2025-02-13 03:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33][root][INFO] - Training Epoch: 2/2, step 2694/7134 completed (loss: 0.09082164615392685, acc: 0.9838235378265381)
[2025-02-13 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33][root][INFO] - Training Epoch: 2/2, step 2695/7134 completed (loss: 0.03675145283341408, acc: 0.9932998418807983)
[2025-02-13 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33][root][INFO] - Training Epoch: 2/2, step 2696/7134 completed (loss: 0.07871390879154205, acc: 0.9745628237724304)
[2025-02-13 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:34][root][INFO] - Training Epoch: 2/2, step 2697/7134 completed (loss: 0.08704914152622223, acc: 0.9768875241279602)
[2025-02-13 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:34][root][INFO] - Training Epoch: 2/2, step 2698/7134 completed (loss: 0.030613617971539497, acc: 0.9897435903549194)
[2025-02-13 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35][root][INFO] - Training Epoch: 2/2, step 2699/7134 completed (loss: 0.03761785477399826, acc: 0.9889624714851379)
[2025-02-13 03:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35][root][INFO] - Training Epoch: 2/2, step 2700/7134 completed (loss: 0.04425051808357239, acc: 0.9910979270935059)
[2025-02-13 03:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35][root][INFO] - Training Epoch: 2/2, step 2701/7134 completed (loss: 0.0716715157032013, acc: 0.978151261806488)
[2025-02-13 03:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36][root][INFO] - Training Epoch: 2/2, step 2702/7134 completed (loss: 0.052170272916555405, acc: 0.9947916865348816)
[2025-02-13 03:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36][root][INFO] - Training Epoch: 2/2, step 2703/7134 completed (loss: 0.021675720810890198, acc: 0.993630588054657)
[2025-02-13 03:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36][root][INFO] - Training Epoch: 2/2, step 2704/7134 completed (loss: 0.05180018022656441, acc: 0.986522912979126)
[2025-02-13 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:37][root][INFO] - Training Epoch: 2/2, step 2705/7134 completed (loss: 0.09739364683628082, acc: 0.9768518805503845)
[2025-02-13 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:37][root][INFO] - Training Epoch: 2/2, step 2706/7134 completed (loss: 0.06156664341688156, acc: 0.9869375824928284)
[2025-02-13 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:38][root][INFO] - Training Epoch: 2/2, step 2707/7134 completed (loss: 0.031090496107935905, acc: 0.9896551966667175)
[2025-02-13 03:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:38][root][INFO] - Training Epoch: 2/2, step 2708/7134 completed (loss: 0.04992491751909256, acc: 0.9859747290611267)
[2025-02-13 03:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:39][root][INFO] - Training Epoch: 2/2, step 2709/7134 completed (loss: 0.03427208587527275, acc: 0.9951298832893372)
[2025-02-13 03:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:39][root][INFO] - Training Epoch: 2/2, step 2710/7134 completed (loss: 0.04064459726214409, acc: 0.9836829900741577)
[2025-02-13 03:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40][root][INFO] - Training Epoch: 2/2, step 2711/7134 completed (loss: 0.03366747125983238, acc: 0.9858490824699402)
[2025-02-13 03:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40][root][INFO] - Training Epoch: 2/2, step 2712/7134 completed (loss: 0.03041309490799904, acc: 0.9910714030265808)
[2025-02-13 03:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40][root][INFO] - Training Epoch: 2/2, step 2713/7134 completed (loss: 0.04936182498931885, acc: 0.9916434288024902)
[2025-02-13 03:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41][root][INFO] - Training Epoch: 2/2, step 2714/7134 completed (loss: 0.031572241336107254, acc: 0.9916805028915405)
[2025-02-13 03:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41][root][INFO] - Training Epoch: 2/2, step 2715/7134 completed (loss: 0.027935899794101715, acc: 0.9924812316894531)
[2025-02-13 03:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:42][root][INFO] - Training Epoch: 2/2, step 2716/7134 completed (loss: 0.03698243573307991, acc: 0.9908257126808167)
[2025-02-13 03:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:42][root][INFO] - Training Epoch: 2/2, step 2717/7134 completed (loss: 0.062356796115636826, acc: 0.987679660320282)
[2025-02-13 03:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43][root][INFO] - Training Epoch: 2/2, step 2718/7134 completed (loss: 0.12780922651290894, acc: 0.9683098793029785)
[2025-02-13 03:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43][root][INFO] - Training Epoch: 2/2, step 2719/7134 completed (loss: 0.05403084680438042, acc: 0.9864636063575745)
[2025-02-13 03:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43][root][INFO] - Training Epoch: 2/2, step 2720/7134 completed (loss: 0.04351402446627617, acc: 0.9842271208763123)
[2025-02-13 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:44][root][INFO] - Training Epoch: 2/2, step 2721/7134 completed (loss: 0.055111195892095566, acc: 0.9869565367698669)
[2025-02-13 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:44][root][INFO] - Training Epoch: 2/2, step 2722/7134 completed (loss: 0.097626693546772, acc: 0.9760869741439819)
[2025-02-13 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45][root][INFO] - Training Epoch: 2/2, step 2723/7134 completed (loss: 0.09083086997270584, acc: 0.9741697311401367)
[2025-02-13 03:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45][root][INFO] - Training Epoch: 2/2, step 2724/7134 completed (loss: 0.06234005466103554, acc: 0.9817739725112915)
[2025-02-13 03:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45][root][INFO] - Training Epoch: 2/2, step 2725/7134 completed (loss: 0.01394590549170971, acc: 0.9950739145278931)
[2025-02-13 03:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:46][root][INFO] - Training Epoch: 2/2, step 2726/7134 completed (loss: 0.07218047976493835, acc: 0.9815497994422913)
[2025-02-13 03:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:46][root][INFO] - Training Epoch: 2/2, step 2727/7134 completed (loss: 0.037714116275310516, acc: 0.9870370626449585)
[2025-02-13 03:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47][root][INFO] - Training Epoch: 2/2, step 2728/7134 completed (loss: 0.09017585963010788, acc: 0.9724770784378052)
[2025-02-13 03:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47][root][INFO] - Training Epoch: 2/2, step 2729/7134 completed (loss: 0.05516795441508293, acc: 0.9860627055168152)
[2025-02-13 03:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47][root][INFO] - Training Epoch: 2/2, step 2730/7134 completed (loss: 0.06669016182422638, acc: 0.9791666865348816)
[2025-02-13 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:48][root][INFO] - Training Epoch: 2/2, step 2731/7134 completed (loss: 0.08473560959100723, acc: 0.9756690859794617)
[2025-02-13 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:48][root][INFO] - Training Epoch: 2/2, step 2732/7134 completed (loss: 0.03948768600821495, acc: 0.9885786771774292)
[2025-02-13 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:49][root][INFO] - Training Epoch: 2/2, step 2733/7134 completed (loss: 0.04625803977251053, acc: 0.9877883195877075)
[2025-02-13 03:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:49][root][INFO] - Training Epoch: 2/2, step 2734/7134 completed (loss: 0.049908820539712906, acc: 0.98591548204422)
[2025-02-13 03:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50][root][INFO] - Training Epoch: 2/2, step 2735/7134 completed (loss: 0.047100525349378586, acc: 0.9878048896789551)
[2025-02-13 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50][root][INFO] - Training Epoch: 2/2, step 2736/7134 completed (loss: 0.06438067555427551, acc: 0.97947758436203)
[2025-02-13 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50][root][INFO] - Training Epoch: 2/2, step 2737/7134 completed (loss: 0.039059847593307495, acc: 0.9884488582611084)
[2025-02-13 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:51][root][INFO] - Training Epoch: 2/2, step 2738/7134 completed (loss: 0.025783950462937355, acc: 0.9885714054107666)
[2025-02-13 03:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:51][root][INFO] - Training Epoch: 2/2, step 2739/7134 completed (loss: 0.05137253180146217, acc: 0.9854545593261719)
[2025-02-13 03:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52][root][INFO] - Training Epoch: 2/2, step 2740/7134 completed (loss: 0.023818708956241608, acc: 0.9927431344985962)
[2025-02-13 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52][root][INFO] - Training Epoch: 2/2, step 2741/7134 completed (loss: 0.061704155057668686, acc: 0.9813084006309509)
[2025-02-13 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52][root][INFO] - Training Epoch: 2/2, step 2742/7134 completed (loss: 0.01521722786128521, acc: 0.9979166388511658)
[2025-02-13 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53][root][INFO] - Training Epoch: 2/2, step 2743/7134 completed (loss: 0.010846235789358616, acc: 0.9973683953285217)
[2025-02-13 03:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53][root][INFO] - Training Epoch: 2/2, step 2744/7134 completed (loss: 0.017837103456258774, acc: 0.9961685538291931)
[2025-02-13 03:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53][root][INFO] - Training Epoch: 2/2, step 2745/7134 completed (loss: 0.0824810117483139, acc: 0.9773519039154053)
[2025-02-13 03:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54][root][INFO] - Training Epoch: 2/2, step 2746/7134 completed (loss: 0.040470052510499954, acc: 0.9871794581413269)
[2025-02-13 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54][root][INFO] - Training Epoch: 2/2, step 2747/7134 completed (loss: 0.025980794802308083, acc: 0.9926289916038513)
[2025-02-13 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54][root][INFO] - Training Epoch: 2/2, step 2748/7134 completed (loss: 0.011807572096586227, acc: 0.9968152642250061)
[2025-02-13 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55][root][INFO] - Training Epoch: 2/2, step 2749/7134 completed (loss: 0.05244733393192291, acc: 0.9890710115432739)
[2025-02-13 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55][root][INFO] - Training Epoch: 2/2, step 2750/7134 completed (loss: 0.02279592491686344, acc: 0.9894737005233765)
[2025-02-13 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55][root][INFO] - Training Epoch: 2/2, step 2751/7134 completed (loss: 0.053446222096681595, acc: 0.9848101139068604)
[2025-02-13 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:56][root][INFO] - Training Epoch: 2/2, step 2752/7134 completed (loss: 0.041860539466142654, acc: 0.9885931611061096)
[2025-02-13 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:56][root][INFO] - Training Epoch: 2/2, step 2753/7134 completed (loss: 0.06459025293588638, acc: 0.9774436354637146)
[2025-02-13 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57][root][INFO] - Training Epoch: 2/2, step 2754/7134 completed (loss: 0.05729822814464569, acc: 0.9797160029411316)
[2025-02-13 03:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57][root][INFO] - Training Epoch: 2/2, step 2755/7134 completed (loss: 0.03271203115582466, acc: 0.9961240291595459)
[2025-02-13 03:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57][root][INFO] - Training Epoch: 2/2, step 2756/7134 completed (loss: 0.057639122009277344, acc: 0.9809523820877075)
[2025-02-13 03:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58][root][INFO] - Training Epoch: 2/2, step 2757/7134 completed (loss: 0.07680615782737732, acc: 0.9808743000030518)
[2025-02-13 03:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58][root][INFO] - Training Epoch: 2/2, step 2758/7134 completed (loss: 0.05450990051031113, acc: 0.9817351698875427)
[2025-02-13 03:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58][root][INFO] - Training Epoch: 2/2, step 2759/7134 completed (loss: 0.02951887995004654, acc: 0.9866220951080322)
[2025-02-13 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:59][root][INFO] - Training Epoch: 2/2, step 2760/7134 completed (loss: 0.03293779492378235, acc: 0.9873015880584717)
[2025-02-13 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:59][root][INFO] - Training Epoch: 2/2, step 2761/7134 completed (loss: 0.08955646306276321, acc: 0.9760100841522217)
[2025-02-13 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:00][root][INFO] - Training Epoch: 2/2, step 2762/7134 completed (loss: 0.04921220615506172, acc: 0.9850402474403381)
[2025-02-13 03:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:00][root][INFO] - Training Epoch: 2/2, step 2763/7134 completed (loss: 0.05167756602168083, acc: 0.9861809015274048)
[2025-02-13 03:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01][root][INFO] - Training Epoch: 2/2, step 2764/7134 completed (loss: 0.0917455404996872, acc: 0.9736495614051819)
[2025-02-13 03:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01][root][INFO] - Training Epoch: 2/2, step 2765/7134 completed (loss: 0.03812741860747337, acc: 0.9928160905838013)
[2025-02-13 03:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01][root][INFO] - Training Epoch: 2/2, step 2766/7134 completed (loss: 0.019660064950585365, acc: 0.993537962436676)
[2025-02-13 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:02][root][INFO] - Training Epoch: 2/2, step 2767/7134 completed (loss: 0.08318225294351578, acc: 0.9790123701095581)
[2025-02-13 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:02][root][INFO] - Training Epoch: 2/2, step 2768/7134 completed (loss: 0.05772676318883896, acc: 0.9801980257034302)
[2025-02-13 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:03][root][INFO] - Training Epoch: 2/2, step 2769/7134 completed (loss: 0.04722411185503006, acc: 0.9876126050949097)
[2025-02-13 03:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:03][root][INFO] - Training Epoch: 2/2, step 2770/7134 completed (loss: 0.02310759201645851, acc: 0.9930459260940552)
[2025-02-13 03:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:04][root][INFO] - Training Epoch: 2/2, step 2771/7134 completed (loss: 0.035337645560503006, acc: 0.9914255142211914)
[2025-02-13 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:04][root][INFO] - Training Epoch: 2/2, step 2772/7134 completed (loss: 0.028854455798864365, acc: 0.991051435470581)
[2025-02-13 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05][root][INFO] - Training Epoch: 2/2, step 2773/7134 completed (loss: 0.022304236888885498, acc: 0.99314284324646)
[2025-02-13 03:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05][root][INFO] - Training Epoch: 2/2, step 2774/7134 completed (loss: 0.06135312467813492, acc: 0.9824561476707458)
[2025-02-13 03:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05][root][INFO] - Training Epoch: 2/2, step 2775/7134 completed (loss: 0.04062287509441376, acc: 0.9926380515098572)
[2025-02-13 03:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:06][root][INFO] - Training Epoch: 2/2, step 2776/7134 completed (loss: 0.026158533990383148, acc: 0.9907975196838379)
[2025-02-13 03:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:06][root][INFO] - Training Epoch: 2/2, step 2777/7134 completed (loss: 0.02599087916314602, acc: 0.9902439117431641)
[2025-02-13 03:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:07][root][INFO] - Training Epoch: 2/2, step 2778/7134 completed (loss: 0.03539992868900299, acc: 0.9918887615203857)
[2025-02-13 03:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:07][root][INFO] - Training Epoch: 2/2, step 2779/7134 completed (loss: 0.056844018399715424, acc: 0.9852320551872253)
[2025-02-13 03:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:08][root][INFO] - Training Epoch: 2/2, step 2780/7134 completed (loss: 0.056446537375450134, acc: 0.9873417615890503)
[2025-02-13 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:08][root][INFO] - Training Epoch: 2/2, step 2781/7134 completed (loss: 0.02572731114923954, acc: 0.9922279715538025)
[2025-02-13 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:09][root][INFO] - Training Epoch: 2/2, step 2782/7134 completed (loss: 0.035200826823711395, acc: 0.9905462265014648)
[2025-02-13 03:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:09][root][INFO] - Training Epoch: 2/2, step 2783/7134 completed (loss: 0.021385548636317253, acc: 0.9924924969673157)
[2025-02-13 03:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10][root][INFO] - Training Epoch: 2/2, step 2784/7134 completed (loss: 0.029557395726442337, acc: 0.9947423934936523)
[2025-02-13 03:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10][root][INFO] - Training Epoch: 2/2, step 2785/7134 completed (loss: 0.030005749315023422, acc: 0.9934498071670532)
[2025-02-13 03:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10][root][INFO] - Training Epoch: 2/2, step 2786/7134 completed (loss: 0.03292921185493469, acc: 0.9906542301177979)
[2025-02-13 03:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:11][root][INFO] - Training Epoch: 2/2, step 2787/7134 completed (loss: 0.018851907923817635, acc: 0.9973439574241638)
[2025-02-13 03:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:11][root][INFO] - Training Epoch: 2/2, step 2788/7134 completed (loss: 0.029908617958426476, acc: 0.9882199168205261)
[2025-02-13 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:12][root][INFO] - Training Epoch: 2/2, step 2789/7134 completed (loss: 0.1327320784330368, acc: 0.9682996869087219)
[2025-02-13 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:12][root][INFO] - Training Epoch: 2/2, step 2790/7134 completed (loss: 0.13548868894577026, acc: 0.9668874144554138)
[2025-02-13 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:13][root][INFO] - Training Epoch: 2/2, step 2791/7134 completed (loss: 0.035780467092990875, acc: 0.9910314083099365)
[2025-02-13 03:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:13][root][INFO] - Training Epoch: 2/2, step 2792/7134 completed (loss: 0.04177715256810188, acc: 0.9918509721755981)
[2025-02-13 03:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14][root][INFO] - Training Epoch: 2/2, step 2793/7134 completed (loss: 0.046449869871139526, acc: 0.9905533194541931)
[2025-02-13 03:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14][root][INFO] - Training Epoch: 2/2, step 2794/7134 completed (loss: 0.033490054309368134, acc: 0.990227997303009)
[2025-02-13 03:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14][root][INFO] - Training Epoch: 2/2, step 2795/7134 completed (loss: 0.11372213810682297, acc: 0.9698376059532166)
[2025-02-13 03:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15][root][INFO] - Training Epoch: 2/2, step 2796/7134 completed (loss: 0.14604352414608002, acc: 0.9631336331367493)
[2025-02-13 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15][root][INFO] - Training Epoch: 2/2, step 2797/7134 completed (loss: 0.0432627908885479, acc: 0.9873417615890503)
[2025-02-13 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15][root][INFO] - Training Epoch: 2/2, step 2798/7134 completed (loss: 0.07653210312128067, acc: 0.970588207244873)
[2025-02-13 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16][root][INFO] - Training Epoch: 2/2, step 2799/7134 completed (loss: 0.0785132497549057, acc: 0.9754816293716431)
[2025-02-13 03:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16][root][INFO] - Training Epoch: 2/2, step 2800/7134 completed (loss: 0.08750587701797485, acc: 0.9771528840065002)
[2025-02-13 03:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16][root][INFO] - Training Epoch: 2/2, step 2801/7134 completed (loss: 0.04174444079399109, acc: 0.9923273921012878)
[2025-02-13 03:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17][root][INFO] - Training Epoch: 2/2, step 2802/7134 completed (loss: 0.047722358256578445, acc: 0.9875156283378601)
[2025-02-13 03:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17][root][INFO] - Training Epoch: 2/2, step 2803/7134 completed (loss: 0.014575170353055, acc: 0.9957567453384399)
[2025-02-13 03:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:18][root][INFO] - Training Epoch: 2/2, step 2804/7134 completed (loss: 0.03283156827092171, acc: 0.9962871074676514)
[2025-02-13 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:18][root][INFO] - Training Epoch: 2/2, step 2805/7134 completed (loss: 0.022779542952775955, acc: 0.9960474371910095)
[2025-02-13 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19][root][INFO] - Training Epoch: 2/2, step 2806/7134 completed (loss: 0.042487043887376785, acc: 0.9852458834648132)
[2025-02-13 03:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19][root][INFO] - Training Epoch: 2/2, step 2807/7134 completed (loss: 0.05725081264972687, acc: 0.9896050095558167)
[2025-02-13 03:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19][root][INFO] - Training Epoch: 2/2, step 2808/7134 completed (loss: 0.019643539562821388, acc: 0.99314284324646)
[2025-02-13 03:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20][root][INFO] - Training Epoch: 2/2, step 2809/7134 completed (loss: 0.02714666724205017, acc: 0.9928698539733887)
[2025-02-13 03:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20][root][INFO] - Training Epoch: 2/2, step 2810/7134 completed (loss: 0.03031938523054123, acc: 0.9916864633560181)
[2025-02-13 03:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:21][root][INFO] - Training Epoch: 2/2, step 2811/7134 completed (loss: 0.014232128858566284, acc: 0.9952267408370972)
[2025-02-13 03:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:21][root][INFO] - Training Epoch: 2/2, step 2812/7134 completed (loss: 0.022654810920357704, acc: 0.9891975522041321)
[2025-02-13 03:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:22][root][INFO] - Training Epoch: 2/2, step 2813/7134 completed (loss: 0.013315442018210888, acc: 0.996221661567688)
[2025-02-13 03:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:22][root][INFO] - Training Epoch: 2/2, step 2814/7134 completed (loss: 0.019471311941742897, acc: 0.9974026083946228)
[2025-02-13 03:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:23][root][INFO] - Training Epoch: 2/2, step 2815/7134 completed (loss: 0.047347258776426315, acc: 0.9881376028060913)
[2025-02-13 03:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:23][root][INFO] - Training Epoch: 2/2, step 2816/7134 completed (loss: 0.043889086693525314, acc: 0.9894598126411438)
[2025-02-13 03:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24][root][INFO] - Training Epoch: 2/2, step 2817/7134 completed (loss: 0.04580867663025856, acc: 0.9884696006774902)
[2025-02-13 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24][root][INFO] - Training Epoch: 2/2, step 2818/7134 completed (loss: 0.0343533493578434, acc: 0.9879385828971863)
[2025-02-13 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24][root][INFO] - Training Epoch: 2/2, step 2819/7134 completed (loss: 0.00847594067454338, acc: 0.9983844757080078)
[2025-02-13 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:25][root][INFO] - Training Epoch: 2/2, step 2820/7134 completed (loss: 0.08892226964235306, acc: 0.969072163105011)
[2025-02-13 03:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:25][root][INFO] - Training Epoch: 2/2, step 2821/7134 completed (loss: 0.04517833888530731, acc: 0.9820788502693176)
[2025-02-13 03:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:26][root][INFO] - Training Epoch: 2/2, step 2822/7134 completed (loss: 0.05488581210374832, acc: 0.9797724485397339)
[2025-02-13 03:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:26][root][INFO] - Training Epoch: 2/2, step 2823/7134 completed (loss: 0.06418059021234512, acc: 0.978723406791687)
[2025-02-13 03:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27][root][INFO] - Training Epoch: 2/2, step 2824/7134 completed (loss: 0.09108205139636993, acc: 0.975649356842041)
[2025-02-13 03:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27][root][INFO] - Training Epoch: 2/2, step 2825/7134 completed (loss: 0.06447159498929977, acc: 0.9865771532058716)
[2025-02-13 03:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27][root][INFO] - Training Epoch: 2/2, step 2826/7134 completed (loss: 0.0648324117064476, acc: 0.9819276928901672)
[2025-02-13 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:28][root][INFO] - Training Epoch: 2/2, step 2827/7134 completed (loss: 0.04729995131492615, acc: 0.9859514832496643)
[2025-02-13 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:28][root][INFO] - Training Epoch: 2/2, step 2828/7134 completed (loss: 0.03703088313341141, acc: 0.9887999892234802)
[2025-02-13 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:29][root][INFO] - Training Epoch: 2/2, step 2829/7134 completed (loss: 0.06989678740501404, acc: 0.9836289286613464)
[2025-02-13 03:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:29][root][INFO] - Training Epoch: 2/2, step 2830/7134 completed (loss: 0.01686488464474678, acc: 0.9963570237159729)
[2025-02-13 03:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30][root][INFO] - Training Epoch: 2/2, step 2831/7134 completed (loss: 0.026481926441192627, acc: 0.9921011328697205)
[2025-02-13 03:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30][root][INFO] - Training Epoch: 2/2, step 2832/7134 completed (loss: 0.05065318942070007, acc: 0.9903846383094788)
[2025-02-13 03:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30][root][INFO] - Training Epoch: 2/2, step 2833/7134 completed (loss: 0.04905217885971069, acc: 0.9906432628631592)
[2025-02-13 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:31][root][INFO] - Training Epoch: 2/2, step 2834/7134 completed (loss: 0.0697355642914772, acc: 0.9809644818305969)
[2025-02-13 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:31][root][INFO] - Training Epoch: 2/2, step 2835/7134 completed (loss: 0.05190246179699898, acc: 0.9838449358940125)
[2025-02-13 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:32][root][INFO] - Training Epoch: 2/2, step 2836/7134 completed (loss: 0.05061083287000656, acc: 0.9857819676399231)
[2025-02-13 03:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:32][root][INFO] - Training Epoch: 2/2, step 2837/7134 completed (loss: 0.023659437894821167, acc: 0.9891892075538635)
[2025-02-13 03:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:33][root][INFO] - Training Epoch: 2/2, step 2838/7134 completed (loss: 0.07415252178907394, acc: 0.9769874215126038)
[2025-02-13 03:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:33][root][INFO] - Training Epoch: 2/2, step 2839/7134 completed (loss: 0.037977106869220734, acc: 0.9924127459526062)
[2025-02-13 03:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34][root][INFO] - Training Epoch: 2/2, step 2840/7134 completed (loss: 0.03321968391537666, acc: 0.988950252532959)
[2025-02-13 03:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34][root][INFO] - Training Epoch: 2/2, step 2841/7134 completed (loss: 0.04480509087443352, acc: 0.9861751198768616)
[2025-02-13 03:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34][root][INFO] - Training Epoch: 2/2, step 2842/7134 completed (loss: 0.035430941730737686, acc: 0.9896103739738464)
[2025-02-13 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:35][root][INFO] - Training Epoch: 2/2, step 2843/7134 completed (loss: 0.027292650192975998, acc: 0.9926470518112183)
[2025-02-13 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:35][root][INFO] - Training Epoch: 2/2, step 2844/7134 completed (loss: 0.10564716160297394, acc: 0.9671682715415955)
[2025-02-13 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:36][root][INFO] - Training Epoch: 2/2, step 2845/7134 completed (loss: 0.03755231574177742, acc: 0.9913344979286194)
[2025-02-13 03:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:36][root][INFO] - Training Epoch: 2/2, step 2846/7134 completed (loss: 0.0699518471956253, acc: 0.9818781018257141)
[2025-02-13 03:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37][root][INFO] - Training Epoch: 2/2, step 2847/7134 completed (loss: 0.051998794078826904, acc: 0.9868035316467285)
[2025-02-13 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37][root][INFO] - Training Epoch: 2/2, step 2848/7134 completed (loss: 0.035831477493047714, acc: 0.9906759858131409)
[2025-02-13 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37][root][INFO] - Training Epoch: 2/2, step 2849/7134 completed (loss: 0.05482644960284233, acc: 0.9903181195259094)
[2025-02-13 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:38][root][INFO] - Training Epoch: 2/2, step 2850/7134 completed (loss: 0.026995884254574776, acc: 0.9913978576660156)
[2025-02-13 03:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:38][root][INFO] - Training Epoch: 2/2, step 2851/7134 completed (loss: 0.02499459870159626, acc: 0.988950252532959)
[2025-02-13 03:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39][root][INFO] - Training Epoch: 2/2, step 2852/7134 completed (loss: 0.020721422508358955, acc: 0.9940357804298401)
[2025-02-13 03:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39][root][INFO] - Training Epoch: 2/2, step 2853/7134 completed (loss: 0.03116408921778202, acc: 0.9923858046531677)
[2025-02-13 03:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39][root][INFO] - Training Epoch: 2/2, step 2854/7134 completed (loss: 0.04259320721030235, acc: 0.9851411581039429)
[2025-02-13 03:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:40][root][INFO] - Training Epoch: 2/2, step 2855/7134 completed (loss: 0.04095054045319557, acc: 0.9919224381446838)
[2025-02-13 03:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:40][root][INFO] - Training Epoch: 2/2, step 2856/7134 completed (loss: 0.011186149902641773, acc: 0.9971791505813599)
[2025-02-13 03:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41][root][INFO] - Training Epoch: 2/2, step 2857/7134 completed (loss: 0.052179910242557526, acc: 0.986994206905365)
[2025-02-13 03:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41][root][INFO] - Training Epoch: 2/2, step 2858/7134 completed (loss: 0.01255632471293211, acc: 0.9968454241752625)
[2025-02-13 03:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41][root][INFO] - Training Epoch: 2/2, step 2859/7134 completed (loss: 0.04400545731186867, acc: 0.9818689227104187)
[2025-02-13 03:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:42][root][INFO] - Training Epoch: 2/2, step 2860/7134 completed (loss: 0.028431329876184464, acc: 0.988990843296051)
[2025-02-13 03:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:42][root][INFO] - Training Epoch: 2/2, step 2861/7134 completed (loss: 0.015385388396680355, acc: 0.9959431886672974)
[2025-02-13 03:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43][root][INFO] - Training Epoch: 2/2, step 2862/7134 completed (loss: 0.016278492286801338, acc: 0.994854211807251)
[2025-02-13 03:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43][root][INFO] - Training Epoch: 2/2, step 2863/7134 completed (loss: 0.011801887303590775, acc: 0.9949044585227966)
[2025-02-13 03:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43][root][INFO] - Training Epoch: 2/2, step 2864/7134 completed (loss: 0.039775315672159195, acc: 0.9922077655792236)
[2025-02-13 03:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:44][root][INFO] - Training Epoch: 2/2, step 2865/7134 completed (loss: 0.022994086146354675, acc: 0.991304337978363)
[2025-02-13 03:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:44][root][INFO] - Training Epoch: 2/2, step 2866/7134 completed (loss: 0.02647007629275322, acc: 0.9925261735916138)
[2025-02-13 03:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45][root][INFO] - Training Epoch: 2/2, step 2867/7134 completed (loss: 0.02634822204709053, acc: 0.9907975196838379)
[2025-02-13 03:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45][root][INFO] - Training Epoch: 2/2, step 2868/7134 completed (loss: 0.007943409495055676, acc: 0.9971910119056702)
[2025-02-13 03:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45][root][INFO] - Training Epoch: 2/2, step 2869/7134 completed (loss: 0.04769390448927879, acc: 0.9941434860229492)
[2025-02-13 03:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:46][root][INFO] - Training Epoch: 2/2, step 2870/7134 completed (loss: 0.057502295821905136, acc: 0.9874213933944702)
[2025-02-13 03:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:46][root][INFO] - Training Epoch: 2/2, step 2871/7134 completed (loss: 0.057461291551589966, acc: 0.9890795350074768)
[2025-02-13 03:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47][root][INFO] - Training Epoch: 2/2, step 2872/7134 completed (loss: 0.004773324821144342, acc: 1.0)
[2025-02-13 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47][root][INFO] - Training Epoch: 2/2, step 2873/7134 completed (loss: 0.022442635148763657, acc: 0.9927007555961609)
[2025-02-13 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47][root][INFO] - Training Epoch: 2/2, step 2874/7134 completed (loss: 0.02442236803472042, acc: 0.9921996593475342)
[2025-02-13 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:48][root][INFO] - Training Epoch: 2/2, step 2875/7134 completed (loss: 0.030142461881041527, acc: 0.9900426864624023)
[2025-02-13 03:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:48][root][INFO] - Training Epoch: 2/2, step 2876/7134 completed (loss: 0.09620974212884903, acc: 0.981566846370697)
[2025-02-13 03:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49][root][INFO] - Training Epoch: 2/2, step 2877/7134 completed (loss: 0.02210315130650997, acc: 0.9912663698196411)
[2025-02-13 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49][root][INFO] - Training Epoch: 2/2, step 2878/7134 completed (loss: 0.024581650272011757, acc: 0.9957627058029175)
[2025-02-13 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49][root][INFO] - Training Epoch: 2/2, step 2879/7134 completed (loss: 0.011296587064862251, acc: 0.9968652129173279)
[2025-02-13 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:50][root][INFO] - Training Epoch: 2/2, step 2880/7134 completed (loss: 0.013557511381804943, acc: 0.996874988079071)
[2025-02-13 03:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:50][root][INFO] - Training Epoch: 2/2, step 2881/7134 completed (loss: 0.027777383103966713, acc: 0.989266574382782)
[2025-02-13 03:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51][root][INFO] - Training Epoch: 2/2, step 2882/7134 completed (loss: 0.0557234100997448, acc: 0.9839704036712646)
[2025-02-13 03:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51][root][INFO] - Training Epoch: 2/2, step 2883/7134 completed (loss: 0.039389029145240784, acc: 0.9894179701805115)
[2025-02-13 03:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51][root][INFO] - Training Epoch: 2/2, step 2884/7134 completed (loss: 0.08444567769765854, acc: 0.9713423848152161)
[2025-02-13 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:52][root][INFO] - Training Epoch: 2/2, step 2885/7134 completed (loss: 0.035258036106824875, acc: 0.987860381603241)
[2025-02-13 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:52][root][INFO] - Training Epoch: 2/2, step 2886/7134 completed (loss: 0.08198655396699905, acc: 0.9805970191955566)
[2025-02-13 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53][root][INFO] - Training Epoch: 2/2, step 2887/7134 completed (loss: 0.049063652753829956, acc: 0.9869822263717651)
[2025-02-13 03:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53][root][INFO] - Training Epoch: 2/2, step 2888/7134 completed (loss: 0.06406628340482712, acc: 0.9805825352668762)
[2025-02-13 03:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53][root][INFO] - Training Epoch: 2/2, step 2889/7134 completed (loss: 0.016542909666895866, acc: 0.9930747747421265)
[2025-02-13 03:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:54][root][INFO] - Training Epoch: 2/2, step 2890/7134 completed (loss: 0.02448783442378044, acc: 0.9960052967071533)
[2025-02-13 03:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:54][root][INFO] - Training Epoch: 2/2, step 2891/7134 completed (loss: 0.045006826519966125, acc: 0.9907692074775696)
[2025-02-13 03:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:55][root][INFO] - Training Epoch: 2/2, step 2892/7134 completed (loss: 0.08235421031713486, acc: 0.9811083078384399)
[2025-02-13 03:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:55][root][INFO] - Training Epoch: 2/2, step 2893/7134 completed (loss: 0.05010959878563881, acc: 0.9861286282539368)
[2025-02-13 03:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56][root][INFO] - Training Epoch: 2/2, step 2894/7134 completed (loss: 0.032989028841257095, acc: 0.9865546226501465)
[2025-02-13 03:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56][root][INFO] - Training Epoch: 2/2, step 2895/7134 completed (loss: 0.021531395614147186, acc: 0.992438554763794)
[2025-02-13 03:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56][root][INFO] - Training Epoch: 2/2, step 2896/7134 completed (loss: 0.027741318568587303, acc: 0.9898219108581543)
[2025-02-13 03:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:57][root][INFO] - Training Epoch: 2/2, step 2897/7134 completed (loss: 0.03883033245801926, acc: 0.9875665903091431)
[2025-02-13 03:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:57][root][INFO] - Training Epoch: 2/2, step 2898/7134 completed (loss: 0.03312660753726959, acc: 0.9921568632125854)
[2025-02-13 03:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:58][root][INFO] - Training Epoch: 2/2, step 2899/7134 completed (loss: 0.015749521553516388, acc: 0.9956395626068115)
[2025-02-13 03:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:58][root][INFO] - Training Epoch: 2/2, step 2900/7134 completed (loss: 0.032545220106840134, acc: 0.9938271641731262)
[2025-02-13 03:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59][root][INFO] - Training Epoch: 2/2, step 2901/7134 completed (loss: 0.0313129648566246, acc: 0.9888535141944885)
[2025-02-13 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59][root][INFO] - Training Epoch: 2/2, step 2902/7134 completed (loss: 0.03314695507287979, acc: 0.9868420958518982)
[2025-02-13 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59][root][INFO] - Training Epoch: 2/2, step 2903/7134 completed (loss: 0.05293814837932587, acc: 0.9851552248001099)
[2025-02-13 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:00][root][INFO] - Training Epoch: 2/2, step 2904/7134 completed (loss: 0.016396453604102135, acc: 0.995006263256073)
[2025-02-13 03:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:00][root][INFO] - Training Epoch: 2/2, step 2905/7134 completed (loss: 0.033666908740997314, acc: 0.987484335899353)
[2025-02-13 03:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01][root][INFO] - Training Epoch: 2/2, step 2906/7134 completed (loss: 0.005504396744072437, acc: 0.9988024234771729)
[2025-02-13 03:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01][root][INFO] - Training Epoch: 2/2, step 2907/7134 completed (loss: 0.047351039946079254, acc: 0.9878869652748108)
[2025-02-13 03:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01][root][INFO] - Training Epoch: 2/2, step 2908/7134 completed (loss: 0.023960387334227562, acc: 0.9948253631591797)
[2025-02-13 03:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:02][root][INFO] - Training Epoch: 2/2, step 2909/7134 completed (loss: 0.028403112664818764, acc: 0.9880095720291138)
[2025-02-13 03:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:02][root][INFO] - Training Epoch: 2/2, step 2910/7134 completed (loss: 0.06762062013149261, acc: 0.9828693866729736)
[2025-02-13 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:03][root][INFO] - Training Epoch: 2/2, step 2911/7134 completed (loss: 0.012936117127537727, acc: 0.9973154067993164)
[2025-02-13 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:03][root][INFO] - Training Epoch: 2/2, step 2912/7134 completed (loss: 0.026188591495156288, acc: 0.9903448224067688)
[2025-02-13 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:04][root][INFO] - Training Epoch: 2/2, step 2913/7134 completed (loss: 0.028628794476389885, acc: 0.9914529919624329)
[2025-02-13 03:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:04][root][INFO] - Training Epoch: 2/2, step 2914/7134 completed (loss: 0.022550005465745926, acc: 0.9975874423980713)
[2025-02-13 03:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05][root][INFO] - Training Epoch: 2/2, step 2915/7134 completed (loss: 0.06628821045160294, acc: 0.9827089309692383)
[2025-02-13 03:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05][root][INFO] - Training Epoch: 2/2, step 2916/7134 completed (loss: 0.05216759815812111, acc: 0.9845361113548279)
[2025-02-13 03:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05][root][INFO] - Training Epoch: 2/2, step 2917/7134 completed (loss: 0.0331612303853035, acc: 0.9899497628211975)
[2025-02-13 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:06][root][INFO] - Training Epoch: 2/2, step 2918/7134 completed (loss: 0.018807316198945045, acc: 0.9918414950370789)
[2025-02-13 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:06][root][INFO] - Training Epoch: 2/2, step 2919/7134 completed (loss: 0.06335386633872986, acc: 0.9894242286682129)
[2025-02-13 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:07][root][INFO] - Training Epoch: 2/2, step 2920/7134 completed (loss: 0.019629361107945442, acc: 0.9920529723167419)
[2025-02-13 03:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:07][root][INFO] - Training Epoch: 2/2, step 2921/7134 completed (loss: 0.014059878885746002, acc: 0.9913793206214905)
[2025-02-13 03:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08][root][INFO] - Training Epoch: 2/2, step 2922/7134 completed (loss: 0.06838195770978928, acc: 0.9886934757232666)
[2025-02-13 03:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08][root][INFO] - Training Epoch: 2/2, step 2923/7134 completed (loss: 0.15568199753761292, acc: 0.9738863110542297)
[2025-02-13 03:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09][root][INFO] - Training Epoch: 2/2, step 2924/7134 completed (loss: 0.014896820299327374, acc: 0.9964622855186462)
[2025-02-13 03:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09][root][INFO] - Training Epoch: 2/2, step 2925/7134 completed (loss: 0.03980395570397377, acc: 0.9901719689369202)
[2025-02-13 03:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09][root][INFO] - Training Epoch: 2/2, step 2926/7134 completed (loss: 0.14247013628482819, acc: 0.9759615659713745)
[2025-02-13 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:10][root][INFO] - Training Epoch: 2/2, step 2927/7134 completed (loss: 0.019845981150865555, acc: 0.9927448630332947)
[2025-02-13 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:10][root][INFO] - Training Epoch: 2/2, step 2928/7134 completed (loss: 0.02762318216264248, acc: 0.9939024448394775)
[2025-02-13 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11][root][INFO] - Training Epoch: 2/2, step 2929/7134 completed (loss: 0.026740383356809616, acc: 0.9947780966758728)
[2025-02-13 03:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11][root][INFO] - Training Epoch: 2/2, step 2930/7134 completed (loss: 0.04400155320763588, acc: 0.988252580165863)
[2025-02-13 03:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:12][root][INFO] - Training Epoch: 2/2, step 2931/7134 completed (loss: 0.04976360872387886, acc: 0.9850522875785828)
[2025-02-13 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:12][root][INFO] - Training Epoch: 2/2, step 2932/7134 completed (loss: 0.024101395159959793, acc: 0.9930475354194641)
[2025-02-13 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13][root][INFO] - Training Epoch: 2/2, step 2933/7134 completed (loss: 0.028291206806898117, acc: 0.9870129823684692)
[2025-02-13 03:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13][root][INFO] - Training Epoch: 2/2, step 2934/7134 completed (loss: 0.015464809723198414, acc: 0.9971791505813599)
[2025-02-13 03:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13][root][INFO] - Training Epoch: 2/2, step 2935/7134 completed (loss: 0.02389497309923172, acc: 0.9944933652877808)
[2025-02-13 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14][root][INFO] - Training Epoch: 2/2, step 2936/7134 completed (loss: 0.03914141282439232, acc: 0.9888097643852234)
[2025-02-13 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14][root][INFO] - Training Epoch: 2/2, step 2937/7134 completed (loss: 0.044260647147893906, acc: 0.9885222315788269)
[2025-02-13 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15][root][INFO] - Training Epoch: 2/2, step 2938/7134 completed (loss: 0.025033991783857346, acc: 0.994413435459137)
[2025-02-13 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15][root][INFO] - Training Epoch: 2/2, step 2939/7134 completed (loss: 0.02451523207128048, acc: 0.9919871687889099)
[2025-02-13 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15][root][INFO] - Training Epoch: 2/2, step 2940/7134 completed (loss: 0.04901963099837303, acc: 0.9890561103820801)
[2025-02-13 03:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:16][root][INFO] - Training Epoch: 2/2, step 2941/7134 completed (loss: 0.023315690457820892, acc: 0.9912917017936707)
[2025-02-13 03:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:16][root][INFO] - Training Epoch: 2/2, step 2942/7134 completed (loss: 0.028350187465548515, acc: 0.9925705790519714)
[2025-02-13 03:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17][root][INFO] - Training Epoch: 2/2, step 2943/7134 completed (loss: 0.014274725690484047, acc: 0.9966273307800293)
[2025-02-13 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17][root][INFO] - Training Epoch: 2/2, step 2944/7134 completed (loss: 0.030398426577448845, acc: 0.9911894202232361)
[2025-02-13 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17][root][INFO] - Training Epoch: 2/2, step 2945/7134 completed (loss: 0.023043034598231316, acc: 0.9906687140464783)
[2025-02-13 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:18][root][INFO] - Training Epoch: 2/2, step 2946/7134 completed (loss: 0.03353777900338173, acc: 0.992732584476471)
[2025-02-13 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:18][root][INFO] - Training Epoch: 2/2, step 2947/7134 completed (loss: 0.045114025473594666, acc: 0.9842767119407654)
[2025-02-13 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19][root][INFO] - Training Epoch: 2/2, step 2948/7134 completed (loss: 0.047923214733600616, acc: 0.9884678721427917)
[2025-02-13 03:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19][root][INFO] - Training Epoch: 2/2, step 2949/7134 completed (loss: 0.0041847992688417435, acc: 1.0)
[2025-02-13 03:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19][root][INFO] - Training Epoch: 2/2, step 2950/7134 completed (loss: 0.01855994202196598, acc: 0.9944827556610107)
[2025-02-13 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:20][root][INFO] - Training Epoch: 2/2, step 2951/7134 completed (loss: 0.011147328652441502, acc: 0.9971014261245728)
[2025-02-13 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:20][root][INFO] - Training Epoch: 2/2, step 2952/7134 completed (loss: 0.012262322939932346, acc: 0.9955489635467529)
[2025-02-13 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21][root][INFO] - Training Epoch: 2/2, step 2953/7134 completed (loss: 0.025835828855633736, acc: 0.9898648858070374)
[2025-02-13 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21][root][INFO] - Training Epoch: 2/2, step 2954/7134 completed (loss: 0.03357116878032684, acc: 0.9909909963607788)
[2025-02-13 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21][root][INFO] - Training Epoch: 2/2, step 2955/7134 completed (loss: 0.022928155958652496, acc: 0.9928977489471436)
[2025-02-13 03:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:22][root][INFO] - Training Epoch: 2/2, step 2956/7134 completed (loss: 0.015260421670973301, acc: 0.9970717430114746)
[2025-02-13 03:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:22][root][INFO] - Training Epoch: 2/2, step 2957/7134 completed (loss: 0.0156854260712862, acc: 0.9973261952400208)
[2025-02-13 03:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:23][root][INFO] - Training Epoch: 2/2, step 2958/7134 completed (loss: 0.02002168633043766, acc: 0.9948717951774597)
[2025-02-13 03:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:23][root][INFO] - Training Epoch: 2/2, step 2959/7134 completed (loss: 0.013039813376963139, acc: 0.9941520690917969)
[2025-02-13 03:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24][root][INFO] - Training Epoch: 2/2, step 2960/7134 completed (loss: 0.020478418096899986, acc: 0.9917355179786682)
[2025-02-13 03:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24][root][INFO] - Training Epoch: 2/2, step 2961/7134 completed (loss: 0.013422999531030655, acc: 0.9959183931350708)
[2025-02-13 03:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24][root][INFO] - Training Epoch: 2/2, step 2962/7134 completed (loss: 0.010404241271317005, acc: 0.9948253631591797)
[2025-02-13 03:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:25][root][INFO] - Training Epoch: 2/2, step 2963/7134 completed (loss: 0.016320759430527687, acc: 0.9947643876075745)
[2025-02-13 03:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:25][root][INFO] - Training Epoch: 2/2, step 2964/7134 completed (loss: 0.015515033155679703, acc: 0.9956076145172119)
[2025-02-13 03:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:26][root][INFO] - Training Epoch: 2/2, step 2965/7134 completed (loss: 0.05361327528953552, acc: 0.9880596995353699)
[2025-02-13 03:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:26][root][INFO] - Training Epoch: 2/2, step 2966/7134 completed (loss: 0.006840860471129417, acc: 0.9978678226470947)
[2025-02-13 03:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27][root][INFO] - Training Epoch: 2/2, step 2967/7134 completed (loss: 0.03816048800945282, acc: 0.9888268113136292)
[2025-02-13 03:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27][root][INFO] - Training Epoch: 2/2, step 2968/7134 completed (loss: 0.11622533947229385, acc: 0.9694533944129944)
[2025-02-13 03:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27][root][INFO] - Training Epoch: 2/2, step 2969/7134 completed (loss: 0.2078433334827423, acc: 0.9497041702270508)
[2025-02-13 03:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28][root][INFO] - Training Epoch: 2/2, step 2970/7134 completed (loss: 0.2608550786972046, acc: 0.9498910903930664)
[2025-02-13 03:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28][root][INFO] - Training Epoch: 2/2, step 2971/7134 completed (loss: 0.05343851447105408, acc: 0.9842312932014465)
[2025-02-13 03:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29][root][INFO] - Training Epoch: 2/2, step 2972/7134 completed (loss: 0.02731725387275219, acc: 0.9910979270935059)
[2025-02-13 03:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29][root][INFO] - Training Epoch: 2/2, step 2973/7134 completed (loss: 0.05301254987716675, acc: 0.9797468185424805)
[2025-02-13 03:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29][root][INFO] - Training Epoch: 2/2, step 2974/7134 completed (loss: 0.04187890887260437, acc: 0.9857327938079834)
[2025-02-13 03:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:30][root][INFO] - Training Epoch: 2/2, step 2975/7134 completed (loss: 0.032510701566934586, acc: 0.988950252532959)
[2025-02-13 03:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:30][root][INFO] - Training Epoch: 2/2, step 2976/7134 completed (loss: 0.048870693892240524, acc: 0.9819148778915405)
[2025-02-13 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:31][root][INFO] - Training Epoch: 2/2, step 2977/7134 completed (loss: 0.04624561965465546, acc: 0.986994206905365)
[2025-02-13 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:31][root][INFO] - Training Epoch: 2/2, step 2978/7134 completed (loss: 0.0554647333920002, acc: 0.980141818523407)
[2025-02-13 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:32][root][INFO] - Training Epoch: 2/2, step 2979/7134 completed (loss: 0.017616422846913338, acc: 0.9968847632408142)
[2025-02-13 03:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:32][root][INFO] - Training Epoch: 2/2, step 2980/7134 completed (loss: 0.048208653926849365, acc: 0.9923567175865173)
[2025-02-13 03:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33][root][INFO] - Training Epoch: 2/2, step 2981/7134 completed (loss: 0.0698646679520607, acc: 0.9823608994483948)
[2025-02-13 03:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33][root][INFO] - Training Epoch: 2/2, step 2982/7134 completed (loss: 0.044894877821207047, acc: 0.9882199168205261)
[2025-02-13 03:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33][root][INFO] - Training Epoch: 2/2, step 2983/7134 completed (loss: 0.0765957310795784, acc: 0.9841269850730896)
[2025-02-13 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:34][root][INFO] - Training Epoch: 2/2, step 2984/7134 completed (loss: 0.02611919865012169, acc: 0.9911949634552002)
[2025-02-13 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:34][root][INFO] - Training Epoch: 2/2, step 2985/7134 completed (loss: 0.04103504866361618, acc: 0.9880810379981995)
[2025-02-13 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:35][root][INFO] - Training Epoch: 2/2, step 2986/7134 completed (loss: 0.017049936577677727, acc: 0.9976359605789185)
[2025-02-13 03:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:35][root][INFO] - Training Epoch: 2/2, step 2987/7134 completed (loss: 0.028467601165175438, acc: 0.9917550086975098)
[2025-02-13 03:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36][root][INFO] - Training Epoch: 2/2, step 2988/7134 completed (loss: 0.01845223270356655, acc: 0.9951338171958923)
[2025-02-13 03:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36][root][INFO] - Training Epoch: 2/2, step 2989/7134 completed (loss: 0.021937331184744835, acc: 0.9921568632125854)
[2025-02-13 03:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36][root][INFO] - Training Epoch: 2/2, step 2990/7134 completed (loss: 0.1307094395160675, acc: 0.9685314893722534)
[2025-02-13 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:37][root][INFO] - Training Epoch: 2/2, step 2991/7134 completed (loss: 0.1173236146569252, acc: 0.9667128920555115)
[2025-02-13 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:37][root][INFO] - Training Epoch: 2/2, step 2992/7134 completed (loss: 0.07055749744176865, acc: 0.9834087491035461)
[2025-02-13 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:38][root][INFO] - Training Epoch: 2/2, step 2993/7134 completed (loss: 0.033119603991508484, acc: 0.9926650524139404)
[2025-02-13 03:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:38][root][INFO] - Training Epoch: 2/2, step 2994/7134 completed (loss: 0.021565090864896774, acc: 0.9928057789802551)
[2025-02-13 03:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39][root][INFO] - Training Epoch: 2/2, step 2995/7134 completed (loss: 0.02151239477097988, acc: 0.9954802393913269)
[2025-02-13 03:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39][root][INFO] - Training Epoch: 2/2, step 2996/7134 completed (loss: 0.03561890870332718, acc: 0.988304078578949)
[2025-02-13 03:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39][root][INFO] - Training Epoch: 2/2, step 2997/7134 completed (loss: 0.0190605279058218, acc: 0.9937629699707031)
[2025-02-13 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:40][root][INFO] - Training Epoch: 2/2, step 2998/7134 completed (loss: 0.02267286740243435, acc: 0.996363639831543)
[2025-02-13 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:40][root][INFO] - Training Epoch: 2/2, step 2999/7134 completed (loss: 0.0178697407245636, acc: 0.9971056580543518)
[2025-02-13 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:41][root][INFO] - Training Epoch: 2/2, step 3000/7134 completed (loss: 0.026847124099731445, acc: 0.9914346933364868)
[2025-02-13 03:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:41][root][INFO] - Training Epoch: 2/2, step 3001/7134 completed (loss: 0.017823440954089165, acc: 0.9964454770088196)
[2025-02-13 03:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:42][root][INFO] - Training Epoch: 2/2, step 3002/7134 completed (loss: 0.027202846482396126, acc: 0.9886621236801147)
[2025-02-13 03:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:42][root][INFO] - Training Epoch: 2/2, step 3003/7134 completed (loss: 0.010195757262408733, acc: 0.9976798295974731)
[2025-02-13 03:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:43][root][INFO] - Training Epoch: 2/2, step 3004/7134 completed (loss: 0.007981113158166409, acc: 0.9977653622627258)
[2025-02-13 03:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:43][root][INFO] - Training Epoch: 2/2, step 3005/7134 completed (loss: 0.0076083410531282425, acc: 0.9988597631454468)
[2025-02-13 03:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44][root][INFO] - Training Epoch: 2/2, step 3006/7134 completed (loss: 0.010969932191073895, acc: 0.9936143159866333)
[2025-02-13 03:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44][root][INFO] - Training Epoch: 2/2, step 3007/7134 completed (loss: 0.02109352871775627, acc: 0.9929742217063904)
[2025-02-13 03:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44][root][INFO] - Training Epoch: 2/2, step 3008/7134 completed (loss: 0.03957575559616089, acc: 0.9874125719070435)
[2025-02-13 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:45][root][INFO] - Training Epoch: 2/2, step 3009/7134 completed (loss: 0.0749010220170021, acc: 0.9810126423835754)
[2025-02-13 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:45][root][INFO] - Training Epoch: 2/2, step 3010/7134 completed (loss: 0.04622160270810127, acc: 0.988950252532959)
[2025-02-13 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:46][root][INFO] - Training Epoch: 2/2, step 3011/7134 completed (loss: 0.07700272649526596, acc: 0.984308123588562)
[2025-02-13 03:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:46][root][INFO] - Training Epoch: 2/2, step 3012/7134 completed (loss: 0.05202885717153549, acc: 0.9873217344284058)
[2025-02-13 03:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47][root][INFO] - Training Epoch: 2/2, step 3013/7134 completed (loss: 0.010421322658658028, acc: 0.9976958632469177)
[2025-02-13 03:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47][root][INFO] - Training Epoch: 2/2, step 3014/7134 completed (loss: 0.03343726322054863, acc: 0.9894490242004395)
[2025-02-13 03:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47][root][INFO] - Training Epoch: 2/2, step 3015/7134 completed (loss: 0.029090825468301773, acc: 0.994490385055542)
[2025-02-13 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:48][root][INFO] - Training Epoch: 2/2, step 3016/7134 completed (loss: 0.014087604358792305, acc: 0.9943740963935852)
[2025-02-13 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:48][root][INFO] - Training Epoch: 2/2, step 3017/7134 completed (loss: 0.03431057184934616, acc: 0.9864406585693359)
[2025-02-13 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49][root][INFO] - Training Epoch: 2/2, step 3018/7134 completed (loss: 0.02952759526669979, acc: 0.9950413107872009)
[2025-02-13 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49][root][INFO] - Training Epoch: 2/2, step 3019/7134 completed (loss: 0.03544071316719055, acc: 0.9901960492134094)
[2025-02-13 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49][root][INFO] - Training Epoch: 2/2, step 3020/7134 completed (loss: 0.038818359375, acc: 0.9910873174667358)
[2025-02-13 03:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:50][root][INFO] - Training Epoch: 2/2, step 3021/7134 completed (loss: 0.016505347564816475, acc: 0.9945454597473145)
[2025-02-13 03:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:50][root][INFO] - Training Epoch: 2/2, step 3022/7134 completed (loss: 0.032026540488004684, acc: 0.9951768517494202)
[2025-02-13 03:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51][root][INFO] - Training Epoch: 2/2, step 3023/7134 completed (loss: 0.030574914067983627, acc: 0.9900596141815186)
[2025-02-13 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51][root][INFO] - Training Epoch: 2/2, step 3024/7134 completed (loss: 0.01186318602412939, acc: 0.9961685538291931)
[2025-02-13 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51][root][INFO] - Training Epoch: 2/2, step 3025/7134 completed (loss: 0.07092615962028503, acc: 0.9844827651977539)
[2025-02-13 03:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:52][root][INFO] - Training Epoch: 2/2, step 3026/7134 completed (loss: 0.030377892777323723, acc: 0.9928186535835266)
[2025-02-13 03:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:52][root][INFO] - Training Epoch: 2/2, step 3027/7134 completed (loss: 0.06091039255261421, acc: 0.9878787994384766)
[2025-02-13 03:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53][root][INFO] - Training Epoch: 2/2, step 3028/7134 completed (loss: 0.01288228016346693, acc: 0.9954648613929749)
[2025-02-13 03:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53][root][INFO] - Training Epoch: 2/2, step 3029/7134 completed (loss: 0.04326140508055687, acc: 0.9938176274299622)
[2025-02-13 03:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53][root][INFO] - Training Epoch: 2/2, step 3030/7134 completed (loss: 0.023229902610182762, acc: 0.9923664331436157)
[2025-02-13 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:54][root][INFO] - Training Epoch: 2/2, step 3031/7134 completed (loss: 0.014119314029812813, acc: 0.9936440587043762)
[2025-02-13 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:54][root][INFO] - Training Epoch: 2/2, step 3032/7134 completed (loss: 0.047842442989349365, acc: 0.9891107082366943)
[2025-02-13 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55][root][INFO] - Training Epoch: 2/2, step 3033/7134 completed (loss: 0.035981737077236176, acc: 0.9876543283462524)
[2025-02-13 03:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55][root][INFO] - Training Epoch: 2/2, step 3034/7134 completed (loss: 0.03591550886631012, acc: 0.9872340559959412)
[2025-02-13 03:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55][root][INFO] - Training Epoch: 2/2, step 3035/7134 completed (loss: 0.0107407933101058, acc: 0.995275616645813)
[2025-02-13 03:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:56][root][INFO] - Training Epoch: 2/2, step 3036/7134 completed (loss: 0.0526517778635025, acc: 0.9882943034172058)
[2025-02-13 03:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:56][root][INFO] - Training Epoch: 2/2, step 3037/7134 completed (loss: 0.020328443497419357, acc: 0.9938367009162903)
[2025-02-13 03:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:57][root][INFO] - Training Epoch: 2/2, step 3038/7134 completed (loss: 0.011786513961851597, acc: 0.9965217113494873)
[2025-02-13 03:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:57][root][INFO] - Training Epoch: 2/2, step 3039/7134 completed (loss: 0.04352465644478798, acc: 0.9938744306564331)
[2025-02-13 03:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58][root][INFO] - Training Epoch: 2/2, step 3040/7134 completed (loss: 0.022637663409113884, acc: 0.994434118270874)
[2025-02-13 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58][root][INFO] - Training Epoch: 2/2, step 3041/7134 completed (loss: 0.054045312106609344, acc: 0.9904305934906006)
[2025-02-13 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58][root][INFO] - Training Epoch: 2/2, step 3042/7134 completed (loss: 0.022538956254720688, acc: 0.9940915703773499)
[2025-02-13 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59][root][INFO] - Training Epoch: 2/2, step 3043/7134 completed (loss: 0.011856669560074806, acc: 0.9942965507507324)
[2025-02-13 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59][root][INFO] - Training Epoch: 2/2, step 3044/7134 completed (loss: 0.02108670026063919, acc: 0.9948186278343201)
[2025-02-13 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59][root][INFO] - Training Epoch: 2/2, step 3045/7134 completed (loss: 0.017894383519887924, acc: 0.994915246963501)
[2025-02-13 03:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:00][root][INFO] - Training Epoch: 2/2, step 3046/7134 completed (loss: 0.03262200206518173, acc: 0.9904371500015259)
[2025-02-13 03:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:00][root][INFO] - Training Epoch: 2/2, step 3047/7134 completed (loss: 0.007371032610535622, acc: 1.0)
[2025-02-13 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:01][root][INFO] - Training Epoch: 2/2, step 3048/7134 completed (loss: 0.018128646537661552, acc: 0.99370276927948)
[2025-02-13 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:01][root][INFO] - Training Epoch: 2/2, step 3049/7134 completed (loss: 0.013097308576107025, acc: 0.9941860437393188)
[2025-02-13 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:02][root][INFO] - Training Epoch: 2/2, step 3050/7134 completed (loss: 0.023810621351003647, acc: 0.9904988408088684)
[2025-02-13 03:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:02][root][INFO] - Training Epoch: 2/2, step 3051/7134 completed (loss: 0.03135582432150841, acc: 0.9864661693572998)
[2025-02-13 03:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03][root][INFO] - Training Epoch: 2/2, step 3052/7134 completed (loss: 0.022316666319966316, acc: 0.9909793734550476)
[2025-02-13 03:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03][root][INFO] - Training Epoch: 2/2, step 3053/7134 completed (loss: 0.014774019829928875, acc: 0.9927536249160767)
[2025-02-13 03:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03][root][INFO] - Training Epoch: 2/2, step 3054/7134 completed (loss: 0.0442328155040741, acc: 0.984415590763092)
[2025-02-13 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:04][root][INFO] - Training Epoch: 2/2, step 3055/7134 completed (loss: 0.02797602489590645, acc: 0.988034188747406)
[2025-02-13 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:04][root][INFO] - Training Epoch: 2/2, step 3056/7134 completed (loss: 0.017312146723270416, acc: 0.9948586225509644)
[2025-02-13 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05][root][INFO] - Training Epoch: 2/2, step 3057/7134 completed (loss: 0.016902290284633636, acc: 0.994452178478241)
[2025-02-13 03:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05][root][INFO] - Training Epoch: 2/2, step 3058/7134 completed (loss: 0.04497203975915909, acc: 0.9909326434135437)
[2025-02-13 03:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06][root][INFO] - Training Epoch: 2/2, step 3059/7134 completed (loss: 0.013497957028448582, acc: 0.9917808175086975)
[2025-02-13 03:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06][root][INFO] - Training Epoch: 2/2, step 3060/7134 completed (loss: 0.023644352331757545, acc: 0.9928401112556458)
[2025-02-13 03:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06][root][INFO] - Training Epoch: 2/2, step 3061/7134 completed (loss: 0.013869691640138626, acc: 0.9924242496490479)
[2025-02-13 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:07][root][INFO] - Training Epoch: 2/2, step 3062/7134 completed (loss: 0.00828255619853735, acc: 0.9986522793769836)
[2025-02-13 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:07][root][INFO] - Training Epoch: 2/2, step 3063/7134 completed (loss: 0.0240060705691576, acc: 0.9922978281974792)
[2025-02-13 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:08][root][INFO] - Training Epoch: 2/2, step 3064/7134 completed (loss: 0.01156655140221119, acc: 0.997183084487915)
[2025-02-13 03:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:08][root][INFO] - Training Epoch: 2/2, step 3065/7134 completed (loss: 0.01494472287595272, acc: 0.9959677457809448)
[2025-02-13 03:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09][root][INFO] - Training Epoch: 2/2, step 3066/7134 completed (loss: 0.007223072461783886, acc: 0.9983870983123779)
[2025-02-13 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09][root][INFO] - Training Epoch: 2/2, step 3067/7134 completed (loss: 0.012146511115133762, acc: 0.9981818199157715)
[2025-02-13 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09][root][INFO] - Training Epoch: 2/2, step 3068/7134 completed (loss: 0.019603000953793526, acc: 0.9932340979576111)
[2025-02-13 03:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:10][root][INFO] - Training Epoch: 2/2, step 3069/7134 completed (loss: 0.014887544326484203, acc: 0.9976958632469177)
[2025-02-13 03:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:10][root][INFO] - Training Epoch: 2/2, step 3070/7134 completed (loss: 0.011514388024806976, acc: 0.9968186616897583)
[2025-02-13 03:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:11][root][INFO] - Training Epoch: 2/2, step 3071/7134 completed (loss: 0.019550403580069542, acc: 0.9948520064353943)
[2025-02-13 03:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:11][root][INFO] - Training Epoch: 2/2, step 3072/7134 completed (loss: 0.019520055502653122, acc: 0.9953325390815735)
[2025-02-13 03:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12][root][INFO] - Training Epoch: 2/2, step 3073/7134 completed (loss: 0.008787238039076328, acc: 0.9974554777145386)
[2025-02-13 03:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12][root][INFO] - Training Epoch: 2/2, step 3074/7134 completed (loss: 0.0179803054779768, acc: 0.9933444261550903)
[2025-02-13 03:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12][root][INFO] - Training Epoch: 2/2, step 3075/7134 completed (loss: 0.014178171753883362, acc: 0.9958333373069763)
[2025-02-13 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:13][root][INFO] - Training Epoch: 2/2, step 3076/7134 completed (loss: 0.025766482576727867, acc: 0.9922308325767517)
[2025-02-13 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:13][root][INFO] - Training Epoch: 2/2, step 3077/7134 completed (loss: 0.04865088686347008, acc: 0.9846994280815125)
[2025-02-13 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:14][root][INFO] - Training Epoch: 2/2, step 3078/7134 completed (loss: 0.04451783373951912, acc: 0.9890710115432739)
[2025-02-13 03:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:14][root][INFO] - Training Epoch: 2/2, step 3079/7134 completed (loss: 0.02523191086947918, acc: 0.990123450756073)
[2025-02-13 03:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15][root][INFO] - Training Epoch: 2/2, step 3080/7134 completed (loss: 0.05202804505825043, acc: 0.9858233332633972)
[2025-02-13 03:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15][root][INFO] - Training Epoch: 2/2, step 3081/7134 completed (loss: 0.030831338837742805, acc: 0.993534505367279)
[2025-02-13 03:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15][root][INFO] - Training Epoch: 2/2, step 3082/7134 completed (loss: 0.05727753788232803, acc: 0.9807460904121399)
[2025-02-13 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:16][root][INFO] - Training Epoch: 2/2, step 3083/7134 completed (loss: 0.039049603044986725, acc: 0.9882199168205261)
[2025-02-13 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:16][root][INFO] - Training Epoch: 2/2, step 3084/7134 completed (loss: 0.04320007562637329, acc: 0.988095223903656)
[2025-02-13 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:17][root][INFO] - Training Epoch: 2/2, step 3085/7134 completed (loss: 0.05526815354824066, acc: 0.9868612885475159)
[2025-02-13 03:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:17][root][INFO] - Training Epoch: 2/2, step 3086/7134 completed (loss: 0.044528912752866745, acc: 0.9831649661064148)
[2025-02-13 03:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:18][root][INFO] - Training Epoch: 2/2, step 3087/7134 completed (loss: 0.035633936524391174, acc: 0.9899569749832153)
[2025-02-13 03:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:18][root][INFO] - Training Epoch: 2/2, step 3088/7134 completed (loss: 0.027909157797694206, acc: 0.990980863571167)
[2025-02-13 03:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:19][root][INFO] - Training Epoch: 2/2, step 3089/7134 completed (loss: 0.03370732441544533, acc: 0.9902912378311157)
[2025-02-13 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:19][root][INFO] - Training Epoch: 2/2, step 3090/7134 completed (loss: 0.06943845748901367, acc: 0.9823369383811951)
[2025-02-13 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20][root][INFO] - Training Epoch: 2/2, step 3091/7134 completed (loss: 0.04083295911550522, acc: 0.9907833933830261)
[2025-02-13 03:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20][root][INFO] - Training Epoch: 2/2, step 3092/7134 completed (loss: 0.044149719178676605, acc: 0.9833641648292542)
[2025-02-13 03:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20][root][INFO] - Training Epoch: 2/2, step 3093/7134 completed (loss: 0.03482304885983467, acc: 0.9900662302970886)
[2025-02-13 03:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:21][root][INFO] - Training Epoch: 2/2, step 3094/7134 completed (loss: 0.013951904140412807, acc: 0.9959127902984619)
[2025-02-13 03:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:21][root][INFO] - Training Epoch: 2/2, step 3095/7134 completed (loss: 0.04036787524819374, acc: 0.987500011920929)
[2025-02-13 03:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22][root][INFO] - Training Epoch: 2/2, step 3096/7134 completed (loss: 0.061724260449409485, acc: 0.9820282459259033)
[2025-02-13 03:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22][root][INFO] - Training Epoch: 2/2, step 3097/7134 completed (loss: 0.06041121482849121, acc: 0.9871794581413269)
[2025-02-13 03:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23][root][INFO] - Training Epoch: 2/2, step 3098/7134 completed (loss: 0.021642742678523064, acc: 0.9952830076217651)
[2025-02-13 03:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23][root][INFO] - Training Epoch: 2/2, step 3099/7134 completed (loss: 0.017175989225506783, acc: 0.9935170412063599)
[2025-02-13 03:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23][root][INFO] - Training Epoch: 2/2, step 3100/7134 completed (loss: 0.025600112974643707, acc: 0.9936169981956482)
[2025-02-13 03:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:24][root][INFO] - Training Epoch: 2/2, step 3101/7134 completed (loss: 0.02320639044046402, acc: 0.9927007555961609)
[2025-02-13 03:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:24][root][INFO] - Training Epoch: 2/2, step 3102/7134 completed (loss: 0.025651998817920685, acc: 0.9881481528282166)
[2025-02-13 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:25][root][INFO] - Training Epoch: 2/2, step 3103/7134 completed (loss: 0.031084878370165825, acc: 0.9904153347015381)
[2025-02-13 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:25][root][INFO] - Training Epoch: 2/2, step 3104/7134 completed (loss: 0.013372519053518772, acc: 0.9961832165718079)
[2025-02-13 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26][root][INFO] - Training Epoch: 2/2, step 3105/7134 completed (loss: 0.03591462969779968, acc: 0.9890109896659851)
[2025-02-13 03:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26][root][INFO] - Training Epoch: 2/2, step 3106/7134 completed (loss: 0.017616134136915207, acc: 0.9933110475540161)
[2025-02-13 03:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26][root][INFO] - Training Epoch: 2/2, step 3107/7134 completed (loss: 0.010680577717721462, acc: 0.9960784316062927)
[2025-02-13 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:27][root][INFO] - Training Epoch: 2/2, step 3108/7134 completed (loss: 0.03382158279418945, acc: 0.9938461780548096)
[2025-02-13 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:27][root][INFO] - Training Epoch: 2/2, step 3109/7134 completed (loss: 0.02083565853536129, acc: 0.9944649338722229)
[2025-02-13 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:28][root][INFO] - Training Epoch: 2/2, step 3110/7134 completed (loss: 0.02462870255112648, acc: 0.992682933807373)
[2025-02-13 03:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:28][root][INFO] - Training Epoch: 2/2, step 3111/7134 completed (loss: 0.11424875259399414, acc: 0.9707903861999512)
[2025-02-13 03:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29][root][INFO] - Training Epoch: 2/2, step 3112/7134 completed (loss: 0.03693506494164467, acc: 0.9947368502616882)
[2025-02-13 03:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29][root][INFO] - Training Epoch: 2/2, step 3113/7134 completed (loss: 0.03088383376598358, acc: 0.9901685118675232)
[2025-02-13 03:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29][root][INFO] - Training Epoch: 2/2, step 3114/7134 completed (loss: 0.02271851897239685, acc: 0.9948275685310364)
[2025-02-13 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:30][root][INFO] - Training Epoch: 2/2, step 3115/7134 completed (loss: 0.05987914279103279, acc: 0.9845678806304932)
[2025-02-13 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:30][root][INFO] - Training Epoch: 2/2, step 3116/7134 completed (loss: 0.022281261160969734, acc: 0.9923076629638672)
[2025-02-13 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:31][root][INFO] - Training Epoch: 2/2, step 3117/7134 completed (loss: 0.0780307799577713, acc: 0.9850543737411499)
[2025-02-13 03:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:31][root][INFO] - Training Epoch: 2/2, step 3118/7134 completed (loss: 0.03685185685753822, acc: 0.990777313709259)
[2025-02-13 03:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32][root][INFO] - Training Epoch: 2/2, step 3119/7134 completed (loss: 0.04683937504887581, acc: 0.9844357967376709)
[2025-02-13 03:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32][root][INFO] - Training Epoch: 2/2, step 3120/7134 completed (loss: 0.04970540478825569, acc: 0.9929478168487549)
[2025-02-13 03:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32][root][INFO] - Training Epoch: 2/2, step 3121/7134 completed (loss: 0.03095744550228119, acc: 0.9897172451019287)
[2025-02-13 03:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:33][root][INFO] - Training Epoch: 2/2, step 3122/7134 completed (loss: 0.03504142537713051, acc: 0.989534854888916)
[2025-02-13 03:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:33][root][INFO] - Training Epoch: 2/2, step 3123/7134 completed (loss: 0.05471113696694374, acc: 0.9825870394706726)
[2025-02-13 03:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:34][root][INFO] - Training Epoch: 2/2, step 3124/7134 completed (loss: 0.03660361468791962, acc: 0.9938119053840637)
[2025-02-13 03:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:34][root][INFO] - Training Epoch: 2/2, step 3125/7134 completed (loss: 0.01822550967335701, acc: 0.9939485788345337)
[2025-02-13 03:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35][root][INFO] - Training Epoch: 2/2, step 3126/7134 completed (loss: 0.029209677129983902, acc: 0.9936467409133911)
[2025-02-13 03:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35][root][INFO] - Training Epoch: 2/2, step 3127/7134 completed (loss: 0.026534391567111015, acc: 0.9906976819038391)
[2025-02-13 03:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35][root][INFO] - Training Epoch: 2/2, step 3128/7134 completed (loss: 0.028803858906030655, acc: 0.9922580718994141)
[2025-02-13 03:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:36][root][INFO] - Training Epoch: 2/2, step 3129/7134 completed (loss: 0.025012889876961708, acc: 0.9913473129272461)
[2025-02-13 03:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:36][root][INFO] - Training Epoch: 2/2, step 3130/7134 completed (loss: 0.02609732374548912, acc: 0.9896789193153381)
[2025-02-13 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:37][root][INFO] - Training Epoch: 2/2, step 3131/7134 completed (loss: 0.04172113537788391, acc: 0.9884058237075806)
[2025-02-13 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:37][root][INFO] - Training Epoch: 2/2, step 3132/7134 completed (loss: 0.008237313479185104, acc: 1.0)
[2025-02-13 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38][root][INFO] - Training Epoch: 2/2, step 3133/7134 completed (loss: 0.10355160385370255, acc: 0.9821428656578064)
[2025-02-13 03:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38][root][INFO] - Training Epoch: 2/2, step 3134/7134 completed (loss: 0.028976377099752426, acc: 0.9908424615859985)
[2025-02-13 03:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38][root][INFO] - Training Epoch: 2/2, step 3135/7134 completed (loss: 0.04801027849316597, acc: 0.986143171787262)
[2025-02-13 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:39][root][INFO] - Training Epoch: 2/2, step 3136/7134 completed (loss: 0.015207231044769287, acc: 0.9982608556747437)
[2025-02-13 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:39][root][INFO] - Training Epoch: 2/2, step 3137/7134 completed (loss: 0.03695995360612869, acc: 0.9841269850730896)
[2025-02-13 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40][root][INFO] - Training Epoch: 2/2, step 3138/7134 completed (loss: 0.04259837418794632, acc: 0.9852941036224365)
[2025-02-13 03:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40][root][INFO] - Training Epoch: 2/2, step 3139/7134 completed (loss: 0.06324466317892075, acc: 0.9785407781600952)
[2025-02-13 03:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40][root][INFO] - Training Epoch: 2/2, step 3140/7134 completed (loss: 0.024992676451802254, acc: 0.9922178983688354)
[2025-02-13 03:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41][root][INFO] - Training Epoch: 2/2, step 3141/7134 completed (loss: 0.048539336770772934, acc: 0.9842657446861267)
[2025-02-13 03:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41][root][INFO] - Training Epoch: 2/2, step 3142/7134 completed (loss: 0.013066195882856846, acc: 0.9970104694366455)
[2025-02-13 03:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42][root][INFO] - Training Epoch: 2/2, step 3143/7134 completed (loss: 0.015974482521414757, acc: 0.9967845678329468)
[2025-02-13 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42][root][INFO] - Training Epoch: 2/2, step 3144/7134 completed (loss: 0.015204151161015034, acc: 0.9937106966972351)
[2025-02-13 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42][root][INFO] - Training Epoch: 2/2, step 3145/7134 completed (loss: 0.04168800264596939, acc: 0.9912472367286682)
[2025-02-13 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:43][root][INFO] - Training Epoch: 2/2, step 3146/7134 completed (loss: 0.019717441871762276, acc: 0.98828125)
[2025-02-13 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:43][root][INFO] - Training Epoch: 2/2, step 3147/7134 completed (loss: 0.03451092913746834, acc: 0.9935794472694397)
[2025-02-13 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:44][root][INFO] - Training Epoch: 2/2, step 3148/7134 completed (loss: 0.027631834149360657, acc: 0.9923469424247742)
[2025-02-13 03:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:44][root][INFO] - Training Epoch: 2/2, step 3149/7134 completed (loss: 0.00724905775859952, acc: 0.9985358715057373)
[2025-02-13 03:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45][root][INFO] - Training Epoch: 2/2, step 3150/7134 completed (loss: 0.1433446854352951, acc: 0.9704142212867737)
[2025-02-13 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45][root][INFO] - Training Epoch: 2/2, step 3151/7134 completed (loss: 0.05782970041036606, acc: 0.984674334526062)
[2025-02-13 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45][root][INFO] - Training Epoch: 2/2, step 3152/7134 completed (loss: 0.03464768826961517, acc: 0.9863636493682861)
[2025-02-13 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:46][root][INFO] - Training Epoch: 2/2, step 3153/7134 completed (loss: 0.042916085571050644, acc: 0.9862155318260193)
[2025-02-13 03:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:46][root][INFO] - Training Epoch: 2/2, step 3154/7134 completed (loss: 0.03171562775969505, acc: 0.9928951859474182)
[2025-02-13 03:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47][root][INFO] - Training Epoch: 2/2, step 3155/7134 completed (loss: 0.0854613184928894, acc: 0.9756592512130737)
[2025-02-13 03:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47][root][INFO] - Training Epoch: 2/2, step 3156/7134 completed (loss: 0.0792541354894638, acc: 0.9762258529663086)
[2025-02-13 03:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47][root][INFO] - Training Epoch: 2/2, step 3157/7134 completed (loss: 0.014094115234911442, acc: 0.9935483932495117)
[2025-02-13 03:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:48][root][INFO] - Training Epoch: 2/2, step 3158/7134 completed (loss: 0.019618619233369827, acc: 0.9950980544090271)
[2025-02-13 03:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:48][root][INFO] - Training Epoch: 2/2, step 3159/7134 completed (loss: 0.0593591034412384, acc: 0.9828431606292725)
[2025-02-13 03:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49][root][INFO] - Training Epoch: 2/2, step 3160/7134 completed (loss: 0.009899163618683815, acc: 0.9972337484359741)
[2025-02-13 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49][root][INFO] - Training Epoch: 2/2, step 3161/7134 completed (loss: 0.027693601325154305, acc: 0.9915966391563416)
[2025-02-13 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49][root][INFO] - Training Epoch: 2/2, step 3162/7134 completed (loss: 0.013830196112394333, acc: 0.9927113652229309)
[2025-02-13 03:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:50][root][INFO] - Training Epoch: 2/2, step 3163/7134 completed (loss: 0.03633471578359604, acc: 0.9883138537406921)
[2025-02-13 03:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:50][root][INFO] - Training Epoch: 2/2, step 3164/7134 completed (loss: 0.046468764543533325, acc: 0.9882746934890747)
[2025-02-13 03:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51][root][INFO] - Training Epoch: 2/2, step 3165/7134 completed (loss: 0.02235475741326809, acc: 0.9933221936225891)
[2025-02-13 03:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51][root][INFO] - Training Epoch: 2/2, step 3166/7134 completed (loss: 0.05195748433470726, acc: 0.9879699349403381)
[2025-02-13 03:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51][root][INFO] - Training Epoch: 2/2, step 3167/7134 completed (loss: 0.09413153678178787, acc: 0.965624988079071)
[2025-02-13 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:52][root][INFO] - Training Epoch: 2/2, step 3168/7134 completed (loss: 0.07100063562393188, acc: 0.9771341681480408)
[2025-02-13 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:52][root][INFO] - Training Epoch: 2/2, step 3169/7134 completed (loss: 0.05930723994970322, acc: 0.9856557250022888)
[2025-02-13 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:53][root][INFO] - Training Epoch: 2/2, step 3170/7134 completed (loss: 0.03388535976409912, acc: 0.9818181991577148)
[2025-02-13 03:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:53][root][INFO] - Training Epoch: 2/2, step 3171/7134 completed (loss: 0.034909892827272415, acc: 0.9868938326835632)
[2025-02-13 03:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54][root][INFO] - Training Epoch: 2/2, step 3172/7134 completed (loss: 0.009996877983212471, acc: 0.9961685538291931)
[2025-02-13 03:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54][root][INFO] - Training Epoch: 2/2, step 3173/7134 completed (loss: 0.03754107654094696, acc: 0.9872449040412903)
[2025-02-13 03:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54][root][INFO] - Training Epoch: 2/2, step 3174/7134 completed (loss: 0.07228699326515198, acc: 0.976331353187561)
[2025-02-13 03:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:55][root][INFO] - Training Epoch: 2/2, step 3175/7134 completed (loss: 0.045856837183237076, acc: 0.98591548204422)
[2025-02-13 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:55][root][INFO] - Training Epoch: 2/2, step 3176/7134 completed (loss: 0.07945328950881958, acc: 0.9789473414421082)
[2025-02-13 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56][root][INFO] - Training Epoch: 2/2, step 3177/7134 completed (loss: 0.07085196673870087, acc: 0.9861111044883728)
[2025-02-13 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56][root][INFO] - Training Epoch: 2/2, step 3178/7134 completed (loss: 0.024945499375462532, acc: 0.9950739145278931)
[2025-02-13 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56][root][INFO] - Training Epoch: 2/2, step 3179/7134 completed (loss: 0.08584201335906982, acc: 0.9800994992256165)
[2025-02-13 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:57][root][INFO] - Training Epoch: 2/2, step 3180/7134 completed (loss: 0.02699870988726616, acc: 0.9957143068313599)
[2025-02-13 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:57][root][INFO] - Training Epoch: 2/2, step 3181/7134 completed (loss: 0.05612959340214729, acc: 0.9890795350074768)
[2025-02-13 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58][root][INFO] - Training Epoch: 2/2, step 3182/7134 completed (loss: 0.03791934996843338, acc: 0.9940387606620789)
[2025-02-13 03:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58][root][INFO] - Training Epoch: 2/2, step 3183/7134 completed (loss: 0.05042510852217674, acc: 0.9881556630134583)
[2025-02-13 03:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58][root][INFO] - Training Epoch: 2/2, step 3184/7134 completed (loss: 0.047459714114665985, acc: 0.988095223903656)
[2025-02-13 03:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:59][root][INFO] - Training Epoch: 2/2, step 3185/7134 completed (loss: 0.012914983555674553, acc: 0.995726466178894)
[2025-02-13 03:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:59][root][INFO] - Training Epoch: 2/2, step 3186/7134 completed (loss: 0.014941425062716007, acc: 0.9970370531082153)
[2025-02-13 03:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00][root][INFO] - Training Epoch: 2/2, step 3187/7134 completed (loss: 0.03740467131137848, acc: 0.9841772317886353)
[2025-02-13 03:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00][root][INFO] - Training Epoch: 2/2, step 3188/7134 completed (loss: 0.029350070282816887, acc: 0.9935317039489746)
[2025-02-13 03:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00][root][INFO] - Training Epoch: 2/2, step 3189/7134 completed (loss: 0.04847221449017525, acc: 0.9826338887214661)
[2025-02-13 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:01][root][INFO] - Training Epoch: 2/2, step 3190/7134 completed (loss: 0.04198652505874634, acc: 0.9898734092712402)
[2025-02-13 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:01][root][INFO] - Training Epoch: 2/2, step 3191/7134 completed (loss: 0.02860494703054428, acc: 0.9885714054107666)
[2025-02-13 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02][root][INFO] - Training Epoch: 2/2, step 3192/7134 completed (loss: 0.04391998425126076, acc: 0.9886578321456909)
[2025-02-13 03:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02][root][INFO] - Training Epoch: 2/2, step 3193/7134 completed (loss: 0.06243591383099556, acc: 0.980966329574585)
[2025-02-13 03:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02][root][INFO] - Training Epoch: 2/2, step 3194/7134 completed (loss: 0.040738802403211594, acc: 0.983660101890564)
[2025-02-13 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:03][root][INFO] - Training Epoch: 2/2, step 3195/7134 completed (loss: 0.032646920531988144, acc: 0.9931389093399048)
[2025-02-13 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:03][root][INFO] - Training Epoch: 2/2, step 3196/7134 completed (loss: 0.024313434958457947, acc: 0.9887850284576416)
[2025-02-13 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04][root][INFO] - Training Epoch: 2/2, step 3197/7134 completed (loss: 0.026047037914395332, acc: 0.997183084487915)
[2025-02-13 03:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04][root][INFO] - Training Epoch: 2/2, step 3198/7134 completed (loss: 0.029358671978116035, acc: 0.9909228682518005)
[2025-02-13 03:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04][root][INFO] - Training Epoch: 2/2, step 3199/7134 completed (loss: 0.029211916029453278, acc: 0.9941262602806091)
[2025-02-13 03:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:05][root][INFO] - Training Epoch: 2/2, step 3200/7134 completed (loss: 0.0365881510078907, acc: 0.9876977205276489)
[2025-02-13 03:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:05][root][INFO] - Training Epoch: 2/2, step 3201/7134 completed (loss: 0.09666988998651505, acc: 0.9804772138595581)
[2025-02-13 03:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06][root][INFO] - Training Epoch: 2/2, step 3202/7134 completed (loss: 0.025248169898986816, acc: 0.9944674968719482)
[2025-02-13 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06][root][INFO] - Training Epoch: 2/2, step 3203/7134 completed (loss: 0.028630303218960762, acc: 0.9921011328697205)
[2025-02-13 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06][root][INFO] - Training Epoch: 2/2, step 3204/7134 completed (loss: 0.06846585124731064, acc: 0.9774436354637146)
[2025-02-13 03:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:07][root][INFO] - Training Epoch: 2/2, step 3205/7134 completed (loss: 0.006645070388913155, acc: 1.0)
[2025-02-13 03:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:07][root][INFO] - Training Epoch: 2/2, step 3206/7134 completed (loss: 0.04119123890995979, acc: 0.9913169145584106)
[2025-02-13 03:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08][root][INFO] - Training Epoch: 2/2, step 3207/7134 completed (loss: 0.03217093273997307, acc: 0.9877488613128662)
[2025-02-13 03:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08][root][INFO] - Training Epoch: 2/2, step 3208/7134 completed (loss: 0.07659820467233658, acc: 0.9852941036224365)
[2025-02-13 03:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08][root][INFO] - Training Epoch: 2/2, step 3209/7134 completed (loss: 0.013517740182578564, acc: 0.9940476417541504)
[2025-02-13 03:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09][root][INFO] - Training Epoch: 2/2, step 3210/7134 completed (loss: 0.009046723134815693, acc: 0.9984939694404602)
[2025-02-13 03:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09][root][INFO] - Training Epoch: 2/2, step 3211/7134 completed (loss: 0.047378990799188614, acc: 0.9916840195655823)
[2025-02-13 03:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:10][root][INFO] - Training Epoch: 2/2, step 3212/7134 completed (loss: 0.006611322518438101, acc: 0.9986013770103455)
[2025-02-13 03:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:10][root][INFO] - Training Epoch: 2/2, step 3213/7134 completed (loss: 0.022864840924739838, acc: 0.9942611455917358)
[2025-02-13 03:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11][root][INFO] - Training Epoch: 2/2, step 3214/7134 completed (loss: 0.02135283127427101, acc: 0.993306577205658)
[2025-02-13 03:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11][root][INFO] - Training Epoch: 2/2, step 3215/7134 completed (loss: 0.03903365135192871, acc: 0.9859943985939026)
[2025-02-13 03:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11][root][INFO] - Training Epoch: 2/2, step 3216/7134 completed (loss: 0.0190272219479084, acc: 0.9957020282745361)
[2025-02-13 03:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12][root][INFO] - Training Epoch: 2/2, step 3217/7134 completed (loss: 0.010844088159501553, acc: 0.9987096786499023)
[2025-02-13 03:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12][root][INFO] - Training Epoch: 2/2, step 3218/7134 completed (loss: 0.046188171952962875, acc: 0.9937402009963989)
[2025-02-13 03:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:13][root][INFO] - Training Epoch: 2/2, step 3219/7134 completed (loss: 0.044129278510808945, acc: 0.9867060780525208)
[2025-02-13 03:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:13][root][INFO] - Training Epoch: 2/2, step 3220/7134 completed (loss: 0.038866691291332245, acc: 0.9906542301177979)
[2025-02-13 03:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14][root][INFO] - Training Epoch: 2/2, step 3221/7134 completed (loss: 0.0315907783806324, acc: 0.9876712560653687)
[2025-02-13 03:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14][root][INFO] - Training Epoch: 2/2, step 3222/7134 completed (loss: 0.050339844077825546, acc: 0.9887359142303467)
[2025-02-13 03:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14][root][INFO] - Training Epoch: 2/2, step 3223/7134 completed (loss: 0.0576857291162014, acc: 0.9888888597488403)
[2025-02-13 03:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15][root][INFO] - Training Epoch: 2/2, step 3224/7134 completed (loss: 0.052869342267513275, acc: 0.9913169145584106)
[2025-02-13 03:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15][root][INFO] - Training Epoch: 2/2, step 3225/7134 completed (loss: 0.03403790295124054, acc: 0.9898862242698669)
[2025-02-13 03:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:16][root][INFO] - Training Epoch: 2/2, step 3226/7134 completed (loss: 0.034310221672058105, acc: 0.985637366771698)
[2025-02-13 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:16][root][INFO] - Training Epoch: 2/2, step 3227/7134 completed (loss: 0.05802697688341141, acc: 0.9868131875991821)
[2025-02-13 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17][root][INFO] - Training Epoch: 2/2, step 3228/7134 completed (loss: 0.027438418939709663, acc: 0.9908015727996826)
[2025-02-13 03:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17][root][INFO] - Training Epoch: 2/2, step 3229/7134 completed (loss: 0.03307868912816048, acc: 0.9889841079711914)
[2025-02-13 03:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17][root][INFO] - Training Epoch: 2/2, step 3230/7134 completed (loss: 0.041512381285429, acc: 0.9837278127670288)
[2025-02-13 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:18][root][INFO] - Training Epoch: 2/2, step 3231/7134 completed (loss: 0.03900131583213806, acc: 0.9924012422561646)
[2025-02-13 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:18][root][INFO] - Training Epoch: 2/2, step 3232/7134 completed (loss: 0.01392741221934557, acc: 0.9932659864425659)
[2025-02-13 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19][root][INFO] - Training Epoch: 2/2, step 3233/7134 completed (loss: 0.04856037348508835, acc: 0.990138053894043)
[2025-02-13 03:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19][root][INFO] - Training Epoch: 2/2, step 3234/7134 completed (loss: 0.02401299960911274, acc: 0.9952606558799744)
[2025-02-13 03:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19][root][INFO] - Training Epoch: 2/2, step 3235/7134 completed (loss: 0.029331235215067863, acc: 0.9866666793823242)
[2025-02-13 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:20][root][INFO] - Training Epoch: 2/2, step 3236/7134 completed (loss: 0.0672936961054802, acc: 0.9852724671363831)
[2025-02-13 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:20][root][INFO] - Training Epoch: 2/2, step 3237/7134 completed (loss: 0.020573079586029053, acc: 0.9935587644577026)
[2025-02-13 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21][root][INFO] - Training Epoch: 2/2, step 3238/7134 completed (loss: 0.03757518529891968, acc: 0.9907975196838379)
[2025-02-13 03:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21][root][INFO] - Training Epoch: 2/2, step 3239/7134 completed (loss: 0.013619642704725266, acc: 0.9936908483505249)
[2025-02-13 03:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21][root][INFO] - Training Epoch: 2/2, step 3240/7134 completed (loss: 0.03289701044559479, acc: 0.9910314083099365)
[2025-02-13 03:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22][root][INFO] - Training Epoch: 2/2, step 3241/7134 completed (loss: 0.015667449682950974, acc: 0.9940652847290039)
[2025-02-13 03:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22][root][INFO] - Training Epoch: 2/2, step 3242/7134 completed (loss: 0.0326707549393177, acc: 0.991830050945282)
[2025-02-13 03:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23][root][INFO] - Training Epoch: 2/2, step 3243/7134 completed (loss: 0.02321634441614151, acc: 0.9959127902984619)
[2025-02-13 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23][root][INFO] - Training Epoch: 2/2, step 3244/7134 completed (loss: 0.011199596337974072, acc: 0.9985652565956116)
[2025-02-13 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23][root][INFO] - Training Epoch: 2/2, step 3245/7134 completed (loss: 0.024186719208955765, acc: 0.9961340427398682)
[2025-02-13 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:24][root][INFO] - Training Epoch: 2/2, step 3246/7134 completed (loss: 0.036246661096811295, acc: 0.985897421836853)
[2025-02-13 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:24][root][INFO] - Training Epoch: 2/2, step 3247/7134 completed (loss: 0.029317907989025116, acc: 0.990275502204895)
[2025-02-13 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:25][root][INFO] - Training Epoch: 2/2, step 3248/7134 completed (loss: 0.015185996890068054, acc: 0.9967159032821655)
[2025-02-13 03:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:25][root][INFO] - Training Epoch: 2/2, step 3249/7134 completed (loss: 0.0226907879114151, acc: 0.9958391189575195)
[2025-02-13 03:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26][root][INFO] - Training Epoch: 2/2, step 3250/7134 completed (loss: 0.03853921592235565, acc: 0.9856114983558655)
[2025-02-13 03:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26][root][INFO] - Training Epoch: 2/2, step 3251/7134 completed (loss: 0.022311747074127197, acc: 0.9942938685417175)
[2025-02-13 03:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26][root][INFO] - Training Epoch: 2/2, step 3252/7134 completed (loss: 0.004177766386419535, acc: 0.99858158826828)
[2025-02-13 03:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:27][root][INFO] - Training Epoch: 2/2, step 3253/7134 completed (loss: 0.022618889808654785, acc: 0.995184600353241)
[2025-02-13 03:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:27][root][INFO] - Training Epoch: 2/2, step 3254/7134 completed (loss: 0.008771703578531742, acc: 0.9983136653900146)
[2025-02-13 03:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28][root][INFO] - Training Epoch: 2/2, step 3255/7134 completed (loss: 0.005497067701071501, acc: 0.9973368644714355)
[2025-02-13 03:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28][root][INFO] - Training Epoch: 2/2, step 3256/7134 completed (loss: 0.0172626581043005, acc: 0.995555579662323)
[2025-02-13 03:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28][root][INFO] - Training Epoch: 2/2, step 3257/7134 completed (loss: 0.02033117040991783, acc: 0.994452178478241)
[2025-02-13 03:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:29][root][INFO] - Training Epoch: 2/2, step 3258/7134 completed (loss: 0.024011896923184395, acc: 0.9865384697914124)
[2025-02-13 03:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:29][root][INFO] - Training Epoch: 2/2, step 3259/7134 completed (loss: 0.0204536784440279, acc: 0.9955882430076599)
[2025-02-13 03:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:30][root][INFO] - Training Epoch: 2/2, step 3260/7134 completed (loss: 0.06810653209686279, acc: 0.9792284965515137)
[2025-02-13 03:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:30][root][INFO] - Training Epoch: 2/2, step 3261/7134 completed (loss: 0.03430042788386345, acc: 0.9865771532058716)
[2025-02-13 03:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31][root][INFO] - Training Epoch: 2/2, step 3262/7134 completed (loss: 0.022105582058429718, acc: 0.9913169145584106)
[2025-02-13 03:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31][root][INFO] - Training Epoch: 2/2, step 3263/7134 completed (loss: 0.0640225037932396, acc: 0.9867841601371765)
[2025-02-13 03:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31][root][INFO] - Training Epoch: 2/2, step 3264/7134 completed (loss: 0.02002563141286373, acc: 0.9944444298744202)
[2025-02-13 03:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:32][root][INFO] - Training Epoch: 2/2, step 3265/7134 completed (loss: 0.08468907326459885, acc: 0.9772079586982727)
[2025-02-13 03:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:32][root][INFO] - Training Epoch: 2/2, step 3266/7134 completed (loss: 0.040212202817201614, acc: 0.9909909963607788)
[2025-02-13 03:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33][root][INFO] - Training Epoch: 2/2, step 3267/7134 completed (loss: 0.06093801185488701, acc: 0.9813753366470337)
[2025-02-13 03:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33][root][INFO] - Training Epoch: 2/2, step 3268/7134 completed (loss: 0.03787073493003845, acc: 0.990755021572113)
[2025-02-13 03:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33][root][INFO] - Training Epoch: 2/2, step 3269/7134 completed (loss: 0.03156334161758423, acc: 0.9886363744735718)
[2025-02-13 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:34][root][INFO] - Training Epoch: 2/2, step 3270/7134 completed (loss: 0.01384410448372364, acc: 0.9952152967453003)
[2025-02-13 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:34][root][INFO] - Training Epoch: 2/2, step 3271/7134 completed (loss: 0.00968831405043602, acc: 0.9966555237770081)
[2025-02-13 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35][root][INFO] - Training Epoch: 2/2, step 3272/7134 completed (loss: 0.027905579656362534, acc: 0.9954198598861694)
[2025-02-13 03:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35][root][INFO] - Training Epoch: 2/2, step 3273/7134 completed (loss: 0.017142118886113167, acc: 0.9961685538291931)
[2025-02-13 03:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35][root][INFO] - Training Epoch: 2/2, step 3274/7134 completed (loss: 0.05142117664217949, acc: 0.9897210001945496)
[2025-02-13 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:36][root][INFO] - Training Epoch: 2/2, step 3275/7134 completed (loss: 0.12254901230335236, acc: 0.9736841917037964)
[2025-02-13 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:36][root][INFO] - Training Epoch: 2/2, step 3276/7134 completed (loss: 0.05704197660088539, acc: 0.9834710955619812)
[2025-02-13 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37][root][INFO] - Training Epoch: 2/2, step 3277/7134 completed (loss: 0.033853575587272644, acc: 0.9908536672592163)
[2025-02-13 03:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37][root][INFO] - Training Epoch: 2/2, step 3278/7134 completed (loss: 0.04663698375225067, acc: 0.9789674878120422)
[2025-02-13 03:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37][root][INFO] - Training Epoch: 2/2, step 3279/7134 completed (loss: 0.03301281854510307, acc: 0.992546558380127)
[2025-02-13 03:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:38][root][INFO] - Training Epoch: 2/2, step 3280/7134 completed (loss: 0.10976779460906982, acc: 0.9709543585777283)
[2025-02-13 03:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:38][root][INFO] - Training Epoch: 2/2, step 3281/7134 completed (loss: 0.05276497080922127, acc: 0.9855072498321533)
[2025-02-13 03:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:39][root][INFO] - Training Epoch: 2/2, step 3282/7134 completed (loss: 0.021900789812207222, acc: 0.9952531456947327)
[2025-02-13 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:39][root][INFO] - Training Epoch: 2/2, step 3283/7134 completed (loss: 0.027373816817998886, acc: 0.991752564907074)
[2025-02-13 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40][root][INFO] - Training Epoch: 2/2, step 3284/7134 completed (loss: 0.05648352578282356, acc: 0.9847792983055115)
[2025-02-13 03:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40][root][INFO] - Training Epoch: 2/2, step 3285/7134 completed (loss: 0.03325092792510986, acc: 0.9938080310821533)
[2025-02-13 03:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40][root][INFO] - Training Epoch: 2/2, step 3286/7134 completed (loss: 0.03115110844373703, acc: 0.9969924688339233)
[2025-02-13 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:41][root][INFO] - Training Epoch: 2/2, step 3287/7134 completed (loss: 0.08829332888126373, acc: 0.9819079041481018)
[2025-02-13 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:41][root][INFO] - Training Epoch: 2/2, step 3288/7134 completed (loss: 0.04075395315885544, acc: 0.9895209670066833)
[2025-02-13 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42][root][INFO] - Training Epoch: 2/2, step 3289/7134 completed (loss: 0.07140057533979416, acc: 0.9789473414421082)
[2025-02-13 03:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42][root][INFO] - Training Epoch: 2/2, step 3290/7134 completed (loss: 0.04817777872085571, acc: 0.9913194179534912)
[2025-02-13 03:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42][root][INFO] - Training Epoch: 2/2, step 3291/7134 completed (loss: 0.044717371463775635, acc: 0.9732739329338074)
[2025-02-13 03:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:43][root][INFO] - Training Epoch: 2/2, step 3292/7134 completed (loss: 0.03293837606906891, acc: 0.9894179701805115)
[2025-02-13 03:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:43][root][INFO] - Training Epoch: 2/2, step 3293/7134 completed (loss: 0.03232355788350105, acc: 0.9896755218505859)
[2025-02-13 03:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44][root][INFO] - Training Epoch: 2/2, step 3294/7134 completed (loss: 0.06973384320735931, acc: 0.9785605072975159)
[2025-02-13 03:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44][root][INFO] - Training Epoch: 2/2, step 3295/7134 completed (loss: 0.02012917585670948, acc: 0.9948805570602417)
[2025-02-13 03:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44][root][INFO] - Training Epoch: 2/2, step 3296/7134 completed (loss: 0.05500215291976929, acc: 0.9780621528625488)
[2025-02-13 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:45][root][INFO] - Training Epoch: 2/2, step 3297/7134 completed (loss: 0.03983587026596069, acc: 0.9906103014945984)
[2025-02-13 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:45][root][INFO] - Training Epoch: 2/2, step 3298/7134 completed (loss: 0.09008512645959854, acc: 0.9760383367538452)
[2025-02-13 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46][root][INFO] - Training Epoch: 2/2, step 3299/7134 completed (loss: 0.03744210675358772, acc: 0.9848993420600891)
[2025-02-13 03:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46][root][INFO] - Training Epoch: 2/2, step 3300/7134 completed (loss: 0.057305485010147095, acc: 0.9839572310447693)
[2025-02-13 03:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46][root][INFO] - Training Epoch: 2/2, step 3301/7134 completed (loss: 0.0365082286298275, acc: 0.9958506226539612)
[2025-02-13 03:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47][root][INFO] - Training Epoch: 2/2, step 3302/7134 completed (loss: 0.04685744643211365, acc: 0.9898989796638489)
[2025-02-13 03:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47][root][INFO] - Training Epoch: 2/2, step 3303/7134 completed (loss: 0.048925261944532394, acc: 0.9889867901802063)
[2025-02-13 03:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47][root][INFO] - Training Epoch: 2/2, step 3304/7134 completed (loss: 0.02521355077624321, acc: 0.9916467666625977)
[2025-02-13 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:48][root][INFO] - Training Epoch: 2/2, step 3305/7134 completed (loss: 0.03129902854561806, acc: 0.9909502267837524)
[2025-02-13 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:48][root][INFO] - Training Epoch: 2/2, step 3306/7134 completed (loss: 0.025305168703198433, acc: 0.9948119521141052)
[2025-02-13 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:49][root][INFO] - Training Epoch: 2/2, step 3307/7134 completed (loss: 0.03287409245967865, acc: 0.9864636063575745)
[2025-02-13 03:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:49][root][INFO] - Training Epoch: 2/2, step 3308/7134 completed (loss: 0.038601815700531006, acc: 0.986994206905365)
[2025-02-13 03:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50][root][INFO] - Training Epoch: 2/2, step 3309/7134 completed (loss: 0.060474980622529984, acc: 0.982300877571106)
[2025-02-13 03:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50][root][INFO] - Training Epoch: 2/2, step 3310/7134 completed (loss: 0.03266005963087082, acc: 0.9936034083366394)
[2025-02-13 03:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50][root][INFO] - Training Epoch: 2/2, step 3311/7134 completed (loss: 0.031140362843871117, acc: 0.9892473220825195)
[2025-02-13 03:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51][root][INFO] - Training Epoch: 2/2, step 3312/7134 completed (loss: 0.01711914874613285, acc: 0.9945454597473145)
[2025-02-13 03:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51][root][INFO] - Training Epoch: 2/2, step 3313/7134 completed (loss: 0.01299657765775919, acc: 0.9976133704185486)
[2025-02-13 03:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51][root][INFO] - Training Epoch: 2/2, step 3314/7134 completed (loss: 0.04191235080361366, acc: 0.989130437374115)
[2025-02-13 03:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:52][root][INFO] - Training Epoch: 2/2, step 3315/7134 completed (loss: 0.06029898300766945, acc: 0.9850746393203735)
[2025-02-13 03:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:52][root][INFO] - Training Epoch: 2/2, step 3316/7134 completed (loss: 0.018133845180273056, acc: 0.9949579834938049)
[2025-02-13 03:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53][root][INFO] - Training Epoch: 2/2, step 3317/7134 completed (loss: 0.07137142866849899, acc: 0.982692301273346)
[2025-02-13 03:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53][root][INFO] - Training Epoch: 2/2, step 3318/7134 completed (loss: 0.02597171626985073, acc: 0.9891696572303772)
[2025-02-13 03:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53][root][INFO] - Training Epoch: 2/2, step 3319/7134 completed (loss: 0.03216809779405594, acc: 0.9869706630706787)
[2025-02-13 03:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:54][root][INFO] - Training Epoch: 2/2, step 3320/7134 completed (loss: 0.06848860532045364, acc: 0.9829843044281006)
[2025-02-13 03:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:54][root][INFO] - Training Epoch: 2/2, step 3321/7134 completed (loss: 0.09857933968305588, acc: 0.9724770784378052)
[2025-02-13 03:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:55][root][INFO] - Training Epoch: 2/2, step 3322/7134 completed (loss: 0.03353014960885048, acc: 0.9909909963607788)
[2025-02-13 03:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:55][root][INFO] - Training Epoch: 2/2, step 3323/7134 completed (loss: 0.03602490574121475, acc: 0.9917241334915161)
[2025-02-13 03:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:56][root][INFO] - Training Epoch: 2/2, step 3324/7134 completed (loss: 0.05411193519830704, acc: 0.9858657121658325)
[2025-02-13 03:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:56][root][INFO] - Training Epoch: 2/2, step 3325/7134 completed (loss: 0.03935829922556877, acc: 0.9905914068222046)
[2025-02-13 03:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57][root][INFO] - Training Epoch: 2/2, step 3326/7134 completed (loss: 0.05478869006037712, acc: 0.9811023473739624)
[2025-02-13 03:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57][root][INFO] - Training Epoch: 2/2, step 3327/7134 completed (loss: 0.030773049220442772, acc: 0.9905277490615845)
[2025-02-13 03:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57][root][INFO] - Training Epoch: 2/2, step 3328/7134 completed (loss: 0.03840986266732216, acc: 0.9949874877929688)
[2025-02-13 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:58][root][INFO] - Training Epoch: 2/2, step 3329/7134 completed (loss: 0.02432449348270893, acc: 0.9963459372520447)
[2025-02-13 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:58][root][INFO] - Training Epoch: 2/2, step 3330/7134 completed (loss: 0.04168454557657242, acc: 0.9884792566299438)
[2025-02-13 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:59][root][INFO] - Training Epoch: 2/2, step 3331/7134 completed (loss: 0.08595003932714462, acc: 0.9709302186965942)
[2025-02-13 03:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:59][root][INFO] - Training Epoch: 2/2, step 3332/7134 completed (loss: 0.10232186317443848, acc: 0.9740484356880188)
[2025-02-13 03:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00][root][INFO] - Training Epoch: 2/2, step 3333/7134 completed (loss: 0.05685143172740936, acc: 0.9897540807723999)
[2025-02-13 03:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00][root][INFO] - Training Epoch: 2/2, step 3334/7134 completed (loss: 0.053600676357746124, acc: 0.9878378510475159)
[2025-02-13 03:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00][root][INFO] - Training Epoch: 2/2, step 3335/7134 completed (loss: 0.027905460447072983, acc: 0.9920886158943176)
[2025-02-13 03:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:01][root][INFO] - Training Epoch: 2/2, step 3336/7134 completed (loss: 0.012466936372220516, acc: 0.9970845580101013)
[2025-02-13 03:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:01][root][INFO] - Training Epoch: 2/2, step 3337/7134 completed (loss: 0.014491992071270943, acc: 0.9960552453994751)
[2025-02-13 03:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02][root][INFO] - Training Epoch: 2/2, step 3338/7134 completed (loss: 0.011031911708414555, acc: 0.9975154995918274)
[2025-02-13 03:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02][root][INFO] - Training Epoch: 2/2, step 3339/7134 completed (loss: 0.05918829143047333, acc: 0.9844868779182434)
[2025-02-13 03:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02][root][INFO] - Training Epoch: 2/2, step 3340/7134 completed (loss: 0.09238513559103012, acc: 0.9728506803512573)
[2025-02-13 03:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:03][root][INFO] - Training Epoch: 2/2, step 3341/7134 completed (loss: 0.04631498456001282, acc: 0.9881481528282166)
[2025-02-13 03:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:03][root][INFO] - Training Epoch: 2/2, step 3342/7134 completed (loss: 0.03215253725647926, acc: 0.9898648858070374)
[2025-02-13 03:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:04][root][INFO] - Training Epoch: 2/2, step 3343/7134 completed (loss: 0.024106750264763832, acc: 0.9939516186714172)
[2025-02-13 03:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:04][root][INFO] - Training Epoch: 2/2, step 3344/7134 completed (loss: 0.013222871348261833, acc: 0.9965694546699524)
[2025-02-13 03:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05][root][INFO] - Training Epoch: 2/2, step 3345/7134 completed (loss: 0.0715220645070076, acc: 0.9837925434112549)
[2025-02-13 03:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05][root][INFO] - Training Epoch: 2/2, step 3346/7134 completed (loss: 0.034001827239990234, acc: 0.988304078578949)
[2025-02-13 03:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05][root][INFO] - Training Epoch: 2/2, step 3347/7134 completed (loss: 0.062352146953344345, acc: 0.9837133288383484)
[2025-02-13 03:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:06][root][INFO] - Training Epoch: 2/2, step 3348/7134 completed (loss: 0.036864086985588074, acc: 0.9925925731658936)
[2025-02-13 03:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:06][root][INFO] - Training Epoch: 2/2, step 3349/7134 completed (loss: 0.026765037328004837, acc: 0.9929453134536743)
[2025-02-13 03:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07][root][INFO] - Training Epoch: 2/2, step 3350/7134 completed (loss: 0.01941889151930809, acc: 0.9918699264526367)
[2025-02-13 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07][root][INFO] - Training Epoch: 2/2, step 3351/7134 completed (loss: 0.0507197380065918, acc: 0.9867374300956726)
[2025-02-13 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07][root][INFO] - Training Epoch: 2/2, step 3352/7134 completed (loss: 0.026376308873295784, acc: 0.9947826266288757)
[2025-02-13 03:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:08][root][INFO] - Training Epoch: 2/2, step 3353/7134 completed (loss: 0.03491704538464546, acc: 0.9881656765937805)
[2025-02-13 03:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:08][root][INFO] - Training Epoch: 2/2, step 3354/7134 completed (loss: 0.026056023314595222, acc: 0.9937984347343445)
[2025-02-13 03:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09][root][INFO] - Training Epoch: 2/2, step 3355/7134 completed (loss: 0.009821758605539799, acc: 0.9971791505813599)
[2025-02-13 03:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09][root][INFO] - Training Epoch: 2/2, step 3356/7134 completed (loss: 0.007216351106762886, acc: 0.9982638955116272)
[2025-02-13 03:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09][root][INFO] - Training Epoch: 2/2, step 3357/7134 completed (loss: 0.03307085856795311, acc: 0.9913420081138611)
[2025-02-13 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:10][root][INFO] - Training Epoch: 2/2, step 3358/7134 completed (loss: 0.03573489189147949, acc: 0.9930796027183533)
[2025-02-13 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:10][root][INFO] - Training Epoch: 2/2, step 3359/7134 completed (loss: 0.02015303075313568, acc: 0.9935275316238403)
[2025-02-13 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11][root][INFO] - Training Epoch: 2/2, step 3360/7134 completed (loss: 0.09568679332733154, acc: 0.9828326106071472)
[2025-02-13 03:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11][root][INFO] - Training Epoch: 2/2, step 3361/7134 completed (loss: 0.03664611652493477, acc: 0.9843400716781616)
[2025-02-13 03:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11][root][INFO] - Training Epoch: 2/2, step 3362/7134 completed (loss: 0.04312917962670326, acc: 0.9863945841789246)
[2025-02-13 03:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:12][root][INFO] - Training Epoch: 2/2, step 3363/7134 completed (loss: 0.018324699252843857, acc: 0.9953051805496216)
[2025-02-13 03:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:12][root][INFO] - Training Epoch: 2/2, step 3364/7134 completed (loss: 0.07075779139995575, acc: 0.9831029176712036)
[2025-02-13 03:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13][root][INFO] - Training Epoch: 2/2, step 3365/7134 completed (loss: 0.0627649798989296, acc: 0.983132541179657)
[2025-02-13 03:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13][root][INFO] - Training Epoch: 2/2, step 3366/7134 completed (loss: 0.022057276219129562, acc: 0.9887892603874207)
[2025-02-13 03:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13][root][INFO] - Training Epoch: 2/2, step 3367/7134 completed (loss: 0.03183750435709953, acc: 0.9931129217147827)
[2025-02-13 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:14][root][INFO] - Training Epoch: 2/2, step 3368/7134 completed (loss: 0.035282202064991, acc: 0.9921996593475342)
[2025-02-13 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:14][root][INFO] - Training Epoch: 2/2, step 3369/7134 completed (loss: 0.04039417952299118, acc: 0.9899280667304993)
[2025-02-13 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:15][root][INFO] - Training Epoch: 2/2, step 3370/7134 completed (loss: 0.015747644007205963, acc: 0.9971387982368469)
[2025-02-13 03:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:15][root][INFO] - Training Epoch: 2/2, step 3371/7134 completed (loss: 0.016789982095360756, acc: 0.9950658082962036)
[2025-02-13 03:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16][root][INFO] - Training Epoch: 2/2, step 3372/7134 completed (loss: 0.026117943227291107, acc: 0.9952977895736694)
[2025-02-13 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16][root][INFO] - Training Epoch: 2/2, step 3373/7134 completed (loss: 0.024832889437675476, acc: 0.9922839403152466)
[2025-02-13 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16][root][INFO] - Training Epoch: 2/2, step 3374/7134 completed (loss: 0.015842707827687263, acc: 0.9958217144012451)
[2025-02-13 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:17][root][INFO] - Training Epoch: 2/2, step 3375/7134 completed (loss: 0.017154548317193985, acc: 0.9942113161087036)
[2025-02-13 03:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:17][root][INFO] - Training Epoch: 2/2, step 3376/7134 completed (loss: 0.007668609265238047, acc: 0.9974457025527954)
[2025-02-13 03:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:18][root][INFO] - Training Epoch: 2/2, step 3377/7134 completed (loss: 0.021638810634613037, acc: 0.9964243173599243)
[2025-02-13 03:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:18][root][INFO] - Training Epoch: 2/2, step 3378/7134 completed (loss: 0.011845006607472897, acc: 0.99609375)
[2025-02-13 03:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19][root][INFO] - Training Epoch: 2/2, step 3379/7134 completed (loss: 0.018728801980614662, acc: 0.995488703250885)
[2025-02-13 03:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19][root][INFO] - Training Epoch: 2/2, step 3380/7134 completed (loss: 0.03771325200796127, acc: 0.9907692074775696)
[2025-02-13 03:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19][root][INFO] - Training Epoch: 2/2, step 3381/7134 completed (loss: 0.031787946820259094, acc: 0.9961240291595459)
[2025-02-13 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:20][root][INFO] - Training Epoch: 2/2, step 3382/7134 completed (loss: 0.013522109016776085, acc: 0.9948096871376038)
[2025-02-13 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:20][root][INFO] - Training Epoch: 2/2, step 3383/7134 completed (loss: 0.03477959334850311, acc: 0.9928264021873474)
[2025-02-13 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21][root][INFO] - Training Epoch: 2/2, step 3384/7134 completed (loss: 0.03169461712241173, acc: 0.9877675771713257)
[2025-02-13 03:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21][root][INFO] - Training Epoch: 2/2, step 3385/7134 completed (loss: 0.029258178547024727, acc: 0.9913669228553772)
[2025-02-13 03:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21][root][INFO] - Training Epoch: 2/2, step 3386/7134 completed (loss: 0.01486674789339304, acc: 0.9957627058029175)
[2025-02-13 03:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:22][root][INFO] - Training Epoch: 2/2, step 3387/7134 completed (loss: 0.0446065217256546, acc: 0.9918367266654968)
[2025-02-13 03:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:22][root][INFO] - Training Epoch: 2/2, step 3388/7134 completed (loss: 0.027504609897732735, acc: 0.9891501069068909)
[2025-02-13 03:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:23][root][INFO] - Training Epoch: 2/2, step 3389/7134 completed (loss: 0.021290279924869537, acc: 0.9947299361228943)
[2025-02-13 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:23][root][INFO] - Training Epoch: 2/2, step 3390/7134 completed (loss: 0.021891804412007332, acc: 0.9935232996940613)
[2025-02-13 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24][root][INFO] - Training Epoch: 2/2, step 3391/7134 completed (loss: 0.003087991615757346, acc: 1.0)
[2025-02-13 03:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24][root][INFO] - Training Epoch: 2/2, step 3392/7134 completed (loss: 0.010285130701959133, acc: 0.9968553185462952)
[2025-02-13 03:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24][root][INFO] - Training Epoch: 2/2, step 3393/7134 completed (loss: 0.02894236333668232, acc: 0.9891975522041321)
[2025-02-13 03:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:25][root][INFO] - Training Epoch: 2/2, step 3394/7134 completed (loss: 0.03440236672759056, acc: 0.9905063509941101)
[2025-02-13 03:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:25][root][INFO] - Training Epoch: 2/2, step 3395/7134 completed (loss: 0.06277511268854141, acc: 0.9805194735527039)
[2025-02-13 03:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26][root][INFO] - Training Epoch: 2/2, step 3396/7134 completed (loss: 0.022022927179932594, acc: 0.9928143620491028)
[2025-02-13 03:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26][root][INFO] - Training Epoch: 2/2, step 3397/7134 completed (loss: 0.02537619322538376, acc: 0.9938875436782837)
[2025-02-13 03:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26][root][INFO] - Training Epoch: 2/2, step 3398/7134 completed (loss: 0.07156161963939667, acc: 0.9892473220825195)
[2025-02-13 03:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:27][root][INFO] - Training Epoch: 2/2, step 3399/7134 completed (loss: 0.09058203548192978, acc: 0.9742646813392639)
[2025-02-13 03:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:27][root][INFO] - Training Epoch: 2/2, step 3400/7134 completed (loss: 0.042779210954904556, acc: 0.9797160029411316)
[2025-02-13 03:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:28][root][INFO] - Training Epoch: 2/2, step 3401/7134 completed (loss: 0.038937732577323914, acc: 0.9877622127532959)
[2025-02-13 03:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:28][root][INFO] - Training Epoch: 2/2, step 3402/7134 completed (loss: 0.044290050864219666, acc: 0.990777313709259)
[2025-02-13 03:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29][root][INFO] - Training Epoch: 2/2, step 3403/7134 completed (loss: 0.07901482284069061, acc: 0.9791666865348816)
[2025-02-13 03:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29][root][INFO] - Training Epoch: 2/2, step 3404/7134 completed (loss: 0.018530137836933136, acc: 0.9970104694366455)
[2025-02-13 03:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29][root][INFO] - Training Epoch: 2/2, step 3405/7134 completed (loss: 0.0678916871547699, acc: 0.9838129281997681)
[2025-02-13 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:30][root][INFO] - Training Epoch: 2/2, step 3406/7134 completed (loss: 0.03828691691160202, acc: 0.9845361113548279)
[2025-02-13 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:30][root][INFO] - Training Epoch: 2/2, step 3407/7134 completed (loss: 0.05204271152615547, acc: 0.9875195026397705)
[2025-02-13 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31][root][INFO] - Training Epoch: 2/2, step 3408/7134 completed (loss: 0.035558685660362244, acc: 0.9861751198768616)
[2025-02-13 03:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31][root][INFO] - Training Epoch: 2/2, step 3409/7134 completed (loss: 0.03900080546736717, acc: 0.9918808937072754)
[2025-02-13 03:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31][root][INFO] - Training Epoch: 2/2, step 3410/7134 completed (loss: 0.037338610738515854, acc: 0.9917355179786682)
[2025-02-13 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:32][root][INFO] - Training Epoch: 2/2, step 3411/7134 completed (loss: 0.04009552672505379, acc: 0.9909443855285645)
[2025-02-13 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:32][root][INFO] - Training Epoch: 2/2, step 3412/7134 completed (loss: 0.04075753316283226, acc: 0.9867674708366394)
[2025-02-13 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:33][root][INFO] - Training Epoch: 2/2, step 3413/7134 completed (loss: 0.03565967455506325, acc: 0.9900000095367432)
[2025-02-13 03:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:33][root][INFO] - Training Epoch: 2/2, step 3414/7134 completed (loss: 0.035733021795749664, acc: 0.9916527271270752)
[2025-02-13 03:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34][root][INFO] - Training Epoch: 2/2, step 3415/7134 completed (loss: 0.036533091217279434, acc: 0.9919871687889099)
[2025-02-13 03:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34][root][INFO] - Training Epoch: 2/2, step 3416/7134 completed (loss: 0.045592498034238815, acc: 0.981992781162262)
[2025-02-13 03:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34][root][INFO] - Training Epoch: 2/2, step 3417/7134 completed (loss: 0.020874962210655212, acc: 0.9932523369789124)
[2025-02-13 03:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:35][root][INFO] - Training Epoch: 2/2, step 3418/7134 completed (loss: 0.026360662654042244, acc: 0.9926362037658691)
[2025-02-13 03:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:35][root][INFO] - Training Epoch: 2/2, step 3419/7134 completed (loss: 0.014836663380265236, acc: 0.9956772327423096)
[2025-02-13 03:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36][root][INFO] - Training Epoch: 2/2, step 3420/7134 completed (loss: 0.016276592388749123, acc: 0.9936808943748474)
[2025-02-13 03:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36][root][INFO] - Training Epoch: 2/2, step 3421/7134 completed (loss: 0.028541769832372665, acc: 0.996889591217041)
[2025-02-13 03:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36][root][INFO] - Training Epoch: 2/2, step 3422/7134 completed (loss: 0.008895457722246647, acc: 0.996497392654419)
[2025-02-13 03:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:37][root][INFO] - Training Epoch: 2/2, step 3423/7134 completed (loss: 0.020038584247231483, acc: 0.9939393997192383)
[2025-02-13 03:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:37][root][INFO] - Training Epoch: 2/2, step 3424/7134 completed (loss: 0.05952577665448189, acc: 0.9789156913757324)
[2025-02-13 03:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:38][root][INFO] - Training Epoch: 2/2, step 3425/7134 completed (loss: 0.02192351594567299, acc: 0.9948717951774597)
[2025-02-13 03:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:38][root][INFO] - Training Epoch: 2/2, step 3426/7134 completed (loss: 0.04201946035027504, acc: 0.9944827556610107)
[2025-02-13 03:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39][root][INFO] - Training Epoch: 2/2, step 3427/7134 completed (loss: 0.03929297998547554, acc: 0.9921011328697205)
[2025-02-13 03:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39][root][INFO] - Training Epoch: 2/2, step 3428/7134 completed (loss: 0.027574295178055763, acc: 0.9957020282745361)
[2025-02-13 03:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39][root][INFO] - Training Epoch: 2/2, step 3429/7134 completed (loss: 0.04309890791773796, acc: 0.9870550036430359)
[2025-02-13 03:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:40][root][INFO] - Training Epoch: 2/2, step 3430/7134 completed (loss: 0.07104503363370895, acc: 0.987034022808075)
[2025-02-13 03:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:40][root][INFO] - Training Epoch: 2/2, step 3431/7134 completed (loss: 0.03603959456086159, acc: 0.9898132681846619)
[2025-02-13 03:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:41][root][INFO] - Training Epoch: 2/2, step 3432/7134 completed (loss: 0.032584041357040405, acc: 0.9931412935256958)
[2025-02-13 03:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:41][root][INFO] - Training Epoch: 2/2, step 3433/7134 completed (loss: 0.05522603169083595, acc: 0.9852941036224365)
[2025-02-13 03:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42][root][INFO] - Training Epoch: 2/2, step 3434/7134 completed (loss: 0.021573495119810104, acc: 0.995230495929718)
[2025-02-13 03:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42][root][INFO] - Training Epoch: 2/2, step 3435/7134 completed (loss: 0.03943237289786339, acc: 0.9887459874153137)
[2025-02-13 03:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42][root][INFO] - Training Epoch: 2/2, step 3436/7134 completed (loss: 0.03630049526691437, acc: 0.9902439117431641)
[2025-02-13 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:43][root][INFO] - Training Epoch: 2/2, step 3437/7134 completed (loss: 0.012421718798577785, acc: 0.99609375)
[2025-02-13 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:43][root][INFO] - Training Epoch: 2/2, step 3438/7134 completed (loss: 0.033026400953531265, acc: 0.9939393997192383)
[2025-02-13 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44][root][INFO] - Training Epoch: 2/2, step 3439/7134 completed (loss: 0.03496013209223747, acc: 0.9909583926200867)
[2025-02-13 03:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44][root][INFO] - Training Epoch: 2/2, step 3440/7134 completed (loss: 0.056655000895261765, acc: 0.9773662686347961)
[2025-02-13 03:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44][root][INFO] - Training Epoch: 2/2, step 3441/7134 completed (loss: 0.027620717883110046, acc: 0.9855491518974304)
[2025-02-13 03:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45][root][INFO] - Training Epoch: 2/2, step 3442/7134 completed (loss: 0.030219554901123047, acc: 0.9945255517959595)
[2025-02-13 03:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45][root][INFO] - Training Epoch: 2/2, step 3443/7134 completed (loss: 0.06871074438095093, acc: 0.983146071434021)
[2025-02-13 03:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46][root][INFO] - Training Epoch: 2/2, step 3444/7134 completed (loss: 0.06091121584177017, acc: 0.9889240264892578)
[2025-02-13 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46][root][INFO] - Training Epoch: 2/2, step 3445/7134 completed (loss: 0.09626200795173645, acc: 0.9741935729980469)
[2025-02-13 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46][root][INFO] - Training Epoch: 2/2, step 3446/7134 completed (loss: 0.08914124965667725, acc: 0.9816053509712219)
[2025-02-13 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47][root][INFO] - Training Epoch: 2/2, step 3447/7134 completed (loss: 0.04771050438284874, acc: 0.9853420257568359)
[2025-02-13 03:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47][root][INFO] - Training Epoch: 2/2, step 3448/7134 completed (loss: 0.02147754281759262, acc: 0.9924924969673157)
[2025-02-13 03:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47][root][INFO] - Training Epoch: 2/2, step 3449/7134 completed (loss: 0.048195309937000275, acc: 0.9897959232330322)
[2025-02-13 03:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:48][root][INFO] - Training Epoch: 2/2, step 3450/7134 completed (loss: 0.029456382617354393, acc: 0.9873417615890503)
[2025-02-13 03:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:48][root][INFO] - Training Epoch: 2/2, step 3451/7134 completed (loss: 0.03545108810067177, acc: 0.9876325130462646)
[2025-02-13 03:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49][root][INFO] - Training Epoch: 2/2, step 3452/7134 completed (loss: 0.03692544996738434, acc: 0.989154040813446)
[2025-02-13 03:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49][root][INFO] - Training Epoch: 2/2, step 3453/7134 completed (loss: 0.04903874546289444, acc: 0.9881154298782349)
[2025-02-13 03:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50][root][INFO] - Training Epoch: 2/2, step 3454/7134 completed (loss: 0.025208069011569023, acc: 0.9929478168487549)
[2025-02-13 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50][root][INFO] - Training Epoch: 2/2, step 3455/7134 completed (loss: 0.01225098967552185, acc: 0.9971014261245728)
[2025-02-13 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50][root][INFO] - Training Epoch: 2/2, step 3456/7134 completed (loss: 0.028093121945858, acc: 0.9941176176071167)
[2025-02-13 03:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:51][root][INFO] - Training Epoch: 2/2, step 3457/7134 completed (loss: 0.016094839200377464, acc: 0.993220329284668)
[2025-02-13 03:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:51][root][INFO] - Training Epoch: 2/2, step 3458/7134 completed (loss: 0.04586441069841385, acc: 0.9878214001655579)
[2025-02-13 03:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52][root][INFO] - Training Epoch: 2/2, step 3459/7134 completed (loss: 0.04976950213313103, acc: 0.9878234267234802)
[2025-02-13 03:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52][root][INFO] - Training Epoch: 2/2, step 3460/7134 completed (loss: 0.12691497802734375, acc: 0.9647436141967773)
[2025-02-13 03:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52][root][INFO] - Training Epoch: 2/2, step 3461/7134 completed (loss: 0.18738269805908203, acc: 0.9586206674575806)
[2025-02-13 03:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:53][root][INFO] - Training Epoch: 2/2, step 3462/7134 completed (loss: 0.09545566141605377, acc: 0.976452112197876)
[2025-02-13 03:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:53][root][INFO] - Training Epoch: 2/2, step 3463/7134 completed (loss: 0.019570045173168182, acc: 0.993966817855835)
[2025-02-13 03:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54][root][INFO] - Training Epoch: 2/2, step 3464/7134 completed (loss: 0.04489883780479431, acc: 0.9865951538085938)
[2025-02-13 03:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54][root][INFO] - Training Epoch: 2/2, step 3465/7134 completed (loss: 0.07954781502485275, acc: 0.9775967597961426)
[2025-02-13 03:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54][root][INFO] - Training Epoch: 2/2, step 3466/7134 completed (loss: 0.014734609983861446, acc: 0.996830403804779)
[2025-02-13 03:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:55][root][INFO] - Training Epoch: 2/2, step 3467/7134 completed (loss: 0.028920873999595642, acc: 0.987261176109314)
[2025-02-13 03:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:55][root][INFO] - Training Epoch: 2/2, step 3468/7134 completed (loss: 0.008557400666177273, acc: 0.9985527992248535)
[2025-02-13 03:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56][root][INFO] - Training Epoch: 2/2, step 3469/7134 completed (loss: 0.03411995619535446, acc: 0.9830188751220703)
[2025-02-13 03:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56][root][INFO] - Training Epoch: 2/2, step 3470/7134 completed (loss: 0.012649359181523323, acc: 0.9953917264938354)
[2025-02-13 03:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56][root][INFO] - Training Epoch: 2/2, step 3471/7134 completed (loss: 0.059242334216833115, acc: 0.9861878156661987)
[2025-02-13 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:57][root][INFO] - Training Epoch: 2/2, step 3472/7134 completed (loss: 0.013227468356490135, acc: 0.997019350528717)
[2025-02-13 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:57][root][INFO] - Training Epoch: 2/2, step 3473/7134 completed (loss: 0.02853730134665966, acc: 0.9884105920791626)
[2025-02-13 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58][root][INFO] - Training Epoch: 2/2, step 3474/7134 completed (loss: 0.047447096556425095, acc: 0.9911660552024841)
[2025-02-13 03:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58][root][INFO] - Training Epoch: 2/2, step 3475/7134 completed (loss: 0.02224770374596119, acc: 0.9946714043617249)
[2025-02-13 03:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58][root][INFO] - Training Epoch: 2/2, step 3476/7134 completed (loss: 0.04457397386431694, acc: 0.9811643958091736)
[2025-02-13 03:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:59][root][INFO] - Training Epoch: 2/2, step 3477/7134 completed (loss: 0.058892589062452316, acc: 0.9868420958518982)
[2025-02-13 03:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:59][root][INFO] - Training Epoch: 2/2, step 3478/7134 completed (loss: 0.016467444598674774, acc: 0.9955089688301086)
[2025-02-13 03:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:00][root][INFO] - Training Epoch: 2/2, step 3479/7134 completed (loss: 0.005960073322057724, acc: 0.998487114906311)
[2025-02-13 03:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:00][root][INFO] - Training Epoch: 2/2, step 3480/7134 completed (loss: 0.02639356069266796, acc: 0.9931880235671997)
[2025-02-13 03:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01][root][INFO] - Training Epoch: 2/2, step 3481/7134 completed (loss: 0.02507888153195381, acc: 0.9946808218955994)
[2025-02-13 03:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01][root][INFO] - Training Epoch: 2/2, step 3482/7134 completed (loss: 0.01989913359284401, acc: 0.9964028596878052)
[2025-02-13 03:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01][root][INFO] - Training Epoch: 2/2, step 3483/7134 completed (loss: 0.0036681003402918577, acc: 1.0)
[2025-02-13 03:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:02][root][INFO] - Training Epoch: 2/2, step 3484/7134 completed (loss: 0.0245343204587698, acc: 0.9941002726554871)
[2025-02-13 03:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:02][root][INFO] - Training Epoch: 2/2, step 3485/7134 completed (loss: 0.02425439842045307, acc: 0.9927849769592285)
[2025-02-13 03:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03][root][INFO] - Training Epoch: 2/2, step 3486/7134 completed (loss: 0.01025359332561493, acc: 0.9982014298439026)
[2025-02-13 03:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03][root][INFO] - Training Epoch: 2/2, step 3487/7134 completed (loss: 0.026204636320471764, acc: 0.9867452383041382)
[2025-02-13 03:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03][root][INFO] - Training Epoch: 2/2, step 3488/7134 completed (loss: 0.018911629915237427, acc: 0.9950658082962036)
[2025-02-13 03:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:04][root][INFO] - Training Epoch: 2/2, step 3489/7134 completed (loss: 0.05078168585896492, acc: 0.9852670431137085)
[2025-02-13 03:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:04][root][INFO] - Training Epoch: 2/2, step 3490/7134 completed (loss: 0.056285981088876724, acc: 0.9896602630615234)
[2025-02-13 03:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05][root][INFO] - Training Epoch: 2/2, step 3491/7134 completed (loss: 0.039194051176309586, acc: 0.9914893507957458)
[2025-02-13 03:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05][root][INFO] - Training Epoch: 2/2, step 3492/7134 completed (loss: 0.027152398601174355, acc: 0.9917864203453064)
[2025-02-13 03:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05][root][INFO] - Training Epoch: 2/2, step 3493/7134 completed (loss: 0.013003334403038025, acc: 0.9971988797187805)
[2025-02-13 03:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06][root][INFO] - Training Epoch: 2/2, step 3494/7134 completed (loss: 0.02965734153985977, acc: 0.9931034445762634)
[2025-02-13 03:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06][root][INFO] - Training Epoch: 2/2, step 3495/7134 completed (loss: 0.009349847212433815, acc: 0.9976019263267517)
[2025-02-13 03:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07][root][INFO] - Training Epoch: 2/2, step 3496/7134 completed (loss: 0.015754885971546173, acc: 0.9954954981803894)
[2025-02-13 03:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07][root][INFO] - Training Epoch: 2/2, step 3497/7134 completed (loss: 0.03189801797270775, acc: 0.9896142482757568)
[2025-02-13 03:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07][root][INFO] - Training Epoch: 2/2, step 3498/7134 completed (loss: 0.02929426170885563, acc: 0.9862805008888245)
[2025-02-13 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:08][root][INFO] - Training Epoch: 2/2, step 3499/7134 completed (loss: 0.009783553890883923, acc: 1.0)
[2025-02-13 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:08][root][INFO] - Training Epoch: 2/2, step 3500/7134 completed (loss: 0.04769653081893921, acc: 0.98531574010849)
[2025-02-13 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09][root][INFO] - Training Epoch: 2/2, step 3501/7134 completed (loss: 0.08014735579490662, acc: 0.9758551120758057)
[2025-02-13 03:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09][root][INFO] - Training Epoch: 2/2, step 3502/7134 completed (loss: 0.08868226408958435, acc: 0.9754385948181152)
[2025-02-13 03:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09][root][INFO] - Training Epoch: 2/2, step 3503/7134 completed (loss: 0.040420956909656525, acc: 0.9861351847648621)
[2025-02-13 03:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:10][root][INFO] - Training Epoch: 2/2, step 3504/7134 completed (loss: 0.008727118372917175, acc: 0.9969834089279175)
[2025-02-13 03:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:10][root][INFO] - Training Epoch: 2/2, step 3505/7134 completed (loss: 0.023411830887198448, acc: 0.9964221715927124)
[2025-02-13 03:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11][root][INFO] - Training Epoch: 2/2, step 3506/7134 completed (loss: 0.026592975482344627, acc: 0.992438554763794)
[2025-02-13 03:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11][root][INFO] - Training Epoch: 2/2, step 3507/7134 completed (loss: 0.00818716362118721, acc: 0.996688723564148)
[2025-02-13 03:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11][root][INFO] - Training Epoch: 2/2, step 3508/7134 completed (loss: 0.02503911592066288, acc: 0.9956140518188477)
[2025-02-13 03:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:12][root][INFO] - Training Epoch: 2/2, step 3509/7134 completed (loss: 0.01667059399187565, acc: 0.9944751262664795)
[2025-02-13 03:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:12][root][INFO] - Training Epoch: 2/2, step 3510/7134 completed (loss: 0.02405916340649128, acc: 0.989708423614502)
[2025-02-13 03:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:13][root][INFO] - Training Epoch: 2/2, step 3511/7134 completed (loss: 0.007388652767986059, acc: 0.9981982111930847)
[2025-02-13 03:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:13][root][INFO] - Training Epoch: 2/2, step 3512/7134 completed (loss: 0.03880750387907028, acc: 0.9841521382331848)
[2025-02-13 03:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14][root][INFO] - Training Epoch: 2/2, step 3513/7134 completed (loss: 0.01709505170583725, acc: 0.9964601993560791)
[2025-02-13 03:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14][root][INFO] - Training Epoch: 2/2, step 3514/7134 completed (loss: 0.022676024585962296, acc: 0.9965437650680542)
[2025-02-13 03:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14][root][INFO] - Training Epoch: 2/2, step 3515/7134 completed (loss: 0.00499491672962904, acc: 1.0)
[2025-02-13 03:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:15][root][INFO] - Training Epoch: 2/2, step 3516/7134 completed (loss: 0.018216047435998917, acc: 0.9945828914642334)
[2025-02-13 03:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:15][root][INFO] - Training Epoch: 2/2, step 3517/7134 completed (loss: 0.005363333038985729, acc: 0.9988080859184265)
[2025-02-13 03:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:16][root][INFO] - Training Epoch: 2/2, step 3518/7134 completed (loss: 0.016582805663347244, acc: 0.9926062822341919)
[2025-02-13 03:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:16][root][INFO] - Training Epoch: 2/2, step 3519/7134 completed (loss: 0.01956961862742901, acc: 0.9943820238113403)
[2025-02-13 03:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17][root][INFO] - Training Epoch: 2/2, step 3520/7134 completed (loss: 0.05654165521264076, acc: 0.9858823418617249)
[2025-02-13 03:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17][root][INFO] - Training Epoch: 2/2, step 3521/7134 completed (loss: 0.00878438912332058, acc: 0.9957325458526611)
[2025-02-13 03:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17][root][INFO] - Training Epoch: 2/2, step 3522/7134 completed (loss: 0.008838697336614132, acc: 0.9973261952400208)
[2025-02-13 03:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:18][root][INFO] - Training Epoch: 2/2, step 3523/7134 completed (loss: 0.004657391458749771, acc: 1.0)
[2025-02-13 03:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:18][root][INFO] - Training Epoch: 2/2, step 3524/7134 completed (loss: 0.03255076706409454, acc: 0.9947916865348816)
[2025-02-13 03:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:19][root][INFO] - Training Epoch: 2/2, step 3525/7134 completed (loss: 0.009242760948836803, acc: 0.9950576424598694)
[2025-02-13 03:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:19][root][INFO] - Training Epoch: 2/2, step 3526/7134 completed (loss: 0.04195994511246681, acc: 0.9923954606056213)
[2025-02-13 03:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:20][root][INFO] - Training Epoch: 2/2, step 3527/7134 completed (loss: 0.02268003113567829, acc: 0.994878351688385)
[2025-02-13 03:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:20][root][INFO] - Training Epoch: 2/2, step 3528/7134 completed (loss: 0.0028135899920016527, acc: 1.0)
[2025-02-13 03:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21][root][INFO] - Training Epoch: 2/2, step 3529/7134 completed (loss: 0.006872531026601791, acc: 1.0)
[2025-02-13 03:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21][root][INFO] - Training Epoch: 2/2, step 3530/7134 completed (loss: 0.022103719413280487, acc: 0.9954128265380859)
[2025-02-13 03:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21][root][INFO] - Training Epoch: 2/2, step 3531/7134 completed (loss: 0.014456531964242458, acc: 0.9954819083213806)
[2025-02-13 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:22][root][INFO] - Training Epoch: 2/2, step 3532/7134 completed (loss: 0.003373779123649001, acc: 1.0)
[2025-02-13 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:22][root][INFO] - Training Epoch: 2/2, step 3533/7134 completed (loss: 0.011660600081086159, acc: 0.9988358616828918)
[2025-02-13 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:23][root][INFO] - Training Epoch: 2/2, step 3534/7134 completed (loss: 0.03887591138482094, acc: 0.9902642369270325)
[2025-02-13 03:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:23][root][INFO] - Training Epoch: 2/2, step 3535/7134 completed (loss: 0.01770576275885105, acc: 0.9953917264938354)
[2025-02-13 03:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24][root][INFO] - Training Epoch: 2/2, step 3536/7134 completed (loss: 0.0464627742767334, acc: 0.9904371500015259)
[2025-02-13 03:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24][root][INFO] - Training Epoch: 2/2, step 3537/7134 completed (loss: 0.02111811190843582, acc: 0.994966447353363)
[2025-02-13 03:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24][root][INFO] - Training Epoch: 2/2, step 3538/7134 completed (loss: 0.04416907951235771, acc: 0.9923954606056213)
[2025-02-13 03:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25][root][INFO] - Training Epoch: 2/2, step 3539/7134 completed (loss: 0.010568266734480858, acc: 0.9975728392601013)
[2025-02-13 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25][root][INFO] - Training Epoch: 2/2, step 3540/7134 completed (loss: 0.04750819504261017, acc: 0.9833080172538757)
[2025-02-13 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25][root][INFO] - Training Epoch: 2/2, step 3541/7134 completed (loss: 0.022013362497091293, acc: 0.9941262602806091)
[2025-02-13 03:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:26][root][INFO] - Training Epoch: 2/2, step 3542/7134 completed (loss: 0.021962115541100502, acc: 0.9969230890274048)
[2025-02-13 03:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:26][root][INFO] - Training Epoch: 2/2, step 3543/7134 completed (loss: 0.04716096818447113, acc: 0.9894366264343262)
[2025-02-13 03:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:27][root][INFO] - Training Epoch: 2/2, step 3544/7134 completed (loss: 0.030961288139224052, acc: 0.9923076629638672)
[2025-02-13 03:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:27][root][INFO] - Training Epoch: 2/2, step 3545/7134 completed (loss: 0.010037839412689209, acc: 0.9984471797943115)
[2025-02-13 03:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28][root][INFO] - Training Epoch: 2/2, step 3546/7134 completed (loss: 0.023852292448282242, acc: 0.9933993220329285)
[2025-02-13 03:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28][root][INFO] - Training Epoch: 2/2, step 3547/7134 completed (loss: 0.04317981004714966, acc: 0.9832167625427246)
[2025-02-13 03:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28][root][INFO] - Training Epoch: 2/2, step 3548/7134 completed (loss: 0.01943828910589218, acc: 0.9933628439903259)
[2025-02-13 03:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:29][root][INFO] - Training Epoch: 2/2, step 3549/7134 completed (loss: 0.03272606059908867, acc: 0.9894179701805115)
[2025-02-13 03:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:29][root][INFO] - Training Epoch: 2/2, step 3550/7134 completed (loss: 0.018215790390968323, acc: 0.9947460889816284)
[2025-02-13 03:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30][root][INFO] - Training Epoch: 2/2, step 3551/7134 completed (loss: 0.021327173337340355, acc: 0.9932659864425659)
[2025-02-13 03:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30][root][INFO] - Training Epoch: 2/2, step 3552/7134 completed (loss: 0.0986253172159195, acc: 0.9700374603271484)
[2025-02-13 03:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30][root][INFO] - Training Epoch: 2/2, step 3553/7134 completed (loss: 0.06670829653739929, acc: 0.9851973652839661)
[2025-02-13 03:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:31][root][INFO] - Training Epoch: 2/2, step 3554/7134 completed (loss: 0.0440407358109951, acc: 0.9894578456878662)
[2025-02-13 03:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:31][root][INFO] - Training Epoch: 2/2, step 3555/7134 completed (loss: 0.029963204637169838, acc: 0.9921507239341736)
[2025-02-13 03:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:32][root][INFO] - Training Epoch: 2/2, step 3556/7134 completed (loss: 0.03475795313715935, acc: 0.9904030561447144)
[2025-02-13 03:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:32][root][INFO] - Training Epoch: 2/2, step 3557/7134 completed (loss: 0.02179133892059326, acc: 0.996370255947113)
[2025-02-13 03:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33][root][INFO] - Training Epoch: 2/2, step 3558/7134 completed (loss: 0.04335227981209755, acc: 0.9876288771629333)
[2025-02-13 03:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33][root][INFO] - Training Epoch: 2/2, step 3559/7134 completed (loss: 0.011806491762399673, acc: 0.9985465407371521)
[2025-02-13 03:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33][root][INFO] - Training Epoch: 2/2, step 3560/7134 completed (loss: 0.01286003552377224, acc: 0.9972602725028992)
[2025-02-13 03:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:34][root][INFO] - Training Epoch: 2/2, step 3561/7134 completed (loss: 0.045428209006786346, acc: 0.9859943985939026)
[2025-02-13 03:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:34][root][INFO] - Training Epoch: 2/2, step 3562/7134 completed (loss: 0.031116781756281853, acc: 0.9892857074737549)
[2025-02-13 03:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:35][root][INFO] - Training Epoch: 2/2, step 3563/7134 completed (loss: 0.019052857533097267, acc: 0.995230495929718)
[2025-02-13 03:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0486, device='cuda:0') eval_epoch_loss=tensor(0.0474, device='cuda:0') eval_epoch_acc=tensor(0.9867, device='cuda:0')
[2025-02-13 04:03:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:03:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:03:34][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04742717742919922/model.pt
[2025-02-13 04:03:34][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:03:34][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.04742717742919922
[2025-02-13 04:03:34][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9866829514503479
[2025-02-13 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:34][root][INFO] - Training Epoch: 2/2, step 3564/7134 completed (loss: 0.05050092563033104, acc: 0.9924812316894531)
[2025-02-13 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:35][root][INFO] - Training Epoch: 2/2, step 3565/7134 completed (loss: 0.07097481936216354, acc: 0.9774436354637146)
[2025-02-13 04:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:35][root][INFO] - Training Epoch: 2/2, step 3566/7134 completed (loss: 0.05096496269106865, acc: 0.9820716977119446)
[2025-02-13 04:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36][root][INFO] - Training Epoch: 2/2, step 3567/7134 completed (loss: 0.08381090313196182, acc: 0.9818887710571289)
[2025-02-13 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36][root][INFO] - Training Epoch: 2/2, step 3568/7134 completed (loss: 0.08398986607789993, acc: 0.9803600907325745)
[2025-02-13 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36][root][INFO] - Training Epoch: 2/2, step 3569/7134 completed (loss: 0.042781054973602295, acc: 0.9874551892280579)
[2025-02-13 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:37][root][INFO] - Training Epoch: 2/2, step 3570/7134 completed (loss: 0.08062272518873215, acc: 0.9851239919662476)
[2025-02-13 04:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:37][root][INFO] - Training Epoch: 2/2, step 3571/7134 completed (loss: 0.06229456141591072, acc: 0.9867374300956726)
[2025-02-13 04:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38][root][INFO] - Training Epoch: 2/2, step 3572/7134 completed (loss: 0.01576777920126915, acc: 0.9986824989318848)
[2025-02-13 04:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38][root][INFO] - Training Epoch: 2/2, step 3573/7134 completed (loss: 0.04642920196056366, acc: 0.9866666793823242)
[2025-02-13 04:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38][root][INFO] - Training Epoch: 2/2, step 3574/7134 completed (loss: 0.05161648243665695, acc: 0.9789271950721741)
[2025-02-13 04:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:39][root][INFO] - Training Epoch: 2/2, step 3575/7134 completed (loss: 0.09023363143205643, acc: 0.9742709994316101)
[2025-02-13 04:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:39][root][INFO] - Training Epoch: 2/2, step 3576/7134 completed (loss: 0.05888393148779869, acc: 0.9834254384040833)
[2025-02-13 04:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:40][root][INFO] - Training Epoch: 2/2, step 3577/7134 completed (loss: 0.03810145705938339, acc: 0.9886220097541809)
[2025-02-13 04:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:40][root][INFO] - Training Epoch: 2/2, step 3578/7134 completed (loss: 0.04066535085439682, acc: 0.9854439496994019)
[2025-02-13 04:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41][root][INFO] - Training Epoch: 2/2, step 3579/7134 completed (loss: 0.03616499900817871, acc: 0.9866666793823242)
[2025-02-13 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41][root][INFO] - Training Epoch: 2/2, step 3580/7134 completed (loss: 0.06189548596739769, acc: 0.9795275330543518)
[2025-02-13 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41][root][INFO] - Training Epoch: 2/2, step 3581/7134 completed (loss: 0.06972012668848038, acc: 0.9808542132377625)
[2025-02-13 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:42][root][INFO] - Training Epoch: 2/2, step 3582/7134 completed (loss: 0.05649630352854729, acc: 0.9810426831245422)
[2025-02-13 04:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:42][root][INFO] - Training Epoch: 2/2, step 3583/7134 completed (loss: 0.06079593300819397, acc: 0.9817629456520081)
[2025-02-13 04:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43][root][INFO] - Training Epoch: 2/2, step 3584/7134 completed (loss: 0.06324934214353561, acc: 0.9894179701805115)
[2025-02-13 04:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43][root][INFO] - Training Epoch: 2/2, step 3585/7134 completed (loss: 0.04879986494779587, acc: 0.984375)
[2025-02-13 04:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43][root][INFO] - Training Epoch: 2/2, step 3586/7134 completed (loss: 0.0432872548699379, acc: 0.9871588945388794)
[2025-02-13 04:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:44][root][INFO] - Training Epoch: 2/2, step 3587/7134 completed (loss: 0.042055822908878326, acc: 0.989847719669342)
[2025-02-13 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:44][root][INFO] - Training Epoch: 2/2, step 3588/7134 completed (loss: 0.06156131252646446, acc: 0.9866864085197449)
[2025-02-13 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45][root][INFO] - Training Epoch: 2/2, step 3589/7134 completed (loss: 0.06236536055803299, acc: 0.9909228682518005)
[2025-02-13 04:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45][root][INFO] - Training Epoch: 2/2, step 3590/7134 completed (loss: 0.02050282619893551, acc: 0.992977499961853)
[2025-02-13 04:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45][root][INFO] - Training Epoch: 2/2, step 3591/7134 completed (loss: 0.030009040609002113, acc: 0.9894894957542419)
[2025-02-13 04:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:46][root][INFO] - Training Epoch: 2/2, step 3592/7134 completed (loss: 0.016787204891443253, acc: 0.9917355179786682)
[2025-02-13 04:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:46][root][INFO] - Training Epoch: 2/2, step 3593/7134 completed (loss: 0.041960228234529495, acc: 0.9894921183586121)
[2025-02-13 04:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47][root][INFO] - Training Epoch: 2/2, step 3594/7134 completed (loss: 0.022250209003686905, acc: 0.9918830990791321)
[2025-02-13 04:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47][root][INFO] - Training Epoch: 2/2, step 3595/7134 completed (loss: 0.038698066025972366, acc: 0.9846860766410828)
[2025-02-13 04:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48][root][INFO] - Training Epoch: 2/2, step 3596/7134 completed (loss: 0.03927686810493469, acc: 0.9903714060783386)
[2025-02-13 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48][root][INFO] - Training Epoch: 2/2, step 3597/7134 completed (loss: 0.02743903174996376, acc: 0.9928057789802551)
[2025-02-13 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48][root][INFO] - Training Epoch: 2/2, step 3598/7134 completed (loss: 0.049976348876953125, acc: 0.97826087474823)
[2025-02-13 04:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:49][root][INFO] - Training Epoch: 2/2, step 3599/7134 completed (loss: 0.044765666127204895, acc: 0.9886506795883179)
[2025-02-13 04:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:49][root][INFO] - Training Epoch: 2/2, step 3600/7134 completed (loss: 0.04942534491419792, acc: 0.9865319728851318)
[2025-02-13 04:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50][root][INFO] - Training Epoch: 2/2, step 3601/7134 completed (loss: 0.11103057116270065, acc: 0.9724137783050537)
[2025-02-13 04:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50][root][INFO] - Training Epoch: 2/2, step 3602/7134 completed (loss: 0.07366305589675903, acc: 0.9766627550125122)
[2025-02-13 04:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:51][root][INFO] - Training Epoch: 2/2, step 3603/7134 completed (loss: 0.02377256378531456, acc: 0.9859374761581421)
[2025-02-13 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:51][root][INFO] - Training Epoch: 2/2, step 3604/7134 completed (loss: 0.0343436561524868, acc: 0.9943714737892151)
[2025-02-13 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52][root][INFO] - Training Epoch: 2/2, step 3605/7134 completed (loss: 0.04334370791912079, acc: 0.9896480441093445)
[2025-02-13 04:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52][root][INFO] - Training Epoch: 2/2, step 3606/7134 completed (loss: 0.04575903341174126, acc: 0.9893428087234497)
[2025-02-13 04:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52][root][INFO] - Training Epoch: 2/2, step 3607/7134 completed (loss: 0.031026309356093407, acc: 0.9899328947067261)
[2025-02-13 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:53][root][INFO] - Training Epoch: 2/2, step 3608/7134 completed (loss: 0.07830328494310379, acc: 0.9811946749687195)
[2025-02-13 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:53][root][INFO] - Training Epoch: 2/2, step 3609/7134 completed (loss: 0.0397263839840889, acc: 0.989847719669342)
[2025-02-13 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:54][root][INFO] - Training Epoch: 2/2, step 3610/7134 completed (loss: 0.04915875568985939, acc: 0.9818181991577148)
[2025-02-13 04:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:54][root][INFO] - Training Epoch: 2/2, step 3611/7134 completed (loss: 0.04731297120451927, acc: 0.9865591526031494)
[2025-02-13 04:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:55][root][INFO] - Training Epoch: 2/2, step 3612/7134 completed (loss: 0.07802607119083405, acc: 0.9771783947944641)
[2025-02-13 04:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:55][root][INFO] - Training Epoch: 2/2, step 3613/7134 completed (loss: 0.05208626016974449, acc: 0.9809160232543945)
[2025-02-13 04:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56][root][INFO] - Training Epoch: 2/2, step 3614/7134 completed (loss: 0.03931190073490143, acc: 0.9879518151283264)
[2025-02-13 04:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56][root][INFO] - Training Epoch: 2/2, step 3615/7134 completed (loss: 0.0216328427195549, acc: 0.9969419240951538)
[2025-02-13 04:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56][root][INFO] - Training Epoch: 2/2, step 3616/7134 completed (loss: 0.03595692291855812, acc: 0.9858712553977966)
[2025-02-13 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:57][root][INFO] - Training Epoch: 2/2, step 3617/7134 completed (loss: 0.03871874138712883, acc: 0.9876237511634827)
[2025-02-13 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:57][root][INFO] - Training Epoch: 2/2, step 3618/7134 completed (loss: 0.09279533475637436, acc: 0.9689542651176453)
[2025-02-13 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:58][root][INFO] - Training Epoch: 2/2, step 3619/7134 completed (loss: 0.08903531730175018, acc: 0.9702380895614624)
[2025-02-13 04:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:58][root][INFO] - Training Epoch: 2/2, step 3620/7134 completed (loss: 0.04454800486564636, acc: 0.9873417615890503)
[2025-02-13 04:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59][root][INFO] - Training Epoch: 2/2, step 3621/7134 completed (loss: 0.029714545235037804, acc: 0.9902912378311157)
[2025-02-13 04:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59][root][INFO] - Training Epoch: 2/2, step 3622/7134 completed (loss: 0.030613910406827927, acc: 0.9909909963607788)
[2025-02-13 04:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59][root][INFO] - Training Epoch: 2/2, step 3623/7134 completed (loss: 0.014231120236217976, acc: 0.9925650358200073)
[2025-02-13 04:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:00][root][INFO] - Training Epoch: 2/2, step 3624/7134 completed (loss: 0.019290341064333916, acc: 0.996666669845581)
[2025-02-13 04:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:00][root][INFO] - Training Epoch: 2/2, step 3625/7134 completed (loss: 0.07097627967596054, acc: 0.9798792600631714)
[2025-02-13 04:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01][root][INFO] - Training Epoch: 2/2, step 3626/7134 completed (loss: 0.15009033679962158, acc: 0.9599999785423279)
[2025-02-13 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01][root][INFO] - Training Epoch: 2/2, step 3627/7134 completed (loss: 0.08372657001018524, acc: 0.9810874462127686)
[2025-02-13 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01][root][INFO] - Training Epoch: 2/2, step 3628/7134 completed (loss: 0.12711845338344574, acc: 0.96875)
[2025-02-13 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02][root][INFO] - Training Epoch: 2/2, step 3629/7134 completed (loss: 0.09925603121519089, acc: 0.977642297744751)
[2025-02-13 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02][root][INFO] - Training Epoch: 2/2, step 3630/7134 completed (loss: 0.07914973050355911, acc: 0.9793281555175781)
[2025-02-13 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02][root][INFO] - Training Epoch: 2/2, step 3631/7134 completed (loss: 0.06543571501970291, acc: 0.9819672107696533)
[2025-02-13 04:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:03][root][INFO] - Training Epoch: 2/2, step 3632/7134 completed (loss: 0.03939756378531456, acc: 0.9899396300315857)
[2025-02-13 04:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:03][root][INFO] - Training Epoch: 2/2, step 3633/7134 completed (loss: 0.0856049656867981, acc: 0.9842180609703064)
[2025-02-13 04:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04][root][INFO] - Training Epoch: 2/2, step 3634/7134 completed (loss: 0.10049837082624435, acc: 0.97773277759552)
[2025-02-13 04:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04][root][INFO] - Training Epoch: 2/2, step 3635/7134 completed (loss: 0.027557598426938057, acc: 0.9910394549369812)
[2025-02-13 04:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04][root][INFO] - Training Epoch: 2/2, step 3636/7134 completed (loss: 0.028902120888233185, acc: 0.9901574850082397)
[2025-02-13 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:05][root][INFO] - Training Epoch: 2/2, step 3637/7134 completed (loss: 0.034593187272548676, acc: 0.9879275560379028)
[2025-02-13 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:05][root][INFO] - Training Epoch: 2/2, step 3638/7134 completed (loss: 0.02734357863664627, acc: 0.9929412007331848)
[2025-02-13 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06][root][INFO] - Training Epoch: 2/2, step 3639/7134 completed (loss: 0.020188825204968452, acc: 0.9909909963607788)
[2025-02-13 04:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06][root][INFO] - Training Epoch: 2/2, step 3640/7134 completed (loss: 0.03761384263634682, acc: 0.9904580116271973)
[2025-02-13 04:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06][root][INFO] - Training Epoch: 2/2, step 3641/7134 completed (loss: 0.0275124479085207, acc: 0.994163453578949)
[2025-02-13 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:07][root][INFO] - Training Epoch: 2/2, step 3642/7134 completed (loss: 0.02601894736289978, acc: 0.9938176274299622)
[2025-02-13 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:07][root][INFO] - Training Epoch: 2/2, step 3643/7134 completed (loss: 0.03185954689979553, acc: 0.9948453903198242)
[2025-02-13 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08][root][INFO] - Training Epoch: 2/2, step 3644/7134 completed (loss: 0.06523191183805466, acc: 0.9821138381958008)
[2025-02-13 04:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08][root][INFO] - Training Epoch: 2/2, step 3645/7134 completed (loss: 0.048475973308086395, acc: 0.98591548204422)
[2025-02-13 04:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08][root][INFO] - Training Epoch: 2/2, step 3646/7134 completed (loss: 0.10427944362163544, acc: 0.9739583134651184)
[2025-02-13 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:09][root][INFO] - Training Epoch: 2/2, step 3647/7134 completed (loss: 0.01777038350701332, acc: 0.9922480583190918)
[2025-02-13 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:09][root][INFO] - Training Epoch: 2/2, step 3648/7134 completed (loss: 0.06092432141304016, acc: 0.9807956218719482)
[2025-02-13 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:10][root][INFO] - Training Epoch: 2/2, step 3649/7134 completed (loss: 0.06587854772806168, acc: 0.981792688369751)
[2025-02-13 04:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:10][root][INFO] - Training Epoch: 2/2, step 3650/7134 completed (loss: 0.045515093952417374, acc: 0.9843137264251709)
[2025-02-13 04:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11][root][INFO] - Training Epoch: 2/2, step 3651/7134 completed (loss: 0.06931597739458084, acc: 0.9825737476348877)
[2025-02-13 04:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11][root][INFO] - Training Epoch: 2/2, step 3652/7134 completed (loss: 0.06209472194314003, acc: 0.9788838624954224)
[2025-02-13 04:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11][root][INFO] - Training Epoch: 2/2, step 3653/7134 completed (loss: 0.08795849233865738, acc: 0.9743589758872986)
[2025-02-13 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12][root][INFO] - Training Epoch: 2/2, step 3654/7134 completed (loss: 0.022222882136702538, acc: 0.9930915236473083)
[2025-02-13 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12][root][INFO] - Training Epoch: 2/2, step 3655/7134 completed (loss: 0.05603982135653496, acc: 0.9890109896659851)
[2025-02-13 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13][root][INFO] - Training Epoch: 2/2, step 3656/7134 completed (loss: 0.09300211071968079, acc: 0.9727272987365723)
[2025-02-13 04:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13][root][INFO] - Training Epoch: 2/2, step 3657/7134 completed (loss: 0.015447304584085941, acc: 0.995192289352417)
[2025-02-13 04:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13][root][INFO] - Training Epoch: 2/2, step 3658/7134 completed (loss: 0.04413947835564613, acc: 0.9922118186950684)
[2025-02-13 04:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14][root][INFO] - Training Epoch: 2/2, step 3659/7134 completed (loss: 0.027852462604641914, acc: 0.9925261735916138)
[2025-02-13 04:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14][root][INFO] - Training Epoch: 2/2, step 3660/7134 completed (loss: 0.05370756611227989, acc: 0.9868852496147156)
[2025-02-13 04:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14][root][INFO] - Training Epoch: 2/2, step 3661/7134 completed (loss: 0.04214074835181236, acc: 0.9857142567634583)
[2025-02-13 04:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:15][root][INFO] - Training Epoch: 2/2, step 3662/7134 completed (loss: 0.08354926109313965, acc: 0.9756097793579102)
[2025-02-13 04:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:15][root][INFO] - Training Epoch: 2/2, step 3663/7134 completed (loss: 0.03390410915017128, acc: 0.9890109896659851)
[2025-02-13 04:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:16][root][INFO] - Training Epoch: 2/2, step 3664/7134 completed (loss: 0.054665010422468185, acc: 0.9893617033958435)
[2025-02-13 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:16][root][INFO] - Training Epoch: 2/2, step 3665/7134 completed (loss: 0.03347726911306381, acc: 0.9934318661689758)
[2025-02-13 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17][root][INFO] - Training Epoch: 2/2, step 3666/7134 completed (loss: 0.04936600103974342, acc: 0.9817517995834351)
[2025-02-13 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17][root][INFO] - Training Epoch: 2/2, step 3667/7134 completed (loss: 0.14946797490119934, acc: 0.965831458568573)
[2025-02-13 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17][root][INFO] - Training Epoch: 2/2, step 3668/7134 completed (loss: 0.047976814210414886, acc: 0.9895287752151489)
[2025-02-13 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18][root][INFO] - Training Epoch: 2/2, step 3669/7134 completed (loss: 0.01806349866092205, acc: 0.996219277381897)
[2025-02-13 04:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18][root][INFO] - Training Epoch: 2/2, step 3670/7134 completed (loss: 0.03484649583697319, acc: 0.9931318759918213)
[2025-02-13 04:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18][root][INFO] - Training Epoch: 2/2, step 3671/7134 completed (loss: 0.01870698854327202, acc: 0.9927745461463928)
[2025-02-13 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:19][root][INFO] - Training Epoch: 2/2, step 3672/7134 completed (loss: 0.02527049370110035, acc: 0.9949495196342468)
[2025-02-13 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:19][root][INFO] - Training Epoch: 2/2, step 3673/7134 completed (loss: 0.03889463469386101, acc: 0.9882943034172058)
[2025-02-13 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:20][root][INFO] - Training Epoch: 2/2, step 3674/7134 completed (loss: 0.013123561628162861, acc: 0.9968503713607788)
[2025-02-13 04:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:20][root][INFO] - Training Epoch: 2/2, step 3675/7134 completed (loss: 0.012415104545652866, acc: 0.9974489808082581)
[2025-02-13 04:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21][root][INFO] - Training Epoch: 2/2, step 3676/7134 completed (loss: 0.019309107214212418, acc: 0.9941691160202026)
[2025-02-13 04:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21][root][INFO] - Training Epoch: 2/2, step 3677/7134 completed (loss: 0.036060456186532974, acc: 0.9863013625144958)
[2025-02-13 04:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21][root][INFO] - Training Epoch: 2/2, step 3678/7134 completed (loss: 0.02851567044854164, acc: 0.9905533194541931)
[2025-02-13 04:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:22][root][INFO] - Training Epoch: 2/2, step 3679/7134 completed (loss: 0.01607055589556694, acc: 0.9966942071914673)
[2025-02-13 04:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:22][root][INFO] - Training Epoch: 2/2, step 3680/7134 completed (loss: 0.03554455190896988, acc: 0.9886363744735718)
[2025-02-13 04:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:23][root][INFO] - Training Epoch: 2/2, step 3681/7134 completed (loss: 0.024272888898849487, acc: 0.9916201233863831)
[2025-02-13 04:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:23][root][INFO] - Training Epoch: 2/2, step 3682/7134 completed (loss: 0.01899554207921028, acc: 0.9947299361228943)
[2025-02-13 04:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24][root][INFO] - Training Epoch: 2/2, step 3683/7134 completed (loss: 0.01608339138329029, acc: 0.995275616645813)
[2025-02-13 04:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24][root][INFO] - Training Epoch: 2/2, step 3684/7134 completed (loss: 0.02852349542081356, acc: 0.9927849769592285)
[2025-02-13 04:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24][root][INFO] - Training Epoch: 2/2, step 3685/7134 completed (loss: 0.01481443177908659, acc: 0.9922928810119629)
[2025-02-13 04:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:25][root][INFO] - Training Epoch: 2/2, step 3686/7134 completed (loss: 0.02442094497382641, acc: 0.9925037622451782)
[2025-02-13 04:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:25][root][INFO] - Training Epoch: 2/2, step 3687/7134 completed (loss: 0.011085158213973045, acc: 0.9953846335411072)
[2025-02-13 04:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:26][root][INFO] - Training Epoch: 2/2, step 3688/7134 completed (loss: 0.009300080128014088, acc: 0.9979715943336487)
[2025-02-13 04:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:26][root][INFO] - Training Epoch: 2/2, step 3689/7134 completed (loss: 0.02213752269744873, acc: 0.9934640526771545)
[2025-02-13 04:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27][root][INFO] - Training Epoch: 2/2, step 3690/7134 completed (loss: 0.012138335034251213, acc: 0.9971387982368469)
[2025-02-13 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27][root][INFO] - Training Epoch: 2/2, step 3691/7134 completed (loss: 0.025960378348827362, acc: 0.9950371980667114)
[2025-02-13 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27][root][INFO] - Training Epoch: 2/2, step 3692/7134 completed (loss: 0.03864622116088867, acc: 0.9893454909324646)
[2025-02-13 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:28][root][INFO] - Training Epoch: 2/2, step 3693/7134 completed (loss: 0.011014861986041069, acc: 0.9952681660652161)
[2025-02-13 04:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:28][root][INFO] - Training Epoch: 2/2, step 3694/7134 completed (loss: 0.021407125517725945, acc: 0.9957325458526611)
[2025-02-13 04:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29][root][INFO] - Training Epoch: 2/2, step 3695/7134 completed (loss: 0.022413654252886772, acc: 0.9929178357124329)
[2025-02-13 04:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29][root][INFO] - Training Epoch: 2/2, step 3696/7134 completed (loss: 0.022162389010190964, acc: 0.9922879338264465)
[2025-02-13 04:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29][root][INFO] - Training Epoch: 2/2, step 3697/7134 completed (loss: 0.013631039299070835, acc: 0.9956268072128296)
[2025-02-13 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:30][root][INFO] - Training Epoch: 2/2, step 3698/7134 completed (loss: 0.017016369849443436, acc: 0.9919742941856384)
[2025-02-13 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:30][root][INFO] - Training Epoch: 2/2, step 3699/7134 completed (loss: 0.004120846278965473, acc: 0.9983108043670654)
[2025-02-13 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:31][root][INFO] - Training Epoch: 2/2, step 3700/7134 completed (loss: 0.014271781779825687, acc: 0.9941691160202026)
[2025-02-13 04:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:31][root][INFO] - Training Epoch: 2/2, step 3701/7134 completed (loss: 0.029552994295954704, acc: 0.9933775067329407)
[2025-02-13 04:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32][root][INFO] - Training Epoch: 2/2, step 3702/7134 completed (loss: 0.027707159519195557, acc: 0.994490385055542)
[2025-02-13 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32][root][INFO] - Training Epoch: 2/2, step 3703/7134 completed (loss: 0.02099870704114437, acc: 0.9961685538291931)
[2025-02-13 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32][root][INFO] - Training Epoch: 2/2, step 3704/7134 completed (loss: 0.004283297341316938, acc: 1.0)
[2025-02-13 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:33][root][INFO] - Training Epoch: 2/2, step 3705/7134 completed (loss: 0.05344086512923241, acc: 0.9840764403343201)
[2025-02-13 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:33][root][INFO] - Training Epoch: 2/2, step 3706/7134 completed (loss: 0.07756839692592621, acc: 0.9803197979927063)
[2025-02-13 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:34][root][INFO] - Training Epoch: 2/2, step 3707/7134 completed (loss: 0.019987361505627632, acc: 0.9929873943328857)
[2025-02-13 04:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:34][root][INFO] - Training Epoch: 2/2, step 3708/7134 completed (loss: 0.04169797524809837, acc: 0.9857142567634583)
[2025-02-13 04:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35][root][INFO] - Training Epoch: 2/2, step 3709/7134 completed (loss: 0.019081035628914833, acc: 0.993914783000946)
[2025-02-13 04:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35][root][INFO] - Training Epoch: 2/2, step 3710/7134 completed (loss: 0.03227291628718376, acc: 0.9866803288459778)
[2025-02-13 04:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35][root][INFO] - Training Epoch: 2/2, step 3711/7134 completed (loss: 0.026131199672818184, acc: 0.9937965273857117)
[2025-02-13 04:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:36][root][INFO] - Training Epoch: 2/2, step 3712/7134 completed (loss: 0.028333744034171104, acc: 0.9940263032913208)
[2025-02-13 04:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:36][root][INFO] - Training Epoch: 2/2, step 3713/7134 completed (loss: 0.04124028608202934, acc: 0.9917898178100586)
[2025-02-13 04:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:37][root][INFO] - Training Epoch: 2/2, step 3714/7134 completed (loss: 0.03004000522196293, acc: 0.9922651648521423)
[2025-02-13 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:37][root][INFO] - Training Epoch: 2/2, step 3715/7134 completed (loss: 0.021520819514989853, acc: 0.996372401714325)
[2025-02-13 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:38][root][INFO] - Training Epoch: 2/2, step 3716/7134 completed (loss: 0.03705736994743347, acc: 0.9864364862442017)
[2025-02-13 04:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:38][root][INFO] - Training Epoch: 2/2, step 3717/7134 completed (loss: 0.024359729140996933, acc: 0.9959016442298889)
[2025-02-13 04:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39][root][INFO] - Training Epoch: 2/2, step 3718/7134 completed (loss: 0.03957238420844078, acc: 0.9894490242004395)
[2025-02-13 04:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39][root][INFO] - Training Epoch: 2/2, step 3719/7134 completed (loss: 0.05071934685111046, acc: 0.9852941036224365)
[2025-02-13 04:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39][root][INFO] - Training Epoch: 2/2, step 3720/7134 completed (loss: 0.015498381108045578, acc: 0.9941349029541016)
[2025-02-13 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:40][root][INFO] - Training Epoch: 2/2, step 3721/7134 completed (loss: 0.023724470287561417, acc: 0.9920106530189514)
[2025-02-13 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:40][root][INFO] - Training Epoch: 2/2, step 3722/7134 completed (loss: 0.0169219933450222, acc: 0.993842363357544)
[2025-02-13 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:41][root][INFO] - Training Epoch: 2/2, step 3723/7134 completed (loss: 0.043839577585458755, acc: 0.9884892106056213)
[2025-02-13 04:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:41][root][INFO] - Training Epoch: 2/2, step 3724/7134 completed (loss: 0.033187177032232285, acc: 0.9899425506591797)
[2025-02-13 04:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42][root][INFO] - Training Epoch: 2/2, step 3725/7134 completed (loss: 0.09105624258518219, acc: 0.974926233291626)
[2025-02-13 04:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42][root][INFO] - Training Epoch: 2/2, step 3726/7134 completed (loss: 0.03187035024166107, acc: 0.9925611019134521)
[2025-02-13 04:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42][root][INFO] - Training Epoch: 2/2, step 3727/7134 completed (loss: 0.024216556921601295, acc: 0.9946595430374146)
[2025-02-13 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:43][root][INFO] - Training Epoch: 2/2, step 3728/7134 completed (loss: 0.010800696909427643, acc: 0.9977452158927917)
[2025-02-13 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:43][root][INFO] - Training Epoch: 2/2, step 3729/7134 completed (loss: 0.023035962134599686, acc: 0.9920454621315002)
[2025-02-13 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:44][root][INFO] - Training Epoch: 2/2, step 3730/7134 completed (loss: 0.018616747111082077, acc: 0.9932157397270203)
[2025-02-13 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:44][root][INFO] - Training Epoch: 2/2, step 3731/7134 completed (loss: 0.019582495093345642, acc: 0.9950186610221863)
[2025-02-13 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45][root][INFO] - Training Epoch: 2/2, step 3732/7134 completed (loss: 0.01663854531943798, acc: 0.9954596757888794)
[2025-02-13 04:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45][root][INFO] - Training Epoch: 2/2, step 3733/7134 completed (loss: 0.05265573784708977, acc: 0.9859747290611267)
[2025-02-13 04:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46][root][INFO] - Training Epoch: 2/2, step 3734/7134 completed (loss: 0.04044255241751671, acc: 0.9886731505393982)
[2025-02-13 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46][root][INFO] - Training Epoch: 2/2, step 3735/7134 completed (loss: 0.044743895530700684, acc: 0.9879518151283264)
[2025-02-13 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46][root][INFO] - Training Epoch: 2/2, step 3736/7134 completed (loss: 0.0949544906616211, acc: 0.9791304469108582)
[2025-02-13 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:47][root][INFO] - Training Epoch: 2/2, step 3737/7134 completed (loss: 0.05609304457902908, acc: 0.9819004535675049)
[2025-02-13 04:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:47][root][INFO] - Training Epoch: 2/2, step 3738/7134 completed (loss: 0.036242205649614334, acc: 0.9933993220329285)
[2025-02-13 04:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48][root][INFO] - Training Epoch: 2/2, step 3739/7134 completed (loss: 0.051147717982530594, acc: 0.9859550595283508)
[2025-02-13 04:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48][root][INFO] - Training Epoch: 2/2, step 3740/7134 completed (loss: 0.033602580428123474, acc: 0.9918830990791321)
[2025-02-13 04:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48][root][INFO] - Training Epoch: 2/2, step 3741/7134 completed (loss: 0.04457767307758331, acc: 0.9900142550468445)
[2025-02-13 04:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:49][root][INFO] - Training Epoch: 2/2, step 3742/7134 completed (loss: 0.0459301583468914, acc: 0.9863247871398926)
[2025-02-13 04:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:49][root][INFO] - Training Epoch: 2/2, step 3743/7134 completed (loss: 0.0516992025077343, acc: 0.9867256879806519)
[2025-02-13 04:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:50][root][INFO] - Training Epoch: 2/2, step 3744/7134 completed (loss: 0.0568699985742569, acc: 0.9841897487640381)
[2025-02-13 04:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:50][root][INFO] - Training Epoch: 2/2, step 3745/7134 completed (loss: 0.0678270012140274, acc: 0.9814385175704956)
[2025-02-13 04:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:50][root][INFO] - Training Epoch: 2/2, step 3746/7134 completed (loss: 0.04975370690226555, acc: 0.9869109988212585)
[2025-02-13 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:51][root][INFO] - Training Epoch: 2/2, step 3747/7134 completed (loss: 0.04074052348732948, acc: 0.9897959232330322)
[2025-02-13 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:51][root][INFO] - Training Epoch: 2/2, step 3748/7134 completed (loss: 0.06594729423522949, acc: 0.9823943376541138)
[2025-02-13 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52][root][INFO] - Training Epoch: 2/2, step 3749/7134 completed (loss: 0.07842917740345001, acc: 0.9805097579956055)
[2025-02-13 04:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52][root][INFO] - Training Epoch: 2/2, step 3750/7134 completed (loss: 0.02886766381561756, acc: 0.991631805896759)
[2025-02-13 04:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52][root][INFO] - Training Epoch: 2/2, step 3751/7134 completed (loss: 0.05631810054183006, acc: 0.9772403836250305)
[2025-02-13 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:53][root][INFO] - Training Epoch: 2/2, step 3752/7134 completed (loss: 0.040695954114198685, acc: 0.9850746393203735)
[2025-02-13 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:53][root][INFO] - Training Epoch: 2/2, step 3753/7134 completed (loss: 0.06348750740289688, acc: 0.980567991733551)
[2025-02-13 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54][root][INFO] - Training Epoch: 2/2, step 3754/7134 completed (loss: 0.017066573724150658, acc: 0.9954476356506348)
[2025-02-13 04:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54][root][INFO] - Training Epoch: 2/2, step 3755/7134 completed (loss: 0.03909630328416824, acc: 0.9888682961463928)
[2025-02-13 04:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54][root][INFO] - Training Epoch: 2/2, step 3756/7134 completed (loss: 0.0468587726354599, acc: 0.9916897416114807)
[2025-02-13 04:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55][root][INFO] - Training Epoch: 2/2, step 3757/7134 completed (loss: 0.07883311063051224, acc: 0.9737274050712585)
[2025-02-13 04:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55][root][INFO] - Training Epoch: 2/2, step 3758/7134 completed (loss: 0.033747803419828415, acc: 0.9909909963607788)
[2025-02-13 04:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56][root][INFO] - Training Epoch: 2/2, step 3759/7134 completed (loss: 0.0556834377348423, acc: 0.9814189076423645)
[2025-02-13 04:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56][root][INFO] - Training Epoch: 2/2, step 3760/7134 completed (loss: 0.057583075016736984, acc: 0.9901130199432373)
[2025-02-13 04:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56][root][INFO] - Training Epoch: 2/2, step 3761/7134 completed (loss: 0.019794274121522903, acc: 0.9953632354736328)
[2025-02-13 04:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:57][root][INFO] - Training Epoch: 2/2, step 3762/7134 completed (loss: 0.03141475468873978, acc: 0.9889958500862122)
[2025-02-13 04:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:57][root][INFO] - Training Epoch: 2/2, step 3763/7134 completed (loss: 0.021471889689564705, acc: 0.9922839403152466)
[2025-02-13 04:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:58][root][INFO] - Training Epoch: 2/2, step 3764/7134 completed (loss: 0.054050493985414505, acc: 0.9822161197662354)
[2025-02-13 04:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:58][root][INFO] - Training Epoch: 2/2, step 3765/7134 completed (loss: 0.022153226658701897, acc: 0.9915966391563416)
[2025-02-13 04:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59][root][INFO] - Training Epoch: 2/2, step 3766/7134 completed (loss: 0.06297991424798965, acc: 0.9871794581413269)
[2025-02-13 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59][root][INFO] - Training Epoch: 2/2, step 3767/7134 completed (loss: 0.012660793960094452, acc: 0.996259331703186)
[2025-02-13 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59][root][INFO] - Training Epoch: 2/2, step 3768/7134 completed (loss: 0.012904058210551739, acc: 0.9958506226539612)
[2025-02-13 04:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:00][root][INFO] - Training Epoch: 2/2, step 3769/7134 completed (loss: 0.03512643277645111, acc: 0.989924430847168)
[2025-02-13 04:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:00][root][INFO] - Training Epoch: 2/2, step 3770/7134 completed (loss: 0.028525611385703087, acc: 0.9929378628730774)
[2025-02-13 04:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:01][root][INFO] - Training Epoch: 2/2, step 3771/7134 completed (loss: 0.027248011901974678, acc: 0.9948052167892456)
[2025-02-13 04:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:01][root][INFO] - Training Epoch: 2/2, step 3772/7134 completed (loss: 0.05682586133480072, acc: 0.9835575222969055)
[2025-02-13 04:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:02][root][INFO] - Training Epoch: 2/2, step 3773/7134 completed (loss: 0.03147607296705246, acc: 0.9921875)
[2025-02-13 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:02][root][INFO] - Training Epoch: 2/2, step 3774/7134 completed (loss: 0.025785578414797783, acc: 0.9930394291877747)
[2025-02-13 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03][root][INFO] - Training Epoch: 2/2, step 3775/7134 completed (loss: 0.0284590907394886, acc: 0.9928366541862488)
[2025-02-13 04:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03][root][INFO] - Training Epoch: 2/2, step 3776/7134 completed (loss: 0.0356491394340992, acc: 0.9881481528282166)
[2025-02-13 04:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03][root][INFO] - Training Epoch: 2/2, step 3777/7134 completed (loss: 0.03341596946120262, acc: 0.9924812316894531)
[2025-02-13 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04][root][INFO] - Training Epoch: 2/2, step 3778/7134 completed (loss: 0.05524663254618645, acc: 0.9843527674674988)
[2025-02-13 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04][root][INFO] - Training Epoch: 2/2, step 3779/7134 completed (loss: 0.031529370695352554, acc: 0.9900285005569458)
[2025-02-13 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05][root][INFO] - Training Epoch: 2/2, step 3780/7134 completed (loss: 0.019137175753712654, acc: 0.9899135231971741)
[2025-02-13 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05][root][INFO] - Training Epoch: 2/2, step 3781/7134 completed (loss: 0.05537537485361099, acc: 0.9862825870513916)
[2025-02-13 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05][root][INFO] - Training Epoch: 2/2, step 3782/7134 completed (loss: 0.031688906252384186, acc: 0.9871794581413269)
[2025-02-13 04:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:06][root][INFO] - Training Epoch: 2/2, step 3783/7134 completed (loss: 0.04046071320772171, acc: 0.9858064651489258)
[2025-02-13 04:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:06][root][INFO] - Training Epoch: 2/2, step 3784/7134 completed (loss: 0.05544344708323479, acc: 0.9860896468162537)
[2025-02-13 04:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07][root][INFO] - Training Epoch: 2/2, step 3785/7134 completed (loss: 0.010443096980452538, acc: 0.9968798756599426)
[2025-02-13 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07][root][INFO] - Training Epoch: 2/2, step 3786/7134 completed (loss: 0.03846060484647751, acc: 0.9857697486877441)
[2025-02-13 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07][root][INFO] - Training Epoch: 2/2, step 3787/7134 completed (loss: 0.03596580773591995, acc: 0.9872408509254456)
[2025-02-13 04:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:08][root][INFO] - Training Epoch: 2/2, step 3788/7134 completed (loss: 0.03212806209921837, acc: 0.9905533194541931)
[2025-02-13 04:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:08][root][INFO] - Training Epoch: 2/2, step 3789/7134 completed (loss: 0.03615060821175575, acc: 0.9873417615890503)
[2025-02-13 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09][root][INFO] - Training Epoch: 2/2, step 3790/7134 completed (loss: 0.09120392054319382, acc: 0.980988621711731)
[2025-02-13 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09][root][INFO] - Training Epoch: 2/2, step 3791/7134 completed (loss: 0.015383237041532993, acc: 0.9974193572998047)
[2025-02-13 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10][root][INFO] - Training Epoch: 2/2, step 3792/7134 completed (loss: 0.024086784571409225, acc: 0.9934318661689758)
[2025-02-13 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10][root][INFO] - Training Epoch: 2/2, step 3793/7134 completed (loss: 0.03894602879881859, acc: 0.9878378510475159)
[2025-02-13 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10][root][INFO] - Training Epoch: 2/2, step 3794/7134 completed (loss: 0.026807527989149094, acc: 0.9896238446235657)
[2025-02-13 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:11][root][INFO] - Training Epoch: 2/2, step 3795/7134 completed (loss: 0.032239362597465515, acc: 0.9891566038131714)
[2025-02-13 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:11][root][INFO] - Training Epoch: 2/2, step 3796/7134 completed (loss: 0.030976222828030586, acc: 0.9912060499191284)
[2025-02-13 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12][root][INFO] - Training Epoch: 2/2, step 3797/7134 completed (loss: 0.017909590154886246, acc: 0.9921773076057434)
[2025-02-13 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12][root][INFO] - Training Epoch: 2/2, step 3798/7134 completed (loss: 0.04557296261191368, acc: 0.9885057210922241)
[2025-02-13 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13][root][INFO] - Training Epoch: 2/2, step 3799/7134 completed (loss: 0.0421181321144104, acc: 0.9871299862861633)
[2025-02-13 04:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13][root][INFO] - Training Epoch: 2/2, step 3800/7134 completed (loss: 0.03794444352388382, acc: 0.9908257126808167)
[2025-02-13 04:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13][root][INFO] - Training Epoch: 2/2, step 3801/7134 completed (loss: 0.018090680241584778, acc: 0.9950617551803589)
[2025-02-13 04:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:14][root][INFO] - Training Epoch: 2/2, step 3802/7134 completed (loss: 0.027363283559679985, acc: 0.9929078221321106)
[2025-02-13 04:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:14][root][INFO] - Training Epoch: 2/2, step 3803/7134 completed (loss: 0.030314546078443527, acc: 0.9901960492134094)
[2025-02-13 04:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15][root][INFO] - Training Epoch: 2/2, step 3804/7134 completed (loss: 0.026174770668148994, acc: 0.9922239780426025)
[2025-02-13 04:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15][root][INFO] - Training Epoch: 2/2, step 3805/7134 completed (loss: 0.014193630777299404, acc: 0.9947643876075745)
[2025-02-13 04:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:16][root][INFO] - Training Epoch: 2/2, step 3806/7134 completed (loss: 0.03701987490057945, acc: 0.9921104311943054)
[2025-02-13 04:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:16][root][INFO] - Training Epoch: 2/2, step 3807/7134 completed (loss: 0.010953333228826523, acc: 0.9970760345458984)
[2025-02-13 04:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17][root][INFO] - Training Epoch: 2/2, step 3808/7134 completed (loss: 0.03664674237370491, acc: 0.9924050569534302)
[2025-02-13 04:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17][root][INFO] - Training Epoch: 2/2, step 3809/7134 completed (loss: 0.05442536994814873, acc: 0.9809644818305969)
[2025-02-13 04:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17][root][INFO] - Training Epoch: 2/2, step 3810/7134 completed (loss: 0.02323242276906967, acc: 0.9910714030265808)
[2025-02-13 04:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:18][root][INFO] - Training Epoch: 2/2, step 3811/7134 completed (loss: 0.04175474867224693, acc: 0.9875862002372742)
[2025-02-13 04:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:18][root][INFO] - Training Epoch: 2/2, step 3812/7134 completed (loss: 0.05502030625939369, acc: 0.9777015447616577)
[2025-02-13 04:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19][root][INFO] - Training Epoch: 2/2, step 3813/7134 completed (loss: 0.05076327919960022, acc: 0.9854881167411804)
[2025-02-13 04:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19][root][INFO] - Training Epoch: 2/2, step 3814/7134 completed (loss: 0.024267280474305153, acc: 0.9919571280479431)
[2025-02-13 04:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19][root][INFO] - Training Epoch: 2/2, step 3815/7134 completed (loss: 0.0319078154861927, acc: 0.9866666793823242)
[2025-02-13 04:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:20][root][INFO] - Training Epoch: 2/2, step 3816/7134 completed (loss: 0.024480069056153297, acc: 0.9936908483505249)
[2025-02-13 04:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:20][root][INFO] - Training Epoch: 2/2, step 3817/7134 completed (loss: 0.04309965670108795, acc: 0.9880095720291138)
[2025-02-13 04:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21][root][INFO] - Training Epoch: 2/2, step 3818/7134 completed (loss: 0.07175153493881226, acc: 0.9858585596084595)
[2025-02-13 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21][root][INFO] - Training Epoch: 2/2, step 3819/7134 completed (loss: 0.05738409236073494, acc: 0.9759036302566528)
[2025-02-13 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21][root][INFO] - Training Epoch: 2/2, step 3820/7134 completed (loss: 0.03394545242190361, acc: 0.9857482314109802)
[2025-02-13 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:22][root][INFO] - Training Epoch: 2/2, step 3821/7134 completed (loss: 0.02224431373178959, acc: 0.989847719669342)
[2025-02-13 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:22][root][INFO] - Training Epoch: 2/2, step 3822/7134 completed (loss: 0.03731325641274452, acc: 0.9882121682167053)
[2025-02-13 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23][root][INFO] - Training Epoch: 2/2, step 3823/7134 completed (loss: 0.014080798253417015, acc: 0.9972337484359741)
[2025-02-13 04:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23][root][INFO] - Training Epoch: 2/2, step 3824/7134 completed (loss: 0.06413496285676956, acc: 0.9848229289054871)
[2025-02-13 04:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23][root][INFO] - Training Epoch: 2/2, step 3825/7134 completed (loss: 0.023110080510377884, acc: 0.9940119981765747)
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24][root][INFO] - Training Epoch: 2/2, step 3826/7134 completed (loss: 0.04973237216472626, acc: 0.9838235378265381)
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24][root][INFO] - Training Epoch: 2/2, step 3827/7134 completed (loss: 0.025592461228370667, acc: 0.994397759437561)
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:25][root][INFO] - Training Epoch: 2/2, step 3828/7134 completed (loss: 0.03359002247452736, acc: 0.9893898963928223)
[2025-02-13 04:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:25][root][INFO] - Training Epoch: 2/2, step 3829/7134 completed (loss: 0.03141709417104721, acc: 0.993966817855835)
[2025-02-13 04:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26][root][INFO] - Training Epoch: 2/2, step 3830/7134 completed (loss: 0.04700116440653801, acc: 0.9882746934890747)
[2025-02-13 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26][root][INFO] - Training Epoch: 2/2, step 3831/7134 completed (loss: 0.03682634234428406, acc: 0.9909909963607788)
[2025-02-13 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26][root][INFO] - Training Epoch: 2/2, step 3832/7134 completed (loss: 0.030060116201639175, acc: 0.994106113910675)
[2025-02-13 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:27][root][INFO] - Training Epoch: 2/2, step 3833/7134 completed (loss: 0.014736311510205269, acc: 0.9938119053840637)
[2025-02-13 04:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:27][root][INFO] - Training Epoch: 2/2, step 3834/7134 completed (loss: 0.008223491720855236, acc: 0.998701274394989)
[2025-02-13 04:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28][root][INFO] - Training Epoch: 2/2, step 3835/7134 completed (loss: 0.010600765235722065, acc: 0.9951456189155579)
[2025-02-13 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28][root][INFO] - Training Epoch: 2/2, step 3836/7134 completed (loss: 0.047323692589998245, acc: 0.991304337978363)
[2025-02-13 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28][root][INFO] - Training Epoch: 2/2, step 3837/7134 completed (loss: 0.028979580849409103, acc: 0.9956896305084229)
[2025-02-13 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:29][root][INFO] - Training Epoch: 2/2, step 3838/7134 completed (loss: 0.006704631261527538, acc: 0.9986720085144043)
[2025-02-13 04:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:29][root][INFO] - Training Epoch: 2/2, step 3839/7134 completed (loss: 0.0306924507021904, acc: 0.9930939078330994)
[2025-02-13 04:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30][root][INFO] - Training Epoch: 2/2, step 3840/7134 completed (loss: 0.014320222660899162, acc: 0.9946452379226685)
[2025-02-13 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30][root][INFO] - Training Epoch: 2/2, step 3841/7134 completed (loss: 0.04151763767004013, acc: 0.9877913594245911)
[2025-02-13 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30][root][INFO] - Training Epoch: 2/2, step 3842/7134 completed (loss: 0.0036464182194322348, acc: 1.0)
[2025-02-13 04:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:31][root][INFO] - Training Epoch: 2/2, step 3843/7134 completed (loss: 0.021323274821043015, acc: 0.9937434792518616)
[2025-02-13 04:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:31][root][INFO] - Training Epoch: 2/2, step 3844/7134 completed (loss: 0.022142628207802773, acc: 0.9961880445480347)
[2025-02-13 04:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32][root][INFO] - Training Epoch: 2/2, step 3845/7134 completed (loss: 0.03349422663450241, acc: 0.9915825128555298)
[2025-02-13 04:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32][root][INFO] - Training Epoch: 2/2, step 3846/7134 completed (loss: 0.024349398910999298, acc: 0.9966044425964355)
[2025-02-13 04:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32][root][INFO] - Training Epoch: 2/2, step 3847/7134 completed (loss: 0.016475653275847435, acc: 0.9968000054359436)
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33][root][INFO] - Training Epoch: 2/2, step 3848/7134 completed (loss: 0.023495616391301155, acc: 0.9974554777145386)
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33][root][INFO] - Training Epoch: 2/2, step 3849/7134 completed (loss: 0.05520032346248627, acc: 0.987522304058075)
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:34][root][INFO] - Training Epoch: 2/2, step 3850/7134 completed (loss: 0.06101808324456215, acc: 0.9877750873565674)
[2025-02-13 04:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:34][root][INFO] - Training Epoch: 2/2, step 3851/7134 completed (loss: 0.04476584121584892, acc: 0.9874476790428162)
[2025-02-13 04:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35][root][INFO] - Training Epoch: 2/2, step 3852/7134 completed (loss: 0.018888121470808983, acc: 0.9935691356658936)
[2025-02-13 04:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35][root][INFO] - Training Epoch: 2/2, step 3853/7134 completed (loss: 0.04540984332561493, acc: 0.9859353303909302)
[2025-02-13 04:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35][root][INFO] - Training Epoch: 2/2, step 3854/7134 completed (loss: 0.0165353175252676, acc: 0.9957864880561829)
[2025-02-13 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:36][root][INFO] - Training Epoch: 2/2, step 3855/7134 completed (loss: 0.03910830616950989, acc: 0.9912717938423157)
[2025-02-13 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:36][root][INFO] - Training Epoch: 2/2, step 3856/7134 completed (loss: 0.04162289947271347, acc: 0.9888424277305603)
[2025-02-13 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:37][root][INFO] - Training Epoch: 2/2, step 3857/7134 completed (loss: 0.025698808953166008, acc: 0.9919614195823669)
[2025-02-13 04:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:37][root][INFO] - Training Epoch: 2/2, step 3858/7134 completed (loss: 0.05949537083506584, acc: 0.9858356714248657)
[2025-02-13 04:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38][root][INFO] - Training Epoch: 2/2, step 3859/7134 completed (loss: 0.039811909198760986, acc: 0.9923664331436157)
[2025-02-13 04:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38][root][INFO] - Training Epoch: 2/2, step 3860/7134 completed (loss: 0.03287115320563316, acc: 0.9910614490509033)
[2025-02-13 04:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38][root][INFO] - Training Epoch: 2/2, step 3861/7134 completed (loss: 0.03037281334400177, acc: 0.9877551198005676)
[2025-02-13 04:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:39][root][INFO] - Training Epoch: 2/2, step 3862/7134 completed (loss: 0.04209394007921219, acc: 0.9901477694511414)
[2025-02-13 04:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:39][root][INFO] - Training Epoch: 2/2, step 3863/7134 completed (loss: 0.018632732331752777, acc: 0.9938271641731262)
[2025-02-13 04:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:40][root][INFO] - Training Epoch: 2/2, step 3864/7134 completed (loss: 0.0289909765124321, acc: 0.9960835576057434)
[2025-02-13 04:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:40][root][INFO] - Training Epoch: 2/2, step 3865/7134 completed (loss: 0.02618476003408432, acc: 0.9920318722724915)
[2025-02-13 04:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41][root][INFO] - Training Epoch: 2/2, step 3866/7134 completed (loss: 0.04714567959308624, acc: 0.9877899885177612)
[2025-02-13 04:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41][root][INFO] - Training Epoch: 2/2, step 3867/7134 completed (loss: 0.03771314397454262, acc: 0.9867424368858337)
[2025-02-13 04:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41][root][INFO] - Training Epoch: 2/2, step 3868/7134 completed (loss: 0.01953708752989769, acc: 0.9966158866882324)
[2025-02-13 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:42][root][INFO] - Training Epoch: 2/2, step 3869/7134 completed (loss: 0.025115132331848145, acc: 0.9878970980644226)
[2025-02-13 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:42][root][INFO] - Training Epoch: 2/2, step 3870/7134 completed (loss: 0.0649200975894928, acc: 0.9861111044883728)
[2025-02-13 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43][root][INFO] - Training Epoch: 2/2, step 3871/7134 completed (loss: 0.03811916708946228, acc: 0.9862843155860901)
[2025-02-13 04:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43][root][INFO] - Training Epoch: 2/2, step 3872/7134 completed (loss: 0.04267086088657379, acc: 0.9903640151023865)
[2025-02-13 04:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44][root][INFO] - Training Epoch: 2/2, step 3873/7134 completed (loss: 0.026996081694960594, acc: 0.9923273921012878)
[2025-02-13 04:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44][root][INFO] - Training Epoch: 2/2, step 3874/7134 completed (loss: 0.04341394826769829, acc: 0.9889349937438965)
[2025-02-13 04:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44][root][INFO] - Training Epoch: 2/2, step 3875/7134 completed (loss: 0.05541738495230675, acc: 0.9827127456665039)
[2025-02-13 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:45][root][INFO] - Training Epoch: 2/2, step 3876/7134 completed (loss: 0.01634257473051548, acc: 0.9924952983856201)
[2025-02-13 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:45][root][INFO] - Training Epoch: 2/2, step 3877/7134 completed (loss: 0.055165160447359085, acc: 0.9895561337471008)
[2025-02-13 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:46][root][INFO] - Training Epoch: 2/2, step 3878/7134 completed (loss: 0.07700688391923904, acc: 0.9878706336021423)
[2025-02-13 04:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:46][root][INFO] - Training Epoch: 2/2, step 3879/7134 completed (loss: 0.02007180266082287, acc: 0.9944547414779663)
[2025-02-13 04:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47][root][INFO] - Training Epoch: 2/2, step 3880/7134 completed (loss: 0.10199137032032013, acc: 0.9794420003890991)
[2025-02-13 04:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47][root][INFO] - Training Epoch: 2/2, step 3881/7134 completed (loss: 0.09836553782224655, acc: 0.9760000109672546)
[2025-02-13 04:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47][root][INFO] - Training Epoch: 2/2, step 3882/7134 completed (loss: 0.05205236002802849, acc: 0.9850000143051147)
[2025-02-13 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:48][root][INFO] - Training Epoch: 2/2, step 3883/7134 completed (loss: 0.011605296283960342, acc: 0.9986013770103455)
[2025-02-13 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:48][root][INFO] - Training Epoch: 2/2, step 3884/7134 completed (loss: 0.019788261502981186, acc: 0.9931972622871399)
[2025-02-13 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:49][root][INFO] - Training Epoch: 2/2, step 3885/7134 completed (loss: 0.025983572006225586, acc: 0.9921671152114868)
[2025-02-13 04:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:49][root][INFO] - Training Epoch: 2/2, step 3886/7134 completed (loss: 0.030503658577799797, acc: 0.990338146686554)
[2025-02-13 04:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50][root][INFO] - Training Epoch: 2/2, step 3887/7134 completed (loss: 0.03090623952448368, acc: 0.9954751133918762)
[2025-02-13 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50][root][INFO] - Training Epoch: 2/2, step 3888/7134 completed (loss: 0.028402607887983322, acc: 0.9887499809265137)
[2025-02-13 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50][root][INFO] - Training Epoch: 2/2, step 3889/7134 completed (loss: 0.02199527435004711, acc: 0.992337167263031)
[2025-02-13 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:51][root][INFO] - Training Epoch: 2/2, step 3890/7134 completed (loss: 0.019129043444991112, acc: 0.9939831495285034)
[2025-02-13 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:51][root][INFO] - Training Epoch: 2/2, step 3891/7134 completed (loss: 0.012216457165777683, acc: 0.9973856210708618)
[2025-02-13 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:52][root][INFO] - Training Epoch: 2/2, step 3892/7134 completed (loss: 0.023080220445990562, acc: 0.9907692074775696)
[2025-02-13 04:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:52][root][INFO] - Training Epoch: 2/2, step 3893/7134 completed (loss: 0.06917177140712738, acc: 0.9780927896499634)
[2025-02-13 04:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53][root][INFO] - Training Epoch: 2/2, step 3894/7134 completed (loss: 0.012517708353698254, acc: 0.9985954761505127)
[2025-02-13 04:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53][root][INFO] - Training Epoch: 2/2, step 3895/7134 completed (loss: 0.024521855637431145, acc: 0.9922580718994141)
[2025-02-13 04:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53][root][INFO] - Training Epoch: 2/2, step 3896/7134 completed (loss: 0.012626857496798038, acc: 0.9977426528930664)
[2025-02-13 04:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:54][root][INFO] - Training Epoch: 2/2, step 3897/7134 completed (loss: 0.05256974697113037, acc: 0.98591548204422)
[2025-02-13 04:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:54][root][INFO] - Training Epoch: 2/2, step 3898/7134 completed (loss: 0.032339613884687424, acc: 0.9885495901107788)
[2025-02-13 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:55][root][INFO] - Training Epoch: 2/2, step 3899/7134 completed (loss: 0.02310427464544773, acc: 0.9909443855285645)
[2025-02-13 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:55][root][INFO] - Training Epoch: 2/2, step 3900/7134 completed (loss: 0.036862749606370926, acc: 0.9837775230407715)
[2025-02-13 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:56][root][INFO] - Training Epoch: 2/2, step 3901/7134 completed (loss: 0.08077497035264969, acc: 0.9811320900917053)
[2025-02-13 04:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:56][root][INFO] - Training Epoch: 2/2, step 3902/7134 completed (loss: 0.08011356741189957, acc: 0.9827814698219299)
[2025-02-13 04:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57][root][INFO] - Training Epoch: 2/2, step 3903/7134 completed (loss: 0.08140178769826889, acc: 0.9758713245391846)
[2025-02-13 04:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57][root][INFO] - Training Epoch: 2/2, step 3904/7134 completed (loss: 0.033982351422309875, acc: 0.9893454909324646)
[2025-02-13 04:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57][root][INFO] - Training Epoch: 2/2, step 3905/7134 completed (loss: 0.020176950842142105, acc: 0.9929478168487549)
[2025-02-13 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58][root][INFO] - Training Epoch: 2/2, step 3906/7134 completed (loss: 0.011493697762489319, acc: 0.9985632300376892)
[2025-02-13 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58][root][INFO] - Training Epoch: 2/2, step 3907/7134 completed (loss: 0.03201805427670479, acc: 0.9922879338264465)
[2025-02-13 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:59][root][INFO] - Training Epoch: 2/2, step 3908/7134 completed (loss: 0.059232015162706375, acc: 0.9816232919692993)
[2025-02-13 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:59][root][INFO] - Training Epoch: 2/2, step 3909/7134 completed (loss: 0.03728692978620529, acc: 0.9886914491653442)
[2025-02-13 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:00][root][INFO] - Training Epoch: 2/2, step 3910/7134 completed (loss: 0.045736342668533325, acc: 0.9847009778022766)
[2025-02-13 04:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:00][root][INFO] - Training Epoch: 2/2, step 3911/7134 completed (loss: 0.03588911145925522, acc: 0.9902912378311157)
[2025-02-13 04:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:00][root][INFO] - Training Epoch: 2/2, step 3912/7134 completed (loss: 0.012068180367350578, acc: 0.9940476417541504)
[2025-02-13 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:01][root][INFO] - Training Epoch: 2/2, step 3913/7134 completed (loss: 0.03996410593390465, acc: 0.9910256266593933)
[2025-02-13 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:01][root][INFO] - Training Epoch: 2/2, step 3914/7134 completed (loss: 0.026289697736501694, acc: 0.9932659864425659)
[2025-02-13 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:02][root][INFO] - Training Epoch: 2/2, step 3915/7134 completed (loss: 0.017104703933000565, acc: 0.994358241558075)
[2025-02-13 04:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:02][root][INFO] - Training Epoch: 2/2, step 3916/7134 completed (loss: 0.02041848935186863, acc: 0.993630588054657)
[2025-02-13 04:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03][root][INFO] - Training Epoch: 2/2, step 3917/7134 completed (loss: 0.011167739517986774, acc: 0.9975932836532593)
[2025-02-13 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03][root][INFO] - Training Epoch: 2/2, step 3918/7134 completed (loss: 0.0275635477155447, acc: 0.9947916865348816)
[2025-02-13 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03][root][INFO] - Training Epoch: 2/2, step 3919/7134 completed (loss: 0.07267250120639801, acc: 0.9869706630706787)
[2025-02-13 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:04][root][INFO] - Training Epoch: 2/2, step 3920/7134 completed (loss: 0.025315698236227036, acc: 0.9890965819358826)
[2025-02-13 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:04][root][INFO] - Training Epoch: 2/2, step 3921/7134 completed (loss: 0.0336441844701767, acc: 0.9879999756813049)
[2025-02-13 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:05][root][INFO] - Training Epoch: 2/2, step 3922/7134 completed (loss: 0.07539742439985275, acc: 0.9769392013549805)
[2025-02-13 04:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:05][root][INFO] - Training Epoch: 2/2, step 3923/7134 completed (loss: 0.07780783623456955, acc: 0.9845890402793884)
[2025-02-13 04:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06][root][INFO] - Training Epoch: 2/2, step 3924/7134 completed (loss: 0.0363052673637867, acc: 0.9870689511299133)
[2025-02-13 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06][root][INFO] - Training Epoch: 2/2, step 3925/7134 completed (loss: 0.0383966900408268, acc: 0.985358715057373)
[2025-02-13 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06][root][INFO] - Training Epoch: 2/2, step 3926/7134 completed (loss: 0.04665437713265419, acc: 0.9884892106056213)
[2025-02-13 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:07][root][INFO] - Training Epoch: 2/2, step 3927/7134 completed (loss: 0.031185444444417953, acc: 0.9876203536987305)
[2025-02-13 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:07][root][INFO] - Training Epoch: 2/2, step 3928/7134 completed (loss: 0.05831127241253853, acc: 0.982758641242981)
[2025-02-13 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08][root][INFO] - Training Epoch: 2/2, step 3929/7134 completed (loss: 0.028652813285589218, acc: 0.9936407208442688)
[2025-02-13 04:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08][root][INFO] - Training Epoch: 2/2, step 3930/7134 completed (loss: 0.03392139822244644, acc: 0.9873949289321899)
[2025-02-13 04:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08][root][INFO] - Training Epoch: 2/2, step 3931/7134 completed (loss: 0.01961309276521206, acc: 0.9925261735916138)
[2025-02-13 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:09][root][INFO] - Training Epoch: 2/2, step 3932/7134 completed (loss: 0.03766774758696556, acc: 0.9841897487640381)
[2025-02-13 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:09][root][INFO] - Training Epoch: 2/2, step 3933/7134 completed (loss: 0.03184162825345993, acc: 0.9901315569877625)
[2025-02-13 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10][root][INFO] - Training Epoch: 2/2, step 3934/7134 completed (loss: 0.019373901188373566, acc: 0.9929742217063904)
[2025-02-13 04:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10][root][INFO] - Training Epoch: 2/2, step 3935/7134 completed (loss: 0.006940222345292568, acc: 0.9964664578437805)
[2025-02-13 04:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10][root][INFO] - Training Epoch: 2/2, step 3936/7134 completed (loss: 0.04335923492908478, acc: 0.9886792302131653)
[2025-02-13 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:11][root][INFO] - Training Epoch: 2/2, step 3937/7134 completed (loss: 0.0340004488825798, acc: 0.9877551198005676)
[2025-02-13 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:11][root][INFO] - Training Epoch: 2/2, step 3938/7134 completed (loss: 0.0158369243144989, acc: 0.9954751133918762)
[2025-02-13 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12][root][INFO] - Training Epoch: 2/2, step 3939/7134 completed (loss: 0.05888986960053444, acc: 0.9813874959945679)
[2025-02-13 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12][root][INFO] - Training Epoch: 2/2, step 3940/7134 completed (loss: 0.03529008850455284, acc: 0.9899497628211975)
[2025-02-13 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12][root][INFO] - Training Epoch: 2/2, step 3941/7134 completed (loss: 0.03118755668401718, acc: 0.9908536672592163)
[2025-02-13 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13][root][INFO] - Training Epoch: 2/2, step 3942/7134 completed (loss: 0.025199759751558304, acc: 0.9918032884597778)
[2025-02-13 04:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13][root][INFO] - Training Epoch: 2/2, step 3943/7134 completed (loss: 0.012658313848078251, acc: 0.9961389899253845)
[2025-02-13 04:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13][root][INFO] - Training Epoch: 2/2, step 3944/7134 completed (loss: 0.023887870833277702, acc: 0.9944649338722229)
[2025-02-13 04:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14][root][INFO] - Training Epoch: 2/2, step 3945/7134 completed (loss: 0.023653538897633553, acc: 0.9908536672592163)
[2025-02-13 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14][root][INFO] - Training Epoch: 2/2, step 3946/7134 completed (loss: 0.020406346768140793, acc: 0.9958246350288391)
[2025-02-13 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14][root][INFO] - Training Epoch: 2/2, step 3947/7134 completed (loss: 0.018922986462712288, acc: 0.9948186278343201)
[2025-02-13 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:15][root][INFO] - Training Epoch: 2/2, step 3948/7134 completed (loss: 0.02301221340894699, acc: 0.9899497628211975)
[2025-02-13 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:15][root][INFO] - Training Epoch: 2/2, step 3949/7134 completed (loss: 0.031690601259469986, acc: 0.9906716346740723)
[2025-02-13 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][root][INFO] - Training Epoch: 2/2, step 3950/7134 completed (loss: 0.03825177997350693, acc: 0.9910714030265808)
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][root][INFO] - Training Epoch: 2/2, step 3951/7134 completed (loss: 0.022127758711576462, acc: 0.9924812316894531)
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][root][INFO] - Training Epoch: 2/2, step 3952/7134 completed (loss: 0.015331706032156944, acc: 0.9948717951774597)
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17][root][INFO] - Training Epoch: 2/2, step 3953/7134 completed (loss: 0.03422720730304718, acc: 0.992409884929657)
[2025-02-13 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17][root][INFO] - Training Epoch: 2/2, step 3954/7134 completed (loss: 0.04368825629353523, acc: 0.9896907210350037)
[2025-02-13 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17][root][INFO] - Training Epoch: 2/2, step 3955/7134 completed (loss: 0.030824679881334305, acc: 0.9898819327354431)
[2025-02-13 04:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:18][root][INFO] - Training Epoch: 2/2, step 3956/7134 completed (loss: 0.016578055918216705, acc: 0.9968000054359436)
[2025-02-13 04:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:18][root][INFO] - Training Epoch: 2/2, step 3957/7134 completed (loss: 0.023459451273083687, acc: 0.9887387156486511)
[2025-02-13 04:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][root][INFO] - Training Epoch: 2/2, step 3958/7134 completed (loss: 0.05500931665301323, acc: 0.9807692170143127)
[2025-02-13 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][root][INFO] - Training Epoch: 2/2, step 3959/7134 completed (loss: 0.012329401448369026, acc: 0.997802197933197)
[2025-02-13 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][root][INFO] - Training Epoch: 2/2, step 3960/7134 completed (loss: 0.027398081496357918, acc: 0.9914529919624329)
[2025-02-13 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:20][root][INFO] - Training Epoch: 2/2, step 3961/7134 completed (loss: 0.008573300205171108, acc: 0.995502233505249)
[2025-02-13 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:20][root][INFO] - Training Epoch: 2/2, step 3962/7134 completed (loss: 0.016958607360720634, acc: 0.9935275316238403)
[2025-02-13 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21][root][INFO] - Training Epoch: 2/2, step 3963/7134 completed (loss: 0.06370587646961212, acc: 0.9897360801696777)
[2025-02-13 04:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21][root][INFO] - Training Epoch: 2/2, step 3964/7134 completed (loss: 0.048054587095975876, acc: 0.9888888597488403)
[2025-02-13 04:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21][root][INFO] - Training Epoch: 2/2, step 3965/7134 completed (loss: 0.03250043839216232, acc: 0.995708167552948)
[2025-02-13 04:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:22][root][INFO] - Training Epoch: 2/2, step 3966/7134 completed (loss: 0.030557915568351746, acc: 0.987364649772644)
[2025-02-13 04:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:22][root][INFO] - Training Epoch: 2/2, step 3967/7134 completed (loss: 0.09144774079322815, acc: 0.9757365584373474)
[2025-02-13 04:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23][root][INFO] - Training Epoch: 2/2, step 3968/7134 completed (loss: 0.019960524514317513, acc: 0.9960988163948059)
[2025-02-13 04:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23][root][INFO] - Training Epoch: 2/2, step 3969/7134 completed (loss: 0.04028775542974472, acc: 0.981675386428833)
[2025-02-13 04:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24][root][INFO] - Training Epoch: 2/2, step 3970/7134 completed (loss: 0.05060102045536041, acc: 0.9899280667304993)
[2025-02-13 04:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24][root][INFO] - Training Epoch: 2/2, step 3971/7134 completed (loss: 0.11314569413661957, acc: 0.9744245409965515)
[2025-02-13 04:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24][root][INFO] - Training Epoch: 2/2, step 3972/7134 completed (loss: 0.1423034965991974, acc: 0.969072163105011)
[2025-02-13 04:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25][root][INFO] - Training Epoch: 2/2, step 3973/7134 completed (loss: 0.041759610176086426, acc: 0.9879102110862732)
[2025-02-13 04:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25][root][INFO] - Training Epoch: 2/2, step 3974/7134 completed (loss: 0.08483096212148666, acc: 0.9783197641372681)
[2025-02-13 04:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26][root][INFO] - Training Epoch: 2/2, step 3975/7134 completed (loss: 0.026675615459680557, acc: 0.9939576983451843)
[2025-02-13 04:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26][root][INFO] - Training Epoch: 2/2, step 3976/7134 completed (loss: 0.11846067756414413, acc: 0.976401150226593)
[2025-02-13 04:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26][root][INFO] - Training Epoch: 2/2, step 3977/7134 completed (loss: 0.018798254430294037, acc: 0.9965217113494873)
[2025-02-13 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:27][root][INFO] - Training Epoch: 2/2, step 3978/7134 completed (loss: 0.023505987599492073, acc: 0.9948253631591797)
[2025-02-13 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:27][root][INFO] - Training Epoch: 2/2, step 3979/7134 completed (loss: 0.04270942881703377, acc: 0.9872495532035828)
[2025-02-13 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:28][root][INFO] - Training Epoch: 2/2, step 3980/7134 completed (loss: 0.029409855604171753, acc: 0.9893292784690857)
[2025-02-13 04:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:28][root][INFO] - Training Epoch: 2/2, step 3981/7134 completed (loss: 0.06849268078804016, acc: 0.9785932898521423)
[2025-02-13 04:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29][root][INFO] - Training Epoch: 2/2, step 3982/7134 completed (loss: 0.08082078397274017, acc: 0.9784792065620422)
[2025-02-13 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29][root][INFO] - Training Epoch: 2/2, step 3983/7134 completed (loss: 0.23664672672748566, acc: 0.9489361643791199)
[2025-02-13 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29][root][INFO] - Training Epoch: 2/2, step 3984/7134 completed (loss: 0.07412252575159073, acc: 0.9818181991577148)
[2025-02-13 04:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:30][root][INFO] - Training Epoch: 2/2, step 3985/7134 completed (loss: 0.025374574586749077, acc: 0.9964349269866943)
[2025-02-13 04:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:30][root][INFO] - Training Epoch: 2/2, step 3986/7134 completed (loss: 0.15513180196285248, acc: 0.9636363387107849)
[2025-02-13 04:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31][root][INFO] - Training Epoch: 2/2, step 3987/7134 completed (loss: 0.0576186366379261, acc: 0.9865642786026001)
[2025-02-13 04:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31][root][INFO] - Training Epoch: 2/2, step 3988/7134 completed (loss: 0.0594908706843853, acc: 0.983565092086792)
[2025-02-13 04:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31][root][INFO] - Training Epoch: 2/2, step 3989/7134 completed (loss: 0.1551949679851532, acc: 0.9658848643302917)
[2025-02-13 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:32][root][INFO] - Training Epoch: 2/2, step 3990/7134 completed (loss: 0.034591082483530045, acc: 0.9900497794151306)
[2025-02-13 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:32][root][INFO] - Training Epoch: 2/2, step 3991/7134 completed (loss: 0.03780073672533035, acc: 0.9933993220329285)
[2025-02-13 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33][root][INFO] - Training Epoch: 2/2, step 3992/7134 completed (loss: 0.05552074685692787, acc: 0.9864864945411682)
[2025-02-13 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33][root][INFO] - Training Epoch: 2/2, step 3993/7134 completed (loss: 0.05563891679048538, acc: 0.9816513657569885)
[2025-02-13 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33][root][INFO] - Training Epoch: 2/2, step 3994/7134 completed (loss: 0.053461283445358276, acc: 0.9904761910438538)
[2025-02-13 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34][root][INFO] - Training Epoch: 2/2, step 3995/7134 completed (loss: 0.10551555454730988, acc: 0.9806201457977295)
[2025-02-13 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34][root][INFO] - Training Epoch: 2/2, step 3996/7134 completed (loss: 0.04864040017127991, acc: 0.9856262803077698)
[2025-02-13 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34][root][INFO] - Training Epoch: 2/2, step 3997/7134 completed (loss: 0.11969154328107834, acc: 0.9736263751983643)
[2025-02-13 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:35][root][INFO] - Training Epoch: 2/2, step 3998/7134 completed (loss: 0.07863976806402206, acc: 0.9759299755096436)
[2025-02-13 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:35][root][INFO] - Training Epoch: 2/2, step 3999/7134 completed (loss: 0.04518893361091614, acc: 0.9840319156646729)
[2025-02-13 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36][root][INFO] - Training Epoch: 2/2, step 4000/7134 completed (loss: 0.03782355785369873, acc: 0.9903475046157837)
[2025-02-13 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36][root][INFO] - Training Epoch: 2/2, step 4001/7134 completed (loss: 0.07309841364622116, acc: 0.9781312346458435)
[2025-02-13 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36][root][INFO] - Training Epoch: 2/2, step 4002/7134 completed (loss: 0.08952835202217102, acc: 0.983132541179657)
[2025-02-13 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37][root][INFO] - Training Epoch: 2/2, step 4003/7134 completed (loss: 0.09754204005002975, acc: 0.9760147333145142)
[2025-02-13 04:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37][root][INFO] - Training Epoch: 2/2, step 4004/7134 completed (loss: 0.03229182958602905, acc: 0.9930070042610168)
[2025-02-13 04:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37][root][INFO] - Training Epoch: 2/2, step 4005/7134 completed (loss: 0.028328606858849525, acc: 0.9937205910682678)
[2025-02-13 04:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38][root][INFO] - Training Epoch: 2/2, step 4006/7134 completed (loss: 0.02361341379582882, acc: 0.9916753172874451)
[2025-02-13 04:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38][root][INFO] - Training Epoch: 2/2, step 4007/7134 completed (loss: 0.028234615921974182, acc: 0.9925037622451782)
[2025-02-13 04:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:39][root][INFO] - Training Epoch: 2/2, step 4008/7134 completed (loss: 0.01519104279577732, acc: 0.9957716464996338)
[2025-02-13 04:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:39][root][INFO] - Training Epoch: 2/2, step 4009/7134 completed (loss: 0.013012059032917023, acc: 0.9960052967071533)
[2025-02-13 04:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:40][root][INFO] - Training Epoch: 2/2, step 4010/7134 completed (loss: 0.010741475969552994, acc: 1.0)
[2025-02-13 04:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:40][root][INFO] - Training Epoch: 2/2, step 4011/7134 completed (loss: 0.025289366021752357, acc: 0.9946879148483276)
[2025-02-13 04:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41][root][INFO] - Training Epoch: 2/2, step 4012/7134 completed (loss: 0.008322334848344326, acc: 0.9985875487327576)
[2025-02-13 04:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41][root][INFO] - Training Epoch: 2/2, step 4013/7134 completed (loss: 0.01637245900928974, acc: 0.9951768517494202)
[2025-02-13 04:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41][root][INFO] - Training Epoch: 2/2, step 4014/7134 completed (loss: 0.008604884147644043, acc: 0.9968652129173279)
[2025-02-13 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42][root][INFO] - Training Epoch: 2/2, step 4015/7134 completed (loss: 0.01787855289876461, acc: 0.9938555955886841)
[2025-02-13 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42][root][INFO] - Training Epoch: 2/2, step 4016/7134 completed (loss: 0.02875811792910099, acc: 0.9910714030265808)
[2025-02-13 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:43][root][INFO] - Training Epoch: 2/2, step 4017/7134 completed (loss: 0.02326279878616333, acc: 0.9953810572624207)
[2025-02-13 04:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:43][root][INFO] - Training Epoch: 2/2, step 4018/7134 completed (loss: 0.022431667894124985, acc: 0.996363639831543)
[2025-02-13 04:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44][root][INFO] - Training Epoch: 2/2, step 4019/7134 completed (loss: 0.047936685383319855, acc: 0.9873096346855164)
[2025-02-13 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44][root][INFO] - Training Epoch: 2/2, step 4020/7134 completed (loss: 0.036368753761053085, acc: 0.9822404384613037)
[2025-02-13 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44][root][INFO] - Training Epoch: 2/2, step 4021/7134 completed (loss: 0.01285109668970108, acc: 0.9946523904800415)
[2025-02-13 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45][root][INFO] - Training Epoch: 2/2, step 4022/7134 completed (loss: 0.02219581790268421, acc: 0.991411030292511)
[2025-02-13 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45][root][INFO] - Training Epoch: 2/2, step 4023/7134 completed (loss: 0.04118923470377922, acc: 0.9899371266365051)
[2025-02-13 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:46][root][INFO] - Training Epoch: 2/2, step 4024/7134 completed (loss: 0.031117746606469154, acc: 0.9895012974739075)
[2025-02-13 04:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:46][root][INFO] - Training Epoch: 2/2, step 4025/7134 completed (loss: 0.05091214179992676, acc: 0.987542450428009)
[2025-02-13 04:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47][root][INFO] - Training Epoch: 2/2, step 4026/7134 completed (loss: 0.060053322464227676, acc: 0.9882006049156189)
[2025-02-13 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47][root][INFO] - Training Epoch: 2/2, step 4027/7134 completed (loss: 0.03668772429227829, acc: 0.9880095720291138)
[2025-02-13 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47][root][INFO] - Training Epoch: 2/2, step 4028/7134 completed (loss: 0.06137554347515106, acc: 0.9845094680786133)
[2025-02-13 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:48][root][INFO] - Training Epoch: 2/2, step 4029/7134 completed (loss: 0.01951250061392784, acc: 0.9938499331474304)
[2025-02-13 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:48][root][INFO] - Training Epoch: 2/2, step 4030/7134 completed (loss: 0.007570735644549131, acc: 0.9979466199874878)
[2025-02-13 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:49][root][INFO] - Training Epoch: 2/2, step 4031/7134 completed (loss: 0.01775176450610161, acc: 0.9941349029541016)
[2025-02-13 04:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:49][root][INFO] - Training Epoch: 2/2, step 4032/7134 completed (loss: 0.05010226368904114, acc: 0.9919678568840027)
[2025-02-13 04:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50][root][INFO] - Training Epoch: 2/2, step 4033/7134 completed (loss: 0.016516372561454773, acc: 0.9911949634552002)
[2025-02-13 04:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50][root][INFO] - Training Epoch: 2/2, step 4034/7134 completed (loss: 0.005334609653800726, acc: 0.9987819790840149)
[2025-02-13 04:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50][root][INFO] - Training Epoch: 2/2, step 4035/7134 completed (loss: 0.03981102630496025, acc: 0.9881936311721802)
[2025-02-13 04:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51][root][INFO] - Training Epoch: 2/2, step 4036/7134 completed (loss: 0.032711900770664215, acc: 0.9890109896659851)
[2025-02-13 04:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51][root][INFO] - Training Epoch: 2/2, step 4037/7134 completed (loss: 0.020838182419538498, acc: 0.9927797913551331)
[2025-02-13 04:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:52][root][INFO] - Training Epoch: 2/2, step 4038/7134 completed (loss: 0.029381785541772842, acc: 0.9944979548454285)
[2025-02-13 04:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:52][root][INFO] - Training Epoch: 2/2, step 4039/7134 completed (loss: 0.03480237349867821, acc: 0.9886524677276611)
[2025-02-13 04:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53][root][INFO] - Training Epoch: 2/2, step 4040/7134 completed (loss: 0.012911762110888958, acc: 0.993842363357544)
[2025-02-13 04:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53][root][INFO] - Training Epoch: 2/2, step 4041/7134 completed (loss: 0.034437816590070724, acc: 0.9919354915618896)
[2025-02-13 04:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53][root][INFO] - Training Epoch: 2/2, step 4042/7134 completed (loss: 0.02738307975232601, acc: 0.9897058606147766)
[2025-02-13 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:54][root][INFO] - Training Epoch: 2/2, step 4043/7134 completed (loss: 0.013005798682570457, acc: 0.9926578402519226)
[2025-02-13 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:54][root][INFO] - Training Epoch: 2/2, step 4044/7134 completed (loss: 0.008048987947404385, acc: 0.9987373948097229)
[2025-02-13 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:55][root][INFO] - Training Epoch: 2/2, step 4045/7134 completed (loss: 0.047694217413663864, acc: 0.987500011920929)
[2025-02-13 04:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:55][root][INFO] - Training Epoch: 2/2, step 4046/7134 completed (loss: 0.03258305788040161, acc: 0.9917582273483276)
[2025-02-13 04:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56][root][INFO] - Training Epoch: 2/2, step 4047/7134 completed (loss: 0.049213506281375885, acc: 0.9892601370811462)
[2025-02-13 04:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56][root][INFO] - Training Epoch: 2/2, step 4048/7134 completed (loss: 0.023422492668032646, acc: 0.9941725134849548)
[2025-02-13 04:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57][root][INFO] - Training Epoch: 2/2, step 4049/7134 completed (loss: 0.027381232008337975, acc: 0.9931034445762634)
[2025-02-13 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57][root][INFO] - Training Epoch: 2/2, step 4050/7134 completed (loss: 0.04081931337714195, acc: 0.9872340559959412)
[2025-02-13 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57][root][INFO] - Training Epoch: 2/2, step 4051/7134 completed (loss: 0.05437818169593811, acc: 0.9916267991065979)
[2025-02-13 04:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:58][root][INFO] - Training Epoch: 2/2, step 4052/7134 completed (loss: 0.024097977206110954, acc: 0.9935400485992432)
[2025-02-13 04:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:58][root][INFO] - Training Epoch: 2/2, step 4053/7134 completed (loss: 0.024040859192609787, acc: 0.9867374300956726)
[2025-02-13 04:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59][root][INFO] - Training Epoch: 2/2, step 4054/7134 completed (loss: 0.04317130893468857, acc: 0.9940000176429749)
[2025-02-13 04:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59][root][INFO] - Training Epoch: 2/2, step 4055/7134 completed (loss: 0.020791180431842804, acc: 0.9902912378311157)
[2025-02-13 04:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00][root][INFO] - Training Epoch: 2/2, step 4056/7134 completed (loss: 0.022392742335796356, acc: 0.9915151596069336)
[2025-02-13 04:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00][root][INFO] - Training Epoch: 2/2, step 4057/7134 completed (loss: 0.053096748888492584, acc: 0.9888734221458435)
[2025-02-13 04:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00][root][INFO] - Training Epoch: 2/2, step 4058/7134 completed (loss: 0.059230297803878784, acc: 0.9789674878120422)
[2025-02-13 04:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01][root][INFO] - Training Epoch: 2/2, step 4059/7134 completed (loss: 0.027576006948947906, acc: 0.9927007555961609)
[2025-02-13 04:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01][root][INFO] - Training Epoch: 2/2, step 4060/7134 completed (loss: 0.020217686891555786, acc: 0.9938176274299622)
[2025-02-13 04:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02][root][INFO] - Training Epoch: 2/2, step 4061/7134 completed (loss: 0.018021170049905777, acc: 0.994301974773407)
[2025-02-13 04:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02][root][INFO] - Training Epoch: 2/2, step 4062/7134 completed (loss: 0.007038980722427368, acc: 0.9968503713607788)
[2025-02-13 04:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02][root][INFO] - Training Epoch: 2/2, step 4063/7134 completed (loss: 0.01438350323587656, acc: 0.9929328560829163)
[2025-02-13 04:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:03][root][INFO] - Training Epoch: 2/2, step 4064/7134 completed (loss: 0.008623950183391571, acc: 0.9969742894172668)
[2025-02-13 04:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:03][root][INFO] - Training Epoch: 2/2, step 4065/7134 completed (loss: 0.015034504234790802, acc: 0.9944827556610107)
[2025-02-13 04:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04][root][INFO] - Training Epoch: 2/2, step 4066/7134 completed (loss: 0.008168285712599754, acc: 0.9972222447395325)
[2025-02-13 04:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04][root][INFO] - Training Epoch: 2/2, step 4067/7134 completed (loss: 0.031556349247694016, acc: 0.9901960492134094)
[2025-02-13 04:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04][root][INFO] - Training Epoch: 2/2, step 4068/7134 completed (loss: 0.040381886065006256, acc: 0.9888888597488403)
[2025-02-13 04:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:05][root][INFO] - Training Epoch: 2/2, step 4069/7134 completed (loss: 0.02070220746099949, acc: 0.9968701004981995)
[2025-02-13 04:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:05][root][INFO] - Training Epoch: 2/2, step 4070/7134 completed (loss: 0.02521427534520626, acc: 0.9893454909324646)
[2025-02-13 04:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06][root][INFO] - Training Epoch: 2/2, step 4071/7134 completed (loss: 0.02599584497511387, acc: 0.9901153445243835)
[2025-02-13 04:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06][root][INFO] - Training Epoch: 2/2, step 4072/7134 completed (loss: 0.011730809696018696, acc: 0.9971140027046204)
[2025-02-13 04:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06][root][INFO] - Training Epoch: 2/2, step 4073/7134 completed (loss: 0.01800539530813694, acc: 0.9913793206214905)
[2025-02-13 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:07][root][INFO] - Training Epoch: 2/2, step 4074/7134 completed (loss: 0.02445434033870697, acc: 0.995708167552948)
[2025-02-13 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:07][root][INFO] - Training Epoch: 2/2, step 4075/7134 completed (loss: 0.01230858825147152, acc: 0.995502233505249)
[2025-02-13 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:08][root][INFO] - Training Epoch: 2/2, step 4076/7134 completed (loss: 0.01135334838181734, acc: 0.9956584572792053)
[2025-02-13 04:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:08][root][INFO] - Training Epoch: 2/2, step 4077/7134 completed (loss: 0.03809826448559761, acc: 0.989847719669342)
[2025-02-13 04:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09][root][INFO] - Training Epoch: 2/2, step 4078/7134 completed (loss: 0.014238792471587658, acc: 0.995726466178894)
[2025-02-13 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09][root][INFO] - Training Epoch: 2/2, step 4079/7134 completed (loss: 0.018937237560749054, acc: 0.9946523904800415)
[2025-02-13 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09][root][INFO] - Training Epoch: 2/2, step 4080/7134 completed (loss: 0.008127129636704922, acc: 0.996835470199585)
[2025-02-13 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10][root][INFO] - Training Epoch: 2/2, step 4081/7134 completed (loss: 0.005176446866244078, acc: 0.9983498454093933)
[2025-02-13 04:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10][root][INFO] - Training Epoch: 2/2, step 4082/7134 completed (loss: 0.03893812373280525, acc: 0.9815157055854797)
[2025-02-13 04:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10][root][INFO] - Training Epoch: 2/2, step 4083/7134 completed (loss: 0.04078809544444084, acc: 0.9895209670066833)
[2025-02-13 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:11][root][INFO] - Training Epoch: 2/2, step 4084/7134 completed (loss: 0.0783718079328537, acc: 0.9725490212440491)
[2025-02-13 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:11][root][INFO] - Training Epoch: 2/2, step 4085/7134 completed (loss: 0.03242877870798111, acc: 0.9905660152435303)
[2025-02-13 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12][root][INFO] - Training Epoch: 2/2, step 4086/7134 completed (loss: 0.06227382645010948, acc: 0.9836333990097046)
[2025-02-13 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12][root][INFO] - Training Epoch: 2/2, step 4087/7134 completed (loss: 0.026984697207808495, acc: 0.9961240291595459)
[2025-02-13 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13][root][INFO] - Training Epoch: 2/2, step 4088/7134 completed (loss: 0.06244142726063728, acc: 0.9838308691978455)
[2025-02-13 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13][root][INFO] - Training Epoch: 2/2, step 4089/7134 completed (loss: 0.05076010897755623, acc: 0.9840142130851746)
[2025-02-13 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13][root][INFO] - Training Epoch: 2/2, step 4090/7134 completed (loss: 0.094234399497509, acc: 0.9775725603103638)
[2025-02-13 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14][root][INFO] - Training Epoch: 2/2, step 4091/7134 completed (loss: 0.1185954362154007, acc: 0.9713321924209595)
[2025-02-13 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14][root][INFO] - Training Epoch: 2/2, step 4092/7134 completed (loss: 0.047212865203619, acc: 0.9897040128707886)
[2025-02-13 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15][root][INFO] - Training Epoch: 2/2, step 4093/7134 completed (loss: 0.019017715007066727, acc: 0.9963570237159729)
[2025-02-13 04:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15][root][INFO] - Training Epoch: 2/2, step 4094/7134 completed (loss: 0.0588611401617527, acc: 0.9864864945411682)
[2025-02-13 04:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15][root][INFO] - Training Epoch: 2/2, step 4095/7134 completed (loss: 0.020073048770427704, acc: 0.9971056580543518)
[2025-02-13 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16][root][INFO] - Training Epoch: 2/2, step 4096/7134 completed (loss: 0.07821694761514664, acc: 0.9850746393203735)
[2025-02-13 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16][root][INFO] - Training Epoch: 2/2, step 4097/7134 completed (loss: 0.0468464158475399, acc: 0.9903225898742676)
[2025-02-13 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:17][root][INFO] - Training Epoch: 2/2, step 4098/7134 completed (loss: 0.035564739257097244, acc: 0.9918699264526367)
[2025-02-13 04:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:17][root][INFO] - Training Epoch: 2/2, step 4099/7134 completed (loss: 0.03426317870616913, acc: 0.9894366264343262)
[2025-02-13 04:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18][root][INFO] - Training Epoch: 2/2, step 4100/7134 completed (loss: 0.008310269564390182, acc: 0.998487114906311)
[2025-02-13 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18][root][INFO] - Training Epoch: 2/2, step 4101/7134 completed (loss: 0.01981908641755581, acc: 0.9942965507507324)
[2025-02-13 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18][root][INFO] - Training Epoch: 2/2, step 4102/7134 completed (loss: 0.028700148686766624, acc: 0.9926874041557312)
[2025-02-13 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19][root][INFO] - Training Epoch: 2/2, step 4103/7134 completed (loss: 0.0038044077809900045, acc: 1.0)
[2025-02-13 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19][root][INFO] - Training Epoch: 2/2, step 4104/7134 completed (loss: 0.008775508962571621, acc: 0.9983079433441162)
[2025-02-13 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20][root][INFO] - Training Epoch: 2/2, step 4105/7134 completed (loss: 0.022724436596035957, acc: 0.995468258857727)
[2025-02-13 04:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20][root][INFO] - Training Epoch: 2/2, step 4106/7134 completed (loss: 0.02057008445262909, acc: 0.9938650131225586)
[2025-02-13 04:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20][root][INFO] - Training Epoch: 2/2, step 4107/7134 completed (loss: 0.022037696093320847, acc: 0.9966386556625366)
[2025-02-13 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21][root][INFO] - Training Epoch: 2/2, step 4108/7134 completed (loss: 0.024042071774601936, acc: 0.9935275316238403)
[2025-02-13 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21][root][INFO] - Training Epoch: 2/2, step 4109/7134 completed (loss: 0.013321277685463428, acc: 0.9983948469161987)
[2025-02-13 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22][root][INFO] - Training Epoch: 2/2, step 4110/7134 completed (loss: 0.002868782030418515, acc: 1.0)
[2025-02-13 04:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22][root][INFO] - Training Epoch: 2/2, step 4111/7134 completed (loss: 0.013263548724353313, acc: 0.9968454241752625)
[2025-02-13 04:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22][root][INFO] - Training Epoch: 2/2, step 4112/7134 completed (loss: 0.029606958851218224, acc: 0.9892473220825195)
[2025-02-13 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23][root][INFO] - Training Epoch: 2/2, step 4113/7134 completed (loss: 0.03514450043439865, acc: 0.9906976819038391)
[2025-02-13 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23][root][INFO] - Training Epoch: 2/2, step 4114/7134 completed (loss: 0.09154021739959717, acc: 0.9695122241973877)
[2025-02-13 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24][root][INFO] - Training Epoch: 2/2, step 4115/7134 completed (loss: 0.038033489137887955, acc: 0.9850746393203735)
[2025-02-13 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24][root][INFO] - Training Epoch: 2/2, step 4116/7134 completed (loss: 0.007940898649394512, acc: 0.9983713626861572)
[2025-02-13 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24][root][INFO] - Training Epoch: 2/2, step 4117/7134 completed (loss: 0.017930177971720695, acc: 0.9936908483505249)
[2025-02-13 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25][root][INFO] - Training Epoch: 2/2, step 4118/7134 completed (loss: 0.012935221195220947, acc: 0.9950860142707825)
[2025-02-13 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25][root][INFO] - Training Epoch: 2/2, step 4119/7134 completed (loss: 0.04045700281858444, acc: 0.9903069734573364)
[2025-02-13 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26][root][INFO] - Training Epoch: 2/2, step 4120/7134 completed (loss: 0.03515439108014107, acc: 0.9934102296829224)
[2025-02-13 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26][root][INFO] - Training Epoch: 2/2, step 4121/7134 completed (loss: 0.04686799272894859, acc: 0.9914772510528564)
[2025-02-13 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26][root][INFO] - Training Epoch: 2/2, step 4122/7134 completed (loss: 0.022542236372828484, acc: 0.9923664331436157)
[2025-02-13 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27][root][INFO] - Training Epoch: 2/2, step 4123/7134 completed (loss: 0.030768983066082, acc: 0.9909502267837524)
[2025-02-13 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27][root][INFO] - Training Epoch: 2/2, step 4124/7134 completed (loss: 0.03107827715575695, acc: 0.9915134310722351)
[2025-02-13 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28][root][INFO] - Training Epoch: 2/2, step 4125/7134 completed (loss: 0.018876278772950172, acc: 0.9941176176071167)
[2025-02-13 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28][root][INFO] - Training Epoch: 2/2, step 4126/7134 completed (loss: 0.015084413811564445, acc: 0.9957507252693176)
[2025-02-13 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28][root][INFO] - Training Epoch: 2/2, step 4127/7134 completed (loss: 0.03577710688114166, acc: 0.9906250238418579)
[2025-02-13 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:29][root][INFO] - Training Epoch: 2/2, step 4128/7134 completed (loss: 0.025010762736201286, acc: 0.9906103014945984)
[2025-02-13 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:29][root][INFO] - Training Epoch: 2/2, step 4129/7134 completed (loss: 0.011312177404761314, acc: 0.9940652847290039)
[2025-02-13 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30][root][INFO] - Training Epoch: 2/2, step 4130/7134 completed (loss: 0.01974763348698616, acc: 0.9922928810119629)
[2025-02-13 04:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30][root][INFO] - Training Epoch: 2/2, step 4131/7134 completed (loss: 0.012896179221570492, acc: 0.9944547414779663)
[2025-02-13 04:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30][root][INFO] - Training Epoch: 2/2, step 4132/7134 completed (loss: 0.025786785408854485, acc: 0.9933035969734192)
[2025-02-13 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31][root][INFO] - Training Epoch: 2/2, step 4133/7134 completed (loss: 0.04328024014830589, acc: 0.9833333492279053)
[2025-02-13 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31][root][INFO] - Training Epoch: 2/2, step 4134/7134 completed (loss: 0.0526871494948864, acc: 0.9810426831245422)
[2025-02-13 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32][root][INFO] - Training Epoch: 2/2, step 4135/7134 completed (loss: 0.01941343955695629, acc: 0.9946714043617249)
[2025-02-13 04:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32][root][INFO] - Training Epoch: 2/2, step 4136/7134 completed (loss: 0.023518895730376244, acc: 0.9964601993560791)
[2025-02-13 04:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32][root][INFO] - Training Epoch: 2/2, step 4137/7134 completed (loss: 0.025834716856479645, acc: 0.990234375)
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33][root][INFO] - Training Epoch: 2/2, step 4138/7134 completed (loss: 0.027346745133399963, acc: 0.9890282154083252)
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33][root][INFO] - Training Epoch: 2/2, step 4139/7134 completed (loss: 0.023001838475465775, acc: 0.995708167552948)
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:34][root][INFO] - Training Epoch: 2/2, step 4140/7134 completed (loss: 0.04276224225759506, acc: 0.9889655113220215)
[2025-02-13 04:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:34][root][INFO] - Training Epoch: 2/2, step 4141/7134 completed (loss: 0.062224604189395905, acc: 0.982807993888855)
[2025-02-13 04:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:35][root][INFO] - Training Epoch: 2/2, step 4142/7134 completed (loss: 0.07819227874279022, acc: 0.9805492162704468)
[2025-02-13 04:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:35][root][INFO] - Training Epoch: 2/2, step 4143/7134 completed (loss: 0.04369845613837242, acc: 0.9918367266654968)
[2025-02-13 04:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36][root][INFO] - Training Epoch: 2/2, step 4144/7134 completed (loss: 0.06473343074321747, acc: 0.9829156994819641)
[2025-02-13 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36][root][INFO] - Training Epoch: 2/2, step 4145/7134 completed (loss: 0.030468087643384933, acc: 0.9909747242927551)
[2025-02-13 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36][root][INFO] - Training Epoch: 2/2, step 4146/7134 completed (loss: 0.07576625049114227, acc: 0.9858585596084595)
[2025-02-13 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37][root][INFO] - Training Epoch: 2/2, step 4147/7134 completed (loss: 0.06976041197776794, acc: 0.9885057210922241)
[2025-02-13 04:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37][root][INFO] - Training Epoch: 2/2, step 4148/7134 completed (loss: 0.12357663363218307, acc: 0.969111979007721)
[2025-02-13 04:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37][root][INFO] - Training Epoch: 2/2, step 4149/7134 completed (loss: 0.11068861931562424, acc: 0.9706666469573975)
[2025-02-13 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:38][root][INFO] - Training Epoch: 2/2, step 4150/7134 completed (loss: 0.11608990281820297, acc: 0.9678068161010742)
[2025-02-13 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:38][root][INFO] - Training Epoch: 2/2, step 4151/7134 completed (loss: 0.14672090113162994, acc: 0.9764492511749268)
[2025-02-13 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:39][root][INFO] - Training Epoch: 2/2, step 4152/7134 completed (loss: 0.05261597782373428, acc: 0.9860896468162537)
[2025-02-13 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:39][root][INFO] - Training Epoch: 2/2, step 4153/7134 completed (loss: 0.08556962013244629, acc: 0.9831288456916809)
[2025-02-13 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40][root][INFO] - Training Epoch: 2/2, step 4154/7134 completed (loss: 0.051791492849588394, acc: 0.9849537014961243)
[2025-02-13 04:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40][root][INFO] - Training Epoch: 2/2, step 4155/7134 completed (loss: 0.05175461620092392, acc: 0.9845626354217529)
[2025-02-13 04:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40][root][INFO] - Training Epoch: 2/2, step 4156/7134 completed (loss: 0.05721202865242958, acc: 0.9833101630210876)
[2025-02-13 04:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:41][root][INFO] - Training Epoch: 2/2, step 4157/7134 completed (loss: 0.045120567083358765, acc: 0.9880239367485046)
[2025-02-13 04:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:41][root][INFO] - Training Epoch: 2/2, step 4158/7134 completed (loss: 0.12326254695653915, acc: 0.9691516757011414)
[2025-02-13 04:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42][root][INFO] - Training Epoch: 2/2, step 4159/7134 completed (loss: 0.04792160913348198, acc: 0.9847198724746704)
[2025-02-13 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42][root][INFO] - Training Epoch: 2/2, step 4160/7134 completed (loss: 0.07565045356750488, acc: 0.9792453050613403)
[2025-02-13 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43][root][INFO] - Training Epoch: 2/2, step 4161/7134 completed (loss: 0.049734313040971756, acc: 0.9899598360061646)
[2025-02-13 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43][root][INFO] - Training Epoch: 2/2, step 4162/7134 completed (loss: 0.05948353931307793, acc: 0.9836065769195557)
[2025-02-13 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43][root][INFO] - Training Epoch: 2/2, step 4163/7134 completed (loss: 0.055766016244888306, acc: 0.9878970980644226)
[2025-02-13 04:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:44][root][INFO] - Training Epoch: 2/2, step 4164/7134 completed (loss: 0.04915214702486992, acc: 0.9840348362922668)
[2025-02-13 04:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:44][root][INFO] - Training Epoch: 2/2, step 4165/7134 completed (loss: 0.05853728577494621, acc: 0.9850187301635742)
[2025-02-13 04:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:45][root][INFO] - Training Epoch: 2/2, step 4166/7134 completed (loss: 0.049922019243240356, acc: 0.9838472604751587)
[2025-02-13 04:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:45][root][INFO] - Training Epoch: 2/2, step 4167/7134 completed (loss: 0.01177566684782505, acc: 0.9982638955116272)
[2025-02-13 04:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46][root][INFO] - Training Epoch: 2/2, step 4168/7134 completed (loss: 0.03629249706864357, acc: 0.9930555820465088)
[2025-02-13 04:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46][root][INFO] - Training Epoch: 2/2, step 4169/7134 completed (loss: 0.055712223052978516, acc: 0.9915522933006287)
[2025-02-13 04:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47][root][INFO] - Training Epoch: 2/2, step 4170/7134 completed (loss: 0.07975228875875473, acc: 0.9766454100608826)
[2025-02-13 04:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47][root][INFO] - Training Epoch: 2/2, step 4171/7134 completed (loss: 0.020171822980046272, acc: 0.9942330121994019)
[2025-02-13 04:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47][root][INFO] - Training Epoch: 2/2, step 4172/7134 completed (loss: 0.05118393898010254, acc: 0.9862843155860901)
[2025-02-13 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48][root][INFO] - Training Epoch: 2/2, step 4173/7134 completed (loss: 0.04604794830083847, acc: 0.9849785566329956)
[2025-02-13 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48][root][INFO] - Training Epoch: 2/2, step 4174/7134 completed (loss: 0.0052902596071362495, acc: 1.0)
[2025-02-13 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:49][root][INFO] - Training Epoch: 2/2, step 4175/7134 completed (loss: 0.03149793669581413, acc: 0.9914089441299438)
[2025-02-13 04:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:49][root][INFO] - Training Epoch: 2/2, step 4176/7134 completed (loss: 0.016408191993832588, acc: 0.9940387606620789)
[2025-02-13 04:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50][root][INFO] - Training Epoch: 2/2, step 4177/7134 completed (loss: 0.018438449129462242, acc: 0.9934297204017639)
[2025-02-13 04:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50][root][INFO] - Training Epoch: 2/2, step 4178/7134 completed (loss: 0.017850713804364204, acc: 0.9924471378326416)
[2025-02-13 04:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50][root][INFO] - Training Epoch: 2/2, step 4179/7134 completed (loss: 0.010506636463105679, acc: 0.9960629940032959)
[2025-02-13 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:51][root][INFO] - Training Epoch: 2/2, step 4180/7134 completed (loss: 0.01089775562286377, acc: 0.9985875487327576)
[2025-02-13 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:51][root][INFO] - Training Epoch: 2/2, step 4181/7134 completed (loss: 0.00982697680592537, acc: 0.995106041431427)
[2025-02-13 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52][root][INFO] - Training Epoch: 2/2, step 4182/7134 completed (loss: 0.004928643349558115, acc: 1.0)
[2025-02-13 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52][root][INFO] - Training Epoch: 2/2, step 4183/7134 completed (loss: 0.013384438119828701, acc: 0.9958275556564331)
[2025-02-13 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:53][root][INFO] - Training Epoch: 2/2, step 4184/7134 completed (loss: 0.028195954859256744, acc: 0.9920424222946167)
[2025-02-13 04:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:53][root][INFO] - Training Epoch: 2/2, step 4185/7134 completed (loss: 0.007848490960896015, acc: 0.9977169036865234)
[2025-02-13 04:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54][root][INFO] - Training Epoch: 2/2, step 4186/7134 completed (loss: 0.0630684345960617, acc: 0.9903730154037476)
[2025-02-13 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54][root][INFO] - Training Epoch: 2/2, step 4187/7134 completed (loss: 0.05150816962122917, acc: 0.9921875)
[2025-02-13 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54][root][INFO] - Training Epoch: 2/2, step 4188/7134 completed (loss: 0.009929461404681206, acc: 0.9950000047683716)
[2025-02-13 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:55][root][INFO] - Training Epoch: 2/2, step 4189/7134 completed (loss: 0.016984159126877785, acc: 0.9972028136253357)
[2025-02-13 04:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:55][root][INFO] - Training Epoch: 2/2, step 4190/7134 completed (loss: 0.021474063396453857, acc: 0.9957386255264282)
[2025-02-13 04:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56][root][INFO] - Training Epoch: 2/2, step 4191/7134 completed (loss: 0.028533540666103363, acc: 0.9953917264938354)
[2025-02-13 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56][root][INFO] - Training Epoch: 2/2, step 4192/7134 completed (loss: 0.00952002964913845, acc: 0.9968051314353943)
[2025-02-13 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56][root][INFO] - Training Epoch: 2/2, step 4193/7134 completed (loss: 0.02087341994047165, acc: 0.9959893226623535)
[2025-02-13 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:57][root][INFO] - Training Epoch: 2/2, step 4194/7134 completed (loss: 0.010053738951683044, acc: 0.9969742894172668)
[2025-02-13 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:57][root][INFO] - Training Epoch: 2/2, step 4195/7134 completed (loss: 0.03616863861680031, acc: 0.989159882068634)
[2025-02-13 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:58][root][INFO] - Training Epoch: 2/2, step 4196/7134 completed (loss: 0.006927973590791225, acc: 0.9965397715568542)
[2025-02-13 04:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:58][root][INFO] - Training Epoch: 2/2, step 4197/7134 completed (loss: 0.032917387783527374, acc: 0.9896507263183594)
[2025-02-13 04:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59][root][INFO] - Training Epoch: 2/2, step 4198/7134 completed (loss: 0.01848568581044674, acc: 0.9944444298744202)
[2025-02-13 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59][root][INFO] - Training Epoch: 2/2, step 4199/7134 completed (loss: 0.02444491721689701, acc: 0.9927007555961609)
[2025-02-13 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59][root][INFO] - Training Epoch: 2/2, step 4200/7134 completed (loss: 0.035322900861501694, acc: 0.9892473220825195)
[2025-02-13 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:00][root][INFO] - Training Epoch: 2/2, step 4201/7134 completed (loss: 0.012782640755176544, acc: 0.9982876777648926)
[2025-02-13 04:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:00][root][INFO] - Training Epoch: 2/2, step 4202/7134 completed (loss: 0.04715731367468834, acc: 0.9883720874786377)
[2025-02-13 04:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01][root][INFO] - Training Epoch: 2/2, step 4203/7134 completed (loss: 0.04793454334139824, acc: 0.9843993782997131)
[2025-02-13 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01][root][INFO] - Training Epoch: 2/2, step 4204/7134 completed (loss: 0.08242902904748917, acc: 0.9809941649436951)
[2025-02-13 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01][root][INFO] - Training Epoch: 2/2, step 4205/7134 completed (loss: 0.03857363387942314, acc: 0.9913669228553772)
[2025-02-13 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02][root][INFO] - Training Epoch: 2/2, step 4206/7134 completed (loss: 0.060706209391355515, acc: 0.9873060584068298)
[2025-02-13 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02][root][INFO] - Training Epoch: 2/2, step 4207/7134 completed (loss: 0.047538209706544876, acc: 0.9888535141944885)
[2025-02-13 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:03][root][INFO] - Training Epoch: 2/2, step 4208/7134 completed (loss: 0.057698749005794525, acc: 0.9872408509254456)
[2025-02-13 04:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:03][root][INFO] - Training Epoch: 2/2, step 4209/7134 completed (loss: 0.05359038710594177, acc: 0.9858356714248657)
[2025-02-13 04:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04][root][INFO] - Training Epoch: 2/2, step 4210/7134 completed (loss: 0.007056018803268671, acc: 1.0)
[2025-02-13 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04][root][INFO] - Training Epoch: 2/2, step 4211/7134 completed (loss: 0.027417132630944252, acc: 0.9861751198768616)
[2025-02-13 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04][root][INFO] - Training Epoch: 2/2, step 4212/7134 completed (loss: 0.030218014493584633, acc: 0.9934533834457397)
[2025-02-13 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05][root][INFO] - Training Epoch: 2/2, step 4213/7134 completed (loss: 0.011341770179569721, acc: 0.9971387982368469)
[2025-02-13 04:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05][root][INFO] - Training Epoch: 2/2, step 4214/7134 completed (loss: 0.06333786994218826, acc: 0.9820895791053772)
[2025-02-13 04:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05][root][INFO] - Training Epoch: 2/2, step 4215/7134 completed (loss: 0.045506205409765244, acc: 0.9868131875991821)
[2025-02-13 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:06][root][INFO] - Training Epoch: 2/2, step 4216/7134 completed (loss: 0.04151281714439392, acc: 0.9890310764312744)
[2025-02-13 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:06][root][INFO] - Training Epoch: 2/2, step 4217/7134 completed (loss: 0.023024125024676323, acc: 0.9902098178863525)
[2025-02-13 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07][root][INFO] - Training Epoch: 2/2, step 4218/7134 completed (loss: 0.03706871345639229, acc: 0.9896907210350037)
[2025-02-13 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07][root][INFO] - Training Epoch: 2/2, step 4219/7134 completed (loss: 0.024063019081950188, acc: 0.9919678568840027)
[2025-02-13 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07][root][INFO] - Training Epoch: 2/2, step 4220/7134 completed (loss: 0.052606336772441864, acc: 0.9856114983558655)
[2025-02-13 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08][root][INFO] - Training Epoch: 2/2, step 4221/7134 completed (loss: 0.035373736172914505, acc: 0.9929203391075134)
[2025-02-13 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08][root][INFO] - Training Epoch: 2/2, step 4222/7134 completed (loss: 0.10847844928503036, acc: 0.9755555391311646)
[2025-02-13 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09][root][INFO] - Training Epoch: 2/2, step 4223/7134 completed (loss: 0.035162899643182755, acc: 0.9880794882774353)
[2025-02-13 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09][root][INFO] - Training Epoch: 2/2, step 4224/7134 completed (loss: 0.062376655638217926, acc: 0.9848812222480774)
[2025-02-13 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09][root][INFO] - Training Epoch: 2/2, step 4225/7134 completed (loss: 0.09625904262065887, acc: 0.979626476764679)
[2025-02-13 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10][root][INFO] - Training Epoch: 2/2, step 4226/7134 completed (loss: 0.05418160557746887, acc: 0.9872286319732666)
[2025-02-13 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10][root][INFO] - Training Epoch: 2/2, step 4227/7134 completed (loss: 0.09482639282941818, acc: 0.9877150058746338)
[2025-02-13 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11][root][INFO] - Training Epoch: 2/2, step 4228/7134 completed (loss: 0.10705338418483734, acc: 0.9758620858192444)
[2025-02-13 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11][root][INFO] - Training Epoch: 2/2, step 4229/7134 completed (loss: 0.08884161710739136, acc: 0.9790356159210205)
[2025-02-13 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11][root][INFO] - Training Epoch: 2/2, step 4230/7134 completed (loss: 0.06478678435087204, acc: 0.9852941036224365)
[2025-02-13 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:12][root][INFO] - Training Epoch: 2/2, step 4231/7134 completed (loss: 0.12714911997318268, acc: 0.970534086227417)
[2025-02-13 04:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:12][root][INFO] - Training Epoch: 2/2, step 4232/7134 completed (loss: 0.08258340507745743, acc: 0.9740458130836487)
[2025-02-13 04:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13][root][INFO] - Training Epoch: 2/2, step 4233/7134 completed (loss: 0.09514661878347397, acc: 0.9799666404724121)
[2025-02-13 04:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13][root][INFO] - Training Epoch: 2/2, step 4234/7134 completed (loss: 0.06107231602072716, acc: 0.9888712167739868)
[2025-02-13 04:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13][root][INFO] - Training Epoch: 2/2, step 4235/7134 completed (loss: 0.03594035282731056, acc: 0.9889415502548218)
[2025-02-13 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14][root][INFO] - Training Epoch: 2/2, step 4236/7134 completed (loss: 0.017250625416636467, acc: 0.997474730014801)
[2025-02-13 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14][root][INFO] - Training Epoch: 2/2, step 4237/7134 completed (loss: 0.03177588805556297, acc: 0.9896907210350037)
[2025-02-13 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15][root][INFO] - Training Epoch: 2/2, step 4238/7134 completed (loss: 0.041317641735076904, acc: 0.9918166995048523)
[2025-02-13 04:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15][root][INFO] - Training Epoch: 2/2, step 4239/7134 completed (loss: 0.027006573975086212, acc: 0.9919999837875366)
[2025-02-13 04:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15][root][INFO] - Training Epoch: 2/2, step 4240/7134 completed (loss: 0.05075881630182266, acc: 0.9854604005813599)
[2025-02-13 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:16][root][INFO] - Training Epoch: 2/2, step 4241/7134 completed (loss: 0.015741148963570595, acc: 0.9934924244880676)
[2025-02-13 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:16][root][INFO] - Training Epoch: 2/2, step 4242/7134 completed (loss: 0.03611792251467705, acc: 0.9888641238212585)
[2025-02-13 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17][root][INFO] - Training Epoch: 2/2, step 4243/7134 completed (loss: 0.06387673318386078, acc: 0.9873417615890503)
[2025-02-13 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17][root][INFO] - Training Epoch: 2/2, step 4244/7134 completed (loss: 0.035181675106287, acc: 0.9911308288574219)
[2025-02-13 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17][root][INFO] - Training Epoch: 2/2, step 4245/7134 completed (loss: 0.03206726163625717, acc: 0.9926650524139404)
[2025-02-13 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:18][root][INFO] - Training Epoch: 2/2, step 4246/7134 completed (loss: 0.04004964232444763, acc: 0.9936608672142029)
[2025-02-13 04:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:18][root][INFO] - Training Epoch: 2/2, step 4247/7134 completed (loss: 0.029598413035273552, acc: 0.987522304058075)
[2025-02-13 04:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19][root][INFO] - Training Epoch: 2/2, step 4248/7134 completed (loss: 0.05404696613550186, acc: 0.9807692170143127)
[2025-02-13 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19][root][INFO] - Training Epoch: 2/2, step 4249/7134 completed (loss: 0.053878143429756165, acc: 0.9888888597488403)
[2025-02-13 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19][root][INFO] - Training Epoch: 2/2, step 4250/7134 completed (loss: 0.06700749695301056, acc: 0.9851379990577698)
[2025-02-13 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:20][root][INFO] - Training Epoch: 2/2, step 4251/7134 completed (loss: 0.012442399747669697, acc: 0.9967105388641357)
[2025-02-13 04:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:20][root][INFO] - Training Epoch: 2/2, step 4252/7134 completed (loss: 0.03740997239947319, acc: 0.9888712167739868)
[2025-02-13 04:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21][root][INFO] - Training Epoch: 2/2, step 4253/7134 completed (loss: 0.04177644103765488, acc: 0.9913232326507568)
[2025-02-13 04:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21][root][INFO] - Training Epoch: 2/2, step 4254/7134 completed (loss: 0.01923995092511177, acc: 0.9925187230110168)
[2025-02-13 04:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21][root][INFO] - Training Epoch: 2/2, step 4255/7134 completed (loss: 0.034147825092077255, acc: 0.9931507110595703)
[2025-02-13 04:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:22][root][INFO] - Training Epoch: 2/2, step 4256/7134 completed (loss: 0.03092670440673828, acc: 0.9909502267837524)
[2025-02-13 04:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:22][root][INFO] - Training Epoch: 2/2, step 4257/7134 completed (loss: 0.016487669199705124, acc: 0.9946523904800415)
[2025-02-13 04:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23][root][INFO] - Training Epoch: 2/2, step 4258/7134 completed (loss: 0.010134904645383358, acc: 0.9980915784835815)
[2025-02-13 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23][root][INFO] - Training Epoch: 2/2, step 4259/7134 completed (loss: 0.07508236169815063, acc: 0.9813753366470337)
[2025-02-13 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23][root][INFO] - Training Epoch: 2/2, step 4260/7134 completed (loss: 0.014425650238990784, acc: 0.9937888383865356)
[2025-02-13 04:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:24][root][INFO] - Training Epoch: 2/2, step 4261/7134 completed (loss: 0.02130972594022751, acc: 0.990439772605896)
[2025-02-13 04:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:24][root][INFO] - Training Epoch: 2/2, step 4262/7134 completed (loss: 0.014601195231080055, acc: 0.9968701004981995)
[2025-02-13 04:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25][root][INFO] - Training Epoch: 2/2, step 4263/7134 completed (loss: 0.06709480285644531, acc: 0.9915540814399719)
[2025-02-13 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25][root][INFO] - Training Epoch: 2/2, step 4264/7134 completed (loss: 0.04967754706740379, acc: 0.978723406791687)
[2025-02-13 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26][root][INFO] - Training Epoch: 2/2, step 4265/7134 completed (loss: 0.025736995041370392, acc: 0.9886363744735718)
[2025-02-13 04:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26][root][INFO] - Training Epoch: 2/2, step 4266/7134 completed (loss: 0.02554069086909294, acc: 0.9928143620491028)
[2025-02-13 04:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26][root][INFO] - Training Epoch: 2/2, step 4267/7134 completed (loss: 0.029623981565237045, acc: 0.9903030395507812)
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27][root][INFO] - Training Epoch: 2/2, step 4268/7134 completed (loss: 0.06627938151359558, acc: 0.9799599051475525)
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27][root][INFO] - Training Epoch: 2/2, step 4269/7134 completed (loss: 0.100062794983387, acc: 0.9697842001914978)
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:28][root][INFO] - Training Epoch: 2/2, step 4270/7134 completed (loss: 0.04698297753930092, acc: 0.9856770634651184)
[2025-02-13 04:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:28][root][INFO] - Training Epoch: 2/2, step 4271/7134 completed (loss: 0.031729213893413544, acc: 0.9863013625144958)
[2025-02-13 04:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29][root][INFO] - Training Epoch: 2/2, step 4272/7134 completed (loss: 0.027124224230647087, acc: 0.9894366264343262)
[2025-02-13 04:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29][root][INFO] - Training Epoch: 2/2, step 4273/7134 completed (loss: 0.026575705036520958, acc: 0.9912170767784119)
[2025-02-13 04:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30][root][INFO] - Training Epoch: 2/2, step 4274/7134 completed (loss: 0.06052188202738762, acc: 0.9824766516685486)
[2025-02-13 04:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30][root][INFO] - Training Epoch: 2/2, step 4275/7134 completed (loss: 0.017661752179265022, acc: 0.993686854839325)
[2025-02-13 04:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30][root][INFO] - Training Epoch: 2/2, step 4276/7134 completed (loss: 0.04771529883146286, acc: 0.9896907210350037)
[2025-02-13 04:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31][root][INFO] - Training Epoch: 2/2, step 4277/7134 completed (loss: 0.023610712960362434, acc: 0.9920634627342224)
[2025-02-13 04:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31][root][INFO] - Training Epoch: 2/2, step 4278/7134 completed (loss: 0.03822288662195206, acc: 0.9888888597488403)
[2025-02-13 04:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32][root][INFO] - Training Epoch: 2/2, step 4279/7134 completed (loss: 0.025443414226174355, acc: 0.992559552192688)
[2025-02-13 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32][root][INFO] - Training Epoch: 2/2, step 4280/7134 completed (loss: 0.007995675317943096, acc: 1.0)
[2025-02-13 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:33][root][INFO] - Training Epoch: 2/2, step 4281/7134 completed (loss: 0.033048875629901886, acc: 0.9876237511634827)
[2025-02-13 04:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:33][root][INFO] - Training Epoch: 2/2, step 4282/7134 completed (loss: 0.010953932069242, acc: 0.9981481432914734)
[2025-02-13 04:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34][root][INFO] - Training Epoch: 2/2, step 4283/7134 completed (loss: 0.020194627344608307, acc: 0.9942660331726074)
[2025-02-13 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34][root][INFO] - Training Epoch: 2/2, step 4284/7134 completed (loss: 0.04714592173695564, acc: 0.9818840622901917)
[2025-02-13 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34][root][INFO] - Training Epoch: 2/2, step 4285/7134 completed (loss: 0.012986903078854084, acc: 0.9982876777648926)
[2025-02-13 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:35][root][INFO] - Training Epoch: 2/2, step 4286/7134 completed (loss: 0.04438451677560806, acc: 0.9874100685119629)
[2025-02-13 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:35][root][INFO] - Training Epoch: 2/2, step 4287/7134 completed (loss: 0.047040682286024094, acc: 0.9871134161949158)
[2025-02-13 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:36][root][INFO] - Training Epoch: 2/2, step 4288/7134 completed (loss: 0.04316413030028343, acc: 0.9858155846595764)
[2025-02-13 04:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:36][root][INFO] - Training Epoch: 2/2, step 4289/7134 completed (loss: 0.03896314650774002, acc: 0.9879699349403381)
[2025-02-13 04:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37][root][INFO] - Training Epoch: 2/2, step 4290/7134 completed (loss: 0.022588102146983147, acc: 0.9905277490615845)
[2025-02-13 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37][root][INFO] - Training Epoch: 2/2, step 4291/7134 completed (loss: 0.02722763642668724, acc: 0.9904648661613464)
[2025-02-13 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37][root][INFO] - Training Epoch: 2/2, step 4292/7134 completed (loss: 0.022646024823188782, acc: 0.9939758777618408)
[2025-02-13 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38][root][INFO] - Training Epoch: 2/2, step 4293/7134 completed (loss: 0.010783637873828411, acc: 0.9979838728904724)
[2025-02-13 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38][root][INFO] - Training Epoch: 2/2, step 4294/7134 completed (loss: 0.014833186753094196, acc: 0.9927219748497009)
[2025-02-13 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39][root][INFO] - Training Epoch: 2/2, step 4295/7134 completed (loss: 0.023870190605521202, acc: 0.9908758997917175)
[2025-02-13 04:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39][root][INFO] - Training Epoch: 2/2, step 4296/7134 completed (loss: 0.019379256293177605, acc: 0.9975669384002686)
[2025-02-13 04:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39][root][INFO] - Training Epoch: 2/2, step 4297/7134 completed (loss: 0.013305357657372952, acc: 0.9953051805496216)
[2025-02-13 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40][root][INFO] - Training Epoch: 2/2, step 4298/7134 completed (loss: 0.03208012878894806, acc: 0.9903714060783386)
[2025-02-13 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40][root][INFO] - Training Epoch: 2/2, step 4299/7134 completed (loss: 0.028626056388020515, acc: 0.9914039969444275)
[2025-02-13 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41][root][INFO] - Training Epoch: 2/2, step 4300/7134 completed (loss: 0.010730229318141937, acc: 0.9978070259094238)
[2025-02-13 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41][root][INFO] - Training Epoch: 2/2, step 4301/7134 completed (loss: 0.028066612780094147, acc: 0.9918032884597778)
[2025-02-13 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42][root][INFO] - Training Epoch: 2/2, step 4302/7134 completed (loss: 0.014066963456571102, acc: 0.9955223798751831)
[2025-02-13 04:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42][root][INFO] - Training Epoch: 2/2, step 4303/7134 completed (loss: 0.04829197749495506, acc: 0.9832935333251953)
[2025-02-13 04:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42][root][INFO] - Training Epoch: 2/2, step 4304/7134 completed (loss: 0.02368256263434887, acc: 0.9925705790519714)
[2025-02-13 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:43][root][INFO] - Training Epoch: 2/2, step 4305/7134 completed (loss: 0.024914218112826347, acc: 0.9917840361595154)
[2025-02-13 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:43][root][INFO] - Training Epoch: 2/2, step 4306/7134 completed (loss: 0.06304483115673065, acc: 0.9855453372001648)
[2025-02-13 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:44][root][INFO] - Training Epoch: 2/2, step 4307/7134 completed (loss: 0.05566553771495819, acc: 0.9823434948921204)
[2025-02-13 04:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:44][root][INFO] - Training Epoch: 2/2, step 4308/7134 completed (loss: 0.04315274953842163, acc: 0.988399088382721)
[2025-02-13 04:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:45][root][INFO] - Training Epoch: 2/2, step 4309/7134 completed (loss: 0.031011400744318962, acc: 0.992601752281189)
[2025-02-13 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:45][root][INFO] - Training Epoch: 2/2, step 4310/7134 completed (loss: 0.018323281779885292, acc: 0.9941245317459106)
[2025-02-13 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46][root][INFO] - Training Epoch: 2/2, step 4311/7134 completed (loss: 0.014031150378286839, acc: 0.9942029118537903)
[2025-02-13 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46][root][INFO] - Training Epoch: 2/2, step 4312/7134 completed (loss: 0.020447801798582077, acc: 0.9918256402015686)
[2025-02-13 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46][root][INFO] - Training Epoch: 2/2, step 4313/7134 completed (loss: 0.014306505210697651, acc: 0.9948052167892456)
[2025-02-13 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:47][root][INFO] - Training Epoch: 2/2, step 4314/7134 completed (loss: 0.02614646591246128, acc: 0.9879153966903687)
[2025-02-13 04:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:47][root][INFO] - Training Epoch: 2/2, step 4315/7134 completed (loss: 0.02821618691086769, acc: 0.9921568632125854)
[2025-02-13 04:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48][root][INFO] - Training Epoch: 2/2, step 4316/7134 completed (loss: 0.029078038409352303, acc: 0.9910714030265808)
[2025-02-13 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48][root][INFO] - Training Epoch: 2/2, step 4317/7134 completed (loss: 0.04295286163687706, acc: 0.9888476133346558)
[2025-02-13 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49][root][INFO] - Training Epoch: 2/2, step 4318/7134 completed (loss: 0.025336990132927895, acc: 0.9910314083099365)
[2025-02-13 04:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49][root][INFO] - Training Epoch: 2/2, step 4319/7134 completed (loss: 0.011981823481619358, acc: 0.9988851547241211)
[2025-02-13 04:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49][root][INFO] - Training Epoch: 2/2, step 4320/7134 completed (loss: 0.020234916359186172, acc: 0.995945930480957)
[2025-02-13 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50][root][INFO] - Training Epoch: 2/2, step 4321/7134 completed (loss: 0.03845040127635002, acc: 0.9907621145248413)
[2025-02-13 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50][root][INFO] - Training Epoch: 2/2, step 4322/7134 completed (loss: 0.028194177895784378, acc: 0.9905660152435303)
[2025-02-13 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51][root][INFO] - Training Epoch: 2/2, step 4323/7134 completed (loss: 0.05324535071849823, acc: 0.9845857620239258)
[2025-02-13 04:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51][root][INFO] - Training Epoch: 2/2, step 4324/7134 completed (loss: 0.0158047117292881, acc: 0.9961240291595459)
[2025-02-13 04:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51][root][INFO] - Training Epoch: 2/2, step 4325/7134 completed (loss: 0.025429617613554, acc: 0.9920254945755005)
[2025-02-13 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:52][root][INFO] - Training Epoch: 2/2, step 4326/7134 completed (loss: 0.01942419819533825, acc: 0.9929577708244324)
[2025-02-13 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:52][root][INFO] - Training Epoch: 2/2, step 4327/7134 completed (loss: 0.015767419710755348, acc: 0.9930070042610168)
[2025-02-13 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53][root][INFO] - Training Epoch: 2/2, step 4328/7134 completed (loss: 0.03708259016275406, acc: 0.9906014800071716)
[2025-02-13 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53][root][INFO] - Training Epoch: 2/2, step 4329/7134 completed (loss: 0.03237135708332062, acc: 0.9947575330734253)
[2025-02-13 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53][root][INFO] - Training Epoch: 2/2, step 4330/7134 completed (loss: 0.03792129456996918, acc: 0.9850746393203735)
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54][root][INFO] - Training Epoch: 2/2, step 4331/7134 completed (loss: 0.005521082319319248, acc: 1.0)
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54][root][INFO] - Training Epoch: 2/2, step 4332/7134 completed (loss: 0.03863747790455818, acc: 0.9885931611061096)
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55][root][INFO] - Training Epoch: 2/2, step 4333/7134 completed (loss: 0.027553411200642586, acc: 0.9893778562545776)
[2025-02-13 04:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55][root][INFO] - Training Epoch: 2/2, step 4334/7134 completed (loss: 0.03316253423690796, acc: 0.9880239367485046)
[2025-02-13 04:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55][root][INFO] - Training Epoch: 2/2, step 4335/7134 completed (loss: 0.04110192880034447, acc: 0.9899193644523621)
[2025-02-13 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56][root][INFO] - Training Epoch: 2/2, step 4336/7134 completed (loss: 0.004441750701516867, acc: 1.0)
[2025-02-13 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56][root][INFO] - Training Epoch: 2/2, step 4337/7134 completed (loss: 0.016096126288175583, acc: 0.9952531456947327)
[2025-02-13 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57][root][INFO] - Training Epoch: 2/2, step 4338/7134 completed (loss: 0.012063134461641312, acc: 0.998420238494873)
[2025-02-13 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57][root][INFO] - Training Epoch: 2/2, step 4339/7134 completed (loss: 0.031584639102220535, acc: 0.9883381724357605)
[2025-02-13 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57][root][INFO] - Training Epoch: 2/2, step 4340/7134 completed (loss: 0.02558458037674427, acc: 0.9966216087341309)
[2025-02-13 04:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:58][root][INFO] - Training Epoch: 2/2, step 4341/7134 completed (loss: 0.03630882501602173, acc: 0.9940564632415771)
[2025-02-13 04:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:58][root][INFO] - Training Epoch: 2/2, step 4342/7134 completed (loss: 0.03614550828933716, acc: 0.9919871687889099)
[2025-02-13 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:59][root][INFO] - Training Epoch: 2/2, step 4343/7134 completed (loss: 0.020543072372674942, acc: 0.9950082898139954)
[2025-02-13 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:59][root][INFO] - Training Epoch: 2/2, step 4344/7134 completed (loss: 0.03256765380501747, acc: 0.9937629699707031)
[2025-02-13 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00][root][INFO] - Training Epoch: 2/2, step 4345/7134 completed (loss: 0.025449132546782494, acc: 0.9917184114456177)
[2025-02-13 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00][root][INFO] - Training Epoch: 2/2, step 4346/7134 completed (loss: 0.0074339210987091064, acc: 0.9979296326637268)
[2025-02-13 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00][root][INFO] - Training Epoch: 2/2, step 4347/7134 completed (loss: 0.048165712505578995, acc: 0.9900990128517151)
[2025-02-13 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01][root][INFO] - Training Epoch: 2/2, step 4348/7134 completed (loss: 0.016845280304551125, acc: 0.9968000054359436)
[2025-02-13 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01][root][INFO] - Training Epoch: 2/2, step 4349/7134 completed (loss: 0.008022183552384377, acc: 0.9976905584335327)
[2025-02-13 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01][root][INFO] - Training Epoch: 2/2, step 4350/7134 completed (loss: 0.0359431691467762, acc: 0.9908376932144165)
[2025-02-13 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:02][root][INFO] - Training Epoch: 2/2, step 4351/7134 completed (loss: 0.05901763215661049, acc: 0.9807956218719482)
[2025-02-13 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:02][root][INFO] - Training Epoch: 2/2, step 4352/7134 completed (loss: 0.042974088340997696, acc: 0.9873257279396057)
[2025-02-13 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:03][root][INFO] - Training Epoch: 2/2, step 4353/7134 completed (loss: 0.03343362361192703, acc: 0.990728497505188)
[2025-02-13 04:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:03][root][INFO] - Training Epoch: 2/2, step 4354/7134 completed (loss: 0.03015856444835663, acc: 0.9893333315849304)
[2025-02-13 04:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04][root][INFO] - Training Epoch: 2/2, step 4355/7134 completed (loss: 0.029470741748809814, acc: 0.9887359142303467)
[2025-02-13 04:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04][root][INFO] - Training Epoch: 2/2, step 4356/7134 completed (loss: 0.020772097632288933, acc: 0.9916368126869202)
[2025-02-13 04:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04][root][INFO] - Training Epoch: 2/2, step 4357/7134 completed (loss: 0.04391659423708916, acc: 0.9889763593673706)
[2025-02-13 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:05][root][INFO] - Training Epoch: 2/2, step 4358/7134 completed (loss: 0.032773301005363464, acc: 0.9904191493988037)
[2025-02-13 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:05][root][INFO] - Training Epoch: 2/2, step 4359/7134 completed (loss: 0.03597017005085945, acc: 0.9873217344284058)
[2025-02-13 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:06][root][INFO] - Training Epoch: 2/2, step 4360/7134 completed (loss: 0.04895719513297081, acc: 0.9846368432044983)
[2025-02-13 04:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:06][root][INFO] - Training Epoch: 2/2, step 4361/7134 completed (loss: 0.021651865914463997, acc: 0.9913294911384583)
[2025-02-13 04:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07][root][INFO] - Training Epoch: 2/2, step 4362/7134 completed (loss: 0.026804152876138687, acc: 0.9901356101036072)
[2025-02-13 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07][root][INFO] - Training Epoch: 2/2, step 4363/7134 completed (loss: 0.023178581148386, acc: 0.9917355179786682)
[2025-02-13 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07][root][INFO] - Training Epoch: 2/2, step 4364/7134 completed (loss: 0.015010538510978222, acc: 0.995312511920929)
[2025-02-13 04:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:08][root][INFO] - Training Epoch: 2/2, step 4365/7134 completed (loss: 0.021407991647720337, acc: 0.9905660152435303)
[2025-02-13 04:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:08][root][INFO] - Training Epoch: 2/2, step 4366/7134 completed (loss: 0.059663016349077225, acc: 0.9806700944900513)
[2025-02-13 04:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:09][root][INFO] - Training Epoch: 2/2, step 4367/7134 completed (loss: 0.02559537999331951, acc: 0.9899713397026062)
[2025-02-13 04:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:09][root][INFO] - Training Epoch: 2/2, step 4368/7134 completed (loss: 0.014120712876319885, acc: 0.992443323135376)
[2025-02-13 04:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10][root][INFO] - Training Epoch: 2/2, step 4369/7134 completed (loss: 0.01622077077627182, acc: 0.9950000047683716)
[2025-02-13 04:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10][root][INFO] - Training Epoch: 2/2, step 4370/7134 completed (loss: 0.041663020849227905, acc: 0.9911727905273438)
[2025-02-13 04:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10][root][INFO] - Training Epoch: 2/2, step 4371/7134 completed (loss: 0.037953175604343414, acc: 0.9868074059486389)
[2025-02-13 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:11][root][INFO] - Training Epoch: 2/2, step 4372/7134 completed (loss: 0.03302896022796631, acc: 0.9913941621780396)
[2025-02-13 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:11][root][INFO] - Training Epoch: 2/2, step 4373/7134 completed (loss: 0.03191080316901207, acc: 0.9892984628677368)
[2025-02-13 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:12][root][INFO] - Training Epoch: 2/2, step 4374/7134 completed (loss: 0.037526581436395645, acc: 0.9856114983558655)
[2025-02-13 04:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:12][root][INFO] - Training Epoch: 2/2, step 4375/7134 completed (loss: 0.021591003984212875, acc: 0.992277979850769)
[2025-02-13 04:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13][root][INFO] - Training Epoch: 2/2, step 4376/7134 completed (loss: 0.03217897191643715, acc: 0.9909502267837524)
[2025-02-13 04:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13][root][INFO] - Training Epoch: 2/2, step 4377/7134 completed (loss: 0.04831656813621521, acc: 0.9925834536552429)
[2025-02-13 04:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13][root][INFO] - Training Epoch: 2/2, step 4378/7134 completed (loss: 0.026560038328170776, acc: 0.9901960492134094)
[2025-02-13 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:14][root][INFO] - Training Epoch: 2/2, step 4379/7134 completed (loss: 0.0754363015294075, acc: 0.9753788113594055)
[2025-02-13 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:14][root][INFO] - Training Epoch: 2/2, step 4380/7134 completed (loss: 0.045895736664533615, acc: 0.9894921183586121)
[2025-02-13 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:15][root][INFO] - Training Epoch: 2/2, step 4381/7134 completed (loss: 0.016349362209439278, acc: 0.9944827556610107)
[2025-02-13 04:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:15][root][INFO] - Training Epoch: 2/2, step 4382/7134 completed (loss: 0.007639545947313309, acc: 1.0)
[2025-02-13 04:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][root][INFO] - Training Epoch: 2/2, step 4383/7134 completed (loss: 0.025235192850232124, acc: 0.9876203536987305)
[2025-02-13 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][root][INFO] - Training Epoch: 2/2, step 4384/7134 completed (loss: 0.0166302677243948, acc: 0.9937185645103455)
[2025-02-13 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][root][INFO] - Training Epoch: 2/2, step 4385/7134 completed (loss: 0.05078956484794617, acc: 0.9843546152114868)
[2025-02-13 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:17][root][INFO] - Training Epoch: 2/2, step 4386/7134 completed (loss: 0.03995088115334511, acc: 0.9869668483734131)
[2025-02-13 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:17][root][INFO] - Training Epoch: 2/2, step 4387/7134 completed (loss: 0.045444127172231674, acc: 0.9855072498321533)
[2025-02-13 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:18][root][INFO] - Training Epoch: 2/2, step 4388/7134 completed (loss: 0.025638991966843605, acc: 0.9906832575798035)
[2025-02-13 04:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:18][root][INFO] - Training Epoch: 2/2, step 4389/7134 completed (loss: 0.0556144043803215, acc: 0.985318124294281)
[2025-02-13 04:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19][root][INFO] - Training Epoch: 2/2, step 4390/7134 completed (loss: 0.07305776327848434, acc: 0.9761570692062378)
[2025-02-13 04:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19][root][INFO] - Training Epoch: 2/2, step 4391/7134 completed (loss: 0.052460722625255585, acc: 0.9853420257568359)
[2025-02-13 04:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19][root][INFO] - Training Epoch: 2/2, step 4392/7134 completed (loss: 0.023317519575357437, acc: 0.9950980544090271)
[2025-02-13 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20][root][INFO] - Training Epoch: 2/2, step 4393/7134 completed (loss: 0.020839514210820198, acc: 0.9943740963935852)
[2025-02-13 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20][root][INFO] - Training Epoch: 2/2, step 4394/7134 completed (loss: 0.02694951742887497, acc: 0.9929478168487549)
[2025-02-13 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21][root][INFO] - Training Epoch: 2/2, step 4395/7134 completed (loss: 0.04331420361995697, acc: 0.9811617136001587)
[2025-02-13 04:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21][root][INFO] - Training Epoch: 2/2, step 4396/7134 completed (loss: 0.06836166232824326, acc: 0.9796748161315918)
[2025-02-13 04:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21][root][INFO] - Training Epoch: 2/2, step 4397/7134 completed (loss: 0.0434725247323513, acc: 0.9835293889045715)
[2025-02-13 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:22][root][INFO] - Training Epoch: 2/2, step 4398/7134 completed (loss: 0.009333017282187939, acc: 0.997474730014801)
[2025-02-13 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:22][root][INFO] - Training Epoch: 2/2, step 4399/7134 completed (loss: 0.040532320737838745, acc: 0.9919614195823669)
[2025-02-13 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:23][root][INFO] - Training Epoch: 2/2, step 4400/7134 completed (loss: 0.06401976197957993, acc: 0.9851301312446594)
[2025-02-13 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:23][root][INFO] - Training Epoch: 2/2, step 4401/7134 completed (loss: 0.06108812987804413, acc: 0.9855072498321533)
[2025-02-13 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24][root][INFO] - Training Epoch: 2/2, step 4402/7134 completed (loss: 0.09506645798683167, acc: 0.9680232405662537)
[2025-02-13 04:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24][root][INFO] - Training Epoch: 2/2, step 4403/7134 completed (loss: 0.12504184246063232, acc: 0.9663212299346924)
[2025-02-13 04:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24][root][INFO] - Training Epoch: 2/2, step 4404/7134 completed (loss: 0.12448260933160782, acc: 0.9707446694374084)
[2025-02-13 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25][root][INFO] - Training Epoch: 2/2, step 4405/7134 completed (loss: 0.02266453206539154, acc: 0.9935204982757568)
[2025-02-13 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25][root][INFO] - Training Epoch: 2/2, step 4406/7134 completed (loss: 0.02752382680773735, acc: 0.9910913109779358)
[2025-02-13 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:26][root][INFO] - Training Epoch: 2/2, step 4407/7134 completed (loss: 0.043539948761463165, acc: 0.9861271381378174)
[2025-02-13 04:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:26][root][INFO] - Training Epoch: 2/2, step 4408/7134 completed (loss: 0.07124911993741989, acc: 0.9825378060340881)
[2025-02-13 04:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:27][root][INFO] - Training Epoch: 2/2, step 4409/7134 completed (loss: 0.04939889907836914, acc: 0.9853300452232361)
[2025-02-13 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:27][root][INFO] - Training Epoch: 2/2, step 4410/7134 completed (loss: 0.053139578551054, acc: 0.9856630563735962)
[2025-02-13 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28][root][INFO] - Training Epoch: 2/2, step 4411/7134 completed (loss: 0.10724420100450516, acc: 0.9741007089614868)
[2025-02-13 04:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28][root][INFO] - Training Epoch: 2/2, step 4412/7134 completed (loss: 0.12820793688297272, acc: 0.9595808386802673)
[2025-02-13 04:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28][root][INFO] - Training Epoch: 2/2, step 4413/7134 completed (loss: 0.08224605023860931, acc: 0.9795918464660645)
[2025-02-13 04:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:29][root][INFO] - Training Epoch: 2/2, step 4414/7134 completed (loss: 0.03020239621400833, acc: 0.9893364906311035)
[2025-02-13 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:29][root][INFO] - Training Epoch: 2/2, step 4415/7134 completed (loss: 0.057808417826890945, acc: 0.9838107228279114)
[2025-02-13 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:30][root][INFO] - Training Epoch: 2/2, step 4416/7134 completed (loss: 0.05180218815803528, acc: 0.9871645569801331)
[2025-02-13 04:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:30][root][INFO] - Training Epoch: 2/2, step 4417/7134 completed (loss: 0.07349590957164764, acc: 0.9855769276618958)
[2025-02-13 04:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31][root][INFO] - Training Epoch: 2/2, step 4418/7134 completed (loss: 0.06021709367632866, acc: 0.9830729365348816)
[2025-02-13 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31][root][INFO] - Training Epoch: 2/2, step 4419/7134 completed (loss: 0.07378872483968735, acc: 0.9771634340286255)
[2025-02-13 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31][root][INFO] - Training Epoch: 2/2, step 4420/7134 completed (loss: 0.0699247419834137, acc: 0.97648686170578)
[2025-02-13 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:32][root][INFO] - Training Epoch: 2/2, step 4421/7134 completed (loss: 0.04595469683408737, acc: 0.9887514114379883)
[2025-02-13 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:32][root][INFO] - Training Epoch: 2/2, step 4422/7134 completed (loss: 0.06587297469377518, acc: 0.9748822450637817)
[2025-02-13 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33][root][INFO] - Training Epoch: 2/2, step 4423/7134 completed (loss: 0.03928491100668907, acc: 0.9885844588279724)
[2025-02-13 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33][root][INFO] - Training Epoch: 2/2, step 4424/7134 completed (loss: 0.03073117323219776, acc: 0.9894894957542419)
[2025-02-13 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:34][root][INFO] - Training Epoch: 2/2, step 4425/7134 completed (loss: 0.027761494740843773, acc: 0.9894737005233765)
[2025-02-13 04:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:34][root][INFO] - Training Epoch: 2/2, step 4426/7134 completed (loss: 0.04911000654101372, acc: 0.9844881296157837)
[2025-02-13 04:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:34][root][INFO] - Training Epoch: 2/2, step 4427/7134 completed (loss: 0.07921064645051956, acc: 0.9710843563079834)
[2025-02-13 04:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35][root][INFO] - Training Epoch: 2/2, step 4428/7134 completed (loss: 0.023191526532173157, acc: 0.9929577708244324)
[2025-02-13 04:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35][root][INFO] - Training Epoch: 2/2, step 4429/7134 completed (loss: 0.03352843225002289, acc: 0.9845361113548279)
[2025-02-13 04:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36][root][INFO] - Training Epoch: 2/2, step 4430/7134 completed (loss: 0.04107385501265526, acc: 0.9878048896789551)
[2025-02-13 04:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36][root][INFO] - Training Epoch: 2/2, step 4431/7134 completed (loss: 0.05233655497431755, acc: 0.9875583052635193)
[2025-02-13 04:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36][root][INFO] - Training Epoch: 2/2, step 4432/7134 completed (loss: 0.01254010945558548, acc: 0.996927797794342)
[2025-02-13 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37][root][INFO] - Training Epoch: 2/2, step 4433/7134 completed (loss: 0.03496776148676872, acc: 0.9918414950370789)
[2025-02-13 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37][root][INFO] - Training Epoch: 2/2, step 4434/7134 completed (loss: 0.02261471003293991, acc: 0.9909909963607788)
[2025-02-13 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:38][root][INFO] - Training Epoch: 2/2, step 4435/7134 completed (loss: 0.018243838101625443, acc: 0.9907578825950623)
[2025-02-13 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:38][root][INFO] - Training Epoch: 2/2, step 4436/7134 completed (loss: 0.059437524527311325, acc: 0.9790502786636353)
[2025-02-13 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39][root][INFO] - Training Epoch: 2/2, step 4437/7134 completed (loss: 0.015508984215557575, acc: 0.9947506785392761)
[2025-02-13 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39][root][INFO] - Training Epoch: 2/2, step 4438/7134 completed (loss: 0.02929418906569481, acc: 0.993220329284668)
[2025-02-13 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39][root][INFO] - Training Epoch: 2/2, step 4439/7134 completed (loss: 0.010349971242249012, acc: 0.995121955871582)
[2025-02-13 04:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:40][root][INFO] - Training Epoch: 2/2, step 4440/7134 completed (loss: 0.026530176401138306, acc: 0.9903314709663391)
[2025-02-13 04:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:40][root][INFO] - Training Epoch: 2/2, step 4441/7134 completed (loss: 0.02349584735929966, acc: 0.9953917264938354)
[2025-02-13 04:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:41][root][INFO] - Training Epoch: 2/2, step 4442/7134 completed (loss: 0.050485968589782715, acc: 0.9884169697761536)
[2025-02-13 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:41][root][INFO] - Training Epoch: 2/2, step 4443/7134 completed (loss: 0.043129634112119675, acc: 0.9884615540504456)
[2025-02-13 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42][root][INFO] - Training Epoch: 2/2, step 4444/7134 completed (loss: 0.023787183687090874, acc: 0.9918434023857117)
[2025-02-13 04:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42][root][INFO] - Training Epoch: 2/2, step 4445/7134 completed (loss: 0.03412152826786041, acc: 0.9874301552772522)
[2025-02-13 04:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42][root][INFO] - Training Epoch: 2/2, step 4446/7134 completed (loss: 0.013296335004270077, acc: 0.9951691031455994)
[2025-02-13 04:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:43][root][INFO] - Training Epoch: 2/2, step 4447/7134 completed (loss: 0.025527488440275192, acc: 0.9941037893295288)
[2025-02-13 04:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:43][root][INFO] - Training Epoch: 2/2, step 4448/7134 completed (loss: 0.042628929018974304, acc: 0.9872029423713684)
[2025-02-13 04:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44][root][INFO] - Training Epoch: 2/2, step 4449/7134 completed (loss: 0.04320535063743591, acc: 0.9795538783073425)
[2025-02-13 04:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44][root][INFO] - Training Epoch: 2/2, step 4450/7134 completed (loss: 0.02899223379790783, acc: 0.9881481528282166)
[2025-02-13 04:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44][root][INFO] - Training Epoch: 2/2, step 4451/7134 completed (loss: 0.08961275219917297, acc: 0.9817578792572021)
[2025-02-13 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:45][root][INFO] - Training Epoch: 2/2, step 4452/7134 completed (loss: 0.013027516193687916, acc: 0.9919999837875366)
[2025-02-13 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:45][root][INFO] - Training Epoch: 2/2, step 4453/7134 completed (loss: 0.047733791172504425, acc: 0.9904371500015259)
[2025-02-13 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46][root][INFO] - Training Epoch: 2/2, step 4454/7134 completed (loss: 0.03101382590830326, acc: 0.9943342804908752)
[2025-02-13 04:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46][root][INFO] - Training Epoch: 2/2, step 4455/7134 completed (loss: 0.1062002032995224, acc: 0.9834254384040833)
[2025-02-13 04:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46][root][INFO] - Training Epoch: 2/2, step 4456/7134 completed (loss: 0.013742315582931042, acc: 0.9967637658119202)
[2025-02-13 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:47][root][INFO] - Training Epoch: 2/2, step 4457/7134 completed (loss: 0.021040091291069984, acc: 0.9925093650817871)
[2025-02-13 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:47][root][INFO] - Training Epoch: 2/2, step 4458/7134 completed (loss: 0.008748463355004787, acc: 0.9967266917228699)
[2025-02-13 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48][root][INFO] - Training Epoch: 2/2, step 4459/7134 completed (loss: 0.021108059212565422, acc: 0.9917582273483276)
[2025-02-13 04:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48][root][INFO] - Training Epoch: 2/2, step 4460/7134 completed (loss: 0.02295740693807602, acc: 0.9914425611495972)
[2025-02-13 04:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48][root][INFO] - Training Epoch: 2/2, step 4461/7134 completed (loss: 0.04201731085777283, acc: 0.9901599287986755)
[2025-02-13 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49][root][INFO] - Training Epoch: 2/2, step 4462/7134 completed (loss: 0.007030311040580273, acc: 0.9972752332687378)
[2025-02-13 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49][root][INFO] - Training Epoch: 2/2, step 4463/7134 completed (loss: 0.016033511608839035, acc: 0.99622642993927)
[2025-02-13 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:50][root][INFO] - Training Epoch: 2/2, step 4464/7134 completed (loss: 0.031375542283058167, acc: 0.9915682673454285)
[2025-02-13 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:50][root][INFO] - Training Epoch: 2/2, step 4465/7134 completed (loss: 0.009588537737727165, acc: 0.9961340427398682)
[2025-02-13 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:51][root][INFO] - Training Epoch: 2/2, step 4466/7134 completed (loss: 0.013245724141597748, acc: 0.9950617551803589)
[2025-02-13 04:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:51][root][INFO] - Training Epoch: 2/2, step 4467/7134 completed (loss: 0.022169865667819977, acc: 0.9924623370170593)
[2025-02-13 04:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52][root][INFO] - Training Epoch: 2/2, step 4468/7134 completed (loss: 0.008040380664169788, acc: 0.9988109469413757)
[2025-02-13 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52][root][INFO] - Training Epoch: 2/2, step 4469/7134 completed (loss: 0.019187793135643005, acc: 0.9942196607589722)
[2025-02-13 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52][root][INFO] - Training Epoch: 2/2, step 4470/7134 completed (loss: 0.01322153490036726, acc: 0.9966850876808167)
[2025-02-13 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:53][root][INFO] - Training Epoch: 2/2, step 4471/7134 completed (loss: 0.00865598302334547, acc: 0.9975429773330688)
[2025-02-13 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:53][root][INFO] - Training Epoch: 2/2, step 4472/7134 completed (loss: 0.03501901030540466, acc: 0.9934123754501343)
[2025-02-13 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:54][root][INFO] - Training Epoch: 2/2, step 4473/7134 completed (loss: 0.06305921077728271, acc: 0.9831932783126831)
[2025-02-13 04:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:54][root][INFO] - Training Epoch: 2/2, step 4474/7134 completed (loss: 0.056552667170763016, acc: 0.9846583008766174)
[2025-02-13 04:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55][root][INFO] - Training Epoch: 2/2, step 4475/7134 completed (loss: 0.09541745483875275, acc: 0.9702233076095581)
[2025-02-13 04:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55][root][INFO] - Training Epoch: 2/2, step 4476/7134 completed (loss: 0.043617989867925644, acc: 0.9849108457565308)
[2025-02-13 04:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55][root][INFO] - Training Epoch: 2/2, step 4477/7134 completed (loss: 0.006045651622116566, acc: 0.9975550174713135)
[2025-02-13 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:56][root][INFO] - Training Epoch: 2/2, step 4478/7134 completed (loss: 0.01775144599378109, acc: 0.9975278377532959)
[2025-02-13 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:56][root][INFO] - Training Epoch: 2/2, step 4479/7134 completed (loss: 0.014194129034876823, acc: 0.9973045587539673)
[2025-02-13 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:57][root][INFO] - Training Epoch: 2/2, step 4480/7134 completed (loss: 0.024326948449015617, acc: 0.9929906725883484)
[2025-02-13 04:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:57][root][INFO] - Training Epoch: 2/2, step 4481/7134 completed (loss: 0.03348618373274803, acc: 0.9929906725883484)
[2025-02-13 04:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58][root][INFO] - Training Epoch: 2/2, step 4482/7134 completed (loss: 0.011644262820482254, acc: 0.9964028596878052)
[2025-02-13 04:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58][root][INFO] - Training Epoch: 2/2, step 4483/7134 completed (loss: 0.026467982679605484, acc: 0.9925705790519714)
[2025-02-13 04:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58][root][INFO] - Training Epoch: 2/2, step 4484/7134 completed (loss: 0.011679479852318764, acc: 0.9973545074462891)
[2025-02-13 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:59][root][INFO] - Training Epoch: 2/2, step 4485/7134 completed (loss: 0.010002554394304752, acc: 0.9969372153282166)
[2025-02-13 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:59][root][INFO] - Training Epoch: 2/2, step 4486/7134 completed (loss: 0.00979609228670597, acc: 0.9985074400901794)
[2025-02-13 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00][root][INFO] - Training Epoch: 2/2, step 4487/7134 completed (loss: 0.005320937838405371, acc: 0.9986206889152527)
[2025-02-13 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00][root][INFO] - Training Epoch: 2/2, step 4488/7134 completed (loss: 0.00938801746815443, acc: 0.9964028596878052)
[2025-02-13 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00][root][INFO] - Training Epoch: 2/2, step 4489/7134 completed (loss: 0.010564401745796204, acc: 0.9982817769050598)
[2025-02-13 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:01][root][INFO] - Training Epoch: 2/2, step 4490/7134 completed (loss: 0.011426957324147224, acc: 0.9962962865829468)
[2025-02-13 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:01][root][INFO] - Training Epoch: 2/2, step 4491/7134 completed (loss: 0.002211964223533869, acc: 1.0)
[2025-02-13 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:02][root][INFO] - Training Epoch: 2/2, step 4492/7134 completed (loss: 0.010331987403333187, acc: 0.9944853186607361)
[2025-02-13 04:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:02][root][INFO] - Training Epoch: 2/2, step 4493/7134 completed (loss: 0.006514390464872122, acc: 0.9986245036125183)
[2025-02-13 04:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:03][root][INFO] - Training Epoch: 2/2, step 4494/7134 completed (loss: 0.02288568764925003, acc: 0.9942113161087036)
[2025-02-13 04:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:03][root][INFO] - Training Epoch: 2/2, step 4495/7134 completed (loss: 0.01244195643812418, acc: 0.9974457025527954)
[2025-02-13 04:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:03][root][INFO] - Training Epoch: 2/2, step 4496/7134 completed (loss: 0.01771320402622223, acc: 0.9914529919624329)
[2025-02-13 04:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:04][root][INFO] - Training Epoch: 2/2, step 4497/7134 completed (loss: 0.013629346154630184, acc: 0.9973509907722473)
[2025-02-13 04:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:04][root][INFO] - Training Epoch: 2/2, step 4498/7134 completed (loss: 0.04547742381691933, acc: 0.9927431344985962)
[2025-02-13 04:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05][root][INFO] - Training Epoch: 2/2, step 4499/7134 completed (loss: 0.02567131444811821, acc: 0.9930434823036194)
[2025-02-13 04:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05][root][INFO] - Training Epoch: 2/2, step 4500/7134 completed (loss: 0.027062302455306053, acc: 0.9930070042610168)
[2025-02-13 04:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05][root][INFO] - Training Epoch: 2/2, step 4501/7134 completed (loss: 0.017949819564819336, acc: 0.99589604139328)
[2025-02-13 04:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:06][root][INFO] - Training Epoch: 2/2, step 4502/7134 completed (loss: 0.011675572022795677, acc: 0.9970015287399292)
[2025-02-13 04:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:06][root][INFO] - Training Epoch: 2/2, step 4503/7134 completed (loss: 0.011993983760476112, acc: 0.9967897534370422)
[2025-02-13 04:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07][root][INFO] - Training Epoch: 2/2, step 4504/7134 completed (loss: 0.026214664801955223, acc: 0.9895366430282593)
[2025-02-13 04:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07][root][INFO] - Training Epoch: 2/2, step 4505/7134 completed (loss: 0.023910565301775932, acc: 0.9901719689369202)
[2025-02-13 04:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07][root][INFO] - Training Epoch: 2/2, step 4506/7134 completed (loss: 0.02005605958402157, acc: 0.9897040128707886)
[2025-02-13 04:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:08][root][INFO] - Training Epoch: 2/2, step 4507/7134 completed (loss: 0.017885372042655945, acc: 0.9943100810050964)
[2025-02-13 04:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:08][root][INFO] - Training Epoch: 2/2, step 4508/7134 completed (loss: 0.040309470146894455, acc: 0.9886040091514587)
[2025-02-13 04:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09][root][INFO] - Training Epoch: 2/2, step 4509/7134 completed (loss: 0.031867582350969315, acc: 0.991946280002594)
[2025-02-13 04:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09][root][INFO] - Training Epoch: 2/2, step 4510/7134 completed (loss: 0.04265682399272919, acc: 0.9858956336975098)
[2025-02-13 04:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10][root][INFO] - Training Epoch: 2/2, step 4511/7134 completed (loss: 0.05673115327954292, acc: 0.9831223487854004)
[2025-02-13 04:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10][root][INFO] - Training Epoch: 2/2, step 4512/7134 completed (loss: 0.025390803813934326, acc: 0.9910394549369812)
[2025-02-13 04:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10][root][INFO] - Training Epoch: 2/2, step 4513/7134 completed (loss: 0.028161874040961266, acc: 0.9943820238113403)
[2025-02-13 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:11][root][INFO] - Training Epoch: 2/2, step 4514/7134 completed (loss: 0.0762074738740921, acc: 0.9810218811035156)
[2025-02-13 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:11][root][INFO] - Training Epoch: 2/2, step 4515/7134 completed (loss: 0.021387286484241486, acc: 0.9940298795700073)
[2025-02-13 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:12][root][INFO] - Training Epoch: 2/2, step 4516/7134 completed (loss: 0.01696091517806053, acc: 0.9926560521125793)
[2025-02-13 04:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:12][root][INFO] - Training Epoch: 2/2, step 4517/7134 completed (loss: 0.033035025000572205, acc: 0.991037130355835)
[2025-02-13 04:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13][root][INFO] - Training Epoch: 2/2, step 4518/7134 completed (loss: 0.04215407744050026, acc: 0.9897959232330322)
[2025-02-13 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13][root][INFO] - Training Epoch: 2/2, step 4519/7134 completed (loss: 0.025553707033395767, acc: 0.993954062461853)
[2025-02-13 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13][root][INFO] - Training Epoch: 2/2, step 4520/7134 completed (loss: 0.018525728955864906, acc: 0.9957627058029175)
[2025-02-13 04:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:14][root][INFO] - Training Epoch: 2/2, step 4521/7134 completed (loss: 0.05752785503864288, acc: 0.9855595827102661)
[2025-02-13 04:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:14][root][INFO] - Training Epoch: 2/2, step 4522/7134 completed (loss: 0.030108798295259476, acc: 0.989924430847168)
[2025-02-13 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:15][root][INFO] - Training Epoch: 2/2, step 4523/7134 completed (loss: 0.026386110112071037, acc: 0.9881656765937805)
[2025-02-13 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:15][root][INFO] - Training Epoch: 2/2, step 4524/7134 completed (loss: 0.044251974672079086, acc: 0.9851552248001099)
[2025-02-13 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:16][root][INFO] - Training Epoch: 2/2, step 4525/7134 completed (loss: 0.0231774915009737, acc: 0.9933333396911621)
[2025-02-13 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:16][root][INFO] - Training Epoch: 2/2, step 4526/7134 completed (loss: 0.03002971038222313, acc: 0.9896774291992188)
[2025-02-13 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17][root][INFO] - Training Epoch: 2/2, step 4527/7134 completed (loss: 0.02135084755718708, acc: 0.9910813570022583)
[2025-02-13 04:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17][root][INFO] - Training Epoch: 2/2, step 4528/7134 completed (loss: 0.0437544584274292, acc: 0.9858611822128296)
[2025-02-13 04:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17][root][INFO] - Training Epoch: 2/2, step 4529/7134 completed (loss: 0.05789938196539879, acc: 0.9883419871330261)
[2025-02-13 04:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:18][root][INFO] - Training Epoch: 2/2, step 4530/7134 completed (loss: 0.018863197416067123, acc: 0.9929971694946289)
[2025-02-13 04:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:18][root][INFO] - Training Epoch: 2/2, step 4531/7134 completed (loss: 0.024452053010463715, acc: 0.9928186535835266)
[2025-02-13 04:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19][root][INFO] - Training Epoch: 2/2, step 4532/7134 completed (loss: 0.03496966511011124, acc: 0.9910358786582947)
[2025-02-13 04:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19][root][INFO] - Training Epoch: 2/2, step 4533/7134 completed (loss: 0.020523738116025925, acc: 0.9940688014030457)
[2025-02-13 04:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20][root][INFO] - Training Epoch: 2/2, step 4534/7134 completed (loss: 0.012012086808681488, acc: 0.9964788556098938)
[2025-02-13 04:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20][root][INFO] - Training Epoch: 2/2, step 4535/7134 completed (loss: 0.02301453799009323, acc: 0.9943052530288696)
[2025-02-13 04:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20][root][INFO] - Training Epoch: 2/2, step 4536/7134 completed (loss: 0.027187295258045197, acc: 0.9913793206214905)
[2025-02-13 04:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21][root][INFO] - Training Epoch: 2/2, step 4537/7134 completed (loss: 0.03151227533817291, acc: 0.9932960867881775)
[2025-02-13 04:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21][root][INFO] - Training Epoch: 2/2, step 4538/7134 completed (loss: 0.03153606131672859, acc: 0.9908046126365662)
[2025-02-13 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:22][root][INFO] - Training Epoch: 2/2, step 4539/7134 completed (loss: 0.030310912057757378, acc: 0.9892125129699707)
[2025-02-13 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:22][root][INFO] - Training Epoch: 2/2, step 4540/7134 completed (loss: 0.030641866847872734, acc: 0.9913294911384583)
[2025-02-13 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23][root][INFO] - Training Epoch: 2/2, step 4541/7134 completed (loss: 0.03912609815597534, acc: 0.9856262803077698)
[2025-02-13 04:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23][root][INFO] - Training Epoch: 2/2, step 4542/7134 completed (loss: 0.04452621564269066, acc: 0.9891774654388428)
[2025-02-13 04:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23][root][INFO] - Training Epoch: 2/2, step 4543/7134 completed (loss: 0.05936737358570099, acc: 0.9821746945381165)
[2025-02-13 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:24][root][INFO] - Training Epoch: 2/2, step 4544/7134 completed (loss: 0.030348720028996468, acc: 0.9923780560493469)
[2025-02-13 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:24][root][INFO] - Training Epoch: 2/2, step 4545/7134 completed (loss: 0.06009093299508095, acc: 0.9865125417709351)
[2025-02-13 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25][root][INFO] - Training Epoch: 2/2, step 4546/7134 completed (loss: 0.04805869236588478, acc: 0.9932773113250732)
[2025-02-13 04:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25][root][INFO] - Training Epoch: 2/2, step 4547/7134 completed (loss: 0.02889188379049301, acc: 0.9861591458320618)
[2025-02-13 04:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25][root][INFO] - Training Epoch: 2/2, step 4548/7134 completed (loss: 0.0336884930729866, acc: 0.9867330193519592)
[2025-02-13 04:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26][root][INFO] - Training Epoch: 2/2, step 4549/7134 completed (loss: 0.04921134561300278, acc: 0.9849785566329956)
[2025-02-13 04:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26][root][INFO] - Training Epoch: 2/2, step 4550/7134 completed (loss: 0.047918397933244705, acc: 0.9932998418807983)
[2025-02-13 04:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27][root][INFO] - Training Epoch: 2/2, step 4551/7134 completed (loss: 0.03396540880203247, acc: 0.9928571581840515)
[2025-02-13 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27][root][INFO] - Training Epoch: 2/2, step 4552/7134 completed (loss: 0.028256261721253395, acc: 0.9904761910438538)
[2025-02-13 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27][root][INFO] - Training Epoch: 2/2, step 4553/7134 completed (loss: 0.03411386162042618, acc: 0.9881556630134583)
[2025-02-13 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:28][root][INFO] - Training Epoch: 2/2, step 4554/7134 completed (loss: 0.04673835262656212, acc: 0.9837837815284729)
[2025-02-13 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:28][root][INFO] - Training Epoch: 2/2, step 4555/7134 completed (loss: 0.030166514217853546, acc: 0.9916897416114807)
[2025-02-13 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29][root][INFO] - Training Epoch: 2/2, step 4556/7134 completed (loss: 0.06247451901435852, acc: 0.9834586381912231)
[2025-02-13 04:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29][root][INFO] - Training Epoch: 2/2, step 4557/7134 completed (loss: 0.05267161875963211, acc: 0.985401451587677)
[2025-02-13 04:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29][root][INFO] - Training Epoch: 2/2, step 4558/7134 completed (loss: 0.043758612126111984, acc: 0.9831649661064148)
[2025-02-13 04:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:30][root][INFO] - Training Epoch: 2/2, step 4559/7134 completed (loss: 0.04300238564610481, acc: 0.9831932783126831)
[2025-02-13 04:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:30][root][INFO] - Training Epoch: 2/2, step 4560/7134 completed (loss: 0.025418788194656372, acc: 0.9945651888847351)
[2025-02-13 04:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31][root][INFO] - Training Epoch: 2/2, step 4561/7134 completed (loss: 0.04715424031019211, acc: 0.9862744808197021)
[2025-02-13 04:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31][root][INFO] - Training Epoch: 2/2, step 4562/7134 completed (loss: 0.04039694368839264, acc: 0.9834254384040833)
[2025-02-13 04:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31][root][INFO] - Training Epoch: 2/2, step 4563/7134 completed (loss: 0.030551616102457047, acc: 0.9908397197723389)
[2025-02-13 04:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32][root][INFO] - Training Epoch: 2/2, step 4564/7134 completed (loss: 0.04314993694424629, acc: 0.9844852089881897)
[2025-02-13 04:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32][root][INFO] - Training Epoch: 2/2, step 4565/7134 completed (loss: 0.0245184525847435, acc: 0.990439772605896)
[2025-02-13 04:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33][root][INFO] - Training Epoch: 2/2, step 4566/7134 completed (loss: 0.029645003378391266, acc: 0.9928315281867981)
[2025-02-13 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33][root][INFO] - Training Epoch: 2/2, step 4567/7134 completed (loss: 0.012658840976655483, acc: 0.9963235259056091)
[2025-02-13 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33][root][INFO] - Training Epoch: 2/2, step 4568/7134 completed (loss: 0.052566882222890854, acc: 0.9862385392189026)
[2025-02-13 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:34][root][INFO] - Training Epoch: 2/2, step 4569/7134 completed (loss: 0.013788496144115925, acc: 0.9941176176071167)
[2025-02-13 04:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:34][root][INFO] - Training Epoch: 2/2, step 4570/7134 completed (loss: 0.0202163252979517, acc: 0.9927113652229309)
[2025-02-13 04:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35][root][INFO] - Training Epoch: 2/2, step 4571/7134 completed (loss: 0.02221149392426014, acc: 0.9935815334320068)
[2025-02-13 04:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35][root][INFO] - Training Epoch: 2/2, step 4572/7134 completed (loss: 0.030051065608859062, acc: 0.9922118186950684)
[2025-02-13 04:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35][root][INFO] - Training Epoch: 2/2, step 4573/7134 completed (loss: 0.028412694111466408, acc: 0.9929971694946289)
[2025-02-13 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:36][root][INFO] - Training Epoch: 2/2, step 4574/7134 completed (loss: 0.011142703704535961, acc: 0.9946996569633484)
[2025-02-13 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:36][root][INFO] - Training Epoch: 2/2, step 4575/7134 completed (loss: 0.03611097112298012, acc: 0.9893617033958435)
[2025-02-13 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:37][root][INFO] - Training Epoch: 2/2, step 4576/7134 completed (loss: 0.009078689850866795, acc: 0.9962359070777893)
[2025-02-13 04:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:37][root][INFO] - Training Epoch: 2/2, step 4577/7134 completed (loss: 0.02687220834195614, acc: 0.992977499961853)
[2025-02-13 04:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38][root][INFO] - Training Epoch: 2/2, step 4578/7134 completed (loss: 0.01308402605354786, acc: 0.9956331849098206)
[2025-02-13 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38][root][INFO] - Training Epoch: 2/2, step 4579/7134 completed (loss: 0.04629812389612198, acc: 0.9904912710189819)
[2025-02-13 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38][root][INFO] - Training Epoch: 2/2, step 4580/7134 completed (loss: 0.018965639173984528, acc: 0.9925373196601868)
[2025-02-13 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:39][root][INFO] - Training Epoch: 2/2, step 4581/7134 completed (loss: 0.019289422780275345, acc: 0.9929453134536743)
[2025-02-13 04:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:39][root][INFO] - Training Epoch: 2/2, step 4582/7134 completed (loss: 0.01580791175365448, acc: 0.9917762875556946)
[2025-02-13 04:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40][root][INFO] - Training Epoch: 2/2, step 4583/7134 completed (loss: 0.008389392867684364, acc: 0.9950980544090271)
[2025-02-13 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40][root][INFO] - Training Epoch: 2/2, step 4584/7134 completed (loss: 0.011706524528563023, acc: 0.9951691031455994)
[2025-02-13 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40][root][INFO] - Training Epoch: 2/2, step 4585/7134 completed (loss: 0.008320614695549011, acc: 0.9983818531036377)
[2025-02-13 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:41][root][INFO] - Training Epoch: 2/2, step 4586/7134 completed (loss: 0.02029687538743019, acc: 0.9921976327896118)
[2025-02-13 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:41][root][INFO] - Training Epoch: 2/2, step 4587/7134 completed (loss: 0.022652562707662582, acc: 0.9945205450057983)
[2025-02-13 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:42][root][INFO] - Training Epoch: 2/2, step 4588/7134 completed (loss: 0.042550407350063324, acc: 0.9840116500854492)
[2025-02-13 04:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:42][root][INFO] - Training Epoch: 2/2, step 4589/7134 completed (loss: 0.04631125181913376, acc: 0.9865269660949707)
[2025-02-13 04:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43][root][INFO] - Training Epoch: 2/2, step 4590/7134 completed (loss: 0.0314343087375164, acc: 0.9917126893997192)
[2025-02-13 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43][root][INFO] - Training Epoch: 2/2, step 4591/7134 completed (loss: 0.023888517171144485, acc: 0.9881266355514526)
[2025-02-13 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43][root][INFO] - Training Epoch: 2/2, step 4592/7134 completed (loss: 0.032296303659677505, acc: 0.9883889555931091)
[2025-02-13 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44][root][INFO] - Training Epoch: 2/2, step 4593/7134 completed (loss: 0.03683702275156975, acc: 0.9925925731658936)
[2025-02-13 04:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44][root][INFO] - Training Epoch: 2/2, step 4594/7134 completed (loss: 0.037349849939346313, acc: 0.9893333315849304)
[2025-02-13 04:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45][root][INFO] - Training Epoch: 2/2, step 4595/7134 completed (loss: 0.022904694080352783, acc: 0.9925705790519714)
[2025-02-13 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45][root][INFO] - Training Epoch: 2/2, step 4596/7134 completed (loss: 0.03044789843261242, acc: 0.9873015880584717)
[2025-02-13 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45][root][INFO] - Training Epoch: 2/2, step 4597/7134 completed (loss: 0.016121651977300644, acc: 0.9963548183441162)
[2025-02-13 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:46][root][INFO] - Training Epoch: 2/2, step 4598/7134 completed (loss: 0.04971563443541527, acc: 0.9865030646324158)
[2025-02-13 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:46][root][INFO] - Training Epoch: 2/2, step 4599/7134 completed (loss: 0.01954854279756546, acc: 0.9920508861541748)
[2025-02-13 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:47][root][INFO] - Training Epoch: 2/2, step 4600/7134 completed (loss: 0.01754157990217209, acc: 0.9959404468536377)
[2025-02-13 04:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:47][root][INFO] - Training Epoch: 2/2, step 4601/7134 completed (loss: 0.03976019471883774, acc: 0.9843546152114868)
[2025-02-13 04:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48][root][INFO] - Training Epoch: 2/2, step 4602/7134 completed (loss: 0.027701187878847122, acc: 0.9904631972312927)
[2025-02-13 04:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48][root][INFO] - Training Epoch: 2/2, step 4603/7134 completed (loss: 0.02961447276175022, acc: 0.9944827556610107)
[2025-02-13 04:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48][root][INFO] - Training Epoch: 2/2, step 4604/7134 completed (loss: 0.018259380012750626, acc: 0.9887640476226807)
[2025-02-13 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:49][root][INFO] - Training Epoch: 2/2, step 4605/7134 completed (loss: 0.037707190960645676, acc: 0.9847161769866943)
[2025-02-13 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:49][root][INFO] - Training Epoch: 2/2, step 4606/7134 completed (loss: 0.014396521262824535, acc: 0.9954128265380859)
[2025-02-13 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:50][root][INFO] - Training Epoch: 2/2, step 4607/7134 completed (loss: 0.012644555419683456, acc: 0.9915825128555298)
[2025-02-13 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:50][root][INFO] - Training Epoch: 2/2, step 4608/7134 completed (loss: 0.009767537005245686, acc: 0.9970370531082153)
[2025-02-13 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51][root][INFO] - Training Epoch: 2/2, step 4609/7134 completed (loss: 0.017950832843780518, acc: 0.9944953918457031)
[2025-02-13 04:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51][root][INFO] - Training Epoch: 2/2, step 4610/7134 completed (loss: 0.018945282325148582, acc: 0.9907038807868958)
[2025-02-13 04:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51][root][INFO] - Training Epoch: 2/2, step 4611/7134 completed (loss: 0.00891829188913107, acc: 0.9968701004981995)
[2025-02-13 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:52][root][INFO] - Training Epoch: 2/2, step 4612/7134 completed (loss: 0.019722359254956245, acc: 0.9944238066673279)
[2025-02-13 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:52][root][INFO] - Training Epoch: 2/2, step 4613/7134 completed (loss: 0.017996056005358696, acc: 0.9947916865348816)
[2025-02-13 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53][root][INFO] - Training Epoch: 2/2, step 4614/7134 completed (loss: 0.019759133458137512, acc: 0.9924242496490479)
[2025-02-13 04:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53][root][INFO] - Training Epoch: 2/2, step 4615/7134 completed (loss: 0.03577835485339165, acc: 0.9864314794540405)
[2025-02-13 04:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53][root][INFO] - Training Epoch: 2/2, step 4616/7134 completed (loss: 0.04001057147979736, acc: 0.9881656765937805)
[2025-02-13 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:54][root][INFO] - Training Epoch: 2/2, step 4617/7134 completed (loss: 0.02328258752822876, acc: 0.9894894957542419)
[2025-02-13 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:54][root][INFO] - Training Epoch: 2/2, step 4618/7134 completed (loss: 0.02987334504723549, acc: 0.9883494973182678)
[2025-02-13 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55][root][INFO] - Training Epoch: 2/2, step 4619/7134 completed (loss: 0.010068521834909916, acc: 1.0)
[2025-02-13 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55][root][INFO] - Training Epoch: 2/2, step 4620/7134 completed (loss: 0.013607207685709, acc: 0.9942029118537903)
[2025-02-13 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55][root][INFO] - Training Epoch: 2/2, step 4621/7134 completed (loss: 0.014424550347030163, acc: 0.9922839403152466)
[2025-02-13 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:56][root][INFO] - Training Epoch: 2/2, step 4622/7134 completed (loss: 0.025111550465226173, acc: 0.995006263256073)
[2025-02-13 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:56][root][INFO] - Training Epoch: 2/2, step 4623/7134 completed (loss: 0.039247699081897736, acc: 0.9869281053543091)
[2025-02-13 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:57][root][INFO] - Training Epoch: 2/2, step 4624/7134 completed (loss: 0.054461635649204254, acc: 0.9835391044616699)
[2025-02-13 04:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:57][root][INFO] - Training Epoch: 2/2, step 4625/7134 completed (loss: 0.09609386324882507, acc: 0.9688013195991516)
[2025-02-13 04:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58][root][INFO] - Training Epoch: 2/2, step 4626/7134 completed (loss: 0.04545021802186966, acc: 0.9892617464065552)
[2025-02-13 04:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58][root][INFO] - Training Epoch: 2/2, step 4627/7134 completed (loss: 0.06636357307434082, acc: 0.9787557125091553)
[2025-02-13 04:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58][root][INFO] - Training Epoch: 2/2, step 4628/7134 completed (loss: 0.04649445414543152, acc: 0.9826086759567261)
[2025-02-13 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:59][root][INFO] - Training Epoch: 2/2, step 4629/7134 completed (loss: 0.058818548917770386, acc: 0.9794871807098389)
[2025-02-13 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:59][root][INFO] - Training Epoch: 2/2, step 4630/7134 completed (loss: 0.040259990841150284, acc: 0.9894319772720337)
[2025-02-13 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:00][root][INFO] - Training Epoch: 2/2, step 4631/7134 completed (loss: 0.03907593712210655, acc: 0.9878318309783936)
[2025-02-13 04:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:00][root][INFO] - Training Epoch: 2/2, step 4632/7134 completed (loss: 0.03761071711778641, acc: 0.9897058606147766)
[2025-02-13 04:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01][root][INFO] - Training Epoch: 2/2, step 4633/7134 completed (loss: 0.042227186262607574, acc: 0.9872832298278809)
[2025-02-13 04:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01][root][INFO] - Training Epoch: 2/2, step 4634/7134 completed (loss: 0.028202762827277184, acc: 0.9902912378311157)
[2025-02-13 04:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01][root][INFO] - Training Epoch: 2/2, step 4635/7134 completed (loss: 0.05629580095410347, acc: 0.9893292784690857)
[2025-02-13 04:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:02][root][INFO] - Training Epoch: 2/2, step 4636/7134 completed (loss: 0.011902078986167908, acc: 0.9974226951599121)
[2025-02-13 04:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:02][root][INFO] - Training Epoch: 2/2, step 4637/7134 completed (loss: 0.02398935705423355, acc: 0.9919354915618896)
[2025-02-13 04:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03][root][INFO] - Training Epoch: 2/2, step 4638/7134 completed (loss: 0.036233287304639816, acc: 0.9894319772720337)
[2025-02-13 04:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03][root][INFO] - Training Epoch: 2/2, step 4639/7134 completed (loss: 0.03411915898323059, acc: 0.9939172863960266)
[2025-02-13 04:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03][root][INFO] - Training Epoch: 2/2, step 4640/7134 completed (loss: 0.041134994477033615, acc: 0.9884615540504456)
[2025-02-13 04:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:04][root][INFO] - Training Epoch: 2/2, step 4641/7134 completed (loss: 0.017847711220383644, acc: 0.9971590638160706)
[2025-02-13 04:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:04][root][INFO] - Training Epoch: 2/2, step 4642/7134 completed (loss: 0.014459120109677315, acc: 0.9920634627342224)
[2025-02-13 04:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:05][root][INFO] - Training Epoch: 2/2, step 4643/7134 completed (loss: 0.02303755283355713, acc: 0.9952662587165833)
[2025-02-13 04:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:05][root][INFO] - Training Epoch: 2/2, step 4644/7134 completed (loss: 0.017902052029967308, acc: 0.997633159160614)
[2025-02-13 04:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:06][root][INFO] - Training Epoch: 2/2, step 4645/7134 completed (loss: 0.01662592589855194, acc: 0.9944367408752441)
[2025-02-13 04:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:06][root][INFO] - Training Epoch: 2/2, step 4646/7134 completed (loss: 0.015184752643108368, acc: 0.9937205910682678)
[2025-02-13 04:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07][root][INFO] - Training Epoch: 2/2, step 4647/7134 completed (loss: 0.06208052113652229, acc: 0.9895833134651184)
[2025-02-13 04:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07][root][INFO] - Training Epoch: 2/2, step 4648/7134 completed (loss: 0.018241384997963905, acc: 0.9945130348205566)
[2025-02-13 04:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07][root][INFO] - Training Epoch: 2/2, step 4649/7134 completed (loss: 0.027652475982904434, acc: 0.9929947257041931)
[2025-02-13 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:08][root][INFO] - Training Epoch: 2/2, step 4650/7134 completed (loss: 0.03787973150610924, acc: 0.988095223903656)
[2025-02-13 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:08][root][INFO] - Training Epoch: 2/2, step 4651/7134 completed (loss: 0.06930246204137802, acc: 0.9771863222122192)
[2025-02-13 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:09][root][INFO] - Training Epoch: 2/2, step 4652/7134 completed (loss: 0.030781729146838188, acc: 0.9923780560493469)
[2025-02-13 04:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:09][root][INFO] - Training Epoch: 2/2, step 4653/7134 completed (loss: 0.04217143356800079, acc: 0.9860215187072754)
[2025-02-13 04:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:10][root][INFO] - Training Epoch: 2/2, step 4654/7134 completed (loss: 0.0603976771235466, acc: 0.9805115461349487)
[2025-02-13 04:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:10][root][INFO] - Training Epoch: 2/2, step 4655/7134 completed (loss: 0.04096687585115433, acc: 0.9904240965843201)
[2025-02-13 04:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11][root][INFO] - Training Epoch: 2/2, step 4656/7134 completed (loss: 0.03352326899766922, acc: 0.9924242496490479)
[2025-02-13 04:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11][root][INFO] - Training Epoch: 2/2, step 4657/7134 completed (loss: 0.04520253464579582, acc: 0.9915013909339905)
[2025-02-13 04:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11][root][INFO] - Training Epoch: 2/2, step 4658/7134 completed (loss: 0.04323435202240944, acc: 0.9851149916648865)
[2025-02-13 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:12][root][INFO] - Training Epoch: 2/2, step 4659/7134 completed (loss: 0.06560665369033813, acc: 0.9845758080482483)
[2025-02-13 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:12][root][INFO] - Training Epoch: 2/2, step 4660/7134 completed (loss: 0.07289785146713257, acc: 0.9811912178993225)
[2025-02-13 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13][root][INFO] - Training Epoch: 2/2, step 4661/7134 completed (loss: 0.03265893831849098, acc: 0.9923780560493469)
[2025-02-13 04:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13][root][INFO] - Training Epoch: 2/2, step 4662/7134 completed (loss: 0.06606447696685791, acc: 0.9829931855201721)
[2025-02-13 04:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13][root][INFO] - Training Epoch: 2/2, step 4663/7134 completed (loss: 0.057816632091999054, acc: 0.9866310358047485)
[2025-02-13 04:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14][root][INFO] - Training Epoch: 2/2, step 4664/7134 completed (loss: 0.03469904884696007, acc: 0.9933949708938599)
[2025-02-13 04:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14][root][INFO] - Training Epoch: 2/2, step 4665/7134 completed (loss: 0.023839987814426422, acc: 0.9972714781761169)
[2025-02-13 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:15][root][INFO] - Training Epoch: 2/2, step 4666/7134 completed (loss: 0.03331436216831207, acc: 0.9921773076057434)
[2025-02-13 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:15][root][INFO] - Training Epoch: 2/2, step 4667/7134 completed (loss: 0.05368628725409508, acc: 0.9827883243560791)
[2025-02-13 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16][root][INFO] - Training Epoch: 2/2, step 4668/7134 completed (loss: 0.017520247027277946, acc: 0.9946236610412598)
[2025-02-13 04:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16][root][INFO] - Training Epoch: 2/2, step 4669/7134 completed (loss: 0.02425350993871689, acc: 0.9911242723464966)
[2025-02-13 04:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16][root][INFO] - Training Epoch: 2/2, step 4670/7134 completed (loss: 0.06392600387334824, acc: 0.9824561476707458)
[2025-02-13 04:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:17][root][INFO] - Training Epoch: 2/2, step 4671/7134 completed (loss: 0.03443522751331329, acc: 0.9901269674301147)
[2025-02-13 04:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:17][root][INFO] - Training Epoch: 2/2, step 4672/7134 completed (loss: 0.045214418321847916, acc: 0.9878048896789551)
[2025-02-13 04:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18][root][INFO] - Training Epoch: 2/2, step 4673/7134 completed (loss: 0.04092717170715332, acc: 0.9828392863273621)
[2025-02-13 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18][root][INFO] - Training Epoch: 2/2, step 4674/7134 completed (loss: 0.03396254777908325, acc: 0.9920760989189148)
[2025-02-13 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18][root][INFO] - Training Epoch: 2/2, step 4675/7134 completed (loss: 0.03520180657505989, acc: 0.9923567175865173)
[2025-02-13 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:19][root][INFO] - Training Epoch: 2/2, step 4676/7134 completed (loss: 0.01599201001226902, acc: 0.9954819083213806)
[2025-02-13 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:19][root][INFO] - Training Epoch: 2/2, step 4677/7134 completed (loss: 0.025993945077061653, acc: 0.9934210777282715)
[2025-02-13 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20][root][INFO] - Training Epoch: 2/2, step 4678/7134 completed (loss: 0.04852769523859024, acc: 0.9866270422935486)
[2025-02-13 04:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20][root][INFO] - Training Epoch: 2/2, step 4679/7134 completed (loss: 0.03297828510403633, acc: 0.9889655113220215)
[2025-02-13 04:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20][root][INFO] - Training Epoch: 2/2, step 4680/7134 completed (loss: 0.05086512118577957, acc: 0.985981285572052)
[2025-02-13 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:21][root][INFO] - Training Epoch: 2/2, step 4681/7134 completed (loss: 0.03173629567027092, acc: 0.989230751991272)
[2025-02-13 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:21][root][INFO] - Training Epoch: 2/2, step 4682/7134 completed (loss: 0.019046612083911896, acc: 0.9963503479957581)
[2025-02-13 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:22][root][INFO] - Training Epoch: 2/2, step 4683/7134 completed (loss: 0.03013533353805542, acc: 0.9900142550468445)
[2025-02-13 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:22][root][INFO] - Training Epoch: 2/2, step 4684/7134 completed (loss: 0.05401317775249481, acc: 0.9821802973747253)
[2025-02-13 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23][root][INFO] - Training Epoch: 2/2, step 4685/7134 completed (loss: 0.03617218881845474, acc: 0.9866803288459778)
[2025-02-13 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23][root][INFO] - Training Epoch: 2/2, step 4686/7134 completed (loss: 0.028752995654940605, acc: 0.9930151104927063)
[2025-02-13 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23][root][INFO] - Training Epoch: 2/2, step 4687/7134 completed (loss: 0.09466049820184708, acc: 0.9693251252174377)
[2025-02-13 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:24][root][INFO] - Training Epoch: 2/2, step 4688/7134 completed (loss: 0.13562345504760742, acc: 0.9589322209358215)
[2025-02-13 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:24][root][INFO] - Training Epoch: 2/2, step 4689/7134 completed (loss: 0.10359915345907211, acc: 0.9749631881713867)
[2025-02-13 04:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:25][root][INFO] - Training Epoch: 2/2, step 4690/7134 completed (loss: 0.06544621288776398, acc: 0.9853249192237854)
[2025-02-13 04:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:25][root][INFO] - Training Epoch: 2/2, step 4691/7134 completed (loss: 0.030401501804590225, acc: 0.9863945841789246)
[2025-02-13 04:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26][root][INFO] - Training Epoch: 2/2, step 4692/7134 completed (loss: 0.11144529283046722, acc: 0.9723991751670837)
[2025-02-13 04:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26][root][INFO] - Training Epoch: 2/2, step 4693/7134 completed (loss: 0.1401969939470291, acc: 0.965413510799408)
[2025-02-13 04:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26][root][INFO] - Training Epoch: 2/2, step 4694/7134 completed (loss: 0.1393749862909317, acc: 0.9756592512130737)
[2025-02-13 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:27][root][INFO] - Training Epoch: 2/2, step 4695/7134 completed (loss: 0.05478539690375328, acc: 0.9819375872612)
[2025-02-13 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:27][root][INFO] - Training Epoch: 2/2, step 4696/7134 completed (loss: 0.02134002558887005, acc: 0.9925705790519714)
[2025-02-13 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28][root][INFO] - Training Epoch: 2/2, step 4697/7134 completed (loss: 0.02842407114803791, acc: 0.9906014800071716)
[2025-02-13 04:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28][root][INFO] - Training Epoch: 2/2, step 4698/7134 completed (loss: 0.09366418421268463, acc: 0.9751908183097839)
[2025-02-13 04:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28][root][INFO] - Training Epoch: 2/2, step 4699/7134 completed (loss: 0.053836338222026825, acc: 0.9908592104911804)
[2025-02-13 04:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:29][root][INFO] - Training Epoch: 2/2, step 4700/7134 completed (loss: 0.0531449168920517, acc: 0.9821162223815918)
[2025-02-13 04:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:29][root][INFO] - Training Epoch: 2/2, step 4701/7134 completed (loss: 0.04754181206226349, acc: 0.9872204661369324)
[2025-02-13 04:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:30][root][INFO] - Training Epoch: 2/2, step 4702/7134 completed (loss: 0.04352334141731262, acc: 0.987261176109314)
[2025-02-13 04:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:30][root][INFO] - Training Epoch: 2/2, step 4703/7134 completed (loss: 0.03620237112045288, acc: 0.9891892075538635)
[2025-02-13 04:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31][root][INFO] - Training Epoch: 2/2, step 4704/7134 completed (loss: 0.020369380712509155, acc: 0.9941383600234985)
[2025-02-13 04:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31][root][INFO] - Training Epoch: 2/2, step 4705/7134 completed (loss: 0.030569808557629585, acc: 0.9871345162391663)
[2025-02-13 04:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31][root][INFO] - Training Epoch: 2/2, step 4706/7134 completed (loss: 0.08884429931640625, acc: 0.9781931638717651)
[2025-02-13 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:32][root][INFO] - Training Epoch: 2/2, step 4707/7134 completed (loss: 0.14561323821544647, acc: 0.9710884094238281)
[2025-02-13 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:32][root][INFO] - Training Epoch: 2/2, step 4708/7134 completed (loss: 0.0166719239205122, acc: 0.9925187230110168)
[2025-02-13 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:33][root][INFO] - Training Epoch: 2/2, step 4709/7134 completed (loss: 0.021212896332144737, acc: 0.9934123754501343)
[2025-02-13 04:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:33][root][INFO] - Training Epoch: 2/2, step 4710/7134 completed (loss: 0.034509409219026566, acc: 0.9912663698196411)
[2025-02-13 04:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34][root][INFO] - Training Epoch: 2/2, step 4711/7134 completed (loss: 0.054889678955078125, acc: 0.9830220937728882)
[2025-02-13 04:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34][root][INFO] - Training Epoch: 2/2, step 4712/7134 completed (loss: 0.06654514372348785, acc: 0.9818181991577148)
[2025-02-13 04:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34][root][INFO] - Training Epoch: 2/2, step 4713/7134 completed (loss: 0.016740061342716217, acc: 0.9964285492897034)
[2025-02-13 04:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:35][root][INFO] - Training Epoch: 2/2, step 4714/7134 completed (loss: 0.04664275050163269, acc: 0.9831775426864624)
[2025-02-13 04:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:35][root][INFO] - Training Epoch: 2/2, step 4715/7134 completed (loss: 0.08683472871780396, acc: 0.9765458703041077)
[2025-02-13 04:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36][root][INFO] - Training Epoch: 2/2, step 4716/7134 completed (loss: 0.007894158363342285, acc: 0.9969512224197388)
[2025-02-13 04:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36][root][INFO] - Training Epoch: 2/2, step 4717/7134 completed (loss: 0.030654868111014366, acc: 0.995555579662323)
[2025-02-13 04:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36][root][INFO] - Training Epoch: 2/2, step 4718/7134 completed (loss: 0.031008874997496605, acc: 0.9900662302970886)
[2025-02-13 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:37][root][INFO] - Training Epoch: 2/2, step 4719/7134 completed (loss: 0.02309565059840679, acc: 0.9941002726554871)
[2025-02-13 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:37][root][INFO] - Training Epoch: 2/2, step 4720/7134 completed (loss: 0.005184610839933157, acc: 0.9985975027084351)
[2025-02-13 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38][root][INFO] - Training Epoch: 2/2, step 4721/7134 completed (loss: 0.0070572467520833015, acc: 0.9956772327423096)
[2025-02-13 04:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38][root][INFO] - Training Epoch: 2/2, step 4722/7134 completed (loss: 0.016311101615428925, acc: 0.9911242723464966)
[2025-02-13 04:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38][root][INFO] - Training Epoch: 2/2, step 4723/7134 completed (loss: 0.008276489563286304, acc: 0.9969512224197388)
[2025-02-13 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:39][root][INFO] - Training Epoch: 2/2, step 4724/7134 completed (loss: 0.026112567633390427, acc: 0.9911242723464966)
[2025-02-13 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:39][root][INFO] - Training Epoch: 2/2, step 4725/7134 completed (loss: 0.011678637936711311, acc: 0.9973614811897278)
[2025-02-13 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:40][root][INFO] - Training Epoch: 2/2, step 4726/7134 completed (loss: 0.04137585312128067, acc: 0.9872340559959412)
[2025-02-13 04:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:40][root][INFO] - Training Epoch: 2/2, step 4727/7134 completed (loss: 0.03860774263739586, acc: 0.9890282154083252)
[2025-02-13 04:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41][root][INFO] - Training Epoch: 2/2, step 4728/7134 completed (loss: 0.03035586141049862, acc: 0.9868203997612)
[2025-02-13 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41][root][INFO] - Training Epoch: 2/2, step 4729/7134 completed (loss: 0.020177381113171577, acc: 0.9943661689758301)
[2025-02-13 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41][root][INFO] - Training Epoch: 2/2, step 4730/7134 completed (loss: 0.019307194277644157, acc: 0.9938744306564331)
[2025-02-13 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:42][root][INFO] - Training Epoch: 2/2, step 4731/7134 completed (loss: 0.029111014679074287, acc: 0.9923664331436157)
[2025-02-13 04:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:42][root][INFO] - Training Epoch: 2/2, step 4732/7134 completed (loss: 0.050524402409791946, acc: 0.9836660623550415)
[2025-02-13 04:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43][root][INFO] - Training Epoch: 2/2, step 4733/7134 completed (loss: 0.007134812884032726, acc: 0.9967690110206604)
[2025-02-13 04:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43][root][INFO] - Training Epoch: 2/2, step 4734/7134 completed (loss: 0.038219254463911057, acc: 0.9856915473937988)
[2025-02-13 04:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43][root][INFO] - Training Epoch: 2/2, step 4735/7134 completed (loss: 0.018873203545808792, acc: 0.9954954981803894)
[2025-02-13 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:44][root][INFO] - Training Epoch: 2/2, step 4736/7134 completed (loss: 0.053887754678726196, acc: 0.990138053894043)
[2025-02-13 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:44][root][INFO] - Training Epoch: 2/2, step 4737/7134 completed (loss: 0.023592544719576836, acc: 0.9952977895736694)
[2025-02-13 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45][root][INFO] - Training Epoch: 2/2, step 4738/7134 completed (loss: 0.0160298440605402, acc: 0.9939302206039429)
[2025-02-13 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45][root][INFO] - Training Epoch: 2/2, step 4739/7134 completed (loss: 0.08153443038463593, acc: 0.9712460041046143)
[2025-02-13 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45][root][INFO] - Training Epoch: 2/2, step 4740/7134 completed (loss: 0.06463296711444855, acc: 0.9805389046669006)
[2025-02-13 04:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:46][root][INFO] - Training Epoch: 2/2, step 4741/7134 completed (loss: 0.04579876735806465, acc: 0.9870610237121582)
[2025-02-13 04:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:46][root][INFO] - Training Epoch: 2/2, step 4742/7134 completed (loss: 0.07165984064340591, acc: 0.982300877571106)
[2025-02-13 04:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47][root][INFO] - Training Epoch: 2/2, step 4743/7134 completed (loss: 0.042284052819013596, acc: 0.9839857816696167)
[2025-02-13 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47][root][INFO] - Training Epoch: 2/2, step 4744/7134 completed (loss: 0.031158503144979477, acc: 0.9917355179786682)
[2025-02-13 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47][root][INFO] - Training Epoch: 2/2, step 4745/7134 completed (loss: 0.031504370272159576, acc: 0.9899665713310242)
[2025-02-13 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48][root][INFO] - Training Epoch: 2/2, step 4746/7134 completed (loss: 0.028328701853752136, acc: 0.998161792755127)
[2025-02-13 04:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48][root][INFO] - Training Epoch: 2/2, step 4747/7134 completed (loss: 0.020390313118696213, acc: 0.9946019053459167)
[2025-02-13 04:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48][root][INFO] - Training Epoch: 2/2, step 4748/7134 completed (loss: 0.05783487856388092, acc: 0.9808259606361389)
[2025-02-13 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49][root][INFO] - Training Epoch: 2/2, step 4749/7134 completed (loss: 0.04188309237360954, acc: 0.9933333396911621)
[2025-02-13 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49][root][INFO] - Training Epoch: 2/2, step 4750/7134 completed (loss: 0.052163757383823395, acc: 0.9807322025299072)
[2025-02-13 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:50][root][INFO] - Training Epoch: 2/2, step 4751/7134 completed (loss: 0.019419267773628235, acc: 0.9943422675132751)
[2025-02-13 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:50][root][INFO] - Training Epoch: 2/2, step 4752/7134 completed (loss: 0.04631049558520317, acc: 0.9923195242881775)
[2025-02-13 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:51][root][INFO] - Training Epoch: 2/2, step 4753/7134 completed (loss: 0.07623592764139175, acc: 0.9804216623306274)
[2025-02-13 04:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:51][root][INFO] - Training Epoch: 2/2, step 4754/7134 completed (loss: 0.01666681468486786, acc: 0.997063159942627)
[2025-02-13 04:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52][root][INFO] - Training Epoch: 2/2, step 4755/7134 completed (loss: 0.054544128477573395, acc: 0.9861303567886353)
[2025-02-13 04:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52][root][INFO] - Training Epoch: 2/2, step 4756/7134 completed (loss: 0.062160391360521317, acc: 0.9895397424697876)
[2025-02-13 04:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52][root][INFO] - Training Epoch: 2/2, step 4757/7134 completed (loss: 0.025027522817254066, acc: 0.9942594766616821)
[2025-02-13 04:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53][root][INFO] - Training Epoch: 2/2, step 4758/7134 completed (loss: 0.02716662921011448, acc: 0.9915966391563416)
[2025-02-13 04:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53][root][INFO] - Training Epoch: 2/2, step 4759/7134 completed (loss: 0.011912131682038307, acc: 0.998275876045227)
[2025-02-13 04:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54][root][INFO] - Training Epoch: 2/2, step 4760/7134 completed (loss: 0.030331280082464218, acc: 0.9937888383865356)
[2025-02-13 04:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54][root][INFO] - Training Epoch: 2/2, step 4761/7134 completed (loss: 0.06415338814258575, acc: 0.989313006401062)
[2025-02-13 04:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54][root][INFO] - Training Epoch: 2/2, step 4762/7134 completed (loss: 0.0534525029361248, acc: 0.9891892075538635)
[2025-02-13 04:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:55][root][INFO] - Training Epoch: 2/2, step 4763/7134 completed (loss: 0.05184260010719299, acc: 0.9863221645355225)
[2025-02-13 04:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:55][root][INFO] - Training Epoch: 2/2, step 4764/7134 completed (loss: 0.0323091484606266, acc: 0.988875150680542)
[2025-02-13 04:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56][root][INFO] - Training Epoch: 2/2, step 4765/7134 completed (loss: 0.05356666445732117, acc: 0.9870503544807434)
[2025-02-13 04:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56][root][INFO] - Training Epoch: 2/2, step 4766/7134 completed (loss: 0.04163181781768799, acc: 0.9897040128707886)
[2025-02-13 04:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56][root][INFO] - Training Epoch: 2/2, step 4767/7134 completed (loss: 0.05488366261124611, acc: 0.9796437621116638)
[2025-02-13 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:57][root][INFO] - Training Epoch: 2/2, step 4768/7134 completed (loss: 0.05259491503238678, acc: 0.9823848009109497)
[2025-02-13 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:57][root][INFO] - Training Epoch: 2/2, step 4769/7134 completed (loss: 0.039886415004730225, acc: 0.9903692007064819)
[2025-02-13 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:58][root][INFO] - Training Epoch: 2/2, step 4770/7134 completed (loss: 0.026189077645540237, acc: 0.991919219493866)
[2025-02-13 04:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:58][root][INFO] - Training Epoch: 2/2, step 4771/7134 completed (loss: 0.06254716217517853, acc: 0.9869186282157898)
[2025-02-13 04:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59][root][INFO] - Training Epoch: 2/2, step 4772/7134 completed (loss: 0.04116057604551315, acc: 0.9920254945755005)
[2025-02-13 04:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59][root][INFO] - Training Epoch: 2/2, step 4773/7134 completed (loss: 0.04365599527955055, acc: 0.9863013625144958)
[2025-02-13 04:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59][root][INFO] - Training Epoch: 2/2, step 4774/7134 completed (loss: 0.031155524775385857, acc: 0.992977499961853)
[2025-02-13 04:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:00][root][INFO] - Training Epoch: 2/2, step 4775/7134 completed (loss: 0.0429014228284359, acc: 0.988727867603302)
[2025-02-13 04:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:00][root][INFO] - Training Epoch: 2/2, step 4776/7134 completed (loss: 0.04538111761212349, acc: 0.9871588945388794)
[2025-02-13 04:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01][root][INFO] - Training Epoch: 2/2, step 4777/7134 completed (loss: 0.014967397786676884, acc: 0.9966611266136169)
[2025-02-13 04:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01][root][INFO] - Training Epoch: 2/2, step 4778/7134 completed (loss: 0.029096834361553192, acc: 0.9934354424476624)
[2025-02-13 04:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01][root][INFO] - Training Epoch: 2/2, step 4779/7134 completed (loss: 0.017818879336118698, acc: 0.9963167309761047)
[2025-02-13 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:02][root][INFO] - Training Epoch: 2/2, step 4780/7134 completed (loss: 0.004697935190051794, acc: 0.9984423518180847)
[2025-02-13 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:02][root][INFO] - Training Epoch: 2/2, step 4781/7134 completed (loss: 0.018084552139043808, acc: 0.9921721816062927)
[2025-02-13 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:03][root][INFO] - Training Epoch: 2/2, step 4782/7134 completed (loss: 0.010740368627011776, acc: 0.9951377511024475)
[2025-02-13 04:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:03][root][INFO] - Training Epoch: 2/2, step 4783/7134 completed (loss: 0.05457185581326485, acc: 0.9875444769859314)
[2025-02-13 04:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04][root][INFO] - Training Epoch: 2/2, step 4784/7134 completed (loss: 0.02130304090678692, acc: 0.9945429563522339)
[2025-02-13 04:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04][root][INFO] - Training Epoch: 2/2, step 4785/7134 completed (loss: 0.04363613575696945, acc: 0.9906367063522339)
[2025-02-13 04:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04][root][INFO] - Training Epoch: 2/2, step 4786/7134 completed (loss: 0.036916885524988174, acc: 0.9922360181808472)
[2025-02-13 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:05][root][INFO] - Training Epoch: 2/2, step 4787/7134 completed (loss: 0.00664999894797802, acc: 0.9984177350997925)
[2025-02-13 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:05][root][INFO] - Training Epoch: 2/2, step 4788/7134 completed (loss: 0.01009219791740179, acc: 0.9964538812637329)
[2025-02-13 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06][root][INFO] - Training Epoch: 2/2, step 4789/7134 completed (loss: 0.023519782349467278, acc: 0.993630588054657)
[2025-02-13 04:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06][root][INFO] - Training Epoch: 2/2, step 4790/7134 completed (loss: 0.014725526794791222, acc: 0.9945651888847351)
[2025-02-13 04:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06][root][INFO] - Training Epoch: 2/2, step 4791/7134 completed (loss: 0.012868612073361874, acc: 0.9962756037712097)
[2025-02-13 04:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:07][root][INFO] - Training Epoch: 2/2, step 4792/7134 completed (loss: 0.0063133640214800835, acc: 0.9983277320861816)
[2025-02-13 04:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:07][root][INFO] - Training Epoch: 2/2, step 4793/7134 completed (loss: 0.06219841539859772, acc: 0.983561635017395)
[2025-02-13 04:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08][root][INFO] - Training Epoch: 2/2, step 4794/7134 completed (loss: 0.05023429915308952, acc: 0.9828571677207947)
[2025-02-13 04:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08][root][INFO] - Training Epoch: 2/2, step 4795/7134 completed (loss: 0.018284952268004417, acc: 0.991525411605835)
[2025-02-13 04:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08][root][INFO] - Training Epoch: 2/2, step 4796/7134 completed (loss: 0.020932769402861595, acc: 0.9905660152435303)
[2025-02-13 04:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:09][root][INFO] - Training Epoch: 2/2, step 4797/7134 completed (loss: 0.018512384966015816, acc: 0.9937888383865356)
[2025-02-13 04:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:09][root][INFO] - Training Epoch: 2/2, step 4798/7134 completed (loss: 0.01560299564152956, acc: 0.9920106530189514)
[2025-02-13 04:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:10][root][INFO] - Training Epoch: 2/2, step 4799/7134 completed (loss: 0.020492812618613243, acc: 0.9929278492927551)
[2025-02-13 04:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:10][root][INFO] - Training Epoch: 2/2, step 4800/7134 completed (loss: 0.01826641708612442, acc: 0.9940387606620789)
[2025-02-13 04:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11][root][INFO] - Training Epoch: 2/2, step 4801/7134 completed (loss: 0.014753120951354504, acc: 0.9977375268936157)
[2025-02-13 04:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11][root][INFO] - Training Epoch: 2/2, step 4802/7134 completed (loss: 0.0360492505133152, acc: 0.9869109988212585)
[2025-02-13 04:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11][root][INFO] - Training Epoch: 2/2, step 4803/7134 completed (loss: 0.04676487296819687, acc: 0.9867647290229797)
[2025-02-13 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:12][root][INFO] - Training Epoch: 2/2, step 4804/7134 completed (loss: 0.03611854836344719, acc: 0.9846153855323792)
[2025-02-13 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:12][root][INFO] - Training Epoch: 2/2, step 4805/7134 completed (loss: 0.03512856736779213, acc: 0.991909384727478)
[2025-02-13 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:13][root][INFO] - Training Epoch: 2/2, step 4806/7134 completed (loss: 0.02880493737757206, acc: 0.9933444261550903)
[2025-02-13 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:13][root][INFO] - Training Epoch: 2/2, step 4807/7134 completed (loss: 0.03852693364024162, acc: 0.9939209818840027)
[2025-02-13 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14][root][INFO] - Training Epoch: 2/2, step 4808/7134 completed (loss: 0.03191598504781723, acc: 0.9946808218955994)
[2025-02-13 04:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14][root][INFO] - Training Epoch: 2/2, step 4809/7134 completed (loss: 0.019838489592075348, acc: 0.9968253970146179)
[2025-02-13 04:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14][root][INFO] - Training Epoch: 2/2, step 4810/7134 completed (loss: 0.03847439959645271, acc: 0.995720386505127)
[2025-02-13 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:15][root][INFO] - Training Epoch: 2/2, step 4811/7134 completed (loss: 0.04333607107400894, acc: 0.9903100728988647)
[2025-02-13 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:15][root][INFO] - Training Epoch: 2/2, step 4812/7134 completed (loss: 0.0988043025135994, acc: 0.9819193482398987)
[2025-02-13 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16][root][INFO] - Training Epoch: 2/2, step 4813/7134 completed (loss: 0.017048684880137444, acc: 0.9948052167892456)
[2025-02-13 04:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16][root][INFO] - Training Epoch: 2/2, step 4814/7134 completed (loss: 0.025335827842354774, acc: 0.9948979616165161)
[2025-02-13 04:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16][root][INFO] - Training Epoch: 2/2, step 4815/7134 completed (loss: 0.0021069967187941074, acc: 1.0)
[2025-02-13 04:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:17][root][INFO] - Training Epoch: 2/2, step 4816/7134 completed (loss: 0.004468670580536127, acc: 0.9975247383117676)
[2025-02-13 04:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:17][root][INFO] - Training Epoch: 2/2, step 4817/7134 completed (loss: 0.008735175244510174, acc: 0.9975903630256653)
[2025-02-13 04:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18][root][INFO] - Training Epoch: 2/2, step 4818/7134 completed (loss: 0.02148216962814331, acc: 0.9969230890274048)
[2025-02-13 04:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18][root][INFO] - Training Epoch: 2/2, step 4819/7134 completed (loss: 0.020837442949414253, acc: 0.9959431886672974)
[2025-02-13 04:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18][root][INFO] - Training Epoch: 2/2, step 4820/7134 completed (loss: 0.016504380851984024, acc: 0.9974259734153748)
[2025-02-13 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:19][root][INFO] - Training Epoch: 2/2, step 4821/7134 completed (loss: 0.021771235391497612, acc: 0.99301677942276)
[2025-02-13 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:19][root][INFO] - Training Epoch: 2/2, step 4822/7134 completed (loss: 0.02401762828230858, acc: 0.996219277381897)
[2025-02-13 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20][root][INFO] - Training Epoch: 2/2, step 4823/7134 completed (loss: 0.05691705644130707, acc: 0.9873617887496948)
[2025-02-13 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20][root][INFO] - Training Epoch: 2/2, step 4824/7134 completed (loss: 0.031610071659088135, acc: 0.9885844588279724)
[2025-02-13 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20][root][INFO] - Training Epoch: 2/2, step 4825/7134 completed (loss: 0.00945256557315588, acc: 0.9963503479957581)
[2025-02-13 04:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:21][root][INFO] - Training Epoch: 2/2, step 4826/7134 completed (loss: 0.02462843246757984, acc: 0.991416335105896)
[2025-02-13 04:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:21][root][INFO] - Training Epoch: 2/2, step 4827/7134 completed (loss: 0.023466059938073158, acc: 0.9948096871376038)
[2025-02-13 04:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22][root][INFO] - Training Epoch: 2/2, step 4828/7134 completed (loss: 0.009253737516701221, acc: 0.9987849593162537)
[2025-02-13 04:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22][root][INFO] - Training Epoch: 2/2, step 4829/7134 completed (loss: 0.008574220351874828, acc: 0.9961538314819336)
[2025-02-13 04:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23][root][INFO] - Training Epoch: 2/2, step 4830/7134 completed (loss: 0.011468357406556606, acc: 0.9955621361732483)
[2025-02-13 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23][root][INFO] - Training Epoch: 2/2, step 4831/7134 completed (loss: 0.007831508293747902, acc: 0.9970760345458984)
[2025-02-13 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23][root][INFO] - Training Epoch: 2/2, step 4832/7134 completed (loss: 0.026077590882778168, acc: 0.9934853315353394)
[2025-02-13 04:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:24][root][INFO] - Training Epoch: 2/2, step 4833/7134 completed (loss: 0.008937553502619267, acc: 0.9974586963653564)
[2025-02-13 04:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:24][root][INFO] - Training Epoch: 2/2, step 4834/7134 completed (loss: 0.023511668667197227, acc: 0.9940476417541504)
[2025-02-13 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:25][root][INFO] - Training Epoch: 2/2, step 4835/7134 completed (loss: 0.013756346888840199, acc: 0.9945945739746094)
[2025-02-13 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:25][root][INFO] - Training Epoch: 2/2, step 4836/7134 completed (loss: 0.00955286342650652, acc: 0.9969465732574463)
[2025-02-13 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26][root][INFO] - Training Epoch: 2/2, step 4837/7134 completed (loss: 0.05143490433692932, acc: 0.9889705777168274)
[2025-02-13 04:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26][root][INFO] - Training Epoch: 2/2, step 4838/7134 completed (loss: 0.01654018834233284, acc: 0.9967793822288513)
[2025-02-13 04:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26][root][INFO] - Training Epoch: 2/2, step 4839/7134 completed (loss: 0.04511215537786484, acc: 0.9913941621780396)
[2025-02-13 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:27][root][INFO] - Training Epoch: 2/2, step 4840/7134 completed (loss: 0.03051498532295227, acc: 0.9913294911384583)
[2025-02-13 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:27][root][INFO] - Training Epoch: 2/2, step 4841/7134 completed (loss: 0.0303829126060009, acc: 0.992548406124115)
[2025-02-13 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:28][root][INFO] - Training Epoch: 2/2, step 4842/7134 completed (loss: 0.04512565955519676, acc: 0.9885203838348389)
[2025-02-13 04:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:28][root][INFO] - Training Epoch: 2/2, step 4843/7134 completed (loss: 0.021177692338824272, acc: 0.9940564632415771)
[2025-02-13 04:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29][root][INFO] - Training Epoch: 2/2, step 4844/7134 completed (loss: 0.02138804830610752, acc: 0.993537962436676)
[2025-02-13 04:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29][root][INFO] - Training Epoch: 2/2, step 4845/7134 completed (loss: 0.024876952171325684, acc: 0.9931694269180298)
[2025-02-13 04:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29][root][INFO] - Training Epoch: 2/2, step 4846/7134 completed (loss: 0.02347615733742714, acc: 0.9918919205665588)
[2025-02-13 04:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:30][root][INFO] - Training Epoch: 2/2, step 4847/7134 completed (loss: 0.02213493175804615, acc: 0.9946091771125793)
[2025-02-13 04:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:30][root][INFO] - Training Epoch: 2/2, step 4848/7134 completed (loss: 0.023414522409439087, acc: 0.9972375631332397)
[2025-02-13 04:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31][root][INFO] - Training Epoch: 2/2, step 4849/7134 completed (loss: 0.019205357879400253, acc: 0.9955423474311829)
[2025-02-13 04:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31][root][INFO] - Training Epoch: 2/2, step 4850/7134 completed (loss: 0.03452812135219574, acc: 0.9926560521125793)
[2025-02-13 04:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31][root][INFO] - Training Epoch: 2/2, step 4851/7134 completed (loss: 0.027729790657758713, acc: 0.9894859790802002)
[2025-02-13 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:32][root][INFO] - Training Epoch: 2/2, step 4852/7134 completed (loss: 0.027687616646289825, acc: 0.989347517490387)
[2025-02-13 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:32][root][INFO] - Training Epoch: 2/2, step 4853/7134 completed (loss: 0.06471128016710281, acc: 0.9885877370834351)
[2025-02-13 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:33][root][INFO] - Training Epoch: 2/2, step 4854/7134 completed (loss: 0.03084411285817623, acc: 0.9946666955947876)
[2025-02-13 04:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:33][root][INFO] - Training Epoch: 2/2, step 4855/7134 completed (loss: 0.025728339329361916, acc: 0.9927448630332947)
[2025-02-13 04:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34][root][INFO] - Training Epoch: 2/2, step 4856/7134 completed (loss: 0.03013312816619873, acc: 0.9923664331436157)
[2025-02-13 04:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34][root][INFO] - Training Epoch: 2/2, step 4857/7134 completed (loss: 0.009505797177553177, acc: 0.9971222877502441)
[2025-02-13 04:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34][root][INFO] - Training Epoch: 2/2, step 4858/7134 completed (loss: 0.02177443355321884, acc: 0.994301974773407)
[2025-02-13 04:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:35][root][INFO] - Training Epoch: 2/2, step 4859/7134 completed (loss: 0.015918347984552383, acc: 0.9948520064353943)
[2025-02-13 04:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:35][root][INFO] - Training Epoch: 2/2, step 4860/7134 completed (loss: 0.011402699165046215, acc: 0.9972752332687378)
[2025-02-13 04:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36][root][INFO] - Training Epoch: 2/2, step 4861/7134 completed (loss: 0.014539184048771858, acc: 0.9946091771125793)
[2025-02-13 04:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36][root][INFO] - Training Epoch: 2/2, step 4862/7134 completed (loss: 0.02364458702504635, acc: 0.995708167552948)
[2025-02-13 04:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36][root][INFO] - Training Epoch: 2/2, step 4863/7134 completed (loss: 0.02129662036895752, acc: 0.9963503479957581)
[2025-02-13 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:37][root][INFO] - Training Epoch: 2/2, step 4864/7134 completed (loss: 0.01794668287038803, acc: 0.9923076629638672)
[2025-02-13 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:37][root][INFO] - Training Epoch: 2/2, step 4865/7134 completed (loss: 0.028634510934352875, acc: 0.9938949942588806)
[2025-02-13 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:38][root][INFO] - Training Epoch: 2/2, step 4866/7134 completed (loss: 0.029847638681530952, acc: 0.993514895439148)
[2025-02-13 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:38][root][INFO] - Training Epoch: 2/2, step 4867/7134 completed (loss: 0.04292002320289612, acc: 0.9894179701805115)
[2025-02-13 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39][root][INFO] - Training Epoch: 2/2, step 4868/7134 completed (loss: 0.04480762779712677, acc: 0.9903100728988647)
[2025-02-13 04:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39][root][INFO] - Training Epoch: 2/2, step 4869/7134 completed (loss: 0.04001249000430107, acc: 0.9911110997200012)
[2025-02-13 04:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39][root][INFO] - Training Epoch: 2/2, step 4870/7134 completed (loss: 0.02139933593571186, acc: 0.9953051805496216)
[2025-02-13 04:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:40][root][INFO] - Training Epoch: 2/2, step 4871/7134 completed (loss: 0.03389957919716835, acc: 0.9938271641731262)
[2025-02-13 04:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:40][root][INFO] - Training Epoch: 2/2, step 4872/7134 completed (loss: 0.012169650755822659, acc: 0.9958275556564331)
[2025-02-13 04:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:41][root][INFO] - Training Epoch: 2/2, step 4873/7134 completed (loss: 0.021964237093925476, acc: 0.9908854365348816)
[2025-02-13 04:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:41][root][INFO] - Training Epoch: 2/2, step 4874/7134 completed (loss: 0.017916373908519745, acc: 0.99314284324646)
[2025-02-13 04:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42][root][INFO] - Training Epoch: 2/2, step 4875/7134 completed (loss: 0.022985728457570076, acc: 0.9916782379150391)
[2025-02-13 04:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42][root][INFO] - Training Epoch: 2/2, step 4876/7134 completed (loss: 0.054236967116594315, acc: 0.985200822353363)
[2025-02-13 04:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42][root][INFO] - Training Epoch: 2/2, step 4877/7134 completed (loss: 0.03450220450758934, acc: 0.9870967864990234)
[2025-02-13 04:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:43][root][INFO] - Training Epoch: 2/2, step 4878/7134 completed (loss: 0.02482076920568943, acc: 0.9896907210350037)
[2025-02-13 04:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:43][root][INFO] - Training Epoch: 2/2, step 4879/7134 completed (loss: 0.051923006772994995, acc: 0.9849397540092468)
[2025-02-13 04:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44][root][INFO] - Training Epoch: 2/2, step 4880/7134 completed (loss: 0.008538725785911083, acc: 0.9987030029296875)
[2025-02-13 04:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44][root][INFO] - Training Epoch: 2/2, step 4881/7134 completed (loss: 0.039471566677093506, acc: 0.9868612885475159)
[2025-02-13 04:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44][root][INFO] - Training Epoch: 2/2, step 4882/7134 completed (loss: 0.03587585687637329, acc: 0.9819079041481018)
[2025-02-13 04:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:45][root][INFO] - Training Epoch: 2/2, step 4883/7134 completed (loss: 0.02692871168255806, acc: 0.9868612885475159)
[2025-02-13 04:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:45][root][INFO] - Training Epoch: 2/2, step 4884/7134 completed (loss: 0.009089646860957146, acc: 0.9953917264938354)
[2025-02-13 04:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:46][root][INFO] - Training Epoch: 2/2, step 4885/7134 completed (loss: 0.010174735449254513, acc: 0.9973614811897278)
[2025-02-13 04:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:46][root][INFO] - Training Epoch: 2/2, step 4886/7134 completed (loss: 0.025998873636126518, acc: 0.9923076629638672)
[2025-02-13 04:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47][root][INFO] - Training Epoch: 2/2, step 4887/7134 completed (loss: 0.00653200875967741, acc: 0.9988052845001221)
[2025-02-13 04:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47][root][INFO] - Training Epoch: 2/2, step 4888/7134 completed (loss: 0.03948470950126648, acc: 0.9931318759918213)
[2025-02-13 04:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47][root][INFO] - Training Epoch: 2/2, step 4889/7134 completed (loss: 0.005893524736166, acc: 0.9987966418266296)
[2025-02-13 04:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48][root][INFO] - Training Epoch: 2/2, step 4890/7134 completed (loss: 0.03425551950931549, acc: 0.9952380657196045)
[2025-02-13 04:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48][root][INFO] - Training Epoch: 2/2, step 4891/7134 completed (loss: 0.04801960289478302, acc: 0.9863184094429016)
[2025-02-13 04:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:49][root][INFO] - Training Epoch: 2/2, step 4892/7134 completed (loss: 0.03431379422545433, acc: 0.9916107654571533)
[2025-02-13 04:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:49][root][INFO] - Training Epoch: 2/2, step 4893/7134 completed (loss: 0.12821312248706818, acc: 0.9630606770515442)
[2025-02-13 04:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50][root][INFO] - Training Epoch: 2/2, step 4894/7134 completed (loss: 0.00946764089167118, acc: 0.9985590577125549)
[2025-02-13 04:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50][root][INFO] - Training Epoch: 2/2, step 4895/7134 completed (loss: 0.008895042352378368, acc: 0.9987499713897705)
[2025-02-13 04:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50][root][INFO] - Training Epoch: 2/2, step 4896/7134 completed (loss: 0.009676122106611729, acc: 0.9985954761505127)
[2025-02-13 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:51][root][INFO] - Training Epoch: 2/2, step 4897/7134 completed (loss: 0.0385143980383873, acc: 0.9894737005233765)
[2025-02-13 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:51][root][INFO] - Training Epoch: 2/2, step 4898/7134 completed (loss: 0.016847187653183937, acc: 0.99622642993927)
[2025-02-13 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:52][root][INFO] - Training Epoch: 2/2, step 4899/7134 completed (loss: 0.020004263147711754, acc: 0.9941775798797607)
[2025-02-13 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:52][root][INFO] - Training Epoch: 2/2, step 4900/7134 completed (loss: 0.023223653435707092, acc: 0.9947368502616882)
[2025-02-13 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53][root][INFO] - Training Epoch: 2/2, step 4901/7134 completed (loss: 0.0178709514439106, acc: 0.9939849376678467)
[2025-02-13 04:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53][root][INFO] - Training Epoch: 2/2, step 4902/7134 completed (loss: 0.01299224328249693, acc: 0.9960212111473083)
[2025-02-13 04:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53][root][INFO] - Training Epoch: 2/2, step 4903/7134 completed (loss: 0.005921453237533569, acc: 0.9982078671455383)
[2025-02-13 04:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54][root][INFO] - Training Epoch: 2/2, step 4904/7134 completed (loss: 0.015047091990709305, acc: 0.9940298795700073)
[2025-02-13 04:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54][root][INFO] - Training Epoch: 2/2, step 4905/7134 completed (loss: 0.0030624172650277615, acc: 1.0)
[2025-02-13 04:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:55][root][INFO] - Training Epoch: 2/2, step 4906/7134 completed (loss: 0.010838665068149567, acc: 0.9983136653900146)
[2025-02-13 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:55][root][INFO] - Training Epoch: 2/2, step 4907/7134 completed (loss: 0.01237462554126978, acc: 0.996835470199585)
[2025-02-13 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56][root][INFO] - Training Epoch: 2/2, step 4908/7134 completed (loss: 0.005348076578229666, acc: 0.9983999729156494)
[2025-02-13 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56][root][INFO] - Training Epoch: 2/2, step 4909/7134 completed (loss: 0.021710751578211784, acc: 0.9967585206031799)
[2025-02-13 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56][root][INFO] - Training Epoch: 2/2, step 4910/7134 completed (loss: 0.019872797653079033, acc: 0.9921630024909973)
[2025-02-13 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:57][root][INFO] - Training Epoch: 2/2, step 4911/7134 completed (loss: 0.02186751551926136, acc: 0.9934036731719971)
[2025-02-13 04:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:57][root][INFO] - Training Epoch: 2/2, step 4912/7134 completed (loss: 0.02331022173166275, acc: 0.9927361011505127)
[2025-02-13 04:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58][root][INFO] - Training Epoch: 2/2, step 4913/7134 completed (loss: 0.020232507959008217, acc: 0.9932432174682617)
[2025-02-13 04:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58][root][INFO] - Training Epoch: 2/2, step 4914/7134 completed (loss: 0.021955816075205803, acc: 0.9954128265380859)
[2025-02-13 04:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58][root][INFO] - Training Epoch: 2/2, step 4915/7134 completed (loss: 0.0030519769061356783, acc: 1.0)
[2025-02-13 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:59][root][INFO] - Training Epoch: 2/2, step 4916/7134 completed (loss: 0.039735570549964905, acc: 0.987864077091217)
[2025-02-13 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:59][root][INFO] - Training Epoch: 2/2, step 4917/7134 completed (loss: 0.04438852146267891, acc: 0.9888734221458435)
[2025-02-13 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:00][root][INFO] - Training Epoch: 2/2, step 4918/7134 completed (loss: 0.0037758781109005213, acc: 0.998633861541748)
[2025-02-13 04:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:00][root][INFO] - Training Epoch: 2/2, step 4919/7134 completed (loss: 0.03450834006071091, acc: 0.99210524559021)
[2025-02-13 04:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01][root][INFO] - Training Epoch: 2/2, step 4920/7134 completed (loss: 0.05930037423968315, acc: 0.9909560680389404)
[2025-02-13 04:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01][root][INFO] - Training Epoch: 2/2, step 4921/7134 completed (loss: 0.028967445716261864, acc: 0.9956958293914795)
[2025-02-13 04:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01][root][INFO] - Training Epoch: 2/2, step 4922/7134 completed (loss: 0.020670277997851372, acc: 0.994557797908783)
[2025-02-13 04:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:02][root][INFO] - Training Epoch: 2/2, step 4923/7134 completed (loss: 0.01192568615078926, acc: 0.9955947399139404)
[2025-02-13 04:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:02][root][INFO] - Training Epoch: 2/2, step 4924/7134 completed (loss: 0.01757080666720867, acc: 0.9980000257492065)
[2025-02-13 04:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03][root][INFO] - Training Epoch: 2/2, step 4925/7134 completed (loss: 0.02310912311077118, acc: 0.9885550737380981)
[2025-02-13 04:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03][root][INFO] - Training Epoch: 2/2, step 4926/7134 completed (loss: 0.01308300532400608, acc: 0.9942611455917358)
[2025-02-13 04:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03][root][INFO] - Training Epoch: 2/2, step 4927/7134 completed (loss: 0.02482340857386589, acc: 0.9901960492134094)
[2025-02-13 04:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:04][root][INFO] - Training Epoch: 2/2, step 4928/7134 completed (loss: 0.009972997941076756, acc: 0.997682511806488)
[2025-02-13 04:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:04][root][INFO] - Training Epoch: 2/2, step 4929/7134 completed (loss: 0.02046164683997631, acc: 0.9937984347343445)
[2025-02-13 04:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05][root][INFO] - Training Epoch: 2/2, step 4930/7134 completed (loss: 0.03204313665628433, acc: 0.9954751133918762)
[2025-02-13 04:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05][root][INFO] - Training Epoch: 2/2, step 4931/7134 completed (loss: 0.08220674097537994, acc: 0.9878234267234802)
[2025-02-13 04:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06][root][INFO] - Training Epoch: 2/2, step 4932/7134 completed (loss: 0.01612073741853237, acc: 0.9932523369789124)
[2025-02-13 04:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06][root][INFO] - Training Epoch: 2/2, step 4933/7134 completed (loss: 0.02841905876994133, acc: 0.993819534778595)
[2025-02-13 04:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06][root][INFO] - Training Epoch: 2/2, step 4934/7134 completed (loss: 0.005899668671190739, acc: 0.9972936511039734)
[2025-02-13 04:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:07][root][INFO] - Training Epoch: 2/2, step 4935/7134 completed (loss: 0.011009089648723602, acc: 0.99726402759552)
[2025-02-13 04:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:07][root][INFO] - Training Epoch: 2/2, step 4936/7134 completed (loss: 0.02115473710000515, acc: 0.9907407164573669)
[2025-02-13 04:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08][root][INFO] - Training Epoch: 2/2, step 4937/7134 completed (loss: 0.01733056828379631, acc: 0.9925261735916138)
[2025-02-13 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08][root][INFO] - Training Epoch: 2/2, step 4938/7134 completed (loss: 0.01973380334675312, acc: 0.9919028282165527)
[2025-02-13 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08][root][INFO] - Training Epoch: 2/2, step 4939/7134 completed (loss: 0.019761603325605392, acc: 0.9885807633399963)
[2025-02-13 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:09][root][INFO] - Training Epoch: 2/2, step 4940/7134 completed (loss: 0.03001265414059162, acc: 0.9954545497894287)
[2025-02-13 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:09][root][INFO] - Training Epoch: 2/2, step 4941/7134 completed (loss: 0.02247096225619316, acc: 0.9946164488792419)
[2025-02-13 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:10][root][INFO] - Training Epoch: 2/2, step 4942/7134 completed (loss: 0.03087385930120945, acc: 0.9906166195869446)
[2025-02-13 04:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:10][root][INFO] - Training Epoch: 2/2, step 4943/7134 completed (loss: 0.0350114107131958, acc: 0.9890561103820801)
[2025-02-13 04:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11][root][INFO] - Training Epoch: 2/2, step 4944/7134 completed (loss: 0.027426637709140778, acc: 0.9894737005233765)
[2025-02-13 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11][root][INFO] - Training Epoch: 2/2, step 4945/7134 completed (loss: 0.02279609441757202, acc: 0.9919354915618896)
[2025-02-13 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11][root][INFO] - Training Epoch: 2/2, step 4946/7134 completed (loss: 0.05389149487018585, acc: 0.9802197813987732)
[2025-02-13 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12][root][INFO] - Training Epoch: 2/2, step 4947/7134 completed (loss: 0.022169899195432663, acc: 0.9933481216430664)
[2025-02-13 04:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12][root][INFO] - Training Epoch: 2/2, step 4948/7134 completed (loss: 0.026459714397788048, acc: 0.9946523904800415)
[2025-02-13 04:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12][root][INFO] - Training Epoch: 2/2, step 4949/7134 completed (loss: 0.02074052207171917, acc: 0.9968253970146179)
[2025-02-13 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:13][root][INFO] - Training Epoch: 2/2, step 4950/7134 completed (loss: 0.011233964934945107, acc: 0.9982486963272095)
[2025-02-13 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:13][root][INFO] - Training Epoch: 2/2, step 4951/7134 completed (loss: 0.015037285163998604, acc: 0.9971510171890259)
[2025-02-13 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14][root][INFO] - Training Epoch: 2/2, step 4952/7134 completed (loss: 0.02097718045115471, acc: 0.9950330853462219)
[2025-02-13 04:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14][root][INFO] - Training Epoch: 2/2, step 4953/7134 completed (loss: 0.009162422269582748, acc: 0.9956427216529846)
[2025-02-13 04:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14][root][INFO] - Training Epoch: 2/2, step 4954/7134 completed (loss: 0.009754401631653309, acc: 0.9942857027053833)
[2025-02-13 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:15][root][INFO] - Training Epoch: 2/2, step 4955/7134 completed (loss: 0.019546430557966232, acc: 0.9953161478042603)
[2025-02-13 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:15][root][INFO] - Training Epoch: 2/2, step 4956/7134 completed (loss: 0.013733035884797573, acc: 0.9957325458526611)
[2025-02-13 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16][root][INFO] - Training Epoch: 2/2, step 4957/7134 completed (loss: 0.02260916493833065, acc: 0.9894179701805115)
[2025-02-13 04:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16][root][INFO] - Training Epoch: 2/2, step 4958/7134 completed (loss: 0.00927529763430357, acc: 0.9956834316253662)
[2025-02-13 04:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16][root][INFO] - Training Epoch: 2/2, step 4959/7134 completed (loss: 0.006622715387493372, acc: 0.9970588088035583)
[2025-02-13 04:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17][root][INFO] - Training Epoch: 2/2, step 4960/7134 completed (loss: 0.0091167027130723, acc: 0.998275876045227)
[2025-02-13 04:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17][root][INFO] - Training Epoch: 2/2, step 4961/7134 completed (loss: 0.008763223886489868, acc: 0.9963570237159729)
[2025-02-13 04:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18][root][INFO] - Training Epoch: 2/2, step 4962/7134 completed (loss: 0.003922197036445141, acc: 1.0)
[2025-02-13 04:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18][root][INFO] - Training Epoch: 2/2, step 4963/7134 completed (loss: 0.021835029125213623, acc: 0.9918830990791321)
[2025-02-13 04:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18][root][INFO] - Training Epoch: 2/2, step 4964/7134 completed (loss: 0.015251166187226772, acc: 0.995121955871582)
[2025-02-13 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:19][root][INFO] - Training Epoch: 2/2, step 4965/7134 completed (loss: 0.031887732446193695, acc: 0.9915966391563416)
[2025-02-13 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:19][root][INFO] - Training Epoch: 2/2, step 4966/7134 completed (loss: 0.052794020622968674, acc: 0.9861496090888977)
[2025-02-13 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20][root][INFO] - Training Epoch: 2/2, step 4967/7134 completed (loss: 0.030912119895219803, acc: 0.990234375)
[2025-02-13 04:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20][root][INFO] - Training Epoch: 2/2, step 4968/7134 completed (loss: 0.06273500621318817, acc: 0.9768683314323425)
[2025-02-13 04:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20][root][INFO] - Training Epoch: 2/2, step 4969/7134 completed (loss: 0.05547342076897621, acc: 0.9845361113548279)
[2025-02-13 04:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:21][root][INFO] - Training Epoch: 2/2, step 4970/7134 completed (loss: 0.016010163351893425, acc: 0.9899665713310242)
[2025-02-13 04:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:21][root][INFO] - Training Epoch: 2/2, step 4971/7134 completed (loss: 0.016952715814113617, acc: 0.993220329284668)
[2025-02-13 04:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22][root][INFO] - Training Epoch: 2/2, step 4972/7134 completed (loss: 0.010108143091201782, acc: 0.9971791505813599)
[2025-02-13 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22][root][INFO] - Training Epoch: 2/2, step 4973/7134 completed (loss: 0.03839350491762161, acc: 0.991769552230835)
[2025-02-13 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22][root][INFO] - Training Epoch: 2/2, step 4974/7134 completed (loss: 0.034263238310813904, acc: 0.9879759550094604)
[2025-02-13 04:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:23][root][INFO] - Training Epoch: 2/2, step 4975/7134 completed (loss: 0.04969125986099243, acc: 0.9872204661369324)
[2025-02-13 04:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:23][root][INFO] - Training Epoch: 2/2, step 4976/7134 completed (loss: 0.022587604820728302, acc: 0.9906322956085205)
[2025-02-13 04:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24][root][INFO] - Training Epoch: 2/2, step 4977/7134 completed (loss: 0.06690734624862671, acc: 0.9784946441650391)
[2025-02-13 04:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24][root][INFO] - Training Epoch: 2/2, step 4978/7134 completed (loss: 0.015236393548548222, acc: 0.9950310587882996)
[2025-02-13 04:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24][root][INFO] - Training Epoch: 2/2, step 4979/7134 completed (loss: 0.09204196184873581, acc: 0.9802306294441223)
[2025-02-13 04:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:25][root][INFO] - Training Epoch: 2/2, step 4980/7134 completed (loss: 0.05104217305779457, acc: 0.988727867603302)
[2025-02-13 04:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:25][root][INFO] - Training Epoch: 2/2, step 4981/7134 completed (loss: 0.031682997941970825, acc: 0.9872408509254456)
[2025-02-13 04:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26][root][INFO] - Training Epoch: 2/2, step 4982/7134 completed (loss: 0.05062789097428322, acc: 0.9858299493789673)
[2025-02-13 04:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26][root][INFO] - Training Epoch: 2/2, step 4983/7134 completed (loss: 0.04878303036093712, acc: 0.9875389337539673)
[2025-02-13 04:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26][root][INFO] - Training Epoch: 2/2, step 4984/7134 completed (loss: 0.041647784411907196, acc: 0.987500011920929)
[2025-02-13 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:27][root][INFO] - Training Epoch: 2/2, step 4985/7134 completed (loss: 0.023212719708681107, acc: 0.9938837885856628)
[2025-02-13 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:27][root][INFO] - Training Epoch: 2/2, step 4986/7134 completed (loss: 0.049046095460653305, acc: 0.9851411581039429)
[2025-02-13 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28][root][INFO] - Training Epoch: 2/2, step 4987/7134 completed (loss: 0.026966990903019905, acc: 0.9920477271080017)
[2025-02-13 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28][root][INFO] - Training Epoch: 2/2, step 4988/7134 completed (loss: 0.013816174119710922, acc: 0.9964664578437805)
[2025-02-13 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28][root][INFO] - Training Epoch: 2/2, step 4989/7134 completed (loss: 0.019839566200971603, acc: 0.9925742745399475)
[2025-02-13 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29][root][INFO] - Training Epoch: 2/2, step 4990/7134 completed (loss: 0.032557740807533264, acc: 0.9932432174682617)
[2025-02-13 04:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29][root][INFO] - Training Epoch: 2/2, step 4991/7134 completed (loss: 0.015222902409732342, acc: 0.9932998418807983)
[2025-02-13 04:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30][root][INFO] - Training Epoch: 2/2, step 4992/7134 completed (loss: 0.1056666225194931, acc: 0.9737417697906494)
[2025-02-13 04:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30][root][INFO] - Training Epoch: 2/2, step 4993/7134 completed (loss: 0.007660834118723869, acc: 1.0)
[2025-02-13 04:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30][root][INFO] - Training Epoch: 2/2, step 4994/7134 completed (loss: 0.027021927759051323, acc: 0.9957447052001953)
[2025-02-13 04:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31][root][INFO] - Training Epoch: 2/2, step 4995/7134 completed (loss: 0.035816870629787445, acc: 0.9877551198005676)
[2025-02-13 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31][root][INFO] - Training Epoch: 2/2, step 4996/7134 completed (loss: 0.0185112152248621, acc: 0.9926199316978455)
[2025-02-13 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31][root][INFO] - Training Epoch: 2/2, step 4997/7134 completed (loss: 0.030518466606736183, acc: 0.9948630332946777)
[2025-02-13 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32][root][INFO] - Training Epoch: 2/2, step 4998/7134 completed (loss: 0.08207781612873077, acc: 0.9836065769195557)
[2025-02-13 04:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32][root][INFO] - Training Epoch: 2/2, step 4999/7134 completed (loss: 0.029231112450361252, acc: 0.9931153059005737)
[2025-02-13 04:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32][root][INFO] - Training Epoch: 2/2, step 5000/7134 completed (loss: 0.021751992404460907, acc: 0.9930232763290405)
[2025-02-13 04:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:33][root][INFO] - Training Epoch: 2/2, step 5001/7134 completed (loss: 0.043104443699121475, acc: 0.9898256063461304)
[2025-02-13 04:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:33][root][INFO] - Training Epoch: 2/2, step 5002/7134 completed (loss: 0.035023946315050125, acc: 0.9962825179100037)
[2025-02-13 04:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34][root][INFO] - Training Epoch: 2/2, step 5003/7134 completed (loss: 0.021344570443034172, acc: 0.9936000108718872)
[2025-02-13 04:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34][root][INFO] - Training Epoch: 2/2, step 5004/7134 completed (loss: 0.017746375873684883, acc: 0.9933775067329407)
[2025-02-13 04:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34][root][INFO] - Training Epoch: 2/2, step 5005/7134 completed (loss: 0.030516665428876877, acc: 0.992682933807373)
[2025-02-13 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:35][root][INFO] - Training Epoch: 2/2, step 5006/7134 completed (loss: 0.02522295154631138, acc: 0.9941262602806091)
[2025-02-13 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:35][root][INFO] - Training Epoch: 2/2, step 5007/7134 completed (loss: 0.012445728294551373, acc: 0.9971751570701599)
[2025-02-13 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36][root][INFO] - Training Epoch: 2/2, step 5008/7134 completed (loss: 0.038591060787439346, acc: 0.9901574850082397)
[2025-02-13 04:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36][root][INFO] - Training Epoch: 2/2, step 5009/7134 completed (loss: 0.04148624464869499, acc: 0.9950371980667114)
[2025-02-13 04:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36][root][INFO] - Training Epoch: 2/2, step 5010/7134 completed (loss: 0.024934785440564156, acc: 0.9959514141082764)
[2025-02-13 04:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:37][root][INFO] - Training Epoch: 2/2, step 5011/7134 completed (loss: 0.034749869257211685, acc: 0.9869918823242188)
[2025-02-13 04:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:37][root][INFO] - Training Epoch: 2/2, step 5012/7134 completed (loss: 0.05457324534654617, acc: 0.982758641242981)
[2025-02-13 04:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38][root][INFO] - Training Epoch: 2/2, step 5013/7134 completed (loss: 0.006213256623595953, acc: 0.9979959726333618)
[2025-02-13 04:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38][root][INFO] - Training Epoch: 2/2, step 5014/7134 completed (loss: 0.03517073765397072, acc: 0.9918032884597778)
[2025-02-13 04:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38][root][INFO] - Training Epoch: 2/2, step 5015/7134 completed (loss: 0.07858961075544357, acc: 0.9875862002372742)
[2025-02-13 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:39][root][INFO] - Training Epoch: 2/2, step 5016/7134 completed (loss: 0.022240912541747093, acc: 0.9909583926200867)
[2025-02-13 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:39][root][INFO] - Training Epoch: 2/2, step 5017/7134 completed (loss: 0.01457980740815401, acc: 0.9945945739746094)
[2025-02-13 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:40][root][INFO] - Training Epoch: 2/2, step 5018/7134 completed (loss: 0.022470060735940933, acc: 0.9946808218955994)
[2025-02-13 04:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:40][root][INFO] - Training Epoch: 2/2, step 5019/7134 completed (loss: 0.02417570725083351, acc: 0.9928774833679199)
[2025-02-13 04:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41][root][INFO] - Training Epoch: 2/2, step 5020/7134 completed (loss: 0.029316013678908348, acc: 0.9912499785423279)
[2025-02-13 04:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41][root][INFO] - Training Epoch: 2/2, step 5021/7134 completed (loss: 0.0346798300743103, acc: 0.986270010471344)
[2025-02-13 04:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41][root][INFO] - Training Epoch: 2/2, step 5022/7134 completed (loss: 0.021797966212034225, acc: 0.9969135522842407)
[2025-02-13 04:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:42][root][INFO] - Training Epoch: 2/2, step 5023/7134 completed (loss: 0.03467698395252228, acc: 0.9893617033958435)
[2025-02-13 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:42][root][INFO] - Training Epoch: 2/2, step 5024/7134 completed (loss: 0.023809069767594337, acc: 0.9961389899253845)
[2025-02-13 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43][root][INFO] - Training Epoch: 2/2, step 5025/7134 completed (loss: 0.02430623769760132, acc: 0.9927536249160767)
[2025-02-13 04:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43][root][INFO] - Training Epoch: 2/2, step 5026/7134 completed (loss: 0.03856397047638893, acc: 0.9894921183586121)
[2025-02-13 04:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43][root][INFO] - Training Epoch: 2/2, step 5027/7134 completed (loss: 0.018261581659317017, acc: 0.9927140474319458)
[2025-02-13 04:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44][root][INFO] - Training Epoch: 2/2, step 5028/7134 completed (loss: 0.013808013871312141, acc: 0.9968652129173279)
[2025-02-13 04:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44][root][INFO] - Training Epoch: 2/2, step 5029/7134 completed (loss: 0.01598133146762848, acc: 0.9924242496490479)
[2025-02-13 04:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45][root][INFO] - Training Epoch: 2/2, step 5030/7134 completed (loss: 0.014517560601234436, acc: 0.9959072470664978)
[2025-02-13 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45][root][INFO] - Training Epoch: 2/2, step 5031/7134 completed (loss: 0.07090209424495697, acc: 0.988095223903656)
[2025-02-13 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45][root][INFO] - Training Epoch: 2/2, step 5032/7134 completed (loss: 0.020459601655602455, acc: 0.9983766078948975)
[2025-02-13 04:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:46][root][INFO] - Training Epoch: 2/2, step 5033/7134 completed (loss: 0.006724723614752293, acc: 0.9983974099159241)
[2025-02-13 04:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:46][root][INFO] - Training Epoch: 2/2, step 5034/7134 completed (loss: 0.012746904045343399, acc: 0.9964538812637329)
[2025-02-13 04:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47][root][INFO] - Training Epoch: 2/2, step 5035/7134 completed (loss: 0.003116088453680277, acc: 1.0)
[2025-02-13 04:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47][root][INFO] - Training Epoch: 2/2, step 5036/7134 completed (loss: 0.04049813002347946, acc: 0.9888392686843872)
[2025-02-13 04:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47][root][INFO] - Training Epoch: 2/2, step 5037/7134 completed (loss: 0.056940708309412, acc: 0.989276111125946)
[2025-02-13 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:48][root][INFO] - Training Epoch: 2/2, step 5038/7134 completed (loss: 0.029297197237610817, acc: 0.9955456852912903)
[2025-02-13 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:48][root][INFO] - Training Epoch: 2/2, step 5039/7134 completed (loss: 0.03514361009001732, acc: 0.9905063509941101)
[2025-02-13 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49][root][INFO] - Training Epoch: 2/2, step 5040/7134 completed (loss: 0.009326943196356297, acc: 0.9961758852005005)
[2025-02-13 04:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49][root][INFO] - Training Epoch: 2/2, step 5041/7134 completed (loss: 0.008147857151925564, acc: 0.9946949481964111)
[2025-02-13 04:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49][root][INFO] - Training Epoch: 2/2, step 5042/7134 completed (loss: 0.03771783411502838, acc: 0.9906759858131409)
[2025-02-13 04:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:50][root][INFO] - Training Epoch: 2/2, step 5043/7134 completed (loss: 0.029067808762192726, acc: 0.9881423115730286)
[2025-02-13 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:50][root][INFO] - Training Epoch: 2/2, step 5044/7134 completed (loss: 0.024016935378313065, acc: 0.9912280440330505)
[2025-02-13 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51][root][INFO] - Training Epoch: 2/2, step 5045/7134 completed (loss: 0.04675870016217232, acc: 0.982503354549408)
[2025-02-13 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51][root][INFO] - Training Epoch: 2/2, step 5046/7134 completed (loss: 0.05018473044037819, acc: 0.9877192974090576)
[2025-02-13 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51][root][INFO] - Training Epoch: 2/2, step 5047/7134 completed (loss: 0.052059829235076904, acc: 0.9834710955619812)
[2025-02-13 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52][root][INFO] - Training Epoch: 2/2, step 5048/7134 completed (loss: 0.031712617725133896, acc: 0.9923896789550781)
[2025-02-13 04:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52][root][INFO] - Training Epoch: 2/2, step 5049/7134 completed (loss: 0.06672985851764679, acc: 0.9836512207984924)
[2025-02-13 04:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52][root][INFO] - Training Epoch: 2/2, step 5050/7134 completed (loss: 0.04300396516919136, acc: 0.9915611743927002)
[2025-02-13 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:53][root][INFO] - Training Epoch: 2/2, step 5051/7134 completed (loss: 0.040212616324424744, acc: 0.9898550510406494)
[2025-02-13 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:53][root][INFO] - Training Epoch: 2/2, step 5052/7134 completed (loss: 0.024452853947877884, acc: 0.9909090995788574)
[2025-02-13 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54][root][INFO] - Training Epoch: 2/2, step 5053/7134 completed (loss: 0.06968169659376144, acc: 0.9830188751220703)
[2025-02-13 04:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54][root][INFO] - Training Epoch: 2/2, step 5054/7134 completed (loss: 0.05671434476971626, acc: 0.9810725450515747)
[2025-02-13 04:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54][root][INFO] - Training Epoch: 2/2, step 5055/7134 completed (loss: 0.015917355194687843, acc: 0.9959183931350708)
[2025-02-13 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:55][root][INFO] - Training Epoch: 2/2, step 5056/7134 completed (loss: 0.05288640037178993, acc: 0.9857142567634583)
[2025-02-13 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:55][root][INFO] - Training Epoch: 2/2, step 5057/7134 completed (loss: 0.0761043131351471, acc: 0.9829303026199341)
[2025-02-13 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:56][root][INFO] - Training Epoch: 2/2, step 5058/7134 completed (loss: 0.06571974605321884, acc: 0.9840579628944397)
[2025-02-13 04:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:56][root][INFO] - Training Epoch: 2/2, step 5059/7134 completed (loss: 0.03416069597005844, acc: 0.9908257126808167)
[2025-02-13 04:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57][root][INFO] - Training Epoch: 2/2, step 5060/7134 completed (loss: 0.019723670557141304, acc: 0.9955423474311829)
[2025-02-13 04:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57][root][INFO] - Training Epoch: 2/2, step 5061/7134 completed (loss: 0.037333860993385315, acc: 0.9848812222480774)
[2025-02-13 04:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57][root][INFO] - Training Epoch: 2/2, step 5062/7134 completed (loss: 0.05053310841321945, acc: 0.9828473329544067)
[2025-02-13 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:58][root][INFO] - Training Epoch: 2/2, step 5063/7134 completed (loss: 0.03582190349698067, acc: 0.9883138537406921)
[2025-02-13 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:58][root][INFO] - Training Epoch: 2/2, step 5064/7134 completed (loss: 0.02361319214105606, acc: 0.9948630332946777)
[2025-02-13 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:59][root][INFO] - Training Epoch: 2/2, step 5065/7134 completed (loss: 0.038504283875226974, acc: 0.995768666267395)
[2025-02-13 04:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:59][root][INFO] - Training Epoch: 2/2, step 5066/7134 completed (loss: 0.037554703652858734, acc: 0.9878970980644226)
[2025-02-13 04:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00][root][INFO] - Training Epoch: 2/2, step 5067/7134 completed (loss: 0.017979811877012253, acc: 0.9961013793945312)
[2025-02-13 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00][root][INFO] - Training Epoch: 2/2, step 5068/7134 completed (loss: 0.0334111712872982, acc: 0.9889415502548218)
[2025-02-13 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00][root][INFO] - Training Epoch: 2/2, step 5069/7134 completed (loss: 0.02208767458796501, acc: 0.9906250238418579)
[2025-02-13 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:01][root][INFO] - Training Epoch: 2/2, step 5070/7134 completed (loss: 0.021575836464762688, acc: 0.9942528605461121)
[2025-02-13 04:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:01][root][INFO] - Training Epoch: 2/2, step 5071/7134 completed (loss: 0.0494479201734066, acc: 0.9873772859573364)
[2025-02-13 04:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02][root][INFO] - Training Epoch: 2/2, step 5072/7134 completed (loss: 0.05222534388303757, acc: 0.9849435091018677)
[2025-02-13 04:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02][root][INFO] - Training Epoch: 2/2, step 5073/7134 completed (loss: 0.0468393936753273, acc: 0.9864603281021118)
[2025-02-13 04:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02][root][INFO] - Training Epoch: 2/2, step 5074/7134 completed (loss: 0.03962651640176773, acc: 0.9883913993835449)
[2025-02-13 04:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:03][root][INFO] - Training Epoch: 2/2, step 5075/7134 completed (loss: 0.03428380563855171, acc: 0.9941860437393188)
[2025-02-13 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:03][root][INFO] - Training Epoch: 2/2, step 5076/7134 completed (loss: 0.03645145520567894, acc: 0.9904305934906006)
[2025-02-13 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04][root][INFO] - Training Epoch: 2/2, step 5077/7134 completed (loss: 0.021732816472649574, acc: 0.9920886158943176)
[2025-02-13 04:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04][root][INFO] - Training Epoch: 2/2, step 5078/7134 completed (loss: 0.06922488659620285, acc: 0.9853556752204895)
[2025-02-13 04:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04][root][INFO] - Training Epoch: 2/2, step 5079/7134 completed (loss: 0.022221939638257027, acc: 0.9918367266654968)
[2025-02-13 04:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:05][root][INFO] - Training Epoch: 2/2, step 5080/7134 completed (loss: 0.015682458877563477, acc: 0.9940898418426514)
[2025-02-13 04:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:05][root][INFO] - Training Epoch: 2/2, step 5081/7134 completed (loss: 0.009055177681148052, acc: 0.9945651888847351)
[2025-02-13 04:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:06][root][INFO] - Training Epoch: 2/2, step 5082/7134 completed (loss: 0.015188204124569893, acc: 0.9965986609458923)
[2025-02-13 04:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:06][root][INFO] - Training Epoch: 2/2, step 5083/7134 completed (loss: 0.0482010655105114, acc: 0.9943740963935852)
[2025-02-13 04:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:07][root][INFO] - Training Epoch: 2/2, step 5084/7134 completed (loss: 0.01315052155405283, acc: 0.9942129850387573)
[2025-02-13 04:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:07][root][INFO] - Training Epoch: 2/2, step 5085/7134 completed (loss: 0.0180419459939003, acc: 0.9940828680992126)
[2025-02-13 04:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08][root][INFO] - Training Epoch: 2/2, step 5086/7134 completed (loss: 0.032421283423900604, acc: 0.9911054372787476)
[2025-02-13 04:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08][root][INFO] - Training Epoch: 2/2, step 5087/7134 completed (loss: 0.014233815483748913, acc: 0.9959839582443237)
[2025-02-13 04:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08][root][INFO] - Training Epoch: 2/2, step 5088/7134 completed (loss: 0.00861620157957077, acc: 0.9985358715057373)
[2025-02-13 04:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:09][root][INFO] - Training Epoch: 2/2, step 5089/7134 completed (loss: 0.01293663028627634, acc: 0.9975062608718872)
[2025-02-13 04:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:09][root][INFO] - Training Epoch: 2/2, step 5090/7134 completed (loss: 0.011426558718085289, acc: 0.9964994192123413)
[2025-02-13 04:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:10][root][INFO] - Training Epoch: 2/2, step 5091/7134 completed (loss: 0.00853646732866764, acc: 0.9986824989318848)
[2025-02-13 04:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:10][root][INFO] - Training Epoch: 2/2, step 5092/7134 completed (loss: 0.02906317636370659, acc: 0.9944674968719482)
[2025-02-13 04:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11][root][INFO] - Training Epoch: 2/2, step 5093/7134 completed (loss: 0.023189136758446693, acc: 0.9917126893997192)
[2025-02-13 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11][root][INFO] - Training Epoch: 2/2, step 5094/7134 completed (loss: 0.03876786306500435, acc: 0.9909090995788574)
[2025-02-13 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11][root][INFO] - Training Epoch: 2/2, step 5095/7134 completed (loss: 0.07557053118944168, acc: 0.9812792539596558)
[2025-02-13 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:12][root][INFO] - Training Epoch: 2/2, step 5096/7134 completed (loss: 0.03513277322053909, acc: 0.9907514452934265)
[2025-02-13 04:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:12][root][INFO] - Training Epoch: 2/2, step 5097/7134 completed (loss: 0.027614109218120575, acc: 0.990231990814209)
[2025-02-13 04:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:13][root][INFO] - Training Epoch: 2/2, step 5098/7134 completed (loss: 0.019479598850011826, acc: 0.9921259880065918)
[2025-02-13 04:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:13][root][INFO] - Training Epoch: 2/2, step 5099/7134 completed (loss: 0.027950750663876534, acc: 0.9896073937416077)
[2025-02-13 04:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14][root][INFO] - Training Epoch: 2/2, step 5100/7134 completed (loss: 0.029853571206331253, acc: 0.992290735244751)
[2025-02-13 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14][root][INFO] - Training Epoch: 2/2, step 5101/7134 completed (loss: 0.01147141121327877, acc: 0.996503472328186)
[2025-02-13 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14][root][INFO] - Training Epoch: 2/2, step 5102/7134 completed (loss: 0.031684063374996185, acc: 0.9904502034187317)
[2025-02-13 04:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:15][root][INFO] - Training Epoch: 2/2, step 5103/7134 completed (loss: 0.02908528409898281, acc: 0.9908362030982971)
[2025-02-13 04:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:15][root][INFO] - Training Epoch: 2/2, step 5104/7134 completed (loss: 0.019620567560195923, acc: 0.9955703020095825)
[2025-02-13 04:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:16][root][INFO] - Training Epoch: 2/2, step 5105/7134 completed (loss: 0.02657787688076496, acc: 0.992548406124115)
[2025-02-13 04:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:16][root][INFO] - Training Epoch: 2/2, step 5106/7134 completed (loss: 0.01767132803797722, acc: 0.9925925731658936)
[2025-02-13 04:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17][root][INFO] - Training Epoch: 2/2, step 5107/7134 completed (loss: 0.08082566410303116, acc: 0.9841521382331848)
[2025-02-13 04:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17][root][INFO] - Training Epoch: 2/2, step 5108/7134 completed (loss: 0.044164497405290604, acc: 0.9860627055168152)
[2025-02-13 04:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17][root][INFO] - Training Epoch: 2/2, step 5109/7134 completed (loss: 0.054530251771211624, acc: 0.9867256879806519)
[2025-02-13 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:18][root][INFO] - Training Epoch: 2/2, step 5110/7134 completed (loss: 0.05645119026303291, acc: 0.9846153855323792)
[2025-02-13 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:18][root][INFO] - Training Epoch: 2/2, step 5111/7134 completed (loss: 0.021907638758420944, acc: 0.9931507110595703)
[2025-02-13 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:19][root][INFO] - Training Epoch: 2/2, step 5112/7134 completed (loss: 0.05947379022836685, acc: 0.9887920022010803)
[2025-02-13 04:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:19][root][INFO] - Training Epoch: 2/2, step 5113/7134 completed (loss: 0.051306404173374176, acc: 0.9850746393203735)
[2025-02-13 04:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20][root][INFO] - Training Epoch: 2/2, step 5114/7134 completed (loss: 0.02542678266763687, acc: 0.9944367408752441)
[2025-02-13 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20][root][INFO] - Training Epoch: 2/2, step 5115/7134 completed (loss: 0.015894263982772827, acc: 0.9948387145996094)
[2025-02-13 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20][root][INFO] - Training Epoch: 2/2, step 5116/7134 completed (loss: 0.012002517469227314, acc: 0.9961038827896118)
[2025-02-13 04:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:21][root][INFO] - Training Epoch: 2/2, step 5117/7134 completed (loss: 0.022648800164461136, acc: 0.9930264949798584)
[2025-02-13 04:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:21][root][INFO] - Training Epoch: 2/2, step 5118/7134 completed (loss: 0.04804941266775131, acc: 0.9908952713012695)
[2025-02-13 04:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:22][root][INFO] - Training Epoch: 2/2, step 5119/7134 completed (loss: 0.02679038979113102, acc: 0.993122398853302)
[2025-02-13 04:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:22][root][INFO] - Training Epoch: 2/2, step 5120/7134 completed (loss: 0.06312531232833862, acc: 0.985602080821991)
[2025-02-13 04:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23][root][INFO] - Training Epoch: 2/2, step 5121/7134 completed (loss: 0.021471792832016945, acc: 0.994452178478241)
[2025-02-13 04:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23][root][INFO] - Training Epoch: 2/2, step 5122/7134 completed (loss: 0.039131999015808105, acc: 0.9859353303909302)
[2025-02-13 04:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23][root][INFO] - Training Epoch: 2/2, step 5123/7134 completed (loss: 0.026149064302444458, acc: 0.9927623867988586)
[2025-02-13 04:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:24][root][INFO] - Training Epoch: 2/2, step 5124/7134 completed (loss: 0.03591648489236832, acc: 0.9888198971748352)
[2025-02-13 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:24][root][INFO] - Training Epoch: 2/2, step 5125/7134 completed (loss: 0.02802959643304348, acc: 0.9968404173851013)
[2025-02-13 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:25][root][INFO] - Training Epoch: 2/2, step 5126/7134 completed (loss: 0.017092429101467133, acc: 0.9948717951774597)
[2025-02-13 04:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:25][root][INFO] - Training Epoch: 2/2, step 5127/7134 completed (loss: 0.026505881920456886, acc: 0.9930070042610168)
[2025-02-13 04:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26][root][INFO] - Training Epoch: 2/2, step 5128/7134 completed (loss: 0.01299720536917448, acc: 0.995604395866394)
[2025-02-13 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26][root][INFO] - Training Epoch: 2/2, step 5129/7134 completed (loss: 0.044753264635801315, acc: 0.9823529124259949)
[2025-02-13 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26][root][INFO] - Training Epoch: 2/2, step 5130/7134 completed (loss: 0.013566629029810429, acc: 0.9937597513198853)
[2025-02-13 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:27][root][INFO] - Training Epoch: 2/2, step 5131/7134 completed (loss: 0.031089095398783684, acc: 0.9918830990791321)
[2025-02-13 04:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:27][root][INFO] - Training Epoch: 2/2, step 5132/7134 completed (loss: 0.009942321106791496, acc: 0.998019814491272)
[2025-02-13 04:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28][root][INFO] - Training Epoch: 2/2, step 5133/7134 completed (loss: 0.04581481218338013, acc: 0.9829192757606506)
[2025-02-13 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28][root][INFO] - Training Epoch: 2/2, step 5134/7134 completed (loss: 0.015026348643004894, acc: 0.998123824596405)
[2025-02-13 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28][root][INFO] - Training Epoch: 2/2, step 5135/7134 completed (loss: 0.04967036098241806, acc: 0.991428554058075)
[2025-02-13 04:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:29][root][INFO] - Training Epoch: 2/2, step 5136/7134 completed (loss: 0.02823237143456936, acc: 0.9890710115432739)
[2025-02-13 04:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:29][root][INFO] - Training Epoch: 2/2, step 5137/7134 completed (loss: 0.015174368396401405, acc: 0.9963302612304688)
[2025-02-13 04:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:30][root][INFO] - Training Epoch: 2/2, step 5138/7134 completed (loss: 0.014618463814258575, acc: 0.9962499737739563)
[2025-02-13 04:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:30][root][INFO] - Training Epoch: 2/2, step 5139/7134 completed (loss: 0.013195428997278214, acc: 0.9971387982368469)
[2025-02-13 04:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31][root][INFO] - Training Epoch: 2/2, step 5140/7134 completed (loss: 0.022826118394732475, acc: 0.9946977496147156)
[2025-02-13 04:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31][root][INFO] - Training Epoch: 2/2, step 5141/7134 completed (loss: 0.024523738771677017, acc: 0.9961389899253845)
[2025-02-13 04:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31][root][INFO] - Training Epoch: 2/2, step 5142/7134 completed (loss: 0.012093923054635525, acc: 0.9952940940856934)
[2025-02-13 04:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:32][root][INFO] - Training Epoch: 2/2, step 5143/7134 completed (loss: 0.01871771179139614, acc: 0.9923858046531677)
[2025-02-13 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:32][root][INFO] - Training Epoch: 2/2, step 5144/7134 completed (loss: 0.016547303646802902, acc: 0.9941657185554504)
[2025-02-13 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33][root][INFO] - Training Epoch: 2/2, step 5145/7134 completed (loss: 0.009801900945603848, acc: 0.997952938079834)
[2025-02-13 04:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33][root][INFO] - Training Epoch: 2/2, step 5146/7134 completed (loss: 0.04681970924139023, acc: 0.9843924045562744)
[2025-02-13 04:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33][root][INFO] - Training Epoch: 2/2, step 5147/7134 completed (loss: 0.019132668152451515, acc: 0.9918032884597778)
[2025-02-13 04:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34][root][INFO] - Training Epoch: 2/2, step 5148/7134 completed (loss: 0.06327493488788605, acc: 0.9853528738021851)
[2025-02-13 04:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34][root][INFO] - Training Epoch: 2/2, step 5149/7134 completed (loss: 0.028235694393515587, acc: 0.9900497794151306)
[2025-02-13 04:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:35][root][INFO] - Training Epoch: 2/2, step 5150/7134 completed (loss: 0.049624308943748474, acc: 0.9830508232116699)
[2025-02-13 04:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:35][root][INFO] - Training Epoch: 2/2, step 5151/7134 completed (loss: 0.038777489215135574, acc: 0.9888613820075989)
[2025-02-13 04:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36][root][INFO] - Training Epoch: 2/2, step 5152/7134 completed (loss: 0.02410568855702877, acc: 0.9906166195869446)
[2025-02-13 04:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36][root][INFO] - Training Epoch: 2/2, step 5153/7134 completed (loss: 0.013794886879622936, acc: 0.9958847761154175)
[2025-02-13 04:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36][root][INFO] - Training Epoch: 2/2, step 5154/7134 completed (loss: 0.018651682883501053, acc: 0.9931740760803223)
[2025-02-13 04:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:37][root][INFO] - Training Epoch: 2/2, step 5155/7134 completed (loss: 0.03963969275355339, acc: 0.9954819083213806)
[2025-02-13 04:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:37][root][INFO] - Training Epoch: 2/2, step 5156/7134 completed (loss: 0.0184677354991436, acc: 0.9940416812896729)
[2025-02-13 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:38][root][INFO] - Training Epoch: 2/2, step 5157/7134 completed (loss: 0.04398873448371887, acc: 0.9889975786209106)
[2025-02-13 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:38][root][INFO] - Training Epoch: 2/2, step 5158/7134 completed (loss: 0.07069169729948044, acc: 0.9792284965515137)
[2025-02-13 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:39][root][INFO] - Training Epoch: 2/2, step 5159/7134 completed (loss: 0.01915106177330017, acc: 0.9934980273246765)
[2025-02-13 04:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:39][root][INFO] - Training Epoch: 2/2, step 5160/7134 completed (loss: 0.048731960356235504, acc: 0.9855907559394836)
[2025-02-13 04:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40][root][INFO] - Training Epoch: 2/2, step 5161/7134 completed (loss: 0.06705785542726517, acc: 0.9790209531784058)
[2025-02-13 04:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40][root][INFO] - Training Epoch: 2/2, step 5162/7134 completed (loss: 0.01619727350771427, acc: 0.9935170412063599)
[2025-02-13 04:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40][root][INFO] - Training Epoch: 2/2, step 5163/7134 completed (loss: 0.01772436499595642, acc: 0.9943342804908752)
[2025-02-13 04:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:41][root][INFO] - Training Epoch: 2/2, step 5164/7134 completed (loss: 0.01568908989429474, acc: 0.9966386556625366)
[2025-02-13 04:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:41][root][INFO] - Training Epoch: 2/2, step 5165/7134 completed (loss: 0.03182009607553482, acc: 0.9924699068069458)
[2025-02-13 04:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42][root][INFO] - Training Epoch: 2/2, step 5166/7134 completed (loss: 0.007058838848024607, acc: 1.0)
[2025-02-13 04:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42][root][INFO] - Training Epoch: 2/2, step 5167/7134 completed (loss: 0.04675309732556343, acc: 0.9775640964508057)
[2025-02-13 04:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42][root][INFO] - Training Epoch: 2/2, step 5168/7134 completed (loss: 0.03216642141342163, acc: 0.9875776171684265)
[2025-02-13 04:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:43][root][INFO] - Training Epoch: 2/2, step 5169/7134 completed (loss: 0.0420260913670063, acc: 0.9844236969947815)
[2025-02-13 04:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:43][root][INFO] - Training Epoch: 2/2, step 5170/7134 completed (loss: 0.03208136558532715, acc: 0.9894039630889893)
[2025-02-13 04:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44][root][INFO] - Training Epoch: 2/2, step 5171/7134 completed (loss: 0.01811942271888256, acc: 0.997863233089447)
[2025-02-13 04:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44][root][INFO] - Training Epoch: 2/2, step 5172/7134 completed (loss: 0.065533347427845, acc: 0.9829642176628113)
[2025-02-13 04:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44][root][INFO] - Training Epoch: 2/2, step 5173/7134 completed (loss: 0.04660605266690254, acc: 0.9858267903327942)
[2025-02-13 04:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:45][root][INFO] - Training Epoch: 2/2, step 5174/7134 completed (loss: 0.015909038484096527, acc: 0.9974457025527954)
[2025-02-13 04:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:45][root][INFO] - Training Epoch: 2/2, step 5175/7134 completed (loss: 0.010616699233651161, acc: 0.9958563446998596)
[2025-02-13 04:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:46][root][INFO] - Training Epoch: 2/2, step 5176/7134 completed (loss: 0.011763762682676315, acc: 0.9969651103019714)
[2025-02-13 04:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:46][root][INFO] - Training Epoch: 2/2, step 5177/7134 completed (loss: 0.04896879568696022, acc: 0.9895651936531067)
[2025-02-13 04:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47][root][INFO] - Training Epoch: 2/2, step 5178/7134 completed (loss: 0.08666262775659561, acc: 0.9756097793579102)
[2025-02-13 04:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47][root][INFO] - Training Epoch: 2/2, step 5179/7134 completed (loss: 0.020916901528835297, acc: 0.9940828680992126)
[2025-02-13 04:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47][root][INFO] - Training Epoch: 2/2, step 5180/7134 completed (loss: 0.05198952928185463, acc: 0.9831144213676453)
[2025-02-13 04:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:48][root][INFO] - Training Epoch: 2/2, step 5181/7134 completed (loss: 0.05769214779138565, acc: 0.9867021441459656)
[2025-02-13 04:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:48][root][INFO] - Training Epoch: 2/2, step 5182/7134 completed (loss: 0.12980833649635315, acc: 0.9666666388511658)
[2025-02-13 04:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49][root][INFO] - Training Epoch: 2/2, step 5183/7134 completed (loss: 0.046366289258003235, acc: 0.9900990128517151)
[2025-02-13 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49][root][INFO] - Training Epoch: 2/2, step 5184/7134 completed (loss: 0.02206259034574032, acc: 0.9926650524139404)
[2025-02-13 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49][root][INFO] - Training Epoch: 2/2, step 5185/7134 completed (loss: 0.04263124614953995, acc: 0.9879518151283264)
[2025-02-13 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:50][root][INFO] - Training Epoch: 2/2, step 5186/7134 completed (loss: 0.038451455533504486, acc: 0.9857434034347534)
[2025-02-13 04:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:50][root][INFO] - Training Epoch: 2/2, step 5187/7134 completed (loss: 0.020305082201957703, acc: 0.9942857027053833)
[2025-02-13 04:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51][root][INFO] - Training Epoch: 2/2, step 5188/7134 completed (loss: 0.10339827835559845, acc: 0.9727272987365723)
[2025-02-13 04:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51][root][INFO] - Training Epoch: 2/2, step 5189/7134 completed (loss: 0.06035899743437767, acc: 0.9872449040412903)
[2025-02-13 04:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51][root][INFO] - Training Epoch: 2/2, step 5190/7134 completed (loss: 0.03823874890804291, acc: 0.9904988408088684)
[2025-02-13 04:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:52][root][INFO] - Training Epoch: 2/2, step 5191/7134 completed (loss: 0.0999208316206932, acc: 0.9707174301147461)
[2025-02-13 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:52][root][INFO] - Training Epoch: 2/2, step 5192/7134 completed (loss: 0.04529563710093498, acc: 0.9916765689849854)
[2025-02-13 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53][root][INFO] - Training Epoch: 2/2, step 5193/7134 completed (loss: 0.015249490737915039, acc: 0.9960629940032959)
[2025-02-13 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53][root][INFO] - Training Epoch: 2/2, step 5194/7134 completed (loss: 0.056378960609436035, acc: 0.9821782112121582)
[2025-02-13 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53][root][INFO] - Training Epoch: 2/2, step 5195/7134 completed (loss: 0.04564955085515976, acc: 0.9890350699424744)
[2025-02-13 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54][root][INFO] - Training Epoch: 2/2, step 5196/7134 completed (loss: 0.0381154902279377, acc: 0.9880383014678955)
[2025-02-13 04:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54][root][INFO] - Training Epoch: 2/2, step 5197/7134 completed (loss: 0.02790193073451519, acc: 0.991769552230835)
[2025-02-13 04:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54][root][INFO] - Training Epoch: 2/2, step 5198/7134 completed (loss: 0.022734809666872025, acc: 0.9916840195655823)
[2025-02-13 04:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:55][root][INFO] - Training Epoch: 2/2, step 5199/7134 completed (loss: 0.07250957936048508, acc: 0.9825581312179565)
[2025-02-13 04:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:55][root][INFO] - Training Epoch: 2/2, step 5200/7134 completed (loss: 0.020806143060326576, acc: 0.9921414256095886)
[2025-02-13 04:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:56][root][INFO] - Training Epoch: 2/2, step 5201/7134 completed (loss: 0.046844542026519775, acc: 0.9915561079978943)
[2025-02-13 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:56][root][INFO] - Training Epoch: 2/2, step 5202/7134 completed (loss: 0.061877306550741196, acc: 0.9862637519836426)
[2025-02-13 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57][root][INFO] - Training Epoch: 2/2, step 5203/7134 completed (loss: 0.048695776611566544, acc: 0.9863945841789246)
[2025-02-13 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57][root][INFO] - Training Epoch: 2/2, step 5204/7134 completed (loss: 0.018510673195123672, acc: 0.9938144087791443)
[2025-02-13 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57][root][INFO] - Training Epoch: 2/2, step 5205/7134 completed (loss: 0.030406594276428223, acc: 0.9887640476226807)
[2025-02-13 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:58][root][INFO] - Training Epoch: 2/2, step 5206/7134 completed (loss: 0.043155573308467865, acc: 0.9935794472694397)
[2025-02-13 04:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:58][root][INFO] - Training Epoch: 2/2, step 5207/7134 completed (loss: 0.057769227772951126, acc: 0.9839034080505371)
[2025-02-13 04:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59][root][INFO] - Training Epoch: 2/2, step 5208/7134 completed (loss: 0.019655508920550346, acc: 0.9947229623794556)
[2025-02-13 04:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59][root][INFO] - Training Epoch: 2/2, step 5209/7134 completed (loss: 0.027775725349783897, acc: 0.9879518151283264)
[2025-02-13 04:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59][root][INFO] - Training Epoch: 2/2, step 5210/7134 completed (loss: 0.02448081225156784, acc: 0.9933244585990906)
[2025-02-13 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:00][root][INFO] - Training Epoch: 2/2, step 5211/7134 completed (loss: 0.03414009511470795, acc: 0.9861111044883728)
[2025-02-13 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:00][root][INFO] - Training Epoch: 2/2, step 5212/7134 completed (loss: 0.05995487421751022, acc: 0.979522168636322)
[2025-02-13 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01][root][INFO] - Training Epoch: 2/2, step 5213/7134 completed (loss: 0.050905365496873856, acc: 0.9910072088241577)
[2025-02-13 04:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01][root][INFO] - Training Epoch: 2/2, step 5214/7134 completed (loss: 0.08665426820516586, acc: 0.9721029996871948)
[2025-02-13 04:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01][root][INFO] - Training Epoch: 2/2, step 5215/7134 completed (loss: 0.05911344662308693, acc: 0.9795657992362976)
[2025-02-13 04:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02][root][INFO] - Training Epoch: 2/2, step 5216/7134 completed (loss: 0.04553651064634323, acc: 0.9847908616065979)
[2025-02-13 04:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02][root][INFO] - Training Epoch: 2/2, step 5217/7134 completed (loss: 0.05504997819662094, acc: 0.9817073345184326)
[2025-02-13 04:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02][root][INFO] - Training Epoch: 2/2, step 5218/7134 completed (loss: 0.02660512365400791, acc: 0.9955056309700012)
[2025-02-13 04:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03][root][INFO] - Training Epoch: 2/2, step 5219/7134 completed (loss: 0.05622771382331848, acc: 0.9878934621810913)
[2025-02-13 04:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03][root][INFO] - Training Epoch: 2/2, step 5220/7134 completed (loss: 0.0554678849875927, acc: 0.9821109175682068)
[2025-02-13 04:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03][root][INFO] - Training Epoch: 2/2, step 5221/7134 completed (loss: 0.05464516580104828, acc: 0.9890965819358826)
[2025-02-13 04:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:04][root][INFO] - Training Epoch: 2/2, step 5222/7134 completed (loss: 0.023557744920253754, acc: 0.9937694668769836)
[2025-02-13 04:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:04][root][INFO] - Training Epoch: 2/2, step 5223/7134 completed (loss: 0.025286400690674782, acc: 0.9907833933830261)
[2025-02-13 04:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05][root][INFO] - Training Epoch: 2/2, step 5224/7134 completed (loss: 0.05789889767765999, acc: 0.9841827750205994)
[2025-02-13 04:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05][root][INFO] - Training Epoch: 2/2, step 5225/7134 completed (loss: 0.060751792043447495, acc: 0.9854604005813599)
[2025-02-13 04:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05][root][INFO] - Training Epoch: 2/2, step 5226/7134 completed (loss: 0.05237477645277977, acc: 0.9868203997612)
[2025-02-13 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:06][root][INFO] - Training Epoch: 2/2, step 5227/7134 completed (loss: 0.040379393845796585, acc: 0.9961038827896118)
[2025-02-13 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:06][root][INFO] - Training Epoch: 2/2, step 5228/7134 completed (loss: 0.054667674005031586, acc: 0.9898256063461304)
[2025-02-13 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:07][root][INFO] - Training Epoch: 2/2, step 5229/7134 completed (loss: 0.030869564041495323, acc: 0.98959881067276)
[2025-02-13 04:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:07][root][INFO] - Training Epoch: 2/2, step 5230/7134 completed (loss: 0.023553516715765, acc: 0.993261456489563)
[2025-02-13 04:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08][root][INFO] - Training Epoch: 2/2, step 5231/7134 completed (loss: 0.039535362273454666, acc: 0.9936708807945251)
[2025-02-13 04:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08][root][INFO] - Training Epoch: 2/2, step 5232/7134 completed (loss: 0.020173154771327972, acc: 0.9910314083099365)
[2025-02-13 04:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08][root][INFO] - Training Epoch: 2/2, step 5233/7134 completed (loss: 0.024239428341388702, acc: 0.994369387626648)
[2025-02-13 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:09][root][INFO] - Training Epoch: 2/2, step 5234/7134 completed (loss: 0.028771966695785522, acc: 0.9963280558586121)
[2025-02-13 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:09][root][INFO] - Training Epoch: 2/2, step 5235/7134 completed (loss: 0.015316533856093884, acc: 0.9949579834938049)
[2025-02-13 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:10][root][INFO] - Training Epoch: 2/2, step 5236/7134 completed (loss: 0.007959284819662571, acc: 0.9982638955116272)
[2025-02-13 04:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:10][root][INFO] - Training Epoch: 2/2, step 5237/7134 completed (loss: 0.007438951171934605, acc: 0.9964157938957214)
[2025-02-13 04:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11][root][INFO] - Training Epoch: 2/2, step 5238/7134 completed (loss: 0.013517050072550774, acc: 0.9964788556098938)
[2025-02-13 04:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11][root][INFO] - Training Epoch: 2/2, step 5239/7134 completed (loss: 0.013506353832781315, acc: 0.9942085146903992)
[2025-02-13 04:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11][root][INFO] - Training Epoch: 2/2, step 5240/7134 completed (loss: 0.02098996378481388, acc: 0.9958506226539612)
[2025-02-13 04:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:12][root][INFO] - Training Epoch: 2/2, step 5241/7134 completed (loss: 0.03706939518451691, acc: 0.9890282154083252)
[2025-02-13 04:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:12][root][INFO] - Training Epoch: 2/2, step 5242/7134 completed (loss: 0.010801928117871284, acc: 0.9966942071914673)
[2025-02-13 04:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13][root][INFO] - Training Epoch: 2/2, step 5243/7134 completed (loss: 0.02496599778532982, acc: 0.9948275685310364)
[2025-02-13 04:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13][root][INFO] - Training Epoch: 2/2, step 5244/7134 completed (loss: 0.016139870509505272, acc: 0.9931034445762634)
[2025-02-13 04:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13][root][INFO] - Training Epoch: 2/2, step 5245/7134 completed (loss: 0.013064991682767868, acc: 0.9965096116065979)
[2025-02-13 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:14][root][INFO] - Training Epoch: 2/2, step 5246/7134 completed (loss: 0.01854376681149006, acc: 0.9907975196838379)
[2025-02-13 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:14][root][INFO] - Training Epoch: 2/2, step 5247/7134 completed (loss: 0.01466423086822033, acc: 0.9944444298744202)
[2025-02-13 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15][root][INFO] - Training Epoch: 2/2, step 5248/7134 completed (loss: 0.05038265883922577, acc: 0.9868913888931274)
[2025-02-13 04:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15][root][INFO] - Training Epoch: 2/2, step 5249/7134 completed (loss: 0.006516703404486179, acc: 0.99842768907547)
[2025-02-13 04:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15][root][INFO] - Training Epoch: 2/2, step 5250/7134 completed (loss: 0.025418737903237343, acc: 0.9936948418617249)
[2025-02-13 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:16][root][INFO] - Training Epoch: 2/2, step 5251/7134 completed (loss: 0.02300451323390007, acc: 0.9915151596069336)
[2025-02-13 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:16][root][INFO] - Training Epoch: 2/2, step 5252/7134 completed (loss: 0.0458163321018219, acc: 0.9836448431015015)
[2025-02-13 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:17][root][INFO] - Training Epoch: 2/2, step 5253/7134 completed (loss: 0.014813925139605999, acc: 0.9934640526771545)
[2025-02-13 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:17][root][INFO] - Training Epoch: 2/2, step 5254/7134 completed (loss: 0.05086136609315872, acc: 0.9864176511764526)
[2025-02-13 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18][root][INFO] - Training Epoch: 2/2, step 5255/7134 completed (loss: 0.03393709287047386, acc: 0.9890410900115967)
[2025-02-13 04:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18][root][INFO] - Training Epoch: 2/2, step 5256/7134 completed (loss: 0.04863521456718445, acc: 0.9860896468162537)
[2025-02-13 04:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18][root][INFO] - Training Epoch: 2/2, step 5257/7134 completed (loss: 0.05367876961827278, acc: 0.9847792983055115)
[2025-02-13 04:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:19][root][INFO] - Training Epoch: 2/2, step 5258/7134 completed (loss: 0.022606998682022095, acc: 0.9872449040412903)
[2025-02-13 04:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:19][root][INFO] - Training Epoch: 2/2, step 5259/7134 completed (loss: 0.03832351043820381, acc: 0.9936708807945251)
[2025-02-13 04:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:20][root][INFO] - Training Epoch: 2/2, step 5260/7134 completed (loss: 0.02526293508708477, acc: 0.992682933807373)
[2025-02-13 04:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:20][root][INFO] - Training Epoch: 2/2, step 5261/7134 completed (loss: 0.0624585822224617, acc: 0.9884615540504456)
[2025-02-13 04:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21][root][INFO] - Training Epoch: 2/2, step 5262/7134 completed (loss: 0.015452336519956589, acc: 0.9931034445762634)
[2025-02-13 04:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21][root][INFO] - Training Epoch: 2/2, step 5263/7134 completed (loss: 0.040389854460954666, acc: 0.9859648942947388)
[2025-02-13 04:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21][root][INFO] - Training Epoch: 2/2, step 5264/7134 completed (loss: 0.02183833159506321, acc: 0.9914945363998413)
[2025-02-13 04:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:22][root][INFO] - Training Epoch: 2/2, step 5265/7134 completed (loss: 0.030505238100886345, acc: 0.9936467409133911)
[2025-02-13 04:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:22][root][INFO] - Training Epoch: 2/2, step 5266/7134 completed (loss: 0.020349707454442978, acc: 0.9945205450057983)
[2025-02-13 04:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:23][root][INFO] - Training Epoch: 2/2, step 5267/7134 completed (loss: 0.07238908857107162, acc: 0.9797822833061218)
[2025-02-13 04:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:23][root][INFO] - Training Epoch: 2/2, step 5268/7134 completed (loss: 0.03913343325257301, acc: 0.9827255010604858)
[2025-02-13 04:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24][root][INFO] - Training Epoch: 2/2, step 5269/7134 completed (loss: 0.06380271911621094, acc: 0.9767441749572754)
[2025-02-13 04:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24][root][INFO] - Training Epoch: 2/2, step 5270/7134 completed (loss: 0.023649735376238823, acc: 0.9919785857200623)
[2025-02-13 04:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24][root][INFO] - Training Epoch: 2/2, step 5271/7134 completed (loss: 0.05005139857530594, acc: 0.9823633432388306)
[2025-02-13 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:25][root][INFO] - Training Epoch: 2/2, step 5272/7134 completed (loss: 0.018339872360229492, acc: 0.9941349029541016)
[2025-02-13 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:25][root][INFO] - Training Epoch: 2/2, step 5273/7134 completed (loss: 0.0196759682148695, acc: 0.9949066042900085)
[2025-02-13 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:26][root][INFO] - Training Epoch: 2/2, step 5274/7134 completed (loss: 0.036876410245895386, acc: 0.9880596995353699)
[2025-02-13 04:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:26][root][INFO] - Training Epoch: 2/2, step 5275/7134 completed (loss: 0.015456072054803371, acc: 0.9926874041557312)
[2025-02-13 04:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27][root][INFO] - Training Epoch: 2/2, step 5276/7134 completed (loss: 0.03385041281580925, acc: 0.9865471124649048)
[2025-02-13 04:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27][root][INFO] - Training Epoch: 2/2, step 5277/7134 completed (loss: 0.02246139571070671, acc: 0.9927954077720642)
[2025-02-13 04:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27][root][INFO] - Training Epoch: 2/2, step 5278/7134 completed (loss: 0.032809413969516754, acc: 0.9908257126808167)
[2025-02-13 04:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:28][root][INFO] - Training Epoch: 2/2, step 5279/7134 completed (loss: 0.008806316182017326, acc: 0.9980545043945312)
[2025-02-13 04:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:28][root][INFO] - Training Epoch: 2/2, step 5280/7134 completed (loss: 0.0299459807574749, acc: 0.9933664798736572)
[2025-02-13 04:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29][root][INFO] - Training Epoch: 2/2, step 5281/7134 completed (loss: 0.015387049876153469, acc: 0.9966722130775452)
[2025-02-13 04:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29][root][INFO] - Training Epoch: 2/2, step 5282/7134 completed (loss: 0.01761954464018345, acc: 0.9942938685417175)
[2025-02-13 04:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29][root][INFO] - Training Epoch: 2/2, step 5283/7134 completed (loss: 0.01772579364478588, acc: 0.9931600689888)
[2025-02-13 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:30][root][INFO] - Training Epoch: 2/2, step 5284/7134 completed (loss: 0.031491756439208984, acc: 0.9944674968719482)
[2025-02-13 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:30][root][INFO] - Training Epoch: 2/2, step 5285/7134 completed (loss: 0.03185702860355377, acc: 0.9893491268157959)
[2025-02-13 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:31][root][INFO] - Training Epoch: 2/2, step 5286/7134 completed (loss: 0.032216593623161316, acc: 0.9941176176071167)
[2025-02-13 04:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:31][root][INFO] - Training Epoch: 2/2, step 5287/7134 completed (loss: 0.01910797692835331, acc: 0.9932249188423157)
[2025-02-13 04:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32][root][INFO] - Training Epoch: 2/2, step 5288/7134 completed (loss: 0.010678436607122421, acc: 0.995398759841919)
[2025-02-13 04:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32][root][INFO] - Training Epoch: 2/2, step 5289/7134 completed (loss: 0.034265272319316864, acc: 0.9929178357124329)
[2025-02-13 04:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32][root][INFO] - Training Epoch: 2/2, step 5290/7134 completed (loss: 0.02693057991564274, acc: 0.9911392331123352)
[2025-02-13 04:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:33][root][INFO] - Training Epoch: 2/2, step 5291/7134 completed (loss: 0.02375911921262741, acc: 0.9951515197753906)
[2025-02-13 04:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:33][root][INFO] - Training Epoch: 2/2, step 5292/7134 completed (loss: 0.010474901646375656, acc: 0.9974554777145386)
[2025-02-13 04:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:34][root][INFO] - Training Epoch: 2/2, step 5293/7134 completed (loss: 0.023554805666208267, acc: 0.9927007555961609)
[2025-02-13 04:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:34][root][INFO] - Training Epoch: 2/2, step 5294/7134 completed (loss: 0.023677023127675056, acc: 0.9944211840629578)
[2025-02-13 04:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35][root][INFO] - Training Epoch: 2/2, step 5295/7134 completed (loss: 0.010105820372700691, acc: 0.9963235259056091)
[2025-02-13 04:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35][root][INFO] - Training Epoch: 2/2, step 5296/7134 completed (loss: 0.013468174263834953, acc: 0.9958275556564331)
[2025-02-13 04:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35][root][INFO] - Training Epoch: 2/2, step 5297/7134 completed (loss: 0.03223870322108269, acc: 0.9899425506591797)
[2025-02-13 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:36][root][INFO] - Training Epoch: 2/2, step 5298/7134 completed (loss: 0.019765548408031464, acc: 0.9965811967849731)
[2025-02-13 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:36][root][INFO] - Training Epoch: 2/2, step 5299/7134 completed (loss: 0.02175910770893097, acc: 0.9967897534370422)
[2025-02-13 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37][root][INFO] - Training Epoch: 2/2, step 5300/7134 completed (loss: 0.01808248460292816, acc: 0.9926578402519226)
[2025-02-13 04:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37][root][INFO] - Training Epoch: 2/2, step 5301/7134 completed (loss: 0.012803628109395504, acc: 0.9955947399139404)
[2025-02-13 04:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37][root][INFO] - Training Epoch: 2/2, step 5302/7134 completed (loss: 0.06649579107761383, acc: 0.9848024249076843)
[2025-02-13 04:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38][root][INFO] - Training Epoch: 2/2, step 5303/7134 completed (loss: 0.04489646852016449, acc: 0.991150438785553)
[2025-02-13 04:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38][root][INFO] - Training Epoch: 2/2, step 5304/7134 completed (loss: 0.06754590570926666, acc: 0.9866888523101807)
[2025-02-13 04:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:39][root][INFO] - Training Epoch: 2/2, step 5305/7134 completed (loss: 0.05726487189531326, acc: 0.9839416146278381)
[2025-02-13 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:39][root][INFO] - Training Epoch: 2/2, step 5306/7134 completed (loss: 0.012274805456399918, acc: 0.9986110925674438)
[2025-02-13 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40][root][INFO] - Training Epoch: 2/2, step 5307/7134 completed (loss: 0.05252633988857269, acc: 0.9845678806304932)
[2025-02-13 04:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40][root][INFO] - Training Epoch: 2/2, step 5308/7134 completed (loss: 0.04623087868094444, acc: 0.9909365773200989)
[2025-02-13 04:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40][root][INFO] - Training Epoch: 2/2, step 5309/7134 completed (loss: 0.055708229541778564, acc: 0.980322003364563)
[2025-02-13 04:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:41][root][INFO] - Training Epoch: 2/2, step 5310/7134 completed (loss: 0.046903517097234726, acc: 0.9832636117935181)
[2025-02-13 04:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:41][root][INFO] - Training Epoch: 2/2, step 5311/7134 completed (loss: 0.05711640417575836, acc: 0.9807383418083191)
[2025-02-13 04:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42][root][INFO] - Training Epoch: 2/2, step 5312/7134 completed (loss: 0.020044485107064247, acc: 0.9943714737892151)
[2025-02-13 04:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42][root][INFO] - Training Epoch: 2/2, step 5313/7134 completed (loss: 0.028007101267576218, acc: 0.9908925294876099)
[2025-02-13 04:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42][root][INFO] - Training Epoch: 2/2, step 5314/7134 completed (loss: 0.030239271000027657, acc: 0.9912891983985901)
[2025-02-13 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:43][root][INFO] - Training Epoch: 2/2, step 5315/7134 completed (loss: 0.02707197330892086, acc: 0.9885433912277222)
[2025-02-13 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:43][root][INFO] - Training Epoch: 2/2, step 5316/7134 completed (loss: 0.04465284198522568, acc: 0.9899159669876099)
[2025-02-13 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44][root][INFO] - Training Epoch: 2/2, step 5317/7134 completed (loss: 0.02664455957710743, acc: 0.9910394549369812)
[2025-02-13 04:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44][root][INFO] - Training Epoch: 2/2, step 5318/7134 completed (loss: 0.02176244743168354, acc: 0.9913644194602966)
[2025-02-13 04:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44][root][INFO] - Training Epoch: 2/2, step 5319/7134 completed (loss: 0.040158290416002274, acc: 0.9908758997917175)
[2025-02-13 04:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:45][root][INFO] - Training Epoch: 2/2, step 5320/7134 completed (loss: 0.036561671644449234, acc: 0.9953415989875793)
[2025-02-13 04:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:45][root][INFO] - Training Epoch: 2/2, step 5321/7134 completed (loss: 0.016174878925085068, acc: 0.9937984347343445)
[2025-02-13 04:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46][root][INFO] - Training Epoch: 2/2, step 5322/7134 completed (loss: 0.016857990995049477, acc: 0.9924585223197937)
[2025-02-13 04:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46][root][INFO] - Training Epoch: 2/2, step 5323/7134 completed (loss: 0.03415362164378166, acc: 0.989393949508667)
[2025-02-13 04:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46][root][INFO] - Training Epoch: 2/2, step 5324/7134 completed (loss: 0.014206181280314922, acc: 0.9953632354736328)
[2025-02-13 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:47][root][INFO] - Training Epoch: 2/2, step 5325/7134 completed (loss: 0.027941767126321793, acc: 0.991482138633728)
[2025-02-13 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:47][root][INFO] - Training Epoch: 2/2, step 5326/7134 completed (loss: 0.020517103374004364, acc: 0.9935483932495117)
[2025-02-13 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:48][root][INFO] - Training Epoch: 2/2, step 5327/7134 completed (loss: 0.026886899024248123, acc: 0.9927431344985962)
[2025-02-13 04:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:48][root][INFO] - Training Epoch: 2/2, step 5328/7134 completed (loss: 0.014560114592313766, acc: 0.9944953918457031)
[2025-02-13 04:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49][root][INFO] - Training Epoch: 2/2, step 5329/7134 completed (loss: 0.03186359256505966, acc: 0.990275502204895)
[2025-02-13 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49][root][INFO] - Training Epoch: 2/2, step 5330/7134 completed (loss: 0.027327705174684525, acc: 0.9906250238418579)
[2025-02-13 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49][root][INFO] - Training Epoch: 2/2, step 5331/7134 completed (loss: 0.030623972415924072, acc: 0.9929947257041931)
[2025-02-13 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:50][root][INFO] - Training Epoch: 2/2, step 5332/7134 completed (loss: 0.03462783247232437, acc: 0.995121955871582)
[2025-02-13 04:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:50][root][INFO] - Training Epoch: 2/2, step 5333/7134 completed (loss: 0.013519464060664177, acc: 0.9968847632408142)
[2025-02-13 04:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51][root][INFO] - Training Epoch: 2/2, step 5334/7134 completed (loss: 0.04463714733719826, acc: 0.992343008518219)
[2025-02-13 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51][root][INFO] - Training Epoch: 2/2, step 5335/7134 completed (loss: 0.03335264325141907, acc: 0.9936102032661438)
[2025-02-13 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51][root][INFO] - Training Epoch: 2/2, step 5336/7134 completed (loss: 0.027462437748908997, acc: 0.989062488079071)
[2025-02-13 04:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:52][root][INFO] - Training Epoch: 2/2, step 5337/7134 completed (loss: 0.02104017697274685, acc: 0.9899777173995972)
[2025-02-13 04:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:52][root][INFO] - Training Epoch: 2/2, step 5338/7134 completed (loss: 0.015360691584646702, acc: 0.9965753555297852)
[2025-02-13 04:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:53][root][INFO] - Training Epoch: 2/2, step 5339/7134 completed (loss: 0.029704662039875984, acc: 0.9922279715538025)
[2025-02-13 04:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:53][root][INFO] - Training Epoch: 2/2, step 5340/7134 completed (loss: 0.04915476590394974, acc: 0.9809358716011047)
[2025-02-13 04:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54][root][INFO] - Training Epoch: 2/2, step 5341/7134 completed (loss: 0.010287275537848473, acc: 0.996610164642334)
[2025-02-13 04:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54][root][INFO] - Training Epoch: 2/2, step 5342/7134 completed (loss: 0.03755618631839752, acc: 0.9866844415664673)
[2025-02-13 04:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54][root][INFO] - Training Epoch: 2/2, step 5343/7134 completed (loss: 0.04241025447845459, acc: 0.9868074059486389)
[2025-02-13 04:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:55][root][INFO] - Training Epoch: 2/2, step 5344/7134 completed (loss: 0.02979659102857113, acc: 0.9894737005233765)
[2025-02-13 04:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:55][root][INFO] - Training Epoch: 2/2, step 5345/7134 completed (loss: 0.025253966450691223, acc: 0.9908046126365662)
[2025-02-13 04:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:56][root][INFO] - Training Epoch: 2/2, step 5346/7134 completed (loss: 0.036469966173172, acc: 0.9874081611633301)
[2025-02-13 04:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:00][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0512, device='cuda:0') eval_epoch_loss=tensor(0.0500, device='cuda:0') eval_epoch_acc=tensor(0.9866, device='cuda:0')
[2025-02-13 04:20:00][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:20:00][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:20:00][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_5347_loss_0.04996589198708534/model.pt
[2025-02-13 04:20:00][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:01][root][INFO] - Training Epoch: 2/2, step 5347/7134 completed (loss: 0.021252481266856194, acc: 0.9920424222946167)
[2025-02-13 04:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:01][root][INFO] - Training Epoch: 2/2, step 5348/7134 completed (loss: 0.035203590989112854, acc: 0.990304708480835)
[2025-02-13 04:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02][root][INFO] - Training Epoch: 2/2, step 5349/7134 completed (loss: 0.03827524930238724, acc: 0.9870800971984863)
[2025-02-13 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02][root][INFO] - Training Epoch: 2/2, step 5350/7134 completed (loss: 0.029483210295438766, acc: 0.9854689836502075)
[2025-02-13 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02][root][INFO] - Training Epoch: 2/2, step 5351/7134 completed (loss: 0.030672116205096245, acc: 0.9900744557380676)
[2025-02-13 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:03][root][INFO] - Training Epoch: 2/2, step 5352/7134 completed (loss: 0.028970306739211082, acc: 0.9935317039489746)
[2025-02-13 04:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:03][root][INFO] - Training Epoch: 2/2, step 5353/7134 completed (loss: 0.010798760689795017, acc: 0.9977452158927917)
[2025-02-13 04:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:04][root][INFO] - Training Epoch: 2/2, step 5354/7134 completed (loss: 0.034038152545690536, acc: 0.9889867901802063)
[2025-02-13 04:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:04][root][INFO] - Training Epoch: 2/2, step 5355/7134 completed (loss: 0.06476586312055588, acc: 0.9846335649490356)
[2025-02-13 04:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05][root][INFO] - Training Epoch: 2/2, step 5356/7134 completed (loss: 0.04944872856140137, acc: 0.9850560426712036)
[2025-02-13 04:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05][root][INFO] - Training Epoch: 2/2, step 5357/7134 completed (loss: 0.04761485755443573, acc: 0.9843205809593201)
[2025-02-13 04:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05][root][INFO] - Training Epoch: 2/2, step 5358/7134 completed (loss: 0.03812843933701515, acc: 0.9903448224067688)
[2025-02-13 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:06][root][INFO] - Training Epoch: 2/2, step 5359/7134 completed (loss: 0.010769917629659176, acc: 0.997668981552124)
[2025-02-13 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:06][root][INFO] - Training Epoch: 2/2, step 5360/7134 completed (loss: 0.010023741982877254, acc: 0.9958791136741638)
[2025-02-13 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:07][root][INFO] - Training Epoch: 2/2, step 5361/7134 completed (loss: 0.03022339940071106, acc: 0.9904191493988037)
[2025-02-13 04:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:07][root][INFO] - Training Epoch: 2/2, step 5362/7134 completed (loss: 0.01831861212849617, acc: 0.9925373196601868)
[2025-02-13 04:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08][root][INFO] - Training Epoch: 2/2, step 5363/7134 completed (loss: 0.046376168727874756, acc: 0.9840116500854492)
[2025-02-13 04:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08][root][INFO] - Training Epoch: 2/2, step 5364/7134 completed (loss: 0.019742794334888458, acc: 0.9922077655792236)
[2025-02-13 04:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08][root][INFO] - Training Epoch: 2/2, step 5365/7134 completed (loss: 0.013003677129745483, acc: 0.9964726567268372)
[2025-02-13 04:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:09][root][INFO] - Training Epoch: 2/2, step 5366/7134 completed (loss: 0.050381358712911606, acc: 0.9826388955116272)
[2025-02-13 04:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:09][root][INFO] - Training Epoch: 2/2, step 5367/7134 completed (loss: 0.03162072226405144, acc: 0.9924623370170593)
[2025-02-13 04:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:10][root][INFO] - Training Epoch: 2/2, step 5368/7134 completed (loss: 0.030331548303365707, acc: 0.9866220951080322)
[2025-02-13 04:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:10][root][INFO] - Training Epoch: 2/2, step 5369/7134 completed (loss: 0.02043556049466133, acc: 0.993630588054657)
[2025-02-13 04:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11][root][INFO] - Training Epoch: 2/2, step 5370/7134 completed (loss: 0.052221689373254776, acc: 0.9890109896659851)
[2025-02-13 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11][root][INFO] - Training Epoch: 2/2, step 5371/7134 completed (loss: 0.07619793713092804, acc: 0.9817276000976562)
[2025-02-13 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11][root][INFO] - Training Epoch: 2/2, step 5372/7134 completed (loss: 0.007037673145532608, acc: 0.9985975027084351)
[2025-02-13 04:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:12][root][INFO] - Training Epoch: 2/2, step 5373/7134 completed (loss: 0.05994628369808197, acc: 0.9874476790428162)
[2025-02-13 04:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:12][root][INFO] - Training Epoch: 2/2, step 5374/7134 completed (loss: 0.0281869787722826, acc: 0.9935691356658936)
[2025-02-13 04:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13][root][INFO] - Training Epoch: 2/2, step 5375/7134 completed (loss: 0.03944580629467964, acc: 0.9865900278091431)
[2025-02-13 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13][root][INFO] - Training Epoch: 2/2, step 5376/7134 completed (loss: 0.021374130621552467, acc: 0.99303138256073)
[2025-02-13 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13][root][INFO] - Training Epoch: 2/2, step 5377/7134 completed (loss: 0.03386635705828667, acc: 0.9924127459526062)
[2025-02-13 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:14][root][INFO] - Training Epoch: 2/2, step 5378/7134 completed (loss: 0.01720944233238697, acc: 0.9939320683479309)
[2025-02-13 04:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:14][root][INFO] - Training Epoch: 2/2, step 5379/7134 completed (loss: 0.027879958972334862, acc: 0.9948119521141052)
[2025-02-13 04:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15][root][INFO] - Training Epoch: 2/2, step 5380/7134 completed (loss: 0.03293541446328163, acc: 0.9911816716194153)
[2025-02-13 04:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15][root][INFO] - Training Epoch: 2/2, step 5381/7134 completed (loss: 0.04866265133023262, acc: 0.9888424277305603)
[2025-02-13 04:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15][root][INFO] - Training Epoch: 2/2, step 5382/7134 completed (loss: 0.014932974241673946, acc: 0.9965870380401611)
[2025-02-13 04:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:16][root][INFO] - Training Epoch: 2/2, step 5383/7134 completed (loss: 0.017410660162568092, acc: 0.9943181872367859)
[2025-02-13 04:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:16][root][INFO] - Training Epoch: 2/2, step 5384/7134 completed (loss: 0.009219532832503319, acc: 0.9952606558799744)
[2025-02-13 04:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:17][root][INFO] - Training Epoch: 2/2, step 5385/7134 completed (loss: 0.021594516932964325, acc: 0.9920739531517029)
[2025-02-13 04:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:17][root][INFO] - Training Epoch: 2/2, step 5386/7134 completed (loss: 0.009932426735758781, acc: 0.9971910119056702)
[2025-02-13 04:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18][root][INFO] - Training Epoch: 2/2, step 5387/7134 completed (loss: 0.01358766295015812, acc: 0.9961904883384705)
[2025-02-13 04:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18][root][INFO] - Training Epoch: 2/2, step 5388/7134 completed (loss: 0.02002355456352234, acc: 0.9924242496490479)
[2025-02-13 04:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18][root][INFO] - Training Epoch: 2/2, step 5389/7134 completed (loss: 0.022592077031731606, acc: 0.995207667350769)
[2025-02-13 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:19][root][INFO] - Training Epoch: 2/2, step 5390/7134 completed (loss: 0.04137727618217468, acc: 0.9904912710189819)
[2025-02-13 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:19][root][INFO] - Training Epoch: 2/2, step 5391/7134 completed (loss: 0.05310339480638504, acc: 0.9862778782844543)
[2025-02-13 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20][root][INFO] - Training Epoch: 2/2, step 5392/7134 completed (loss: 0.04621174558997154, acc: 0.9847561120986938)
[2025-02-13 04:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20][root][INFO] - Training Epoch: 2/2, step 5393/7134 completed (loss: 0.017362793907523155, acc: 0.9921875)
[2025-02-13 04:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20][root][INFO] - Training Epoch: 2/2, step 5394/7134 completed (loss: 0.03541036695241928, acc: 0.988095223903656)
[2025-02-13 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:21][root][INFO] - Training Epoch: 2/2, step 5395/7134 completed (loss: 0.018841248005628586, acc: 0.9924585223197937)
[2025-02-13 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:21][root][INFO] - Training Epoch: 2/2, step 5396/7134 completed (loss: 0.036183685064315796, acc: 0.9894737005233765)
[2025-02-13 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:22][root][INFO] - Training Epoch: 2/2, step 5397/7134 completed (loss: 0.017255499958992004, acc: 0.9932318329811096)
[2025-02-13 04:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:22][root][INFO] - Training Epoch: 2/2, step 5398/7134 completed (loss: 0.02379555068910122, acc: 0.99262535572052)
[2025-02-13 04:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23][root][INFO] - Training Epoch: 2/2, step 5399/7134 completed (loss: 0.022597825154662132, acc: 0.9904240965843201)
[2025-02-13 04:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23][root][INFO] - Training Epoch: 2/2, step 5400/7134 completed (loss: 0.09862761944532394, acc: 0.9749216437339783)
[2025-02-13 04:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23][root][INFO] - Training Epoch: 2/2, step 5401/7134 completed (loss: 0.026009656488895416, acc: 0.995184600353241)
[2025-02-13 04:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:24][root][INFO] - Training Epoch: 2/2, step 5402/7134 completed (loss: 0.015340332873165607, acc: 0.9955947399139404)
[2025-02-13 04:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:24][root][INFO] - Training Epoch: 2/2, step 5403/7134 completed (loss: 0.04648207128047943, acc: 0.9918434023857117)
[2025-02-13 04:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25][root][INFO] - Training Epoch: 2/2, step 5404/7134 completed (loss: 0.060578010976314545, acc: 0.9838926196098328)
[2025-02-13 04:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25][root][INFO] - Training Epoch: 2/2, step 5405/7134 completed (loss: 0.018200842663645744, acc: 0.9947643876075745)
[2025-02-13 04:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25][root][INFO] - Training Epoch: 2/2, step 5406/7134 completed (loss: 0.014637453481554985, acc: 0.9961013793945312)
[2025-02-13 04:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:26][root][INFO] - Training Epoch: 2/2, step 5407/7134 completed (loss: 0.019113104790449142, acc: 0.9951691031455994)
[2025-02-13 04:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:26][root][INFO] - Training Epoch: 2/2, step 5408/7134 completed (loss: 0.05022495985031128, acc: 0.9875195026397705)
[2025-02-13 04:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:27][root][INFO] - Training Epoch: 2/2, step 5409/7134 completed (loss: 0.08994224667549133, acc: 0.9863842725753784)
[2025-02-13 04:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:27][root][INFO] - Training Epoch: 2/2, step 5410/7134 completed (loss: 0.040857356041669846, acc: 0.9868766665458679)
[2025-02-13 04:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28][root][INFO] - Training Epoch: 2/2, step 5411/7134 completed (loss: 0.023709731176495552, acc: 0.9956458806991577)
[2025-02-13 04:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28][root][INFO] - Training Epoch: 2/2, step 5412/7134 completed (loss: 0.06530018150806427, acc: 0.9884615540504456)
[2025-02-13 04:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28][root][INFO] - Training Epoch: 2/2, step 5413/7134 completed (loss: 0.061060965061187744, acc: 0.9785714149475098)
[2025-02-13 04:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:29][root][INFO] - Training Epoch: 2/2, step 5414/7134 completed (loss: 0.0821111872792244, acc: 0.9794344305992126)
[2025-02-13 04:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:29][root][INFO] - Training Epoch: 2/2, step 5415/7134 completed (loss: 0.04606292396783829, acc: 0.9856733679771423)
[2025-02-13 04:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:30][root][INFO] - Training Epoch: 2/2, step 5416/7134 completed (loss: 0.05214393511414528, acc: 0.9802555441856384)
[2025-02-13 04:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:30][root][INFO] - Training Epoch: 2/2, step 5417/7134 completed (loss: 0.042656052857637405, acc: 0.9912609457969666)
[2025-02-13 04:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31][root][INFO] - Training Epoch: 2/2, step 5418/7134 completed (loss: 0.03142224997282028, acc: 0.9881831407546997)
[2025-02-13 04:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31][root][INFO] - Training Epoch: 2/2, step 5419/7134 completed (loss: 0.08239823579788208, acc: 0.9762712121009827)
[2025-02-13 04:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31][root][INFO] - Training Epoch: 2/2, step 5420/7134 completed (loss: 0.0626058429479599, acc: 0.9825174808502197)
[2025-02-13 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:32][root][INFO] - Training Epoch: 2/2, step 5421/7134 completed (loss: 0.09193583577871323, acc: 0.9765990376472473)
[2025-02-13 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:32][root][INFO] - Training Epoch: 2/2, step 5422/7134 completed (loss: 0.034497469663619995, acc: 0.9911242723464966)
[2025-02-13 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33][root][INFO] - Training Epoch: 2/2, step 5423/7134 completed (loss: 0.06523510068655014, acc: 0.9824047088623047)
[2025-02-13 04:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33][root][INFO] - Training Epoch: 2/2, step 5424/7134 completed (loss: 0.026978228241205215, acc: 0.9894366264343262)
[2025-02-13 04:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33][root][INFO] - Training Epoch: 2/2, step 5425/7134 completed (loss: 0.02403401769697666, acc: 0.9941747784614563)
[2025-02-13 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:34][root][INFO] - Training Epoch: 2/2, step 5426/7134 completed (loss: 0.04951644688844681, acc: 0.9879518151283264)
[2025-02-13 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:34][root][INFO] - Training Epoch: 2/2, step 5427/7134 completed (loss: 0.027322540059685707, acc: 0.9905213117599487)
[2025-02-13 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35][root][INFO] - Training Epoch: 2/2, step 5428/7134 completed (loss: 0.03424244746565819, acc: 0.991482138633728)
[2025-02-13 04:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35][root][INFO] - Training Epoch: 2/2, step 5429/7134 completed (loss: 0.057102251797914505, acc: 0.9855595827102661)
[2025-02-13 04:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35][root][INFO] - Training Epoch: 2/2, step 5430/7134 completed (loss: 0.024643227458000183, acc: 0.9937304258346558)
[2025-02-13 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:36][root][INFO] - Training Epoch: 2/2, step 5431/7134 completed (loss: 0.022810587659478188, acc: 0.9943289160728455)
[2025-02-13 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:36][root][INFO] - Training Epoch: 2/2, step 5432/7134 completed (loss: 0.01696380227804184, acc: 0.9962335228919983)
[2025-02-13 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37][root][INFO] - Training Epoch: 2/2, step 5433/7134 completed (loss: 0.04436248540878296, acc: 0.991919219493866)
[2025-02-13 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37][root][INFO] - Training Epoch: 2/2, step 5434/7134 completed (loss: 0.02504807896912098, acc: 0.9939637780189514)
[2025-02-13 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37][root][INFO] - Training Epoch: 2/2, step 5435/7134 completed (loss: 0.02272310107946396, acc: 0.9937238693237305)
[2025-02-13 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38][root][INFO] - Training Epoch: 2/2, step 5436/7134 completed (loss: 0.028493162244558334, acc: 0.9925816059112549)
[2025-02-13 04:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38][root][INFO] - Training Epoch: 2/2, step 5437/7134 completed (loss: 0.019650066271424294, acc: 0.9965096116065979)
[2025-02-13 04:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39][root][INFO] - Training Epoch: 2/2, step 5438/7134 completed (loss: 0.02429273910820484, acc: 0.9922480583190918)
[2025-02-13 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39][root][INFO] - Training Epoch: 2/2, step 5439/7134 completed (loss: 0.03788048401474953, acc: 0.9840116500854492)
[2025-02-13 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39][root][INFO] - Training Epoch: 2/2, step 5440/7134 completed (loss: 0.039737097918987274, acc: 0.9910600185394287)
[2025-02-13 04:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:40][root][INFO] - Training Epoch: 2/2, step 5441/7134 completed (loss: 0.03561409190297127, acc: 0.9914320707321167)
[2025-02-13 04:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:40][root][INFO] - Training Epoch: 2/2, step 5442/7134 completed (loss: 0.019635360687971115, acc: 0.9959404468536377)
[2025-02-13 04:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41][root][INFO] - Training Epoch: 2/2, step 5443/7134 completed (loss: 0.030607672408223152, acc: 0.9900568127632141)
[2025-02-13 04:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41][root][INFO] - Training Epoch: 2/2, step 5444/7134 completed (loss: 0.025683937594294548, acc: 0.989051103591919)
[2025-02-13 04:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41][root][INFO] - Training Epoch: 2/2, step 5445/7134 completed (loss: 0.027949223294854164, acc: 0.9908592104911804)
[2025-02-13 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:42][root][INFO] - Training Epoch: 2/2, step 5446/7134 completed (loss: 0.022436026483774185, acc: 0.9957982897758484)
[2025-02-13 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:42][root][INFO] - Training Epoch: 2/2, step 5447/7134 completed (loss: 0.024680335074663162, acc: 0.9919484853744507)
[2025-02-13 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:43][root][INFO] - Training Epoch: 2/2, step 5448/7134 completed (loss: 0.03663177415728569, acc: 0.9846368432044983)
[2025-02-13 04:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:43][root][INFO] - Training Epoch: 2/2, step 5449/7134 completed (loss: 0.01584162376821041, acc: 0.9964912533760071)
[2025-02-13 04:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44][root][INFO] - Training Epoch: 2/2, step 5450/7134 completed (loss: 0.012271113693714142, acc: 0.9961389899253845)
[2025-02-13 04:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44][root][INFO] - Training Epoch: 2/2, step 5451/7134 completed (loss: 0.031137103214859962, acc: 0.9908257126808167)
[2025-02-13 04:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44][root][INFO] - Training Epoch: 2/2, step 5452/7134 completed (loss: 0.05272822454571724, acc: 0.9896640777587891)
[2025-02-13 04:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:45][root][INFO] - Training Epoch: 2/2, step 5453/7134 completed (loss: 0.042909637093544006, acc: 0.9926380515098572)
[2025-02-13 04:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:45][root][INFO] - Training Epoch: 2/2, step 5454/7134 completed (loss: 0.022295212373137474, acc: 0.9921773076057434)
[2025-02-13 04:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46][root][INFO] - Training Epoch: 2/2, step 5455/7134 completed (loss: 0.07627430558204651, acc: 0.9797979593276978)
[2025-02-13 04:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46][root][INFO] - Training Epoch: 2/2, step 5456/7134 completed (loss: 0.03594471886754036, acc: 0.989924430847168)
[2025-02-13 04:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47][root][INFO] - Training Epoch: 2/2, step 5457/7134 completed (loss: 0.0469539500772953, acc: 0.990604043006897)
[2025-02-13 04:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47][root][INFO] - Training Epoch: 2/2, step 5458/7134 completed (loss: 0.02396535873413086, acc: 0.9910827875137329)
[2025-02-13 04:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47][root][INFO] - Training Epoch: 2/2, step 5459/7134 completed (loss: 0.007152874954044819, acc: 0.9985097050666809)
[2025-02-13 04:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:48][root][INFO] - Training Epoch: 2/2, step 5460/7134 completed (loss: 0.023888984695076942, acc: 0.9944367408752441)
[2025-02-13 04:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:48][root][INFO] - Training Epoch: 2/2, step 5461/7134 completed (loss: 0.03183748945593834, acc: 0.9911699891090393)
[2025-02-13 04:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:49][root][INFO] - Training Epoch: 2/2, step 5462/7134 completed (loss: 0.0069425348192453384, acc: 0.9985693693161011)
[2025-02-13 04:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:49][root][INFO] - Training Epoch: 2/2, step 5463/7134 completed (loss: 0.02497166022658348, acc: 0.9923664331436157)
[2025-02-13 04:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50][root][INFO] - Training Epoch: 2/2, step 5464/7134 completed (loss: 0.028646228834986687, acc: 0.9894319772720337)
[2025-02-13 04:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50][root][INFO] - Training Epoch: 2/2, step 5465/7134 completed (loss: 0.0403413288295269, acc: 0.9925261735916138)
[2025-02-13 04:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50][root][INFO] - Training Epoch: 2/2, step 5466/7134 completed (loss: 0.016729064285755157, acc: 0.9953970313072205)
[2025-02-13 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:51][root][INFO] - Training Epoch: 2/2, step 5467/7134 completed (loss: 0.014498820528388023, acc: 0.9953051805496216)
[2025-02-13 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:51][root][INFO] - Training Epoch: 2/2, step 5468/7134 completed (loss: 0.027820654213428497, acc: 0.9918793439865112)
[2025-02-13 04:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:52][root][INFO] - Training Epoch: 2/2, step 5469/7134 completed (loss: 0.016037296503782272, acc: 0.9950920343399048)
[2025-02-13 04:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:52][root][INFO] - Training Epoch: 2/2, step 5470/7134 completed (loss: 0.005407707300037146, acc: 1.0)
[2025-02-13 04:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:53][root][INFO] - Training Epoch: 2/2, step 5471/7134 completed (loss: 0.01961115188896656, acc: 0.9945945739746094)
[2025-02-13 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:53][root][INFO] - Training Epoch: 2/2, step 5472/7134 completed (loss: 0.020031506195664406, acc: 0.996259331703186)
[2025-02-13 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54][root][INFO] - Training Epoch: 2/2, step 5473/7134 completed (loss: 0.011686992831528187, acc: 0.9963189959526062)
[2025-02-13 04:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54][root][INFO] - Training Epoch: 2/2, step 5474/7134 completed (loss: 0.02315532974898815, acc: 0.9928571581840515)
[2025-02-13 04:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54][root][INFO] - Training Epoch: 2/2, step 5475/7134 completed (loss: 0.014611871913075447, acc: 0.9957325458526611)
[2025-02-13 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:55][root][INFO] - Training Epoch: 2/2, step 5476/7134 completed (loss: 0.0094226049259305, acc: 0.9971056580543518)
[2025-02-13 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:55][root][INFO] - Training Epoch: 2/2, step 5477/7134 completed (loss: 0.01659337431192398, acc: 0.9975369572639465)
[2025-02-13 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:56][root][INFO] - Training Epoch: 2/2, step 5478/7134 completed (loss: 0.04987721890211105, acc: 0.9862778782844543)
[2025-02-13 04:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:56][root][INFO] - Training Epoch: 2/2, step 5479/7134 completed (loss: 0.0373033806681633, acc: 0.9850543737411499)
[2025-02-13 04:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:57][root][INFO] - Training Epoch: 2/2, step 5480/7134 completed (loss: 0.03859531134366989, acc: 0.9894737005233765)
[2025-02-13 04:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:57][root][INFO] - Training Epoch: 2/2, step 5481/7134 completed (loss: 0.021740010008215904, acc: 0.9898989796638489)
[2025-02-13 04:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:58][root][INFO] - Training Epoch: 2/2, step 5482/7134 completed (loss: 0.036500487476587296, acc: 0.9891172647476196)
[2025-02-13 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:58][root][INFO] - Training Epoch: 2/2, step 5483/7134 completed (loss: 0.03843831643462181, acc: 0.9860724210739136)
[2025-02-13 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:59][root][INFO] - Training Epoch: 2/2, step 5484/7134 completed (loss: 0.045210275799036026, acc: 0.9833101630210876)
[2025-02-13 04:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:59][root][INFO] - Training Epoch: 2/2, step 5485/7134 completed (loss: 0.03849918022751808, acc: 0.9891107082366943)
[2025-02-13 04:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00][root][INFO] - Training Epoch: 2/2, step 5486/7134 completed (loss: 0.0484532006084919, acc: 0.9861591458320618)
[2025-02-13 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00][root][INFO] - Training Epoch: 2/2, step 5487/7134 completed (loss: 0.019039498642086983, acc: 0.9934959411621094)
[2025-02-13 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00][root][INFO] - Training Epoch: 2/2, step 5488/7134 completed (loss: 0.050223611295223236, acc: 0.9855263233184814)
[2025-02-13 04:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:01][root][INFO] - Training Epoch: 2/2, step 5489/7134 completed (loss: 0.03164149448275566, acc: 0.9938080310821533)
[2025-02-13 04:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:01][root][INFO] - Training Epoch: 2/2, step 5490/7134 completed (loss: 0.022914914414286613, acc: 0.9908883571624756)
[2025-02-13 04:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02][root][INFO] - Training Epoch: 2/2, step 5491/7134 completed (loss: 0.021150516346096992, acc: 0.992668628692627)
[2025-02-13 04:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02][root][INFO] - Training Epoch: 2/2, step 5492/7134 completed (loss: 0.03141328692436218, acc: 0.9915397763252258)
[2025-02-13 04:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02][root][INFO] - Training Epoch: 2/2, step 5493/7134 completed (loss: 0.03335697576403618, acc: 0.9864130616188049)
[2025-02-13 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03][root][INFO] - Training Epoch: 2/2, step 5494/7134 completed (loss: 0.01726621575653553, acc: 0.9928876161575317)
[2025-02-13 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03][root][INFO] - Training Epoch: 2/2, step 5495/7134 completed (loss: 0.020006529986858368, acc: 0.9964157938957214)
[2025-02-13 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03][root][INFO] - Training Epoch: 2/2, step 5496/7134 completed (loss: 0.04136138781905174, acc: 0.9883268475532532)
[2025-02-13 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:04][root][INFO] - Training Epoch: 2/2, step 5497/7134 completed (loss: 0.05997898802161217, acc: 0.980966329574585)
[2025-02-13 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:04][root][INFO] - Training Epoch: 2/2, step 5498/7134 completed (loss: 0.020128324627876282, acc: 0.9907161593437195)
[2025-02-13 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:05][root][INFO] - Training Epoch: 2/2, step 5499/7134 completed (loss: 0.02937476895749569, acc: 0.9899497628211975)
[2025-02-13 04:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:05][root][INFO] - Training Epoch: 2/2, step 5500/7134 completed (loss: 0.029971066862344742, acc: 0.9918144345283508)
[2025-02-13 04:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06][root][INFO] - Training Epoch: 2/2, step 5501/7134 completed (loss: 0.01606723852455616, acc: 0.9950617551803589)
[2025-02-13 04:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06][root][INFO] - Training Epoch: 2/2, step 5502/7134 completed (loss: 0.03186999633908272, acc: 0.9838056564331055)
[2025-02-13 04:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06][root][INFO] - Training Epoch: 2/2, step 5503/7134 completed (loss: 0.03769364953041077, acc: 0.9880715608596802)
[2025-02-13 04:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:07][root][INFO] - Training Epoch: 2/2, step 5504/7134 completed (loss: 0.016156017780303955, acc: 0.9951573610305786)
[2025-02-13 04:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:07][root][INFO] - Training Epoch: 2/2, step 5505/7134 completed (loss: 0.02842569909989834, acc: 0.9843971729278564)
[2025-02-13 04:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:08][root][INFO] - Training Epoch: 2/2, step 5506/7134 completed (loss: 0.02713906578719616, acc: 0.9896193742752075)
[2025-02-13 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:08][root][INFO] - Training Epoch: 2/2, step 5507/7134 completed (loss: 0.036756481975317, acc: 0.9845938086509705)
[2025-02-13 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09][root][INFO] - Training Epoch: 2/2, step 5508/7134 completed (loss: 0.017310569062829018, acc: 0.9941588640213013)
[2025-02-13 04:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09][root][INFO] - Training Epoch: 2/2, step 5509/7134 completed (loss: 0.027653768658638, acc: 0.9941520690917969)
[2025-02-13 04:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09][root][INFO] - Training Epoch: 2/2, step 5510/7134 completed (loss: 0.03858558461070061, acc: 0.9871465563774109)
[2025-02-13 04:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10][root][INFO] - Training Epoch: 2/2, step 5511/7134 completed (loss: 0.040443673729896545, acc: 0.9886877536773682)
[2025-02-13 04:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10][root][INFO] - Training Epoch: 2/2, step 5512/7134 completed (loss: 0.010413000360131264, acc: 0.9947299361228943)
[2025-02-13 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:11][root][INFO] - Training Epoch: 2/2, step 5513/7134 completed (loss: 0.028974177315831184, acc: 0.99048912525177)
[2025-02-13 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:11][root][INFO] - Training Epoch: 2/2, step 5514/7134 completed (loss: 0.09135778993368149, acc: 0.9763681888580322)
[2025-02-13 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12][root][INFO] - Training Epoch: 2/2, step 5515/7134 completed (loss: 0.032075993716716766, acc: 0.9871612191200256)
[2025-02-13 04:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12][root][INFO] - Training Epoch: 2/2, step 5516/7134 completed (loss: 0.027709048241376877, acc: 0.9929676651954651)
[2025-02-13 04:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12][root][INFO] - Training Epoch: 2/2, step 5517/7134 completed (loss: 0.03196254000067711, acc: 0.9906914830207825)
[2025-02-13 04:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:13][root][INFO] - Training Epoch: 2/2, step 5518/7134 completed (loss: 0.03307630494236946, acc: 0.989130437374115)
[2025-02-13 04:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:13][root][INFO] - Training Epoch: 2/2, step 5519/7134 completed (loss: 0.008945551700890064, acc: 0.9972413778305054)
[2025-02-13 04:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:14][root][INFO] - Training Epoch: 2/2, step 5520/7134 completed (loss: 0.019598007202148438, acc: 0.9932050108909607)
[2025-02-13 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:14][root][INFO] - Training Epoch: 2/2, step 5521/7134 completed (loss: 0.015273131430149078, acc: 0.9937185645103455)
[2025-02-13 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15][root][INFO] - Training Epoch: 2/2, step 5522/7134 completed (loss: 0.03760417923331261, acc: 0.98562091588974)
[2025-02-13 04:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15][root][INFO] - Training Epoch: 2/2, step 5523/7134 completed (loss: 0.04069659858942032, acc: 0.992977499961853)
[2025-02-13 04:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15][root][INFO] - Training Epoch: 2/2, step 5524/7134 completed (loss: 0.03764038532972336, acc: 0.9876161217689514)
[2025-02-13 04:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:16][root][INFO] - Training Epoch: 2/2, step 5525/7134 completed (loss: 0.03558250144124031, acc: 0.9882869720458984)
[2025-02-13 04:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:16][root][INFO] - Training Epoch: 2/2, step 5526/7134 completed (loss: 0.028411777690052986, acc: 0.9926362037658691)
[2025-02-13 04:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:17][root][INFO] - Training Epoch: 2/2, step 5527/7134 completed (loss: 0.024336373433470726, acc: 0.9915966391563416)
[2025-02-13 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:17][root][INFO] - Training Epoch: 2/2, step 5528/7134 completed (loss: 0.027425851672887802, acc: 0.9928994178771973)
[2025-02-13 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18][root][INFO] - Training Epoch: 2/2, step 5529/7134 completed (loss: 0.05173391103744507, acc: 0.9858155846595764)
[2025-02-13 04:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18][root][INFO] - Training Epoch: 2/2, step 5530/7134 completed (loss: 0.018227137625217438, acc: 0.9940387606620789)
[2025-02-13 04:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18][root][INFO] - Training Epoch: 2/2, step 5531/7134 completed (loss: 0.03474031388759613, acc: 0.9925280213356018)
[2025-02-13 04:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:19][root][INFO] - Training Epoch: 2/2, step 5532/7134 completed (loss: 0.060139626264572144, acc: 0.982694685459137)
[2025-02-13 04:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:19][root][INFO] - Training Epoch: 2/2, step 5533/7134 completed (loss: 0.010791026055812836, acc: 0.9965986609458923)
[2025-02-13 04:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:20][root][INFO] - Training Epoch: 2/2, step 5534/7134 completed (loss: 0.035678569227457047, acc: 0.9894859790802002)
[2025-02-13 04:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:20][root][INFO] - Training Epoch: 2/2, step 5535/7134 completed (loss: 0.025980373844504356, acc: 0.9902067184448242)
[2025-02-13 04:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:21][root][INFO] - Training Epoch: 2/2, step 5536/7134 completed (loss: 0.027045561000704765, acc: 0.9918604493141174)
[2025-02-13 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:21][root][INFO] - Training Epoch: 2/2, step 5537/7134 completed (loss: 0.02957688271999359, acc: 0.9894982576370239)
[2025-02-13 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22][root][INFO] - Training Epoch: 2/2, step 5538/7134 completed (loss: 0.02129773609340191, acc: 0.9965870380401611)
[2025-02-13 04:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22][root][INFO] - Training Epoch: 2/2, step 5539/7134 completed (loss: 0.02712906152009964, acc: 0.9932432174682617)
[2025-02-13 04:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22][root][INFO] - Training Epoch: 2/2, step 5540/7134 completed (loss: 0.044588856399059296, acc: 0.9894737005233765)
[2025-02-13 04:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:23][root][INFO] - Training Epoch: 2/2, step 5541/7134 completed (loss: 0.02236744947731495, acc: 0.9933510422706604)
[2025-02-13 04:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:23][root][INFO] - Training Epoch: 2/2, step 5542/7134 completed (loss: 0.007789555471390486, acc: 0.9971469044685364)
[2025-02-13 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:24][root][INFO] - Training Epoch: 2/2, step 5543/7134 completed (loss: 0.026965143159031868, acc: 0.9944567680358887)
[2025-02-13 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:24][root][INFO] - Training Epoch: 2/2, step 5544/7134 completed (loss: 0.013062105514109135, acc: 0.9967741966247559)
[2025-02-13 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:25][root][INFO] - Training Epoch: 2/2, step 5545/7134 completed (loss: 0.03482416644692421, acc: 0.9890710115432739)
[2025-02-13 04:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:25][root][INFO] - Training Epoch: 2/2, step 5546/7134 completed (loss: 0.028792526572942734, acc: 0.9900332093238831)
[2025-02-13 04:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26][root][INFO] - Training Epoch: 2/2, step 5547/7134 completed (loss: 0.020204728469252586, acc: 0.9907833933830261)
[2025-02-13 04:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26][root][INFO] - Training Epoch: 2/2, step 5548/7134 completed (loss: 0.017790086567401886, acc: 0.9953488111495972)
[2025-02-13 04:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26][root][INFO] - Training Epoch: 2/2, step 5549/7134 completed (loss: 0.018138999119400978, acc: 0.9942857027053833)
[2025-02-13 04:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:27][root][INFO] - Training Epoch: 2/2, step 5550/7134 completed (loss: 0.011051409877836704, acc: 0.9952996373176575)
[2025-02-13 04:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:27][root][INFO] - Training Epoch: 2/2, step 5551/7134 completed (loss: 0.04640386253595352, acc: 0.9851149916648865)
[2025-02-13 04:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:28][root][INFO] - Training Epoch: 2/2, step 5552/7134 completed (loss: 0.008327736519277096, acc: 0.9964328408241272)
[2025-02-13 04:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:28][root][INFO] - Training Epoch: 2/2, step 5553/7134 completed (loss: 0.03094683587551117, acc: 0.993819534778595)
[2025-02-13 04:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29][root][INFO] - Training Epoch: 2/2, step 5554/7134 completed (loss: 0.010657521896064281, acc: 0.9963325262069702)
[2025-02-13 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29][root][INFO] - Training Epoch: 2/2, step 5555/7134 completed (loss: 0.025362158194184303, acc: 0.990641713142395)
[2025-02-13 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29][root][INFO] - Training Epoch: 2/2, step 5556/7134 completed (loss: 0.02830657549202442, acc: 0.990920901298523)
[2025-02-13 04:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:30][root][INFO] - Training Epoch: 2/2, step 5557/7134 completed (loss: 0.019156597554683685, acc: 0.9971988797187805)
[2025-02-13 04:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:30][root][INFO] - Training Epoch: 2/2, step 5558/7134 completed (loss: 0.014503837563097477, acc: 0.9950310587882996)
[2025-02-13 04:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31][root][INFO] - Training Epoch: 2/2, step 5559/7134 completed (loss: 0.02833073027431965, acc: 0.9950310587882996)
[2025-02-13 04:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31][root][INFO] - Training Epoch: 2/2, step 5560/7134 completed (loss: 0.042919598519802094, acc: 0.9909326434135437)
[2025-02-13 04:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32][root][INFO] - Training Epoch: 2/2, step 5561/7134 completed (loss: 0.030757831409573555, acc: 0.9943289160728455)
[2025-02-13 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32][root][INFO] - Training Epoch: 2/2, step 5562/7134 completed (loss: 0.036814264953136444, acc: 0.9948805570602417)
[2025-02-13 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32][root][INFO] - Training Epoch: 2/2, step 5563/7134 completed (loss: 0.031248262152075768, acc: 0.9924924969673157)
[2025-02-13 04:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:33][root][INFO] - Training Epoch: 2/2, step 5564/7134 completed (loss: 0.019799193367362022, acc: 0.9972106218338013)
[2025-02-13 04:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:33][root][INFO] - Training Epoch: 2/2, step 5565/7134 completed (loss: 0.005428014788776636, acc: 0.9984639286994934)
[2025-02-13 04:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:34][root][INFO] - Training Epoch: 2/2, step 5566/7134 completed (loss: 0.012474487535655499, acc: 0.9977037906646729)
[2025-02-13 04:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:34][root][INFO] - Training Epoch: 2/2, step 5567/7134 completed (loss: 0.017417892813682556, acc: 0.9920091032981873)
[2025-02-13 04:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35][root][INFO] - Training Epoch: 2/2, step 5568/7134 completed (loss: 0.01911275088787079, acc: 0.9961340427398682)
[2025-02-13 04:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35][root][INFO] - Training Epoch: 2/2, step 5569/7134 completed (loss: 0.007453080266714096, acc: 0.9977169036865234)
[2025-02-13 04:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35][root][INFO] - Training Epoch: 2/2, step 5570/7134 completed (loss: 0.031924448907375336, acc: 0.991304337978363)
[2025-02-13 04:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:36][root][INFO] - Training Epoch: 2/2, step 5571/7134 completed (loss: 0.023163730278611183, acc: 0.9940564632415771)
[2025-02-13 04:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:36][root][INFO] - Training Epoch: 2/2, step 5572/7134 completed (loss: 0.03771332651376724, acc: 0.9931318759918213)
[2025-02-13 04:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:37][root][INFO] - Training Epoch: 2/2, step 5573/7134 completed (loss: 0.026765698567032814, acc: 0.9927219748497009)
[2025-02-13 04:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:37][root][INFO] - Training Epoch: 2/2, step 5574/7134 completed (loss: 0.005346922669559717, acc: 0.9984126687049866)
[2025-02-13 04:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38][root][INFO] - Training Epoch: 2/2, step 5575/7134 completed (loss: 0.01744646392762661, acc: 0.9952152967453003)
[2025-02-13 04:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38][root][INFO] - Training Epoch: 2/2, step 5576/7134 completed (loss: 0.028448566794395447, acc: 0.9914039969444275)
[2025-02-13 04:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38][root][INFO] - Training Epoch: 2/2, step 5577/7134 completed (loss: 0.034993819892406464, acc: 0.9912739992141724)
[2025-02-13 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:39][root][INFO] - Training Epoch: 2/2, step 5578/7134 completed (loss: 0.014618382789194584, acc: 0.9942611455917358)
[2025-02-13 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:39][root][INFO] - Training Epoch: 2/2, step 5579/7134 completed (loss: 0.023961693048477173, acc: 0.9875518679618835)
[2025-02-13 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:40][root][INFO] - Training Epoch: 2/2, step 5580/7134 completed (loss: 0.009255013428628445, acc: 0.9986357688903809)
[2025-02-13 04:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:40][root][INFO] - Training Epoch: 2/2, step 5581/7134 completed (loss: 0.013721671886742115, acc: 0.9933510422706604)
[2025-02-13 04:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41][root][INFO] - Training Epoch: 2/2, step 5582/7134 completed (loss: 0.014798654243350029, acc: 0.9947848916053772)
[2025-02-13 04:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41][root][INFO] - Training Epoch: 2/2, step 5583/7134 completed (loss: 0.021541181951761246, acc: 0.99370276927948)
[2025-02-13 04:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41][root][INFO] - Training Epoch: 2/2, step 5584/7134 completed (loss: 0.01883392594754696, acc: 0.9955621361732483)
[2025-02-13 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:42][root][INFO] - Training Epoch: 2/2, step 5585/7134 completed (loss: 0.047838397324085236, acc: 0.9890909194946289)
[2025-02-13 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:42][root][INFO] - Training Epoch: 2/2, step 5586/7134 completed (loss: 0.015065276063978672, acc: 0.9946714043617249)
[2025-02-13 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:43][root][INFO] - Training Epoch: 2/2, step 5587/7134 completed (loss: 0.00933439563959837, acc: 0.9972260594367981)
[2025-02-13 04:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:43][root][INFO] - Training Epoch: 2/2, step 5588/7134 completed (loss: 0.009214550256729126, acc: 0.998487114906311)
[2025-02-13 04:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44][root][INFO] - Training Epoch: 2/2, step 5589/7134 completed (loss: 0.02538297511637211, acc: 0.9968454241752625)
[2025-02-13 04:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44][root][INFO] - Training Epoch: 2/2, step 5590/7134 completed (loss: 0.020402194932103157, acc: 0.9929577708244324)
[2025-02-13 04:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44][root][INFO] - Training Epoch: 2/2, step 5591/7134 completed (loss: 0.010537873022258282, acc: 0.9973822236061096)
[2025-02-13 04:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:45][root][INFO] - Training Epoch: 2/2, step 5592/7134 completed (loss: 0.012929136864840984, acc: 0.9972565174102783)
[2025-02-13 04:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:45][root][INFO] - Training Epoch: 2/2, step 5593/7134 completed (loss: 0.016820289194583893, acc: 0.9935232996940613)
[2025-02-13 04:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46][root][INFO] - Training Epoch: 2/2, step 5594/7134 completed (loss: 0.007151127327233553, acc: 0.9986522793769836)
[2025-02-13 04:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46][root][INFO] - Training Epoch: 2/2, step 5595/7134 completed (loss: 0.026722906157374382, acc: 0.9919571280479431)
[2025-02-13 04:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46][root][INFO] - Training Epoch: 2/2, step 5596/7134 completed (loss: 0.00872121937572956, acc: 0.9959677457809448)
[2025-02-13 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:47][root][INFO] - Training Epoch: 2/2, step 5597/7134 completed (loss: 0.03494721278548241, acc: 0.9889807105064392)
[2025-02-13 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:47][root][INFO] - Training Epoch: 2/2, step 5598/7134 completed (loss: 0.01856471598148346, acc: 0.994413435459137)
[2025-02-13 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:48][root][INFO] - Training Epoch: 2/2, step 5599/7134 completed (loss: 0.05698035657405853, acc: 0.9877049326896667)
[2025-02-13 04:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:48][root][INFO] - Training Epoch: 2/2, step 5600/7134 completed (loss: 0.01574726589024067, acc: 0.9957020282745361)
[2025-02-13 04:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49][root][INFO] - Training Epoch: 2/2, step 5601/7134 completed (loss: 0.0236924197524786, acc: 0.991304337978363)
[2025-02-13 04:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49][root][INFO] - Training Epoch: 2/2, step 5602/7134 completed (loss: 0.0368901789188385, acc: 0.9855072498321533)
[2025-02-13 04:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49][root][INFO] - Training Epoch: 2/2, step 5603/7134 completed (loss: 0.006826780270785093, acc: 1.0)
[2025-02-13 04:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50][root][INFO] - Training Epoch: 2/2, step 5604/7134 completed (loss: 0.041988879442214966, acc: 0.9823151230812073)
[2025-02-13 04:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50][root][INFO] - Training Epoch: 2/2, step 5605/7134 completed (loss: 0.025230592116713524, acc: 0.9932088255882263)
[2025-02-13 04:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50][root][INFO] - Training Epoch: 2/2, step 5606/7134 completed (loss: 0.03500780463218689, acc: 0.9871794581413269)
[2025-02-13 04:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:51][root][INFO] - Training Epoch: 2/2, step 5607/7134 completed (loss: 0.016593074426054955, acc: 0.995488703250885)
[2025-02-13 04:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:51][root][INFO] - Training Epoch: 2/2, step 5608/7134 completed (loss: 0.02162807621061802, acc: 0.9934318661689758)
[2025-02-13 04:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52][root][INFO] - Training Epoch: 2/2, step 5609/7134 completed (loss: 0.037468135356903076, acc: 0.9895651936531067)
[2025-02-13 04:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52][root][INFO] - Training Epoch: 2/2, step 5610/7134 completed (loss: 0.016578735783696175, acc: 0.9939637780189514)
[2025-02-13 04:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52][root][INFO] - Training Epoch: 2/2, step 5611/7134 completed (loss: 0.022977041080594063, acc: 0.9936708807945251)
[2025-02-13 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:53][root][INFO] - Training Epoch: 2/2, step 5612/7134 completed (loss: 0.03265887498855591, acc: 0.9907975196838379)
[2025-02-13 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:53][root][INFO] - Training Epoch: 2/2, step 5613/7134 completed (loss: 0.034009505063295364, acc: 0.991055428981781)
[2025-02-13 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:54][root][INFO] - Training Epoch: 2/2, step 5614/7134 completed (loss: 0.04494516924023628, acc: 0.9871428608894348)
[2025-02-13 04:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:54][root][INFO] - Training Epoch: 2/2, step 5615/7134 completed (loss: 0.03319967910647392, acc: 0.982758641242981)
[2025-02-13 04:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55][root][INFO] - Training Epoch: 2/2, step 5616/7134 completed (loss: 0.012417996302247047, acc: 0.9970282316207886)
[2025-02-13 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55][root][INFO] - Training Epoch: 2/2, step 5617/7134 completed (loss: 0.008708483539521694, acc: 0.9983766078948975)
[2025-02-13 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55][root][INFO] - Training Epoch: 2/2, step 5618/7134 completed (loss: 0.04793248325586319, acc: 0.9788618087768555)
[2025-02-13 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:56][root][INFO] - Training Epoch: 2/2, step 5619/7134 completed (loss: 0.04218175262212753, acc: 0.9858012199401855)
[2025-02-13 04:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:56][root][INFO] - Training Epoch: 2/2, step 5620/7134 completed (loss: 0.027941934764385223, acc: 0.9906542301177979)
[2025-02-13 04:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57][root][INFO] - Training Epoch: 2/2, step 5621/7134 completed (loss: 0.03066098503768444, acc: 0.9923760890960693)
[2025-02-13 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57][root][INFO] - Training Epoch: 2/2, step 5622/7134 completed (loss: 0.04955526813864708, acc: 0.9918256402015686)
[2025-02-13 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57][root][INFO] - Training Epoch: 2/2, step 5623/7134 completed (loss: 0.024215299636125565, acc: 0.9912891983985901)
[2025-02-13 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:58][root][INFO] - Training Epoch: 2/2, step 5624/7134 completed (loss: 0.03568316623568535, acc: 0.9918166995048523)
[2025-02-13 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:58][root][INFO] - Training Epoch: 2/2, step 5625/7134 completed (loss: 0.006563074886798859, acc: 0.9976470470428467)
[2025-02-13 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59][root][INFO] - Training Epoch: 2/2, step 5626/7134 completed (loss: 0.009460010565817356, acc: 0.9983079433441162)
[2025-02-13 04:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59][root][INFO] - Training Epoch: 2/2, step 5627/7134 completed (loss: 0.09704695641994476, acc: 0.9758812785148621)
[2025-02-13 04:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59][root][INFO] - Training Epoch: 2/2, step 5628/7134 completed (loss: 0.03699270263314247, acc: 0.9872495532035828)
[2025-02-13 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00][root][INFO] - Training Epoch: 2/2, step 5629/7134 completed (loss: 0.03325631842017174, acc: 0.9841827750205994)
[2025-02-13 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00][root][INFO] - Training Epoch: 2/2, step 5630/7134 completed (loss: 0.041973844170570374, acc: 0.9835729002952576)
[2025-02-13 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01][root][INFO] - Training Epoch: 2/2, step 5631/7134 completed (loss: 0.05406687408685684, acc: 0.9817578792572021)
[2025-02-13 04:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01][root][INFO] - Training Epoch: 2/2, step 5632/7134 completed (loss: 0.02722812257707119, acc: 0.9927404522895813)
[2025-02-13 04:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01][root][INFO] - Training Epoch: 2/2, step 5633/7134 completed (loss: 0.02424759417772293, acc: 0.9934853315353394)
[2025-02-13 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:02][root][INFO] - Training Epoch: 2/2, step 5634/7134 completed (loss: 0.03800386190414429, acc: 0.9930192232131958)
[2025-02-13 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:02][root][INFO] - Training Epoch: 2/2, step 5635/7134 completed (loss: 0.017672382295131683, acc: 0.9966216087341309)
[2025-02-13 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03][root][INFO] - Training Epoch: 2/2, step 5636/7134 completed (loss: 0.03211692348122597, acc: 0.9936102032661438)
[2025-02-13 04:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03][root][INFO] - Training Epoch: 2/2, step 5637/7134 completed (loss: 0.034610483795404434, acc: 0.9881955981254578)
[2025-02-13 04:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03][root][INFO] - Training Epoch: 2/2, step 5638/7134 completed (loss: 0.028292838484048843, acc: 0.994140625)
[2025-02-13 04:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:04][root][INFO] - Training Epoch: 2/2, step 5639/7134 completed (loss: 0.018550412729382515, acc: 0.995726466178894)
[2025-02-13 04:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:04][root][INFO] - Training Epoch: 2/2, step 5640/7134 completed (loss: 0.03943336755037308, acc: 0.9886178970336914)
[2025-02-13 04:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05][root][INFO] - Training Epoch: 2/2, step 5641/7134 completed (loss: 0.03514707833528519, acc: 0.9924242496490479)
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05][root][INFO] - Training Epoch: 2/2, step 5642/7134 completed (loss: 0.03195338323712349, acc: 0.9909090995788574)
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05][root][INFO] - Training Epoch: 2/2, step 5643/7134 completed (loss: 0.04357191175222397, acc: 0.98893803358078)
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:06][root][INFO] - Training Epoch: 2/2, step 5644/7134 completed (loss: 0.03392920270562172, acc: 0.9878048896789551)
[2025-02-13 04:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:06][root][INFO] - Training Epoch: 2/2, step 5645/7134 completed (loss: 0.01428877841681242, acc: 0.9955817461013794)
[2025-02-13 04:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07][root][INFO] - Training Epoch: 2/2, step 5646/7134 completed (loss: 0.03377147763967514, acc: 0.9888535141944885)
[2025-02-13 04:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07][root][INFO] - Training Epoch: 2/2, step 5647/7134 completed (loss: 0.036107055842876434, acc: 0.9906976819038391)
[2025-02-13 04:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07][root][INFO] - Training Epoch: 2/2, step 5648/7134 completed (loss: 0.07502733170986176, acc: 0.9850746393203735)
[2025-02-13 04:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:08][root][INFO] - Training Epoch: 2/2, step 5649/7134 completed (loss: 0.04134044051170349, acc: 0.9877675771713257)
[2025-02-13 04:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:08][root][INFO] - Training Epoch: 2/2, step 5650/7134 completed (loss: 0.026179932057857513, acc: 0.994854211807251)
[2025-02-13 04:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09][root][INFO] - Training Epoch: 2/2, step 5651/7134 completed (loss: 0.016905272379517555, acc: 0.9926035404205322)
[2025-02-13 04:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09][root][INFO] - Training Epoch: 2/2, step 5652/7134 completed (loss: 0.030562592670321465, acc: 0.9887820482254028)
[2025-02-13 04:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09][root][INFO] - Training Epoch: 2/2, step 5653/7134 completed (loss: 0.0638054683804512, acc: 0.9765625)
[2025-02-13 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10][root][INFO] - Training Epoch: 2/2, step 5654/7134 completed (loss: 0.01925824210047722, acc: 0.9939939975738525)
[2025-02-13 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10][root][INFO] - Training Epoch: 2/2, step 5655/7134 completed (loss: 0.040163010358810425, acc: 0.988095223903656)
[2025-02-13 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11][root][INFO] - Training Epoch: 2/2, step 5656/7134 completed (loss: 0.02298644930124283, acc: 0.9904761910438538)
[2025-02-13 04:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11][root][INFO] - Training Epoch: 2/2, step 5657/7134 completed (loss: 0.03409464657306671, acc: 0.9912152290344238)
[2025-02-13 04:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11][root][INFO] - Training Epoch: 2/2, step 5658/7134 completed (loss: 0.034015629440546036, acc: 0.9875690340995789)
[2025-02-13 04:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:12][root][INFO] - Training Epoch: 2/2, step 5659/7134 completed (loss: 0.024017633870244026, acc: 0.9929078221321106)
[2025-02-13 04:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:12][root][INFO] - Training Epoch: 2/2, step 5660/7134 completed (loss: 0.01659935899078846, acc: 0.9933884143829346)
[2025-02-13 04:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:13][root][INFO] - Training Epoch: 2/2, step 5661/7134 completed (loss: 0.004860741551965475, acc: 1.0)
[2025-02-13 04:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:13][root][INFO] - Training Epoch: 2/2, step 5662/7134 completed (loss: 0.01155779417604208, acc: 0.9967948794364929)
[2025-02-13 04:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14][root][INFO] - Training Epoch: 2/2, step 5663/7134 completed (loss: 0.00970590952783823, acc: 0.9969183206558228)
[2025-02-13 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14][root][INFO] - Training Epoch: 2/2, step 5664/7134 completed (loss: 0.06953616440296173, acc: 0.9866270422935486)
[2025-02-13 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14][root][INFO] - Training Epoch: 2/2, step 5665/7134 completed (loss: 0.01498501654714346, acc: 0.9942938685417175)
[2025-02-13 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:15][root][INFO] - Training Epoch: 2/2, step 5666/7134 completed (loss: 0.0064263371750712395, acc: 0.9972413778305054)
[2025-02-13 04:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:15][root][INFO] - Training Epoch: 2/2, step 5667/7134 completed (loss: 0.028623173013329506, acc: 0.9945454597473145)
[2025-02-13 04:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16][root][INFO] - Training Epoch: 2/2, step 5668/7134 completed (loss: 0.026967477053403854, acc: 0.9933554530143738)
[2025-02-13 04:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16][root][INFO] - Training Epoch: 2/2, step 5669/7134 completed (loss: 0.011279696598649025, acc: 0.9970674514770508)
[2025-02-13 04:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16][root][INFO] - Training Epoch: 2/2, step 5670/7134 completed (loss: 0.019977331161499023, acc: 0.9916666746139526)
[2025-02-13 04:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17][root][INFO] - Training Epoch: 2/2, step 5671/7134 completed (loss: 0.01400755625218153, acc: 0.9963099360466003)
[2025-02-13 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17][root][INFO] - Training Epoch: 2/2, step 5672/7134 completed (loss: 0.01850413903594017, acc: 0.9968454241752625)
[2025-02-13 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18][root][INFO] - Training Epoch: 2/2, step 5673/7134 completed (loss: 0.012557008303701878, acc: 0.9985228776931763)
[2025-02-13 04:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18][root][INFO] - Training Epoch: 2/2, step 5674/7134 completed (loss: 0.018725810572504997, acc: 0.9938837885856628)
[2025-02-13 04:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18][root][INFO] - Training Epoch: 2/2, step 5675/7134 completed (loss: 0.026462554931640625, acc: 0.9950248599052429)
[2025-02-13 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:19][root][INFO] - Training Epoch: 2/2, step 5676/7134 completed (loss: 0.07689821720123291, acc: 0.9852070808410645)
[2025-02-13 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:19][root][INFO] - Training Epoch: 2/2, step 5677/7134 completed (loss: 0.04059381037950516, acc: 0.984375)
[2025-02-13 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20][root][INFO] - Training Epoch: 2/2, step 5678/7134 completed (loss: 0.027084609493613243, acc: 0.9926470518112183)
[2025-02-13 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20][root][INFO] - Training Epoch: 2/2, step 5679/7134 completed (loss: 0.06590937077999115, acc: 0.9751098155975342)
[2025-02-13 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21][root][INFO] - Training Epoch: 2/2, step 5680/7134 completed (loss: 0.0766834169626236, acc: 0.9746646881103516)
[2025-02-13 04:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21][root][INFO] - Training Epoch: 2/2, step 5681/7134 completed (loss: 0.0415581613779068, acc: 0.9864253401756287)
[2025-02-13 04:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21][root][INFO] - Training Epoch: 2/2, step 5682/7134 completed (loss: 0.02879699319601059, acc: 0.9884560108184814)
[2025-02-13 04:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22][root][INFO] - Training Epoch: 2/2, step 5683/7134 completed (loss: 0.045627411454916, acc: 0.9795022010803223)
[2025-02-13 04:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22][root][INFO] - Training Epoch: 2/2, step 5684/7134 completed (loss: 0.05559315159916878, acc: 0.9865092635154724)
[2025-02-13 04:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23][root][INFO] - Training Epoch: 2/2, step 5685/7134 completed (loss: 0.037740711122751236, acc: 0.9889705777168274)
[2025-02-13 04:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23][root][INFO] - Training Epoch: 2/2, step 5686/7134 completed (loss: 0.059143610298633575, acc: 0.9830795526504517)
[2025-02-13 04:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23][root][INFO] - Training Epoch: 2/2, step 5687/7134 completed (loss: 0.054878149181604385, acc: 0.9871794581413269)
[2025-02-13 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24][root][INFO] - Training Epoch: 2/2, step 5688/7134 completed (loss: 0.05615696683526039, acc: 0.9853137731552124)
[2025-02-13 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24][root][INFO] - Training Epoch: 2/2, step 5689/7134 completed (loss: 0.05146634578704834, acc: 0.9878234267234802)
[2025-02-13 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:25][root][INFO] - Training Epoch: 2/2, step 5690/7134 completed (loss: 0.02947801537811756, acc: 0.9929245114326477)
[2025-02-13 04:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:25][root][INFO] - Training Epoch: 2/2, step 5691/7134 completed (loss: 0.037892840802669525, acc: 0.9888198971748352)
[2025-02-13 04:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26][root][INFO] - Training Epoch: 2/2, step 5692/7134 completed (loss: 0.060428436845541, acc: 0.9879356622695923)
[2025-02-13 04:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26][root][INFO] - Training Epoch: 2/2, step 5693/7134 completed (loss: 0.019295139238238335, acc: 0.995502233505249)
[2025-02-13 04:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26][root][INFO] - Training Epoch: 2/2, step 5694/7134 completed (loss: 0.05033904314041138, acc: 0.9884225726127625)
[2025-02-13 04:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:27][root][INFO] - Training Epoch: 2/2, step 5695/7134 completed (loss: 0.01608496904373169, acc: 0.9957020282745361)
[2025-02-13 04:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:27][root][INFO] - Training Epoch: 2/2, step 5696/7134 completed (loss: 0.031527888029813766, acc: 0.9894117712974548)
[2025-02-13 04:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28][root][INFO] - Training Epoch: 2/2, step 5697/7134 completed (loss: 0.03161146864295006, acc: 0.995768666267395)
[2025-02-13 04:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28][root][INFO] - Training Epoch: 2/2, step 5698/7134 completed (loss: 0.03743589296936989, acc: 0.9918367266654968)
[2025-02-13 04:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:29][root][INFO] - Training Epoch: 2/2, step 5699/7134 completed (loss: 0.03924873098731041, acc: 0.987261176109314)
[2025-02-13 04:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:29][root][INFO] - Training Epoch: 2/2, step 5700/7134 completed (loss: 0.019406791776418686, acc: 0.9914893507957458)
[2025-02-13 04:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30][root][INFO] - Training Epoch: 2/2, step 5701/7134 completed (loss: 0.031202901154756546, acc: 0.9962453246116638)
[2025-02-13 04:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30][root][INFO] - Training Epoch: 2/2, step 5702/7134 completed (loss: 0.034790877252817154, acc: 0.9945873022079468)
[2025-02-13 04:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30][root][INFO] - Training Epoch: 2/2, step 5703/7134 completed (loss: 0.02209484949707985, acc: 0.9943740963935852)
[2025-02-13 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31][root][INFO] - Training Epoch: 2/2, step 5704/7134 completed (loss: 0.03908565640449524, acc: 0.9885877370834351)
[2025-02-13 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31][root][INFO] - Training Epoch: 2/2, step 5705/7134 completed (loss: 0.014348733238875866, acc: 0.9964285492897034)
[2025-02-13 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32][root][INFO] - Training Epoch: 2/2, step 5706/7134 completed (loss: 0.01138093788176775, acc: 0.9977628588676453)
[2025-02-13 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32][root][INFO] - Training Epoch: 2/2, step 5707/7134 completed (loss: 0.027657493948936462, acc: 0.98591548204422)
[2025-02-13 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32][root][INFO] - Training Epoch: 2/2, step 5708/7134 completed (loss: 0.01715875416994095, acc: 0.995555579662323)
[2025-02-13 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:33][root][INFO] - Training Epoch: 2/2, step 5709/7134 completed (loss: 0.04745756834745407, acc: 0.9917920827865601)
[2025-02-13 04:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:33][root][INFO] - Training Epoch: 2/2, step 5710/7134 completed (loss: 0.02550390362739563, acc: 0.9898219108581543)
[2025-02-13 04:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34][root][INFO] - Training Epoch: 2/2, step 5711/7134 completed (loss: 0.04295256361365318, acc: 0.9882199168205261)
[2025-02-13 04:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34][root][INFO] - Training Epoch: 2/2, step 5712/7134 completed (loss: 0.04199489578604698, acc: 0.984000027179718)
[2025-02-13 04:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34][root][INFO] - Training Epoch: 2/2, step 5713/7134 completed (loss: 0.02686011977493763, acc: 0.9885641932487488)
[2025-02-13 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35][root][INFO] - Training Epoch: 2/2, step 5714/7134 completed (loss: 0.02379053458571434, acc: 0.9950186610221863)
[2025-02-13 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35][root][INFO] - Training Epoch: 2/2, step 5715/7134 completed (loss: 0.06250421702861786, acc: 0.9888268113136292)
[2025-02-13 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:36][root][INFO] - Training Epoch: 2/2, step 5716/7134 completed (loss: 0.08492353558540344, acc: 0.9822379946708679)
[2025-02-13 04:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:36][root][INFO] - Training Epoch: 2/2, step 5717/7134 completed (loss: 0.06274856626987457, acc: 0.9828926920890808)
[2025-02-13 04:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37][root][INFO] - Training Epoch: 2/2, step 5718/7134 completed (loss: 0.026399333029985428, acc: 0.9905362725257874)
[2025-02-13 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37][root][INFO] - Training Epoch: 2/2, step 5719/7134 completed (loss: 0.01870521903038025, acc: 0.995502233505249)
[2025-02-13 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37][root][INFO] - Training Epoch: 2/2, step 5720/7134 completed (loss: 0.032746821641922, acc: 0.9861830472946167)
[2025-02-13 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38][root][INFO] - Training Epoch: 2/2, step 5721/7134 completed (loss: 0.014706475660204887, acc: 0.9947368502616882)
[2025-02-13 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38][root][INFO] - Training Epoch: 2/2, step 5722/7134 completed (loss: 0.01521544624119997, acc: 0.9955654144287109)
[2025-02-13 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38][root][INFO] - Training Epoch: 2/2, step 5723/7134 completed (loss: 0.032163891941308975, acc: 0.9909228682518005)
[2025-02-13 04:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39][root][INFO] - Training Epoch: 2/2, step 5724/7134 completed (loss: 0.021591706201434135, acc: 0.9904000163078308)
[2025-02-13 04:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39][root][INFO] - Training Epoch: 2/2, step 5725/7134 completed (loss: 0.010842842049896717, acc: 0.9985714554786682)
[2025-02-13 04:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40][root][INFO] - Training Epoch: 2/2, step 5726/7134 completed (loss: 0.043246712535619736, acc: 0.9923547506332397)
[2025-02-13 04:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40][root][INFO] - Training Epoch: 2/2, step 5727/7134 completed (loss: 0.011895962990820408, acc: 0.9986631274223328)
[2025-02-13 04:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40][root][INFO] - Training Epoch: 2/2, step 5728/7134 completed (loss: 0.007717258762568235, acc: 1.0)
[2025-02-13 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:41][root][INFO] - Training Epoch: 2/2, step 5729/7134 completed (loss: 0.010016167536377907, acc: 0.996666669845581)
[2025-02-13 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:41][root][INFO] - Training Epoch: 2/2, step 5730/7134 completed (loss: 0.05667232722043991, acc: 0.9858585596084595)
[2025-02-13 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42][root][INFO] - Training Epoch: 2/2, step 5731/7134 completed (loss: 0.022894226014614105, acc: 0.9933554530143738)
[2025-02-13 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42][root][INFO] - Training Epoch: 2/2, step 5732/7134 completed (loss: 0.0895029753446579, acc: 0.9786407947540283)
[2025-02-13 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42][root][INFO] - Training Epoch: 2/2, step 5733/7134 completed (loss: 0.024117939174175262, acc: 0.9927140474319458)
[2025-02-13 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43][root][INFO] - Training Epoch: 2/2, step 5734/7134 completed (loss: 0.010241899639368057, acc: 1.0)
[2025-02-13 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43][root][INFO] - Training Epoch: 2/2, step 5735/7134 completed (loss: 0.038591668009757996, acc: 0.9874551892280579)
[2025-02-13 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:44][root][INFO] - Training Epoch: 2/2, step 5736/7134 completed (loss: 0.03474106267094612, acc: 0.9864661693572998)
[2025-02-13 04:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:44][root][INFO] - Training Epoch: 2/2, step 5737/7134 completed (loss: 0.03002035617828369, acc: 0.9894259572029114)
[2025-02-13 04:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45][root][INFO] - Training Epoch: 2/2, step 5738/7134 completed (loss: 0.026627082377672195, acc: 0.9915356636047363)
[2025-02-13 04:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45][root][INFO] - Training Epoch: 2/2, step 5739/7134 completed (loss: 0.03495173156261444, acc: 0.9831578731536865)
[2025-02-13 04:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45][root][INFO] - Training Epoch: 2/2, step 5740/7134 completed (loss: 0.02705460786819458, acc: 0.9910485744476318)
[2025-02-13 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46][root][INFO] - Training Epoch: 2/2, step 5741/7134 completed (loss: 0.03425664082169533, acc: 0.9910141229629517)
[2025-02-13 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46][root][INFO] - Training Epoch: 2/2, step 5742/7134 completed (loss: 0.031273189932107925, acc: 0.9881734848022461)
[2025-02-13 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:47][root][INFO] - Training Epoch: 2/2, step 5743/7134 completed (loss: 0.01739358715713024, acc: 0.9950980544090271)
[2025-02-13 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:47][root][INFO] - Training Epoch: 2/2, step 5744/7134 completed (loss: 0.009092236869037151, acc: 0.9979079365730286)
[2025-02-13 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48][root][INFO] - Training Epoch: 2/2, step 5745/7134 completed (loss: 0.03218566253781319, acc: 0.9896373152732849)
[2025-02-13 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48][root][INFO] - Training Epoch: 2/2, step 5746/7134 completed (loss: 0.05090557038784027, acc: 0.9897058606147766)
[2025-02-13 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48][root][INFO] - Training Epoch: 2/2, step 5747/7134 completed (loss: 0.016757028177380562, acc: 0.9930747747421265)
[2025-02-13 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49][root][INFO] - Training Epoch: 2/2, step 5748/7134 completed (loss: 0.011350351385772228, acc: 0.9945155382156372)
[2025-02-13 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49][root][INFO] - Training Epoch: 2/2, step 5749/7134 completed (loss: 0.029670996591448784, acc: 0.9921466112136841)
[2025-02-13 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50][root][INFO] - Training Epoch: 2/2, step 5750/7134 completed (loss: 0.03576064482331276, acc: 0.9900990128517151)
[2025-02-13 04:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50][root][INFO] - Training Epoch: 2/2, step 5751/7134 completed (loss: 0.025468196719884872, acc: 0.9884467124938965)
[2025-02-13 04:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50][root][INFO] - Training Epoch: 2/2, step 5752/7134 completed (loss: 0.057437002658843994, acc: 0.988399088382721)
[2025-02-13 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:51][root][INFO] - Training Epoch: 2/2, step 5753/7134 completed (loss: 0.05306325480341911, acc: 0.987500011920929)
[2025-02-13 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:51][root][INFO] - Training Epoch: 2/2, step 5754/7134 completed (loss: 0.04288094863295555, acc: 0.9821656346321106)
[2025-02-13 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:52][root][INFO] - Training Epoch: 2/2, step 5755/7134 completed (loss: 0.030942311510443687, acc: 0.9882869720458984)
[2025-02-13 04:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:52][root][INFO] - Training Epoch: 2/2, step 5756/7134 completed (loss: 0.03594040870666504, acc: 0.9906914830207825)
[2025-02-13 04:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53][root][INFO] - Training Epoch: 2/2, step 5757/7134 completed (loss: 0.016569368541240692, acc: 0.9936143159866333)
[2025-02-13 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53][root][INFO] - Training Epoch: 2/2, step 5758/7134 completed (loss: 0.034762781113386154, acc: 0.9882352948188782)
[2025-02-13 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53][root][INFO] - Training Epoch: 2/2, step 5759/7134 completed (loss: 0.02932434156537056, acc: 0.9838274717330933)
[2025-02-13 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:54][root][INFO] - Training Epoch: 2/2, step 5760/7134 completed (loss: 0.021869122982025146, acc: 0.9954198598861694)
[2025-02-13 04:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:54][root][INFO] - Training Epoch: 2/2, step 5761/7134 completed (loss: 0.023779183626174927, acc: 0.9926739931106567)
[2025-02-13 04:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55][root][INFO] - Training Epoch: 2/2, step 5762/7134 completed (loss: 0.05910229682922363, acc: 0.9835766553878784)
[2025-02-13 04:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55][root][INFO] - Training Epoch: 2/2, step 5763/7134 completed (loss: 0.025525851175189018, acc: 0.9949811697006226)
[2025-02-13 04:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55][root][INFO] - Training Epoch: 2/2, step 5764/7134 completed (loss: 0.013451329432427883, acc: 0.9948717951774597)
[2025-02-13 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:56][root][INFO] - Training Epoch: 2/2, step 5765/7134 completed (loss: 0.012058406136929989, acc: 0.9971988797187805)
[2025-02-13 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:56][root][INFO] - Training Epoch: 2/2, step 5766/7134 completed (loss: 0.01872039958834648, acc: 0.9970104694366455)
[2025-02-13 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57][root][INFO] - Training Epoch: 2/2, step 5767/7134 completed (loss: 0.010456056334078312, acc: 0.9958275556564331)
[2025-02-13 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57][root][INFO] - Training Epoch: 2/2, step 5768/7134 completed (loss: 0.01395975612103939, acc: 0.992514967918396)
[2025-02-13 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57][root][INFO] - Training Epoch: 2/2, step 5769/7134 completed (loss: 0.007380844559520483, acc: 0.9967266917228699)
[2025-02-13 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:58][root][INFO] - Training Epoch: 2/2, step 5770/7134 completed (loss: 0.031085746362805367, acc: 0.9914320707321167)
[2025-02-13 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:58][root][INFO] - Training Epoch: 2/2, step 5771/7134 completed (loss: 0.03695822134613991, acc: 0.98777174949646)
[2025-02-13 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:59][root][INFO] - Training Epoch: 2/2, step 5772/7134 completed (loss: 0.04742187634110451, acc: 0.9878261089324951)
[2025-02-13 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:59][root][INFO] - Training Epoch: 2/2, step 5773/7134 completed (loss: 0.01461057923734188, acc: 0.9949832558631897)
[2025-02-13 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00][root][INFO] - Training Epoch: 2/2, step 5774/7134 completed (loss: 0.006748704705387354, acc: 0.998516321182251)
[2025-02-13 04:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00][root][INFO] - Training Epoch: 2/2, step 5775/7134 completed (loss: 0.011458156630396843, acc: 0.9967051148414612)
[2025-02-13 04:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00][root][INFO] - Training Epoch: 2/2, step 5776/7134 completed (loss: 0.03726363927125931, acc: 0.983146071434021)
[2025-02-13 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:01][root][INFO] - Training Epoch: 2/2, step 5777/7134 completed (loss: 0.05512909218668938, acc: 0.9888613820075989)
[2025-02-13 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:01][root][INFO] - Training Epoch: 2/2, step 5778/7134 completed (loss: 0.07380715757608414, acc: 0.9807692170143127)
[2025-02-13 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:02][root][INFO] - Training Epoch: 2/2, step 5779/7134 completed (loss: 0.03488803654909134, acc: 0.9917469024658203)
[2025-02-13 04:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:02][root][INFO] - Training Epoch: 2/2, step 5780/7134 completed (loss: 0.025261513888835907, acc: 0.9916467666625977)
[2025-02-13 04:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:03][root][INFO] - Training Epoch: 2/2, step 5781/7134 completed (loss: 0.02531472034752369, acc: 0.9930939078330994)
[2025-02-13 04:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:03][root][INFO] - Training Epoch: 2/2, step 5782/7134 completed (loss: 0.010803647339344025, acc: 0.9974093437194824)
[2025-02-13 04:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04][root][INFO] - Training Epoch: 2/2, step 5783/7134 completed (loss: 0.008231058716773987, acc: 0.9973439574241638)
[2025-02-13 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04][root][INFO] - Training Epoch: 2/2, step 5784/7134 completed (loss: 0.02132343128323555, acc: 0.9946164488792419)
[2025-02-13 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04][root][INFO] - Training Epoch: 2/2, step 5785/7134 completed (loss: 0.022392038255929947, acc: 0.9919137358665466)
[2025-02-13 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:05][root][INFO] - Training Epoch: 2/2, step 5786/7134 completed (loss: 0.019303984940052032, acc: 0.9918032884597778)
[2025-02-13 04:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:05][root][INFO] - Training Epoch: 2/2, step 5787/7134 completed (loss: 0.0159672349691391, acc: 0.996268630027771)
[2025-02-13 04:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06][root][INFO] - Training Epoch: 2/2, step 5788/7134 completed (loss: 0.007335323840379715, acc: 0.9969230890274048)
[2025-02-13 04:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06][root][INFO] - Training Epoch: 2/2, step 5789/7134 completed (loss: 0.015804655849933624, acc: 0.9937965273857117)
[2025-02-13 04:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06][root][INFO] - Training Epoch: 2/2, step 5790/7134 completed (loss: 0.025416366755962372, acc: 0.9862637519836426)
[2025-02-13 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:07][root][INFO] - Training Epoch: 2/2, step 5791/7134 completed (loss: 0.029414989054203033, acc: 0.9914089441299438)
[2025-02-13 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:07][root][INFO] - Training Epoch: 2/2, step 5792/7134 completed (loss: 0.058847274631261826, acc: 0.9864176511764526)
[2025-02-13 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08][root][INFO] - Training Epoch: 2/2, step 5793/7134 completed (loss: 0.06279538571834564, acc: 0.9827044010162354)
[2025-02-13 04:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08][root][INFO] - Training Epoch: 2/2, step 5794/7134 completed (loss: 0.035313572734594345, acc: 0.9878787994384766)
[2025-02-13 04:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08][root][INFO] - Training Epoch: 2/2, step 5795/7134 completed (loss: 0.09660089761018753, acc: 0.9747899174690247)
[2025-02-13 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09][root][INFO] - Training Epoch: 2/2, step 5796/7134 completed (loss: 0.025727620348334312, acc: 0.9915397763252258)
[2025-02-13 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09][root][INFO] - Training Epoch: 2/2, step 5797/7134 completed (loss: 0.022604897618293762, acc: 0.9919999837875366)
[2025-02-13 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10][root][INFO] - Training Epoch: 2/2, step 5798/7134 completed (loss: 0.19762735068798065, acc: 0.9599999785423279)
[2025-02-13 04:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10][root][INFO] - Training Epoch: 2/2, step 5799/7134 completed (loss: 0.07983472943305969, acc: 0.9783037304878235)
[2025-02-13 04:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10][root][INFO] - Training Epoch: 2/2, step 5800/7134 completed (loss: 0.042046308517456055, acc: 0.9852941036224365)
[2025-02-13 04:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:11][root][INFO] - Training Epoch: 2/2, step 5801/7134 completed (loss: 0.0398281067609787, acc: 0.9878048896789551)
[2025-02-13 04:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:11][root][INFO] - Training Epoch: 2/2, step 5802/7134 completed (loss: 0.02113686501979828, acc: 0.9944547414779663)
[2025-02-13 04:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12][root][INFO] - Training Epoch: 2/2, step 5803/7134 completed (loss: 0.020184185355901718, acc: 0.9923469424247742)
[2025-02-13 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12][root][INFO] - Training Epoch: 2/2, step 5804/7134 completed (loss: 0.06045146659016609, acc: 0.9797022938728333)
[2025-02-13 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13][root][INFO] - Training Epoch: 2/2, step 5805/7134 completed (loss: 0.03764480724930763, acc: 0.9913669228553772)
[2025-02-13 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13][root][INFO] - Training Epoch: 2/2, step 5806/7134 completed (loss: 0.02870332822203636, acc: 0.9904371500015259)
[2025-02-13 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13][root][INFO] - Training Epoch: 2/2, step 5807/7134 completed (loss: 0.04519083350896835, acc: 0.9863842725753784)
[2025-02-13 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:14][root][INFO] - Training Epoch: 2/2, step 5808/7134 completed (loss: 0.021534068509936333, acc: 0.9943740963935852)
[2025-02-13 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:14][root][INFO] - Training Epoch: 2/2, step 5809/7134 completed (loss: 0.03111124224960804, acc: 0.9934210777282715)
[2025-02-13 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15][root][INFO] - Training Epoch: 2/2, step 5810/7134 completed (loss: 0.019763199612498283, acc: 0.9898580312728882)
[2025-02-13 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15][root][INFO] - Training Epoch: 2/2, step 5811/7134 completed (loss: 0.03390007093548775, acc: 0.983132541179657)
[2025-02-13 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15][root][INFO] - Training Epoch: 2/2, step 5812/7134 completed (loss: 0.012317797169089317, acc: 0.9966996908187866)
[2025-02-13 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16][root][INFO] - Training Epoch: 2/2, step 5813/7134 completed (loss: 0.017025193199515343, acc: 0.994339644908905)
[2025-02-13 04:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16][root][INFO] - Training Epoch: 2/2, step 5814/7134 completed (loss: 0.010805507190525532, acc: 0.9966158866882324)
[2025-02-13 04:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17][root][INFO] - Training Epoch: 2/2, step 5815/7134 completed (loss: 0.019820794463157654, acc: 0.9962335228919983)
[2025-02-13 04:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17][root][INFO] - Training Epoch: 2/2, step 5816/7134 completed (loss: 0.007043363526463509, acc: 0.9975062608718872)
[2025-02-13 04:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17][root][INFO] - Training Epoch: 2/2, step 5817/7134 completed (loss: 0.044113192707300186, acc: 0.989924430847168)
[2025-02-13 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:18][root][INFO] - Training Epoch: 2/2, step 5818/7134 completed (loss: 0.030018487945199013, acc: 0.9866666793823242)
[2025-02-13 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:18][root][INFO] - Training Epoch: 2/2, step 5819/7134 completed (loss: 0.022599201649427414, acc: 0.9948275685310364)
[2025-02-13 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19][root][INFO] - Training Epoch: 2/2, step 5820/7134 completed (loss: 0.03812922537326813, acc: 0.9876033067703247)
[2025-02-13 04:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19][root][INFO] - Training Epoch: 2/2, step 5821/7134 completed (loss: 0.01872166059911251, acc: 0.9938367009162903)
[2025-02-13 04:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19][root][INFO] - Training Epoch: 2/2, step 5822/7134 completed (loss: 0.024703269824385643, acc: 0.9919354915618896)
[2025-02-13 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:20][root][INFO] - Training Epoch: 2/2, step 5823/7134 completed (loss: 0.018283164128661156, acc: 0.9922239780426025)
[2025-02-13 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:20][root][INFO] - Training Epoch: 2/2, step 5824/7134 completed (loss: 0.012950742617249489, acc: 0.9984447956085205)
[2025-02-13 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21][root][INFO] - Training Epoch: 2/2, step 5825/7134 completed (loss: 0.04419935122132301, acc: 0.991349458694458)
[2025-02-13 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21][root][INFO] - Training Epoch: 2/2, step 5826/7134 completed (loss: 0.018831821158528328, acc: 0.9929078221321106)
[2025-02-13 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21][root][INFO] - Training Epoch: 2/2, step 5827/7134 completed (loss: 0.009150141850113869, acc: 0.9961538314819336)
[2025-02-13 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:22][root][INFO] - Training Epoch: 2/2, step 5828/7134 completed (loss: 0.024027667939662933, acc: 0.9910846948623657)
[2025-02-13 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:22][root][INFO] - Training Epoch: 2/2, step 5829/7134 completed (loss: 0.02014761045575142, acc: 0.9934123754501343)
[2025-02-13 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23][root][INFO] - Training Epoch: 2/2, step 5830/7134 completed (loss: 0.018824689090251923, acc: 0.996219277381897)
[2025-02-13 04:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23][root][INFO] - Training Epoch: 2/2, step 5831/7134 completed (loss: 0.012704158201813698, acc: 0.9976744055747986)
[2025-02-13 04:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23][root][INFO] - Training Epoch: 2/2, step 5832/7134 completed (loss: 0.047431424260139465, acc: 0.9898648858070374)
[2025-02-13 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24][root][INFO] - Training Epoch: 2/2, step 5833/7134 completed (loss: 0.01812705770134926, acc: 0.9928315281867981)
[2025-02-13 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24][root][INFO] - Training Epoch: 2/2, step 5834/7134 completed (loss: 0.021675121039152145, acc: 0.9958333373069763)
[2025-02-13 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25][root][INFO] - Training Epoch: 2/2, step 5835/7134 completed (loss: 0.01032042596489191, acc: 0.9971098303794861)
[2025-02-13 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25][root][INFO] - Training Epoch: 2/2, step 5836/7134 completed (loss: 0.02144927717745304, acc: 0.9927954077720642)
[2025-02-13 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25][root][INFO] - Training Epoch: 2/2, step 5837/7134 completed (loss: 0.05347442999482155, acc: 0.9915540814399719)
[2025-02-13 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:26][root][INFO] - Training Epoch: 2/2, step 5838/7134 completed (loss: 0.03739301860332489, acc: 0.9934102296829224)
[2025-02-13 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:26][root][INFO] - Training Epoch: 2/2, step 5839/7134 completed (loss: 0.01747557520866394, acc: 0.9928571581840515)
[2025-02-13 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27][root][INFO] - Training Epoch: 2/2, step 5840/7134 completed (loss: 0.013608042150735855, acc: 0.995230495929718)
[2025-02-13 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27][root][INFO] - Training Epoch: 2/2, step 5841/7134 completed (loss: 0.026025988161563873, acc: 0.9888357520103455)
[2025-02-13 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27][root][INFO] - Training Epoch: 2/2, step 5842/7134 completed (loss: 0.016110669821500778, acc: 0.9927536249160767)
[2025-02-13 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28][root][INFO] - Training Epoch: 2/2, step 5843/7134 completed (loss: 0.029291324317455292, acc: 0.993306577205658)
[2025-02-13 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28][root][INFO] - Training Epoch: 2/2, step 5844/7134 completed (loss: 0.009241453371942043, acc: 0.9985775351524353)
[2025-02-13 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29][root][INFO] - Training Epoch: 2/2, step 5845/7134 completed (loss: 0.01571241207420826, acc: 0.995726466178894)
[2025-02-13 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29][root][INFO] - Training Epoch: 2/2, step 5846/7134 completed (loss: 0.029049070551991463, acc: 0.9917920827865601)
[2025-02-13 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30][root][INFO] - Training Epoch: 2/2, step 5847/7134 completed (loss: 0.009101797826588154, acc: 0.9972527623176575)
[2025-02-13 04:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30][root][INFO] - Training Epoch: 2/2, step 5848/7134 completed (loss: 0.018838271498680115, acc: 0.9910714030265808)
[2025-02-13 04:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30][root][INFO] - Training Epoch: 2/2, step 5849/7134 completed (loss: 0.031203968450427055, acc: 0.9896755218505859)
[2025-02-13 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:31][root][INFO] - Training Epoch: 2/2, step 5850/7134 completed (loss: 0.013684304431080818, acc: 0.9938931465148926)
[2025-02-13 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:31][root][INFO] - Training Epoch: 2/2, step 5851/7134 completed (loss: 0.020566027611494064, acc: 0.9934210777282715)
[2025-02-13 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32][root][INFO] - Training Epoch: 2/2, step 5852/7134 completed (loss: 0.010490010492503643, acc: 0.9956204295158386)
[2025-02-13 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32][root][INFO] - Training Epoch: 2/2, step 5853/7134 completed (loss: 0.03831794112920761, acc: 0.9918808937072754)
[2025-02-13 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32][root][INFO] - Training Epoch: 2/2, step 5854/7134 completed (loss: 0.01673380471765995, acc: 0.991150438785553)
[2025-02-13 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33][root][INFO] - Training Epoch: 2/2, step 5855/7134 completed (loss: 0.032196491956710815, acc: 0.9896373152732849)
[2025-02-13 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33][root][INFO] - Training Epoch: 2/2, step 5856/7134 completed (loss: 0.05301491171121597, acc: 0.9833837151527405)
[2025-02-13 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34][root][INFO] - Training Epoch: 2/2, step 5857/7134 completed (loss: 0.021376147866249084, acc: 0.9921011328697205)
[2025-02-13 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34][root][INFO] - Training Epoch: 2/2, step 5858/7134 completed (loss: 0.011543326079845428, acc: 0.9981516003608704)
[2025-02-13 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34][root][INFO] - Training Epoch: 2/2, step 5859/7134 completed (loss: 0.027723677456378937, acc: 0.994140625)
[2025-02-13 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:35][root][INFO] - Training Epoch: 2/2, step 5860/7134 completed (loss: 0.025160040706396103, acc: 0.9889807105064392)
[2025-02-13 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:35][root][INFO] - Training Epoch: 2/2, step 5861/7134 completed (loss: 0.01332424208521843, acc: 0.9959431886672974)
[2025-02-13 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36][root][INFO] - Training Epoch: 2/2, step 5862/7134 completed (loss: 0.015562399290502071, acc: 0.9935622215270996)
[2025-02-13 04:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36][root][INFO] - Training Epoch: 2/2, step 5863/7134 completed (loss: 0.03717225417494774, acc: 0.9924242496490479)
[2025-02-13 04:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36][root][INFO] - Training Epoch: 2/2, step 5864/7134 completed (loss: 0.05324126034975052, acc: 0.985029935836792)
[2025-02-13 04:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:37][root][INFO] - Training Epoch: 2/2, step 5865/7134 completed (loss: 0.0410696305334568, acc: 0.9904761910438538)
[2025-02-13 04:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:37][root][INFO] - Training Epoch: 2/2, step 5866/7134 completed (loss: 0.013977430760860443, acc: 0.9933862686157227)
[2025-02-13 04:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38][root][INFO] - Training Epoch: 2/2, step 5867/7134 completed (loss: 0.008353758603334427, acc: 1.0)
[2025-02-13 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38][root][INFO] - Training Epoch: 2/2, step 5868/7134 completed (loss: 0.033490508794784546, acc: 0.9879310131072998)
[2025-02-13 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38][root][INFO] - Training Epoch: 2/2, step 5869/7134 completed (loss: 0.04443112015724182, acc: 0.9895522594451904)
[2025-02-13 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39][root][INFO] - Training Epoch: 2/2, step 5870/7134 completed (loss: 0.05712268128991127, acc: 0.9895397424697876)
[2025-02-13 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39][root][INFO] - Training Epoch: 2/2, step 5871/7134 completed (loss: 0.02300211787223816, acc: 0.9915074110031128)
[2025-02-13 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40][root][INFO] - Training Epoch: 2/2, step 5872/7134 completed (loss: 0.028735823929309845, acc: 0.9878234267234802)
[2025-02-13 04:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40][root][INFO] - Training Epoch: 2/2, step 5873/7134 completed (loss: 0.03144676610827446, acc: 0.9906666874885559)
[2025-02-13 04:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40][root][INFO] - Training Epoch: 2/2, step 5874/7134 completed (loss: 0.010008284822106361, acc: 0.9959839582443237)
[2025-02-13 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:41][root][INFO] - Training Epoch: 2/2, step 5875/7134 completed (loss: 0.036733172833919525, acc: 0.9840707778930664)
[2025-02-13 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:41][root][INFO] - Training Epoch: 2/2, step 5876/7134 completed (loss: 0.007614989299327135, acc: 0.9975728392601013)
[2025-02-13 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42][root][INFO] - Training Epoch: 2/2, step 5877/7134 completed (loss: 0.030726296827197075, acc: 0.9911167621612549)
[2025-02-13 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42][root][INFO] - Training Epoch: 2/2, step 5878/7134 completed (loss: 0.016750048846006393, acc: 0.9956140518188477)
[2025-02-13 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42][root][INFO] - Training Epoch: 2/2, step 5879/7134 completed (loss: 0.006192687898874283, acc: 1.0)
[2025-02-13 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43][root][INFO] - Training Epoch: 2/2, step 5880/7134 completed (loss: 0.008854378946125507, acc: 0.995555579662323)
[2025-02-13 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43][root][INFO] - Training Epoch: 2/2, step 5881/7134 completed (loss: 0.008797132410109043, acc: 0.9980806112289429)
[2025-02-13 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43][root][INFO] - Training Epoch: 2/2, step 5882/7134 completed (loss: 0.013670274056494236, acc: 0.9976470470428467)
[2025-02-13 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44][root][INFO] - Training Epoch: 2/2, step 5883/7134 completed (loss: 0.022978391498327255, acc: 0.9906367063522339)
[2025-02-13 04:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44][root][INFO] - Training Epoch: 2/2, step 5884/7134 completed (loss: 0.04090367257595062, acc: 0.9817517995834351)
[2025-02-13 04:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44][root][INFO] - Training Epoch: 2/2, step 5885/7134 completed (loss: 0.06840000301599503, acc: 0.98097825050354)
[2025-02-13 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45][root][INFO] - Training Epoch: 2/2, step 5886/7134 completed (loss: 0.010410779155790806, acc: 0.9971428513526917)
[2025-02-13 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45][root][INFO] - Training Epoch: 2/2, step 5887/7134 completed (loss: 0.026951296254992485, acc: 0.9915074110031128)
[2025-02-13 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46][root][INFO] - Training Epoch: 2/2, step 5888/7134 completed (loss: 0.05437570810317993, acc: 0.9864864945411682)
[2025-02-13 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46][root][INFO] - Training Epoch: 2/2, step 5889/7134 completed (loss: 0.012084313668310642, acc: 0.9955357313156128)
[2025-02-13 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46][root][INFO] - Training Epoch: 2/2, step 5890/7134 completed (loss: 0.014953126199543476, acc: 0.9947552680969238)
[2025-02-13 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:47][root][INFO] - Training Epoch: 2/2, step 5891/7134 completed (loss: 0.029599342495203018, acc: 0.9871086478233337)
[2025-02-13 04:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:47][root][INFO] - Training Epoch: 2/2, step 5892/7134 completed (loss: 0.04495837166905403, acc: 0.9927113652229309)
[2025-02-13 04:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48][root][INFO] - Training Epoch: 2/2, step 5893/7134 completed (loss: 0.03917212784290314, acc: 0.9889763593673706)
[2025-02-13 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48][root][INFO] - Training Epoch: 2/2, step 5894/7134 completed (loss: 0.003550437279045582, acc: 1.0)
[2025-02-13 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48][root][INFO] - Training Epoch: 2/2, step 5895/7134 completed (loss: 0.012739084661006927, acc: 0.9952380657196045)
[2025-02-13 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49][root][INFO] - Training Epoch: 2/2, step 5896/7134 completed (loss: 0.03546057268977165, acc: 0.988095223903656)
[2025-02-13 04:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49][root][INFO] - Training Epoch: 2/2, step 5897/7134 completed (loss: 0.010734041221439838, acc: 0.9958677887916565)
[2025-02-13 04:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49][root][INFO] - Training Epoch: 2/2, step 5898/7134 completed (loss: 0.006240937393158674, acc: 1.0)
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50][root][INFO] - Training Epoch: 2/2, step 5899/7134 completed (loss: 0.01165834441781044, acc: 1.0)
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50][root][INFO] - Training Epoch: 2/2, step 5900/7134 completed (loss: 0.011529333889484406, acc: 0.9940476417541504)
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50][root][INFO] - Training Epoch: 2/2, step 5901/7134 completed (loss: 0.03784044459462166, acc: 0.9900000095367432)
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:51][root][INFO] - Training Epoch: 2/2, step 5902/7134 completed (loss: 0.042454905807971954, acc: 0.9793814420700073)
[2025-02-13 04:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:51][root][INFO] - Training Epoch: 2/2, step 5903/7134 completed (loss: 0.031025517731904984, acc: 0.9908758997917175)
[2025-02-13 04:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52][root][INFO] - Training Epoch: 2/2, step 5904/7134 completed (loss: 0.028465567156672478, acc: 0.9904371500015259)
[2025-02-13 04:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52][root][INFO] - Training Epoch: 2/2, step 5905/7134 completed (loss: 0.02729869820177555, acc: 0.9916550517082214)
[2025-02-13 04:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52][root][INFO] - Training Epoch: 2/2, step 5906/7134 completed (loss: 0.05301971361041069, acc: 0.9828009605407715)
[2025-02-13 04:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:53][root][INFO] - Training Epoch: 2/2, step 5907/7134 completed (loss: 0.039290349930524826, acc: 0.9870634078979492)
[2025-02-13 04:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:53][root][INFO] - Training Epoch: 2/2, step 5908/7134 completed (loss: 0.020074276253581047, acc: 0.9936061501502991)
[2025-02-13 04:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:54][root][INFO] - Training Epoch: 2/2, step 5909/7134 completed (loss: 0.025189192965626717, acc: 0.9870316982269287)
[2025-02-13 04:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:54][root][INFO] - Training Epoch: 2/2, step 5910/7134 completed (loss: 0.04300997406244278, acc: 0.9833564758300781)
[2025-02-13 04:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55][root][INFO] - Training Epoch: 2/2, step 5911/7134 completed (loss: 0.03526252135634422, acc: 0.9933884143829346)
[2025-02-13 04:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55][root][INFO] - Training Epoch: 2/2, step 5912/7134 completed (loss: 0.020711589604616165, acc: 0.9887482523918152)
[2025-02-13 04:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56][root][INFO] - Training Epoch: 2/2, step 5913/7134 completed (loss: 0.07462568581104279, acc: 0.979619562625885)
[2025-02-13 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56][root][INFO] - Training Epoch: 2/2, step 5914/7134 completed (loss: 0.017860379070043564, acc: 0.9967897534370422)
[2025-02-13 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57][root][INFO] - Training Epoch: 2/2, step 5915/7134 completed (loss: 0.024310356006026268, acc: 0.9953488111495972)
[2025-02-13 04:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57][root][INFO] - Training Epoch: 2/2, step 5916/7134 completed (loss: 0.034047842025756836, acc: 0.9921362996101379)
[2025-02-13 04:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57][root][INFO] - Training Epoch: 2/2, step 5917/7134 completed (loss: 0.035720568150281906, acc: 0.9913344979286194)
[2025-02-13 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58][root][INFO] - Training Epoch: 2/2, step 5918/7134 completed (loss: 0.07804016768932343, acc: 0.9796807169914246)
[2025-02-13 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58][root][INFO] - Training Epoch: 2/2, step 5919/7134 completed (loss: 0.03236501291394234, acc: 0.9924012422561646)
[2025-02-13 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59][root][INFO] - Training Epoch: 2/2, step 5920/7134 completed (loss: 0.024968380108475685, acc: 0.9950082898139954)
[2025-02-13 04:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59][root][INFO] - Training Epoch: 2/2, step 5921/7134 completed (loss: 0.017411252483725548, acc: 0.9937369227409363)
[2025-02-13 04:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59][root][INFO] - Training Epoch: 2/2, step 5922/7134 completed (loss: 0.12969185411930084, acc: 0.9769503474235535)
[2025-02-13 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:00][root][INFO] - Training Epoch: 2/2, step 5923/7134 completed (loss: 0.05614839494228363, acc: 0.9897511005401611)
[2025-02-13 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:00][root][INFO] - Training Epoch: 2/2, step 5924/7134 completed (loss: 0.03356501832604408, acc: 0.9880668520927429)
[2025-02-13 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01][root][INFO] - Training Epoch: 2/2, step 5925/7134 completed (loss: 0.15740495920181274, acc: 0.9677419066429138)
[2025-02-13 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01][root][INFO] - Training Epoch: 2/2, step 5926/7134 completed (loss: 0.06386227160692215, acc: 0.9783333539962769)
[2025-02-13 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01][root][INFO] - Training Epoch: 2/2, step 5927/7134 completed (loss: 0.06699817627668381, acc: 0.9799554347991943)
[2025-02-13 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:02][root][INFO] - Training Epoch: 2/2, step 5928/7134 completed (loss: 0.15432921051979065, acc: 0.9554794430732727)
[2025-02-13 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:02][root][INFO] - Training Epoch: 2/2, step 5929/7134 completed (loss: 0.08492319285869598, acc: 0.9776119589805603)
[2025-02-13 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03][root][INFO] - Training Epoch: 2/2, step 5930/7134 completed (loss: 0.05796302109956741, acc: 0.9888641238212585)
[2025-02-13 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03][root][INFO] - Training Epoch: 2/2, step 5931/7134 completed (loss: 0.03180629014968872, acc: 0.9913606643676758)
[2025-02-13 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03][root][INFO] - Training Epoch: 2/2, step 5932/7134 completed (loss: 0.1005912721157074, acc: 0.9685184955596924)
[2025-02-13 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:04][root][INFO] - Training Epoch: 2/2, step 5933/7134 completed (loss: 0.09885936975479126, acc: 0.9748427867889404)
[2025-02-13 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:04][root][INFO] - Training Epoch: 2/2, step 5934/7134 completed (loss: 0.04487404227256775, acc: 0.9875665903091431)
[2025-02-13 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05][root][INFO] - Training Epoch: 2/2, step 5935/7134 completed (loss: 0.05001504719257355, acc: 0.9864077568054199)
[2025-02-13 04:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05][root][INFO] - Training Epoch: 2/2, step 5936/7134 completed (loss: 0.054542507976293564, acc: 0.9846860766410828)
[2025-02-13 04:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05][root][INFO] - Training Epoch: 2/2, step 5937/7134 completed (loss: 0.022823719307780266, acc: 0.9952152967453003)
[2025-02-13 04:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:06][root][INFO] - Training Epoch: 2/2, step 5938/7134 completed (loss: 0.08715575933456421, acc: 0.9770554304122925)
[2025-02-13 04:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:06][root][INFO] - Training Epoch: 2/2, step 5939/7134 completed (loss: 0.08278045803308487, acc: 0.9773755669593811)
[2025-02-13 04:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07][root][INFO] - Training Epoch: 2/2, step 5940/7134 completed (loss: 0.04294956102967262, acc: 0.9897959232330322)
[2025-02-13 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07][root][INFO] - Training Epoch: 2/2, step 5941/7134 completed (loss: 0.04953126236796379, acc: 0.9906250238418579)
[2025-02-13 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07][root][INFO] - Training Epoch: 2/2, step 5942/7134 completed (loss: 0.013908599503338337, acc: 0.9961758852005005)
[2025-02-13 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:08][root][INFO] - Training Epoch: 2/2, step 5943/7134 completed (loss: 0.01218909677118063, acc: 1.0)
[2025-02-13 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:08][root][INFO] - Training Epoch: 2/2, step 5944/7134 completed (loss: 0.04510863125324249, acc: 0.9829843044281006)
[2025-02-13 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09][root][INFO] - Training Epoch: 2/2, step 5945/7134 completed (loss: 0.03410938009619713, acc: 0.9879879951477051)
[2025-02-13 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09][root][INFO] - Training Epoch: 2/2, step 5946/7134 completed (loss: 0.02859441563487053, acc: 0.9927536249160767)
[2025-02-13 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09][root][INFO] - Training Epoch: 2/2, step 5947/7134 completed (loss: 0.02929222397506237, acc: 0.9900867342948914)
[2025-02-13 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:10][root][INFO] - Training Epoch: 2/2, step 5948/7134 completed (loss: 0.060740575194358826, acc: 0.9760000109672546)
[2025-02-13 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:10][root][INFO] - Training Epoch: 2/2, step 5949/7134 completed (loss: 0.00998308788985014, acc: 0.998745322227478)
[2025-02-13 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11][root][INFO] - Training Epoch: 2/2, step 5950/7134 completed (loss: 0.07467019557952881, acc: 0.978622317314148)
[2025-02-13 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11][root][INFO] - Training Epoch: 2/2, step 5951/7134 completed (loss: 0.04022430628538132, acc: 0.9856528043746948)
[2025-02-13 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12][root][INFO] - Training Epoch: 2/2, step 5952/7134 completed (loss: 0.15521949529647827, acc: 0.9671875238418579)
[2025-02-13 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12][root][INFO] - Training Epoch: 2/2, step 5953/7134 completed (loss: 0.03002876788377762, acc: 0.9917355179786682)
[2025-02-13 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12][root][INFO] - Training Epoch: 2/2, step 5954/7134 completed (loss: 0.022849271073937416, acc: 0.9923858046531677)
[2025-02-13 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][root][INFO] - Training Epoch: 2/2, step 5955/7134 completed (loss: 0.03185727447271347, acc: 0.9952380657196045)
[2025-02-13 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][root][INFO] - Training Epoch: 2/2, step 5956/7134 completed (loss: 0.047869134694337845, acc: 0.9862744808197021)
[2025-02-13 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][root][INFO] - Training Epoch: 2/2, step 5957/7134 completed (loss: 0.051159877330064774, acc: 0.9880059957504272)
[2025-02-13 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:14][root][INFO] - Training Epoch: 2/2, step 5958/7134 completed (loss: 0.029627008363604546, acc: 0.990604043006897)
[2025-02-13 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:14][root][INFO] - Training Epoch: 2/2, step 5959/7134 completed (loss: 0.05501844733953476, acc: 0.9879840016365051)
[2025-02-13 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15][root][INFO] - Training Epoch: 2/2, step 5960/7134 completed (loss: 0.010324235074222088, acc: 0.9957864880561829)
[2025-02-13 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15][root][INFO] - Training Epoch: 2/2, step 5961/7134 completed (loss: 0.027947822585701942, acc: 0.9959016442298889)
[2025-02-13 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15][root][INFO] - Training Epoch: 2/2, step 5962/7134 completed (loss: 0.006950421258807182, acc: 0.9983739852905273)
[2025-02-13 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:16][root][INFO] - Training Epoch: 2/2, step 5963/7134 completed (loss: 0.021284397691488266, acc: 0.99301677942276)
[2025-02-13 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:16][root][INFO] - Training Epoch: 2/2, step 5964/7134 completed (loss: 0.009446796029806137, acc: 0.996303141117096)
[2025-02-13 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17][root][INFO] - Training Epoch: 2/2, step 5965/7134 completed (loss: 0.02437475509941578, acc: 0.9940029978752136)
[2025-02-13 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17][root][INFO] - Training Epoch: 2/2, step 5966/7134 completed (loss: 0.03929679095745087, acc: 0.9889042973518372)
[2025-02-13 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18][root][INFO] - Training Epoch: 2/2, step 5967/7134 completed (loss: 0.019492559134960175, acc: 0.9925373196601868)
[2025-02-13 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18][root][INFO] - Training Epoch: 2/2, step 5968/7134 completed (loss: 0.04182201996445656, acc: 0.9885844588279724)
[2025-02-13 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18][root][INFO] - Training Epoch: 2/2, step 5969/7134 completed (loss: 0.025175318121910095, acc: 0.995356023311615)
[2025-02-13 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19][root][INFO] - Training Epoch: 2/2, step 5970/7134 completed (loss: 0.04847213625907898, acc: 0.993852436542511)
[2025-02-13 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19][root][INFO] - Training Epoch: 2/2, step 5971/7134 completed (loss: 0.01284104771912098, acc: 0.9962406158447266)
[2025-02-13 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20][root][INFO] - Training Epoch: 2/2, step 5972/7134 completed (loss: 0.06921173632144928, acc: 0.988252580165863)
[2025-02-13 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20][root][INFO] - Training Epoch: 2/2, step 5973/7134 completed (loss: 0.06972047686576843, acc: 0.9798561334609985)
[2025-02-13 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20][root][INFO] - Training Epoch: 2/2, step 5974/7134 completed (loss: 0.016085246577858925, acc: 0.9930675625801086)
[2025-02-13 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21][root][INFO] - Training Epoch: 2/2, step 5975/7134 completed (loss: 0.00931225810199976, acc: 0.9970501661300659)
[2025-02-13 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21][root][INFO] - Training Epoch: 2/2, step 5976/7134 completed (loss: 0.014411192387342453, acc: 0.9938650131225586)
[2025-02-13 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:22][root][INFO] - Training Epoch: 2/2, step 5977/7134 completed (loss: 0.014514326117932796, acc: 0.9962825179100037)
[2025-02-13 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:22][root][INFO] - Training Epoch: 2/2, step 5978/7134 completed (loss: 0.020908523350954056, acc: 0.9967948794364929)
[2025-02-13 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23][root][INFO] - Training Epoch: 2/2, step 5979/7134 completed (loss: 0.022701501846313477, acc: 0.9925925731658936)
[2025-02-13 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23][root][INFO] - Training Epoch: 2/2, step 5980/7134 completed (loss: 0.013753730803728104, acc: 0.99615877866745)
[2025-02-13 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23][root][INFO] - Training Epoch: 2/2, step 5981/7134 completed (loss: 0.013022775761783123, acc: 0.9976878762245178)
[2025-02-13 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24][root][INFO] - Training Epoch: 2/2, step 5982/7134 completed (loss: 0.03014860674738884, acc: 0.9920477271080017)
[2025-02-13 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24][root][INFO] - Training Epoch: 2/2, step 5983/7134 completed (loss: 0.038164086639881134, acc: 0.987500011920929)
[2025-02-13 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25][root][INFO] - Training Epoch: 2/2, step 5984/7134 completed (loss: 0.025197455659508705, acc: 0.9914893507957458)
[2025-02-13 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25][root][INFO] - Training Epoch: 2/2, step 5985/7134 completed (loss: 0.027359534054994583, acc: 0.9949937462806702)
[2025-02-13 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26][root][INFO] - Training Epoch: 2/2, step 5986/7134 completed (loss: 0.03845968842506409, acc: 0.9885350465774536)
[2025-02-13 04:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26][root][INFO] - Training Epoch: 2/2, step 5987/7134 completed (loss: 0.04578445851802826, acc: 0.9860140085220337)
[2025-02-13 04:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26][root][INFO] - Training Epoch: 2/2, step 5988/7134 completed (loss: 0.02127453312277794, acc: 0.9933333396911621)
[2025-02-13 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27][root][INFO] - Training Epoch: 2/2, step 5989/7134 completed (loss: 0.030839532613754272, acc: 0.9889298677444458)
[2025-02-13 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27][root][INFO] - Training Epoch: 2/2, step 5990/7134 completed (loss: 0.022934671491384506, acc: 0.9931507110595703)
[2025-02-13 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28][root][INFO] - Training Epoch: 2/2, step 5991/7134 completed (loss: 0.11304152011871338, acc: 0.9804469347000122)
[2025-02-13 04:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28][root][INFO] - Training Epoch: 2/2, step 5992/7134 completed (loss: 0.01507414598017931, acc: 0.996515691280365)
[2025-02-13 04:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28][root][INFO] - Training Epoch: 2/2, step 5993/7134 completed (loss: 0.03217807039618492, acc: 0.9902912378311157)
[2025-02-13 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29][root][INFO] - Training Epoch: 2/2, step 5994/7134 completed (loss: 0.018807969987392426, acc: 0.9942965507507324)
[2025-02-13 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29][root][INFO] - Training Epoch: 2/2, step 5995/7134 completed (loss: 0.006394159514456987, acc: 0.9985915422439575)
[2025-02-13 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:30][root][INFO] - Training Epoch: 2/2, step 5996/7134 completed (loss: 0.048022542148828506, acc: 0.9890453815460205)
[2025-02-13 04:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:30][root][INFO] - Training Epoch: 2/2, step 5997/7134 completed (loss: 0.037858862429857254, acc: 0.9933244585990906)
[2025-02-13 04:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][root][INFO] - Training Epoch: 2/2, step 5998/7134 completed (loss: 0.028120899572968483, acc: 0.9937655925750732)
[2025-02-13 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][root][INFO] - Training Epoch: 2/2, step 5999/7134 completed (loss: 0.022881530225276947, acc: 0.9938042163848877)
[2025-02-13 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][root][INFO] - Training Epoch: 2/2, step 6000/7134 completed (loss: 0.022050591185688972, acc: 0.9900166392326355)
[2025-02-13 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32][root][INFO] - Training Epoch: 2/2, step 6001/7134 completed (loss: 0.00855981931090355, acc: 1.0)
[2025-02-13 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32][root][INFO] - Training Epoch: 2/2, step 6002/7134 completed (loss: 0.04267486557364464, acc: 0.9863547682762146)
[2025-02-13 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33][root][INFO] - Training Epoch: 2/2, step 6003/7134 completed (loss: 0.022363383322954178, acc: 0.9911894202232361)
[2025-02-13 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33][root][INFO] - Training Epoch: 2/2, step 6004/7134 completed (loss: 0.03480665385723114, acc: 0.9916550517082214)
[2025-02-13 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34][root][INFO] - Training Epoch: 2/2, step 6005/7134 completed (loss: 0.024716738611459732, acc: 0.9893454909324646)
[2025-02-13 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34][root][INFO] - Training Epoch: 2/2, step 6006/7134 completed (loss: 0.039153803139925, acc: 0.9864603281021118)
[2025-02-13 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34][root][INFO] - Training Epoch: 2/2, step 6007/7134 completed (loss: 0.01654931902885437, acc: 0.9966611266136169)
[2025-02-13 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:35][root][INFO] - Training Epoch: 2/2, step 6008/7134 completed (loss: 0.03304154798388481, acc: 0.9851632118225098)
[2025-02-13 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:35][root][INFO] - Training Epoch: 2/2, step 6009/7134 completed (loss: 0.03592385724186897, acc: 0.9906832575798035)
[2025-02-13 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:36][root][INFO] - Training Epoch: 2/2, step 6010/7134 completed (loss: 0.052266158163547516, acc: 0.9850948452949524)
[2025-02-13 04:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:36][root][INFO] - Training Epoch: 2/2, step 6011/7134 completed (loss: 0.03683256730437279, acc: 0.9883419871330261)
[2025-02-13 04:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37][root][INFO] - Training Epoch: 2/2, step 6012/7134 completed (loss: 0.03486187756061554, acc: 0.9850746393203735)
[2025-02-13 04:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37][root][INFO] - Training Epoch: 2/2, step 6013/7134 completed (loss: 0.04594210907816887, acc: 0.9852941036224365)
[2025-02-13 04:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37][root][INFO] - Training Epoch: 2/2, step 6014/7134 completed (loss: 0.02497146837413311, acc: 0.9909256100654602)
[2025-02-13 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38][root][INFO] - Training Epoch: 2/2, step 6015/7134 completed (loss: 0.05358206480741501, acc: 0.9876033067703247)
[2025-02-13 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38][root][INFO] - Training Epoch: 2/2, step 6016/7134 completed (loss: 0.010771612636744976, acc: 0.9964157938957214)
[2025-02-13 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38][root][INFO] - Training Epoch: 2/2, step 6017/7134 completed (loss: 0.018316954374313354, acc: 0.9950494766235352)
[2025-02-13 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39][root][INFO] - Training Epoch: 2/2, step 6018/7134 completed (loss: 0.010583067312836647, acc: 1.0)
[2025-02-13 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39][root][INFO] - Training Epoch: 2/2, step 6019/7134 completed (loss: 0.02868196927011013, acc: 0.9913793206214905)
[2025-02-13 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39][root][INFO] - Training Epoch: 2/2, step 6020/7134 completed (loss: 0.002999926218762994, acc: 1.0)
[2025-02-13 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39][root][INFO] - Training Epoch: 2/2, step 6021/7134 completed (loss: 0.0049417526461184025, acc: 1.0)
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40][root][INFO] - Training Epoch: 2/2, step 6022/7134 completed (loss: 0.023742495104670525, acc: 0.9889298677444458)
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40][root][INFO] - Training Epoch: 2/2, step 6023/7134 completed (loss: 0.021108325570821762, acc: 0.9945799708366394)
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40][root][INFO] - Training Epoch: 2/2, step 6024/7134 completed (loss: 0.008432101458311081, acc: 1.0)
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41][root][INFO] - Training Epoch: 2/2, step 6025/7134 completed (loss: 0.013662592507898808, acc: 0.9979079365730286)
[2025-02-13 04:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41][root][INFO] - Training Epoch: 2/2, step 6026/7134 completed (loss: 0.028030332177877426, acc: 0.9885714054107666)
[2025-02-13 04:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41][root][INFO] - Training Epoch: 2/2, step 6027/7134 completed (loss: 0.016942381858825684, acc: 0.9974293112754822)
[2025-02-13 04:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42][root][INFO] - Training Epoch: 2/2, step 6028/7134 completed (loss: 0.008379035629332066, acc: 1.0)
[2025-02-13 04:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42][root][INFO] - Training Epoch: 2/2, step 6029/7134 completed (loss: 0.028750833123922348, acc: 0.9927797913551331)
[2025-02-13 04:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42][root][INFO] - Training Epoch: 2/2, step 6030/7134 completed (loss: 0.007760188542306423, acc: 0.9961685538291931)
[2025-02-13 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43][root][INFO] - Training Epoch: 2/2, step 6031/7134 completed (loss: 0.015035239979624748, acc: 0.9970760345458984)
[2025-02-13 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43][root][INFO] - Training Epoch: 2/2, step 6032/7134 completed (loss: 0.01681254245340824, acc: 0.9974683523178101)
[2025-02-13 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44][root][INFO] - Training Epoch: 2/2, step 6033/7134 completed (loss: 0.07319048792123795, acc: 0.9836065769195557)
[2025-02-13 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44][root][INFO] - Training Epoch: 2/2, step 6034/7134 completed (loss: 0.09245197474956512, acc: 0.9754098653793335)
[2025-02-13 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44][root][INFO] - Training Epoch: 2/2, step 6035/7134 completed (loss: 0.07014058530330658, acc: 0.9766082167625427)
[2025-02-13 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:45][root][INFO] - Training Epoch: 2/2, step 6036/7134 completed (loss: 0.04419255629181862, acc: 0.9874125719070435)
[2025-02-13 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:45][root][INFO] - Training Epoch: 2/2, step 6037/7134 completed (loss: 0.02781461551785469, acc: 0.989830493927002)
[2025-02-13 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:46][root][INFO] - Training Epoch: 2/2, step 6038/7134 completed (loss: 0.02444901503622532, acc: 0.991051435470581)
[2025-02-13 04:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:46][root][INFO] - Training Epoch: 2/2, step 6039/7134 completed (loss: 0.03355962410569191, acc: 0.990510106086731)
[2025-02-13 04:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47][root][INFO] - Training Epoch: 2/2, step 6040/7134 completed (loss: 0.06408195197582245, acc: 0.9833679795265198)
[2025-02-13 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47][root][INFO] - Training Epoch: 2/2, step 6041/7134 completed (loss: 0.036602653563022614, acc: 0.9895150661468506)
[2025-02-13 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47][root][INFO] - Training Epoch: 2/2, step 6042/7134 completed (loss: 0.018105316907167435, acc: 0.9940000176429749)
[2025-02-13 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48][root][INFO] - Training Epoch: 2/2, step 6043/7134 completed (loss: 0.024340633302927017, acc: 0.9928057789802551)
[2025-02-13 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48][root][INFO] - Training Epoch: 2/2, step 6044/7134 completed (loss: 0.05696416273713112, acc: 0.9876543283462524)
[2025-02-13 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49][root][INFO] - Training Epoch: 2/2, step 6045/7134 completed (loss: 0.039255160838365555, acc: 0.9893491268157959)
[2025-02-13 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49][root][INFO] - Training Epoch: 2/2, step 6046/7134 completed (loss: 0.03479258343577385, acc: 0.9861351847648621)
[2025-02-13 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50][root][INFO] - Training Epoch: 2/2, step 6047/7134 completed (loss: 0.09944283962249756, acc: 0.9763912558555603)
[2025-02-13 04:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50][root][INFO] - Training Epoch: 2/2, step 6048/7134 completed (loss: 0.047311607748270035, acc: 0.9839679598808289)
[2025-02-13 04:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50][root][INFO] - Training Epoch: 2/2, step 6049/7134 completed (loss: 0.009531037881970406, acc: 0.9982935190200806)
[2025-02-13 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:51][root][INFO] - Training Epoch: 2/2, step 6050/7134 completed (loss: 0.0561661571264267, acc: 0.9794801473617554)
[2025-02-13 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:51][root][INFO] - Training Epoch: 2/2, step 6051/7134 completed (loss: 0.07030412554740906, acc: 0.9793205261230469)
[2025-02-13 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:52][root][INFO] - Training Epoch: 2/2, step 6052/7134 completed (loss: 0.06060849130153656, acc: 0.9811617136001587)
[2025-02-13 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:52][root][INFO] - Training Epoch: 2/2, step 6053/7134 completed (loss: 0.06621811538934708, acc: 0.9802631735801697)
[2025-02-13 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53][root][INFO] - Training Epoch: 2/2, step 6054/7134 completed (loss: 0.03481985256075859, acc: 0.9927536249160767)
[2025-02-13 04:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53][root][INFO] - Training Epoch: 2/2, step 6055/7134 completed (loss: 0.044422879815101624, acc: 0.9857819676399231)
[2025-02-13 04:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53][root][INFO] - Training Epoch: 2/2, step 6056/7134 completed (loss: 0.04704819247126579, acc: 0.9884792566299438)
[2025-02-13 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:54][root][INFO] - Training Epoch: 2/2, step 6057/7134 completed (loss: 0.03283737227320671, acc: 0.9901315569877625)
[2025-02-13 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:54][root][INFO] - Training Epoch: 2/2, step 6058/7134 completed (loss: 0.010940429754555225, acc: 0.9954954981803894)
[2025-02-13 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55][root][INFO] - Training Epoch: 2/2, step 6059/7134 completed (loss: 0.01473105326294899, acc: 0.9893993139266968)
[2025-02-13 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55][root][INFO] - Training Epoch: 2/2, step 6060/7134 completed (loss: 0.02607165090739727, acc: 0.9894737005233765)
[2025-02-13 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55][root][INFO] - Training Epoch: 2/2, step 6061/7134 completed (loss: 0.06471718847751617, acc: 0.9815100431442261)
[2025-02-13 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:56][root][INFO] - Training Epoch: 2/2, step 6062/7134 completed (loss: 0.052961550652980804, acc: 0.9844961166381836)
[2025-02-13 04:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:56][root][INFO] - Training Epoch: 2/2, step 6063/7134 completed (loss: 0.067728690803051, acc: 0.9819168448448181)
[2025-02-13 04:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:57][root][INFO] - Training Epoch: 2/2, step 6064/7134 completed (loss: 0.02716708742082119, acc: 0.9927641153335571)
[2025-02-13 04:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:57][root][INFO] - Training Epoch: 2/2, step 6065/7134 completed (loss: 0.04451087489724159, acc: 0.9878493547439575)
[2025-02-13 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58][root][INFO] - Training Epoch: 2/2, step 6066/7134 completed (loss: 0.06526504456996918, acc: 0.9736408591270447)
[2025-02-13 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58][root][INFO] - Training Epoch: 2/2, step 6067/7134 completed (loss: 0.04453582316637039, acc: 0.9855700135231018)
[2025-02-13 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59][root][INFO] - Training Epoch: 2/2, step 6068/7134 completed (loss: 0.07304422557353973, acc: 0.9862328171730042)
[2025-02-13 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59][root][INFO] - Training Epoch: 2/2, step 6069/7134 completed (loss: 0.057514503598213196, acc: 0.9810218811035156)
[2025-02-13 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00][root][INFO] - Training Epoch: 2/2, step 6070/7134 completed (loss: 0.07220304012298584, acc: 0.9773635268211365)
[2025-02-13 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00][root][INFO] - Training Epoch: 2/2, step 6071/7134 completed (loss: 0.04414502531290054, acc: 0.9919246435165405)
[2025-02-13 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00][root][INFO] - Training Epoch: 2/2, step 6072/7134 completed (loss: 0.09055909514427185, acc: 0.9768907427787781)
[2025-02-13 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01][root][INFO] - Training Epoch: 2/2, step 6073/7134 completed (loss: 0.04791100695729256, acc: 0.9832167625427246)
[2025-02-13 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01][root][INFO] - Training Epoch: 2/2, step 6074/7134 completed (loss: 0.04042767733335495, acc: 0.9897058606147766)
[2025-02-13 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02][root][INFO] - Training Epoch: 2/2, step 6075/7134 completed (loss: 0.0444556400179863, acc: 0.9892086386680603)
[2025-02-13 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02][root][INFO] - Training Epoch: 2/2, step 6076/7134 completed (loss: 0.05538692697882652, acc: 0.982758641242981)
[2025-02-13 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02][root][INFO] - Training Epoch: 2/2, step 6077/7134 completed (loss: 0.031584665179252625, acc: 0.9904153347015381)
[2025-02-13 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:03][root][INFO] - Training Epoch: 2/2, step 6078/7134 completed (loss: 0.05957058072090149, acc: 0.9873239398002625)
[2025-02-13 04:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:03][root][INFO] - Training Epoch: 2/2, step 6079/7134 completed (loss: 0.02836701273918152, acc: 0.9908758997917175)
[2025-02-13 04:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04][root][INFO] - Training Epoch: 2/2, step 6080/7134 completed (loss: 0.02354332059621811, acc: 0.9915134310722351)
[2025-02-13 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04][root][INFO] - Training Epoch: 2/2, step 6081/7134 completed (loss: 0.022530503571033478, acc: 0.9923664331436157)
[2025-02-13 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04][root][INFO] - Training Epoch: 2/2, step 6082/7134 completed (loss: 0.033586882054805756, acc: 0.993630588054657)
[2025-02-13 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05][root][INFO] - Training Epoch: 2/2, step 6083/7134 completed (loss: 0.011916402727365494, acc: 0.9950860142707825)
[2025-02-13 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05][root][INFO] - Training Epoch: 2/2, step 6084/7134 completed (loss: 0.039409786462783813, acc: 0.9921383857727051)
[2025-02-13 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:06][root][INFO] - Training Epoch: 2/2, step 6085/7134 completed (loss: 0.019284697249531746, acc: 0.9982110857963562)
[2025-02-13 04:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:06][root][INFO] - Training Epoch: 2/2, step 6086/7134 completed (loss: 0.01815456710755825, acc: 0.9957864880561829)
[2025-02-13 04:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:07][root][INFO] - Training Epoch: 2/2, step 6087/7134 completed (loss: 0.05288168415427208, acc: 0.9811320900917053)
[2025-02-13 04:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:07][root][INFO] - Training Epoch: 2/2, step 6088/7134 completed (loss: 0.029575670138001442, acc: 0.9906166195869446)
[2025-02-13 04:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][root][INFO] - Training Epoch: 2/2, step 6089/7134 completed (loss: 0.0348329097032547, acc: 0.9889655113220215)
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][root][INFO] - Training Epoch: 2/2, step 6090/7134 completed (loss: 0.0217753816395998, acc: 0.9958847761154175)
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][root][INFO] - Training Epoch: 2/2, step 6091/7134 completed (loss: 0.037591446191072464, acc: 0.9904458522796631)
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:09][root][INFO] - Training Epoch: 2/2, step 6092/7134 completed (loss: 0.04415982961654663, acc: 0.9841269850730896)
[2025-02-13 04:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:09][root][INFO] - Training Epoch: 2/2, step 6093/7134 completed (loss: 0.05254549905657768, acc: 0.9851577281951904)
[2025-02-13 04:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10][root][INFO] - Training Epoch: 2/2, step 6094/7134 completed (loss: 0.034144096076488495, acc: 0.9901960492134094)
[2025-02-13 04:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10][root][INFO] - Training Epoch: 2/2, step 6095/7134 completed (loss: 0.019652334973216057, acc: 0.9956896305084229)
[2025-02-13 04:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10][root][INFO] - Training Epoch: 2/2, step 6096/7134 completed (loss: 0.04152049869298935, acc: 0.9868735074996948)
[2025-02-13 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:11][root][INFO] - Training Epoch: 2/2, step 6097/7134 completed (loss: 0.03681468963623047, acc: 0.9889196753501892)
[2025-02-13 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:11][root][INFO] - Training Epoch: 2/2, step 6098/7134 completed (loss: 0.02957511693239212, acc: 0.992277979850769)
[2025-02-13 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12][root][INFO] - Training Epoch: 2/2, step 6099/7134 completed (loss: 0.0547141507267952, acc: 0.9892966151237488)
[2025-02-13 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12][root][INFO] - Training Epoch: 2/2, step 6100/7134 completed (loss: 0.06562476605176926, acc: 0.9847561120986938)
[2025-02-13 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12][root][INFO] - Training Epoch: 2/2, step 6101/7134 completed (loss: 0.0163713488727808, acc: 0.9968051314353943)
[2025-02-13 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13][root][INFO] - Training Epoch: 2/2, step 6102/7134 completed (loss: 0.03745397552847862, acc: 0.9911971688270569)
[2025-02-13 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13][root][INFO] - Training Epoch: 2/2, step 6103/7134 completed (loss: 0.0409650020301342, acc: 0.9833333492279053)
[2025-02-13 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14][root][INFO] - Training Epoch: 2/2, step 6104/7134 completed (loss: 0.07114072144031525, acc: 0.9733333587646484)
[2025-02-13 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14][root][INFO] - Training Epoch: 2/2, step 6105/7134 completed (loss: 0.06273166090250015, acc: 0.9836309552192688)
[2025-02-13 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14][root][INFO] - Training Epoch: 2/2, step 6106/7134 completed (loss: 0.03933551535010338, acc: 0.9856887459754944)
[2025-02-13 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:15][root][INFO] - Training Epoch: 2/2, step 6107/7134 completed (loss: 0.05433721840381622, acc: 0.9810924530029297)
[2025-02-13 04:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:15][root][INFO] - Training Epoch: 2/2, step 6108/7134 completed (loss: 0.028180621564388275, acc: 0.9872495532035828)
[2025-02-13 04:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][root][INFO] - Training Epoch: 2/2, step 6109/7134 completed (loss: 0.013801325112581253, acc: 0.9957627058029175)
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][root][INFO] - Training Epoch: 2/2, step 6110/7134 completed (loss: 0.07871069759130478, acc: 0.9764957427978516)
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][root][INFO] - Training Epoch: 2/2, step 6111/7134 completed (loss: 0.06554565578699112, acc: 0.9812332391738892)
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:17][root][INFO] - Training Epoch: 2/2, step 6112/7134 completed (loss: 0.025273632258176804, acc: 0.9899665713310242)
[2025-02-13 04:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:17][root][INFO] - Training Epoch: 2/2, step 6113/7134 completed (loss: 0.021284619346261024, acc: 0.9924127459526062)
[2025-02-13 04:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18][root][INFO] - Training Epoch: 2/2, step 6114/7134 completed (loss: 0.0660814642906189, acc: 0.9818181991577148)
[2025-02-13 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18][root][INFO] - Training Epoch: 2/2, step 6115/7134 completed (loss: 0.04406684264540672, acc: 0.9885057210922241)
[2025-02-13 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18][root][INFO] - Training Epoch: 2/2, step 6116/7134 completed (loss: 0.05572891607880592, acc: 0.9881423115730286)
[2025-02-13 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19][root][INFO] - Training Epoch: 2/2, step 6117/7134 completed (loss: 0.08262919634580612, acc: 0.9836065769195557)
[2025-02-13 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19][root][INFO] - Training Epoch: 2/2, step 6118/7134 completed (loss: 0.045495837926864624, acc: 0.9889705777168274)
[2025-02-13 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20][root][INFO] - Training Epoch: 2/2, step 6119/7134 completed (loss: 0.03387869521975517, acc: 0.9879952073097229)
[2025-02-13 04:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20][root][INFO] - Training Epoch: 2/2, step 6120/7134 completed (loss: 0.02936154045164585, acc: 0.9928443431854248)
[2025-02-13 04:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20][root][INFO] - Training Epoch: 2/2, step 6121/7134 completed (loss: 0.048488833010196686, acc: 0.9874100685119629)
[2025-02-13 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:21][root][INFO] - Training Epoch: 2/2, step 6122/7134 completed (loss: 0.023744840174913406, acc: 0.9905660152435303)
[2025-02-13 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:21][root][INFO] - Training Epoch: 2/2, step 6123/7134 completed (loss: 0.07711227238178253, acc: 0.9792099595069885)
[2025-02-13 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22][root][INFO] - Training Epoch: 2/2, step 6124/7134 completed (loss: 0.02015519328415394, acc: 0.9923076629638672)
[2025-02-13 04:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22][root][INFO] - Training Epoch: 2/2, step 6125/7134 completed (loss: 0.08822812139987946, acc: 0.9783132672309875)
[2025-02-13 04:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22][root][INFO] - Training Epoch: 2/2, step 6126/7134 completed (loss: 0.051505912095308304, acc: 0.9874607920646667)
[2025-02-13 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:23][root][INFO] - Training Epoch: 2/2, step 6127/7134 completed (loss: 0.022459357976913452, acc: 0.9952380657196045)
[2025-02-13 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:23][root][INFO] - Training Epoch: 2/2, step 6128/7134 completed (loss: 0.03006567992269993, acc: 0.9878542423248291)
[2025-02-13 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24][root][INFO] - Training Epoch: 2/2, step 6129/7134 completed (loss: 0.055795952677726746, acc: 0.9803094267845154)
[2025-02-13 04:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24][root][INFO] - Training Epoch: 2/2, step 6130/7134 completed (loss: 0.017390359193086624, acc: 0.9928315281867981)
[2025-02-13 04:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24][root][INFO] - Training Epoch: 2/2, step 6131/7134 completed (loss: 0.03513850271701813, acc: 0.9916167855262756)
[2025-02-13 04:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:25][root][INFO] - Training Epoch: 2/2, step 6132/7134 completed (loss: 0.03606782481074333, acc: 0.989062488079071)
[2025-02-13 04:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:25][root][INFO] - Training Epoch: 2/2, step 6133/7134 completed (loss: 0.0513172522187233, acc: 0.9866504669189453)
[2025-02-13 04:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26][root][INFO] - Training Epoch: 2/2, step 6134/7134 completed (loss: 0.06548979878425598, acc: 0.9800918698310852)
[2025-02-13 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26][root][INFO] - Training Epoch: 2/2, step 6135/7134 completed (loss: 0.011978445574641228, acc: 0.9982578158378601)
[2025-02-13 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26][root][INFO] - Training Epoch: 2/2, step 6136/7134 completed (loss: 0.05344703420996666, acc: 0.994397759437561)
[2025-02-13 04:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:27][root][INFO] - Training Epoch: 2/2, step 6137/7134 completed (loss: 0.06234673410654068, acc: 0.9875195026397705)
[2025-02-13 04:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:27][root][INFO] - Training Epoch: 2/2, step 6138/7134 completed (loss: 0.04841824993491173, acc: 0.9861751198768616)
[2025-02-13 04:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28][root][INFO] - Training Epoch: 2/2, step 6139/7134 completed (loss: 0.04437081515789032, acc: 0.9870370626449585)
[2025-02-13 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28][root][INFO] - Training Epoch: 2/2, step 6140/7134 completed (loss: 0.0313357375562191, acc: 0.9946996569633484)
[2025-02-13 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28][root][INFO] - Training Epoch: 2/2, step 6141/7134 completed (loss: 0.07255120575428009, acc: 0.9877675771713257)
[2025-02-13 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:29][root][INFO] - Training Epoch: 2/2, step 6142/7134 completed (loss: 0.025163494050502777, acc: 0.9884892106056213)
[2025-02-13 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:29][root][INFO] - Training Epoch: 2/2, step 6143/7134 completed (loss: 0.02394871786236763, acc: 0.995945930480957)
[2025-02-13 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30][root][INFO] - Training Epoch: 2/2, step 6144/7134 completed (loss: 0.02300608530640602, acc: 0.9915966391563416)
[2025-02-13 04:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30][root][INFO] - Training Epoch: 2/2, step 6145/7134 completed (loss: 0.057514090090990067, acc: 0.9867899417877197)
[2025-02-13 04:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30][root][INFO] - Training Epoch: 2/2, step 6146/7134 completed (loss: 0.0536905936896801, acc: 0.9849170446395874)
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31][root][INFO] - Training Epoch: 2/2, step 6147/7134 completed (loss: 0.03917417675256729, acc: 0.9900793433189392)
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31][root][INFO] - Training Epoch: 2/2, step 6148/7134 completed (loss: 0.03029595874249935, acc: 0.9865996837615967)
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32][root][INFO] - Training Epoch: 2/2, step 6149/7134 completed (loss: 0.036481693387031555, acc: 0.9871382713317871)
[2025-02-13 04:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32][root][INFO] - Training Epoch: 2/2, step 6150/7134 completed (loss: 0.025483107194304466, acc: 0.9912280440330505)
[2025-02-13 04:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32][root][INFO] - Training Epoch: 2/2, step 6151/7134 completed (loss: 0.02639017440378666, acc: 0.9905808568000793)
[2025-02-13 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33][root][INFO] - Training Epoch: 2/2, step 6152/7134 completed (loss: 0.03623807802796364, acc: 0.9879336357116699)
[2025-02-13 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33][root][INFO] - Training Epoch: 2/2, step 6153/7134 completed (loss: 0.014411336742341518, acc: 0.9971056580543518)
[2025-02-13 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34][root][INFO] - Training Epoch: 2/2, step 6154/7134 completed (loss: 0.03988736495375633, acc: 0.9902371168136597)
[2025-02-13 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34][root][INFO] - Training Epoch: 2/2, step 6155/7134 completed (loss: 0.0360560342669487, acc: 0.9881305694580078)
[2025-02-13 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35][root][INFO] - Training Epoch: 2/2, step 6156/7134 completed (loss: 0.023643607273697853, acc: 0.9936608672142029)
[2025-02-13 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35][root][INFO] - Training Epoch: 2/2, step 6157/7134 completed (loss: 0.023092638701200485, acc: 0.9921630024909973)
[2025-02-13 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35][root][INFO] - Training Epoch: 2/2, step 6158/7134 completed (loss: 0.06277456134557724, acc: 0.9891501069068909)
[2025-02-13 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36][root][INFO] - Training Epoch: 2/2, step 6159/7134 completed (loss: 0.03396762162446976, acc: 0.9891435503959656)
[2025-02-13 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36][root][INFO] - Training Epoch: 2/2, step 6160/7134 completed (loss: 0.06723003089427948, acc: 0.9803030490875244)
[2025-02-13 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37][root][INFO] - Training Epoch: 2/2, step 6161/7134 completed (loss: 0.05406412482261658, acc: 0.9912126660346985)
[2025-02-13 04:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37][root][INFO] - Training Epoch: 2/2, step 6162/7134 completed (loss: 0.043256498873233795, acc: 0.9885433912277222)
[2025-02-13 04:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:38][root][INFO] - Training Epoch: 2/2, step 6163/7134 completed (loss: 0.09623391181230545, acc: 0.9839285612106323)
[2025-02-13 04:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:38][root][INFO] - Training Epoch: 2/2, step 6164/7134 completed (loss: 0.06121859699487686, acc: 0.980169951915741)
[2025-02-13 04:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:38][root][INFO] - Training Epoch: 2/2, step 6165/7134 completed (loss: 0.050741519778966904, acc: 0.9889298677444458)
[2025-02-13 04:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:39][root][INFO] - Training Epoch: 2/2, step 6166/7134 completed (loss: 0.03253205493092537, acc: 0.994727611541748)
[2025-02-13 04:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:39][root][INFO] - Training Epoch: 2/2, step 6167/7134 completed (loss: 0.028336642310023308, acc: 0.9929278492927551)
[2025-02-13 04:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:40][root][INFO] - Training Epoch: 2/2, step 6168/7134 completed (loss: 0.04746314510703087, acc: 0.9858430027961731)
[2025-02-13 04:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:40][root][INFO] - Training Epoch: 2/2, step 6169/7134 completed (loss: 0.04659532755613327, acc: 0.991253674030304)
[2025-02-13 04:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:40][root][INFO] - Training Epoch: 2/2, step 6170/7134 completed (loss: 0.029753651469945908, acc: 0.9943289160728455)
[2025-02-13 04:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:41][root][INFO] - Training Epoch: 2/2, step 6171/7134 completed (loss: 0.010862503200769424, acc: 0.9947437644004822)
[2025-02-13 04:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:41][root][INFO] - Training Epoch: 2/2, step 6172/7134 completed (loss: 0.014012430794537067, acc: 0.9959920048713684)
[2025-02-13 04:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42][root][INFO] - Training Epoch: 2/2, step 6173/7134 completed (loss: 0.012713509611785412, acc: 0.9926362037658691)
[2025-02-13 04:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42][root][INFO] - Training Epoch: 2/2, step 6174/7134 completed (loss: 0.005073453765362501, acc: 0.9985337257385254)
[2025-02-13 04:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42][root][INFO] - Training Epoch: 2/2, step 6175/7134 completed (loss: 0.021637890487909317, acc: 0.9926035404205322)
[2025-02-13 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:43][root][INFO] - Training Epoch: 2/2, step 6176/7134 completed (loss: 0.033049892634153366, acc: 0.986975371837616)
[2025-02-13 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:43][root][INFO] - Training Epoch: 2/2, step 6177/7134 completed (loss: 0.024826327338814735, acc: 0.9897698163986206)
[2025-02-13 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44][root][INFO] - Training Epoch: 2/2, step 6178/7134 completed (loss: 0.018530208617448807, acc: 0.9970501661300659)
[2025-02-13 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44][root][INFO] - Training Epoch: 2/2, step 6179/7134 completed (loss: 0.03147554025053978, acc: 0.9922360181808472)
[2025-02-13 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44][root][INFO] - Training Epoch: 2/2, step 6180/7134 completed (loss: 0.11636032164096832, acc: 0.9681528806686401)
[2025-02-13 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45][root][INFO] - Training Epoch: 2/2, step 6181/7134 completed (loss: 0.22376951575279236, acc: 0.9462810158729553)
[2025-02-13 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45][root][INFO] - Training Epoch: 2/2, step 6182/7134 completed (loss: 0.029231606051325798, acc: 0.9888682961463928)
[2025-02-13 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46][root][INFO] - Training Epoch: 2/2, step 6183/7134 completed (loss: 0.050210755318403244, acc: 0.9892802238464355)
[2025-02-13 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46][root][INFO] - Training Epoch: 2/2, step 6184/7134 completed (loss: 0.08479958772659302, acc: 0.9697580933570862)
[2025-02-13 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46][root][INFO] - Training Epoch: 2/2, step 6185/7134 completed (loss: 0.035698167979717255, acc: 0.9852941036224365)
[2025-02-13 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:47][root][INFO] - Training Epoch: 2/2, step 6186/7134 completed (loss: 0.030693484470248222, acc: 0.9860140085220337)
[2025-02-13 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:47][root][INFO] - Training Epoch: 2/2, step 6187/7134 completed (loss: 0.03433910384774208, acc: 0.9894179701805115)
[2025-02-13 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48][root][INFO] - Training Epoch: 2/2, step 6188/7134 completed (loss: 0.0485810786485672, acc: 0.9816642999649048)
[2025-02-13 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48][root][INFO] - Training Epoch: 2/2, step 6189/7134 completed (loss: 0.08003833144903183, acc: 0.9826839566230774)
[2025-02-13 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49][root][INFO] - Training Epoch: 2/2, step 6190/7134 completed (loss: 0.04923325777053833, acc: 0.985049843788147)
[2025-02-13 04:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49][root][INFO] - Training Epoch: 2/2, step 6191/7134 completed (loss: 0.03991255164146423, acc: 0.9860140085220337)
[2025-02-13 04:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49][root][INFO] - Training Epoch: 2/2, step 6192/7134 completed (loss: 0.03556806966662407, acc: 0.986066460609436)
[2025-02-13 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50][root][INFO] - Training Epoch: 2/2, step 6193/7134 completed (loss: 0.04375900328159332, acc: 0.9869203567504883)
[2025-02-13 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50][root][INFO] - Training Epoch: 2/2, step 6194/7134 completed (loss: 0.03922724723815918, acc: 0.9805447459220886)
[2025-02-13 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:51][root][INFO] - Training Epoch: 2/2, step 6195/7134 completed (loss: 0.04688958451151848, acc: 0.9856459498405457)
[2025-02-13 04:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:51][root][INFO] - Training Epoch: 2/2, step 6196/7134 completed (loss: 0.033298738300800323, acc: 0.9862259030342102)
[2025-02-13 04:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52][root][INFO] - Training Epoch: 2/2, step 6197/7134 completed (loss: 0.07269559800624847, acc: 0.9793103337287903)
[2025-02-13 04:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52][root][INFO] - Training Epoch: 2/2, step 6198/7134 completed (loss: 0.027737688273191452, acc: 0.9896265268325806)
[2025-02-13 04:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52][root][INFO] - Training Epoch: 2/2, step 6199/7134 completed (loss: 0.04605231061577797, acc: 0.9895591735839844)
[2025-02-13 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53][root][INFO] - Training Epoch: 2/2, step 6200/7134 completed (loss: 0.029938513413071632, acc: 0.9902439117431641)
[2025-02-13 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53][root][INFO] - Training Epoch: 2/2, step 6201/7134 completed (loss: 0.03336166962981224, acc: 0.9933110475540161)
[2025-02-13 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54][root][INFO] - Training Epoch: 2/2, step 6202/7134 completed (loss: 0.0349021777510643, acc: 0.989154040813446)
[2025-02-13 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54][root][INFO] - Training Epoch: 2/2, step 6203/7134 completed (loss: 0.0181571114808321, acc: 0.9961734414100647)
[2025-02-13 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55][root][INFO] - Training Epoch: 2/2, step 6204/7134 completed (loss: 0.00991908460855484, acc: 1.0)
[2025-02-13 04:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55][root][INFO] - Training Epoch: 2/2, step 6205/7134 completed (loss: 0.02666986547410488, acc: 0.9931740760803223)
[2025-02-13 04:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55][root][INFO] - Training Epoch: 2/2, step 6206/7134 completed (loss: 0.025446448475122452, acc: 0.9884225726127625)
[2025-02-13 04:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56][root][INFO] - Training Epoch: 2/2, step 6207/7134 completed (loss: 0.11001909524202347, acc: 0.9719626307487488)
[2025-02-13 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56][root][INFO] - Training Epoch: 2/2, step 6208/7134 completed (loss: 0.034320201724767685, acc: 0.9872685074806213)
[2025-02-13 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:57][root][INFO] - Training Epoch: 2/2, step 6209/7134 completed (loss: 0.04211768880486488, acc: 0.9844760894775391)
[2025-02-13 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:57][root][INFO] - Training Epoch: 2/2, step 6210/7134 completed (loss: 0.024700237438082695, acc: 0.9912087917327881)
[2025-02-13 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58][root][INFO] - Training Epoch: 2/2, step 6211/7134 completed (loss: 0.0373108945786953, acc: 0.9891745448112488)
[2025-02-13 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58][root][INFO] - Training Epoch: 2/2, step 6212/7134 completed (loss: 0.024777112528681755, acc: 0.9964912533760071)
[2025-02-13 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58][root][INFO] - Training Epoch: 2/2, step 6213/7134 completed (loss: 0.01748381368815899, acc: 0.9937629699707031)
[2025-02-13 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:59][root][INFO] - Training Epoch: 2/2, step 6214/7134 completed (loss: 0.016617029905319214, acc: 0.9948652386665344)
[2025-02-13 04:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:59][root][INFO] - Training Epoch: 2/2, step 6215/7134 completed (loss: 0.03307413309812546, acc: 0.9884792566299438)
[2025-02-13 04:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00][root][INFO] - Training Epoch: 2/2, step 6216/7134 completed (loss: 0.01349595282226801, acc: 0.9978723526000977)
[2025-02-13 04:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00][root][INFO] - Training Epoch: 2/2, step 6217/7134 completed (loss: 0.033527590334415436, acc: 0.9932885766029358)
[2025-02-13 04:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00][root][INFO] - Training Epoch: 2/2, step 6218/7134 completed (loss: 0.025418588891625404, acc: 0.9893292784690857)
[2025-02-13 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:01][root][INFO] - Training Epoch: 2/2, step 6219/7134 completed (loss: 0.019375918433070183, acc: 0.9943740963935852)
[2025-02-13 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:01][root][INFO] - Training Epoch: 2/2, step 6220/7134 completed (loss: 0.041595932096242905, acc: 0.9833333492279053)
[2025-02-13 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02][root][INFO] - Training Epoch: 2/2, step 6221/7134 completed (loss: 0.007687130477279425, acc: 0.9964912533760071)
[2025-02-13 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02][root][INFO] - Training Epoch: 2/2, step 6222/7134 completed (loss: 0.04274782910943031, acc: 0.9890710115432739)
[2025-02-13 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02][root][INFO] - Training Epoch: 2/2, step 6223/7134 completed (loss: 0.03324558585882187, acc: 0.9914529919624329)
[2025-02-13 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:03][root][INFO] - Training Epoch: 2/2, step 6224/7134 completed (loss: 0.006721996236592531, acc: 0.9985486268997192)
[2025-02-13 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:03][root][INFO] - Training Epoch: 2/2, step 6225/7134 completed (loss: 0.020536724478006363, acc: 0.9936224222183228)
[2025-02-13 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04][root][INFO] - Training Epoch: 2/2, step 6226/7134 completed (loss: 0.03197813779115677, acc: 0.9855832457542419)
[2025-02-13 04:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04][root][INFO] - Training Epoch: 2/2, step 6227/7134 completed (loss: 0.007829824462532997, acc: 0.9973226189613342)
[2025-02-13 04:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05][root][INFO] - Training Epoch: 2/2, step 6228/7134 completed (loss: 0.021347902715206146, acc: 0.9901719689369202)
[2025-02-13 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05][root][INFO] - Training Epoch: 2/2, step 6229/7134 completed (loss: 0.06586574763059616, acc: 0.9837586879730225)
[2025-02-13 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05][root][INFO] - Training Epoch: 2/2, step 6230/7134 completed (loss: 0.027086148038506508, acc: 0.9900249242782593)
[2025-02-13 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:06][root][INFO] - Training Epoch: 2/2, step 6231/7134 completed (loss: 0.013799281790852547, acc: 0.9974160194396973)
[2025-02-13 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:06][root][INFO] - Training Epoch: 2/2, step 6232/7134 completed (loss: 0.016658242791891098, acc: 0.9949495196342468)
[2025-02-13 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:07][root][INFO] - Training Epoch: 2/2, step 6233/7134 completed (loss: 0.022174622863531113, acc: 0.9906166195869446)
[2025-02-13 04:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:07][root][INFO] - Training Epoch: 2/2, step 6234/7134 completed (loss: 0.04239005222916603, acc: 0.9826202988624573)
[2025-02-13 04:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08][root][INFO] - Training Epoch: 2/2, step 6235/7134 completed (loss: 0.021358177065849304, acc: 0.9934810996055603)
[2025-02-13 04:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08][root][INFO] - Training Epoch: 2/2, step 6236/7134 completed (loss: 0.013893834315240383, acc: 0.9959072470664978)
[2025-02-13 04:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08][root][INFO] - Training Epoch: 2/2, step 6237/7134 completed (loss: 0.023230280727148056, acc: 0.9923547506332397)
[2025-02-13 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:09][root][INFO] - Training Epoch: 2/2, step 6238/7134 completed (loss: 0.006174084730446339, acc: 1.0)
[2025-02-13 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:09][root][INFO] - Training Epoch: 2/2, step 6239/7134 completed (loss: 0.03312677890062332, acc: 0.9928315281867981)
[2025-02-13 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:10][root][INFO] - Training Epoch: 2/2, step 6240/7134 completed (loss: 0.003883151337504387, acc: 0.9985895752906799)
[2025-02-13 04:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:10][root][INFO] - Training Epoch: 2/2, step 6241/7134 completed (loss: 0.024725381284952164, acc: 0.9936708807945251)
[2025-02-13 04:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11][root][INFO] - Training Epoch: 2/2, step 6242/7134 completed (loss: 0.010374198667705059, acc: 0.9965986609458923)
[2025-02-13 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11][root][INFO] - Training Epoch: 2/2, step 6243/7134 completed (loss: 0.017088860273361206, acc: 0.993630588054657)
[2025-02-13 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11][root][INFO] - Training Epoch: 2/2, step 6244/7134 completed (loss: 0.011276413686573505, acc: 0.9986282587051392)
[2025-02-13 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:12][root][INFO] - Training Epoch: 2/2, step 6245/7134 completed (loss: 0.019014356657862663, acc: 0.99452805519104)
[2025-02-13 04:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:12][root][INFO] - Training Epoch: 2/2, step 6246/7134 completed (loss: 0.017799818888306618, acc: 0.992094874382019)
[2025-02-13 04:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13][root][INFO] - Training Epoch: 2/2, step 6247/7134 completed (loss: 0.023931629955768585, acc: 0.9891566038131714)
[2025-02-13 04:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13][root][INFO] - Training Epoch: 2/2, step 6248/7134 completed (loss: 0.03863900154829025, acc: 0.9875389337539673)
[2025-02-13 04:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13][root][INFO] - Training Epoch: 2/2, step 6249/7134 completed (loss: 0.011105109937489033, acc: 0.9952830076217651)
[2025-02-13 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:14][root][INFO] - Training Epoch: 2/2, step 6250/7134 completed (loss: 0.002468930324539542, acc: 1.0)
[2025-02-13 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:14][root][INFO] - Training Epoch: 2/2, step 6251/7134 completed (loss: 0.01738807000219822, acc: 0.995398759841919)
[2025-02-13 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15][root][INFO] - Training Epoch: 2/2, step 6252/7134 completed (loss: 0.010477891191840172, acc: 0.9983818531036377)
[2025-02-13 04:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15][root][INFO] - Training Epoch: 2/2, step 6253/7134 completed (loss: 0.015455617569386959, acc: 0.9965517520904541)
[2025-02-13 04:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15][root][INFO] - Training Epoch: 2/2, step 6254/7134 completed (loss: 0.024070559069514275, acc: 0.9948096871376038)
[2025-02-13 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:16][root][INFO] - Training Epoch: 2/2, step 6255/7134 completed (loss: 0.015314129181206226, acc: 0.9945799708366394)
[2025-02-13 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:16][root][INFO] - Training Epoch: 2/2, step 6256/7134 completed (loss: 0.006954109761863947, acc: 0.9971631169319153)
[2025-02-13 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17][root][INFO] - Training Epoch: 2/2, step 6257/7134 completed (loss: 0.02041046880185604, acc: 0.9956834316253662)
[2025-02-13 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17][root][INFO] - Training Epoch: 2/2, step 6258/7134 completed (loss: 0.009625720791518688, acc: 0.9968000054359436)
[2025-02-13 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18][root][INFO] - Training Epoch: 2/2, step 6259/7134 completed (loss: 0.013351642526686192, acc: 0.9939939975738525)
[2025-02-13 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18][root][INFO] - Training Epoch: 2/2, step 6260/7134 completed (loss: 0.023350294679403305, acc: 0.9916107654571533)
[2025-02-13 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18][root][INFO] - Training Epoch: 2/2, step 6261/7134 completed (loss: 0.018942201510071754, acc: 0.9954268336296082)
[2025-02-13 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19][root][INFO] - Training Epoch: 2/2, step 6262/7134 completed (loss: 0.007877119816839695, acc: 0.9971305727958679)
[2025-02-13 04:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19][root][INFO] - Training Epoch: 2/2, step 6263/7134 completed (loss: 0.002355156699195504, acc: 1.0)
[2025-02-13 04:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20][root][INFO] - Training Epoch: 2/2, step 6264/7134 completed (loss: 0.005748264025896788, acc: 0.9967159032821655)
[2025-02-13 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20][root][INFO] - Training Epoch: 2/2, step 6265/7134 completed (loss: 0.008058596402406693, acc: 0.9985207319259644)
[2025-02-13 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20][root][INFO] - Training Epoch: 2/2, step 6266/7134 completed (loss: 0.008323081769049168, acc: 0.9947552680969238)
[2025-02-13 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:21][root][INFO] - Training Epoch: 2/2, step 6267/7134 completed (loss: 0.019021030515432358, acc: 0.9951140284538269)
[2025-02-13 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:21][root][INFO] - Training Epoch: 2/2, step 6268/7134 completed (loss: 0.016749931499361992, acc: 0.9985119104385376)
[2025-02-13 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22][root][INFO] - Training Epoch: 2/2, step 6269/7134 completed (loss: 0.016035208478569984, acc: 0.9957355856895447)
[2025-02-13 04:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22][root][INFO] - Training Epoch: 2/2, step 6270/7134 completed (loss: 0.003583001671358943, acc: 0.9984662532806396)
[2025-02-13 04:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22][root][INFO] - Training Epoch: 2/2, step 6271/7134 completed (loss: 0.017583714798092842, acc: 0.9973261952400208)
[2025-02-13 04:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:23][root][INFO] - Training Epoch: 2/2, step 6272/7134 completed (loss: 0.006308054085820913, acc: 0.9985315799713135)
[2025-02-13 04:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:23][root][INFO] - Training Epoch: 2/2, step 6273/7134 completed (loss: 0.007907673716545105, acc: 0.99863201379776)
[2025-02-13 04:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:24][root][INFO] - Training Epoch: 2/2, step 6274/7134 completed (loss: 0.012462439015507698, acc: 0.9971469044685364)
[2025-02-13 04:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:24][root][INFO] - Training Epoch: 2/2, step 6275/7134 completed (loss: 0.03032318875193596, acc: 0.9939117431640625)
[2025-02-13 04:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25][root][INFO] - Training Epoch: 2/2, step 6276/7134 completed (loss: 0.02052241563796997, acc: 0.9947460889816284)
[2025-02-13 04:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25][root][INFO] - Training Epoch: 2/2, step 6277/7134 completed (loss: 0.04893502965569496, acc: 0.9841628670692444)
[2025-02-13 04:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25][root][INFO] - Training Epoch: 2/2, step 6278/7134 completed (loss: 0.004020349122583866, acc: 1.0)
[2025-02-13 04:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26][root][INFO] - Training Epoch: 2/2, step 6279/7134 completed (loss: 0.008044998161494732, acc: 0.9982876777648926)
[2025-02-13 04:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26][root][INFO] - Training Epoch: 2/2, step 6280/7134 completed (loss: 0.018085341900587082, acc: 0.9895287752151489)
[2025-02-13 04:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26][root][INFO] - Training Epoch: 2/2, step 6281/7134 completed (loss: 0.013980680145323277, acc: 0.9965338110923767)
[2025-02-13 04:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:27][root][INFO] - Training Epoch: 2/2, step 6282/7134 completed (loss: 0.00658661313354969, acc: 0.9968553185462952)
[2025-02-13 04:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:27][root][INFO] - Training Epoch: 2/2, step 6283/7134 completed (loss: 0.03271874040365219, acc: 0.9912663698196411)
[2025-02-13 04:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28][root][INFO] - Training Epoch: 2/2, step 6284/7134 completed (loss: 0.04198772460222244, acc: 0.9824945330619812)
[2025-02-13 04:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28][root][INFO] - Training Epoch: 2/2, step 6285/7134 completed (loss: 0.015405896119773388, acc: 0.9947506785392761)
[2025-02-13 04:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28][root][INFO] - Training Epoch: 2/2, step 6286/7134 completed (loss: 0.007620569784194231, acc: 0.9967105388641357)
[2025-02-13 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:29][root][INFO] - Training Epoch: 2/2, step 6287/7134 completed (loss: 0.0241011343896389, acc: 0.9926470518112183)
[2025-02-13 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:29][root][INFO] - Training Epoch: 2/2, step 6288/7134 completed (loss: 0.006910381838679314, acc: 0.9967319965362549)
[2025-02-13 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30][root][INFO] - Training Epoch: 2/2, step 6289/7134 completed (loss: 0.022946452721953392, acc: 0.9945945739746094)
[2025-02-13 04:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30][root][INFO] - Training Epoch: 2/2, step 6290/7134 completed (loss: 0.01220074761658907, acc: 0.9983999729156494)
[2025-02-13 04:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30][root][INFO] - Training Epoch: 2/2, step 6291/7134 completed (loss: 0.008968334645032883, acc: 0.996363639831543)
[2025-02-13 04:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:31][root][INFO] - Training Epoch: 2/2, step 6292/7134 completed (loss: 0.026752755045890808, acc: 0.9908883571624756)
[2025-02-13 04:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:31][root][INFO] - Training Epoch: 2/2, step 6293/7134 completed (loss: 0.047537077218294144, acc: 0.988054633140564)
[2025-02-13 04:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32][root][INFO] - Training Epoch: 2/2, step 6294/7134 completed (loss: 0.022327808663249016, acc: 0.9935170412063599)
[2025-02-13 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32][root][INFO] - Training Epoch: 2/2, step 6295/7134 completed (loss: 0.034943051636219025, acc: 0.9873417615890503)
[2025-02-13 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32][root][INFO] - Training Epoch: 2/2, step 6296/7134 completed (loss: 0.04920971766114235, acc: 0.9862204790115356)
[2025-02-13 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33][root][INFO] - Training Epoch: 2/2, step 6297/7134 completed (loss: 0.024736564606428146, acc: 0.9901639223098755)
[2025-02-13 04:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33][root][INFO] - Training Epoch: 2/2, step 6298/7134 completed (loss: 0.030690934509038925, acc: 0.9862542748451233)
[2025-02-13 04:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34][root][INFO] - Training Epoch: 2/2, step 6299/7134 completed (loss: 0.05600309371948242, acc: 0.9824281334877014)
[2025-02-13 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34][root][INFO] - Training Epoch: 2/2, step 6300/7134 completed (loss: 0.03529735654592514, acc: 0.9882746934890747)
[2025-02-13 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34][root][INFO] - Training Epoch: 2/2, step 6301/7134 completed (loss: 0.010018154978752136, acc: 0.9965517520904541)
[2025-02-13 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35][root][INFO] - Training Epoch: 2/2, step 6302/7134 completed (loss: 0.0069381012581288815, acc: 0.9972375631332397)
[2025-02-13 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35][root][INFO] - Training Epoch: 2/2, step 6303/7134 completed (loss: 0.009353927336633205, acc: 0.9989350438117981)
[2025-02-13 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:36][root][INFO] - Training Epoch: 2/2, step 6304/7134 completed (loss: 0.02429029904305935, acc: 0.9891892075538635)
[2025-02-13 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:36][root][INFO] - Training Epoch: 2/2, step 6305/7134 completed (loss: 0.007729758974164724, acc: 0.9983633160591125)
[2025-02-13 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37][root][INFO] - Training Epoch: 2/2, step 6306/7134 completed (loss: 0.03687620162963867, acc: 0.9903961420059204)
[2025-02-13 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37][root][INFO] - Training Epoch: 2/2, step 6307/7134 completed (loss: 0.04084886237978935, acc: 0.9947090148925781)
[2025-02-13 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37][root][INFO] - Training Epoch: 2/2, step 6308/7134 completed (loss: 0.01796380802989006, acc: 0.9950860142707825)
[2025-02-13 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:38][root][INFO] - Training Epoch: 2/2, step 6309/7134 completed (loss: 0.011832904070615768, acc: 0.995199978351593)
[2025-02-13 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:38][root][INFO] - Training Epoch: 2/2, step 6310/7134 completed (loss: 0.028761940076947212, acc: 0.9890109896659851)
[2025-02-13 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:39][root][INFO] - Training Epoch: 2/2, step 6311/7134 completed (loss: 0.019702546298503876, acc: 0.9963099360466003)
[2025-02-13 04:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:39][root][INFO] - Training Epoch: 2/2, step 6312/7134 completed (loss: 0.021881770342588425, acc: 0.9937106966972351)
[2025-02-13 04:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40][root][INFO] - Training Epoch: 2/2, step 6313/7134 completed (loss: 0.014148307032883167, acc: 0.9950920343399048)
[2025-02-13 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40][root][INFO] - Training Epoch: 2/2, step 6314/7134 completed (loss: 0.013326470740139484, acc: 0.9987389445304871)
[2025-02-13 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40][root][INFO] - Training Epoch: 2/2, step 6315/7134 completed (loss: 0.0406268872320652, acc: 0.9918032884597778)
[2025-02-13 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:41][root][INFO] - Training Epoch: 2/2, step 6316/7134 completed (loss: 0.020016593858599663, acc: 0.9967585206031799)
[2025-02-13 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:41][root][INFO] - Training Epoch: 2/2, step 6317/7134 completed (loss: 0.00759587436914444, acc: 1.0)
[2025-02-13 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:42][root][INFO] - Training Epoch: 2/2, step 6318/7134 completed (loss: 0.008889660239219666, acc: 0.995230495929718)
[2025-02-13 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:42][root][INFO] - Training Epoch: 2/2, step 6319/7134 completed (loss: 0.009794789366424084, acc: 0.9969135522842407)
[2025-02-13 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43][root][INFO] - Training Epoch: 2/2, step 6320/7134 completed (loss: 0.014780459925532341, acc: 0.9936808943748474)
[2025-02-13 04:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43][root][INFO] - Training Epoch: 2/2, step 6321/7134 completed (loss: 0.0071501159109175205, acc: 0.9966517686843872)
[2025-02-13 04:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43][root][INFO] - Training Epoch: 2/2, step 6322/7134 completed (loss: 0.02901303768157959, acc: 0.992732584476471)
[2025-02-13 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:44][root][INFO] - Training Epoch: 2/2, step 6323/7134 completed (loss: 0.013724388554692268, acc: 0.9954954981803894)
[2025-02-13 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:44][root][INFO] - Training Epoch: 2/2, step 6324/7134 completed (loss: 0.013819477520883083, acc: 0.9946737885475159)
[2025-02-13 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:45][root][INFO] - Training Epoch: 2/2, step 6325/7134 completed (loss: 0.013795791193842888, acc: 0.99245285987854)
[2025-02-13 04:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:45][root][INFO] - Training Epoch: 2/2, step 6326/7134 completed (loss: 0.026955392211675644, acc: 0.9919742941856384)
[2025-02-13 04:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46][root][INFO] - Training Epoch: 2/2, step 6327/7134 completed (loss: 0.017943818122148514, acc: 0.9920844435691833)
[2025-02-13 04:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46][root][INFO] - Training Epoch: 2/2, step 6328/7134 completed (loss: 0.02011192962527275, acc: 0.9920254945755005)
[2025-02-13 04:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46][root][INFO] - Training Epoch: 2/2, step 6329/7134 completed (loss: 0.016381550580263138, acc: 0.9954545497894287)
[2025-02-13 04:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:47][root][INFO] - Training Epoch: 2/2, step 6330/7134 completed (loss: 0.011171934194862843, acc: 0.9950371980667114)
[2025-02-13 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:47][root][INFO] - Training Epoch: 2/2, step 6331/7134 completed (loss: 0.0122739989310503, acc: 0.9946996569633484)
[2025-02-13 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48][root][INFO] - Training Epoch: 2/2, step 6332/7134 completed (loss: 0.04692074656486511, acc: 0.9851301312446594)
[2025-02-13 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48][root][INFO] - Training Epoch: 2/2, step 6333/7134 completed (loss: 0.03187226504087448, acc: 0.9888888597488403)
[2025-02-13 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48][root][INFO] - Training Epoch: 2/2, step 6334/7134 completed (loss: 0.0288643017411232, acc: 0.9905303120613098)
[2025-02-13 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:49][root][INFO] - Training Epoch: 2/2, step 6335/7134 completed (loss: 0.008250122889876366, acc: 0.994966447353363)
[2025-02-13 04:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:49][root][INFO] - Training Epoch: 2/2, step 6336/7134 completed (loss: 0.02280012145638466, acc: 0.9908257126808167)
[2025-02-13 04:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50][root][INFO] - Training Epoch: 2/2, step 6337/7134 completed (loss: 0.00828301627188921, acc: 0.9981752038002014)
[2025-02-13 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50][root][INFO] - Training Epoch: 2/2, step 6338/7134 completed (loss: 0.019925272092223167, acc: 0.9921011328697205)
[2025-02-13 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50][root][INFO] - Training Epoch: 2/2, step 6339/7134 completed (loss: 0.013796770013868809, acc: 0.9948096871376038)
[2025-02-13 04:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:51][root][INFO] - Training Epoch: 2/2, step 6340/7134 completed (loss: 0.015486722812056541, acc: 0.9941349029541016)
[2025-02-13 04:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:51][root][INFO] - Training Epoch: 2/2, step 6341/7134 completed (loss: 0.008136818185448647, acc: 0.9981024861335754)
[2025-02-13 04:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:52][root][INFO] - Training Epoch: 2/2, step 6342/7134 completed (loss: 0.021125294268131256, acc: 0.9905787110328674)
[2025-02-13 04:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:52][root][INFO] - Training Epoch: 2/2, step 6343/7134 completed (loss: 0.029505150392651558, acc: 0.9948365092277527)
[2025-02-13 04:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53][root][INFO] - Training Epoch: 2/2, step 6344/7134 completed (loss: 0.010756237432360649, acc: 0.99726402759552)
[2025-02-13 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53][root][INFO] - Training Epoch: 2/2, step 6345/7134 completed (loss: 0.01710464060306549, acc: 0.9918032884597778)
[2025-02-13 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53][root][INFO] - Training Epoch: 2/2, step 6346/7134 completed (loss: 0.05121269449591637, acc: 0.9860681295394897)
[2025-02-13 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:54][root][INFO] - Training Epoch: 2/2, step 6347/7134 completed (loss: 0.019971659407019615, acc: 0.9978355169296265)
[2025-02-13 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:54][root][INFO] - Training Epoch: 2/2, step 6348/7134 completed (loss: 0.03623310849070549, acc: 0.9913194179534912)
[2025-02-13 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55][root][INFO] - Training Epoch: 2/2, step 6349/7134 completed (loss: 0.028130099177360535, acc: 0.9890710115432739)
[2025-02-13 04:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55][root][INFO] - Training Epoch: 2/2, step 6350/7134 completed (loss: 0.04162263497710228, acc: 0.9904761910438538)
[2025-02-13 04:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55][root][INFO] - Training Epoch: 2/2, step 6351/7134 completed (loss: 0.0384005606174469, acc: 0.9895522594451904)
[2025-02-13 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:56][root][INFO] - Training Epoch: 2/2, step 6352/7134 completed (loss: 0.10844049602746964, acc: 0.9747399687767029)
[2025-02-13 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:56][root][INFO] - Training Epoch: 2/2, step 6353/7134 completed (loss: 0.018886037170886993, acc: 0.9926793575286865)
[2025-02-13 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57][root][INFO] - Training Epoch: 2/2, step 6354/7134 completed (loss: 0.0154586061835289, acc: 0.9944598078727722)
[2025-02-13 04:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57][root][INFO] - Training Epoch: 2/2, step 6355/7134 completed (loss: 0.005677244625985622, acc: 0.9983818531036377)
[2025-02-13 04:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57][root][INFO] - Training Epoch: 2/2, step 6356/7134 completed (loss: 0.021698039025068283, acc: 0.9910314083099365)
[2025-02-13 04:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:58][root][INFO] - Training Epoch: 2/2, step 6357/7134 completed (loss: 0.007236808072775602, acc: 0.9958333373069763)
[2025-02-13 04:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:58][root][INFO] - Training Epoch: 2/2, step 6358/7134 completed (loss: 0.019596518948674202, acc: 0.9943661689758301)
[2025-02-13 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:59][root][INFO] - Training Epoch: 2/2, step 6359/7134 completed (loss: 0.03743361681699753, acc: 0.9915966391563416)
[2025-02-13 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:59][root][INFO] - Training Epoch: 2/2, step 6360/7134 completed (loss: 0.01625794917345047, acc: 0.9952229261398315)
[2025-02-13 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00][root][INFO] - Training Epoch: 2/2, step 6361/7134 completed (loss: 0.04470004513859749, acc: 0.9872262477874756)
[2025-02-13 04:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00][root][INFO] - Training Epoch: 2/2, step 6362/7134 completed (loss: 0.018795793876051903, acc: 0.9954268336296082)
[2025-02-13 04:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00][root][INFO] - Training Epoch: 2/2, step 6363/7134 completed (loss: 0.025125721469521523, acc: 0.9941349029541016)
[2025-02-13 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:01][root][INFO] - Training Epoch: 2/2, step 6364/7134 completed (loss: 0.008418502286076546, acc: 0.9985693693161011)
[2025-02-13 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:01][root][INFO] - Training Epoch: 2/2, step 6365/7134 completed (loss: 0.014437953941524029, acc: 0.9918699264526367)
[2025-02-13 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02][root][INFO] - Training Epoch: 2/2, step 6366/7134 completed (loss: 0.023518767207860947, acc: 0.9947159886360168)
[2025-02-13 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02][root][INFO] - Training Epoch: 2/2, step 6367/7134 completed (loss: 0.03789478540420532, acc: 0.9905481934547424)
[2025-02-13 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02][root][INFO] - Training Epoch: 2/2, step 6368/7134 completed (loss: 0.06464484333992004, acc: 0.9844632744789124)
[2025-02-13 04:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:03][root][INFO] - Training Epoch: 2/2, step 6369/7134 completed (loss: 0.032748207449913025, acc: 0.9908015727996826)
[2025-02-13 04:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:03][root][INFO] - Training Epoch: 2/2, step 6370/7134 completed (loss: 0.09323328733444214, acc: 0.9749340415000916)
[2025-02-13 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04][root][INFO] - Training Epoch: 2/2, step 6371/7134 completed (loss: 0.039557717740535736, acc: 0.9875346422195435)
[2025-02-13 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04][root][INFO] - Training Epoch: 2/2, step 6372/7134 completed (loss: 0.09528354555368423, acc: 0.9750480055809021)
[2025-02-13 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05][root][INFO] - Training Epoch: 2/2, step 6373/7134 completed (loss: 0.04375794529914856, acc: 0.9836065769195557)
[2025-02-13 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05][root][INFO] - Training Epoch: 2/2, step 6374/7134 completed (loss: 0.01215589139610529, acc: 0.9958506226539612)
[2025-02-13 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05][root][INFO] - Training Epoch: 2/2, step 6375/7134 completed (loss: 0.04463644698262215, acc: 0.9858793616294861)
[2025-02-13 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:06][root][INFO] - Training Epoch: 2/2, step 6376/7134 completed (loss: 0.01727323792874813, acc: 0.9926199316978455)
[2025-02-13 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:06][root][INFO] - Training Epoch: 2/2, step 6377/7134 completed (loss: 0.03227395564317703, acc: 0.9908424615859985)
[2025-02-13 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:07][root][INFO] - Training Epoch: 2/2, step 6378/7134 completed (loss: 0.04770948737859726, acc: 0.984994649887085)
[2025-02-13 04:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:07][root][INFO] - Training Epoch: 2/2, step 6379/7134 completed (loss: 0.02025494910776615, acc: 0.9949238300323486)
[2025-02-13 04:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08][root][INFO] - Training Epoch: 2/2, step 6380/7134 completed (loss: 0.039566680788993835, acc: 0.9910179376602173)
[2025-02-13 04:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08][root][INFO] - Training Epoch: 2/2, step 6381/7134 completed (loss: 0.024051718413829803, acc: 0.9938931465148926)
[2025-02-13 04:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08][root][INFO] - Training Epoch: 2/2, step 6382/7134 completed (loss: 0.0041788555681705475, acc: 1.0)
[2025-02-13 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:09][root][INFO] - Training Epoch: 2/2, step 6383/7134 completed (loss: 0.06264010071754456, acc: 0.9891451597213745)
[2025-02-13 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:09][root][INFO] - Training Epoch: 2/2, step 6384/7134 completed (loss: 0.024077314883470535, acc: 0.994020938873291)
[2025-02-13 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:10][root][INFO] - Training Epoch: 2/2, step 6385/7134 completed (loss: 0.07044297456741333, acc: 0.9868263602256775)
[2025-02-13 04:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:10][root][INFO] - Training Epoch: 2/2, step 6386/7134 completed (loss: 0.008089973591268063, acc: 0.9987789988517761)
[2025-02-13 04:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11][root][INFO] - Training Epoch: 2/2, step 6387/7134 completed (loss: 0.03983457013964653, acc: 0.9903225898742676)
[2025-02-13 04:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11][root][INFO] - Training Epoch: 2/2, step 6388/7134 completed (loss: 0.041303884238004684, acc: 0.9880239367485046)
[2025-02-13 04:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11][root][INFO] - Training Epoch: 2/2, step 6389/7134 completed (loss: 0.05558760464191437, acc: 0.9841269850730896)
[2025-02-13 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:12][root][INFO] - Training Epoch: 2/2, step 6390/7134 completed (loss: 0.05396227166056633, acc: 0.9848178029060364)
[2025-02-13 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:12][root][INFO] - Training Epoch: 2/2, step 6391/7134 completed (loss: 0.07429493218660355, acc: 0.9820742607116699)
[2025-02-13 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13][root][INFO] - Training Epoch: 2/2, step 6392/7134 completed (loss: 0.05198125168681145, acc: 0.9846153855323792)
[2025-02-13 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13][root][INFO] - Training Epoch: 2/2, step 6393/7134 completed (loss: 0.017671843990683556, acc: 0.9943116903305054)
[2025-02-13 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:14][root][INFO] - Training Epoch: 2/2, step 6394/7134 completed (loss: 0.05652706325054169, acc: 0.9847009778022766)
[2025-02-13 04:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:14][root][INFO] - Training Epoch: 2/2, step 6395/7134 completed (loss: 0.02728705108165741, acc: 0.9950799345970154)
[2025-02-13 04:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:15][root][INFO] - Training Epoch: 2/2, step 6396/7134 completed (loss: 0.05244091525673866, acc: 0.9873853325843811)
[2025-02-13 04:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:15][root][INFO] - Training Epoch: 2/2, step 6397/7134 completed (loss: 0.022136522457003593, acc: 0.9925558567047119)
[2025-02-13 04:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16][root][INFO] - Training Epoch: 2/2, step 6398/7134 completed (loss: 0.025994278490543365, acc: 0.9887482523918152)
[2025-02-13 04:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16][root][INFO] - Training Epoch: 2/2, step 6399/7134 completed (loss: 0.05162499099969864, acc: 0.9849624037742615)
[2025-02-13 04:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16][root][INFO] - Training Epoch: 2/2, step 6400/7134 completed (loss: 0.021399976685643196, acc: 0.995726466178894)
[2025-02-13 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:17][root][INFO] - Training Epoch: 2/2, step 6401/7134 completed (loss: 0.033717185258865356, acc: 0.9913138151168823)
[2025-02-13 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:17][root][INFO] - Training Epoch: 2/2, step 6402/7134 completed (loss: 0.042766496539115906, acc: 0.9909194111824036)
[2025-02-13 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:18][root][INFO] - Training Epoch: 2/2, step 6403/7134 completed (loss: 0.03220918029546738, acc: 0.9897959232330322)
[2025-02-13 04:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:18][root][INFO] - Training Epoch: 2/2, step 6404/7134 completed (loss: 0.03201715648174286, acc: 0.9930459260940552)
[2025-02-13 04:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:19][root][INFO] - Training Epoch: 2/2, step 6405/7134 completed (loss: 0.016541535034775734, acc: 0.9946236610412598)
[2025-02-13 04:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:19][root][INFO] - Training Epoch: 2/2, step 6406/7134 completed (loss: 0.030244912952184677, acc: 0.9920182228088379)
[2025-02-13 04:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20][root][INFO] - Training Epoch: 2/2, step 6407/7134 completed (loss: 0.022414647042751312, acc: 0.9934297204017639)
[2025-02-13 04:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20][root][INFO] - Training Epoch: 2/2, step 6408/7134 completed (loss: 0.05187042057514191, acc: 0.9881831407546997)
[2025-02-13 04:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20][root][INFO] - Training Epoch: 2/2, step 6409/7134 completed (loss: 0.03185208886861801, acc: 0.9947368502616882)
[2025-02-13 04:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:21][root][INFO] - Training Epoch: 2/2, step 6410/7134 completed (loss: 0.01629028096795082, acc: 0.9908088445663452)
[2025-02-13 04:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:21][root][INFO] - Training Epoch: 2/2, step 6411/7134 completed (loss: 0.031821198761463165, acc: 0.9899497628211975)
[2025-02-13 04:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22][root][INFO] - Training Epoch: 2/2, step 6412/7134 completed (loss: 0.009155631996691227, acc: 0.9973333477973938)
[2025-02-13 04:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22][root][INFO] - Training Epoch: 2/2, step 6413/7134 completed (loss: 0.044803496450185776, acc: 0.9863013625144958)
[2025-02-13 04:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22][root][INFO] - Training Epoch: 2/2, step 6414/7134 completed (loss: 0.008726615458726883, acc: 0.9984894394874573)
[2025-02-13 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:23][root][INFO] - Training Epoch: 2/2, step 6415/7134 completed (loss: 0.02134620025753975, acc: 0.9910141229629517)
[2025-02-13 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:23][root][INFO] - Training Epoch: 2/2, step 6416/7134 completed (loss: 0.03209520876407623, acc: 0.9886147975921631)
[2025-02-13 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24][root][INFO] - Training Epoch: 2/2, step 6417/7134 completed (loss: 0.06176779419183731, acc: 0.982425332069397)
[2025-02-13 04:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24][root][INFO] - Training Epoch: 2/2, step 6418/7134 completed (loss: 0.08414675295352936, acc: 0.976123571395874)
[2025-02-13 04:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24][root][INFO] - Training Epoch: 2/2, step 6419/7134 completed (loss: 0.050880398601293564, acc: 0.9869494438171387)
[2025-02-13 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:25][root][INFO] - Training Epoch: 2/2, step 6420/7134 completed (loss: 0.02989240549504757, acc: 0.9899135231971741)
[2025-02-13 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:25][root][INFO] - Training Epoch: 2/2, step 6421/7134 completed (loss: 0.028136201202869415, acc: 0.9910846948623657)
[2025-02-13 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:26][root][INFO] - Training Epoch: 2/2, step 6422/7134 completed (loss: 0.033694107085466385, acc: 0.9890561103820801)
[2025-02-13 04:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:26][root][INFO] - Training Epoch: 2/2, step 6423/7134 completed (loss: 0.04932323470711708, acc: 0.9912023544311523)
[2025-02-13 04:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:27][root][INFO] - Training Epoch: 2/2, step 6424/7134 completed (loss: 0.0361650176346302, acc: 0.9861286282539368)
[2025-02-13 04:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:27][root][INFO] - Training Epoch: 2/2, step 6425/7134 completed (loss: 0.02817661501467228, acc: 0.9895697236061096)
[2025-02-13 04:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28][root][INFO] - Training Epoch: 2/2, step 6426/7134 completed (loss: 0.03041740693151951, acc: 0.9907651543617249)
[2025-02-13 04:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28][root][INFO] - Training Epoch: 2/2, step 6427/7134 completed (loss: 0.04807319492101669, acc: 0.9872159361839294)
[2025-02-13 04:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28][root][INFO] - Training Epoch: 2/2, step 6428/7134 completed (loss: 0.01951027102768421, acc: 0.9916201233863831)
[2025-02-13 04:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:29][root][INFO] - Training Epoch: 2/2, step 6429/7134 completed (loss: 0.013479134067893028, acc: 0.9939393997192383)
[2025-02-13 04:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:29][root][INFO] - Training Epoch: 2/2, step 6430/7134 completed (loss: 0.04681658744812012, acc: 0.9841726422309875)
[2025-02-13 04:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30][root][INFO] - Training Epoch: 2/2, step 6431/7134 completed (loss: 0.05687427148222923, acc: 0.9868420958518982)
[2025-02-13 04:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30][root][INFO] - Training Epoch: 2/2, step 6432/7134 completed (loss: 0.1434166580438614, acc: 0.9696969985961914)
[2025-02-13 04:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30][root][INFO] - Training Epoch: 2/2, step 6433/7134 completed (loss: 0.03192271664738655, acc: 0.9904371500015259)
[2025-02-13 04:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:31][root][INFO] - Training Epoch: 2/2, step 6434/7134 completed (loss: 0.006171346642076969, acc: 1.0)
[2025-02-13 04:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:31][root][INFO] - Training Epoch: 2/2, step 6435/7134 completed (loss: 0.04207273945212364, acc: 0.9878683090209961)
[2025-02-13 04:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:32][root][INFO] - Training Epoch: 2/2, step 6436/7134 completed (loss: 0.01160958968102932, acc: 0.9953632354736328)
[2025-02-13 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:32][root][INFO] - Training Epoch: 2/2, step 6437/7134 completed (loss: 0.032231081277132034, acc: 0.9916666746139526)
[2025-02-13 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33][root][INFO] - Training Epoch: 2/2, step 6438/7134 completed (loss: 0.032926592975854874, acc: 0.990111231803894)
[2025-02-13 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33][root][INFO] - Training Epoch: 2/2, step 6439/7134 completed (loss: 0.03868551924824715, acc: 0.98740154504776)
[2025-02-13 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33][root][INFO] - Training Epoch: 2/2, step 6440/7134 completed (loss: 0.018381234258413315, acc: 0.9952210187911987)
[2025-02-13 04:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:34][root][INFO] - Training Epoch: 2/2, step 6441/7134 completed (loss: 0.016650421544909477, acc: 0.9955686926841736)
[2025-02-13 04:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:34][root][INFO] - Training Epoch: 2/2, step 6442/7134 completed (loss: 0.025325417518615723, acc: 0.9954596757888794)
[2025-02-13 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35][root][INFO] - Training Epoch: 2/2, step 6443/7134 completed (loss: 0.016489116474986076, acc: 0.9950494766235352)
[2025-02-13 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35][root][INFO] - Training Epoch: 2/2, step 6444/7134 completed (loss: 0.027191825211048126, acc: 0.9916201233863831)
[2025-02-13 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:36][root][INFO] - Training Epoch: 2/2, step 6445/7134 completed (loss: 0.0438726432621479, acc: 0.9889025688171387)
[2025-02-13 04:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:36][root][INFO] - Training Epoch: 2/2, step 6446/7134 completed (loss: 0.018654363229870796, acc: 0.9964243173599243)
[2025-02-13 04:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37][root][INFO] - Training Epoch: 2/2, step 6447/7134 completed (loss: 0.028216000646352768, acc: 0.9907940030097961)
[2025-02-13 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37][root][INFO] - Training Epoch: 2/2, step 6448/7134 completed (loss: 0.08040868490934372, acc: 0.9828660488128662)
[2025-02-13 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37][root][INFO] - Training Epoch: 2/2, step 6449/7134 completed (loss: 0.01748006045818329, acc: 0.9971014261245728)
[2025-02-13 04:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:38][root][INFO] - Training Epoch: 2/2, step 6450/7134 completed (loss: 0.020153861492872238, acc: 0.9940476417541504)
[2025-02-13 04:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:38][root][INFO] - Training Epoch: 2/2, step 6451/7134 completed (loss: 0.03635656088590622, acc: 0.9882467985153198)
[2025-02-13 04:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:39][root][INFO] - Training Epoch: 2/2, step 6452/7134 completed (loss: 0.013660815544426441, acc: 0.9955406785011292)
[2025-02-13 04:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:39][root][INFO] - Training Epoch: 2/2, step 6453/7134 completed (loss: 0.016369551420211792, acc: 0.9946236610412598)
[2025-02-13 04:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:40][root][INFO] - Training Epoch: 2/2, step 6454/7134 completed (loss: 0.03036162070930004, acc: 0.9952885508537292)
[2025-02-13 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:40][root][INFO] - Training Epoch: 2/2, step 6455/7134 completed (loss: 0.0166765246540308, acc: 0.9966044425964355)
[2025-02-13 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41][root][INFO] - Training Epoch: 2/2, step 6456/7134 completed (loss: 0.022528717294335365, acc: 0.9938650131225586)
[2025-02-13 04:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41][root][INFO] - Training Epoch: 2/2, step 6457/7134 completed (loss: 0.00799653772264719, acc: 0.9977973699569702)
[2025-02-13 04:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41][root][INFO] - Training Epoch: 2/2, step 6458/7134 completed (loss: 0.015541906468570232, acc: 0.9949579834938049)
[2025-02-13 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:42][root][INFO] - Training Epoch: 2/2, step 6459/7134 completed (loss: 0.001342636183835566, acc: 1.0)
[2025-02-13 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:42][root][INFO] - Training Epoch: 2/2, step 6460/7134 completed (loss: 0.005072919651865959, acc: 0.9976387023925781)
[2025-02-13 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:43][root][INFO] - Training Epoch: 2/2, step 6461/7134 completed (loss: 0.00957176461815834, acc: 0.9973718523979187)
[2025-02-13 04:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:43][root][INFO] - Training Epoch: 2/2, step 6462/7134 completed (loss: 0.012210684828460217, acc: 0.9975399971008301)
[2025-02-13 04:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:44][root][INFO] - Training Epoch: 2/2, step 6463/7134 completed (loss: 0.015989400446414948, acc: 0.9954441785812378)
[2025-02-13 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:44][root][INFO] - Training Epoch: 2/2, step 6464/7134 completed (loss: 0.015412507578730583, acc: 0.9947984218597412)
[2025-02-13 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45][root][INFO] - Training Epoch: 2/2, step 6465/7134 completed (loss: 0.016870029270648956, acc: 0.9961439371109009)
[2025-02-13 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45][root][INFO] - Training Epoch: 2/2, step 6466/7134 completed (loss: 0.013034248724579811, acc: 0.9960370063781738)
[2025-02-13 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45][root][INFO] - Training Epoch: 2/2, step 6467/7134 completed (loss: 0.028775181621313095, acc: 0.9923664331436157)
[2025-02-13 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:46][root][INFO] - Training Epoch: 2/2, step 6468/7134 completed (loss: 0.0194683987647295, acc: 0.9906191229820251)
[2025-02-13 04:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:46][root][INFO] - Training Epoch: 2/2, step 6469/7134 completed (loss: 0.03022024780511856, acc: 0.9914737939834595)
[2025-02-13 04:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:47][root][INFO] - Training Epoch: 2/2, step 6470/7134 completed (loss: 0.052607908844947815, acc: 0.9853300452232361)
[2025-02-13 04:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:47][root][INFO] - Training Epoch: 2/2, step 6471/7134 completed (loss: 0.011841967701911926, acc: 0.9951140284538269)
[2025-02-13 04:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48][root][INFO] - Training Epoch: 2/2, step 6472/7134 completed (loss: 0.05855241045355797, acc: 0.9893617033958435)
[2025-02-13 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48][root][INFO] - Training Epoch: 2/2, step 6473/7134 completed (loss: 0.010361145250499249, acc: 0.9959785342216492)
[2025-02-13 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48][root][INFO] - Training Epoch: 2/2, step 6474/7134 completed (loss: 0.012642978690564632, acc: 0.9959623217582703)
[2025-02-13 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49][root][INFO] - Training Epoch: 2/2, step 6475/7134 completed (loss: 0.024075714871287346, acc: 0.9950000047683716)
[2025-02-13 04:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49][root][INFO] - Training Epoch: 2/2, step 6476/7134 completed (loss: 0.04555632174015045, acc: 0.9933993220329285)
[2025-02-13 04:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49][root][INFO] - Training Epoch: 2/2, step 6477/7134 completed (loss: 0.02942681685090065, acc: 0.9936440587043762)
[2025-02-13 04:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:50][root][INFO] - Training Epoch: 2/2, step 6478/7134 completed (loss: 0.012679321691393852, acc: 0.9944649338722229)
[2025-02-13 04:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:50][root][INFO] - Training Epoch: 2/2, step 6479/7134 completed (loss: 0.011410431005060673, acc: 0.9966499209403992)
[2025-02-13 04:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:51][root][INFO] - Training Epoch: 2/2, step 6480/7134 completed (loss: 0.041881293058395386, acc: 0.9845361113548279)
[2025-02-13 04:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:51][root][INFO] - Training Epoch: 2/2, step 6481/7134 completed (loss: 0.009151384234428406, acc: 0.9982993006706238)
[2025-02-13 04:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52][root][INFO] - Training Epoch: 2/2, step 6482/7134 completed (loss: 0.012446019798517227, acc: 0.9950000047683716)
[2025-02-13 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52][root][INFO] - Training Epoch: 2/2, step 6483/7134 completed (loss: 0.04790746048092842, acc: 0.9909747242927551)
[2025-02-13 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52][root][INFO] - Training Epoch: 2/2, step 6484/7134 completed (loss: 0.026358747854828835, acc: 0.992668628692627)
[2025-02-13 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:53][root][INFO] - Training Epoch: 2/2, step 6485/7134 completed (loss: 0.009837224148213863, acc: 0.9980236887931824)
[2025-02-13 04:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:53][root][INFO] - Training Epoch: 2/2, step 6486/7134 completed (loss: 0.08616713434457779, acc: 0.9818181991577148)
[2025-02-13 04:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54][root][INFO] - Training Epoch: 2/2, step 6487/7134 completed (loss: 0.0984826311469078, acc: 0.97773277759552)
[2025-02-13 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54][root][INFO] - Training Epoch: 2/2, step 6488/7134 completed (loss: 0.06694163382053375, acc: 0.9758551120758057)
[2025-02-13 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54][root][INFO] - Training Epoch: 2/2, step 6489/7134 completed (loss: 0.07841498404741287, acc: 0.9813432693481445)
[2025-02-13 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:55][root][INFO] - Training Epoch: 2/2, step 6490/7134 completed (loss: 0.04166239872574806, acc: 0.9931389093399048)
[2025-02-13 04:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:55][root][INFO] - Training Epoch: 2/2, step 6491/7134 completed (loss: 0.060745470225811005, acc: 0.9865047335624695)
[2025-02-13 04:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56][root][INFO] - Training Epoch: 2/2, step 6492/7134 completed (loss: 0.01688913255929947, acc: 0.9961190223693848)
[2025-02-13 04:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56][root][INFO] - Training Epoch: 2/2, step 6493/7134 completed (loss: 0.05414612218737602, acc: 0.9872773289680481)
[2025-02-13 04:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56][root][INFO] - Training Epoch: 2/2, step 6494/7134 completed (loss: 0.025980588048696518, acc: 0.9963099360466003)
[2025-02-13 04:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:57][root][INFO] - Training Epoch: 2/2, step 6495/7134 completed (loss: 0.036566440016031265, acc: 0.9859747290611267)
[2025-02-13 04:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:57][root][INFO] - Training Epoch: 2/2, step 6496/7134 completed (loss: 0.02809540182352066, acc: 0.99370276927948)
[2025-02-13 04:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:58][root][INFO] - Training Epoch: 2/2, step 6497/7134 completed (loss: 0.05236188322305679, acc: 0.9813374876976013)
[2025-02-13 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:58][root][INFO] - Training Epoch: 2/2, step 6498/7134 completed (loss: 0.01433639507740736, acc: 0.995121955871582)
[2025-02-13 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59][root][INFO] - Training Epoch: 2/2, step 6499/7134 completed (loss: 0.023630859330296516, acc: 0.9900373816490173)
[2025-02-13 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59][root][INFO] - Training Epoch: 2/2, step 6500/7134 completed (loss: 0.055817216634750366, acc: 0.9831932783126831)
[2025-02-13 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59][root][INFO] - Training Epoch: 2/2, step 6501/7134 completed (loss: 0.08276265859603882, acc: 0.9769821166992188)
[2025-02-13 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:00][root][INFO] - Training Epoch: 2/2, step 6502/7134 completed (loss: 0.022743260487914085, acc: 0.9940617680549622)
[2025-02-13 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:00][root][INFO] - Training Epoch: 2/2, step 6503/7134 completed (loss: 0.03609517961740494, acc: 0.9882698059082031)
[2025-02-13 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:01][root][INFO] - Training Epoch: 2/2, step 6504/7134 completed (loss: 0.037225835025310516, acc: 0.9905213117599487)
[2025-02-13 04:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:01][root][INFO] - Training Epoch: 2/2, step 6505/7134 completed (loss: 0.058658499270677567, acc: 0.9826086759567261)
[2025-02-13 04:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02][root][INFO] - Training Epoch: 2/2, step 6506/7134 completed (loss: 0.05186828225851059, acc: 0.9837133288383484)
[2025-02-13 04:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02][root][INFO] - Training Epoch: 2/2, step 6507/7134 completed (loss: 0.056936804205179214, acc: 0.9848713874816895)
[2025-02-13 04:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02][root][INFO] - Training Epoch: 2/2, step 6508/7134 completed (loss: 0.05454812943935394, acc: 0.9906103014945984)
[2025-02-13 04:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:03][root][INFO] - Training Epoch: 2/2, step 6509/7134 completed (loss: 0.0285366028547287, acc: 0.9900332093238831)
[2025-02-13 04:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:03][root][INFO] - Training Epoch: 2/2, step 6510/7134 completed (loss: 0.03751759231090546, acc: 0.9883720874786377)
[2025-02-13 04:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:04][root][INFO] - Training Epoch: 2/2, step 6511/7134 completed (loss: 0.04252662509679794, acc: 0.9879699349403381)
[2025-02-13 04:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:04][root][INFO] - Training Epoch: 2/2, step 6512/7134 completed (loss: 0.015877028927206993, acc: 0.9961977005004883)
[2025-02-13 04:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05][root][INFO] - Training Epoch: 2/2, step 6513/7134 completed (loss: 0.036035940051078796, acc: 0.9891975522041321)
[2025-02-13 04:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05][root][INFO] - Training Epoch: 2/2, step 6514/7134 completed (loss: 0.04402058944106102, acc: 0.9886363744735718)
[2025-02-13 04:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05][root][INFO] - Training Epoch: 2/2, step 6515/7134 completed (loss: 0.032336048781871796, acc: 0.9885786771774292)
[2025-02-13 04:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:06][root][INFO] - Training Epoch: 2/2, step 6516/7134 completed (loss: 0.06370166689157486, acc: 0.9840849041938782)
[2025-02-13 04:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:06][root][INFO] - Training Epoch: 2/2, step 6517/7134 completed (loss: 0.022087406367063522, acc: 0.9947368502616882)
[2025-02-13 04:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:07][root][INFO] - Training Epoch: 2/2, step 6518/7134 completed (loss: 0.01753990724682808, acc: 0.9944827556610107)
[2025-02-13 04:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:07][root][INFO] - Training Epoch: 2/2, step 6519/7134 completed (loss: 0.025335902348160744, acc: 0.9929078221321106)
[2025-02-13 04:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08][root][INFO] - Training Epoch: 2/2, step 6520/7134 completed (loss: 0.011531827040016651, acc: 0.9984447956085205)
[2025-02-13 04:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08][root][INFO] - Training Epoch: 2/2, step 6521/7134 completed (loss: 0.014183237217366695, acc: 0.9947916865348816)
[2025-02-13 04:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08][root][INFO] - Training Epoch: 2/2, step 6522/7134 completed (loss: 0.03462034836411476, acc: 0.9914236664772034)
[2025-02-13 04:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:09][root][INFO] - Training Epoch: 2/2, step 6523/7134 completed (loss: 0.06868179142475128, acc: 0.9818511605262756)
[2025-02-13 04:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:09][root][INFO] - Training Epoch: 2/2, step 6524/7134 completed (loss: 0.0625949427485466, acc: 0.9858611822128296)
[2025-02-13 04:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10][root][INFO] - Training Epoch: 2/2, step 6525/7134 completed (loss: 0.011826340109109879, acc: 0.9939758777618408)
[2025-02-13 04:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10][root][INFO] - Training Epoch: 2/2, step 6526/7134 completed (loss: 0.05851699039340019, acc: 0.9923175573348999)
[2025-02-13 04:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10][root][INFO] - Training Epoch: 2/2, step 6527/7134 completed (loss: 0.03347023203969002, acc: 0.9955489635467529)
[2025-02-13 04:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:11][root][INFO] - Training Epoch: 2/2, step 6528/7134 completed (loss: 0.055118851363658905, acc: 0.9891696572303772)
[2025-02-13 04:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:11][root][INFO] - Training Epoch: 2/2, step 6529/7134 completed (loss: 0.08455932140350342, acc: 0.981203019618988)
[2025-02-13 04:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12][root][INFO] - Training Epoch: 2/2, step 6530/7134 completed (loss: 0.02679951675236225, acc: 0.9914407730102539)
[2025-02-13 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12][root][INFO] - Training Epoch: 2/2, step 6531/7134 completed (loss: 0.01154467836022377, acc: 0.9956647157669067)
[2025-02-13 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12][root][INFO] - Training Epoch: 2/2, step 6532/7134 completed (loss: 0.05359729006886482, acc: 0.9887359142303467)
[2025-02-13 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:13][root][INFO] - Training Epoch: 2/2, step 6533/7134 completed (loss: 0.015416081063449383, acc: 0.9955223798751831)
[2025-02-13 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:13][root][INFO] - Training Epoch: 2/2, step 6534/7134 completed (loss: 0.05902579426765442, acc: 0.989276111125946)
[2025-02-13 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14][root][INFO] - Training Epoch: 2/2, step 6535/7134 completed (loss: 0.014755581505596638, acc: 1.0)
[2025-02-13 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14][root][INFO] - Training Epoch: 2/2, step 6536/7134 completed (loss: 0.026232954114675522, acc: 0.9900793433189392)
[2025-02-13 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14][root][INFO] - Training Epoch: 2/2, step 6537/7134 completed (loss: 0.07311902195215225, acc: 0.971238911151886)
[2025-02-13 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15][root][INFO] - Training Epoch: 2/2, step 6538/7134 completed (loss: 0.061359718441963196, acc: 0.980461835861206)
[2025-02-13 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15][root][INFO] - Training Epoch: 2/2, step 6539/7134 completed (loss: 0.0064498321153223515, acc: 0.9969879388809204)
[2025-02-13 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15][root][INFO] - Training Epoch: 2/2, step 6540/7134 completed (loss: 0.18794021010398865, acc: 0.9562841653823853)
[2025-02-13 04:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:16][root][INFO] - Training Epoch: 2/2, step 6541/7134 completed (loss: 0.033025894314050674, acc: 0.9877451062202454)
[2025-02-13 04:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:16][root][INFO] - Training Epoch: 2/2, step 6542/7134 completed (loss: 0.02885623276233673, acc: 0.991525411605835)
[2025-02-13 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:17][root][INFO] - Training Epoch: 2/2, step 6543/7134 completed (loss: 0.008282365277409554, acc: 0.99790358543396)
[2025-02-13 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:17][root][INFO] - Training Epoch: 2/2, step 6544/7134 completed (loss: 0.050966255366802216, acc: 0.9880715608596802)
[2025-02-13 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18][root][INFO] - Training Epoch: 2/2, step 6545/7134 completed (loss: 0.013858424499630928, acc: 0.9962825179100037)
[2025-02-13 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18][root][INFO] - Training Epoch: 2/2, step 6546/7134 completed (loss: 0.02848462015390396, acc: 0.990227997303009)
[2025-02-13 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18][root][INFO] - Training Epoch: 2/2, step 6547/7134 completed (loss: 0.09524571150541306, acc: 0.9812606573104858)
[2025-02-13 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:19][root][INFO] - Training Epoch: 2/2, step 6548/7134 completed (loss: 0.011732777580618858, acc: 0.9960317611694336)
[2025-02-13 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:19][root][INFO] - Training Epoch: 2/2, step 6549/7134 completed (loss: 0.07796359062194824, acc: 0.9864077568054199)
[2025-02-13 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20][root][INFO] - Training Epoch: 2/2, step 6550/7134 completed (loss: 0.0623125396668911, acc: 0.9800000190734863)
[2025-02-13 04:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20][root][INFO] - Training Epoch: 2/2, step 6551/7134 completed (loss: 0.011774001643061638, acc: 0.9965831637382507)
[2025-02-13 04:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20][root][INFO] - Training Epoch: 2/2, step 6552/7134 completed (loss: 0.0349406898021698, acc: 0.9920318722724915)
[2025-02-13 04:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:21][root][INFO] - Training Epoch: 2/2, step 6553/7134 completed (loss: 0.017882486805319786, acc: 0.9936386942863464)
[2025-02-13 04:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:21][root][INFO] - Training Epoch: 2/2, step 6554/7134 completed (loss: 0.02782673016190529, acc: 0.9956756830215454)
[2025-02-13 04:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:22][root][INFO] - Training Epoch: 2/2, step 6555/7134 completed (loss: 0.04078302159905434, acc: 0.9909456968307495)
[2025-02-13 04:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:22][root][INFO] - Training Epoch: 2/2, step 6556/7134 completed (loss: 0.034809064120054245, acc: 0.9905263185501099)
[2025-02-13 04:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:23][root][INFO] - Training Epoch: 2/2, step 6557/7134 completed (loss: 0.025874750688672066, acc: 0.9939516186714172)
[2025-02-13 04:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:23][root][INFO] - Training Epoch: 2/2, step 6558/7134 completed (loss: 0.03127564862370491, acc: 0.991304337978363)
[2025-02-13 04:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:24][root][INFO] - Training Epoch: 2/2, step 6559/7134 completed (loss: 0.022536035627126694, acc: 0.9972260594367981)
[2025-02-13 04:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:24][root][INFO] - Training Epoch: 2/2, step 6560/7134 completed (loss: 0.01844080351293087, acc: 0.9940546751022339)
[2025-02-13 04:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:25][root][INFO] - Training Epoch: 2/2, step 6561/7134 completed (loss: 0.015278195030987263, acc: 0.9929824471473694)
[2025-02-13 04:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:25][root][INFO] - Training Epoch: 2/2, step 6562/7134 completed (loss: 0.02965833619236946, acc: 0.9910314083099365)
[2025-02-13 04:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26][root][INFO] - Training Epoch: 2/2, step 6563/7134 completed (loss: 0.03373101353645325, acc: 0.9883585572242737)
[2025-02-13 04:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26][root][INFO] - Training Epoch: 2/2, step 6564/7134 completed (loss: 0.039882637560367584, acc: 0.9892037510871887)
[2025-02-13 04:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26][root][INFO] - Training Epoch: 2/2, step 6565/7134 completed (loss: 0.033206161111593246, acc: 0.9894291758537292)
[2025-02-13 04:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:27][root][INFO] - Training Epoch: 2/2, step 6566/7134 completed (loss: 0.01794215850532055, acc: 0.9962013363838196)
[2025-02-13 04:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:27][root][INFO] - Training Epoch: 2/2, step 6567/7134 completed (loss: 0.012470557354390621, acc: 0.9959473013877869)
[2025-02-13 04:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:28][root][INFO] - Training Epoch: 2/2, step 6568/7134 completed (loss: 0.017140399664640427, acc: 0.9936948418617249)
[2025-02-13 04:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:28][root][INFO] - Training Epoch: 2/2, step 6569/7134 completed (loss: 0.010229011066257954, acc: 0.997802197933197)
[2025-02-13 04:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:29][root][INFO] - Training Epoch: 2/2, step 6570/7134 completed (loss: 0.012537708505988121, acc: 0.9976771473884583)
[2025-02-13 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:29][root][INFO] - Training Epoch: 2/2, step 6571/7134 completed (loss: 0.011071240529417992, acc: 0.9973822236061096)
[2025-02-13 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:30][root][INFO] - Training Epoch: 2/2, step 6572/7134 completed (loss: 0.0391206368803978, acc: 0.9900221824645996)
[2025-02-13 04:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:30][root][INFO] - Training Epoch: 2/2, step 6573/7134 completed (loss: 0.019934438169002533, acc: 0.9920993447303772)
[2025-02-13 04:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31][root][INFO] - Training Epoch: 2/2, step 6574/7134 completed (loss: 0.0138443848118186, acc: 0.9948875308036804)
[2025-02-13 04:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31][root][INFO] - Training Epoch: 2/2, step 6575/7134 completed (loss: 0.025484666228294373, acc: 0.9922651648521423)
[2025-02-13 04:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31][root][INFO] - Training Epoch: 2/2, step 6576/7134 completed (loss: 0.010847450233995914, acc: 0.9968487620353699)
[2025-02-13 04:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:32][root][INFO] - Training Epoch: 2/2, step 6577/7134 completed (loss: 0.032140325754880905, acc: 0.9929971694946289)
[2025-02-13 04:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33][root][INFO] - Training Epoch: 2/2, step 6578/7134 completed (loss: 0.014362109825015068, acc: 0.9973309636116028)
[2025-02-13 04:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33][root][INFO] - Training Epoch: 2/2, step 6579/7134 completed (loss: 0.028806794434785843, acc: 0.9945553541183472)
[2025-02-13 04:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33][root][INFO] - Training Epoch: 2/2, step 6580/7134 completed (loss: 0.010643327608704567, acc: 0.9941176176071167)
[2025-02-13 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:34][root][INFO] - Training Epoch: 2/2, step 6581/7134 completed (loss: 0.022860519587993622, acc: 0.9926199316978455)
[2025-02-13 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:34][root][INFO] - Training Epoch: 2/2, step 6582/7134 completed (loss: 0.03547241911292076, acc: 0.9884892106056213)
[2025-02-13 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:35][root][INFO] - Training Epoch: 2/2, step 6583/7134 completed (loss: 0.007513448130339384, acc: 0.9977116584777832)
[2025-02-13 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:35][root][INFO] - Training Epoch: 2/2, step 6584/7134 completed (loss: 0.03341669216752052, acc: 0.990208089351654)
[2025-02-13 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36][root][INFO] - Training Epoch: 2/2, step 6585/7134 completed (loss: 0.020783182233572006, acc: 0.9901315569877625)
[2025-02-13 04:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36][root][INFO] - Training Epoch: 2/2, step 6586/7134 completed (loss: 0.01801997795701027, acc: 0.9935232996940613)
[2025-02-13 04:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36][root][INFO] - Training Epoch: 2/2, step 6587/7134 completed (loss: 0.06374676525592804, acc: 0.9846698045730591)
[2025-02-13 04:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:37][root][INFO] - Training Epoch: 2/2, step 6588/7134 completed (loss: 0.03756791725754738, acc: 0.9911949634552002)
[2025-02-13 04:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:37][root][INFO] - Training Epoch: 2/2, step 6589/7134 completed (loss: 0.034467484802007675, acc: 0.9917012453079224)
[2025-02-13 04:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:38][root][INFO] - Training Epoch: 2/2, step 6590/7134 completed (loss: 0.031045518815517426, acc: 0.9930151104927063)
[2025-02-13 04:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:38][root][INFO] - Training Epoch: 2/2, step 6591/7134 completed (loss: 0.04275622218847275, acc: 0.9876084327697754)
[2025-02-13 04:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:39][root][INFO] - Training Epoch: 2/2, step 6592/7134 completed (loss: 0.05107789859175682, acc: 0.9852440357208252)
[2025-02-13 04:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:39][root][INFO] - Training Epoch: 2/2, step 6593/7134 completed (loss: 0.03496132418513298, acc: 0.9893454909324646)
[2025-02-13 04:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:39][root][INFO] - Training Epoch: 2/2, step 6594/7134 completed (loss: 0.02734588272869587, acc: 0.9917840361595154)
[2025-02-13 04:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:40][root][INFO] - Training Epoch: 2/2, step 6595/7134 completed (loss: 0.052975431084632874, acc: 0.9881305694580078)
[2025-02-13 04:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:40][root][INFO] - Training Epoch: 2/2, step 6596/7134 completed (loss: 0.021440697833895683, acc: 0.9939302206039429)
[2025-02-13 04:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41][root][INFO] - Training Epoch: 2/2, step 6597/7134 completed (loss: 0.023147232830524445, acc: 0.9917452931404114)
[2025-02-13 04:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41][root][INFO] - Training Epoch: 2/2, step 6598/7134 completed (loss: 0.024205144494771957, acc: 0.9879194498062134)
[2025-02-13 04:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42][root][INFO] - Training Epoch: 2/2, step 6599/7134 completed (loss: 0.031255729496479034, acc: 0.987922728061676)
[2025-02-13 04:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42][root][INFO] - Training Epoch: 2/2, step 6600/7134 completed (loss: 0.038335200399160385, acc: 0.9889065027236938)
[2025-02-13 04:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42][root][INFO] - Training Epoch: 2/2, step 6601/7134 completed (loss: 0.04397307336330414, acc: 0.9894366264343262)
[2025-02-13 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:43][root][INFO] - Training Epoch: 2/2, step 6602/7134 completed (loss: 0.016334977000951767, acc: 0.9963898658752441)
[2025-02-13 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:43][root][INFO] - Training Epoch: 2/2, step 6603/7134 completed (loss: 0.025525862351059914, acc: 0.9901823401451111)
[2025-02-13 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:44][root][INFO] - Training Epoch: 2/2, step 6604/7134 completed (loss: 0.028242431581020355, acc: 0.9971264600753784)
[2025-02-13 04:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:44][root][INFO] - Training Epoch: 2/2, step 6605/7134 completed (loss: 0.08112717419862747, acc: 0.9737206101417542)
[2025-02-13 04:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45][root][INFO] - Training Epoch: 2/2, step 6606/7134 completed (loss: 0.02990058995783329, acc: 0.9924242496490479)
[2025-02-13 04:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45][root][INFO] - Training Epoch: 2/2, step 6607/7134 completed (loss: 0.07392540574073792, acc: 0.9853723645210266)
[2025-02-13 04:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46][root][INFO] - Training Epoch: 2/2, step 6608/7134 completed (loss: 0.023899106308817863, acc: 0.9946879148483276)
[2025-02-13 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46][root][INFO] - Training Epoch: 2/2, step 6609/7134 completed (loss: 0.03450637310743332, acc: 0.9853300452232361)
[2025-02-13 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46][root][INFO] - Training Epoch: 2/2, step 6610/7134 completed (loss: 0.06552338600158691, acc: 0.9811320900917053)
[2025-02-13 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:47][root][INFO] - Training Epoch: 2/2, step 6611/7134 completed (loss: 0.08193734288215637, acc: 0.9723076820373535)
[2025-02-13 04:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:47][root][INFO] - Training Epoch: 2/2, step 6612/7134 completed (loss: 0.04683004319667816, acc: 0.9836639165878296)
[2025-02-13 04:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:48][root][INFO] - Training Epoch: 2/2, step 6613/7134 completed (loss: 0.03443056344985962, acc: 0.988095223903656)
[2025-02-13 04:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:48][root][INFO] - Training Epoch: 2/2, step 6614/7134 completed (loss: 0.05061502382159233, acc: 0.9900990128517151)
[2025-02-13 04:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49][root][INFO] - Training Epoch: 2/2, step 6615/7134 completed (loss: 0.019575342535972595, acc: 0.9943422675132751)
[2025-02-13 04:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49][root][INFO] - Training Epoch: 2/2, step 6616/7134 completed (loss: 0.03297927975654602, acc: 0.9941314458847046)
[2025-02-13 04:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49][root][INFO] - Training Epoch: 2/2, step 6617/7134 completed (loss: 0.026081979274749756, acc: 0.9942362904548645)
[2025-02-13 04:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:50][root][INFO] - Training Epoch: 2/2, step 6618/7134 completed (loss: 0.022167587652802467, acc: 0.9940564632415771)
[2025-02-13 04:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:50][root][INFO] - Training Epoch: 2/2, step 6619/7134 completed (loss: 0.03231289982795715, acc: 0.9899103045463562)
[2025-02-13 04:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:51][root][INFO] - Training Epoch: 2/2, step 6620/7134 completed (loss: 0.02170313335955143, acc: 0.9917469024658203)
[2025-02-13 04:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:51][root][INFO] - Training Epoch: 2/2, step 6621/7134 completed (loss: 0.03534504771232605, acc: 0.9872832298278809)
[2025-02-13 04:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52][root][INFO] - Training Epoch: 2/2, step 6622/7134 completed (loss: 0.04071749001741409, acc: 0.9904371500015259)
[2025-02-13 04:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52][root][INFO] - Training Epoch: 2/2, step 6623/7134 completed (loss: 0.017671067267656326, acc: 0.9949685335159302)
[2025-02-13 04:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52][root][INFO] - Training Epoch: 2/2, step 6624/7134 completed (loss: 0.01701251044869423, acc: 0.99609375)
[2025-02-13 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:53][root][INFO] - Training Epoch: 2/2, step 6625/7134 completed (loss: 0.023438092321157455, acc: 0.9944979548454285)
[2025-02-13 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:53][root][INFO] - Training Epoch: 2/2, step 6626/7134 completed (loss: 0.012812543660402298, acc: 0.996363639831543)
[2025-02-13 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:54][root][INFO] - Training Epoch: 2/2, step 6627/7134 completed (loss: 0.027066783979535103, acc: 0.9913513660430908)
[2025-02-13 04:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:54][root][INFO] - Training Epoch: 2/2, step 6628/7134 completed (loss: 0.04429425299167633, acc: 0.9881656765937805)
[2025-02-13 04:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:55][root][INFO] - Training Epoch: 2/2, step 6629/7134 completed (loss: 0.0503263957798481, acc: 0.9884467124938965)
[2025-02-13 04:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:55][root][INFO] - Training Epoch: 2/2, step 6630/7134 completed (loss: 0.030383072793483734, acc: 0.9899665713310242)
[2025-02-13 04:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56][root][INFO] - Training Epoch: 2/2, step 6631/7134 completed (loss: 0.02770802564918995, acc: 0.9908257126808167)
[2025-02-13 04:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56][root][INFO] - Training Epoch: 2/2, step 6632/7134 completed (loss: 0.023419780656695366, acc: 0.9943757057189941)
[2025-02-13 04:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56][root][INFO] - Training Epoch: 2/2, step 6633/7134 completed (loss: 0.030132515355944633, acc: 0.9939024448394775)
[2025-02-13 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:57][root][INFO] - Training Epoch: 2/2, step 6634/7134 completed (loss: 0.02537899650633335, acc: 0.991055428981781)
[2025-02-13 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:57][root][INFO] - Training Epoch: 2/2, step 6635/7134 completed (loss: 0.07498563081026077, acc: 0.9820972084999084)
[2025-02-13 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:58][root][INFO] - Training Epoch: 2/2, step 6636/7134 completed (loss: 0.03152035176753998, acc: 0.9885844588279724)
[2025-02-13 04:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:58][root][INFO] - Training Epoch: 2/2, step 6637/7134 completed (loss: 0.020291198045015335, acc: 0.9927219748497009)
[2025-02-13 04:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59][root][INFO] - Training Epoch: 2/2, step 6638/7134 completed (loss: 0.016425589099526405, acc: 0.9949173927307129)
[2025-02-13 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59][root][INFO] - Training Epoch: 2/2, step 6639/7134 completed (loss: 0.012901187874376774, acc: 0.9985097050666809)
[2025-02-13 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59][root][INFO] - Training Epoch: 2/2, step 6640/7134 completed (loss: 0.016548078507184982, acc: 0.9938744306564331)
[2025-02-13 04:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:00][root][INFO] - Training Epoch: 2/2, step 6641/7134 completed (loss: 0.05901346355676651, acc: 0.9847596883773804)
[2025-02-13 04:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:00][root][INFO] - Training Epoch: 2/2, step 6642/7134 completed (loss: 0.03164132684469223, acc: 0.990963876247406)
[2025-02-13 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:01][root][INFO] - Training Epoch: 2/2, step 6643/7134 completed (loss: 0.017411988228559494, acc: 0.9960370063781738)
[2025-02-13 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:01][root][INFO] - Training Epoch: 2/2, step 6644/7134 completed (loss: 0.045704830437898636, acc: 0.9888097643852234)
[2025-02-13 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:02][root][INFO] - Training Epoch: 2/2, step 6645/7134 completed (loss: 0.019833985716104507, acc: 0.9938575029373169)
[2025-02-13 04:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:02][root][INFO] - Training Epoch: 2/2, step 6646/7134 completed (loss: 0.02382063679397106, acc: 0.9933920502662659)
[2025-02-13 04:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:03][root][INFO] - Training Epoch: 2/2, step 6647/7134 completed (loss: 0.013034912757575512, acc: 0.9945651888847351)
[2025-02-13 04:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:03][root][INFO] - Training Epoch: 2/2, step 6648/7134 completed (loss: 0.05239607021212578, acc: 0.9807999730110168)
[2025-02-13 04:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:04][root][INFO] - Training Epoch: 2/2, step 6649/7134 completed (loss: 0.03865731507539749, acc: 0.990554928779602)
[2025-02-13 04:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:04][root][INFO] - Training Epoch: 2/2, step 6650/7134 completed (loss: 0.007897092029452324, acc: 0.9966517686843872)
[2025-02-13 04:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05][root][INFO] - Training Epoch: 2/2, step 6651/7134 completed (loss: 0.022418668493628502, acc: 0.9931787252426147)
[2025-02-13 04:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05][root][INFO] - Training Epoch: 2/2, step 6652/7134 completed (loss: 0.028247805312275887, acc: 0.9915459156036377)
[2025-02-13 04:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05][root][INFO] - Training Epoch: 2/2, step 6653/7134 completed (loss: 0.03519934043288231, acc: 0.9868565201759338)
[2025-02-13 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:06][root][INFO] - Training Epoch: 2/2, step 6654/7134 completed (loss: 0.05600367859005928, acc: 0.9844961166381836)
[2025-02-13 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:06][root][INFO] - Training Epoch: 2/2, step 6655/7134 completed (loss: 0.034930624067783356, acc: 0.9900744557380676)
[2025-02-13 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:07][root][INFO] - Training Epoch: 2/2, step 6656/7134 completed (loss: 0.044059351086616516, acc: 0.9860050678253174)
[2025-02-13 04:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:07][root][INFO] - Training Epoch: 2/2, step 6657/7134 completed (loss: 0.02496122010052204, acc: 0.9932249188423157)
[2025-02-13 04:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08][root][INFO] - Training Epoch: 2/2, step 6658/7134 completed (loss: 0.007815925404429436, acc: 1.0)
[2025-02-13 04:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08][root][INFO] - Training Epoch: 2/2, step 6659/7134 completed (loss: 0.021589480340480804, acc: 0.99609375)
[2025-02-13 04:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08][root][INFO] - Training Epoch: 2/2, step 6660/7134 completed (loss: 0.027319174259901047, acc: 0.995039701461792)
[2025-02-13 04:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:09][root][INFO] - Training Epoch: 2/2, step 6661/7134 completed (loss: 0.020056281238794327, acc: 0.9938119053840637)
[2025-02-13 04:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:09][root][INFO] - Training Epoch: 2/2, step 6662/7134 completed (loss: 0.04493359476327896, acc: 0.9806867241859436)
[2025-02-13 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:10][root][INFO] - Training Epoch: 2/2, step 6663/7134 completed (loss: 0.027361076325178146, acc: 0.9923497438430786)
[2025-02-13 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:10][root][INFO] - Training Epoch: 2/2, step 6664/7134 completed (loss: 0.030327437445521355, acc: 0.9870848655700684)
[2025-02-13 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11][root][INFO] - Training Epoch: 2/2, step 6665/7134 completed (loss: 0.06206227466464043, acc: 0.9707792401313782)
[2025-02-13 04:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11][root][INFO] - Training Epoch: 2/2, step 6666/7134 completed (loss: 0.020913081243634224, acc: 0.9936548471450806)
[2025-02-13 04:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11][root][INFO] - Training Epoch: 2/2, step 6667/7134 completed (loss: 0.03943789377808571, acc: 0.9858657121658325)
[2025-02-13 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:12][root][INFO] - Training Epoch: 2/2, step 6668/7134 completed (loss: 0.008987289853394032, acc: 0.9965338110923767)
[2025-02-13 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:12][root][INFO] - Training Epoch: 2/2, step 6669/7134 completed (loss: 0.022085580974817276, acc: 0.9916107654571533)
[2025-02-13 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13][root][INFO] - Training Epoch: 2/2, step 6670/7134 completed (loss: 0.023278748616576195, acc: 0.9949832558631897)
[2025-02-13 04:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13][root][INFO] - Training Epoch: 2/2, step 6671/7134 completed (loss: 0.012241472490131855, acc: 0.9941349029541016)
[2025-02-13 04:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13][root][INFO] - Training Epoch: 2/2, step 6672/7134 completed (loss: 0.015349588356912136, acc: 0.9943714737892151)
[2025-02-13 04:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:14][root][INFO] - Training Epoch: 2/2, step 6673/7134 completed (loss: 0.018745312467217445, acc: 0.9926578402519226)
[2025-02-13 04:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:14][root][INFO] - Training Epoch: 2/2, step 6674/7134 completed (loss: 0.007410528603941202, acc: 1.0)
[2025-02-13 04:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15][root][INFO] - Training Epoch: 2/2, step 6675/7134 completed (loss: 0.014069506898522377, acc: 0.9950617551803589)
[2025-02-13 04:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15][root][INFO] - Training Epoch: 2/2, step 6676/7134 completed (loss: 0.01950087398290634, acc: 0.9928401112556458)
[2025-02-13 04:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15][root][INFO] - Training Epoch: 2/2, step 6677/7134 completed (loss: 0.04701978340744972, acc: 0.9885495901107788)
[2025-02-13 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:16][root][INFO] - Training Epoch: 2/2, step 6678/7134 completed (loss: 0.01367136463522911, acc: 0.9946428537368774)
[2025-02-13 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:16][root][INFO] - Training Epoch: 2/2, step 6679/7134 completed (loss: 0.00913360994309187, acc: 0.9956331849098206)
[2025-02-13 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17][root][INFO] - Training Epoch: 2/2, step 6680/7134 completed (loss: 0.014436233788728714, acc: 0.9972972869873047)
[2025-02-13 04:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17][root][INFO] - Training Epoch: 2/2, step 6681/7134 completed (loss: 0.03171224892139435, acc: 0.9937629699707031)
[2025-02-13 04:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17][root][INFO] - Training Epoch: 2/2, step 6682/7134 completed (loss: 0.04335900396108627, acc: 0.9953271150588989)
[2025-02-13 04:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:18][root][INFO] - Training Epoch: 2/2, step 6683/7134 completed (loss: 0.01658029481768608, acc: 0.9945054650306702)
[2025-02-13 04:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:18][root][INFO] - Training Epoch: 2/2, step 6684/7134 completed (loss: 0.03323536738753319, acc: 0.9928571581840515)
[2025-02-13 04:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19][root][INFO] - Training Epoch: 2/2, step 6685/7134 completed (loss: 0.016149381175637245, acc: 0.9923664331436157)
[2025-02-13 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19][root][INFO] - Training Epoch: 2/2, step 6686/7134 completed (loss: 0.04889730364084244, acc: 0.9917355179786682)
[2025-02-13 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19][root][INFO] - Training Epoch: 2/2, step 6687/7134 completed (loss: 0.051457781344652176, acc: 0.9814814925193787)
[2025-02-13 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:20][root][INFO] - Training Epoch: 2/2, step 6688/7134 completed (loss: 0.034940704703330994, acc: 0.9828178882598877)
[2025-02-13 04:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:20][root][INFO] - Training Epoch: 2/2, step 6689/7134 completed (loss: 0.03908253461122513, acc: 0.9900990128517151)
[2025-02-13 04:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21][root][INFO] - Training Epoch: 2/2, step 6690/7134 completed (loss: 0.07264531403779984, acc: 0.9733656048774719)
[2025-02-13 04:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21][root][INFO] - Training Epoch: 2/2, step 6691/7134 completed (loss: 0.01352525595575571, acc: 0.9967266917228699)
[2025-02-13 04:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21][root][INFO] - Training Epoch: 2/2, step 6692/7134 completed (loss: 0.025882603600621223, acc: 0.9936440587043762)
[2025-02-13 04:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:22][root][INFO] - Training Epoch: 2/2, step 6693/7134 completed (loss: 0.01791144162416458, acc: 0.9950494766235352)
[2025-02-13 04:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:22][root][INFO] - Training Epoch: 2/2, step 6694/7134 completed (loss: 0.018894001841545105, acc: 0.9938837885856628)
[2025-02-13 04:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23][root][INFO] - Training Epoch: 2/2, step 6695/7134 completed (loss: 0.03910934925079346, acc: 0.9849520921707153)
[2025-02-13 04:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23][root][INFO] - Training Epoch: 2/2, step 6696/7134 completed (loss: 0.05004866421222687, acc: 0.9860681295394897)
[2025-02-13 04:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23][root][INFO] - Training Epoch: 2/2, step 6697/7134 completed (loss: 0.045372121036052704, acc: 0.9885246157646179)
[2025-02-13 04:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:24][root][INFO] - Training Epoch: 2/2, step 6698/7134 completed (loss: 0.014932215213775635, acc: 0.9940652847290039)
[2025-02-13 04:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:24][root][INFO] - Training Epoch: 2/2, step 6699/7134 completed (loss: 0.027199864387512207, acc: 0.9908883571624756)
[2025-02-13 04:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25][root][INFO] - Training Epoch: 2/2, step 6700/7134 completed (loss: 0.04573812335729599, acc: 0.9834558963775635)
[2025-02-13 04:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25][root][INFO] - Training Epoch: 2/2, step 6701/7134 completed (loss: 0.09124399721622467, acc: 0.9722222089767456)
[2025-02-13 04:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25][root][INFO] - Training Epoch: 2/2, step 6702/7134 completed (loss: 0.023974889889359474, acc: 0.9882121682167053)
[2025-02-13 04:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:26][root][INFO] - Training Epoch: 2/2, step 6703/7134 completed (loss: 0.06942737102508545, acc: 0.9819079041481018)
[2025-02-13 04:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:26][root][INFO] - Training Epoch: 2/2, step 6704/7134 completed (loss: 0.08300061523914337, acc: 0.9754204154014587)
[2025-02-13 04:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27][root][INFO] - Training Epoch: 2/2, step 6705/7134 completed (loss: 0.03765389695763588, acc: 0.9869961142539978)
[2025-02-13 04:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27][root][INFO] - Training Epoch: 2/2, step 6706/7134 completed (loss: 0.02745695412158966, acc: 0.9917355179786682)
[2025-02-13 04:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27][root][INFO] - Training Epoch: 2/2, step 6707/7134 completed (loss: 0.040415938943624496, acc: 0.9881656765937805)
[2025-02-13 04:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28][root][INFO] - Training Epoch: 2/2, step 6708/7134 completed (loss: 0.03408612683415413, acc: 0.9900990128517151)
[2025-02-13 04:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28][root][INFO] - Training Epoch: 2/2, step 6709/7134 completed (loss: 0.02008228935301304, acc: 0.9940119981765747)
[2025-02-13 04:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:29][root][INFO] - Training Epoch: 2/2, step 6710/7134 completed (loss: 0.06844163686037064, acc: 0.9853747487068176)
[2025-02-13 04:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:29][root][INFO] - Training Epoch: 2/2, step 6711/7134 completed (loss: 0.02154085971415043, acc: 0.9920634627342224)
[2025-02-13 04:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30][root][INFO] - Training Epoch: 2/2, step 6712/7134 completed (loss: 0.0408068485558033, acc: 0.9848713874816895)
[2025-02-13 04:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30][root][INFO] - Training Epoch: 2/2, step 6713/7134 completed (loss: 0.051307354122400284, acc: 0.9855282306671143)
[2025-02-13 04:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30][root][INFO] - Training Epoch: 2/2, step 6714/7134 completed (loss: 0.02792769856750965, acc: 0.9881129264831543)
[2025-02-13 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:31][root][INFO] - Training Epoch: 2/2, step 6715/7134 completed (loss: 0.06155921891331673, acc: 0.9795918464660645)
[2025-02-13 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:31][root][INFO] - Training Epoch: 2/2, step 6716/7134 completed (loss: 0.01377277635037899, acc: 0.992514967918396)
[2025-02-13 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32][root][INFO] - Training Epoch: 2/2, step 6717/7134 completed (loss: 0.022284455597400665, acc: 0.990338146686554)
[2025-02-13 04:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32][root][INFO] - Training Epoch: 2/2, step 6718/7134 completed (loss: 0.05643164739012718, acc: 0.9833333492279053)
[2025-02-13 04:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32][root][INFO] - Training Epoch: 2/2, step 6719/7134 completed (loss: 0.02438933774828911, acc: 0.9885621070861816)
[2025-02-13 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:33][root][INFO] - Training Epoch: 2/2, step 6720/7134 completed (loss: 0.054165713489055634, acc: 0.9874301552772522)
[2025-02-13 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:33][root][INFO] - Training Epoch: 2/2, step 6721/7134 completed (loss: 0.046469613909721375, acc: 0.9875195026397705)
[2025-02-13 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34][root][INFO] - Training Epoch: 2/2, step 6722/7134 completed (loss: 0.05956199765205383, acc: 0.9912126660346985)
[2025-02-13 04:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34][root][INFO] - Training Epoch: 2/2, step 6723/7134 completed (loss: 0.022801782935857773, acc: 0.9916666746139526)
[2025-02-13 04:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34][root][INFO] - Training Epoch: 2/2, step 6724/7134 completed (loss: 0.027660377323627472, acc: 0.9941434860229492)
[2025-02-13 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:35][root][INFO] - Training Epoch: 2/2, step 6725/7134 completed (loss: 0.023642322048544884, acc: 0.9897611141204834)
[2025-02-13 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:35][root][INFO] - Training Epoch: 2/2, step 6726/7134 completed (loss: 0.05200796574354172, acc: 0.9919678568840027)
[2025-02-13 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36][root][INFO] - Training Epoch: 2/2, step 6727/7134 completed (loss: 0.05421062186360359, acc: 0.9841549396514893)
[2025-02-13 04:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36][root][INFO] - Training Epoch: 2/2, step 6728/7134 completed (loss: 0.04182079806923866, acc: 0.9923195242881775)
[2025-02-13 04:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36][root][INFO] - Training Epoch: 2/2, step 6729/7134 completed (loss: 0.014510232023894787, acc: 0.9972826242446899)
[2025-02-13 04:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37][root][INFO] - Training Epoch: 2/2, step 6730/7134 completed (loss: 0.0286676287651062, acc: 0.9905808568000793)
[2025-02-13 04:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37][root][INFO] - Training Epoch: 2/2, step 6731/7134 completed (loss: 0.020230300724506378, acc: 0.993966817855835)
[2025-02-13 04:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38][root][INFO] - Training Epoch: 2/2, step 6732/7134 completed (loss: 0.02997272089123726, acc: 0.9889937043190002)
[2025-02-13 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38][root][INFO] - Training Epoch: 2/2, step 6733/7134 completed (loss: 0.03553337603807449, acc: 0.9886147975921631)
[2025-02-13 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38][root][INFO] - Training Epoch: 2/2, step 6734/7134 completed (loss: 0.046602699905633926, acc: 0.9906542301177979)
[2025-02-13 04:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:39][root][INFO] - Training Epoch: 2/2, step 6735/7134 completed (loss: 0.04397367686033249, acc: 0.9907833933830261)
[2025-02-13 04:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:39][root][INFO] - Training Epoch: 2/2, step 6736/7134 completed (loss: 0.035037439316511154, acc: 0.9930362105369568)
[2025-02-13 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:40][root][INFO] - Training Epoch: 2/2, step 6737/7134 completed (loss: 0.03983161970973015, acc: 0.987034022808075)
[2025-02-13 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:40][root][INFO] - Training Epoch: 2/2, step 6738/7134 completed (loss: 0.025247320532798767, acc: 0.996268630027771)
[2025-02-13 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41][root][INFO] - Training Epoch: 2/2, step 6739/7134 completed (loss: 0.028407562524080276, acc: 0.9908088445663452)
[2025-02-13 04:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41][root][INFO] - Training Epoch: 2/2, step 6740/7134 completed (loss: 0.04590178653597832, acc: 0.9881154298782349)
[2025-02-13 04:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41][root][INFO] - Training Epoch: 2/2, step 6741/7134 completed (loss: 0.024416295811533928, acc: 0.9898403286933899)
[2025-02-13 04:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:42][root][INFO] - Training Epoch: 2/2, step 6742/7134 completed (loss: 0.02036985568702221, acc: 0.9913420081138611)
[2025-02-13 04:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:42][root][INFO] - Training Epoch: 2/2, step 6743/7134 completed (loss: 0.014772425405681133, acc: 0.9959127902984619)
[2025-02-13 04:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43][root][INFO] - Training Epoch: 2/2, step 6744/7134 completed (loss: 0.018526926636695862, acc: 0.9967948794364929)
[2025-02-13 04:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43][root][INFO] - Training Epoch: 2/2, step 6745/7134 completed (loss: 0.009779082611203194, acc: 0.9986245036125183)
[2025-02-13 04:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43][root][INFO] - Training Epoch: 2/2, step 6746/7134 completed (loss: 0.010069634765386581, acc: 0.998236358165741)
[2025-02-13 04:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44][root][INFO] - Training Epoch: 2/2, step 6747/7134 completed (loss: 0.0229205209761858, acc: 0.9937888383865356)
[2025-02-13 04:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44][root][INFO] - Training Epoch: 2/2, step 6748/7134 completed (loss: 0.021561389788985252, acc: 0.9977452158927917)
[2025-02-13 04:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:45][root][INFO] - Training Epoch: 2/2, step 6749/7134 completed (loss: 0.020832588896155357, acc: 0.9934640526771545)
[2025-02-13 04:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:45][root][INFO] - Training Epoch: 2/2, step 6750/7134 completed (loss: 0.01300605945289135, acc: 0.9988465905189514)
[2025-02-13 04:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:46][root][INFO] - Training Epoch: 2/2, step 6751/7134 completed (loss: 0.01175433024764061, acc: 0.9965753555297852)
[2025-02-13 04:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:46][root][INFO] - Training Epoch: 2/2, step 6752/7134 completed (loss: 0.01324098277837038, acc: 0.9970972537994385)
[2025-02-13 04:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47][root][INFO] - Training Epoch: 2/2, step 6753/7134 completed (loss: 0.014771933667361736, acc: 0.997706413269043)
[2025-02-13 04:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47][root][INFO] - Training Epoch: 2/2, step 6754/7134 completed (loss: 0.003528211498633027, acc: 1.0)
[2025-02-13 04:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47][root][INFO] - Training Epoch: 2/2, step 6755/7134 completed (loss: 0.012926317751407623, acc: 0.9975369572639465)
[2025-02-13 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:48][root][INFO] - Training Epoch: 2/2, step 6756/7134 completed (loss: 0.005577158182859421, acc: 1.0)
[2025-02-13 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:48][root][INFO] - Training Epoch: 2/2, step 6757/7134 completed (loss: 0.008072678931057453, acc: 0.9954268336296082)
[2025-02-13 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:49][root][INFO] - Training Epoch: 2/2, step 6758/7134 completed (loss: 0.01302344724535942, acc: 0.9961190223693848)
[2025-02-13 04:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:49][root][INFO] - Training Epoch: 2/2, step 6759/7134 completed (loss: 0.02440538816154003, acc: 0.9932885766029358)
[2025-02-13 04:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50][root][INFO] - Training Epoch: 2/2, step 6760/7134 completed (loss: 0.020035291090607643, acc: 0.9934383034706116)
[2025-02-13 04:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50][root][INFO] - Training Epoch: 2/2, step 6761/7134 completed (loss: 0.021580472588539124, acc: 0.9903614521026611)
[2025-02-13 04:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50][root][INFO] - Training Epoch: 2/2, step 6762/7134 completed (loss: 0.044812362641096115, acc: 0.9877451062202454)
[2025-02-13 04:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51][root][INFO] - Training Epoch: 2/2, step 6763/7134 completed (loss: 0.017037294805049896, acc: 0.9962359070777893)
[2025-02-13 04:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51][root][INFO] - Training Epoch: 2/2, step 6764/7134 completed (loss: 0.024661585688591003, acc: 0.9928160905838013)
[2025-02-13 04:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52][root][INFO] - Training Epoch: 2/2, step 6765/7134 completed (loss: 0.015164395794272423, acc: 0.9973368644714355)
[2025-02-13 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52][root][INFO] - Training Epoch: 2/2, step 6766/7134 completed (loss: 0.004069410730153322, acc: 1.0)
[2025-02-13 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52][root][INFO] - Training Epoch: 2/2, step 6767/7134 completed (loss: 0.016233688220381737, acc: 0.9977477192878723)
[2025-02-13 04:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:53][root][INFO] - Training Epoch: 2/2, step 6768/7134 completed (loss: 0.005849374923855066, acc: 1.0)
[2025-02-13 04:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:53][root][INFO] - Training Epoch: 2/2, step 6769/7134 completed (loss: 0.019807064905762672, acc: 0.9922839403152466)
[2025-02-13 04:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:54][root][INFO] - Training Epoch: 2/2, step 6770/7134 completed (loss: 0.027548890560865402, acc: 0.9879194498062134)
[2025-02-13 04:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:54][root][INFO] - Training Epoch: 2/2, step 6771/7134 completed (loss: 0.020671583712100983, acc: 0.9952681660652161)
[2025-02-13 04:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55][root][INFO] - Training Epoch: 2/2, step 6772/7134 completed (loss: 0.07191634923219681, acc: 0.9857397675514221)
[2025-02-13 04:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55][root][INFO] - Training Epoch: 2/2, step 6773/7134 completed (loss: 0.03203408047556877, acc: 0.9889240264892578)
[2025-02-13 04:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55][root][INFO] - Training Epoch: 2/2, step 6774/7134 completed (loss: 0.03099077381193638, acc: 0.993630588054657)
[2025-02-13 04:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:56][root][INFO] - Training Epoch: 2/2, step 6775/7134 completed (loss: 0.009204905480146408, acc: 0.9981096386909485)
[2025-02-13 04:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:56][root][INFO] - Training Epoch: 2/2, step 6776/7134 completed (loss: 0.030350055545568466, acc: 0.9893617033958435)
[2025-02-13 04:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:57][root][INFO] - Training Epoch: 2/2, step 6777/7134 completed (loss: 0.021814141422510147, acc: 0.9932885766029358)
[2025-02-13 04:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:57][root][INFO] - Training Epoch: 2/2, step 6778/7134 completed (loss: 0.027139581739902496, acc: 0.9955423474311829)
[2025-02-13 04:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58][root][INFO] - Training Epoch: 2/2, step 6779/7134 completed (loss: 0.009000048041343689, acc: 1.0)
[2025-02-13 04:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58][root][INFO] - Training Epoch: 2/2, step 6780/7134 completed (loss: 0.017376627773046494, acc: 0.9970104694366455)
[2025-02-13 04:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58][root][INFO] - Training Epoch: 2/2, step 6781/7134 completed (loss: 0.01621376909315586, acc: 0.9940119981765747)
[2025-02-13 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:59][root][INFO] - Training Epoch: 2/2, step 6782/7134 completed (loss: 0.023168327286839485, acc: 0.9947916865348816)
[2025-02-13 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:59][root][INFO] - Training Epoch: 2/2, step 6783/7134 completed (loss: 0.03493620455265045, acc: 0.9884836673736572)
[2025-02-13 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00][root][INFO] - Training Epoch: 2/2, step 6784/7134 completed (loss: 0.014783781953155994, acc: 0.9950082898139954)
[2025-02-13 04:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00][root][INFO] - Training Epoch: 2/2, step 6785/7134 completed (loss: 0.016661901026964188, acc: 0.9937106966972351)
[2025-02-13 04:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00][root][INFO] - Training Epoch: 2/2, step 6786/7134 completed (loss: 0.013264026492834091, acc: 0.9956834316253662)
[2025-02-13 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:01][root][INFO] - Training Epoch: 2/2, step 6787/7134 completed (loss: 0.024772673845291138, acc: 0.99210524559021)
[2025-02-13 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:01][root][INFO] - Training Epoch: 2/2, step 6788/7134 completed (loss: 0.010197893716394901, acc: 0.9964028596878052)
[2025-02-13 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:02][root][INFO] - Training Epoch: 2/2, step 6789/7134 completed (loss: 0.014413250610232353, acc: 0.9961089491844177)
[2025-02-13 04:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:02][root][INFO] - Training Epoch: 2/2, step 6790/7134 completed (loss: 0.034239016473293304, acc: 0.9899425506591797)
[2025-02-13 04:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03][root][INFO] - Training Epoch: 2/2, step 6791/7134 completed (loss: 0.02212253026664257, acc: 0.991482138633728)
[2025-02-13 04:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03][root][INFO] - Training Epoch: 2/2, step 6792/7134 completed (loss: 0.012866413220763206, acc: 0.9944953918457031)
[2025-02-13 04:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03][root][INFO] - Training Epoch: 2/2, step 6793/7134 completed (loss: 0.010407734662294388, acc: 0.997890293598175)
[2025-02-13 04:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:04][root][INFO] - Training Epoch: 2/2, step 6794/7134 completed (loss: 0.006775537971407175, acc: 0.9983579516410828)
[2025-02-13 04:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:04][root][INFO] - Training Epoch: 2/2, step 6795/7134 completed (loss: 0.0050662001594901085, acc: 1.0)
[2025-02-13 04:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05][root][INFO] - Training Epoch: 2/2, step 6796/7134 completed (loss: 0.00843215174973011, acc: 0.9970674514770508)
[2025-02-13 04:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05][root][INFO] - Training Epoch: 2/2, step 6797/7134 completed (loss: 0.04910784587264061, acc: 0.9854439496994019)
[2025-02-13 04:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05][root][INFO] - Training Epoch: 2/2, step 6798/7134 completed (loss: 0.022275112569332123, acc: 0.9889807105064392)
[2025-02-13 04:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06][root][INFO] - Training Epoch: 2/2, step 6799/7134 completed (loss: 0.012158998288214207, acc: 0.99726402759552)
[2025-02-13 04:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06][root][INFO] - Training Epoch: 2/2, step 6800/7134 completed (loss: 0.026505229994654655, acc: 0.988950252532959)
[2025-02-13 04:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:07][root][INFO] - Training Epoch: 2/2, step 6801/7134 completed (loss: 0.023579468950629234, acc: 0.9905511736869812)
[2025-02-13 04:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:07][root][INFO] - Training Epoch: 2/2, step 6802/7134 completed (loss: 0.015089809894561768, acc: 0.9956076145172119)
[2025-02-13 04:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08][root][INFO] - Training Epoch: 2/2, step 6803/7134 completed (loss: 0.013615588657557964, acc: 0.9949685335159302)
[2025-02-13 04:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08][root][INFO] - Training Epoch: 2/2, step 6804/7134 completed (loss: 0.02408425882458687, acc: 0.9937499761581421)
[2025-02-13 04:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08][root][INFO] - Training Epoch: 2/2, step 6805/7134 completed (loss: 0.020527034997940063, acc: 0.9967948794364929)
[2025-02-13 04:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:09][root][INFO] - Training Epoch: 2/2, step 6806/7134 completed (loss: 0.03508978337049484, acc: 0.9936708807945251)
[2025-02-13 04:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:09][root][INFO] - Training Epoch: 2/2, step 6807/7134 completed (loss: 0.016819151118397713, acc: 0.9942528605461121)
[2025-02-13 04:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:10][root][INFO] - Training Epoch: 2/2, step 6808/7134 completed (loss: 0.04133504256606102, acc: 0.9888357520103455)
[2025-02-13 04:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:10][root][INFO] - Training Epoch: 2/2, step 6809/7134 completed (loss: 0.026147596538066864, acc: 0.9905149340629578)
[2025-02-13 04:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11][root][INFO] - Training Epoch: 2/2, step 6810/7134 completed (loss: 0.018075238913297653, acc: 0.99210524559021)
[2025-02-13 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11][root][INFO] - Training Epoch: 2/2, step 6811/7134 completed (loss: 0.037208203226327896, acc: 0.993446946144104)
[2025-02-13 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11][root][INFO] - Training Epoch: 2/2, step 6812/7134 completed (loss: 0.014237366616725922, acc: 0.9951377511024475)
[2025-02-13 04:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:12][root][INFO] - Training Epoch: 2/2, step 6813/7134 completed (loss: 0.007405215408653021, acc: 0.9985915422439575)
[2025-02-13 04:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:12][root][INFO] - Training Epoch: 2/2, step 6814/7134 completed (loss: 0.01660391129553318, acc: 0.9973118305206299)
[2025-02-13 04:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:13][root][INFO] - Training Epoch: 2/2, step 6815/7134 completed (loss: 0.028583811596035957, acc: 0.9908015727996826)
[2025-02-13 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:13][root][INFO] - Training Epoch: 2/2, step 6816/7134 completed (loss: 0.020115086808800697, acc: 0.9949832558631897)
[2025-02-13 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14][root][INFO] - Training Epoch: 2/2, step 6817/7134 completed (loss: 0.05069083720445633, acc: 0.9897435903549194)
[2025-02-13 04:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14][root][INFO] - Training Epoch: 2/2, step 6818/7134 completed (loss: 0.011608578264713287, acc: 0.9940029978752136)
[2025-02-13 04:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14][root][INFO] - Training Epoch: 2/2, step 6819/7134 completed (loss: 0.03768574818968773, acc: 0.9892086386680603)
[2025-02-13 04:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:15][root][INFO] - Training Epoch: 2/2, step 6820/7134 completed (loss: 0.01716284081339836, acc: 0.9945945739746094)
[2025-02-13 04:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:15][root][INFO] - Training Epoch: 2/2, step 6821/7134 completed (loss: 0.015478646382689476, acc: 0.9898989796638489)
[2025-02-13 04:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16][root][INFO] - Training Epoch: 2/2, step 6822/7134 completed (loss: 0.046357955783605576, acc: 0.9877836108207703)
[2025-02-13 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16][root][INFO] - Training Epoch: 2/2, step 6823/7134 completed (loss: 0.029982000589370728, acc: 0.9899857044219971)
[2025-02-13 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16][root][INFO] - Training Epoch: 2/2, step 6824/7134 completed (loss: 0.0637831762433052, acc: 0.9838709831237793)
[2025-02-13 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17][root][INFO] - Training Epoch: 2/2, step 6825/7134 completed (loss: 0.008393351919949055, acc: 0.997560977935791)
[2025-02-13 04:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17][root][INFO] - Training Epoch: 2/2, step 6826/7134 completed (loss: 0.03419872745871544, acc: 0.9848484992980957)
[2025-02-13 04:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18][root][INFO] - Training Epoch: 2/2, step 6827/7134 completed (loss: 0.11053533852100372, acc: 0.9716494679450989)
[2025-02-13 04:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18][root][INFO] - Training Epoch: 2/2, step 6828/7134 completed (loss: 0.029797857627272606, acc: 0.9872262477874756)
[2025-02-13 04:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18][root][INFO] - Training Epoch: 2/2, step 6829/7134 completed (loss: 0.043192651122808456, acc: 0.9851632118225098)
[2025-02-13 04:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:19][root][INFO] - Training Epoch: 2/2, step 6830/7134 completed (loss: 0.02296416088938713, acc: 0.9950576424598694)
[2025-02-13 04:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:19][root][INFO] - Training Epoch: 2/2, step 6831/7134 completed (loss: 0.05048486217856407, acc: 0.9865591526031494)
[2025-02-13 04:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20][root][INFO] - Training Epoch: 2/2, step 6832/7134 completed (loss: 0.02315455861389637, acc: 0.996303141117096)
[2025-02-13 04:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20][root][INFO] - Training Epoch: 2/2, step 6833/7134 completed (loss: 0.0433228500187397, acc: 0.9839228391647339)
[2025-02-13 04:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20][root][INFO] - Training Epoch: 2/2, step 6834/7134 completed (loss: 0.03292747959494591, acc: 0.9897785186767578)
[2025-02-13 04:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:21][root][INFO] - Training Epoch: 2/2, step 6835/7134 completed (loss: 0.018121302127838135, acc: 0.9935064911842346)
[2025-02-13 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:21][root][INFO] - Training Epoch: 2/2, step 6836/7134 completed (loss: 0.03848796710371971, acc: 0.9861809015274048)
[2025-02-13 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22][root][INFO] - Training Epoch: 2/2, step 6837/7134 completed (loss: 0.032815784215927124, acc: 0.991411030292511)
[2025-02-13 04:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22][root][INFO] - Training Epoch: 2/2, step 6838/7134 completed (loss: 0.062140174210071564, acc: 0.9814814925193787)
[2025-02-13 04:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22][root][INFO] - Training Epoch: 2/2, step 6839/7134 completed (loss: 0.08565328270196915, acc: 0.9702970385551453)
[2025-02-13 04:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:23][root][INFO] - Training Epoch: 2/2, step 6840/7134 completed (loss: 0.06770553439855576, acc: 0.9813753366470337)
[2025-02-13 04:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:23][root][INFO] - Training Epoch: 2/2, step 6841/7134 completed (loss: 0.027941280975937843, acc: 0.9884560108184814)
[2025-02-13 04:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24][root][INFO] - Training Epoch: 2/2, step 6842/7134 completed (loss: 0.022613750770688057, acc: 0.9912280440330505)
[2025-02-13 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24][root][INFO] - Training Epoch: 2/2, step 6843/7134 completed (loss: 0.044474925845861435, acc: 0.9852034449577332)
[2025-02-13 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24][root][INFO] - Training Epoch: 2/2, step 6844/7134 completed (loss: 0.013081925921142101, acc: 0.9948186278343201)
[2025-02-13 04:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:25][root][INFO] - Training Epoch: 2/2, step 6845/7134 completed (loss: 0.017547408118844032, acc: 0.9931972622871399)
[2025-02-13 04:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:25][root][INFO] - Training Epoch: 2/2, step 6846/7134 completed (loss: 0.02497410774230957, acc: 0.9928366541862488)
[2025-02-13 04:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26][root][INFO] - Training Epoch: 2/2, step 6847/7134 completed (loss: 0.02853401005268097, acc: 0.989393949508667)
[2025-02-13 04:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26][root][INFO] - Training Epoch: 2/2, step 6848/7134 completed (loss: 0.014193283393979073, acc: 0.9972972869873047)
[2025-02-13 04:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27][root][INFO] - Training Epoch: 2/2, step 6849/7134 completed (loss: 0.022824538871645927, acc: 0.9965635538101196)
[2025-02-13 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27][root][INFO] - Training Epoch: 2/2, step 6850/7134 completed (loss: 0.01067365426570177, acc: 0.9977116584777832)
[2025-02-13 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27][root][INFO] - Training Epoch: 2/2, step 6851/7134 completed (loss: 0.054138634353876114, acc: 0.9844852089881897)
[2025-02-13 04:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:28][root][INFO] - Training Epoch: 2/2, step 6852/7134 completed (loss: 0.01538512110710144, acc: 0.9947552680969238)
[2025-02-13 04:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:28][root][INFO] - Training Epoch: 2/2, step 6853/7134 completed (loss: 0.02659945748746395, acc: 0.9930843710899353)
[2025-02-13 04:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29][root][INFO] - Training Epoch: 2/2, step 6854/7134 completed (loss: 0.02940673567354679, acc: 0.9913232326507568)
[2025-02-13 04:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29][root][INFO] - Training Epoch: 2/2, step 6855/7134 completed (loss: 0.012225687503814697, acc: 0.9929245114326477)
[2025-02-13 04:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29][root][INFO] - Training Epoch: 2/2, step 6856/7134 completed (loss: 0.005137493368238211, acc: 0.9985693693161011)
[2025-02-13 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:30][root][INFO] - Training Epoch: 2/2, step 6857/7134 completed (loss: 0.01619713194668293, acc: 0.9970674514770508)
[2025-02-13 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:30][root][INFO] - Training Epoch: 2/2, step 6858/7134 completed (loss: 0.005744973197579384, acc: 1.0)
[2025-02-13 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31][root][INFO] - Training Epoch: 2/2, step 6859/7134 completed (loss: 0.003989534918218851, acc: 0.9974392056465149)
[2025-02-13 04:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31][root][INFO] - Training Epoch: 2/2, step 6860/7134 completed (loss: 0.01196912582963705, acc: 0.9982269406318665)
[2025-02-13 04:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31][root][INFO] - Training Epoch: 2/2, step 6861/7134 completed (loss: 0.010975140146911144, acc: 0.9982699155807495)
[2025-02-13 04:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:32][root][INFO] - Training Epoch: 2/2, step 6862/7134 completed (loss: 0.026519279927015305, acc: 0.9923809766769409)
[2025-02-13 04:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:32][root][INFO] - Training Epoch: 2/2, step 6863/7134 completed (loss: 0.008692638948559761, acc: 0.9985875487327576)
[2025-02-13 04:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:33][root][INFO] - Training Epoch: 2/2, step 6864/7134 completed (loss: 0.008616169914603233, acc: 0.9966722130775452)
[2025-02-13 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:33][root][INFO] - Training Epoch: 2/2, step 6865/7134 completed (loss: 0.024601634591817856, acc: 0.9896907210350037)
[2025-02-13 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34][root][INFO] - Training Epoch: 2/2, step 6866/7134 completed (loss: 0.008723633363842964, acc: 0.9984802603721619)
[2025-02-13 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34][root][INFO] - Training Epoch: 2/2, step 6867/7134 completed (loss: 0.0031106271781027317, acc: 1.0)
[2025-02-13 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34][root][INFO] - Training Epoch: 2/2, step 6868/7134 completed (loss: 0.00363279995508492, acc: 1.0)
[2025-02-13 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35][root][INFO] - Training Epoch: 2/2, step 6869/7134 completed (loss: 0.007768701296299696, acc: 0.9957173466682434)
[2025-02-13 04:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35][root][INFO] - Training Epoch: 2/2, step 6870/7134 completed (loss: 0.015821974724531174, acc: 0.9967426657676697)
[2025-02-13 04:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36][root][INFO] - Training Epoch: 2/2, step 6871/7134 completed (loss: 0.0031649444717913866, acc: 1.0)
[2025-02-13 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36][root][INFO] - Training Epoch: 2/2, step 6872/7134 completed (loss: 0.013066614046692848, acc: 0.9986149668693542)
[2025-02-13 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36][root][INFO] - Training Epoch: 2/2, step 6873/7134 completed (loss: 0.008413262665271759, acc: 0.9957982897758484)
[2025-02-13 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:37][root][INFO] - Training Epoch: 2/2, step 6874/7134 completed (loss: 0.009417546913027763, acc: 0.9982269406318665)
[2025-02-13 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:37][root][INFO] - Training Epoch: 2/2, step 6875/7134 completed (loss: 0.008080822415649891, acc: 0.9953632354736328)
[2025-02-13 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38][root][INFO] - Training Epoch: 2/2, step 6876/7134 completed (loss: 0.009693902917206287, acc: 0.9949238300323486)
[2025-02-13 04:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38][root][INFO] - Training Epoch: 2/2, step 6877/7134 completed (loss: 0.013891947455704212, acc: 0.9971910119056702)
[2025-02-13 04:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38][root][INFO] - Training Epoch: 2/2, step 6878/7134 completed (loss: 0.009999004192650318, acc: 0.9970015287399292)
[2025-02-13 04:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39][root][INFO] - Training Epoch: 2/2, step 6879/7134 completed (loss: 0.005439986940473318, acc: 1.0)
[2025-02-13 04:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39][root][INFO] - Training Epoch: 2/2, step 6880/7134 completed (loss: 0.0048377905040979385, acc: 0.9976717233657837)
[2025-02-13 04:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40][root][INFO] - Training Epoch: 2/2, step 6881/7134 completed (loss: 0.04051120951771736, acc: 0.9939246773719788)
[2025-02-13 04:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40][root][INFO] - Training Epoch: 2/2, step 6882/7134 completed (loss: 0.03435711935162544, acc: 0.9906014800071716)
[2025-02-13 04:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40][root][INFO] - Training Epoch: 2/2, step 6883/7134 completed (loss: 0.006359927821904421, acc: 0.9983498454093933)
[2025-02-13 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:41][root][INFO] - Training Epoch: 2/2, step 6884/7134 completed (loss: 0.04881330206990242, acc: 0.992443323135376)
[2025-02-13 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:41][root][INFO] - Training Epoch: 2/2, step 6885/7134 completed (loss: 0.04064745083451271, acc: 0.9910714030265808)
[2025-02-13 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42][root][INFO] - Training Epoch: 2/2, step 6886/7134 completed (loss: 0.015507861040532589, acc: 0.995245635509491)
[2025-02-13 04:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42][root][INFO] - Training Epoch: 2/2, step 6887/7134 completed (loss: 0.01889178901910782, acc: 0.9938398599624634)
[2025-02-13 04:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43][root][INFO] - Training Epoch: 2/2, step 6888/7134 completed (loss: 0.02216404117643833, acc: 0.9950166344642639)
[2025-02-13 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43][root][INFO] - Training Epoch: 2/2, step 6889/7134 completed (loss: 0.004805587697774172, acc: 0.9984685778617859)
[2025-02-13 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43][root][INFO] - Training Epoch: 2/2, step 6890/7134 completed (loss: 0.02338205836713314, acc: 0.9933949708938599)
[2025-02-13 04:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:44][root][INFO] - Training Epoch: 2/2, step 6891/7134 completed (loss: 0.015842445194721222, acc: 0.9940740466117859)
[2025-02-13 04:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:44][root][INFO] - Training Epoch: 2/2, step 6892/7134 completed (loss: 0.012355831451714039, acc: 0.9973958134651184)
[2025-02-13 04:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:45][root][INFO] - Training Epoch: 2/2, step 6893/7134 completed (loss: 0.03290710225701332, acc: 0.9894859790802002)
[2025-02-13 04:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:45][root][INFO] - Training Epoch: 2/2, step 6894/7134 completed (loss: 0.032651081681251526, acc: 0.99210524559021)
[2025-02-13 04:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46][root][INFO] - Training Epoch: 2/2, step 6895/7134 completed (loss: 0.034549325704574585, acc: 0.9892086386680603)
[2025-02-13 04:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46][root][INFO] - Training Epoch: 2/2, step 6896/7134 completed (loss: 0.01914161816239357, acc: 0.991304337978363)
[2025-02-13 04:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46][root][INFO] - Training Epoch: 2/2, step 6897/7134 completed (loss: 0.011701501905918121, acc: 0.9963189959526062)
[2025-02-13 04:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:47][root][INFO] - Training Epoch: 2/2, step 6898/7134 completed (loss: 0.010148811154067516, acc: 0.9972527623176575)
[2025-02-13 04:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:47][root][INFO] - Training Epoch: 2/2, step 6899/7134 completed (loss: 0.02984865941107273, acc: 0.9924623370170593)
[2025-02-13 04:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:48][root][INFO] - Training Epoch: 2/2, step 6900/7134 completed (loss: 0.024981055408716202, acc: 0.9957567453384399)
[2025-02-13 04:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:48][root][INFO] - Training Epoch: 2/2, step 6901/7134 completed (loss: 0.0296455267816782, acc: 0.9907529950141907)
[2025-02-13 04:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49][root][INFO] - Training Epoch: 2/2, step 6902/7134 completed (loss: 0.018644437193870544, acc: 0.9916782379150391)
[2025-02-13 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49][root][INFO] - Training Epoch: 2/2, step 6903/7134 completed (loss: 0.015419559553265572, acc: 0.9961977005004883)
[2025-02-13 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49][root][INFO] - Training Epoch: 2/2, step 6904/7134 completed (loss: 0.010824809782207012, acc: 0.9950248599052429)
[2025-02-13 04:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:50][root][INFO] - Training Epoch: 2/2, step 6905/7134 completed (loss: 0.024967404082417488, acc: 0.9936061501502991)
[2025-02-13 04:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:50][root][INFO] - Training Epoch: 2/2, step 6906/7134 completed (loss: 0.03516243025660515, acc: 0.9940119981765747)
[2025-02-13 04:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:51][root][INFO] - Training Epoch: 2/2, step 6907/7134 completed (loss: 0.03489234670996666, acc: 0.9937965273857117)
[2025-02-13 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:51][root][INFO] - Training Epoch: 2/2, step 6908/7134 completed (loss: 0.018179522827267647, acc: 0.9948186278343201)
[2025-02-13 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52][root][INFO] - Training Epoch: 2/2, step 6909/7134 completed (loss: 0.016896655783057213, acc: 0.996052622795105)
[2025-02-13 04:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52][root][INFO] - Training Epoch: 2/2, step 6910/7134 completed (loss: 0.020010987296700478, acc: 0.9961783289909363)
[2025-02-13 04:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52][root][INFO] - Training Epoch: 2/2, step 6911/7134 completed (loss: 0.03207152336835861, acc: 0.9941860437393188)
[2025-02-13 04:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:53][root][INFO] - Training Epoch: 2/2, step 6912/7134 completed (loss: 0.031710486859083176, acc: 0.995006263256073)
[2025-02-13 04:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:53][root][INFO] - Training Epoch: 2/2, step 6913/7134 completed (loss: 0.029537945985794067, acc: 0.9914737939834595)
[2025-02-13 04:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54][root][INFO] - Training Epoch: 2/2, step 6914/7134 completed (loss: 0.07501055300235748, acc: 0.9865319728851318)
[2025-02-13 04:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54][root][INFO] - Training Epoch: 2/2, step 6915/7134 completed (loss: 0.021582430228590965, acc: 0.9948186278343201)
[2025-02-13 04:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:55][root][INFO] - Training Epoch: 2/2, step 6916/7134 completed (loss: 0.020815616473555565, acc: 0.9975460171699524)
[2025-02-13 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:55][root][INFO] - Training Epoch: 2/2, step 6917/7134 completed (loss: 0.04629906639456749, acc: 0.9911616444587708)
[2025-02-13 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56][root][INFO] - Training Epoch: 2/2, step 6918/7134 completed (loss: 0.01129157841205597, acc: 0.9964871406555176)
[2025-02-13 04:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56][root][INFO] - Training Epoch: 2/2, step 6919/7134 completed (loss: 0.027648381888866425, acc: 0.9953863620758057)
[2025-02-13 04:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56][root][INFO] - Training Epoch: 2/2, step 6920/7134 completed (loss: 0.009252526797354221, acc: 0.995726466178894)
[2025-02-13 04:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57][root][INFO] - Training Epoch: 2/2, step 6921/7134 completed (loss: 0.04637579619884491, acc: 0.991916835308075)
[2025-02-13 04:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57][root][INFO] - Training Epoch: 2/2, step 6922/7134 completed (loss: 0.035339031368494034, acc: 0.9906396269798279)
[2025-02-13 04:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:58][root][INFO] - Training Epoch: 2/2, step 6923/7134 completed (loss: 0.045790135860443115, acc: 0.9922178983688354)
[2025-02-13 04:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:58][root][INFO] - Training Epoch: 2/2, step 6924/7134 completed (loss: 0.016363773494958878, acc: 0.9945504069328308)
[2025-02-13 04:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59][root][INFO] - Training Epoch: 2/2, step 6925/7134 completed (loss: 0.012585685588419437, acc: 0.9973958134651184)
[2025-02-13 04:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59][root][INFO] - Training Epoch: 2/2, step 6926/7134 completed (loss: 0.018251238390803337, acc: 0.9927667379379272)
[2025-02-13 04:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59][root][INFO] - Training Epoch: 2/2, step 6927/7134 completed (loss: 0.049947842955589294, acc: 0.9856114983558655)
[2025-02-13 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:00][root][INFO] - Training Epoch: 2/2, step 6928/7134 completed (loss: 0.07124822586774826, acc: 0.9823874831199646)
[2025-02-13 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:00][root][INFO] - Training Epoch: 2/2, step 6929/7134 completed (loss: 0.02546634152531624, acc: 0.994350254535675)
[2025-02-13 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:01][root][INFO] - Training Epoch: 2/2, step 6930/7134 completed (loss: 0.027645761147141457, acc: 0.9875930547714233)
[2025-02-13 04:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:01][root][INFO] - Training Epoch: 2/2, step 6931/7134 completed (loss: 0.025101598352193832, acc: 0.9939024448394775)
[2025-02-13 04:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02][root][INFO] - Training Epoch: 2/2, step 6932/7134 completed (loss: 0.031794916838407516, acc: 0.9897058606147766)
[2025-02-13 04:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02][root][INFO] - Training Epoch: 2/2, step 6933/7134 completed (loss: 0.07697410136461258, acc: 0.9805951118469238)
[2025-02-13 04:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02][root][INFO] - Training Epoch: 2/2, step 6934/7134 completed (loss: 0.03438327834010124, acc: 0.9903692007064819)
[2025-02-13 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:03][root][INFO] - Training Epoch: 2/2, step 6935/7134 completed (loss: 0.028680969029664993, acc: 0.9921466112136841)
[2025-02-13 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:03][root][INFO] - Training Epoch: 2/2, step 6936/7134 completed (loss: 0.012922371737658978, acc: 0.9959999918937683)
[2025-02-13 04:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:04][root][INFO] - Training Epoch: 2/2, step 6937/7134 completed (loss: 0.08094082772731781, acc: 0.9789842367172241)
[2025-02-13 04:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:04][root][INFO] - Training Epoch: 2/2, step 6938/7134 completed (loss: 0.08703505992889404, acc: 0.981249988079071)
[2025-02-13 04:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05][root][INFO] - Training Epoch: 2/2, step 6939/7134 completed (loss: 0.07346814125776291, acc: 0.9888734221458435)
[2025-02-13 04:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05][root][INFO] - Training Epoch: 2/2, step 6940/7134 completed (loss: 0.05374033376574516, acc: 0.9863013625144958)
[2025-02-13 04:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05][root][INFO] - Training Epoch: 2/2, step 6941/7134 completed (loss: 0.024939898401498795, acc: 0.9947916865348816)
[2025-02-13 04:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06][root][INFO] - Training Epoch: 2/2, step 6942/7134 completed (loss: 0.03192996606230736, acc: 0.9917808175086975)
[2025-02-13 04:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06][root][INFO] - Training Epoch: 2/2, step 6943/7134 completed (loss: 0.03699205815792084, acc: 0.9937238693237305)
[2025-02-13 04:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07][root][INFO] - Training Epoch: 2/2, step 6944/7134 completed (loss: 0.023708350956439972, acc: 0.9952229261398315)
[2025-02-13 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07][root][INFO] - Training Epoch: 2/2, step 6945/7134 completed (loss: 0.051854562014341354, acc: 0.9877836108207703)
[2025-02-13 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07][root][INFO] - Training Epoch: 2/2, step 6946/7134 completed (loss: 0.022176416590809822, acc: 0.9914039969444275)
[2025-02-13 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:08][root][INFO] - Training Epoch: 2/2, step 6947/7134 completed (loss: 0.03616456314921379, acc: 0.991304337978363)
[2025-02-13 04:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:08][root][INFO] - Training Epoch: 2/2, step 6948/7134 completed (loss: 0.028582101687788963, acc: 0.9899497628211975)
[2025-02-13 04:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09][root][INFO] - Training Epoch: 2/2, step 6949/7134 completed (loss: 0.0455571785569191, acc: 0.9893162250518799)
[2025-02-13 04:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09][root][INFO] - Training Epoch: 2/2, step 6950/7134 completed (loss: 0.02091403864324093, acc: 0.9974489808082581)
[2025-02-13 04:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09][root][INFO] - Training Epoch: 2/2, step 6951/7134 completed (loss: 0.019659897312521935, acc: 0.993220329284668)
[2025-02-13 04:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10][root][INFO] - Training Epoch: 2/2, step 6952/7134 completed (loss: 0.013947155326604843, acc: 0.9953271150588989)
[2025-02-13 04:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10][root][INFO] - Training Epoch: 2/2, step 6953/7134 completed (loss: 0.015253104269504547, acc: 0.9934425950050354)
[2025-02-13 04:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10][root][INFO] - Training Epoch: 2/2, step 6954/7134 completed (loss: 0.050637099891901016, acc: 0.9872068166732788)
[2025-02-13 04:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:11][root][INFO] - Training Epoch: 2/2, step 6955/7134 completed (loss: 0.02786257490515709, acc: 0.9939024448394775)
[2025-02-13 04:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:11][root][INFO] - Training Epoch: 2/2, step 6956/7134 completed (loss: 0.027598606422543526, acc: 0.9928951859474182)
[2025-02-13 04:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12][root][INFO] - Training Epoch: 2/2, step 6957/7134 completed (loss: 0.013586917892098427, acc: 0.9960861206054688)
[2025-02-13 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12][root][INFO] - Training Epoch: 2/2, step 6958/7134 completed (loss: 0.040899671614170074, acc: 0.9824561476707458)
[2025-02-13 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12][root][INFO] - Training Epoch: 2/2, step 6959/7134 completed (loss: 0.03080632910132408, acc: 0.9866962432861328)
[2025-02-13 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:13][root][INFO] - Training Epoch: 2/2, step 6960/7134 completed (loss: 0.021742437034845352, acc: 0.9934959411621094)
[2025-02-13 04:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:13][root][INFO] - Training Epoch: 2/2, step 6961/7134 completed (loss: 0.03125346824526787, acc: 0.9939393997192383)
[2025-02-13 04:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14][root][INFO] - Training Epoch: 2/2, step 6962/7134 completed (loss: 0.03146779537200928, acc: 0.9930434823036194)
[2025-02-13 04:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14][root][INFO] - Training Epoch: 2/2, step 6963/7134 completed (loss: 0.012551340274512768, acc: 0.9983165264129639)
[2025-02-13 04:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14][root][INFO] - Training Epoch: 2/2, step 6964/7134 completed (loss: 0.016382817178964615, acc: 0.994584858417511)
[2025-02-13 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:15][root][INFO] - Training Epoch: 2/2, step 6965/7134 completed (loss: 0.013516401872038841, acc: 0.9945054650306702)
[2025-02-13 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:15][root][INFO] - Training Epoch: 2/2, step 6966/7134 completed (loss: 0.027070391923189163, acc: 0.9850373864173889)
[2025-02-13 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16][root][INFO] - Training Epoch: 2/2, step 6967/7134 completed (loss: 0.017926165834069252, acc: 0.9924585223197937)
[2025-02-13 04:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16][root][INFO] - Training Epoch: 2/2, step 6968/7134 completed (loss: 0.026300830766558647, acc: 0.9913793206214905)
[2025-02-13 04:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16][root][INFO] - Training Epoch: 2/2, step 6969/7134 completed (loss: 0.06090313568711281, acc: 0.9833679795265198)
[2025-02-13 04:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:17][root][INFO] - Training Epoch: 2/2, step 6970/7134 completed (loss: 0.06570357084274292, acc: 0.9789473414421082)
[2025-02-13 04:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:17][root][INFO] - Training Epoch: 2/2, step 6971/7134 completed (loss: 0.04279417172074318, acc: 0.984455943107605)
[2025-02-13 04:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18][root][INFO] - Training Epoch: 2/2, step 6972/7134 completed (loss: 0.018837928771972656, acc: 0.9928698539733887)
[2025-02-13 04:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18][root][INFO] - Training Epoch: 2/2, step 6973/7134 completed (loss: 0.0331634059548378, acc: 0.9933920502662659)
[2025-02-13 04:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18][root][INFO] - Training Epoch: 2/2, step 6974/7134 completed (loss: 0.038542285561561584, acc: 0.9896551966667175)
[2025-02-13 04:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:19][root][INFO] - Training Epoch: 2/2, step 6975/7134 completed (loss: 0.0306264478713274, acc: 0.9934924244880676)
[2025-02-13 04:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:19][root][INFO] - Training Epoch: 2/2, step 6976/7134 completed (loss: 0.04408976435661316, acc: 0.9861351847648621)
[2025-02-13 04:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20][root][INFO] - Training Epoch: 2/2, step 6977/7134 completed (loss: 0.03563627600669861, acc: 0.9887217879295349)
[2025-02-13 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20][root][INFO] - Training Epoch: 2/2, step 6978/7134 completed (loss: 0.01466476172208786, acc: 0.9983739852905273)
[2025-02-13 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20][root][INFO] - Training Epoch: 2/2, step 6979/7134 completed (loss: 0.030689815059304237, acc: 0.9939024448394775)
[2025-02-13 04:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:21][root][INFO] - Training Epoch: 2/2, step 6980/7134 completed (loss: 0.09190475940704346, acc: 0.9723270535469055)
[2025-02-13 04:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:21][root][INFO] - Training Epoch: 2/2, step 6981/7134 completed (loss: 0.09803705662488937, acc: 0.9782886505126953)
[2025-02-13 04:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:22][root][INFO] - Training Epoch: 2/2, step 6982/7134 completed (loss: 0.06239326670765877, acc: 0.9853836894035339)
[2025-02-13 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:22][root][INFO] - Training Epoch: 2/2, step 6983/7134 completed (loss: 0.09663911908864975, acc: 0.9777448177337646)
[2025-02-13 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:23][root][INFO] - Training Epoch: 2/2, step 6984/7134 completed (loss: 0.03082474134862423, acc: 0.9935691356658936)
[2025-02-13 04:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:23][root][INFO] - Training Epoch: 2/2, step 6985/7134 completed (loss: 0.09247253090143204, acc: 0.9794871807098389)
[2025-02-13 04:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24][root][INFO] - Training Epoch: 2/2, step 6986/7134 completed (loss: 0.02800581604242325, acc: 0.9881129264831543)
[2025-02-13 04:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24][root][INFO] - Training Epoch: 2/2, step 6987/7134 completed (loss: 0.03975118324160576, acc: 0.986868679523468)
[2025-02-13 04:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24][root][INFO] - Training Epoch: 2/2, step 6988/7134 completed (loss: 0.012105311267077923, acc: 1.0)
[2025-02-13 04:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25][root][INFO] - Training Epoch: 2/2, step 6989/7134 completed (loss: 0.033412303775548935, acc: 0.9927272796630859)
[2025-02-13 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25][root][INFO] - Training Epoch: 2/2, step 6990/7134 completed (loss: 0.007680060807615519, acc: 1.0)
[2025-02-13 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25][root][INFO] - Training Epoch: 2/2, step 6991/7134 completed (loss: 0.045670028775930405, acc: 0.9899749159812927)
[2025-02-13 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:26][root][INFO] - Training Epoch: 2/2, step 6992/7134 completed (loss: 0.03547422215342522, acc: 0.9894737005233765)
[2025-02-13 04:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:26][root][INFO] - Training Epoch: 2/2, step 6993/7134 completed (loss: 0.07902088761329651, acc: 0.9788838624954224)
[2025-02-13 04:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27][root][INFO] - Training Epoch: 2/2, step 6994/7134 completed (loss: 0.08669275045394897, acc: 0.9791332483291626)
[2025-02-13 04:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27][root][INFO] - Training Epoch: 2/2, step 6995/7134 completed (loss: 0.036414481699466705, acc: 0.9882199168205261)
[2025-02-13 04:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27][root][INFO] - Training Epoch: 2/2, step 6996/7134 completed (loss: 0.024567613378167152, acc: 0.9918830990791321)
[2025-02-13 04:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:28][root][INFO] - Training Epoch: 2/2, step 6997/7134 completed (loss: 0.03488703444600105, acc: 0.9906542301177979)
[2025-02-13 04:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:28][root][INFO] - Training Epoch: 2/2, step 6998/7134 completed (loss: 0.04288024827837944, acc: 0.9901960492134094)
[2025-02-13 04:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29][root][INFO] - Training Epoch: 2/2, step 6999/7134 completed (loss: 0.02017253078520298, acc: 0.9934640526771545)
[2025-02-13 04:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29][root][INFO] - Training Epoch: 2/2, step 7000/7134 completed (loss: 0.012170590460300446, acc: 0.9957627058029175)
[2025-02-13 04:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29][root][INFO] - Training Epoch: 2/2, step 7001/7134 completed (loss: 0.06664912402629852, acc: 0.9864457845687866)
[2025-02-13 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:30][root][INFO] - Training Epoch: 2/2, step 7002/7134 completed (loss: 0.016451191157102585, acc: 0.9918864369392395)
[2025-02-13 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:30][root][INFO] - Training Epoch: 2/2, step 7003/7134 completed (loss: 0.012300556525588036, acc: 0.9956188201904297)
[2025-02-13 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:31][root][INFO] - Training Epoch: 2/2, step 7004/7134 completed (loss: 0.05930647253990173, acc: 0.9842519760131836)
[2025-02-13 04:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:31][root][INFO] - Training Epoch: 2/2, step 7005/7134 completed (loss: 0.04655047133564949, acc: 0.9811066389083862)
[2025-02-13 04:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:32][root][INFO] - Training Epoch: 2/2, step 7006/7134 completed (loss: 0.02804119512438774, acc: 0.99452805519104)
[2025-02-13 04:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:32][root][INFO] - Training Epoch: 2/2, step 7007/7134 completed (loss: 0.02658879943192005, acc: 0.9940119981765747)
[2025-02-13 04:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33][root][INFO] - Training Epoch: 2/2, step 7008/7134 completed (loss: 0.02326422743499279, acc: 0.9901356101036072)
[2025-02-13 04:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33][root][INFO] - Training Epoch: 2/2, step 7009/7134 completed (loss: 0.021107852458953857, acc: 0.9942129850387573)
[2025-02-13 04:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33][root][INFO] - Training Epoch: 2/2, step 7010/7134 completed (loss: 0.008081174455583096, acc: 0.9984591603279114)
[2025-02-13 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:34][root][INFO] - Training Epoch: 2/2, step 7011/7134 completed (loss: 0.027928190305829048, acc: 0.9879807829856873)
[2025-02-13 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:34][root][INFO] - Training Epoch: 2/2, step 7012/7134 completed (loss: 0.032012902200222015, acc: 0.9883381724357605)
[2025-02-13 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35][root][INFO] - Training Epoch: 2/2, step 7013/7134 completed (loss: 0.0067898486740887165, acc: 0.9985590577125549)
[2025-02-13 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35][root][INFO] - Training Epoch: 2/2, step 7014/7134 completed (loss: 0.020702961832284927, acc: 0.9919484853744507)
[2025-02-13 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35][root][INFO] - Training Epoch: 2/2, step 7015/7134 completed (loss: 0.023378798738121986, acc: 0.989266574382782)
[2025-02-13 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:36][root][INFO] - Training Epoch: 2/2, step 7016/7134 completed (loss: 0.035556305199861526, acc: 0.9913294911384583)
[2025-02-13 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:36][root][INFO] - Training Epoch: 2/2, step 7017/7134 completed (loss: 0.0429370179772377, acc: 0.9875518679618835)
[2025-02-13 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37][root][INFO] - Training Epoch: 2/2, step 7018/7134 completed (loss: 0.024918079376220703, acc: 0.9937984347343445)
[2025-02-13 04:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37][root][INFO] - Training Epoch: 2/2, step 7019/7134 completed (loss: 0.09972458332777023, acc: 0.9780701994895935)
[2025-02-13 04:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37][root][INFO] - Training Epoch: 2/2, step 7020/7134 completed (loss: 0.016563689336180687, acc: 0.9985895752906799)
[2025-02-13 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:38][root][INFO] - Training Epoch: 2/2, step 7021/7134 completed (loss: 0.020139046013355255, acc: 0.9931856989860535)
[2025-02-13 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:38][root][INFO] - Training Epoch: 2/2, step 7022/7134 completed (loss: 0.08908021450042725, acc: 0.9722222089767456)
[2025-02-13 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39][root][INFO] - Training Epoch: 2/2, step 7023/7134 completed (loss: 0.07087386399507523, acc: 0.9761336445808411)
[2025-02-13 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39][root][INFO] - Training Epoch: 2/2, step 7024/7134 completed (loss: 0.0316428504884243, acc: 0.9904458522796631)
[2025-02-13 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39][root][INFO] - Training Epoch: 2/2, step 7025/7134 completed (loss: 0.0847000852227211, acc: 0.9655172228813171)
[2025-02-13 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40][root][INFO] - Training Epoch: 2/2, step 7026/7134 completed (loss: 0.06251875311136246, acc: 0.9768339991569519)
[2025-02-13 04:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40][root][INFO] - Training Epoch: 2/2, step 7027/7134 completed (loss: 0.05507718771696091, acc: 0.9786585569381714)
[2025-02-13 04:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40][root][INFO] - Training Epoch: 2/2, step 7028/7134 completed (loss: 0.02587864361703396, acc: 0.990314781665802)
[2025-02-13 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41][root][INFO] - Training Epoch: 2/2, step 7029/7134 completed (loss: 0.024312006309628487, acc: 0.9907192587852478)
[2025-02-13 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41][root][INFO] - Training Epoch: 2/2, step 7030/7134 completed (loss: 0.05248238891363144, acc: 0.9857819676399231)
[2025-02-13 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41][root][INFO] - Training Epoch: 2/2, step 7031/7134 completed (loss: 0.055361758917570114, acc: 0.9796954393386841)
[2025-02-13 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42][root][INFO] - Training Epoch: 2/2, step 7032/7134 completed (loss: 0.037607237696647644, acc: 0.9918699264526367)
[2025-02-13 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42][root][INFO] - Training Epoch: 2/2, step 7033/7134 completed (loss: 0.048012617975473404, acc: 0.9794721603393555)
[2025-02-13 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42][root][INFO] - Training Epoch: 2/2, step 7034/7134 completed (loss: 0.058939605951309204, acc: 0.9787985682487488)
[2025-02-13 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43][root][INFO] - Training Epoch: 2/2, step 7035/7134 completed (loss: 0.0718325525522232, acc: 0.976190447807312)
[2025-02-13 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43][root][INFO] - Training Epoch: 2/2, step 7036/7134 completed (loss: 0.015779269859194756, acc: 1.0)
[2025-02-13 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43][root][INFO] - Training Epoch: 2/2, step 7037/7134 completed (loss: 0.022176744416356087, acc: 0.99609375)
[2025-02-13 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44][root][INFO] - Training Epoch: 2/2, step 7038/7134 completed (loss: 0.09379623085260391, acc: 0.9736147522926331)
[2025-02-13 04:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44][root][INFO] - Training Epoch: 2/2, step 7039/7134 completed (loss: 0.016428008675575256, acc: 0.990338146686554)
[2025-02-13 04:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44][root][INFO] - Training Epoch: 2/2, step 7040/7134 completed (loss: 0.089031882584095, acc: 0.9712643623352051)
[2025-02-13 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:45][root][INFO] - Training Epoch: 2/2, step 7041/7134 completed (loss: 0.08133412897586823, acc: 0.9779874086380005)
[2025-02-13 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:45][root][INFO] - Training Epoch: 2/2, step 7042/7134 completed (loss: 0.03838048502802849, acc: 0.9876922965049744)
[2025-02-13 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:45][root][INFO] - Training Epoch: 2/2, step 7043/7134 completed (loss: 0.06291873008012772, acc: 0.9733333587646484)
[2025-02-13 04:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:46][root][INFO] - Training Epoch: 2/2, step 7044/7134 completed (loss: 0.06234842538833618, acc: 0.9816513657569885)
[2025-02-13 04:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:46][root][INFO] - Training Epoch: 2/2, step 7045/7134 completed (loss: 0.03467869013547897, acc: 0.9914966225624084)
[2025-02-13 04:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47][root][INFO] - Training Epoch: 2/2, step 7046/7134 completed (loss: 0.059213705360889435, acc: 0.9836065769195557)
[2025-02-13 04:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47][root][INFO] - Training Epoch: 2/2, step 7047/7134 completed (loss: 0.04886086285114288, acc: 0.9833333492279053)
[2025-02-13 04:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47][root][INFO] - Training Epoch: 2/2, step 7048/7134 completed (loss: 0.00418060040101409, acc: 0.9988248944282532)
[2025-02-13 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:48][root][INFO] - Training Epoch: 2/2, step 7049/7134 completed (loss: 0.02986198104918003, acc: 0.9919785857200623)
[2025-02-13 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:48][root][INFO] - Training Epoch: 2/2, step 7050/7134 completed (loss: 0.025448059663176537, acc: 0.9948186278343201)
[2025-02-13 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:49][root][INFO] - Training Epoch: 2/2, step 7051/7134 completed (loss: 0.028158217668533325, acc: 0.993678867816925)
[2025-02-13 04:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:49][root][INFO] - Training Epoch: 2/2, step 7052/7134 completed (loss: 0.028646083548665047, acc: 0.9906790852546692)
[2025-02-13 04:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50][root][INFO] - Training Epoch: 2/2, step 7053/7134 completed (loss: 0.018995363265275955, acc: 0.9951279163360596)
[2025-02-13 04:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50][root][INFO] - Training Epoch: 2/2, step 7054/7134 completed (loss: 0.06397070735692978, acc: 0.982594907283783)
[2025-02-13 04:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50][root][INFO] - Training Epoch: 2/2, step 7055/7134 completed (loss: 0.03621706739068031, acc: 0.9897959232330322)
[2025-02-13 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:51][root][INFO] - Training Epoch: 2/2, step 7056/7134 completed (loss: 0.04158565029501915, acc: 0.9867947101593018)
[2025-02-13 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:51][root][INFO] - Training Epoch: 2/2, step 7057/7134 completed (loss: 0.0861203521490097, acc: 0.9808219075202942)
[2025-02-13 04:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:52][root][INFO] - Training Epoch: 2/2, step 7058/7134 completed (loss: 0.05580872669816017, acc: 0.9862204790115356)
[2025-02-13 04:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:52][root][INFO] - Training Epoch: 2/2, step 7059/7134 completed (loss: 0.008362253196537495, acc: 0.998603343963623)
[2025-02-13 04:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53][root][INFO] - Training Epoch: 2/2, step 7060/7134 completed (loss: 0.0618506595492363, acc: 0.9822580814361572)
[2025-02-13 04:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53][root][INFO] - Training Epoch: 2/2, step 7061/7134 completed (loss: 0.0310154240578413, acc: 0.9921875)
[2025-02-13 04:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53][root][INFO] - Training Epoch: 2/2, step 7062/7134 completed (loss: 0.06959126144647598, acc: 0.9822221994400024)
[2025-02-13 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:54][root][INFO] - Training Epoch: 2/2, step 7063/7134 completed (loss: 0.05743153393268585, acc: 0.9895424842834473)
[2025-02-13 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:54][root][INFO] - Training Epoch: 2/2, step 7064/7134 completed (loss: 0.0207442045211792, acc: 0.9955489635467529)
[2025-02-13 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55][root][INFO] - Training Epoch: 2/2, step 7065/7134 completed (loss: 0.013288429006934166, acc: 0.9942062497138977)
[2025-02-13 04:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55][root][INFO] - Training Epoch: 2/2, step 7066/7134 completed (loss: 0.01402309350669384, acc: 0.9952437281608582)
[2025-02-13 04:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:56][root][INFO] - Training Epoch: 2/2, step 7067/7134 completed (loss: 0.019750753417611122, acc: 0.9910827875137329)
[2025-02-13 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:56][root][INFO] - Training Epoch: 2/2, step 7068/7134 completed (loss: 0.021700019016861916, acc: 0.9915561079978943)
[2025-02-13 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57][root][INFO] - Training Epoch: 2/2, step 7069/7134 completed (loss: 0.02617393247783184, acc: 0.9940828680992126)
[2025-02-13 04:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57][root][INFO] - Training Epoch: 2/2, step 7070/7134 completed (loss: 0.022303273901343346, acc: 0.9945130348205566)
[2025-02-13 04:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57][root][INFO] - Training Epoch: 2/2, step 7071/7134 completed (loss: 0.041188158094882965, acc: 0.9924812316894531)
[2025-02-13 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:58][root][INFO] - Training Epoch: 2/2, step 7072/7134 completed (loss: 0.016628218814730644, acc: 0.9939393997192383)
[2025-02-13 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:58][root][INFO] - Training Epoch: 2/2, step 7073/7134 completed (loss: 0.026554688811302185, acc: 0.9897611141204834)
[2025-02-13 04:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:59][root][INFO] - Training Epoch: 2/2, step 7074/7134 completed (loss: 0.026335999369621277, acc: 0.9942638874053955)
[2025-02-13 04:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:59][root][INFO] - Training Epoch: 2/2, step 7075/7134 completed (loss: 0.023662937805056572, acc: 0.9910314083099365)
[2025-02-13 04:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00][root][INFO] - Training Epoch: 2/2, step 7076/7134 completed (loss: 0.030411696061491966, acc: 0.9928774833679199)
[2025-02-13 04:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00][root][INFO] - Training Epoch: 2/2, step 7077/7134 completed (loss: 0.05654273182153702, acc: 0.9836065769195557)
[2025-02-13 04:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00][root][INFO] - Training Epoch: 2/2, step 7078/7134 completed (loss: 0.020349111407995224, acc: 0.995192289352417)
[2025-02-13 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:01][root][INFO] - Training Epoch: 2/2, step 7079/7134 completed (loss: 0.023591559380292892, acc: 0.9917898178100586)
[2025-02-13 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:01][root][INFO] - Training Epoch: 2/2, step 7080/7134 completed (loss: 0.02166913077235222, acc: 0.9933554530143738)
[2025-02-13 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02][root][INFO] - Training Epoch: 2/2, step 7081/7134 completed (loss: 0.026645002886652946, acc: 0.9929873943328857)
[2025-02-13 04:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02][root][INFO] - Training Epoch: 2/2, step 7082/7134 completed (loss: 0.03326490521430969, acc: 0.9951534867286682)
[2025-02-13 04:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02][root][INFO] - Training Epoch: 2/2, step 7083/7134 completed (loss: 0.008594637736678123, acc: 0.9949832558631897)
[2025-02-13 04:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:03][root][INFO] - Training Epoch: 2/2, step 7084/7134 completed (loss: 0.03388388082385063, acc: 0.9898403286933899)
[2025-02-13 04:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:03][root][INFO] - Training Epoch: 2/2, step 7085/7134 completed (loss: 0.03870071843266487, acc: 0.9907407164573669)
[2025-02-13 04:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04][root][INFO] - Training Epoch: 2/2, step 7086/7134 completed (loss: 0.014227267354726791, acc: 0.99589604139328)
[2025-02-13 04:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04][root][INFO] - Training Epoch: 2/2, step 7087/7134 completed (loss: 0.023198306560516357, acc: 0.996303141117096)
[2025-02-13 04:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04][root][INFO] - Training Epoch: 2/2, step 7088/7134 completed (loss: 0.042712677270174026, acc: 0.9933775067329407)
[2025-02-13 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:05][root][INFO] - Training Epoch: 2/2, step 7089/7134 completed (loss: 0.08358421176671982, acc: 0.9840764403343201)
[2025-02-13 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:05][root][INFO] - Training Epoch: 2/2, step 7090/7134 completed (loss: 0.02638820745050907, acc: 0.9932249188423157)
[2025-02-13 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06][root][INFO] - Training Epoch: 2/2, step 7091/7134 completed (loss: 0.009016769006848335, acc: 0.9958217144012451)
[2025-02-13 04:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06][root][INFO] - Training Epoch: 2/2, step 7092/7134 completed (loss: 0.017704837024211884, acc: 0.9914236664772034)
[2025-02-13 04:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07][root][INFO] - Training Epoch: 2/2, step 7093/7134 completed (loss: 0.029099950566887856, acc: 0.9920634627342224)
[2025-02-13 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07][root][INFO] - Training Epoch: 2/2, step 7094/7134 completed (loss: 0.014123964123427868, acc: 0.9953917264938354)
[2025-02-13 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07][root][INFO] - Training Epoch: 2/2, step 7095/7134 completed (loss: 0.02912411279976368, acc: 0.9958677887916565)
[2025-02-13 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:08][root][INFO] - Training Epoch: 2/2, step 7096/7134 completed (loss: 0.04411541670560837, acc: 0.9882352948188782)
[2025-02-13 04:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:08][root][INFO] - Training Epoch: 2/2, step 7097/7134 completed (loss: 0.029401402920484543, acc: 0.9899135231971741)
[2025-02-13 04:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09][root][INFO] - Training Epoch: 2/2, step 7098/7134 completed (loss: 0.013958914205431938, acc: 0.995184600353241)
[2025-02-13 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09][root][INFO] - Training Epoch: 2/2, step 7099/7134 completed (loss: 0.024833641946315765, acc: 0.9914529919624329)
[2025-02-13 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09][root][INFO] - Training Epoch: 2/2, step 7100/7134 completed (loss: 0.04341134801506996, acc: 0.9844961166381836)
[2025-02-13 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10][root][INFO] - Training Epoch: 2/2, step 7101/7134 completed (loss: 0.04624184966087341, acc: 0.9874776601791382)
[2025-02-13 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10][root][INFO] - Training Epoch: 2/2, step 7102/7134 completed (loss: 0.00844124797731638, acc: 0.998062014579773)
[2025-02-13 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10][root][INFO] - Training Epoch: 2/2, step 7103/7134 completed (loss: 0.024291440844535828, acc: 0.9959677457809448)
[2025-02-13 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:11][root][INFO] - Training Epoch: 2/2, step 7104/7134 completed (loss: 0.014202666468918324, acc: 0.9958847761154175)
[2025-02-13 04:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:11][root][INFO] - Training Epoch: 2/2, step 7105/7134 completed (loss: 0.03886174038052559, acc: 0.9900000095367432)
[2025-02-13 04:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12][root][INFO] - Training Epoch: 2/2, step 7106/7134 completed (loss: 0.017260249704122543, acc: 0.9966611266136169)
[2025-02-13 04:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12][root][INFO] - Training Epoch: 2/2, step 7107/7134 completed (loss: 0.014502436853945255, acc: 0.9953415989875793)
[2025-02-13 04:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12][root][INFO] - Training Epoch: 2/2, step 7108/7134 completed (loss: 0.028406841680407524, acc: 0.9933333396911621)
[2025-02-13 04:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13][root][INFO] - Training Epoch: 2/2, step 7109/7134 completed (loss: 0.03991864621639252, acc: 0.9887429475784302)
[2025-02-13 04:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13][root][INFO] - Training Epoch: 2/2, step 7110/7134 completed (loss: 0.01918541081249714, acc: 0.996610164642334)
[2025-02-13 04:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13][root][INFO] - Training Epoch: 2/2, step 7111/7134 completed (loss: 0.03570990264415741, acc: 0.9911308288574219)
[2025-02-13 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:14][root][INFO] - Training Epoch: 2/2, step 7112/7134 completed (loss: 0.01750257797539234, acc: 0.9958847761154175)
[2025-02-13 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:14][root][INFO] - Training Epoch: 2/2, step 7113/7134 completed (loss: 0.05846207216382027, acc: 0.9779179692268372)
[2025-02-13 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15][root][INFO] - Training Epoch: 2/2, step 7114/7134 completed (loss: 0.02418322116136551, acc: 0.990439772605896)
[2025-02-13 04:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15][root][INFO] - Training Epoch: 2/2, step 7115/7134 completed (loss: 0.03365461155772209, acc: 0.9876543283462524)
[2025-02-13 04:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15][root][INFO] - Training Epoch: 2/2, step 7116/7134 completed (loss: 0.0212787427008152, acc: 0.994727611541748)
[2025-02-13 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:16][root][INFO] - Training Epoch: 2/2, step 7117/7134 completed (loss: 0.01564628817141056, acc: 0.996666669845581)
[2025-02-13 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:16][root][INFO] - Training Epoch: 2/2, step 7118/7134 completed (loss: 0.04245788976550102, acc: 0.9912790656089783)
[2025-02-13 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17][root][INFO] - Training Epoch: 2/2, step 7119/7134 completed (loss: 0.013405511155724525, acc: 0.9934533834457397)
[2025-02-13 04:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17][root][INFO] - Training Epoch: 2/2, step 7120/7134 completed (loss: 0.03376620635390282, acc: 0.9883177280426025)
[2025-02-13 04:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17][root][INFO] - Training Epoch: 2/2, step 7121/7134 completed (loss: 0.011282175779342651, acc: 0.9982394576072693)
[2025-02-13 04:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18][root][INFO] - Training Epoch: 2/2, step 7122/7134 completed (loss: 0.018254784867167473, acc: 0.9919785857200623)
[2025-02-13 04:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18][root][INFO] - Training Epoch: 2/2, step 7123/7134 completed (loss: 0.01717439480125904, acc: 0.9902439117431641)
[2025-02-13 04:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18][root][INFO] - Training Epoch: 2/2, step 7124/7134 completed (loss: 0.025301484391093254, acc: 0.9906191229820251)
[2025-02-13 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:19][root][INFO] - Training Epoch: 2/2, step 7125/7134 completed (loss: 0.015705175697803497, acc: 0.9933664798736572)
[2025-02-13 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:19][root][INFO] - Training Epoch: 2/2, step 7126/7134 completed (loss: 0.05613615736365318, acc: 0.9831365942955017)
[2025-02-13 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20][root][INFO] - Training Epoch: 2/2, step 7127/7134 completed (loss: 0.018290730193257332, acc: 0.993630588054657)
[2025-02-13 04:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20][root][INFO] - Training Epoch: 2/2, step 7128/7134 completed (loss: 0.023612171411514282, acc: 0.9918699264526367)
[2025-02-13 04:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20][root][INFO] - Training Epoch: 2/2, step 7129/7134 completed (loss: 0.046767957508563995, acc: 0.9798657894134521)
[2025-02-13 04:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:37][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0509, device='cuda:0') eval_epoch_loss=tensor(0.0497, device='cuda:0') eval_epoch_acc=tensor(0.9870, device='cuda:0')
[2025-02-13 04:36:37][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:36:37][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:36:37][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_7130_loss_0.04968715086579323/model.pt
[2025-02-13 04:36:37][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:36:37][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9870140552520752
[2025-02-13 04:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:38][root][INFO] - Training Epoch: 2/2, step 7130/7134 completed (loss: 0.080985426902771, acc: 0.9750000238418579)
[2025-02-13 04:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:38][root][INFO] - Training Epoch: 2/2, step 7131/7134 completed (loss: 0.011292980052530766, acc: 0.9930394291877747)
[2025-02-13 04:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:39][root][INFO] - Training Epoch: 2/2, step 7132/7134 completed (loss: 0.02481071464717388, acc: 0.9932659864425659)
[2025-02-13 04:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:39][root][INFO] - Training Epoch: 2/2, step 7133/7134 completed (loss: 0.010654238052666187, acc: 0.9944547414779663)
[2025-02-13 04:36:39][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.0385, train_epoch_loss=0.0377, epoch time 3981.0773158855736s
[2025-02-13 04:36:39][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2025-02-13 04:36:39][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 30 GB
[2025-02-13 04:36:39][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2025-02-13 04:36:39][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2025-02-13 04:36:39][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 8 GB
[2025-02-13 04:36:39][root][INFO] - Key: avg_train_prep, Value: 1.0796785354614258
[2025-02-13 04:36:39][root][INFO] - Key: avg_train_loss, Value: 0.07593381404876709
[2025-02-13 04:36:39][root][INFO] - Key: avg_train_acc, Value: 0.9795873761177063
[2025-02-13 04:36:39][root][INFO] - Key: avg_eval_prep, Value: 1.0628242492675781
[2025-02-13 04:36:39][root][INFO] - Key: avg_eval_loss, Value: 0.0607844777405262
[2025-02-13 04:36:39][root][INFO] - Key: avg_eval_acc, Value: 0.9836357831954956
[2025-02-13 04:36:39][root][INFO] - Key: avg_epoch_time, Value: 3975.2100779376924
[2025-02-13 04:36:39][root][INFO] - Key: avg_checkpoint_time, Value: 0.3045533783733845
Selected lowest loss checkpoint: asr_epoch_2_step_3564_loss_0.04742717742919922
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04742717742919922/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04742717742919922
[2025-02-13 04:37:04][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-13 04:37:04][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 04:37:04][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme'}
[2025-02-13 04:37:06][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 04:37:12][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 04:37:12][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 04:37:12][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 04:37:12][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 04:37:17][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 04:37:17][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 04:37:17][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 04:37:17][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 04:37:17][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 04:37:18][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 04:37:18][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 04:37:18][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04742717742919922/model.pt
[2025-02-13 04:37:18][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 04:37:18][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 04:37:20][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 04:37:21][root][INFO] - --> Training Set Length = 2620
[2025-02-13 04:37:21][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/repetition_penalty.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-02-13 04:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:54][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_pred_20250213_043721
[2025-02-13 05:28:54][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_gt_20250213_043721
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_gt_20250213_043721
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_pred_20250213_043721
Combined WER: 0.09777357923438607

Filtering repeated words...

Found 0 repeated lines in total.
Filtered Combined WER: 0.09777357923438607
