/usr/share/Modules/init/sh: line 2: unalias: salloc: not found
/usr/share/Modules/init/sh: line 2: unalias: df: not found
train_eval.sh: line 349: [: ==: unary operator expected
wandb: Currently logged in as: jindaz (jindaz-work). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/log/wandb_log/wandb/run-20241110_021244-z1xye9nu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jindaz-work/SLAM-LLM
wandb: üöÄ View run at https://wandb.ai/jindaz-work/SLAM-LLM/runs/z1xye9nu
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 0/1149 [00:00<?, ?it/s]/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 1/1149 [00:06<2:00:16,  6.29s/it]/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 2/1149 [00:06<56:37,  2.96s/it]  Training Epoch: 1:   0%|[34m          [0m| 3/1149 [00:07<35:55,  1.88s/it]Training Epoch: 1:   0%|[34m          [0m| 4/1149 [00:08<26:11,  1.37s/it]Training Epoch: 1:   0%|[34m          [0m| 5/1149 [00:08<20:53,  1.10s/it]Training Epoch: 1:   1%|[34m          [0m| 6/1149 [00:09<19:03,  1.00s/it]Training Epoch: 1:   1%|[34m          [0m| 7/1149 [00:10<16:13,  1.17it/s]Training Epoch: 1:   1%|[34m          [0m| 8/1149 [00:10<15:44,  1.21it/s]Training Epoch: 1:   1%|[34m          [0m| 9/1149 [00:11<14:25,  1.32it/s]Training Epoch: 1:   1%|[34m          [0m| 10/1149 [00:12<13:37,  1.39it/s]Training Epoch: 1:   1%|[34m          [0m| 11/1149 [00:12<13:02,  1.45it/s]Training Epoch: 1:   1%|[34m          [0m| 12/1149 [00:13<12:32,  1.51it/s]Training Epoch: 1:   1%|[34m          [0m| 13/1149 [00:13<12:23,  1.53it/s]Training Epoch: 1:   1%|[34m          [0m| 14/1149 [00:14<11:57,  1.58it/s]Training Epoch: 1:   1%|[34m‚ñè         [0m| 15/1149 [00:15<11:44,  1.61it/s]Training Epoch: 1:   1%|[34m‚ñè         [0m| 16/1149 [00:15<11:35,  1.63it/s]Training Epoch: 1:   1%|[34m‚ñè         [0m| 17/1149 [00:16<11:19,  1.67it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 18/1149 [00:16<11:09,  1.69it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 19/1149 [00:17<11:09,  1.69it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 20/1149 [00:18<11:04,  1.70it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 21/1149 [00:18<11:07,  1.69it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 22/1149 [00:19<11:12,  1.68it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 23/1149 [00:19<11:18,  1.66it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 24/1149 [00:20<11:43,  1.60it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 25/1149 [00:21<11:37,  1.61it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 26/1149 [00:21<11:58,  1.56it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 27/1149 [00:22<11:44,  1.59it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 28/1149 [00:23<11:29,  1.62it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 29/1149 [00:23<11:25,  1.63it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 30/1149 [00:24<11:17,  1.65it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 31/1149 [00:24<11:11,  1.66it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 32/1149 [00:25<11:29,  1.62it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 33/1149 [00:26<11:25,  1.63it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 34/1149 [00:26<11:25,  1.63it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 35/1149 [00:27<11:14,  1.65it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 36/1149 [00:27<11:09,  1.66it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 37/1149 [00:28<10:57,  1.69it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 38/1149 [00:29<10:58,  1.69it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 39/1149 [00:29<10:54,  1.70it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 40/1149 [00:30<10:52,  1.70it/s]Training Epoch: 1:   4%|[34m‚ñé         [0m| 41/1149 [00:30<11:04,  1.67it/s]Training Epoch: 1:   4%|[34m‚ñé         [0m| 42/1149 [00:31<11:08,  1.66it/s]Training Epoch: 1:   4%|[34m‚ñé         [0m| 43/1149 [00:32<11:06,  1.66it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 44/1149 [00:32<11:12,  1.64it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 45/1149 [00:33<11:00,  1.67it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 46/1149 [00:33<11:00,  1.67it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 47/1149 [00:34<10:38,  1.72it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 48/1149 [00:35<11:05,  1.65it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 49/1149 [00:35<11:07,  1.65it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 50/1149 [00:36<10:56,  1.67it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 51/1149 [00:36<10:53,  1.68it/s]Training Epoch: 1:   5%|[34m‚ñç         [0m| 52/1149 [00:37<11:20,  1.61it/s]Training Epoch: 1:   5%|[34m‚ñç         [0m| 53/1149 [00:38<11:13,  1.63it/s]Training Epoch: 1:   5%|[34m‚ñç         [0m| 54/1149 [00:38<10:51,  1.68it/s]Training Epoch: 1:   5%|[34m‚ñç         [0m| 55/1149 [00:39<10:58,  1.66it/s]Error executing job with overrides: ['++model_config.llm_name=llama32_1b', '++model_config.llm_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', '++model_config.llm_dim=2048', '++model_config.encoder_name=wavlm', '++model_config.normalize=true', '++dataset_config.normalize=true', '++model_config.encoder_projector_ds_rate=5', '++model_config.encoder_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', '++model_config.encoder2_name=w2v2', '++model_config.encoder2_path=vitouphy/wav2vec2-xls-r-300m-timit-phoneme', '++model_config.encoder_dim=1024', '++model_config.encoder_projector=dual', '++dataset_config.dataset=speech_dataset', '++dataset_config.train_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', '++dataset_config.file=src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', '++dataset_config.val_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', '++dataset_config.input_type=raw', '++train_config.model_name=asr', '++train_config.num_epochs=10', '++train_config.freeze_encoder=false', '++train_config.freeze_llm=false', '++train_config.batching_strategy=custom', '++train_config.warmup_steps=1000', '++train_config.total_steps=100000', '++train_config.lr=1e-4', '++train_config.validation_interval=3000', '++train_config.batch_size_training=2', '++train_config.val_batch_size=2', '++train_config.num_workers_dataloader=1', '++train_config.output_dir=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', '++train_config.use_fp16=true', '++train_config.use_peft=true', '++train_config.resume_epoch=1', '++train_config.resume_step=0', '++log_config.use_wandb=true', '++log_config.wandb_exp_name=psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', '++dataset_config.input_type=raw']
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/finetune_asr.py", line 51, in <module>
    main_hydra()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/finetune_asr.py", line 47, in main_hydra
    train(kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/pipeline/finetune.py", line 271, in main
    results = train(
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/utils/train_utils.py", line 115, in train
    outputs, *rest = model(**batch)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/slam_model.py", line 389, in forward
    encoder_outs2, audio_mel_post_mask = self.extract_encoder_features(self.model_config.encoder_name, audio, attention_mask, audio_mel, audio_mel_mask, audio_mask, visual, visual_mask)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/slam_model.py", line 322, in extract_encoder_features
    encoder_outs = self.encoder.extract_features(audio, 1 - audio_mask) #(FIX:MZY): 1-audio_mask is needed for wavlm as the padding mask
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/encoder.py", line 120, in extract_features
    features = self.model.extract_features(source, padding_mask)[0]
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 364, in extract_features
    x, layer_results = self.encoder(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 565, in forward
    x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 598, in extract_features
    x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False,
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 693, in forward
    x, attn, pos_bias = self.self_attn(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/modules.py", line 540, in forward
    x, attn = F.multi_head_attention_forward(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py", line 5504, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 230.00 MiB. GPU 
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.041 MB uploadedwandb: | 0.041 MB of 0.041 MB uploadedwandb: 
wandb: Run history:
wandb:                   train_inner/lr ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: train_inner/train_inner_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñà‚ñà
wandb:     train_inner/train_inner_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                   train_inner/lr 1e-05
wandb: train_inner/train_inner_accuracy 0.27273
wandb:     train_inner/train_inner_loss 4.52118
wandb: 
wandb: üöÄ View run psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder at: https://wandb.ai/jindaz-work/SLAM-LLM/runs/z1xye9nu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jindaz-work/SLAM-LLM
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./log/wandb_log/wandb/run-20241110_021244-z1xye9nu/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
Error executing job with overrides: ['++model_config.llm_name=llama32_1b', '++model_config.llm_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', '++model_config.llm_dim=2048', '++model_config.encoder_name=wavlm', '++model_config.normalize=true', '++dataset_config.normalize=true', '++model_config.encoder_projector_ds_rate=5', '++model_config.encoder_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', '++model_config.encoder_dim=1024', '++model_config.encoder_projector=dual', '++model_config.encoder2_name=w2v2', '++model_config.encoder2_path=vitouphy/wav2vec2-xls-r-300m-timit-phoneme', '++dataset_config.dataset=speech_dataset', '++dataset_config.val_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', '++dataset_config.inference_mode=true', '++dataset_config.file=src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', '++train_config.model_name=asr', '++train_config.freeze_encoder=true', '++train_config.freeze_llm=true', '++train_config.batching_strategy=custom', '++train_config.num_epochs=1', '++train_config.val_batch_size=2', '++train_config.num_workers_dataloader=1', '++train_config.output_dir=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', '++decode_log=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder/decode_test_beam4_20241110_021214', '++ckpt_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder//model.pt', '++log_config.wandb_exp_name=psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', '++train_config.use_peft=true', '++dataset_config.input_type=raw']
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/inference_asr_batch.py", line 53, in <module>
    main_hydra()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/inference_asr_batch.py", line 49, in main_hydra
    inference(cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/pipeline/inference_batch.py", line 106, in main
    model, tokenizer = model_factory(train_config, model_config, **kwargs)
  File "examples/asr_librispeech/model/slam_model_asr.py", line 53, in model_factory
    ckpt_dict = torch.load(ckpt_path, map_location="cpu")
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder//model.pt'
