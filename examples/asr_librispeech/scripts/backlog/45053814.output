/work/van-speech-nlp/jindaznb/slamenv/bin/python
Configuration:
Task: all
Prompt Flag: 
Config File: wavlm-mono
Epochs: 10
Batch Size: 4
Data Folder: psst_phoneme
Use PEFT: true
LLM Name: llama32_1b
Freeze Encoder: false
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: 
speech encoder2 path: 
llm_path: 
use_peft: true
use_fp16: 
Final identifier: psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder
No checkpoint found in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder/
Resume epoch: 1
Resume step: 0
[2024-11-10 01:31:31][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': False, 'freeze_encoder2': False}
[2024-11-10 01:31:31][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-10 01:31:31][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-10 01:31:31][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-11-10_01-31-30.txt', 'log_interval': 5}
[2024-11-10 01:31:52][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-10 01:31:58][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-10 01:31:58][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-10 01:31:58][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-10 01:31:58][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-10 01:32:03][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-10 01:32:03][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-10 01:32:03][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2024-11-10 01:32:03][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-10 01:32:03][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-10 01:32:03][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-10 01:32:03][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-10 01:32:03][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-10 01:32:03][slam_llm.utils.train_utils][INFO] - --> asr has 335.773376 Million params

[2024-11-10 01:32:05][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-10 01:32:07][root][INFO] - --> Training Set Length = 2298
[2024-11-10 01:32:07][root][INFO] - --> Validation Set Length = 341
[2024-11-10 01:32:07][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-10 01:32:07][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-10 01:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:10][numexpr.utils][INFO] - Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-11-10 01:32:11][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 7.947604656219482, acc: 0.0357142873108387)
[2024-11-10 01:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:12][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 8.088130950927734, acc: 0.03703703731298447)
[2024-11-10 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:12][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 7.8554487228393555, acc: 0.0)
[2024-11-10 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:13][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 8.20986557006836, acc: 0.0)
[2024-11-10 01:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:13][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 7.860559940338135, acc: 0.0)
[2024-11-10 01:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:14][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 8.402551651000977, acc: 0.10526315867900848)
[2024-11-10 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:14][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 9.1021146774292, acc: 0.0)
[2024-11-10 01:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:15][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 8.463671684265137, acc: 0.0)
[2024-11-10 01:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:16][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 9.068682670593262, acc: 0.0)
[2024-11-10 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:16][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 7.421794891357422, acc: 0.032258063554763794)
[2024-11-10 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:17][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 8.431982040405273, acc: 0.0)
[2024-11-10 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:17][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 7.285228729248047, acc: 0.0)
[2024-11-10 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:17][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 7.354587554931641, acc: 0.10000000149011612)
[2024-11-10 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:18][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 8.037188529968262, acc: 0.05263157933950424)
[2024-11-10 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:18][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 8.637113571166992, acc: 0.0)
[2024-11-10 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:19][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 8.323856353759766, acc: 0.0)
[2024-11-10 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:19][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 7.88950777053833, acc: 0.0357142873108387)
[2024-11-10 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:20][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 8.288507461547852, acc: 0.0)
[2024-11-10 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:20][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 7.137135028839111, acc: 0.0)
[2024-11-10 01:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:21][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 8.723065376281738, acc: 0.05263157933950424)
[2024-11-10 01:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:21][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 8.058789253234863, acc: 0.10526315867900848)
[2024-11-10 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:22][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 8.304973602294922, acc: 0.0)
[2024-11-10 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:23][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 8.463616371154785, acc: 0.0)
[2024-11-10 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:23][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 8.429634094238281, acc: 0.0)
[2024-11-10 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:24][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 8.054743766784668, acc: 0.06666667014360428)
[2024-11-10 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:24][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 7.407552719116211, acc: 0.0)
[2024-11-10 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:32:25][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 7.394260406494141, acc: 0.0476190485060215)
Selected latest checkpoint by epoch: 
No checkpoint found in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder/
[2024-11-10 01:33:05][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-10 01:33:05][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-10 01:33:05][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': ''}
[2024-11-10 01:33:06][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-10 01:33:12][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-10 01:33:12][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-10 01:33:12][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-10 01:33:12][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-10 01:33:17][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-10 01:33:17][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-10 01:33:17][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2024-11-10 01:33:17][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-10 01:33:17][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-10 01:33:17][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-11-10 01:33:17][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2024-11-10 01:33:17][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder//model.pt
No GT file matching pattern '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder/decode_test_beam4_*_gt' found.
Configuration:
Task: all
Prompt Flag: 
Config File: w2p-wavlm-dual
Epochs: 10
Batch Size: 4
Data Folder: psst_phoneme
Use PEFT: true
LLM Name: llama32_1b
Freeze Encoder: false
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: w2v2
speech encoder2 path: vitouphy/wav2vec2-xls-r-300m-timit-phoneme
llm_path: 
Identifier: 
use_peft: true
use_fp16: 
Final identifier: psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder
No checkpoint found in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder/
Resume epoch: 1
Resume step: 0
[2024-11-10 01:33:28][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': False, 'freeze_encoder2': False}
[2024-11-10 01:33:28][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-10 01:33:28][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme'}
[2024-11-10 01:33:28][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-11-10_01-33-28.txt', 'log_interval': 5}
[2024-11-10 01:33:50][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-10 01:33:56][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-10 01:33:56][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-10 01:33:56][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-10 01:33:56][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-10 01:33:58][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-10 01:33:58][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-10 01:33:58][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-10 01:33:58][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-10 01:34:03][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-10 01:34:03][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-10 01:34:03][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2024-11-10 01:34:03][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-10 01:34:03][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-10 01:34:03][slam_llm.utils.train_utils][INFO] - --> Module dual
[2024-11-10 01:34:03][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2024-11-10 01:34:03][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-10 01:34:03][slam_llm.utils.train_utils][INFO] - --> asr has 661.697856 Million params

[2024-11-10 01:34:06][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-10 01:34:07][root][INFO] - --> Training Set Length = 2298
[2024-11-10 01:34:07][root][INFO] - --> Validation Set Length = 341
[2024-11-10 01:34:07][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-10 01:34:07][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-10 01:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:11][numexpr.utils][INFO] - Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-11-10 01:34:13][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 8.467668533325195, acc: 0.0)
[2024-11-10 01:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:14][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 8.369611740112305, acc: 0.0)
[2024-11-10 01:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:14][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 8.355386734008789, acc: 0.0)
[2024-11-10 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:15][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 7.845961093902588, acc: 0.0)
[2024-11-10 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:16][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 8.044638633728027, acc: 0.0)
[2024-11-10 01:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:16][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 8.264692306518555, acc: 0.0)
[2024-11-10 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:17][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 8.450414657592773, acc: 0.0)
[2024-11-10 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:18][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 8.094650268554688, acc: 0.0)
[2024-11-10 01:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:18][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 8.5169677734375, acc: 0.0)
[2024-11-10 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:19][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 7.898481369018555, acc: 0.0)
[2024-11-10 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:20][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 8.00981616973877, acc: 0.0)
[2024-11-10 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:20][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 7.380526542663574, acc: 0.0)
[2024-11-10 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:21][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 7.6444878578186035, acc: 0.0)
[2024-11-10 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:22][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 7.760603904724121, acc: 0.0)
[2024-11-10 01:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:22][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 8.87483024597168, acc: 0.0)
[2024-11-10 01:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:23][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 7.63871955871582, acc: 0.0)
[2024-11-10 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:24][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 7.593449592590332, acc: 0.0)
[2024-11-10 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:24][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 7.747077465057373, acc: 0.0357142873108387)
[2024-11-10 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:25][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 7.232441425323486, acc: 0.0)
[2024-11-10 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:26][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 8.068017959594727, acc: 0.0)
[2024-11-10 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:26][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 7.842970848083496, acc: 0.0)
[2024-11-10 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:27][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 7.9067535400390625, acc: 0.0)
[2024-11-10 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:28][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 7.4266133308410645, acc: 0.0476190485060215)
[2024-11-10 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:28][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 7.426065921783447, acc: 0.0)
[2024-11-10 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:29][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 7.426294803619385, acc: 0.0)
[2024-11-10 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:30][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 6.893221855163574, acc: 0.0)
[2024-11-10 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-10 01:34:30][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 6.974598407745361, acc: 0.0)
Selected latest checkpoint by epoch: 
No checkpoint found in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder/
[2024-11-10 01:34:58][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-10 01:34:58][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-10 01:34:58][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme'}
[2024-11-10 01:34:59][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-10 01:35:05][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-10 01:35:05][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-10 01:35:05][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-10 01:35:05][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-10 01:35:06][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-10 01:35:06][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-10 01:35:06][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-10 01:35:06][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-10 01:35:13][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-10 01:35:13][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-10 01:35:13][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2024-11-10 01:35:13][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-10 01:35:13][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-10 01:35:14][slam_llm.utils.train_utils][INFO] - --> Module dual
[2024-11-10 01:35:14][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2024-11-10 01:35:14][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder//model.pt
No GT file matching pattern '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder/decode_test_beam4_*_gt' found.
