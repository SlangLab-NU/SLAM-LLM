/usr/share/Modules/init/sh: line 2: unalias: salloc: not found
/usr/share/Modules/init/sh: line 2: unalias: df: not found
train_eval.sh: line 349: [: ==: unary operator expected
wandb: Currently logged in as: jindaz (jindaz-work). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/log/wandb_log/wandb/run-20241110_013133-qggcfjds
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jindaz-work/SLAM-LLM
wandb: üöÄ View run at https://wandb.ai/jindaz-work/SLAM-LLM/runs/qggcfjds
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 0/574 [00:00<?, ?it/s]/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 1/574 [00:04<40:21,  4.23s/it]/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 2/574 [00:04<19:51,  2.08s/it]Training Epoch: 1:   1%|[34m          [0m| 3/574 [00:05<12:52,  1.35s/it]Training Epoch: 1:   1%|[34m          [0m| 4/574 [00:05<09:43,  1.02s/it]Training Epoch: 1:   1%|[34m          [0m| 5/574 [00:06<08:40,  1.09it/s]Training Epoch: 1:   1%|[34m          [0m| 6/574 [00:07<07:13,  1.31it/s]Training Epoch: 1:   1%|[34m          [0m| 7/574 [00:07<06:35,  1.43it/s]Training Epoch: 1:   1%|[34m‚ñè         [0m| 8/574 [00:08<06:17,  1.50it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 9/574 [00:08<05:53,  1.60it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 10/574 [00:09<05:35,  1.68it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 11/574 [00:09<05:17,  1.77it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 12/574 [00:10<05:06,  1.83it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 13/574 [00:10<04:49,  1.94it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 14/574 [00:11<04:40,  2.00it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 15/574 [00:11<04:39,  2.00it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 16/574 [00:12<04:35,  2.02it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 17/574 [00:12<04:31,  2.05it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 18/574 [00:13<04:34,  2.02it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 19/574 [00:13<04:42,  1.96it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 20/574 [00:14<04:40,  1.97it/s]Training Epoch: 1:   4%|[34m‚ñé         [0m| 21/574 [00:14<04:43,  1.95it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 22/574 [00:15<04:41,  1.96it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 23/574 [00:15<04:56,  1.86it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 24/574 [00:16<05:04,  1.80it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 25/574 [00:16<04:59,  1.84it/s]Training Epoch: 1:   5%|[34m‚ñç         [0m| 26/574 [00:17<05:04,  1.80it/s]Training Epoch: 1:   5%|[34m‚ñç         [0m| 27/574 [00:18<04:59,  1.83it/s]Error executing job with overrides: ['++model_config.llm_name=llama32_1b', '++model_config.llm_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', '++model_config.llm_dim=2048', '++model_config.encoder_name=wavlm', '++model_config.normalize=true', '++dataset_config.normalize=true', '++model_config.encoder_projector_ds_rate=5', '++model_config.encoder_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', '++model_config.encoder2_name=', '++model_config.encoder2_path=', '++model_config.encoder_dim=1024', '++model_config.encoder_projector=linear', '++dataset_config.dataset=speech_dataset', '++dataset_config.train_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', '++dataset_config.file=src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', '++dataset_config.val_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', '++dataset_config.input_type=raw', '++train_config.model_name=asr', '++train_config.num_epochs=10', '++train_config.freeze_encoder=false', '++train_config.freeze_llm=false', '++train_config.batching_strategy=custom', '++train_config.warmup_steps=1000', '++train_config.total_steps=100000', '++train_config.lr=1e-4', '++train_config.validation_interval=3000', '++train_config.batch_size_training=4', '++train_config.val_batch_size=4', '++train_config.num_workers_dataloader=1', '++train_config.output_dir=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder', '++train_config.use_fp16=true', '++train_config.use_peft=true', '++train_config.resume_epoch=1', '++train_config.resume_step=0', '++log_config.use_wandb=true', '++log_config.wandb_exp_name=psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder', '++dataset_config.input_type=raw']
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/finetune_asr.py", line 51, in <module>
    main_hydra()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/finetune_asr.py", line 47, in main_hydra
    train(kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/pipeline/finetune.py", line 271, in main
    results = train(
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/utils/train_utils.py", line 115, in train
    outputs, *rest = model(**batch)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/slam_model.py", line 384, in forward
    encoder_outs, audio_mel_post_mask = self.extract_encoder_features(self.model_config.encoder_name, audio, attention_mask, audio_mel, audio_mel_mask, audio_mask, visual, visual_mask)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/slam_model.py", line 322, in extract_encoder_features
    encoder_outs = self.encoder.extract_features(audio, 1 - audio_mask) #(FIX:MZY): 1-audio_mask is needed for wavlm as the padding mask
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/encoder.py", line 120, in extract_features
    features = self.model.extract_features(source, padding_mask)[0]
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 364, in extract_features
    x, layer_results = self.encoder(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 565, in forward
    x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 598, in extract_features
    x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False,
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 693, in forward
    x, attn, pos_bias = self.self_attn(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/modules.py", line 540, in forward
    x, attn = F.multi_head_attention_forward(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py", line 5504, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 456.00 MiB. GPU 
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.006 MB uploadedwandb: | 0.006 MB of 0.035 MB uploadedwandb: 
wandb: Run history:
wandb:                   train_inner/lr ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà
wandb: train_inner/train_inner_accuracy ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ
wandb:     train_inner/train_inner_loss ‚ñÖ‚ñà‚ñà‚ñá‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                   train_inner/lr 0.0
wandb: train_inner/train_inner_accuracy 0.0
wandb:     train_inner/train_inner_loss 7.40755
wandb: 
wandb: üöÄ View run psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder at: https://wandb.ai/jindaz-work/SLAM-LLM/runs/qggcfjds
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jindaz-work/SLAM-LLM
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./log/wandb_log/wandb/run-20241110_013133-qggcfjds/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
Error executing job with overrides: ['++model_config.llm_name=llama32_1b', '++model_config.llm_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', '++model_config.llm_dim=2048', '++model_config.encoder_name=wavlm', '++model_config.normalize=true', '++dataset_config.normalize=true', '++model_config.encoder_projector_ds_rate=5', '++model_config.encoder_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', '++model_config.encoder_dim=1024', '++model_config.encoder_projector=linear', '++model_config.encoder2_name=', '++model_config.encoder2_path=', '++dataset_config.dataset=speech_dataset', '++dataset_config.val_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', '++dataset_config.inference_mode=true', '++dataset_config.file=src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', '++train_config.model_name=asr', '++train_config.freeze_encoder=true', '++train_config.freeze_llm=true', '++train_config.batching_strategy=custom', '++train_config.num_epochs=1', '++train_config.val_batch_size=4', '++train_config.num_workers_dataloader=1', '++train_config.output_dir=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder', '++decode_log=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder/decode_test_beam4_20241110_013105', '++ckpt_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder//model.pt', '++log_config.wandb_exp_name=psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder', '++train_config.use_peft=true', '++dataset_config.input_type=raw']
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/inference_asr_batch.py", line 53, in <module>
    main_hydra()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/inference_asr_batch.py", line 49, in main_hydra
    inference(cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/pipeline/inference_batch.py", line 106, in main
    model, tokenizer = model_factory(train_config, model_config, **kwargs)
  File "examples/asr_librispeech/model/slam_model_asr.py", line 53, in model_factory
    ckpt_dict = torch.load(ckpt_path, map_location="cpu")
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_linear_peft_unfreeze_encoder//model.pt'
train_eval.sh: line 349: [: ==: unary operator expected
wandb: Currently logged in as: jindaz (jindaz-work). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/log/wandb_log/wandb/run-20241110_013330-87ntcziz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jindaz-work/SLAM-LLM
wandb: üöÄ View run at https://wandb.ai/jindaz-work/SLAM-LLM/runs/87ntcziz
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 0/574 [00:00<?, ?it/s]/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 1/574 [00:05<53:12,  5.57s/it]/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 2/574 [00:06<25:23,  2.66s/it]Training Epoch: 1:   1%|[34m          [0m| 3/574 [00:06<16:25,  1.73s/it]Training Epoch: 1:   1%|[34m          [0m| 4/574 [00:07<12:39,  1.33s/it]Training Epoch: 1:   1%|[34m          [0m| 5/574 [00:08<10:46,  1.14s/it]Training Epoch: 1:   1%|[34m          [0m| 6/574 [00:08<08:47,  1.08it/s]Training Epoch: 1:   1%|[34m          [0m| 7/574 [00:09<08:27,  1.12it/s]Training Epoch: 1:   1%|[34m‚ñè         [0m| 8/574 [00:10<07:48,  1.21it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 9/574 [00:10<07:09,  1.32it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 10/574 [00:11<06:45,  1.39it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 11/574 [00:12<06:19,  1.49it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 12/574 [00:12<06:15,  1.50it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 13/574 [00:13<06:05,  1.54it/s]Training Epoch: 1:   2%|[34m‚ñè         [0m| 14/574 [00:14<06:14,  1.50it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 15/574 [00:14<06:09,  1.51it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 16/574 [00:15<05:57,  1.56it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 17/574 [00:16<06:07,  1.52it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 18/574 [00:16<06:17,  1.47it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 19/574 [00:17<06:11,  1.49it/s]Training Epoch: 1:   3%|[34m‚ñé         [0m| 20/574 [00:18<06:00,  1.54it/s]Training Epoch: 1:   4%|[34m‚ñé         [0m| 21/574 [00:18<06:03,  1.52it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 22/574 [00:19<05:58,  1.54it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 23/574 [00:20<06:01,  1.52it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 24/574 [00:20<06:04,  1.51it/s]Training Epoch: 1:   4%|[34m‚ñç         [0m| 25/574 [00:21<05:57,  1.54it/s]Training Epoch: 1:   5%|[34m‚ñç         [0m| 26/574 [00:22<06:10,  1.48it/s]Training Epoch: 1:   5%|[34m‚ñç         [0m| 27/574 [00:22<06:00,  1.52it/s]Error executing job with overrides: ['++model_config.llm_name=llama32_1b', '++model_config.llm_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', '++model_config.llm_dim=2048', '++model_config.encoder_name=wavlm', '++model_config.normalize=true', '++dataset_config.normalize=true', '++model_config.encoder_projector_ds_rate=5', '++model_config.encoder_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', '++model_config.encoder2_name=w2v2', '++model_config.encoder2_path=vitouphy/wav2vec2-xls-r-300m-timit-phoneme', '++model_config.encoder_dim=1024', '++model_config.encoder_projector=dual', '++dataset_config.dataset=speech_dataset', '++dataset_config.train_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', '++dataset_config.file=src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', '++dataset_config.val_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', '++dataset_config.input_type=raw', '++train_config.model_name=asr', '++train_config.num_epochs=10', '++train_config.freeze_encoder=false', '++train_config.freeze_llm=false', '++train_config.batching_strategy=custom', '++train_config.warmup_steps=1000', '++train_config.total_steps=100000', '++train_config.lr=1e-4', '++train_config.validation_interval=3000', '++train_config.batch_size_training=4', '++train_config.val_batch_size=4', '++train_config.num_workers_dataloader=1', '++train_config.output_dir=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', '++train_config.use_fp16=true', '++train_config.use_peft=true', '++train_config.resume_epoch=1', '++train_config.resume_step=0', '++log_config.use_wandb=true', '++log_config.wandb_exp_name=psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', '++dataset_config.input_type=raw']
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/finetune_asr.py", line 51, in <module>
    main_hydra()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/finetune_asr.py", line 47, in main_hydra
    train(kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/pipeline/finetune.py", line 271, in main
    results = train(
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/utils/train_utils.py", line 115, in train
    outputs, *rest = model(**batch)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/slam_model.py", line 384, in forward
    encoder_outs, audio_mel_post_mask = self.extract_encoder_features(self.model_config.encoder_name, audio, attention_mask, audio_mel, audio_mel_mask, audio_mask, visual, visual_mask)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/slam_model.py", line 322, in extract_encoder_features
    encoder_outs = self.encoder.extract_features(audio, 1 - audio_mask) #(FIX:MZY): 1-audio_mask is needed for wavlm as the padding mask
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/encoder.py", line 120, in extract_features
    features = self.model.extract_features(source, padding_mask)[0]
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 364, in extract_features
    x, layer_results = self.encoder(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 565, in forward
    x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 598, in extract_features
    x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False,
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/WavLM.py", line 693, in forward
    x, attn, pos_bias = self.self_attn(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/models/wavlm/modules.py", line 540, in forward
    x, attn = F.multi_head_attention_forward(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py", line 5504, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 456.00 MiB. GPU 
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.035 MB uploadedwandb: | 0.036 MB of 0.036 MB uploadedwandb: 
wandb: Run history:
wandb:                   train_inner/lr ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà
wandb: train_inner/train_inner_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:     train_inner/train_inner_loss ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                   train_inner/lr 0.0
wandb: train_inner/train_inner_accuracy 0.0
wandb:     train_inner/train_inner_loss 6.89322
wandb: 
wandb: üöÄ View run psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder at: https://wandb.ai/jindaz-work/SLAM-LLM/runs/87ntcziz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jindaz-work/SLAM-LLM
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./log/wandb_log/wandb/run-20241110_013330-87ntcziz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
Error executing job with overrides: ['++model_config.llm_name=llama32_1b', '++model_config.llm_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', '++model_config.llm_dim=2048', '++model_config.encoder_name=wavlm', '++model_config.normalize=true', '++dataset_config.normalize=true', '++model_config.encoder_projector_ds_rate=5', '++model_config.encoder_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', '++model_config.encoder_dim=1024', '++model_config.encoder_projector=dual', '++model_config.encoder2_name=w2v2', '++model_config.encoder2_path=vitouphy/wav2vec2-xls-r-300m-timit-phoneme', '++dataset_config.dataset=speech_dataset', '++dataset_config.val_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', '++dataset_config.inference_mode=true', '++dataset_config.file=src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', '++train_config.model_name=asr', '++train_config.freeze_encoder=true', '++train_config.freeze_llm=true', '++train_config.batching_strategy=custom', '++train_config.num_epochs=1', '++train_config.val_batch_size=4', '++train_config.num_workers_dataloader=1', '++train_config.output_dir=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', '++decode_log=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder/decode_test_beam4_20241110_013319', '++ckpt_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder//model.pt', '++log_config.wandb_exp_name=psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder', '++train_config.use_peft=true', '++dataset_config.input_type=raw']
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/inference_asr_batch.py", line 53, in <module>
    main_hydra()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/inference_asr_batch.py", line 49, in main_hydra
    inference(cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/pipeline/inference_batch.py", line 106, in main
    model, tokenizer = model_factory(train_config, model_config, **kwargs)
  File "examples/asr_librispeech/model/slam_model_asr.py", line 53, in model_factory
    ckpt_dict = torch.load(ckpt_path, map_location="cpu")
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft_unfreeze_encoder//model.pt'
