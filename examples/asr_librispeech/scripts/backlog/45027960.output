/work/van-speech-nlp/jindaznb/slamenv/bin/python
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: w2v2
speech encoder2 path: vitouphy/wav2vec2-xls-r-300m-timit-phoneme
llm_path: 
Identifier: 
use_peft: true
use_fp16: 
Final identifier: psst_phoneme_wavlm_llama32_1b_dual_peft
No checkpoint found in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/
Resume epoch: 1
Resume step: 0
[2024-11-08 03:24:03][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-08 03:24:03][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-08 03:24:03][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme'}
[2024-11-08 03:24:03][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_dual_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2024-11-08_03-24-02.txt', 'log_interval': 5}
[2024-11-08 03:24:28][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-08 03:24:34][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 03:24:34][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-08 03:24:34][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 03:24:34][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-08 03:24:36][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-08 03:24:36][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-08 03:24:36][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-08 03:24:36][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-08 03:24:45][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 03:24:45][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-08 03:24:45][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2024-11-08 03:24:45][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 03:24:45][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-08 03:24:45][slam_llm.utils.train_utils][INFO] - --> Module dual
[2024-11-08 03:24:45][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2024-11-08 03:24:45][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-08 03:24:45][slam_llm.utils.train_utils][INFO] - --> asr has 346.244736 Million params

[2024-11-08 03:24:48][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-08 03:24:50][root][INFO] - --> Training Set Length = 2298
[2024-11-08 03:24:50][root][INFO] - --> Validation Set Length = 341
[2024-11-08 03:24:50][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-08 03:24:50][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2024-11-08 03:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:24:55][numexpr.utils][INFO] - Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-11-08 03:24:57][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 8.467668533325195, acc: 0.0)
[2024-11-08 03:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:24:58][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 8.369611740112305, acc: 0.0)
[2024-11-08 03:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:24:58][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 8.355386734008789, acc: 0.0)
[2024-11-08 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:24:59][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 7.845961093902588, acc: 0.0)
[2024-11-08 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:24:59][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 8.028204917907715, acc: 0.0)
[2024-11-08 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:00][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 8.247143745422363, acc: 0.0)
[2024-11-08 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:00][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 8.457962989807129, acc: 0.0)
[2024-11-08 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:01][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 8.11304759979248, acc: 0.0)
[2024-11-08 03:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:01][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 8.485758781433105, acc: 0.0)
[2024-11-08 03:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:02][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 7.879504680633545, acc: 0.0)
[2024-11-08 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:02][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 8.033276557922363, acc: 0.0)
[2024-11-08 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:02][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 7.274286270141602, acc: 0.0)
[2024-11-08 03:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:03][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 7.593996524810791, acc: 0.0)
[2024-11-08 03:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:03][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 7.726447582244873, acc: 0.0)
[2024-11-08 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:04][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 8.826881408691406, acc: 0.0)
[2024-11-08 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:04][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 7.613288879394531, acc: 0.0)
[2024-11-08 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:05][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 7.552695274353027, acc: 0.0)
[2024-11-08 03:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:05][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 7.732676982879639, acc: 0.0357142873108387)
[2024-11-08 03:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:05][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 7.196223258972168, acc: 0.0)
[2024-11-08 03:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:06][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 8.026951789855957, acc: 0.0)
[2024-11-08 03:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:06][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 7.761315822601318, acc: 0.0)
[2024-11-08 03:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:07][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 7.846607685089111, acc: 0.0)
[2024-11-08 03:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:07][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 7.348238945007324, acc: 0.0476190485060215)
[2024-11-08 03:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:08][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 7.3881516456604, acc: 0.0)
[2024-11-08 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:08][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 7.298031330108643, acc: 0.0)
[2024-11-08 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:09][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 6.935004234313965, acc: 0.0)
[2024-11-08 03:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:09][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 7.015864849090576, acc: 0.0)
[2024-11-08 03:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:11][root][INFO] - Training Epoch: 1/10, step 27/574 completed (loss: 7.646083831787109, acc: 0.0)
[2024-11-08 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:11][root][INFO] - Training Epoch: 1/10, step 28/574 completed (loss: 7.299009323120117, acc: 0.0)
[2024-11-08 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:12][root][INFO] - Training Epoch: 1/10, step 29/574 completed (loss: 6.747689247131348, acc: 0.0)
[2024-11-08 03:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:12][root][INFO] - Training Epoch: 1/10, step 30/574 completed (loss: 7.21384859085083, acc: 0.0)
[2024-11-08 03:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:12][root][INFO] - Training Epoch: 1/10, step 31/574 completed (loss: 6.915914535522461, acc: 0.0)
[2024-11-08 03:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:13][root][INFO] - Training Epoch: 1/10, step 32/574 completed (loss: 6.285428047180176, acc: 0.03030303120613098)
[2024-11-08 03:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:13][root][INFO] - Training Epoch: 1/10, step 33/574 completed (loss: 7.060206413269043, acc: 0.0)
[2024-11-08 03:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:14][root][INFO] - Training Epoch: 1/10, step 34/574 completed (loss: 5.756013870239258, acc: 0.0476190485060215)
[2024-11-08 03:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:14][root][INFO] - Training Epoch: 1/10, step 35/574 completed (loss: 5.703616619110107, acc: 0.05263157933950424)
[2024-11-08 03:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:15][root][INFO] - Training Epoch: 1/10, step 36/574 completed (loss: 5.769650459289551, acc: 0.05000000074505806)
[2024-11-08 03:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:15][root][INFO] - Training Epoch: 1/10, step 37/574 completed (loss: 6.701905250549316, acc: 0.1428571492433548)
[2024-11-08 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:16][root][INFO] - Training Epoch: 1/10, step 38/574 completed (loss: 5.650413990020752, acc: 0.09090909361839294)
[2024-11-08 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:16][root][INFO] - Training Epoch: 1/10, step 39/574 completed (loss: 6.185150146484375, acc: 0.0357142873108387)
[2024-11-08 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:17][root][INFO] - Training Epoch: 1/10, step 40/574 completed (loss: 6.288881778717041, acc: 0.08571428805589676)
[2024-11-08 03:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:17][root][INFO] - Training Epoch: 1/10, step 41/574 completed (loss: 5.0861053466796875, acc: 0.20000000298023224)
[2024-11-08 03:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:18][root][INFO] - Training Epoch: 1/10, step 42/574 completed (loss: 5.104495525360107, acc: 0.20000000298023224)
[2024-11-08 03:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:18][root][INFO] - Training Epoch: 1/10, step 43/574 completed (loss: 5.4167022705078125, acc: 0.10526315867900848)
[2024-11-08 03:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:19][root][INFO] - Training Epoch: 1/10, step 44/574 completed (loss: 6.2361602783203125, acc: 0.13636364042758942)
[2024-11-08 03:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:20][root][INFO] - Training Epoch: 1/10, step 45/574 completed (loss: 5.4682416915893555, acc: 0.1428571492433548)
[2024-11-08 03:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:20][root][INFO] - Training Epoch: 1/10, step 46/574 completed (loss: 5.968921184539795, acc: 0.0833333358168602)
[2024-11-08 03:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:20][root][INFO] - Training Epoch: 1/10, step 47/574 completed (loss: 5.404698371887207, acc: 0.09677419066429138)
[2024-11-08 03:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:21][root][INFO] - Training Epoch: 1/10, step 48/574 completed (loss: 5.980883598327637, acc: 0.05714285746216774)
[2024-11-08 03:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:21][root][INFO] - Training Epoch: 1/10, step 49/574 completed (loss: 4.376415252685547, acc: 0.20000000298023224)
[2024-11-08 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:22][root][INFO] - Training Epoch: 1/10, step 50/574 completed (loss: 4.0639472007751465, acc: 0.3333333432674408)
[2024-11-08 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:22][root][INFO] - Training Epoch: 1/10, step 51/574 completed (loss: 4.766911029815674, acc: 0.31578946113586426)
[2024-11-08 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:23][root][INFO] - Training Epoch: 1/10, step 52/574 completed (loss: 4.644744396209717, acc: 0.30000001192092896)
[2024-11-08 03:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:23][root][INFO] - Training Epoch: 1/10, step 53/574 completed (loss: 5.314675807952881, acc: 0.2857142984867096)
[2024-11-08 03:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:24][root][INFO] - Training Epoch: 1/10, step 54/574 completed (loss: 4.460195541381836, acc: 0.1818181872367859)
[2024-11-08 03:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:24][root][INFO] - Training Epoch: 1/10, step 55/574 completed (loss: 4.858519077301025, acc: 0.1071428582072258)
[2024-11-08 03:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:29][root][INFO] - Training Epoch: 1/10, step 56/574 completed (loss: 4.640250205993652, acc: 0.2857142984867096)
[2024-11-08 03:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:30][root][INFO] - Training Epoch: 1/10, step 57/574 completed (loss: 4.3664937019348145, acc: 0.1428571492433548)
[2024-11-08 03:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:31][root][INFO] - Training Epoch: 1/10, step 58/574 completed (loss: 4.347637176513672, acc: 0.2631579041481018)
[2024-11-08 03:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:32][root][INFO] - Training Epoch: 1/10, step 59/574 completed (loss: 5.031923770904541, acc: 0.1818181872367859)
[2024-11-08 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:33][root][INFO] - Training Epoch: 1/10, step 60/574 completed (loss: 3.9475386142730713, acc: 0.42105263471603394)
[2024-11-08 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:34][root][INFO] - Training Epoch: 1/10, step 61/574 completed (loss: 4.444046497344971, acc: 0.2857142984867096)
[2024-11-08 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:34][root][INFO] - Training Epoch: 1/10, step 62/574 completed (loss: 4.71686315536499, acc: 0.1388888955116272)
[2024-11-08 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:34][root][INFO] - Training Epoch: 1/10, step 63/574 completed (loss: 4.470386028289795, acc: 0.1785714328289032)
[2024-11-08 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:35][root][INFO] - Training Epoch: 1/10, step 64/574 completed (loss: 3.509941816329956, acc: 0.190476194024086)
[2024-11-08 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:35][root][INFO] - Training Epoch: 1/10, step 65/574 completed (loss: 3.5991051197052, acc: 0.30000001192092896)
[2024-11-08 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:36][root][INFO] - Training Epoch: 1/10, step 66/574 completed (loss: 4.038241863250732, acc: 0.31578946113586426)
[2024-11-08 03:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:36][root][INFO] - Training Epoch: 1/10, step 67/574 completed (loss: 3.7792294025421143, acc: 0.25)
[2024-11-08 03:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:37][root][INFO] - Training Epoch: 1/10, step 68/574 completed (loss: 3.6978025436401367, acc: 0.3181818127632141)
[2024-11-08 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:37][root][INFO] - Training Epoch: 1/10, step 69/574 completed (loss: 4.715259075164795, acc: 0.1515151560306549)
[2024-11-08 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:37][root][INFO] - Training Epoch: 1/10, step 70/574 completed (loss: 4.154366493225098, acc: 0.2222222238779068)
[2024-11-08 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:38][root][INFO] - Training Epoch: 1/10, step 71/574 completed (loss: 4.27140474319458, acc: 0.23333333432674408)
[2024-11-08 03:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:38][root][INFO] - Training Epoch: 1/10, step 72/574 completed (loss: 3.169534206390381, acc: 0.380952388048172)
[2024-11-08 03:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:39][root][INFO] - Training Epoch: 1/10, step 73/574 completed (loss: 3.2177658081054688, acc: 0.3684210479259491)
[2024-11-08 03:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:39][root][INFO] - Training Epoch: 1/10, step 74/574 completed (loss: 3.978231906890869, acc: 0.4000000059604645)
[2024-11-08 03:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:39][root][INFO] - Training Epoch: 1/10, step 75/574 completed (loss: 3.9887521266937256, acc: 0.3333333432674408)
[2024-11-08 03:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:40][root][INFO] - Training Epoch: 1/10, step 76/574 completed (loss: 3.2954530715942383, acc: 0.3181818127632141)
[2024-11-08 03:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:40][root][INFO] - Training Epoch: 1/10, step 77/574 completed (loss: 3.9340624809265137, acc: 0.3571428656578064)
[2024-11-08 03:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:41][root][INFO] - Training Epoch: 1/10, step 78/574 completed (loss: 4.576906204223633, acc: 0.25925925374031067)
[2024-11-08 03:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:41][root][INFO] - Training Epoch: 1/10, step 79/574 completed (loss: 4.319232940673828, acc: 0.20000000298023224)
[2024-11-08 03:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:42][root][INFO] - Training Epoch: 1/10, step 80/574 completed (loss: 3.6888723373413086, acc: 0.3461538553237915)
[2024-11-08 03:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:42][root][INFO] - Training Epoch: 1/10, step 81/574 completed (loss: 2.6802234649658203, acc: 0.4285714328289032)
[2024-11-08 03:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:43][root][INFO] - Training Epoch: 1/10, step 82/574 completed (loss: 2.8998258113861084, acc: 0.31578946113586426)
[2024-11-08 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:43][root][INFO] - Training Epoch: 1/10, step 83/574 completed (loss: 3.2868714332580566, acc: 0.3684210479259491)
[2024-11-08 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:43][root][INFO] - Training Epoch: 1/10, step 84/574 completed (loss: 3.23276686668396, acc: 0.3181818127632141)
[2024-11-08 03:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:44][root][INFO] - Training Epoch: 1/10, step 85/574 completed (loss: 3.162916898727417, acc: 0.4285714328289032)
[2024-11-08 03:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:44][root][INFO] - Training Epoch: 1/10, step 86/574 completed (loss: 3.872581720352173, acc: 0.3333333432674408)
[2024-11-08 03:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:45][root][INFO] - Training Epoch: 1/10, step 87/574 completed (loss: 4.16868257522583, acc: 0.3076923191547394)
[2024-11-08 03:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:45][root][INFO] - Training Epoch: 1/10, step 88/574 completed (loss: 2.920314311981201, acc: 0.2857142984867096)
[2024-11-08 03:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:47][root][INFO] - Training Epoch: 1/10, step 89/574 completed (loss: 3.195675849914551, acc: 0.3684210479259491)
[2024-11-08 03:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:48][root][INFO] - Training Epoch: 1/10, step 90/574 completed (loss: 3.4213333129882812, acc: 0.31578946113586426)
[2024-11-08 03:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:49][root][INFO] - Training Epoch: 1/10, step 91/574 completed (loss: 3.3519318103790283, acc: 0.3181818127632141)
[2024-11-08 03:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:51][root][INFO] - Training Epoch: 1/10, step 92/574 completed (loss: 2.5401360988616943, acc: 0.380952388048172)
[2024-11-08 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:52][root][INFO] - Training Epoch: 1/10, step 93/574 completed (loss: 3.50278902053833, acc: 0.38461539149284363)
[2024-11-08 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:53][root][INFO] - Training Epoch: 1/10, step 94/574 completed (loss: 3.4148712158203125, acc: 0.2857142984867096)
[2024-11-08 03:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:53][root][INFO] - Training Epoch: 1/10, step 95/574 completed (loss: 2.4580490589141846, acc: 0.3333333432674408)
[2024-11-08 03:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:53][root][INFO] - Training Epoch: 1/10, step 96/574 completed (loss: 2.4679431915283203, acc: 0.5789473652839661)
[2024-11-08 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:54][root][INFO] - Training Epoch: 1/10, step 97/574 completed (loss: 2.834887742996216, acc: 0.4000000059604645)
[2024-11-08 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:54][root][INFO] - Training Epoch: 1/10, step 98/574 completed (loss: 3.365294933319092, acc: 0.380952388048172)
[2024-11-08 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:55][root][INFO] - Training Epoch: 1/10, step 99/574 completed (loss: 2.41554856300354, acc: 0.40909090638160706)
[2024-11-08 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:55][root][INFO] - Training Epoch: 1/10, step 100/574 completed (loss: 3.8266608715057373, acc: 0.3333333432674408)
[2024-11-08 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:55][root][INFO] - Training Epoch: 1/10, step 101/574 completed (loss: 3.025329828262329, acc: 0.3199999928474426)
[2024-11-08 03:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:56][root][INFO] - Training Epoch: 1/10, step 102/574 completed (loss: 2.7678210735321045, acc: 0.4285714328289032)
[2024-11-08 03:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:56][root][INFO] - Training Epoch: 1/10, step 103/574 completed (loss: 2.494002103805542, acc: 0.42105263471603394)
[2024-11-08 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:57][root][INFO] - Training Epoch: 1/10, step 104/574 completed (loss: 2.8260674476623535, acc: 0.3636363744735718)
[2024-11-08 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:57][root][INFO] - Training Epoch: 1/10, step 105/574 completed (loss: 2.3371591567993164, acc: 0.5789473652839661)
[2024-11-08 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:58][root][INFO] - Training Epoch: 1/10, step 106/574 completed (loss: 2.288971185684204, acc: 0.4545454680919647)
[2024-11-08 03:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:58][root][INFO] - Training Epoch: 1/10, step 107/574 completed (loss: 3.910066604614258, acc: 0.2857142984867096)
[2024-11-08 03:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:58][root][INFO] - Training Epoch: 1/10, step 108/574 completed (loss: 3.7387802600860596, acc: 0.30000001192092896)
[2024-11-08 03:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:59][root][INFO] - Training Epoch: 1/10, step 109/574 completed (loss: 2.0214576721191406, acc: 0.523809552192688)
[2024-11-08 03:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:25:59][root][INFO] - Training Epoch: 1/10, step 110/574 completed (loss: 2.089592218399048, acc: 0.4285714328289032)
[2024-11-08 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:00][root][INFO] - Training Epoch: 1/10, step 111/574 completed (loss: 2.3888111114501953, acc: 0.42105263471603394)
[2024-11-08 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:00][root][INFO] - Training Epoch: 1/10, step 112/574 completed (loss: 2.9737021923065186, acc: 0.3181818127632141)
[2024-11-08 03:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:01][root][INFO] - Training Epoch: 1/10, step 113/574 completed (loss: 2.300893783569336, acc: 0.5789473652839661)
[2024-11-08 03:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:01][root][INFO] - Training Epoch: 1/10, step 114/574 completed (loss: 2.3291876316070557, acc: 0.40909090638160706)
[2024-11-08 03:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:02][root][INFO] - Training Epoch: 1/10, step 115/574 completed (loss: 3.1435699462890625, acc: 0.3076923191547394)
[2024-11-08 03:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:02][root][INFO] - Training Epoch: 1/10, step 116/574 completed (loss: 2.540740966796875, acc: 0.4399999976158142)
[2024-11-08 03:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:03][root][INFO] - Training Epoch: 1/10, step 117/574 completed (loss: 2.2021820545196533, acc: 0.3499999940395355)
[2024-11-08 03:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:03][root][INFO] - Training Epoch: 1/10, step 118/574 completed (loss: 2.147465229034424, acc: 0.5263158082962036)
[2024-11-08 03:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:04][root][INFO] - Training Epoch: 1/10, step 119/574 completed (loss: 2.445035696029663, acc: 0.4545454680919647)
[2024-11-08 03:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:05][root][INFO] - Training Epoch: 1/10, step 120/574 completed (loss: 1.9930734634399414, acc: 0.5789473652839661)
[2024-11-08 03:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:05][root][INFO] - Training Epoch: 1/10, step 121/574 completed (loss: 2.2215328216552734, acc: 0.40909090638160706)
[2024-11-08 03:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:06][root][INFO] - Training Epoch: 1/10, step 122/574 completed (loss: 3.6041457653045654, acc: 0.2857142984867096)
[2024-11-08 03:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:06][root][INFO] - Training Epoch: 1/10, step 123/574 completed (loss: 3.2444472312927246, acc: 0.38461539149284363)
[2024-11-08 03:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:06][root][INFO] - Training Epoch: 1/10, step 124/574 completed (loss: 2.148881673812866, acc: 0.44999998807907104)
[2024-11-08 03:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:07][root][INFO] - Training Epoch: 1/10, step 125/574 completed (loss: 1.9734615087509155, acc: 0.6000000238418579)
[2024-11-08 03:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:07][root][INFO] - Training Epoch: 1/10, step 126/574 completed (loss: 1.980627179145813, acc: 0.5789473652839661)
[2024-11-08 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:08][root][INFO] - Training Epoch: 1/10, step 127/574 completed (loss: 2.0773065090179443, acc: 0.5)
[2024-11-08 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:08][root][INFO] - Training Epoch: 1/10, step 128/574 completed (loss: 1.9627342224121094, acc: 0.5)
[2024-11-08 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:09][root][INFO] - Training Epoch: 1/10, step 129/574 completed (loss: 1.9994546175003052, acc: 0.4545454680919647)
[2024-11-08 03:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:09][root][INFO] - Training Epoch: 1/10, step 130/574 completed (loss: 3.735213041305542, acc: 0.27272728085517883)
[2024-11-08 03:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:10][root][INFO] - Training Epoch: 1/10, step 131/574 completed (loss: 2.521545886993408, acc: 0.37037035822868347)
[2024-11-08 03:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:10][root][INFO] - Training Epoch: 1/10, step 132/574 completed (loss: 3.424699068069458, acc: 0.27272728085517883)
[2024-11-08 03:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:11][root][INFO] - Training Epoch: 1/10, step 133/574 completed (loss: 1.6848335266113281, acc: 0.6000000238418579)
[2024-11-08 03:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:11][root][INFO] - Training Epoch: 1/10, step 134/574 completed (loss: 1.8911851644515991, acc: 0.6000000238418579)
[2024-11-08 03:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:11][root][INFO] - Training Epoch: 1/10, step 135/574 completed (loss: 2.0215845108032227, acc: 0.42105263471603394)
[2024-11-08 03:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:12][root][INFO] - Training Epoch: 1/10, step 136/574 completed (loss: 1.7433929443359375, acc: 0.4545454680919647)
[2024-11-08 03:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:12][root][INFO] - Training Epoch: 1/10, step 137/574 completed (loss: 1.94394052028656, acc: 0.44999998807907104)
[2024-11-08 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:13][root][INFO] - Training Epoch: 1/10, step 138/574 completed (loss: 1.6963512897491455, acc: 0.5454545617103577)
[2024-11-08 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:13][root][INFO] - Training Epoch: 1/10, step 139/574 completed (loss: 3.047527551651001, acc: 0.27272728085517883)
[2024-11-08 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:14][root][INFO] - Training Epoch: 1/10, step 140/574 completed (loss: 2.343712568283081, acc: 0.40740740299224854)
[2024-11-08 03:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:14][root][INFO] - Training Epoch: 1/10, step 141/574 completed (loss: 3.4538536071777344, acc: 0.27272728085517883)
[2024-11-08 03:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:15][root][INFO] - Training Epoch: 1/10, step 142/574 completed (loss: 1.8849029541015625, acc: 0.5)
[2024-11-08 03:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:26:59][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(8.0113, device='cuda:0') eval_epoch_loss=tensor(2.0809, device='cuda:0') eval_epoch_acc=tensor(0.4974, device='cuda:0')
[2024-11-08 03:26:59][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:26:59][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:27:03][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_1_step_143_loss_2.0808520317077637/model.pt
[2024-11-08 03:27:03][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:27:03][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 2.0808520317077637
[2024-11-08 03:27:03][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.497407466173172
[2024-11-08 03:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:04][root][INFO] - Training Epoch: 1/10, step 143/574 completed (loss: 1.8128093481063843, acc: 0.5)
[2024-11-08 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:04][root][INFO] - Training Epoch: 1/10, step 144/574 completed (loss: 1.3869547843933105, acc: 0.6842105388641357)
[2024-11-08 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:05][root][INFO] - Training Epoch: 1/10, step 145/574 completed (loss: 1.584696650505066, acc: 0.5)
[2024-11-08 03:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:05][root][INFO] - Training Epoch: 1/10, step 146/574 completed (loss: 1.6034367084503174, acc: 0.6000000238418579)
[2024-11-08 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:06][root][INFO] - Training Epoch: 1/10, step 147/574 completed (loss: 1.5763816833496094, acc: 0.5909090638160706)
[2024-11-08 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:06][root][INFO] - Training Epoch: 1/10, step 148/574 completed (loss: 3.0351722240448, acc: 0.3636363744735718)
[2024-11-08 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:07][root][INFO] - Training Epoch: 1/10, step 149/574 completed (loss: 2.0215184688568115, acc: 0.5185185074806213)
[2024-11-08 03:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:07][root][INFO] - Training Epoch: 1/10, step 150/574 completed (loss: 3.0817742347717285, acc: 0.35483869910240173)
[2024-11-08 03:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:08][root][INFO] - Training Epoch: 1/10, step 151/574 completed (loss: 1.3736647367477417, acc: 0.5)
[2024-11-08 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:08][root][INFO] - Training Epoch: 1/10, step 152/574 completed (loss: 1.5217458009719849, acc: 0.550000011920929)
[2024-11-08 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:09][root][INFO] - Training Epoch: 1/10, step 153/574 completed (loss: 1.4916088581085205, acc: 0.6842105388641357)
[2024-11-08 03:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:09][root][INFO] - Training Epoch: 1/10, step 154/574 completed (loss: 1.5290839672088623, acc: 0.5)
[2024-11-08 03:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:09][root][INFO] - Training Epoch: 1/10, step 155/574 completed (loss: 1.6401888132095337, acc: 0.6000000238418579)
[2024-11-08 03:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:10][root][INFO] - Training Epoch: 1/10, step 156/574 completed (loss: 1.6258232593536377, acc: 0.5909090638160706)
[2024-11-08 03:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:10][root][INFO] - Training Epoch: 1/10, step 157/574 completed (loss: 3.1352226734161377, acc: 0.27272728085517883)
[2024-11-08 03:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:13][root][INFO] - Training Epoch: 1/10, step 158/574 completed (loss: 2.170255184173584, acc: 0.5)
[2024-11-08 03:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:14][root][INFO] - Training Epoch: 1/10, step 159/574 completed (loss: 1.6001479625701904, acc: 0.6000000238418579)
[2024-11-08 03:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:14][root][INFO] - Training Epoch: 1/10, step 160/574 completed (loss: 1.5582294464111328, acc: 0.5263158082962036)
[2024-11-08 03:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:15][root][INFO] - Training Epoch: 1/10, step 161/574 completed (loss: 2.924257755279541, acc: 0.3636363744735718)
[2024-11-08 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:16][root][INFO] - Training Epoch: 1/10, step 162/574 completed (loss: 2.556983709335327, acc: 0.4000000059604645)
[2024-11-08 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:17][root][INFO] - Training Epoch: 1/10, step 163/574 completed (loss: 2.082897186279297, acc: 0.5199999809265137)
[2024-11-08 03:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:17][root][INFO] - Training Epoch: 1/10, step 164/574 completed (loss: 2.3805646896362305, acc: 0.5185185074806213)
[2024-11-08 03:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:18][root][INFO] - Training Epoch: 1/10, step 165/574 completed (loss: 2.7722959518432617, acc: 0.2777777910232544)
[2024-11-08 03:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:18][root][INFO] - Training Epoch: 1/10, step 166/574 completed (loss: 1.7675894498825073, acc: 0.5652173757553101)
[2024-11-08 03:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:18][root][INFO] - Training Epoch: 1/10, step 167/574 completed (loss: 1.1260358095169067, acc: 0.761904776096344)
[2024-11-08 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:19][root][INFO] - Training Epoch: 1/10, step 168/574 completed (loss: 1.150654911994934, acc: 0.5789473652839661)
[2024-11-08 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:20][root][INFO] - Training Epoch: 1/10, step 169/574 completed (loss: 1.5323081016540527, acc: 0.5909090638160706)
[2024-11-08 03:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:21][root][INFO] - Training Epoch: 1/10, step 170/574 completed (loss: 1.5959008932113647, acc: 0.5)
[2024-11-08 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:22][root][INFO] - Training Epoch: 1/10, step 171/574 completed (loss: 1.1091060638427734, acc: 0.6818181872367859)
[2024-11-08 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:22][root][INFO] - Training Epoch: 1/10, step 172/574 completed (loss: 2.4073503017425537, acc: 0.3636363744735718)
[2024-11-08 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:23][root][INFO] - Training Epoch: 1/10, step 173/574 completed (loss: 1.9439277648925781, acc: 0.4333333373069763)
[2024-11-08 03:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:24][root][INFO] - Training Epoch: 1/10, step 174/574 completed (loss: 1.3547141551971436, acc: 0.523809552192688)
[2024-11-08 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:24][root][INFO] - Training Epoch: 1/10, step 175/574 completed (loss: 1.140353798866272, acc: 0.7142857313156128)
[2024-11-08 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:25][root][INFO] - Training Epoch: 1/10, step 176/574 completed (loss: 1.4366363286972046, acc: 0.5263158082962036)
[2024-11-08 03:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:26][root][INFO] - Training Epoch: 1/10, step 177/574 completed (loss: 1.242830753326416, acc: 0.6363636255264282)
[2024-11-08 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:27][root][INFO] - Training Epoch: 1/10, step 178/574 completed (loss: 1.0848957300186157, acc: 0.7368420958518982)
[2024-11-08 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:28][root][INFO] - Training Epoch: 1/10, step 179/574 completed (loss: 0.9588060975074768, acc: 0.6363636255264282)
[2024-11-08 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:28][root][INFO] - Training Epoch: 1/10, step 180/574 completed (loss: 1.6456072330474854, acc: 0.5)
[2024-11-08 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:29][root][INFO] - Training Epoch: 1/10, step 181/574 completed (loss: 1.7329896688461304, acc: 0.5)
[2024-11-08 03:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:29][root][INFO] - Training Epoch: 1/10, step 182/574 completed (loss: 2.5686285495758057, acc: 0.2857142984867096)
[2024-11-08 03:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:29][root][INFO] - Training Epoch: 1/10, step 183/574 completed (loss: 1.5623761415481567, acc: 0.5652173757553101)
[2024-11-08 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:30][root][INFO] - Training Epoch: 1/10, step 184/574 completed (loss: 0.9622179865837097, acc: 0.761904776096344)
[2024-11-08 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:30][root][INFO] - Training Epoch: 1/10, step 185/574 completed (loss: 1.3086475133895874, acc: 0.6315789222717285)
[2024-11-08 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:31][root][INFO] - Training Epoch: 1/10, step 186/574 completed (loss: 1.2881771326065063, acc: 0.550000011920929)
[2024-11-08 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:32][root][INFO] - Training Epoch: 1/10, step 187/574 completed (loss: 1.068679690361023, acc: 0.6190476417541504)
[2024-11-08 03:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:32][root][INFO] - Training Epoch: 1/10, step 188/574 completed (loss: 1.068167805671692, acc: 0.6363636255264282)
[2024-11-08 03:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:33][root][INFO] - Training Epoch: 1/10, step 189/574 completed (loss: 1.6535307168960571, acc: 0.47999998927116394)
[2024-11-08 03:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:34][root][INFO] - Training Epoch: 1/10, step 190/574 completed (loss: 1.007545828819275, acc: 0.6499999761581421)
[2024-11-08 03:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:35][root][INFO] - Training Epoch: 1/10, step 191/574 completed (loss: 0.9808995127677917, acc: 0.6190476417541504)
[2024-11-08 03:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:37][root][INFO] - Training Epoch: 1/10, step 192/574 completed (loss: 0.9028990864753723, acc: 0.7894737124443054)
[2024-11-08 03:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:38][root][INFO] - Training Epoch: 1/10, step 193/574 completed (loss: 0.989087700843811, acc: 0.6363636255264282)
[2024-11-08 03:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:40][root][INFO] - Training Epoch: 1/10, step 194/574 completed (loss: 1.103846549987793, acc: 0.6190476417541504)
[2024-11-08 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:41][root][INFO] - Training Epoch: 1/10, step 195/574 completed (loss: 0.9071058630943298, acc: 0.8333333134651184)
[2024-11-08 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:42][root][INFO] - Training Epoch: 1/10, step 196/574 completed (loss: 1.4895455837249756, acc: 0.5483871102333069)
[2024-11-08 03:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:42][root][INFO] - Training Epoch: 1/10, step 197/574 completed (loss: 1.9760677814483643, acc: 0.46666666865348816)
[2024-11-08 03:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:42][root][INFO] - Training Epoch: 1/10, step 198/574 completed (loss: 0.7869168519973755, acc: 0.699999988079071)
[2024-11-08 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:43][root][INFO] - Training Epoch: 1/10, step 199/574 completed (loss: 0.8025169372558594, acc: 0.699999988079071)
[2024-11-08 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:43][root][INFO] - Training Epoch: 1/10, step 200/574 completed (loss: 0.8538795113563538, acc: 0.7894737124443054)
[2024-11-08 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:44][root][INFO] - Training Epoch: 1/10, step 201/574 completed (loss: 1.0094443559646606, acc: 0.5909090638160706)
[2024-11-08 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:44][root][INFO] - Training Epoch: 1/10, step 202/574 completed (loss: 0.8436954617500305, acc: 0.699999988079071)
[2024-11-08 03:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:45][root][INFO] - Training Epoch: 1/10, step 203/574 completed (loss: 1.2071341276168823, acc: 0.5652173757553101)
[2024-11-08 03:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:45][root][INFO] - Training Epoch: 1/10, step 204/574 completed (loss: 2.2822959423065186, acc: 0.4285714328289032)
[2024-11-08 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:46][root][INFO] - Training Epoch: 1/10, step 205/574 completed (loss: 0.607113778591156, acc: 0.8095238208770752)
[2024-11-08 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:46][root][INFO] - Training Epoch: 1/10, step 206/574 completed (loss: 0.7636923789978027, acc: 0.7894737124443054)
[2024-11-08 03:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:47][root][INFO] - Training Epoch: 1/10, step 207/574 completed (loss: 0.8039823174476624, acc: 0.7368420958518982)
[2024-11-08 03:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:47][root][INFO] - Training Epoch: 1/10, step 208/574 completed (loss: 0.8736600875854492, acc: 0.6818181872367859)
[2024-11-08 03:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:48][root][INFO] - Training Epoch: 1/10, step 209/574 completed (loss: 0.9270336627960205, acc: 0.6666666865348816)
[2024-11-08 03:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:48][root][INFO] - Training Epoch: 1/10, step 210/574 completed (loss: 0.6791966557502747, acc: 0.7916666865348816)
[2024-11-08 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:49][root][INFO] - Training Epoch: 1/10, step 211/574 completed (loss: 1.5484896898269653, acc: 0.5483871102333069)
[2024-11-08 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:49][root][INFO] - Training Epoch: 1/10, step 212/574 completed (loss: 2.0349810123443604, acc: 0.3870967626571655)
[2024-11-08 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:50][root][INFO] - Training Epoch: 1/10, step 213/574 completed (loss: 1.862540364265442, acc: 0.5769230723381042)
[2024-11-08 03:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:51][root][INFO] - Training Epoch: 1/10, step 214/574 completed (loss: 0.826907217502594, acc: 0.6190476417541504)
[2024-11-08 03:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:51][root][INFO] - Training Epoch: 1/10, step 215/574 completed (loss: 0.9082874655723572, acc: 0.6842105388641357)
[2024-11-08 03:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:52][root][INFO] - Training Epoch: 1/10, step 216/574 completed (loss: 0.9564156532287598, acc: 0.6315789222717285)
[2024-11-08 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:52][root][INFO] - Training Epoch: 1/10, step 217/574 completed (loss: 0.7766347527503967, acc: 0.7272727489471436)
[2024-11-08 03:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:53][root][INFO] - Training Epoch: 1/10, step 218/574 completed (loss: 1.0338281393051147, acc: 0.7142857313156128)
[2024-11-08 03:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:54][root][INFO] - Training Epoch: 1/10, step 219/574 completed (loss: 1.2673969268798828, acc: 0.625)
[2024-11-08 03:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:54][root][INFO] - Training Epoch: 1/10, step 220/574 completed (loss: 1.5059571266174316, acc: 0.5483871102333069)
[2024-11-08 03:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:54][root][INFO] - Training Epoch: 1/10, step 221/574 completed (loss: 1.6723195314407349, acc: 0.4193548262119293)
[2024-11-08 03:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:55][root][INFO] - Training Epoch: 1/10, step 222/574 completed (loss: 1.2636280059814453, acc: 0.6538461446762085)
[2024-11-08 03:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:56][root][INFO] - Training Epoch: 1/10, step 223/574 completed (loss: 0.7294005155563354, acc: 0.8571428656578064)
[2024-11-08 03:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:57][root][INFO] - Training Epoch: 1/10, step 224/574 completed (loss: 1.191766619682312, acc: 0.7368420958518982)
[2024-11-08 03:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:58][root][INFO] - Training Epoch: 1/10, step 225/574 completed (loss: 1.0450619459152222, acc: 0.6842105388641357)
[2024-11-08 03:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:58][root][INFO] - Training Epoch: 1/10, step 226/574 completed (loss: 1.0475537776947021, acc: 0.6363636255264282)
[2024-11-08 03:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:59][root][INFO] - Training Epoch: 1/10, step 227/574 completed (loss: 1.1114870309829712, acc: 0.7142857313156128)
[2024-11-08 03:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:59][root][INFO] - Training Epoch: 1/10, step 228/574 completed (loss: 0.9996643662452698, acc: 0.625)
[2024-11-08 03:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:27:59][root][INFO] - Training Epoch: 1/10, step 229/574 completed (loss: 0.5993528366088867, acc: 0.8695651888847351)
[2024-11-08 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:00][root][INFO] - Training Epoch: 1/10, step 230/574 completed (loss: 0.7501885890960693, acc: 0.8095238208770752)
[2024-11-08 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:00][root][INFO] - Training Epoch: 1/10, step 231/574 completed (loss: 0.550827145576477, acc: 0.8421052694320679)
[2024-11-08 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:01][root][INFO] - Training Epoch: 1/10, step 232/574 completed (loss: 0.927707850933075, acc: 0.6842105388641357)
[2024-11-08 03:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:01][root][INFO] - Training Epoch: 1/10, step 233/574 completed (loss: 1.1561447381973267, acc: 0.5909090638160706)
[2024-11-08 03:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:02][root][INFO] - Training Epoch: 1/10, step 234/574 completed (loss: 0.8948898315429688, acc: 0.6818181872367859)
[2024-11-08 03:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:03][root][INFO] - Training Epoch: 1/10, step 235/574 completed (loss: 0.9497076272964478, acc: 0.7142857313156128)
[2024-11-08 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:03][root][INFO] - Training Epoch: 1/10, step 236/574 completed (loss: 1.1198853254318237, acc: 0.6296296119689941)
[2024-11-08 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:03][root][INFO] - Training Epoch: 1/10, step 237/574 completed (loss: 2.3519866466522217, acc: 0.39393940567970276)
[2024-11-08 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:04][root][INFO] - Training Epoch: 1/10, step 238/574 completed (loss: 0.9970264434814453, acc: 0.6818181872367859)
[2024-11-08 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:04][root][INFO] - Training Epoch: 1/10, step 239/574 completed (loss: 0.7339943051338196, acc: 0.7142857313156128)
[2024-11-08 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:05][root][INFO] - Training Epoch: 1/10, step 240/574 completed (loss: 0.7781078219413757, acc: 0.7894737124443054)
[2024-11-08 03:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:05][root][INFO] - Training Epoch: 1/10, step 241/574 completed (loss: 0.8503753542900085, acc: 0.7272727489471436)
[2024-11-08 03:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:06][root][INFO] - Training Epoch: 1/10, step 242/574 completed (loss: 1.8838577270507812, acc: 0.5789473652839661)
[2024-11-08 03:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:07][root][INFO] - Training Epoch: 1/10, step 243/574 completed (loss: 1.3846615552902222, acc: 0.7272727489471436)
[2024-11-08 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:07][root][INFO] - Training Epoch: 1/10, step 244/574 completed (loss: 1.4037058353424072, acc: 0.5714285969734192)
[2024-11-08 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:08][root][INFO] - Training Epoch: 1/10, step 245/574 completed (loss: 1.1772974729537964, acc: 0.699999988079071)
[2024-11-08 03:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:08][root][INFO] - Training Epoch: 1/10, step 246/574 completed (loss: 1.6957926750183105, acc: 0.48571428656578064)
[2024-11-08 03:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:08][root][INFO] - Training Epoch: 1/10, step 247/574 completed (loss: 0.908133864402771, acc: 0.695652186870575)
[2024-11-08 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:09][root][INFO] - Training Epoch: 1/10, step 248/574 completed (loss: 0.6541504859924316, acc: 0.8095238208770752)
[2024-11-08 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:09][root][INFO] - Training Epoch: 1/10, step 249/574 completed (loss: 0.721768856048584, acc: 0.7368420958518982)
[2024-11-08 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:10][root][INFO] - Training Epoch: 1/10, step 250/574 completed (loss: 0.7694069743156433, acc: 0.75)
[2024-11-08 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:10][root][INFO] - Training Epoch: 1/10, step 251/574 completed (loss: 1.1443328857421875, acc: 0.6666666865348816)
[2024-11-08 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:10][root][INFO] - Training Epoch: 1/10, step 252/574 completed (loss: 0.8515363931655884, acc: 0.7272727489471436)
[2024-11-08 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:11][root][INFO] - Training Epoch: 1/10, step 253/574 completed (loss: 0.6931454539299011, acc: 0.8928571343421936)
[2024-11-08 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:11][root][INFO] - Training Epoch: 1/10, step 254/574 completed (loss: 0.8729755878448486, acc: 0.7241379022598267)
[2024-11-08 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:12][root][INFO] - Training Epoch: 1/10, step 255/574 completed (loss: 1.6492246389389038, acc: 0.5714285969734192)
[2024-11-08 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:12][root][INFO] - Training Epoch: 1/10, step 256/574 completed (loss: 0.8873541951179504, acc: 0.7142857313156128)
[2024-11-08 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:12][root][INFO] - Training Epoch: 1/10, step 257/574 completed (loss: 0.7105371356010437, acc: 0.761904776096344)
[2024-11-08 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:13][root][INFO] - Training Epoch: 1/10, step 258/574 completed (loss: 0.7155067920684814, acc: 0.7894737124443054)
[2024-11-08 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:14][root][INFO] - Training Epoch: 1/10, step 259/574 completed (loss: 0.7922834157943726, acc: 0.7272727489471436)
[2024-11-08 03:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:14][root][INFO] - Training Epoch: 1/10, step 260/574 completed (loss: 1.003714919090271, acc: 0.5789473652839661)
[2024-11-08 03:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:15][root][INFO] - Training Epoch: 1/10, step 261/574 completed (loss: 0.7121873497962952, acc: 0.6818181872367859)
[2024-11-08 03:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:15][root][INFO] - Training Epoch: 1/10, step 262/574 completed (loss: 1.3480170965194702, acc: 0.6176470518112183)
[2024-11-08 03:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:16][root][INFO] - Training Epoch: 1/10, step 263/574 completed (loss: 0.6912155747413635, acc: 0.7727272510528564)
[2024-11-08 03:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:16][root][INFO] - Training Epoch: 1/10, step 264/574 completed (loss: 0.4776709973812103, acc: 0.8500000238418579)
[2024-11-08 03:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:18][root][INFO] - Training Epoch: 1/10, step 265/574 completed (loss: 0.5370835661888123, acc: 0.7368420958518982)
[2024-11-08 03:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:18][root][INFO] - Training Epoch: 1/10, step 266/574 completed (loss: 0.632999062538147, acc: 0.739130437374115)
[2024-11-08 03:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:18][root][INFO] - Training Epoch: 1/10, step 267/574 completed (loss: 0.5354494452476501, acc: 0.9047619104385376)
[2024-11-08 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:19][root][INFO] - Training Epoch: 1/10, step 268/574 completed (loss: 0.6581953763961792, acc: 0.7586206793785095)
[2024-11-08 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:20][root][INFO] - Training Epoch: 1/10, step 269/574 completed (loss: 0.9337174892425537, acc: 0.7407407164573669)
[2024-11-08 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:20][root][INFO] - Training Epoch: 1/10, step 270/574 completed (loss: 0.7761653661727905, acc: 0.7307692170143127)
[2024-11-08 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:21][root][INFO] - Training Epoch: 1/10, step 271/574 completed (loss: 0.3422103226184845, acc: 0.8571428656578064)
[2024-11-08 03:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:21][root][INFO] - Training Epoch: 1/10, step 272/574 completed (loss: 0.7399277091026306, acc: 0.7894737124443054)
[2024-11-08 03:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:21][root][INFO] - Training Epoch: 1/10, step 273/574 completed (loss: 0.8636981844902039, acc: 0.7894737124443054)
[2024-11-08 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:22][root][INFO] - Training Epoch: 1/10, step 274/574 completed (loss: 0.6939327716827393, acc: 0.7727272510528564)
[2024-11-08 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:22][root][INFO] - Training Epoch: 1/10, step 275/574 completed (loss: 0.5126120448112488, acc: 0.8095238208770752)
[2024-11-08 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:23][root][INFO] - Training Epoch: 1/10, step 276/574 completed (loss: 0.45775094628334045, acc: 0.8275862336158752)
[2024-11-08 03:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:23][root][INFO] - Training Epoch: 1/10, step 277/574 completed (loss: 0.8020724058151245, acc: 0.7777777910232544)
[2024-11-08 03:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:23][root][INFO] - Training Epoch: 1/10, step 278/574 completed (loss: 0.40609410405158997, acc: 0.9047619104385376)
[2024-11-08 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:24][root][INFO] - Training Epoch: 1/10, step 279/574 completed (loss: 0.42677393555641174, acc: 0.8571428656578064)
[2024-11-08 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:24][root][INFO] - Training Epoch: 1/10, step 280/574 completed (loss: 0.49679914116859436, acc: 0.8333333134651184)
[2024-11-08 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:25][root][INFO] - Training Epoch: 1/10, step 281/574 completed (loss: 0.7106821537017822, acc: 0.6818181872367859)
[2024-11-08 03:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:25][root][INFO] - Training Epoch: 1/10, step 282/574 completed (loss: 0.4910823702812195, acc: 0.761904776096344)
[2024-11-08 03:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:26][root][INFO] - Training Epoch: 1/10, step 283/574 completed (loss: 0.4114004075527191, acc: 0.8620689511299133)
[2024-11-08 03:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:26][root][INFO] - Training Epoch: 1/10, step 284/574 completed (loss: 1.199391484260559, acc: 0.6000000238418579)
[2024-11-08 03:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:27][root][INFO] - Training Epoch: 1/10, step 285/574 completed (loss: 1.570114254951477, acc: 0.6206896305084229)
[2024-11-08 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:11][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7619, device='cuda:0') eval_epoch_loss=tensor(0.5664, device='cuda:0') eval_epoch_acc=tensor(0.8138, device='cuda:0')
[2024-11-08 03:29:11][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:29:11][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:29:14][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_1_step_286_loss_0.5663700699806213/model.pt
[2024-11-08 03:29:14][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:29:14][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.5663700699806213
[2024-11-08 03:29:14][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8137851357460022
[2024-11-08 03:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:15][root][INFO] - Training Epoch: 1/10, step 286/574 completed (loss: 0.5021991729736328, acc: 0.8571428656578064)
[2024-11-08 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:15][root][INFO] - Training Epoch: 1/10, step 287/574 completed (loss: 0.46615830063819885, acc: 0.7894737124443054)
[2024-11-08 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:16][root][INFO] - Training Epoch: 1/10, step 288/574 completed (loss: 0.6079858541488647, acc: 0.7894737124443054)
[2024-11-08 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:16][root][INFO] - Training Epoch: 1/10, step 289/574 completed (loss: 0.44870269298553467, acc: 0.8181818127632141)
[2024-11-08 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:17][root][INFO] - Training Epoch: 1/10, step 290/574 completed (loss: 0.4145844578742981, acc: 0.8636363744735718)
[2024-11-08 03:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:17][root][INFO] - Training Epoch: 1/10, step 291/574 completed (loss: 1.0035327672958374, acc: 0.7200000286102295)
[2024-11-08 03:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:17][root][INFO] - Training Epoch: 1/10, step 292/574 completed (loss: 0.5422937273979187, acc: 0.8333333134651184)
[2024-11-08 03:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:18][root][INFO] - Training Epoch: 1/10, step 293/574 completed (loss: 0.5468164086341858, acc: 0.761904776096344)
[2024-11-08 03:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:19][root][INFO] - Training Epoch: 1/10, step 294/574 completed (loss: 0.44983187317848206, acc: 0.8500000238418579)
[2024-11-08 03:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:19][root][INFO] - Training Epoch: 1/10, step 295/574 completed (loss: 0.7138356566429138, acc: 0.7894737124443054)
[2024-11-08 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:20][root][INFO] - Training Epoch: 1/10, step 296/574 completed (loss: 0.5451021790504456, acc: 0.8181818127632141)
[2024-11-08 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:20][root][INFO] - Training Epoch: 1/10, step 297/574 completed (loss: 0.3948711156845093, acc: 0.8571428656578064)
[2024-11-08 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:21][root][INFO] - Training Epoch: 1/10, step 298/574 completed (loss: 0.5710479617118835, acc: 0.7916666865348816)
[2024-11-08 03:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:21][root][INFO] - Training Epoch: 1/10, step 299/574 completed (loss: 0.5065816640853882, acc: 0.8387096524238586)
[2024-11-08 03:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:22][root][INFO] - Training Epoch: 1/10, step 300/574 completed (loss: 0.9001648426055908, acc: 0.7419354915618896)
[2024-11-08 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:22][root][INFO] - Training Epoch: 1/10, step 301/574 completed (loss: 0.5062374472618103, acc: 0.807692289352417)
[2024-11-08 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:22][root][INFO] - Training Epoch: 1/10, step 302/574 completed (loss: 0.4944556951522827, acc: 0.6666666865348816)
[2024-11-08 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:23][root][INFO] - Training Epoch: 1/10, step 303/574 completed (loss: 0.47727254033088684, acc: 0.7894737124443054)
[2024-11-08 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:23][root][INFO] - Training Epoch: 1/10, step 304/574 completed (loss: 0.5057263374328613, acc: 0.7368420958518982)
[2024-11-08 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:24][root][INFO] - Training Epoch: 1/10, step 305/574 completed (loss: 0.5960243344306946, acc: 0.739130437374115)
[2024-11-08 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:24][root][INFO] - Training Epoch: 1/10, step 306/574 completed (loss: 0.7120354175567627, acc: 0.7727272510528564)
[2024-11-08 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:24][root][INFO] - Training Epoch: 1/10, step 307/574 completed (loss: 0.26393821835517883, acc: 0.9230769276618958)
[2024-11-08 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:25][root][INFO] - Training Epoch: 1/10, step 308/574 completed (loss: 0.32123374938964844, acc: 0.9047619104385376)
[2024-11-08 03:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:25][root][INFO] - Training Epoch: 1/10, step 309/574 completed (loss: 0.4524165689945221, acc: 0.8421052694320679)
[2024-11-08 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:26][root][INFO] - Training Epoch: 1/10, step 310/574 completed (loss: 0.3553185760974884, acc: 0.8421052694320679)
[2024-11-08 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:26][root][INFO] - Training Epoch: 1/10, step 311/574 completed (loss: 0.4800248444080353, acc: 0.8181818127632141)
[2024-11-08 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:27][root][INFO] - Training Epoch: 1/10, step 312/574 completed (loss: 0.4547165632247925, acc: 0.8181818127632141)
[2024-11-08 03:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:27][root][INFO] - Training Epoch: 1/10, step 313/574 completed (loss: 0.5190069079399109, acc: 0.8214285969734192)
[2024-11-08 03:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:27][root][INFO] - Training Epoch: 1/10, step 314/574 completed (loss: 0.7551807761192322, acc: 0.8148148059844971)
[2024-11-08 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:28][root][INFO] - Training Epoch: 1/10, step 315/574 completed (loss: 1.227169394493103, acc: 0.6857143044471741)
[2024-11-08 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:28][root][INFO] - Training Epoch: 1/10, step 316/574 completed (loss: 0.5257554650306702, acc: 0.807692289352417)
[2024-11-08 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:29][root][INFO] - Training Epoch: 1/10, step 317/574 completed (loss: 0.2640340030193329, acc: 0.8571428656578064)
[2024-11-08 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:29][root][INFO] - Training Epoch: 1/10, step 318/574 completed (loss: 0.5992428064346313, acc: 0.75)
[2024-11-08 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:30][root][INFO] - Training Epoch: 1/10, step 319/574 completed (loss: 0.41107073426246643, acc: 0.8500000238418579)
[2024-11-08 03:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:30][root][INFO] - Training Epoch: 1/10, step 320/574 completed (loss: 0.5942743420600891, acc: 0.7142857313156128)
[2024-11-08 03:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:30][root][INFO] - Training Epoch: 1/10, step 321/574 completed (loss: 0.49893465638160706, acc: 0.8181818127632141)
[2024-11-08 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:31][root][INFO] - Training Epoch: 1/10, step 322/574 completed (loss: 0.4215633273124695, acc: 0.8695651888847351)
[2024-11-08 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:31][root][INFO] - Training Epoch: 1/10, step 323/574 completed (loss: 0.4849991500377655, acc: 0.8095238208770752)
[2024-11-08 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:32][root][INFO] - Training Epoch: 1/10, step 324/574 completed (loss: 0.4789043664932251, acc: 0.7894737124443054)
[2024-11-08 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:32][root][INFO] - Training Epoch: 1/10, step 325/574 completed (loss: 0.3405155539512634, acc: 0.7727272510528564)
[2024-11-08 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:32][root][INFO] - Training Epoch: 1/10, step 326/574 completed (loss: 0.5087305307388306, acc: 0.8421052694320679)
[2024-11-08 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:33][root][INFO] - Training Epoch: 1/10, step 327/574 completed (loss: 0.5331724882125854, acc: 0.8333333134651184)
[2024-11-08 03:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:33][root][INFO] - Training Epoch: 1/10, step 328/574 completed (loss: 0.5933027863502502, acc: 0.8275862336158752)
[2024-11-08 03:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:33][root][INFO] - Training Epoch: 1/10, step 329/574 completed (loss: 0.5465468764305115, acc: 0.7777777910232544)
[2024-11-08 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:34][root][INFO] - Training Epoch: 1/10, step 330/574 completed (loss: 0.49233976006507874, acc: 0.8571428656578064)
[2024-11-08 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:34][root][INFO] - Training Epoch: 1/10, step 331/574 completed (loss: 0.24810200929641724, acc: 0.8947368264198303)
[2024-11-08 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:35][root][INFO] - Training Epoch: 1/10, step 332/574 completed (loss: 0.42494210600852966, acc: 0.8181818127632141)
[2024-11-08 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:35][root][INFO] - Training Epoch: 1/10, step 333/574 completed (loss: 0.5911733508110046, acc: 0.7894737124443054)
[2024-11-08 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:36][root][INFO] - Training Epoch: 1/10, step 334/574 completed (loss: 0.544647216796875, acc: 0.7272727489471436)
[2024-11-08 03:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:36][root][INFO] - Training Epoch: 1/10, step 335/574 completed (loss: 0.7330912947654724, acc: 0.807692289352417)
[2024-11-08 03:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:36][root][INFO] - Training Epoch: 1/10, step 336/574 completed (loss: 0.6162293553352356, acc: 0.7916666865348816)
[2024-11-08 03:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:37][root][INFO] - Training Epoch: 1/10, step 337/574 completed (loss: 0.4896392822265625, acc: 0.8095238208770752)
[2024-11-08 03:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:37][root][INFO] - Training Epoch: 1/10, step 338/574 completed (loss: 0.32155585289001465, acc: 0.8695651888847351)
[2024-11-08 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:38][root][INFO] - Training Epoch: 1/10, step 339/574 completed (loss: 0.2887325882911682, acc: 0.8636363744735718)
[2024-11-08 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:38][root][INFO] - Training Epoch: 1/10, step 340/574 completed (loss: 0.6330822706222534, acc: 0.8333333134651184)
[2024-11-08 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:39][root][INFO] - Training Epoch: 1/10, step 341/574 completed (loss: 0.5648727416992188, acc: 0.8181818127632141)
[2024-11-08 03:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:39][root][INFO] - Training Epoch: 1/10, step 342/574 completed (loss: 0.39163094758987427, acc: 0.8500000238418579)
[2024-11-08 03:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:40][root][INFO] - Training Epoch: 1/10, step 343/574 completed (loss: 0.4145418107509613, acc: 0.8500000238418579)
[2024-11-08 03:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:40][root][INFO] - Training Epoch: 1/10, step 344/574 completed (loss: 0.6370525360107422, acc: 0.7894737124443054)
[2024-11-08 03:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:40][root][INFO] - Training Epoch: 1/10, step 345/574 completed (loss: 0.21417857706546783, acc: 0.9090909361839294)
[2024-11-08 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:41][root][INFO] - Training Epoch: 1/10, step 346/574 completed (loss: 0.6395547389984131, acc: 0.699999988079071)
[2024-11-08 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:41][root][INFO] - Training Epoch: 1/10, step 347/574 completed (loss: 0.5990702509880066, acc: 0.7916666865348816)
[2024-11-08 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:42][root][INFO] - Training Epoch: 1/10, step 348/574 completed (loss: 0.6661949753761292, acc: 0.782608687877655)
[2024-11-08 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:42][root][INFO] - Training Epoch: 1/10, step 349/574 completed (loss: 0.26235172152519226, acc: 0.8500000238418579)
[2024-11-08 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:43][root][INFO] - Training Epoch: 1/10, step 350/574 completed (loss: 0.4129469096660614, acc: 0.8421052694320679)
[2024-11-08 03:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:43][root][INFO] - Training Epoch: 1/10, step 351/574 completed (loss: 0.3008873462677002, acc: 0.9090909361839294)
[2024-11-08 03:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:44][root][INFO] - Training Epoch: 1/10, step 352/574 completed (loss: 0.47299546003341675, acc: 0.800000011920929)
[2024-11-08 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:44][root][INFO] - Training Epoch: 1/10, step 353/574 completed (loss: 0.23064148426055908, acc: 0.9200000166893005)
[2024-11-08 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:44][root][INFO] - Training Epoch: 1/10, step 354/574 completed (loss: 0.44689705967903137, acc: 0.8399999737739563)
[2024-11-08 03:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:45][root][INFO] - Training Epoch: 1/10, step 355/574 completed (loss: 0.37629690766334534, acc: 0.9047619104385376)
[2024-11-08 03:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:46][root][INFO] - Training Epoch: 1/10, step 356/574 completed (loss: 0.40674471855163574, acc: 0.800000011920929)
[2024-11-08 03:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:46][root][INFO] - Training Epoch: 1/10, step 357/574 completed (loss: 0.3908914029598236, acc: 0.8571428656578064)
[2024-11-08 03:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:46][root][INFO] - Training Epoch: 1/10, step 358/574 completed (loss: 0.34098032116889954, acc: 0.8181818127632141)
[2024-11-08 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:47][root][INFO] - Training Epoch: 1/10, step 359/574 completed (loss: 0.3732149600982666, acc: 0.8928571343421936)
[2024-11-08 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:47][root][INFO] - Training Epoch: 1/10, step 360/574 completed (loss: 0.7720349431037903, acc: 0.7931034564971924)
[2024-11-08 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:48][root][INFO] - Training Epoch: 1/10, step 361/574 completed (loss: 0.5949292182922363, acc: 0.800000011920929)
[2024-11-08 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:48][root][INFO] - Training Epoch: 1/10, step 362/574 completed (loss: 0.4826306402683258, acc: 0.8181818127632141)
[2024-11-08 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:48][root][INFO] - Training Epoch: 1/10, step 363/574 completed (loss: 0.4321036636829376, acc: 0.8421052694320679)
[2024-11-08 03:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:49][root][INFO] - Training Epoch: 1/10, step 364/574 completed (loss: 0.21716034412384033, acc: 0.9047619104385376)
[2024-11-08 03:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:49][root][INFO] - Training Epoch: 1/10, step 365/574 completed (loss: 0.30756643414497375, acc: 0.8947368264198303)
[2024-11-08 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:50][root][INFO] - Training Epoch: 1/10, step 366/574 completed (loss: 0.3702540397644043, acc: 0.8636363744735718)
[2024-11-08 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:50][root][INFO] - Training Epoch: 1/10, step 367/574 completed (loss: 0.5667910575866699, acc: 0.8148148059844971)
[2024-11-08 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:51][root][INFO] - Training Epoch: 1/10, step 368/574 completed (loss: 0.393231064081192, acc: 0.84375)
[2024-11-08 03:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:51][root][INFO] - Training Epoch: 1/10, step 369/574 completed (loss: 0.6602038145065308, acc: 0.8095238208770752)
[2024-11-08 03:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:52][root][INFO] - Training Epoch: 1/10, step 370/574 completed (loss: 0.4157046377658844, acc: 0.8095238208770752)
[2024-11-08 03:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:53][root][INFO] - Training Epoch: 1/10, step 371/574 completed (loss: 0.33603739738464355, acc: 0.8947368264198303)
[2024-11-08 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:54][root][INFO] - Training Epoch: 1/10, step 372/574 completed (loss: 0.3638586699962616, acc: 0.8636363744735718)
[2024-11-08 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:54][root][INFO] - Training Epoch: 1/10, step 373/574 completed (loss: 0.42453405261039734, acc: 0.7894737124443054)
[2024-11-08 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:55][root][INFO] - Training Epoch: 1/10, step 374/574 completed (loss: 0.4567957818508148, acc: 0.8181818127632141)
[2024-11-08 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:55][root][INFO] - Training Epoch: 1/10, step 375/574 completed (loss: 0.3207702934741974, acc: 0.8620689511299133)
[2024-11-08 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:56][root][INFO] - Training Epoch: 1/10, step 376/574 completed (loss: 0.41960957646369934, acc: 0.8709677457809448)
[2024-11-08 03:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:56][root][INFO] - Training Epoch: 1/10, step 377/574 completed (loss: 0.3494245707988739, acc: 0.9047619104385376)
[2024-11-08 03:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:56][root][INFO] - Training Epoch: 1/10, step 378/574 completed (loss: 0.42961007356643677, acc: 0.7142857313156128)
[2024-11-08 03:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:57][root][INFO] - Training Epoch: 1/10, step 379/574 completed (loss: 0.2583916485309601, acc: 0.8947368264198303)
[2024-11-08 03:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:29:58][root][INFO] - Training Epoch: 1/10, step 380/574 completed (loss: 0.3559455871582031, acc: 0.7272727489471436)
[2024-11-08 03:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:00][root][INFO] - Training Epoch: 1/10, step 381/574 completed (loss: 0.5136366486549377, acc: 0.8421052694320679)
[2024-11-08 03:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:00][root][INFO] - Training Epoch: 1/10, step 382/574 completed (loss: 0.31439682841300964, acc: 0.8636363744735718)
[2024-11-08 03:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:01][root][INFO] - Training Epoch: 1/10, step 383/574 completed (loss: 0.2398417741060257, acc: 0.8928571343421936)
[2024-11-08 03:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:01][root][INFO] - Training Epoch: 1/10, step 384/574 completed (loss: 0.3601585626602173, acc: 0.8666666746139526)
[2024-11-08 03:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:02][root][INFO] - Training Epoch: 1/10, step 385/574 completed (loss: 0.38456660509109497, acc: 0.8571428656578064)
[2024-11-08 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:02][root][INFO] - Training Epoch: 1/10, step 386/574 completed (loss: 0.3666289746761322, acc: 0.8500000238418579)
[2024-11-08 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:02][root][INFO] - Training Epoch: 1/10, step 387/574 completed (loss: 0.9811910390853882, acc: 0.7368420958518982)
[2024-11-08 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:03][root][INFO] - Training Epoch: 1/10, step 388/574 completed (loss: 0.32393768429756165, acc: 0.8999999761581421)
[2024-11-08 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:03][root][INFO] - Training Epoch: 1/10, step 389/574 completed (loss: 0.5871949195861816, acc: 0.800000011920929)
[2024-11-08 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:04][root][INFO] - Training Epoch: 1/10, step 390/574 completed (loss: 0.3461664021015167, acc: 0.8181818127632141)
[2024-11-08 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:04][root][INFO] - Training Epoch: 1/10, step 391/574 completed (loss: 0.14943604171276093, acc: 0.9523809552192688)
[2024-11-08 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:04][root][INFO] - Training Epoch: 1/10, step 392/574 completed (loss: 0.560465931892395, acc: 0.8421052694320679)
[2024-11-08 03:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:05][root][INFO] - Training Epoch: 1/10, step 393/574 completed (loss: 0.6942318677902222, acc: 0.7894737124443054)
[2024-11-08 03:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:06][root][INFO] - Training Epoch: 1/10, step 394/574 completed (loss: 0.6191068291664124, acc: 0.7727272510528564)
[2024-11-08 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:06][root][INFO] - Training Epoch: 1/10, step 395/574 completed (loss: 0.2825467586517334, acc: 0.9047619104385376)
[2024-11-08 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:07][root][INFO] - Training Epoch: 1/10, step 396/574 completed (loss: 0.5070980191230774, acc: 0.8275862336158752)
[2024-11-08 03:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:07][root][INFO] - Training Epoch: 1/10, step 397/574 completed (loss: 0.6591811776161194, acc: 0.7666666507720947)
[2024-11-08 03:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:08][root][INFO] - Training Epoch: 1/10, step 398/574 completed (loss: 0.30607229471206665, acc: 0.9047619104385376)
[2024-11-08 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:08][root][INFO] - Training Epoch: 1/10, step 399/574 completed (loss: 0.332560658454895, acc: 0.8888888955116272)
[2024-11-08 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:09][root][INFO] - Training Epoch: 1/10, step 400/574 completed (loss: 0.3570234775543213, acc: 0.8636363744735718)
[2024-11-08 03:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:09][root][INFO] - Training Epoch: 1/10, step 401/574 completed (loss: 0.2860752046108246, acc: 0.8571428656578064)
[2024-11-08 03:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:10][root][INFO] - Training Epoch: 1/10, step 402/574 completed (loss: 0.5266130566596985, acc: 0.875)
[2024-11-08 03:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:10][root][INFO] - Training Epoch: 1/10, step 403/574 completed (loss: 0.3530547022819519, acc: 0.875)
[2024-11-08 03:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:11][root][INFO] - Training Epoch: 1/10, step 404/574 completed (loss: 0.3460482358932495, acc: 0.8857142925262451)
[2024-11-08 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:11][root][INFO] - Training Epoch: 1/10, step 405/574 completed (loss: 0.6127581000328064, acc: 0.7692307829856873)
[2024-11-08 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:12][root][INFO] - Training Epoch: 1/10, step 406/574 completed (loss: 0.5015165209770203, acc: 0.8095238208770752)
[2024-11-08 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:12][root][INFO] - Training Epoch: 1/10, step 407/574 completed (loss: 0.7397698760032654, acc: 0.7894737124443054)
[2024-11-08 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:12][root][INFO] - Training Epoch: 1/10, step 408/574 completed (loss: 0.45238226652145386, acc: 0.8500000238418579)
[2024-11-08 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:13][root][INFO] - Training Epoch: 1/10, step 409/574 completed (loss: 0.4180131256580353, acc: 0.8571428656578064)
[2024-11-08 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:13][root][INFO] - Training Epoch: 1/10, step 410/574 completed (loss: 0.3595723807811737, acc: 0.8181818127632141)
[2024-11-08 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:14][root][INFO] - Training Epoch: 1/10, step 411/574 completed (loss: 0.353459894657135, acc: 0.8571428656578064)
[2024-11-08 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:14][root][INFO] - Training Epoch: 1/10, step 412/574 completed (loss: 0.33343517780303955, acc: 0.8518518805503845)
[2024-11-08 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:15][root][INFO] - Training Epoch: 1/10, step 413/574 completed (loss: 0.33899223804473877, acc: 0.9142857193946838)
[2024-11-08 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:15][root][INFO] - Training Epoch: 1/10, step 414/574 completed (loss: 0.3808155655860901, acc: 0.7692307829856873)
[2024-11-08 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:15][root][INFO] - Training Epoch: 1/10, step 415/574 completed (loss: 0.24446530640125275, acc: 0.9047619104385376)
[2024-11-08 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:16][root][INFO] - Training Epoch: 1/10, step 416/574 completed (loss: 0.5735852122306824, acc: 0.7894737124443054)
[2024-11-08 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:16][root][INFO] - Training Epoch: 1/10, step 417/574 completed (loss: 0.5093312859535217, acc: 0.8421052694320679)
[2024-11-08 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:17][root][INFO] - Training Epoch: 1/10, step 418/574 completed (loss: 0.42312633991241455, acc: 0.8636363744735718)
[2024-11-08 03:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:17][root][INFO] - Training Epoch: 1/10, step 419/574 completed (loss: 0.4411427974700928, acc: 0.761904776096344)
[2024-11-08 03:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:18][root][INFO] - Training Epoch: 1/10, step 420/574 completed (loss: 0.20483601093292236, acc: 0.9583333134651184)
[2024-11-08 03:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:18][root][INFO] - Training Epoch: 1/10, step 421/574 completed (loss: 0.3496010899543762, acc: 0.8709677457809448)
[2024-11-08 03:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:19][root][INFO] - Training Epoch: 1/10, step 422/574 completed (loss: 0.37816646695137024, acc: 0.8387096524238586)
[2024-11-08 03:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:19][root][INFO] - Training Epoch: 1/10, step 423/574 completed (loss: 0.3995242714881897, acc: 0.8461538553237915)
[2024-11-08 03:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:19][root][INFO] - Training Epoch: 1/10, step 424/574 completed (loss: 0.25137820839881897, acc: 0.9523809552192688)
[2024-11-08 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:20][root][INFO] - Training Epoch: 1/10, step 425/574 completed (loss: 0.41594088077545166, acc: 0.8421052694320679)
[2024-11-08 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:20][root][INFO] - Training Epoch: 1/10, step 426/574 completed (loss: 0.28703850507736206, acc: 0.8947368264198303)
[2024-11-08 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:21][root][INFO] - Training Epoch: 1/10, step 427/574 completed (loss: 0.3364565074443817, acc: 0.8636363744735718)
[2024-11-08 03:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:21][root][INFO] - Training Epoch: 1/10, step 428/574 completed (loss: 0.3405711352825165, acc: 0.8571428656578064)
[2024-11-08 03:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:05][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4359, device='cuda:0') eval_epoch_loss=tensor(0.3618, device='cuda:0') eval_epoch_acc=tensor(0.8613, device='cuda:0')
[2024-11-08 03:31:05][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:31:05][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:31:08][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_1_step_429_loss_0.3617952764034271/model.pt
[2024-11-08 03:31:08][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:31:08][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.3617952764034271
[2024-11-08 03:31:08][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8613137602806091
[2024-11-08 03:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:09][root][INFO] - Training Epoch: 1/10, step 429/574 completed (loss: 0.49649131298065186, acc: 0.875)
[2024-11-08 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:09][root][INFO] - Training Epoch: 1/10, step 430/574 completed (loss: 0.3129175007343292, acc: 0.9032257795333862)
[2024-11-08 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:09][root][INFO] - Training Epoch: 1/10, step 431/574 completed (loss: 0.29698020219802856, acc: 0.9354838728904724)
[2024-11-08 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:10][root][INFO] - Training Epoch: 1/10, step 432/574 completed (loss: 0.29150161147117615, acc: 0.8846153616905212)
[2024-11-08 03:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:10][root][INFO] - Training Epoch: 1/10, step 433/574 completed (loss: 0.26435866951942444, acc: 0.8571428656578064)
[2024-11-08 03:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:11][root][INFO] - Training Epoch: 1/10, step 434/574 completed (loss: 0.43784838914871216, acc: 0.7894737124443054)
[2024-11-08 03:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:11][root][INFO] - Training Epoch: 1/10, step 435/574 completed (loss: 0.4588293731212616, acc: 0.7894737124443054)
[2024-11-08 03:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:11][root][INFO] - Training Epoch: 1/10, step 436/574 completed (loss: 0.29888513684272766, acc: 0.9090909361839294)
[2024-11-08 03:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:12][root][INFO] - Training Epoch: 1/10, step 437/574 completed (loss: 0.33530864119529724, acc: 0.9047619104385376)
[2024-11-08 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:12][root][INFO] - Training Epoch: 1/10, step 438/574 completed (loss: 0.3171277344226837, acc: 0.8636363744735718)
[2024-11-08 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:12][root][INFO] - Training Epoch: 1/10, step 439/574 completed (loss: 0.6454107165336609, acc: 0.7857142686843872)
[2024-11-08 03:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:13][root][INFO] - Training Epoch: 1/10, step 440/574 completed (loss: 0.29145440459251404, acc: 0.8095238208770752)
[2024-11-08 03:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:14][root][INFO] - Training Epoch: 1/10, step 441/574 completed (loss: 0.3796851634979248, acc: 0.7894737124443054)
[2024-11-08 03:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:15][root][INFO] - Training Epoch: 1/10, step 442/574 completed (loss: 0.299190878868103, acc: 0.8947368264198303)
[2024-11-08 03:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:16][root][INFO] - Training Epoch: 1/10, step 443/574 completed (loss: 0.2814444601535797, acc: 0.9545454382896423)
[2024-11-08 03:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:16][root][INFO] - Training Epoch: 1/10, step 444/574 completed (loss: 0.5014272928237915, acc: 0.8095238208770752)
[2024-11-08 03:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:17][root][INFO] - Training Epoch: 1/10, step 445/574 completed (loss: 0.20194776356220245, acc: 0.9583333134651184)
[2024-11-08 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:17][root][INFO] - Training Epoch: 1/10, step 446/574 completed (loss: 0.5203586220741272, acc: 0.8387096524238586)
[2024-11-08 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:18][root][INFO] - Training Epoch: 1/10, step 447/574 completed (loss: 0.5079588294029236, acc: 0.9032257795333862)
[2024-11-08 03:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:18][root][INFO] - Training Epoch: 1/10, step 448/574 completed (loss: 0.3348025977611542, acc: 0.8709677457809448)
[2024-11-08 03:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:18][root][INFO] - Training Epoch: 1/10, step 449/574 completed (loss: 0.3841666579246521, acc: 0.8999999761581421)
[2024-11-08 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:19][root][INFO] - Training Epoch: 1/10, step 450/574 completed (loss: 0.21191927790641785, acc: 0.8999999761581421)
[2024-11-08 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:19][root][INFO] - Training Epoch: 1/10, step 451/574 completed (loss: 0.3835620582103729, acc: 0.8421052694320679)
[2024-11-08 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:20][root][INFO] - Training Epoch: 1/10, step 452/574 completed (loss: 0.33619412779808044, acc: 0.8636363744735718)
[2024-11-08 03:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:20][root][INFO] - Training Epoch: 1/10, step 453/574 completed (loss: 0.4199690818786621, acc: 0.8999999761581421)
[2024-11-08 03:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:20][root][INFO] - Training Epoch: 1/10, step 454/574 completed (loss: 0.20259171724319458, acc: 0.9545454382896423)
[2024-11-08 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:21][root][INFO] - Training Epoch: 1/10, step 455/574 completed (loss: 0.3436690866947174, acc: 0.8709677457809448)
[2024-11-08 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:21][root][INFO] - Training Epoch: 1/10, step 456/574 completed (loss: 0.29097476601600647, acc: 0.8500000238418579)
[2024-11-08 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:22][root][INFO] - Training Epoch: 1/10, step 457/574 completed (loss: 0.3473087251186371, acc: 0.800000011920929)
[2024-11-08 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:22][root][INFO] - Training Epoch: 1/10, step 458/574 completed (loss: 0.34469640254974365, acc: 0.8421052694320679)
[2024-11-08 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:23][root][INFO] - Training Epoch: 1/10, step 459/574 completed (loss: 0.3679925799369812, acc: 0.8636363744735718)
[2024-11-08 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:23][root][INFO] - Training Epoch: 1/10, step 460/574 completed (loss: 0.4692113399505615, acc: 0.800000011920929)
[2024-11-08 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:23][root][INFO] - Training Epoch: 1/10, step 461/574 completed (loss: 0.19906193017959595, acc: 0.9545454382896423)
[2024-11-08 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:24][root][INFO] - Training Epoch: 1/10, step 462/574 completed (loss: 0.27832457423210144, acc: 0.939393937587738)
[2024-11-08 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:24][root][INFO] - Training Epoch: 1/10, step 463/574 completed (loss: 0.6622869968414307, acc: 0.8148148059844971)
[2024-11-08 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:24][root][INFO] - Training Epoch: 1/10, step 464/574 completed (loss: 0.3049524128437042, acc: 0.9090909361839294)
[2024-11-08 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:25][root][INFO] - Training Epoch: 1/10, step 465/574 completed (loss: 0.32024872303009033, acc: 0.8500000238418579)
[2024-11-08 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:25][root][INFO] - Training Epoch: 1/10, step 466/574 completed (loss: 0.2572266161441803, acc: 0.949999988079071)
[2024-11-08 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:26][root][INFO] - Training Epoch: 1/10, step 467/574 completed (loss: 0.413754403591156, acc: 0.8421052694320679)
[2024-11-08 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:26][root][INFO] - Training Epoch: 1/10, step 468/574 completed (loss: 0.29843437671661377, acc: 0.8500000238418579)
[2024-11-08 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:26][root][INFO] - Training Epoch: 1/10, step 469/574 completed (loss: 0.35608235001564026, acc: 0.8571428656578064)
[2024-11-08 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:27][root][INFO] - Training Epoch: 1/10, step 470/574 completed (loss: 0.48061585426330566, acc: 0.8333333134651184)
[2024-11-08 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:27][root][INFO] - Training Epoch: 1/10, step 471/574 completed (loss: 0.35012808442115784, acc: 0.8666666746139526)
[2024-11-08 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:28][root][INFO] - Training Epoch: 1/10, step 472/574 completed (loss: 0.5999718308448792, acc: 0.7916666865348816)
[2024-11-08 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:28][root][INFO] - Training Epoch: 1/10, step 473/574 completed (loss: 0.48890796303749084, acc: 0.761904776096344)
[2024-11-08 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:29][root][INFO] - Training Epoch: 1/10, step 474/574 completed (loss: 0.30638694763183594, acc: 0.8947368264198303)
[2024-11-08 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:29][root][INFO] - Training Epoch: 1/10, step 475/574 completed (loss: 0.41905099153518677, acc: 0.8500000238418579)
[2024-11-08 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:29][root][INFO] - Training Epoch: 1/10, step 476/574 completed (loss: 0.4944345951080322, acc: 0.9047619104385376)
[2024-11-08 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:30][root][INFO] - Training Epoch: 1/10, step 477/574 completed (loss: 0.4336496591567993, acc: 0.8181818127632141)
[2024-11-08 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:30][root][INFO] - Training Epoch: 1/10, step 478/574 completed (loss: 0.2544698417186737, acc: 0.9285714030265808)
[2024-11-08 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:31][root][INFO] - Training Epoch: 1/10, step 479/574 completed (loss: 0.6462701559066772, acc: 0.8709677457809448)
[2024-11-08 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:31][root][INFO] - Training Epoch: 1/10, step 480/574 completed (loss: 0.3425586521625519, acc: 0.9032257795333862)
[2024-11-08 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:31][root][INFO] - Training Epoch: 1/10, step 481/574 completed (loss: 0.29925337433815, acc: 0.9047619104385376)
[2024-11-08 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:32][root][INFO] - Training Epoch: 1/10, step 482/574 completed (loss: 0.5938295722007751, acc: 0.8421052694320679)
[2024-11-08 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:32][root][INFO] - Training Epoch: 1/10, step 483/574 completed (loss: 0.43749940395355225, acc: 0.8636363744735718)
[2024-11-08 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:33][root][INFO] - Training Epoch: 1/10, step 484/574 completed (loss: 0.3761712908744812, acc: 0.9090909361839294)
[2024-11-08 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:33][root][INFO] - Training Epoch: 1/10, step 485/574 completed (loss: 0.25211301445961, acc: 0.9285714030265808)
[2024-11-08 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:33][root][INFO] - Training Epoch: 1/10, step 486/574 completed (loss: 0.39126893877983093, acc: 0.8518518805503845)
[2024-11-08 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:34][root][INFO] - Training Epoch: 1/10, step 487/574 completed (loss: 0.42006900906562805, acc: 0.8857142925262451)
[2024-11-08 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:34][root][INFO] - Training Epoch: 1/10, step 488/574 completed (loss: 0.6677159070968628, acc: 0.807692289352417)
[2024-11-08 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:35][root][INFO] - Training Epoch: 1/10, step 489/574 completed (loss: 0.17366598546504974, acc: 0.9047619104385376)
[2024-11-08 03:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:35][root][INFO] - Training Epoch: 1/10, step 490/574 completed (loss: 0.5306031703948975, acc: 0.7894737124443054)
[2024-11-08 03:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:35][root][INFO] - Training Epoch: 1/10, step 491/574 completed (loss: 0.557012677192688, acc: 0.7368420958518982)
[2024-11-08 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:36][root][INFO] - Training Epoch: 1/10, step 492/574 completed (loss: 0.36772650480270386, acc: 0.8636363744735718)
[2024-11-08 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:36][root][INFO] - Training Epoch: 1/10, step 493/574 completed (loss: 0.3599737584590912, acc: 0.8571428656578064)
[2024-11-08 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:37][root][INFO] - Training Epoch: 1/10, step 494/574 completed (loss: 0.39976730942726135, acc: 0.875)
[2024-11-08 03:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:37][root][INFO] - Training Epoch: 1/10, step 495/574 completed (loss: 0.3318041265010834, acc: 0.8214285969734192)
[2024-11-08 03:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:37][root][INFO] - Training Epoch: 1/10, step 496/574 completed (loss: 0.3391004800796509, acc: 0.800000011920929)
[2024-11-08 03:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:38][root][INFO] - Training Epoch: 1/10, step 497/574 completed (loss: 0.2219633162021637, acc: 0.949999988079071)
[2024-11-08 03:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:38][root][INFO] - Training Epoch: 1/10, step 498/574 completed (loss: 0.37677502632141113, acc: 0.8947368264198303)
[2024-11-08 03:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:39][root][INFO] - Training Epoch: 1/10, step 499/574 completed (loss: 0.2857619524002075, acc: 0.8636363744735718)
[2024-11-08 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:39][root][INFO] - Training Epoch: 1/10, step 500/574 completed (loss: 0.5632215738296509, acc: 0.75)
[2024-11-08 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:39][root][INFO] - Training Epoch: 1/10, step 501/574 completed (loss: 0.19150669872760773, acc: 0.9583333134651184)
[2024-11-08 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:40][root][INFO] - Training Epoch: 1/10, step 502/574 completed (loss: 0.43819302320480347, acc: 0.8709677457809448)
[2024-11-08 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:40][root][INFO] - Training Epoch: 1/10, step 503/574 completed (loss: 0.2956578731536865, acc: 0.8387096524238586)
[2024-11-08 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:41][root][INFO] - Training Epoch: 1/10, step 504/574 completed (loss: 0.3047100007534027, acc: 0.8695651888847351)
[2024-11-08 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:41][root][INFO] - Training Epoch: 1/10, step 505/574 completed (loss: 0.4595206081867218, acc: 0.8095238208770752)
[2024-11-08 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:41][root][INFO] - Training Epoch: 1/10, step 506/574 completed (loss: 0.49955835938453674, acc: 0.7894737124443054)
[2024-11-08 03:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:42][root][INFO] - Training Epoch: 1/10, step 507/574 completed (loss: 0.16731102764606476, acc: 0.8999999761581421)
[2024-11-08 03:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:43][root][INFO] - Training Epoch: 1/10, step 508/574 completed (loss: 0.48524266481399536, acc: 0.800000011920929)
[2024-11-08 03:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:43][root][INFO] - Training Epoch: 1/10, step 509/574 completed (loss: 0.3583448529243469, acc: 0.9090909361839294)
[2024-11-08 03:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:44][root][INFO] - Training Epoch: 1/10, step 510/574 completed (loss: 0.24069350957870483, acc: 0.9090909361839294)
[2024-11-08 03:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:44][root][INFO] - Training Epoch: 1/10, step 511/574 completed (loss: 0.2945607900619507, acc: 0.8999999761581421)
[2024-11-08 03:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:48][root][INFO] - Training Epoch: 1/10, step 512/574 completed (loss: 0.3661482632160187, acc: 0.8928571343421936)
[2024-11-08 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:49][root][INFO] - Training Epoch: 1/10, step 513/574 completed (loss: 0.22104480862617493, acc: 0.8571428656578064)
[2024-11-08 03:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:49][root][INFO] - Training Epoch: 1/10, step 514/574 completed (loss: 0.5092460513114929, acc: 0.7368420958518982)
[2024-11-08 03:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:50][root][INFO] - Training Epoch: 1/10, step 515/574 completed (loss: 0.4587624669075012, acc: 0.800000011920929)
[2024-11-08 03:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:51][root][INFO] - Training Epoch: 1/10, step 516/574 completed (loss: 0.3602678179740906, acc: 0.8571428656578064)
[2024-11-08 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:51][root][INFO] - Training Epoch: 1/10, step 517/574 completed (loss: 0.33611002564430237, acc: 0.8636363744735718)
[2024-11-08 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:52][root][INFO] - Training Epoch: 1/10, step 518/574 completed (loss: 0.36656516790390015, acc: 0.8571428656578064)
[2024-11-08 03:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:52][root][INFO] - Training Epoch: 1/10, step 519/574 completed (loss: 0.2635034918785095, acc: 0.9259259104728699)
[2024-11-08 03:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:52][root][INFO] - Training Epoch: 1/10, step 520/574 completed (loss: 0.24011532962322235, acc: 0.9142857193946838)
[2024-11-08 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:54][root][INFO] - Training Epoch: 1/10, step 521/574 completed (loss: 0.2194117307662964, acc: 0.8695651888847351)
[2024-11-08 03:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:54][root][INFO] - Training Epoch: 1/10, step 522/574 completed (loss: 0.32883942127227783, acc: 0.9047619104385376)
[2024-11-08 03:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:55][root][INFO] - Training Epoch: 1/10, step 523/574 completed (loss: 0.6781501173973083, acc: 0.7368420958518982)
[2024-11-08 03:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:56][root][INFO] - Training Epoch: 1/10, step 524/574 completed (loss: 0.29664552211761475, acc: 0.8636363744735718)
[2024-11-08 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:56][root][INFO] - Training Epoch: 1/10, step 525/574 completed (loss: 0.6525609493255615, acc: 0.800000011920929)
[2024-11-08 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:57][root][INFO] - Training Epoch: 1/10, step 526/574 completed (loss: 0.45729973912239075, acc: 0.8181818127632141)
[2024-11-08 03:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:57][root][INFO] - Training Epoch: 1/10, step 527/574 completed (loss: 0.24600878357887268, acc: 0.9090909361839294)
[2024-11-08 03:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:57][root][INFO] - Training Epoch: 1/10, step 528/574 completed (loss: 0.37783804535865784, acc: 0.8571428656578064)
[2024-11-08 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:58][root][INFO] - Training Epoch: 1/10, step 529/574 completed (loss: 0.2636742889881134, acc: 0.9047619104385376)
[2024-11-08 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:58][root][INFO] - Training Epoch: 1/10, step 530/574 completed (loss: 0.4394923746585846, acc: 0.7894737124443054)
[2024-11-08 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:59][root][INFO] - Training Epoch: 1/10, step 531/574 completed (loss: 0.3133392333984375, acc: 0.8500000238418579)
[2024-11-08 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:59][root][INFO] - Training Epoch: 1/10, step 532/574 completed (loss: 0.47760772705078125, acc: 0.8571428656578064)
[2024-11-08 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:31:59][root][INFO] - Training Epoch: 1/10, step 533/574 completed (loss: 0.381063848733902, acc: 0.8181818127632141)
[2024-11-08 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:00][root][INFO] - Training Epoch: 1/10, step 534/574 completed (loss: 0.2441488802433014, acc: 0.9642857313156128)
[2024-11-08 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:00][root][INFO] - Training Epoch: 1/10, step 535/574 completed (loss: 0.41150057315826416, acc: 0.8148148059844971)
[2024-11-08 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:01][root][INFO] - Training Epoch: 1/10, step 536/574 completed (loss: 0.6679285168647766, acc: 0.800000011920929)
[2024-11-08 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:01][root][INFO] - Training Epoch: 1/10, step 537/574 completed (loss: 0.538040041923523, acc: 0.8095238208770752)
[2024-11-08 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:02][root][INFO] - Training Epoch: 1/10, step 538/574 completed (loss: 0.2925555408000946, acc: 0.8947368264198303)
[2024-11-08 03:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:02][root][INFO] - Training Epoch: 1/10, step 539/574 completed (loss: 0.24621173739433289, acc: 0.8999999761581421)
[2024-11-08 03:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:03][root][INFO] - Training Epoch: 1/10, step 540/574 completed (loss: 0.3132888376712799, acc: 0.8571428656578064)
[2024-11-08 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:03][root][INFO] - Training Epoch: 1/10, step 541/574 completed (loss: 0.20599807798862457, acc: 0.9047619104385376)
[2024-11-08 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:03][root][INFO] - Training Epoch: 1/10, step 542/574 completed (loss: 0.17825879156589508, acc: 0.939393937587738)
[2024-11-08 03:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:04][root][INFO] - Training Epoch: 1/10, step 543/574 completed (loss: 0.25865304470062256, acc: 0.8888888955116272)
[2024-11-08 03:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:04][root][INFO] - Training Epoch: 1/10, step 544/574 completed (loss: 0.39731863141059875, acc: 0.8787878751754761)
[2024-11-08 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:05][root][INFO] - Training Epoch: 1/10, step 545/574 completed (loss: 0.20686256885528564, acc: 0.8500000238418579)
[2024-11-08 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:05][root][INFO] - Training Epoch: 1/10, step 546/574 completed (loss: 0.16713044047355652, acc: 0.949999988079071)
[2024-11-08 03:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:06][root][INFO] - Training Epoch: 1/10, step 547/574 completed (loss: 0.4232262670993805, acc: 0.9047619104385376)
[2024-11-08 03:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:06][root][INFO] - Training Epoch: 1/10, step 548/574 completed (loss: 0.23927688598632812, acc: 0.9523809552192688)
[2024-11-08 03:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:07][root][INFO] - Training Epoch: 1/10, step 549/574 completed (loss: 0.1632416546344757, acc: 0.9545454382896423)
[2024-11-08 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:07][root][INFO] - Training Epoch: 1/10, step 550/574 completed (loss: 0.1681344211101532, acc: 0.931034505367279)
[2024-11-08 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:07][root][INFO] - Training Epoch: 1/10, step 551/574 completed (loss: 0.21829527616500854, acc: 0.9032257795333862)
[2024-11-08 03:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:08][root][INFO] - Training Epoch: 1/10, step 552/574 completed (loss: 0.380205899477005, acc: 0.8518518805503845)
[2024-11-08 03:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:08][root][INFO] - Training Epoch: 1/10, step 553/574 completed (loss: 0.40067219734191895, acc: 0.8571428656578064)
[2024-11-08 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:09][root][INFO] - Training Epoch: 1/10, step 554/574 completed (loss: 0.3509528636932373, acc: 0.8947368264198303)
[2024-11-08 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:09][root][INFO] - Training Epoch: 1/10, step 555/574 completed (loss: 0.4226381182670593, acc: 0.75)
[2024-11-08 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:10][root][INFO] - Training Epoch: 1/10, step 556/574 completed (loss: 0.42143768072128296, acc: 0.8571428656578064)
[2024-11-08 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:10][root][INFO] - Training Epoch: 1/10, step 557/574 completed (loss: 0.41252878308296204, acc: 0.8181818127632141)
[2024-11-08 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:11][root][INFO] - Training Epoch: 1/10, step 558/574 completed (loss: 0.24246545135974884, acc: 0.8571428656578064)
[2024-11-08 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:11][root][INFO] - Training Epoch: 1/10, step 559/574 completed (loss: 0.23163282871246338, acc: 0.9259259104728699)
[2024-11-08 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:11][root][INFO] - Training Epoch: 1/10, step 560/574 completed (loss: 0.34207314252853394, acc: 0.8787878751754761)
[2024-11-08 03:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:12][root][INFO] - Training Epoch: 1/10, step 561/574 completed (loss: 0.6788299679756165, acc: 0.761904776096344)
[2024-11-08 03:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:12][root][INFO] - Training Epoch: 1/10, step 562/574 completed (loss: 0.4789424538612366, acc: 0.9047619104385376)
[2024-11-08 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:13][root][INFO] - Training Epoch: 1/10, step 563/574 completed (loss: 0.4515869915485382, acc: 0.8333333134651184)
[2024-11-08 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:13][root][INFO] - Training Epoch: 1/10, step 564/574 completed (loss: 0.22273331880569458, acc: 0.9090909361839294)
[2024-11-08 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:14][root][INFO] - Training Epoch: 1/10, step 565/574 completed (loss: 0.6183998584747314, acc: 0.800000011920929)
[2024-11-08 03:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:14][root][INFO] - Training Epoch: 1/10, step 566/574 completed (loss: 0.22161132097244263, acc: 0.9090909361839294)
[2024-11-08 03:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:15][root][INFO] - Training Epoch: 1/10, step 567/574 completed (loss: 0.33113518357276917, acc: 0.9259259104728699)
[2024-11-08 03:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:15][root][INFO] - Training Epoch: 1/10, step 568/574 completed (loss: 0.2680847644805908, acc: 0.8709677457809448)
[2024-11-08 03:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:16][root][INFO] - Training Epoch: 1/10, step 569/574 completed (loss: 0.6548616886138916, acc: 0.699999988079071)
[2024-11-08 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:16][root][INFO] - Training Epoch: 1/10, step 570/574 completed (loss: 0.2978167235851288, acc: 0.8500000238418579)
[2024-11-08 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:17][root][INFO] - Training Epoch: 1/10, step 571/574 completed (loss: 0.420510470867157, acc: 0.7894737124443054)
[2024-11-08 03:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:00][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3989, device='cuda:0') eval_epoch_loss=tensor(0.3357, device='cuda:0') eval_epoch_acc=tensor(0.8731, device='cuda:0')
[2024-11-08 03:33:00][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:33:00][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:33:04][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_1_step_572_loss_0.33565592765808105/model.pt
[2024-11-08 03:33:04][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:33:04][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.33565592765808105
[2024-11-08 03:33:04][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8730716705322266
[2024-11-08 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:04][root][INFO] - Training Epoch: 1/10, step 572/574 completed (loss: 0.3847980499267578, acc: 0.8636363744735718)
[2024-11-08 03:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:05][root][INFO] - Training Epoch: 1/10, step 573/574 completed (loss: 0.45483213663101196, acc: 0.8500000238418579)
[2024-11-08 03:33:06][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=5.1746, train_epoch_loss=1.6438, epoch time 495.64031916484237s
[2024-11-08 03:33:06][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 03:33:06][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 14 GB
[2024-11-08 03:33:06][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 03:33:06][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2024-11-08 03:33:06][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 03:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:07][root][INFO] - Training Epoch: 2/10, step 0/574 completed (loss: 0.3622846007347107, acc: 0.8214285969734192)
[2024-11-08 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:07][root][INFO] - Training Epoch: 2/10, step 1/574 completed (loss: 0.37454211711883545, acc: 0.8518518805503845)
[2024-11-08 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:08][root][INFO] - Training Epoch: 2/10, step 2/574 completed (loss: 0.413575142621994, acc: 0.8571428656578064)
[2024-11-08 03:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:08][root][INFO] - Training Epoch: 2/10, step 3/574 completed (loss: 0.22209155559539795, acc: 0.9047619104385376)
[2024-11-08 03:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:08][root][INFO] - Training Epoch: 2/10, step 4/574 completed (loss: 0.336211621761322, acc: 0.8095238208770752)
[2024-11-08 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:09][root][INFO] - Training Epoch: 2/10, step 5/574 completed (loss: 0.42918652296066284, acc: 0.8421052694320679)
[2024-11-08 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:09][root][INFO] - Training Epoch: 2/10, step 6/574 completed (loss: 0.23897427320480347, acc: 0.9090909361839294)
[2024-11-08 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:10][root][INFO] - Training Epoch: 2/10, step 7/574 completed (loss: 0.3215591311454773, acc: 0.8571428656578064)
[2024-11-08 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:10][root][INFO] - Training Epoch: 2/10, step 8/574 completed (loss: 0.25973746180534363, acc: 0.9166666865348816)
[2024-11-08 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:11][root][INFO] - Training Epoch: 2/10, step 9/574 completed (loss: 0.2840113341808319, acc: 0.9032257795333862)
[2024-11-08 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:11][root][INFO] - Training Epoch: 2/10, step 10/574 completed (loss: 0.28370657563209534, acc: 0.8709677457809448)
[2024-11-08 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:11][root][INFO] - Training Epoch: 2/10, step 11/574 completed (loss: 0.41679057478904724, acc: 0.8799999952316284)
[2024-11-08 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:12][root][INFO] - Training Epoch: 2/10, step 12/574 completed (loss: 0.21829524636268616, acc: 0.8999999761581421)
[2024-11-08 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:12][root][INFO] - Training Epoch: 2/10, step 13/574 completed (loss: 0.24618831276893616, acc: 0.8947368264198303)
[2024-11-08 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:13][root][INFO] - Training Epoch: 2/10, step 14/574 completed (loss: 0.39558517932891846, acc: 0.8260869383811951)
[2024-11-08 03:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:13][root][INFO] - Training Epoch: 2/10, step 15/574 completed (loss: 0.12469091266393661, acc: 0.9545454382896423)
[2024-11-08 03:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:13][root][INFO] - Training Epoch: 2/10, step 16/574 completed (loss: 0.20719854533672333, acc: 0.9642857313156128)
[2024-11-08 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:14][root][INFO] - Training Epoch: 2/10, step 17/574 completed (loss: 0.23814189434051514, acc: 0.8928571343421936)
[2024-11-08 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:14][root][INFO] - Training Epoch: 2/10, step 18/574 completed (loss: 0.3807872533798218, acc: 0.8636363744735718)
[2024-11-08 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:15][root][INFO] - Training Epoch: 2/10, step 19/574 completed (loss: 0.48571330308914185, acc: 0.8947368264198303)
[2024-11-08 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:15][root][INFO] - Training Epoch: 2/10, step 20/574 completed (loss: 0.3769545555114746, acc: 0.8947368264198303)
[2024-11-08 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:15][root][INFO] - Training Epoch: 2/10, step 21/574 completed (loss: 0.4105989336967468, acc: 0.9090909361839294)
[2024-11-08 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:16][root][INFO] - Training Epoch: 2/10, step 22/574 completed (loss: 0.3675278127193451, acc: 0.8571428656578064)
[2024-11-08 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:16][root][INFO] - Training Epoch: 2/10, step 23/574 completed (loss: 0.14162585139274597, acc: 0.9047619104385376)
[2024-11-08 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:17][root][INFO] - Training Epoch: 2/10, step 24/574 completed (loss: 1.2808769941329956, acc: 0.800000011920929)
[2024-11-08 03:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:17][root][INFO] - Training Epoch: 2/10, step 25/574 completed (loss: 0.4945530295372009, acc: 0.8571428656578064)
[2024-11-08 03:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:17][root][INFO] - Training Epoch: 2/10, step 26/574 completed (loss: 0.31754252314567566, acc: 0.8571428656578064)
[2024-11-08 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:19][root][INFO] - Training Epoch: 2/10, step 27/574 completed (loss: 0.35067418217658997, acc: 0.8421052694320679)
[2024-11-08 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:20][root][INFO] - Training Epoch: 2/10, step 28/574 completed (loss: 0.38099557161331177, acc: 0.8636363744735718)
[2024-11-08 03:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:20][root][INFO] - Training Epoch: 2/10, step 29/574 completed (loss: 0.7886778712272644, acc: 0.75)
[2024-11-08 03:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:20][root][INFO] - Training Epoch: 2/10, step 30/574 completed (loss: 0.285519003868103, acc: 0.9090909361839294)
[2024-11-08 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:21][root][INFO] - Training Epoch: 2/10, step 31/574 completed (loss: 0.3515109121799469, acc: 0.8965517282485962)
[2024-11-08 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:21][root][INFO] - Training Epoch: 2/10, step 32/574 completed (loss: 0.6157065629959106, acc: 0.7878788113594055)
[2024-11-08 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:22][root][INFO] - Training Epoch: 2/10, step 33/574 completed (loss: 0.30751675367355347, acc: 0.95652174949646)
[2024-11-08 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:22][root][INFO] - Training Epoch: 2/10, step 34/574 completed (loss: 0.19135266542434692, acc: 0.9523809552192688)
[2024-11-08 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:23][root][INFO] - Training Epoch: 2/10, step 35/574 completed (loss: 0.49257439374923706, acc: 0.7894737124443054)
[2024-11-08 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:23][root][INFO] - Training Epoch: 2/10, step 36/574 completed (loss: 0.380515456199646, acc: 0.800000011920929)
[2024-11-08 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:23][root][INFO] - Training Epoch: 2/10, step 37/574 completed (loss: 0.28395912051200867, acc: 0.9047619104385376)
[2024-11-08 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:24][root][INFO] - Training Epoch: 2/10, step 38/574 completed (loss: 0.2095383107662201, acc: 0.9545454382896423)
[2024-11-08 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:24][root][INFO] - Training Epoch: 2/10, step 39/574 completed (loss: 0.4673886299133301, acc: 0.7857142686843872)
[2024-11-08 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:25][root][INFO] - Training Epoch: 2/10, step 40/574 completed (loss: 0.3156050145626068, acc: 0.8571428656578064)
[2024-11-08 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:25][root][INFO] - Training Epoch: 2/10, step 41/574 completed (loss: 0.4128175675868988, acc: 0.800000011920929)
[2024-11-08 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:26][root][INFO] - Training Epoch: 2/10, step 42/574 completed (loss: 0.49897098541259766, acc: 0.8500000238418579)
[2024-11-08 03:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:26][root][INFO] - Training Epoch: 2/10, step 43/574 completed (loss: 0.5668764114379883, acc: 0.8421052694320679)
[2024-11-08 03:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:27][root][INFO] - Training Epoch: 2/10, step 44/574 completed (loss: 0.2952587604522705, acc: 0.8636363744735718)
[2024-11-08 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:27][root][INFO] - Training Epoch: 2/10, step 45/574 completed (loss: 0.47614210844039917, acc: 0.8571428656578064)
[2024-11-08 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:28][root][INFO] - Training Epoch: 2/10, step 46/574 completed (loss: 0.31073740124702454, acc: 0.8333333134651184)
[2024-11-08 03:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:28][root][INFO] - Training Epoch: 2/10, step 47/574 completed (loss: 0.31351521611213684, acc: 0.9032257795333862)
[2024-11-08 03:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:28][root][INFO] - Training Epoch: 2/10, step 48/574 completed (loss: 0.27513575553894043, acc: 0.8571428656578064)
[2024-11-08 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:29][root][INFO] - Training Epoch: 2/10, step 49/574 completed (loss: 0.2920166254043579, acc: 0.9200000166893005)
[2024-11-08 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:29][root][INFO] - Training Epoch: 2/10, step 50/574 completed (loss: 0.5775244832038879, acc: 0.8095238208770752)
[2024-11-08 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:30][root][INFO] - Training Epoch: 2/10, step 51/574 completed (loss: 0.4605218470096588, acc: 0.8421052694320679)
[2024-11-08 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:30][root][INFO] - Training Epoch: 2/10, step 52/574 completed (loss: 0.3334727883338928, acc: 0.8500000238418579)
[2024-11-08 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:31][root][INFO] - Training Epoch: 2/10, step 53/574 completed (loss: 0.48235753178596497, acc: 0.8571428656578064)
[2024-11-08 03:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:31][root][INFO] - Training Epoch: 2/10, step 54/574 completed (loss: 0.23532123863697052, acc: 0.9090909361839294)
[2024-11-08 03:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:32][root][INFO] - Training Epoch: 2/10, step 55/574 completed (loss: 0.10580246895551682, acc: 0.9642857313156128)
[2024-11-08 03:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:36][root][INFO] - Training Epoch: 2/10, step 56/574 completed (loss: 0.5175586938858032, acc: 0.7142857313156128)
[2024-11-08 03:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:37][root][INFO] - Training Epoch: 2/10, step 57/574 completed (loss: 0.2457401007413864, acc: 0.9047619104385376)
[2024-11-08 03:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:38][root][INFO] - Training Epoch: 2/10, step 58/574 completed (loss: 0.4079723656177521, acc: 0.8421052694320679)
[2024-11-08 03:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:39][root][INFO] - Training Epoch: 2/10, step 59/574 completed (loss: 0.5482425689697266, acc: 0.7727272510528564)
[2024-11-08 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:40][root][INFO] - Training Epoch: 2/10, step 60/574 completed (loss: 0.3755188584327698, acc: 0.8421052694320679)
[2024-11-08 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:41][root][INFO] - Training Epoch: 2/10, step 61/574 completed (loss: 0.2984440326690674, acc: 0.8571428656578064)
[2024-11-08 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:41][root][INFO] - Training Epoch: 2/10, step 62/574 completed (loss: 0.2948204278945923, acc: 0.9166666865348816)
[2024-11-08 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:41][root][INFO] - Training Epoch: 2/10, step 63/574 completed (loss: 0.5000414252281189, acc: 0.8214285969734192)
[2024-11-08 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:42][root][INFO] - Training Epoch: 2/10, step 64/574 completed (loss: 0.3284376561641693, acc: 0.8095238208770752)
[2024-11-08 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:42][root][INFO] - Training Epoch: 2/10, step 65/574 completed (loss: 0.41488343477249146, acc: 0.800000011920929)
[2024-11-08 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:43][root][INFO] - Training Epoch: 2/10, step 66/574 completed (loss: 0.2946808338165283, acc: 0.8947368264198303)
[2024-11-08 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:43][root][INFO] - Training Epoch: 2/10, step 67/574 completed (loss: 0.42912906408309937, acc: 0.800000011920929)
[2024-11-08 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:43][root][INFO] - Training Epoch: 2/10, step 68/574 completed (loss: 0.16947908699512482, acc: 0.9090909361839294)
[2024-11-08 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:44][root][INFO] - Training Epoch: 2/10, step 69/574 completed (loss: 0.16916117072105408, acc: 0.939393937587738)
[2024-11-08 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:44][root][INFO] - Training Epoch: 2/10, step 70/574 completed (loss: 0.335718035697937, acc: 0.8888888955116272)
[2024-11-08 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:45][root][INFO] - Training Epoch: 2/10, step 71/574 completed (loss: 0.3347814679145813, acc: 0.8999999761581421)
[2024-11-08 03:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:45][root][INFO] - Training Epoch: 2/10, step 72/574 completed (loss: 0.23609179258346558, acc: 0.9047619104385376)
[2024-11-08 03:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:45][root][INFO] - Training Epoch: 2/10, step 73/574 completed (loss: 0.37357863783836365, acc: 0.8947368264198303)
[2024-11-08 03:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:46][root][INFO] - Training Epoch: 2/10, step 74/574 completed (loss: 0.3709305226802826, acc: 0.8500000238418579)
[2024-11-08 03:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:46][root][INFO] - Training Epoch: 2/10, step 75/574 completed (loss: 0.4413447082042694, acc: 0.8571428656578064)
[2024-11-08 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:47][root][INFO] - Training Epoch: 2/10, step 76/574 completed (loss: 0.44031772017478943, acc: 0.8181818127632141)
[2024-11-08 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:47][root][INFO] - Training Epoch: 2/10, step 77/574 completed (loss: 0.2527359426021576, acc: 0.9285714030265808)
[2024-11-08 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:48][root][INFO] - Training Epoch: 2/10, step 78/574 completed (loss: 0.16195020079612732, acc: 0.9629629850387573)
[2024-11-08 03:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:48][root][INFO] - Training Epoch: 2/10, step 79/574 completed (loss: 0.3600394129753113, acc: 0.8571428656578064)
[2024-11-08 03:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:48][root][INFO] - Training Epoch: 2/10, step 80/574 completed (loss: 0.42393824458122253, acc: 0.807692289352417)
[2024-11-08 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:49][root][INFO] - Training Epoch: 2/10, step 81/574 completed (loss: 0.19288742542266846, acc: 0.9523809552192688)
[2024-11-08 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:49][root][INFO] - Training Epoch: 2/10, step 82/574 completed (loss: 0.38180798292160034, acc: 0.8421052694320679)
[2024-11-08 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:50][root][INFO] - Training Epoch: 2/10, step 83/574 completed (loss: 0.2948801815509796, acc: 0.8947368264198303)
[2024-11-08 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:50][root][INFO] - Training Epoch: 2/10, step 84/574 completed (loss: 0.17448486387729645, acc: 1.0)
[2024-11-08 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:50][root][INFO] - Training Epoch: 2/10, step 85/574 completed (loss: 0.16561678051948547, acc: 0.9523809552192688)
[2024-11-08 03:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:51][root][INFO] - Training Epoch: 2/10, step 86/574 completed (loss: 0.19888879358768463, acc: 0.9583333134651184)
[2024-11-08 03:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:52][root][INFO] - Training Epoch: 2/10, step 87/574 completed (loss: 0.8159403204917908, acc: 0.7692307829856873)
[2024-11-08 03:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:52][root][INFO] - Training Epoch: 2/10, step 88/574 completed (loss: 0.21040880680084229, acc: 0.9047619104385376)
[2024-11-08 03:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:54][root][INFO] - Training Epoch: 2/10, step 89/574 completed (loss: 0.37429147958755493, acc: 0.8421052694320679)
[2024-11-08 03:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:55][root][INFO] - Training Epoch: 2/10, step 90/574 completed (loss: 0.4501606225967407, acc: 0.7894737124443054)
[2024-11-08 03:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:56][root][INFO] - Training Epoch: 2/10, step 91/574 completed (loss: 0.33395257592201233, acc: 0.8636363744735718)
[2024-11-08 03:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:57][root][INFO] - Training Epoch: 2/10, step 92/574 completed (loss: 0.3044911026954651, acc: 0.9047619104385376)
[2024-11-08 03:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:59][root][INFO] - Training Epoch: 2/10, step 93/574 completed (loss: 0.6620486974716187, acc: 0.807692289352417)
[2024-11-08 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:33:59][root][INFO] - Training Epoch: 2/10, step 94/574 completed (loss: 0.17557275295257568, acc: 0.9285714030265808)
[2024-11-08 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:00][root][INFO] - Training Epoch: 2/10, step 95/574 completed (loss: 0.26285094022750854, acc: 0.9047619104385376)
[2024-11-08 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:00][root][INFO] - Training Epoch: 2/10, step 96/574 completed (loss: 0.38450124859809875, acc: 0.8421052694320679)
[2024-11-08 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:00][root][INFO] - Training Epoch: 2/10, step 97/574 completed (loss: 0.4457278251647949, acc: 0.800000011920929)
[2024-11-08 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:01][root][INFO] - Training Epoch: 2/10, step 98/574 completed (loss: 0.47455111145973206, acc: 0.8095238208770752)
[2024-11-08 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:01][root][INFO] - Training Epoch: 2/10, step 99/574 completed (loss: 0.4206579923629761, acc: 0.8181818127632141)
[2024-11-08 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:02][root][INFO] - Training Epoch: 2/10, step 100/574 completed (loss: 0.09258702397346497, acc: 1.0)
[2024-11-08 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:02][root][INFO] - Training Epoch: 2/10, step 101/574 completed (loss: 0.5573672652244568, acc: 0.8799999952316284)
[2024-11-08 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:03][root][INFO] - Training Epoch: 2/10, step 102/574 completed (loss: 0.3400631546974182, acc: 0.9047619104385376)
[2024-11-08 03:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:03][root][INFO] - Training Epoch: 2/10, step 103/574 completed (loss: 0.2232893407344818, acc: 0.8947368264198303)
[2024-11-08 03:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:03][root][INFO] - Training Epoch: 2/10, step 104/574 completed (loss: 0.18130788207054138, acc: 0.9090909361839294)
[2024-11-08 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:04][root][INFO] - Training Epoch: 2/10, step 105/574 completed (loss: 0.5979992747306824, acc: 0.7894737124443054)
[2024-11-08 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:04][root][INFO] - Training Epoch: 2/10, step 106/574 completed (loss: 0.8171949982643127, acc: 0.8636363744735718)
[2024-11-08 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:05][root][INFO] - Training Epoch: 2/10, step 107/574 completed (loss: 0.2973814308643341, acc: 0.9285714030265808)
[2024-11-08 03:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:05][root][INFO] - Training Epoch: 2/10, step 108/574 completed (loss: 0.42864158749580383, acc: 0.8333333134651184)
[2024-11-08 03:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:05][root][INFO] - Training Epoch: 2/10, step 109/574 completed (loss: 0.5223824381828308, acc: 0.8571428656578064)
[2024-11-08 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:06][root][INFO] - Training Epoch: 2/10, step 110/574 completed (loss: 0.6262956857681274, acc: 0.8571428656578064)
[2024-11-08 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:06][root][INFO] - Training Epoch: 2/10, step 111/574 completed (loss: 0.3534405529499054, acc: 0.8421052694320679)
[2024-11-08 03:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:07][root][INFO] - Training Epoch: 2/10, step 112/574 completed (loss: 0.33936065435409546, acc: 0.9545454382896423)
[2024-11-08 03:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:07][root][INFO] - Training Epoch: 2/10, step 113/574 completed (loss: 0.278626948595047, acc: 0.8947368264198303)
[2024-11-08 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:08][root][INFO] - Training Epoch: 2/10, step 114/574 completed (loss: 0.13711829483509064, acc: 1.0)
[2024-11-08 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:08][root][INFO] - Training Epoch: 2/10, step 115/574 completed (loss: 0.15739968419075012, acc: 0.9615384340286255)
[2024-11-08 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:09][root][INFO] - Training Epoch: 2/10, step 116/574 completed (loss: 0.38723477721214294, acc: 0.8399999737739563)
[2024-11-08 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:09][root][INFO] - Training Epoch: 2/10, step 117/574 completed (loss: 0.38649970293045044, acc: 0.8500000238418579)
[2024-11-08 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:09][root][INFO] - Training Epoch: 2/10, step 118/574 completed (loss: 0.3119351863861084, acc: 0.7894737124443054)
[2024-11-08 03:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:11][root][INFO] - Training Epoch: 2/10, step 119/574 completed (loss: 0.33746734261512756, acc: 0.8181818127632141)
[2024-11-08 03:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:11][root][INFO] - Training Epoch: 2/10, step 120/574 completed (loss: 0.2561829388141632, acc: 0.8947368264198303)
[2024-11-08 03:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:12][root][INFO] - Training Epoch: 2/10, step 121/574 completed (loss: 0.5532564520835876, acc: 0.8181818127632141)
[2024-11-08 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:12][root][INFO] - Training Epoch: 2/10, step 122/574 completed (loss: 0.18103981018066406, acc: 0.9642857313156128)
[2024-11-08 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:12][root][INFO] - Training Epoch: 2/10, step 123/574 completed (loss: 0.5529688596725464, acc: 0.807692289352417)
[2024-11-08 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:13][root][INFO] - Training Epoch: 2/10, step 124/574 completed (loss: 0.4455718398094177, acc: 0.8500000238418579)
[2024-11-08 03:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:13][root][INFO] - Training Epoch: 2/10, step 125/574 completed (loss: 0.35176652669906616, acc: 0.8999999761581421)
[2024-11-08 03:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:14][root][INFO] - Training Epoch: 2/10, step 126/574 completed (loss: 0.324749618768692, acc: 0.8421052694320679)
[2024-11-08 03:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:14][root][INFO] - Training Epoch: 2/10, step 127/574 completed (loss: 0.28245773911476135, acc: 0.8181818127632141)
[2024-11-08 03:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:14][root][INFO] - Training Epoch: 2/10, step 128/574 completed (loss: 0.5320150256156921, acc: 0.800000011920929)
[2024-11-08 03:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:15][root][INFO] - Training Epoch: 2/10, step 129/574 completed (loss: 0.10987191647291183, acc: 1.0)
[2024-11-08 03:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:15][root][INFO] - Training Epoch: 2/10, step 130/574 completed (loss: 0.15104374289512634, acc: 0.939393937587738)
[2024-11-08 03:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:16][root][INFO] - Training Epoch: 2/10, step 131/574 completed (loss: 0.3076592683792114, acc: 0.8888888955116272)
[2024-11-08 03:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:16][root][INFO] - Training Epoch: 2/10, step 132/574 completed (loss: 0.5216165781021118, acc: 0.8181818127632141)
[2024-11-08 03:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:17][root][INFO] - Training Epoch: 2/10, step 133/574 completed (loss: 0.41767066717147827, acc: 0.8500000238418579)
[2024-11-08 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:17][root][INFO] - Training Epoch: 2/10, step 134/574 completed (loss: 0.42893630266189575, acc: 0.800000011920929)
[2024-11-08 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:17][root][INFO] - Training Epoch: 2/10, step 135/574 completed (loss: 0.6012822985649109, acc: 0.7894737124443054)
[2024-11-08 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:18][root][INFO] - Training Epoch: 2/10, step 136/574 completed (loss: 0.273253470659256, acc: 0.9090909361839294)
[2024-11-08 03:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:18][root][INFO] - Training Epoch: 2/10, step 137/574 completed (loss: 0.637714684009552, acc: 0.75)
[2024-11-08 03:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:19][root][INFO] - Training Epoch: 2/10, step 138/574 completed (loss: 0.49147364497184753, acc: 0.8181818127632141)
[2024-11-08 03:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:19][root][INFO] - Training Epoch: 2/10, step 139/574 completed (loss: 0.24424952268600464, acc: 0.939393937587738)
[2024-11-08 03:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:19][root][INFO] - Training Epoch: 2/10, step 140/574 completed (loss: 0.30734312534332275, acc: 0.8518518805503845)
[2024-11-08 03:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:03][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3865, device='cuda:0') eval_epoch_loss=tensor(0.3268, device='cuda:0') eval_epoch_acc=tensor(0.8886, device='cuda:0')
[2024-11-08 03:35:03][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:35:03][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:35:07][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_2_step_141_loss_0.3267572224140167/model.pt
[2024-11-08 03:35:07][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:35:07][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.3267572224140167
[2024-11-08 03:35:07][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8885951638221741
[2024-11-08 03:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:07][root][INFO] - Training Epoch: 2/10, step 141/574 completed (loss: 0.4463532865047455, acc: 0.8787878751754761)
[2024-11-08 03:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:07][root][INFO] - Training Epoch: 2/10, step 142/574 completed (loss: 0.5232015252113342, acc: 0.8500000238418579)
[2024-11-08 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:08][root][INFO] - Training Epoch: 2/10, step 143/574 completed (loss: 0.4078083634376526, acc: 0.8999999761581421)
[2024-11-08 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:08][root][INFO] - Training Epoch: 2/10, step 144/574 completed (loss: 0.38820141553878784, acc: 0.8421052694320679)
[2024-11-08 03:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:09][root][INFO] - Training Epoch: 2/10, step 145/574 completed (loss: 0.32989615201950073, acc: 0.9090909361839294)
[2024-11-08 03:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:10][root][INFO] - Training Epoch: 2/10, step 146/574 completed (loss: 0.4145137369632721, acc: 0.800000011920929)
[2024-11-08 03:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:10][root][INFO] - Training Epoch: 2/10, step 147/574 completed (loss: 0.20845961570739746, acc: 0.9090909361839294)
[2024-11-08 03:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:10][root][INFO] - Training Epoch: 2/10, step 148/574 completed (loss: 0.4808616042137146, acc: 0.8787878751754761)
[2024-11-08 03:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:11][root][INFO] - Training Epoch: 2/10, step 149/574 completed (loss: 0.2471712976694107, acc: 0.8888888955116272)
[2024-11-08 03:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:11][root][INFO] - Training Epoch: 2/10, step 150/574 completed (loss: 0.36486515402793884, acc: 0.8387096524238586)
[2024-11-08 03:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:11][root][INFO] - Training Epoch: 2/10, step 151/574 completed (loss: 0.4087597727775574, acc: 0.8500000238418579)
[2024-11-08 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:12][root][INFO] - Training Epoch: 2/10, step 152/574 completed (loss: 0.18427753448486328, acc: 0.8999999761581421)
[2024-11-08 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:12][root][INFO] - Training Epoch: 2/10, step 153/574 completed (loss: 0.4119701683521271, acc: 0.7894737124443054)
[2024-11-08 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:13][root][INFO] - Training Epoch: 2/10, step 154/574 completed (loss: 0.2907663583755493, acc: 0.8636363744735718)
[2024-11-08 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:13][root][INFO] - Training Epoch: 2/10, step 155/574 completed (loss: 0.4082542359828949, acc: 0.800000011920929)
[2024-11-08 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:13][root][INFO] - Training Epoch: 2/10, step 156/574 completed (loss: 0.31343328952789307, acc: 0.9090909361839294)
[2024-11-08 03:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:14][root][INFO] - Training Epoch: 2/10, step 157/574 completed (loss: 0.34280285239219666, acc: 0.8787878751754761)
[2024-11-08 03:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:16][root][INFO] - Training Epoch: 2/10, step 158/574 completed (loss: 0.6408606171607971, acc: 0.949999988079071)
[2024-11-08 03:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:17][root][INFO] - Training Epoch: 2/10, step 159/574 completed (loss: 0.21211430430412292, acc: 0.8999999761581421)
[2024-11-08 03:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:17][root][INFO] - Training Epoch: 2/10, step 160/574 completed (loss: 0.4852834939956665, acc: 0.7368420958518982)
[2024-11-08 03:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:18][root][INFO] - Training Epoch: 2/10, step 161/574 completed (loss: 0.49225905537605286, acc: 0.8636363744735718)
[2024-11-08 03:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:19][root][INFO] - Training Epoch: 2/10, step 162/574 completed (loss: 0.44600963592529297, acc: 0.800000011920929)
[2024-11-08 03:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:19][root][INFO] - Training Epoch: 2/10, step 163/574 completed (loss: 0.30359384417533875, acc: 0.8799999952316284)
[2024-11-08 03:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:20][root][INFO] - Training Epoch: 2/10, step 164/574 completed (loss: 0.2703278660774231, acc: 0.8888888955116272)
[2024-11-08 03:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:20][root][INFO] - Training Epoch: 2/10, step 165/574 completed (loss: 0.32745790481567383, acc: 0.9166666865348816)
[2024-11-08 03:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:20][root][INFO] - Training Epoch: 2/10, step 166/574 completed (loss: 0.23932848870754242, acc: 0.8695651888847351)
[2024-11-08 03:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:21][root][INFO] - Training Epoch: 2/10, step 167/574 completed (loss: 0.410329133272171, acc: 0.8095238208770752)
[2024-11-08 03:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:21][root][INFO] - Training Epoch: 2/10, step 168/574 completed (loss: 0.22236306965351105, acc: 0.9473684430122375)
[2024-11-08 03:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:22][root][INFO] - Training Epoch: 2/10, step 169/574 completed (loss: 0.28999993205070496, acc: 0.9090909361839294)
[2024-11-08 03:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:23][root][INFO] - Training Epoch: 2/10, step 170/574 completed (loss: 0.3650180697441101, acc: 0.8999999761581421)
[2024-11-08 03:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:24][root][INFO] - Training Epoch: 2/10, step 171/574 completed (loss: 0.13091601431369781, acc: 0.9545454382896423)
[2024-11-08 03:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:24][root][INFO] - Training Epoch: 2/10, step 172/574 completed (loss: 0.13292862474918365, acc: 0.939393937587738)
[2024-11-08 03:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:24][root][INFO] - Training Epoch: 2/10, step 173/574 completed (loss: 0.30106809735298157, acc: 0.8666666746139526)
[2024-11-08 03:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:25][root][INFO] - Training Epoch: 2/10, step 174/574 completed (loss: 0.4007130265235901, acc: 0.8571428656578064)
[2024-11-08 03:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:25][root][INFO] - Training Epoch: 2/10, step 175/574 completed (loss: 0.5927693247795105, acc: 0.8095238208770752)
[2024-11-08 03:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:26][root][INFO] - Training Epoch: 2/10, step 176/574 completed (loss: 0.3166162669658661, acc: 0.8421052694320679)
[2024-11-08 03:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:27][root][INFO] - Training Epoch: 2/10, step 177/574 completed (loss: 0.3415429890155792, acc: 0.8636363744735718)
[2024-11-08 03:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:28][root][INFO] - Training Epoch: 2/10, step 178/574 completed (loss: 0.3049149811267853, acc: 0.8947368264198303)
[2024-11-08 03:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:29][root][INFO] - Training Epoch: 2/10, step 179/574 completed (loss: 0.2364775687456131, acc: 0.9545454382896423)
[2024-11-08 03:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:29][root][INFO] - Training Epoch: 2/10, step 180/574 completed (loss: 0.03898653760552406, acc: 1.0)
[2024-11-08 03:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:29][root][INFO] - Training Epoch: 2/10, step 181/574 completed (loss: 0.08391666412353516, acc: 0.9666666388511658)
[2024-11-08 03:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:30][root][INFO] - Training Epoch: 2/10, step 182/574 completed (loss: 0.39451345801353455, acc: 0.8857142925262451)
[2024-11-08 03:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:30][root][INFO] - Training Epoch: 2/10, step 183/574 completed (loss: 0.18486320972442627, acc: 0.95652174949646)
[2024-11-08 03:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:31][root][INFO] - Training Epoch: 2/10, step 184/574 completed (loss: 0.31475844979286194, acc: 0.8095238208770752)
[2024-11-08 03:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:31][root][INFO] - Training Epoch: 2/10, step 185/574 completed (loss: 0.35434845089912415, acc: 0.8947368264198303)
[2024-11-08 03:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:32][root][INFO] - Training Epoch: 2/10, step 186/574 completed (loss: 0.342117577791214, acc: 0.8500000238418579)
[2024-11-08 03:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:32][root][INFO] - Training Epoch: 2/10, step 187/574 completed (loss: 0.24573209881782532, acc: 0.8571428656578064)
[2024-11-08 03:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:33][root][INFO] - Training Epoch: 2/10, step 188/574 completed (loss: 0.21342334151268005, acc: 0.9090909361839294)
[2024-11-08 03:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:33][root][INFO] - Training Epoch: 2/10, step 189/574 completed (loss: 0.6363359689712524, acc: 0.800000011920929)
[2024-11-08 03:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:34][root][INFO] - Training Epoch: 2/10, step 190/574 completed (loss: 0.5680055022239685, acc: 0.75)
[2024-11-08 03:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:35][root][INFO] - Training Epoch: 2/10, step 191/574 completed (loss: 0.6607465147972107, acc: 0.8095238208770752)
[2024-11-08 03:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:37][root][INFO] - Training Epoch: 2/10, step 192/574 completed (loss: 0.3689118027687073, acc: 0.8421052694320679)
[2024-11-08 03:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:38][root][INFO] - Training Epoch: 2/10, step 193/574 completed (loss: 0.3107064962387085, acc: 0.8181818127632141)
[2024-11-08 03:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:40][root][INFO] - Training Epoch: 2/10, step 194/574 completed (loss: 0.3902706503868103, acc: 0.8571428656578064)
[2024-11-08 03:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:41][root][INFO] - Training Epoch: 2/10, step 195/574 completed (loss: 0.1307753175497055, acc: 0.9583333134651184)
[2024-11-08 03:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:42][root][INFO] - Training Epoch: 2/10, step 196/574 completed (loss: 0.11213991045951843, acc: 0.9677419066429138)
[2024-11-08 03:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:42][root][INFO] - Training Epoch: 2/10, step 197/574 completed (loss: 0.22439853847026825, acc: 0.8999999761581421)
[2024-11-08 03:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:42][root][INFO] - Training Epoch: 2/10, step 198/574 completed (loss: 0.19122549891471863, acc: 0.949999988079071)
[2024-11-08 03:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:43][root][INFO] - Training Epoch: 2/10, step 199/574 completed (loss: 0.21336424350738525, acc: 0.8999999761581421)
[2024-11-08 03:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:43][root][INFO] - Training Epoch: 2/10, step 200/574 completed (loss: 0.3917351961135864, acc: 0.8421052694320679)
[2024-11-08 03:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:44][root][INFO] - Training Epoch: 2/10, step 201/574 completed (loss: 0.27504006028175354, acc: 0.8636363744735718)
[2024-11-08 03:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:44][root][INFO] - Training Epoch: 2/10, step 202/574 completed (loss: 0.44390368461608887, acc: 0.8500000238418579)
[2024-11-08 03:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:45][root][INFO] - Training Epoch: 2/10, step 203/574 completed (loss: 0.18544425070285797, acc: 0.95652174949646)
[2024-11-08 03:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:45][root][INFO] - Training Epoch: 2/10, step 204/574 completed (loss: 0.3567998707294464, acc: 0.8928571343421936)
[2024-11-08 03:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:45][root][INFO] - Training Epoch: 2/10, step 205/574 completed (loss: 0.3265664577484131, acc: 0.8571428656578064)
[2024-11-08 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:46][root][INFO] - Training Epoch: 2/10, step 206/574 completed (loss: 0.3694564998149872, acc: 0.8421052694320679)
[2024-11-08 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:46][root][INFO] - Training Epoch: 2/10, step 207/574 completed (loss: 0.41146090626716614, acc: 0.7894737124443054)
[2024-11-08 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:47][root][INFO] - Training Epoch: 2/10, step 208/574 completed (loss: 0.3352087140083313, acc: 0.9090909361839294)
[2024-11-08 03:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:47][root][INFO] - Training Epoch: 2/10, step 209/574 completed (loss: 0.42108404636383057, acc: 0.8571428656578064)
[2024-11-08 03:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:48][root][INFO] - Training Epoch: 2/10, step 210/574 completed (loss: 0.2856588065624237, acc: 0.9583333134651184)
[2024-11-08 03:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:48][root][INFO] - Training Epoch: 2/10, step 211/574 completed (loss: 0.30109721422195435, acc: 0.9032257795333862)
[2024-11-08 03:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:48][root][INFO] - Training Epoch: 2/10, step 212/574 completed (loss: 0.280316561460495, acc: 0.9032257795333862)
[2024-11-08 03:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:49][root][INFO] - Training Epoch: 2/10, step 213/574 completed (loss: 0.33024075627326965, acc: 0.8846153616905212)
[2024-11-08 03:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:50][root][INFO] - Training Epoch: 2/10, step 214/574 completed (loss: 0.22352111339569092, acc: 0.9047619104385376)
[2024-11-08 03:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:50][root][INFO] - Training Epoch: 2/10, step 215/574 completed (loss: 0.3121527433395386, acc: 0.7894737124443054)
[2024-11-08 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:51][root][INFO] - Training Epoch: 2/10, step 216/574 completed (loss: 0.21702022850513458, acc: 0.9473684430122375)
[2024-11-08 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:51][root][INFO] - Training Epoch: 2/10, step 217/574 completed (loss: 0.3064509630203247, acc: 0.8636363744735718)
[2024-11-08 03:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:52][root][INFO] - Training Epoch: 2/10, step 218/574 completed (loss: 0.22808851301670074, acc: 0.9523809552192688)
[2024-11-08 03:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:52][root][INFO] - Training Epoch: 2/10, step 219/574 completed (loss: 0.11172326654195786, acc: 0.9583333134651184)
[2024-11-08 03:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:53][root][INFO] - Training Epoch: 2/10, step 220/574 completed (loss: 0.17707929015159607, acc: 0.9354838728904724)
[2024-11-08 03:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:53][root][INFO] - Training Epoch: 2/10, step 221/574 completed (loss: 0.21094684302806854, acc: 0.9354838728904724)
[2024-11-08 03:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:54][root][INFO] - Training Epoch: 2/10, step 222/574 completed (loss: 0.16760295629501343, acc: 0.9230769276618958)
[2024-11-08 03:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:55][root][INFO] - Training Epoch: 2/10, step 223/574 completed (loss: 0.12566183507442474, acc: 0.9523809552192688)
[2024-11-08 03:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:56][root][INFO] - Training Epoch: 2/10, step 224/574 completed (loss: 0.3313499391078949, acc: 0.8421052694320679)
[2024-11-08 03:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:56][root][INFO] - Training Epoch: 2/10, step 225/574 completed (loss: 0.27553853392601013, acc: 0.8421052694320679)
[2024-11-08 03:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:57][root][INFO] - Training Epoch: 2/10, step 226/574 completed (loss: 0.22176474332809448, acc: 0.9545454382896423)
[2024-11-08 03:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:57][root][INFO] - Training Epoch: 2/10, step 227/574 completed (loss: 0.5229604840278625, acc: 0.9047619104385376)
[2024-11-08 03:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:58][root][INFO] - Training Epoch: 2/10, step 228/574 completed (loss: 0.46018457412719727, acc: 0.875)
[2024-11-08 03:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:58][root][INFO] - Training Epoch: 2/10, step 229/574 completed (loss: 0.3711300194263458, acc: 0.8260869383811951)
[2024-11-08 03:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:58][root][INFO] - Training Epoch: 2/10, step 230/574 completed (loss: 0.4504145085811615, acc: 0.9047619104385376)
[2024-11-08 03:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:59][root][INFO] - Training Epoch: 2/10, step 231/574 completed (loss: 0.33522355556488037, acc: 0.7894737124443054)
[2024-11-08 03:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:35:59][root][INFO] - Training Epoch: 2/10, step 232/574 completed (loss: 0.21671558916568756, acc: 0.8947368264198303)
[2024-11-08 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:00][root][INFO] - Training Epoch: 2/10, step 233/574 completed (loss: 0.3355799913406372, acc: 0.9090909361839294)
[2024-11-08 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:01][root][INFO] - Training Epoch: 2/10, step 234/574 completed (loss: 0.40081965923309326, acc: 0.8636363744735718)
[2024-11-08 03:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:01][root][INFO] - Training Epoch: 2/10, step 235/574 completed (loss: 0.3147769570350647, acc: 0.9285714030265808)
[2024-11-08 03:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:02][root][INFO] - Training Epoch: 2/10, step 236/574 completed (loss: 0.4487328827381134, acc: 0.8888888955116272)
[2024-11-08 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:02][root][INFO] - Training Epoch: 2/10, step 237/574 completed (loss: 0.48861491680145264, acc: 0.8787878751754761)
[2024-11-08 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:03][root][INFO] - Training Epoch: 2/10, step 238/574 completed (loss: 0.367294579744339, acc: 0.9090909361839294)
[2024-11-08 03:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:03][root][INFO] - Training Epoch: 2/10, step 239/574 completed (loss: 0.319892555475235, acc: 0.8571428656578064)
[2024-11-08 03:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:03][root][INFO] - Training Epoch: 2/10, step 240/574 completed (loss: 0.3254025876522064, acc: 0.9473684430122375)
[2024-11-08 03:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:04][root][INFO] - Training Epoch: 2/10, step 241/574 completed (loss: 0.4984991252422333, acc: 0.8181818127632141)
[2024-11-08 03:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:05][root][INFO] - Training Epoch: 2/10, step 242/574 completed (loss: 0.3981871008872986, acc: 0.8421052694320679)
[2024-11-08 03:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:05][root][INFO] - Training Epoch: 2/10, step 243/574 completed (loss: 0.21324554085731506, acc: 0.9545454382896423)
[2024-11-08 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:06][root][INFO] - Training Epoch: 2/10, step 244/574 completed (loss: 0.09620587527751923, acc: 0.9642857313156128)
[2024-11-08 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:06][root][INFO] - Training Epoch: 2/10, step 245/574 completed (loss: 0.2665419280529022, acc: 0.8999999761581421)
[2024-11-08 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:07][root][INFO] - Training Epoch: 2/10, step 246/574 completed (loss: 0.3288351595401764, acc: 0.8571428656578064)
[2024-11-08 03:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:07][root][INFO] - Training Epoch: 2/10, step 247/574 completed (loss: 0.25645577907562256, acc: 0.8695651888847351)
[2024-11-08 03:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:07][root][INFO] - Training Epoch: 2/10, step 248/574 completed (loss: 0.38270318508148193, acc: 0.9047619104385376)
[2024-11-08 03:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:08][root][INFO] - Training Epoch: 2/10, step 249/574 completed (loss: 0.40328264236450195, acc: 0.8947368264198303)
[2024-11-08 03:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:08][root][INFO] - Training Epoch: 2/10, step 250/574 completed (loss: 0.3581846356391907, acc: 0.8999999761581421)
[2024-11-08 03:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:09][root][INFO] - Training Epoch: 2/10, step 251/574 completed (loss: 0.5133504271507263, acc: 0.8095238208770752)
[2024-11-08 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:09][root][INFO] - Training Epoch: 2/10, step 252/574 completed (loss: 0.22819703817367554, acc: 0.9090909361839294)
[2024-11-08 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:10][root][INFO] - Training Epoch: 2/10, step 253/574 completed (loss: 0.19381766021251678, acc: 0.8928571343421936)
[2024-11-08 03:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:10][root][INFO] - Training Epoch: 2/10, step 254/574 completed (loss: 0.10359270870685577, acc: 0.9655172228813171)
[2024-11-08 03:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:10][root][INFO] - Training Epoch: 2/10, step 255/574 completed (loss: 0.22817936539649963, acc: 0.9142857193946838)
[2024-11-08 03:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:11][root][INFO] - Training Epoch: 2/10, step 256/574 completed (loss: 0.18755130469799042, acc: 0.9523809552192688)
[2024-11-08 03:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:11][root][INFO] - Training Epoch: 2/10, step 257/574 completed (loss: 0.42902156710624695, acc: 0.8095238208770752)
[2024-11-08 03:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:12][root][INFO] - Training Epoch: 2/10, step 258/574 completed (loss: 0.18381719291210175, acc: 0.9473684430122375)
[2024-11-08 03:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:12][root][INFO] - Training Epoch: 2/10, step 259/574 completed (loss: 0.2546936273574829, acc: 0.8181818127632141)
[2024-11-08 03:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:13][root][INFO] - Training Epoch: 2/10, step 260/574 completed (loss: 0.3220285475254059, acc: 0.8947368264198303)
[2024-11-08 03:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:14][root][INFO] - Training Epoch: 2/10, step 261/574 completed (loss: 0.21866126358509064, acc: 0.8636363744735718)
[2024-11-08 03:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:14][root][INFO] - Training Epoch: 2/10, step 262/574 completed (loss: 0.2854764461517334, acc: 0.9117646813392639)
[2024-11-08 03:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:14][root][INFO] - Training Epoch: 2/10, step 263/574 completed (loss: 0.288359671831131, acc: 0.8181818127632141)
[2024-11-08 03:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:15][root][INFO] - Training Epoch: 2/10, step 264/574 completed (loss: 0.23452946543693542, acc: 0.8999999761581421)
[2024-11-08 03:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:16][root][INFO] - Training Epoch: 2/10, step 265/574 completed (loss: 0.3695260286331177, acc: 0.8421052694320679)
[2024-11-08 03:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:17][root][INFO] - Training Epoch: 2/10, step 266/574 completed (loss: 0.27916911244392395, acc: 0.8695651888847351)
[2024-11-08 03:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:17][root][INFO] - Training Epoch: 2/10, step 267/574 completed (loss: 0.3142031133174896, acc: 0.8571428656578064)
[2024-11-08 03:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:18][root][INFO] - Training Epoch: 2/10, step 268/574 completed (loss: 0.19148428738117218, acc: 0.931034505367279)
[2024-11-08 03:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:18][root][INFO] - Training Epoch: 2/10, step 269/574 completed (loss: 0.20514971017837524, acc: 0.9259259104728699)
[2024-11-08 03:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:19][root][INFO] - Training Epoch: 2/10, step 270/574 completed (loss: 0.3494817316532135, acc: 0.9230769276618958)
[2024-11-08 03:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:19][root][INFO] - Training Epoch: 2/10, step 271/574 completed (loss: 0.14125391840934753, acc: 0.9047619104385376)
[2024-11-08 03:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:19][root][INFO] - Training Epoch: 2/10, step 272/574 completed (loss: 0.1870110034942627, acc: 0.9473684430122375)
[2024-11-08 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:20][root][INFO] - Training Epoch: 2/10, step 273/574 completed (loss: 0.16844533383846283, acc: 0.9473684430122375)
[2024-11-08 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:20][root][INFO] - Training Epoch: 2/10, step 274/574 completed (loss: 0.24590033292770386, acc: 0.9090909361839294)
[2024-11-08 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:21][root][INFO] - Training Epoch: 2/10, step 275/574 completed (loss: 0.17869983613491058, acc: 0.9047619104385376)
[2024-11-08 03:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:21][root][INFO] - Training Epoch: 2/10, step 276/574 completed (loss: 0.47608238458633423, acc: 0.8965517282485962)
[2024-11-08 03:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:21][root][INFO] - Training Epoch: 2/10, step 277/574 completed (loss: 0.12544339895248413, acc: 0.9629629850387573)
[2024-11-08 03:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:22][root][INFO] - Training Epoch: 2/10, step 278/574 completed (loss: 0.17739403247833252, acc: 0.9523809552192688)
[2024-11-08 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:22][root][INFO] - Training Epoch: 2/10, step 279/574 completed (loss: 0.1913774311542511, acc: 0.9523809552192688)
[2024-11-08 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:23][root][INFO] - Training Epoch: 2/10, step 280/574 completed (loss: 0.2299993336200714, acc: 0.8333333134651184)
[2024-11-08 03:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:23][root][INFO] - Training Epoch: 2/10, step 281/574 completed (loss: 0.4329075515270233, acc: 0.8181818127632141)
[2024-11-08 03:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:24][root][INFO] - Training Epoch: 2/10, step 282/574 completed (loss: 0.2154175043106079, acc: 0.9523809552192688)
[2024-11-08 03:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:24][root][INFO] - Training Epoch: 2/10, step 283/574 completed (loss: 0.5340023040771484, acc: 0.8965517282485962)
[2024-11-08 03:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:09][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3640, device='cuda:0') eval_epoch_loss=tensor(0.3104, device='cuda:0') eval_epoch_acc=tensor(0.8968, device='cuda:0')
[2024-11-08 03:37:09][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:37:09][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:37:14][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_2_step_284_loss_0.31038668751716614/model.pt
[2024-11-08 03:37:14][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:37:14][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.31038668751716614
[2024-11-08 03:37:14][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8967944979667664
[2024-11-08 03:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:14][root][INFO] - Training Epoch: 2/10, step 284/574 completed (loss: 0.312949001789093, acc: 0.8999999761581421)
[2024-11-08 03:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:15][root][INFO] - Training Epoch: 2/10, step 285/574 completed (loss: 0.42452704906463623, acc: 0.8965517282485962)
[2024-11-08 03:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:15][root][INFO] - Training Epoch: 2/10, step 286/574 completed (loss: 0.30429425835609436, acc: 0.8571428656578064)
[2024-11-08 03:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:15][root][INFO] - Training Epoch: 2/10, step 287/574 completed (loss: 0.5875846147537231, acc: 0.7894737124443054)
[2024-11-08 03:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:16][root][INFO] - Training Epoch: 2/10, step 288/574 completed (loss: 0.5319311618804932, acc: 0.7894737124443054)
[2024-11-08 03:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:16][root][INFO] - Training Epoch: 2/10, step 289/574 completed (loss: 0.4433935880661011, acc: 0.9090909361839294)
[2024-11-08 03:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:17][root][INFO] - Training Epoch: 2/10, step 290/574 completed (loss: 0.4222330152988434, acc: 0.8181818127632141)
[2024-11-08 03:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:17][root][INFO] - Training Epoch: 2/10, step 291/574 completed (loss: 0.3918740153312683, acc: 0.9200000166893005)
[2024-11-08 03:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:17][root][INFO] - Training Epoch: 2/10, step 292/574 completed (loss: 0.25276023149490356, acc: 0.9333333373069763)
[2024-11-08 03:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:18][root][INFO] - Training Epoch: 2/10, step 293/574 completed (loss: 0.38041865825653076, acc: 0.8571428656578064)
[2024-11-08 03:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:19][root][INFO] - Training Epoch: 2/10, step 294/574 completed (loss: 0.1798488199710846, acc: 0.949999988079071)
[2024-11-08 03:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:20][root][INFO] - Training Epoch: 2/10, step 295/574 completed (loss: 0.4219464659690857, acc: 0.7894737124443054)
[2024-11-08 03:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:20][root][INFO] - Training Epoch: 2/10, step 296/574 completed (loss: 0.4096800982952118, acc: 0.8636363744735718)
[2024-11-08 03:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:20][root][INFO] - Training Epoch: 2/10, step 297/574 completed (loss: 0.5986708998680115, acc: 0.8095238208770752)
[2024-11-08 03:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:21][root][INFO] - Training Epoch: 2/10, step 298/574 completed (loss: 0.3312772214412689, acc: 0.9166666865348816)
[2024-11-08 03:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:21][root][INFO] - Training Epoch: 2/10, step 299/574 completed (loss: 0.2839234173297882, acc: 0.9032257795333862)
[2024-11-08 03:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:21][root][INFO] - Training Epoch: 2/10, step 300/574 completed (loss: 0.31954699754714966, acc: 0.8709677457809448)
[2024-11-08 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:22][root][INFO] - Training Epoch: 2/10, step 301/574 completed (loss: 0.48801517486572266, acc: 0.807692289352417)
[2024-11-08 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:22][root][INFO] - Training Epoch: 2/10, step 302/574 completed (loss: 0.26253214478492737, acc: 0.9047619104385376)
[2024-11-08 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:23][root][INFO] - Training Epoch: 2/10, step 303/574 completed (loss: 0.3762570023536682, acc: 0.8421052694320679)
[2024-11-08 03:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:23][root][INFO] - Training Epoch: 2/10, step 304/574 completed (loss: 0.29057982563972473, acc: 0.8421052694320679)
[2024-11-08 03:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:23][root][INFO] - Training Epoch: 2/10, step 305/574 completed (loss: 0.189130499958992, acc: 0.95652174949646)
[2024-11-08 03:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:24][root][INFO] - Training Epoch: 2/10, step 306/574 completed (loss: 0.21551841497421265, acc: 1.0)
[2024-11-08 03:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:24][root][INFO] - Training Epoch: 2/10, step 307/574 completed (loss: 0.23782631754875183, acc: 0.9230769276618958)
[2024-11-08 03:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:25][root][INFO] - Training Epoch: 2/10, step 308/574 completed (loss: 0.1224290207028389, acc: 0.9523809552192688)
[2024-11-08 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:25][root][INFO] - Training Epoch: 2/10, step 309/574 completed (loss: 0.504530668258667, acc: 0.7894737124443054)
[2024-11-08 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:26][root][INFO] - Training Epoch: 2/10, step 310/574 completed (loss: 0.28820061683654785, acc: 0.8947368264198303)
[2024-11-08 03:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:26][root][INFO] - Training Epoch: 2/10, step 311/574 completed (loss: 0.4300795793533325, acc: 0.8181818127632141)
[2024-11-08 03:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:27][root][INFO] - Training Epoch: 2/10, step 312/574 completed (loss: 0.4008435308933258, acc: 0.8636363744735718)
[2024-11-08 03:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:27][root][INFO] - Training Epoch: 2/10, step 313/574 completed (loss: 0.2558046281337738, acc: 0.8928571343421936)
[2024-11-08 03:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:27][root][INFO] - Training Epoch: 2/10, step 314/574 completed (loss: 0.2293689101934433, acc: 0.8888888955116272)
[2024-11-08 03:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:28][root][INFO] - Training Epoch: 2/10, step 315/574 completed (loss: 0.350363552570343, acc: 0.8857142925262451)
[2024-11-08 03:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:28][root][INFO] - Training Epoch: 2/10, step 316/574 completed (loss: 0.2690226137638092, acc: 0.8846153616905212)
[2024-11-08 03:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:28][root][INFO] - Training Epoch: 2/10, step 317/574 completed (loss: 0.14970143139362335, acc: 0.9523809552192688)
[2024-11-08 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:29][root][INFO] - Training Epoch: 2/10, step 318/574 completed (loss: 0.2887871265411377, acc: 0.8999999761581421)
[2024-11-08 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:29][root][INFO] - Training Epoch: 2/10, step 319/574 completed (loss: 0.07785392552614212, acc: 1.0)
[2024-11-08 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:30][root][INFO] - Training Epoch: 2/10, step 320/574 completed (loss: 0.1657448559999466, acc: 0.9523809552192688)
[2024-11-08 03:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:30][root][INFO] - Training Epoch: 2/10, step 321/574 completed (loss: 0.14782725274562836, acc: 0.9545454382896423)
[2024-11-08 03:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:31][root][INFO] - Training Epoch: 2/10, step 322/574 completed (loss: 0.36858177185058594, acc: 0.8260869383811951)
[2024-11-08 03:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:31][root][INFO] - Training Epoch: 2/10, step 323/574 completed (loss: 0.45598283410072327, acc: 0.8095238208770752)
[2024-11-08 03:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:32][root][INFO] - Training Epoch: 2/10, step 324/574 completed (loss: 0.5330263376235962, acc: 0.7894737124443054)
[2024-11-08 03:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:32][root][INFO] - Training Epoch: 2/10, step 325/574 completed (loss: 0.6417961716651917, acc: 0.8181818127632141)
[2024-11-08 03:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:32][root][INFO] - Training Epoch: 2/10, step 326/574 completed (loss: 0.6496737599372864, acc: 0.7368420958518982)
[2024-11-08 03:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:33][root][INFO] - Training Epoch: 2/10, step 327/574 completed (loss: 0.39735960960388184, acc: 0.8333333134651184)
[2024-11-08 03:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:33][root][INFO] - Training Epoch: 2/10, step 328/574 completed (loss: 0.34826886653900146, acc: 0.8275862336158752)
[2024-11-08 03:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:33][root][INFO] - Training Epoch: 2/10, step 329/574 completed (loss: 0.35866186022758484, acc: 0.8518518805503845)
[2024-11-08 03:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:34][root][INFO] - Training Epoch: 2/10, step 330/574 completed (loss: 0.2652842402458191, acc: 0.8571428656578064)
[2024-11-08 03:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:34][root][INFO] - Training Epoch: 2/10, step 331/574 completed (loss: 0.19126363098621368, acc: 0.9473684430122375)
[2024-11-08 03:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:35][root][INFO] - Training Epoch: 2/10, step 332/574 completed (loss: 0.2708936929702759, acc: 0.9090909361839294)
[2024-11-08 03:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:35][root][INFO] - Training Epoch: 2/10, step 333/574 completed (loss: 0.7022221684455872, acc: 0.7894737124443054)
[2024-11-08 03:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:36][root][INFO] - Training Epoch: 2/10, step 334/574 completed (loss: 0.697550356388092, acc: 0.9090909361839294)
[2024-11-08 03:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:36][root][INFO] - Training Epoch: 2/10, step 335/574 completed (loss: 0.3698873221874237, acc: 0.8461538553237915)
[2024-11-08 03:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:36][root][INFO] - Training Epoch: 2/10, step 336/574 completed (loss: 0.813655436038971, acc: 0.7083333134651184)
[2024-11-08 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:37][root][INFO] - Training Epoch: 2/10, step 337/574 completed (loss: 0.44700750708580017, acc: 0.8095238208770752)
[2024-11-08 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:37][root][INFO] - Training Epoch: 2/10, step 338/574 completed (loss: 0.27335628867149353, acc: 0.9130434989929199)
[2024-11-08 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:38][root][INFO] - Training Epoch: 2/10, step 339/574 completed (loss: 0.2869390845298767, acc: 0.9090909361839294)
[2024-11-08 03:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:38][root][INFO] - Training Epoch: 2/10, step 340/574 completed (loss: 0.7934406399726868, acc: 0.8999999761581421)
[2024-11-08 03:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:38][root][INFO] - Training Epoch: 2/10, step 341/574 completed (loss: 0.3127504587173462, acc: 0.8787878751754761)
[2024-11-08 03:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:39][root][INFO] - Training Epoch: 2/10, step 342/574 completed (loss: 0.38004761934280396, acc: 0.8999999761581421)
[2024-11-08 03:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:39][root][INFO] - Training Epoch: 2/10, step 343/574 completed (loss: 0.4343339502811432, acc: 0.8999999761581421)
[2024-11-08 03:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:40][root][INFO] - Training Epoch: 2/10, step 344/574 completed (loss: 0.47647321224212646, acc: 0.8421052694320679)
[2024-11-08 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:40][root][INFO] - Training Epoch: 2/10, step 345/574 completed (loss: 0.3847435712814331, acc: 0.8181818127632141)
[2024-11-08 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:41][root][INFO] - Training Epoch: 2/10, step 346/574 completed (loss: 0.35162732005119324, acc: 0.8999999761581421)
[2024-11-08 03:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:41][root][INFO] - Training Epoch: 2/10, step 347/574 completed (loss: 0.2977176308631897, acc: 0.875)
[2024-11-08 03:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:41][root][INFO] - Training Epoch: 2/10, step 348/574 completed (loss: 0.31709426641464233, acc: 0.8695651888847351)
[2024-11-08 03:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:42][root][INFO] - Training Epoch: 2/10, step 349/574 completed (loss: 0.2587208151817322, acc: 0.8999999761581421)
[2024-11-08 03:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:42][root][INFO] - Training Epoch: 2/10, step 350/574 completed (loss: 0.31114697456359863, acc: 0.8947368264198303)
[2024-11-08 03:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:43][root][INFO] - Training Epoch: 2/10, step 351/574 completed (loss: 0.31958630681037903, acc: 0.9545454382896423)
[2024-11-08 03:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:43][root][INFO] - Training Epoch: 2/10, step 352/574 completed (loss: 0.3467153310775757, acc: 0.8999999761581421)
[2024-11-08 03:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:44][root][INFO] - Training Epoch: 2/10, step 353/574 completed (loss: 0.21654075384140015, acc: 0.9599999785423279)
[2024-11-08 03:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:44][root][INFO] - Training Epoch: 2/10, step 354/574 completed (loss: 0.4609500765800476, acc: 0.9200000166893005)
[2024-11-08 03:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:45][root][INFO] - Training Epoch: 2/10, step 355/574 completed (loss: 0.401845246553421, acc: 0.8095238208770752)
[2024-11-08 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:45][root][INFO] - Training Epoch: 2/10, step 356/574 completed (loss: 0.34538692235946655, acc: 0.8500000238418579)
[2024-11-08 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:46][root][INFO] - Training Epoch: 2/10, step 357/574 completed (loss: 0.2688118517398834, acc: 0.9047619104385376)
[2024-11-08 03:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:46][root][INFO] - Training Epoch: 2/10, step 358/574 completed (loss: 0.29761597514152527, acc: 0.8636363744735718)
[2024-11-08 03:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:47][root][INFO] - Training Epoch: 2/10, step 359/574 completed (loss: 0.16328608989715576, acc: 0.9642857313156128)
[2024-11-08 03:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:47][root][INFO] - Training Epoch: 2/10, step 360/574 completed (loss: 0.4450671672821045, acc: 0.8965517282485962)
[2024-11-08 03:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:47][root][INFO] - Training Epoch: 2/10, step 361/574 completed (loss: 0.4256977140903473, acc: 0.8285714387893677)
[2024-11-08 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:48][root][INFO] - Training Epoch: 2/10, step 362/574 completed (loss: 0.3070484399795532, acc: 0.9090909361839294)
[2024-11-08 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:48][root][INFO] - Training Epoch: 2/10, step 363/574 completed (loss: 0.4215944707393646, acc: 0.8421052694320679)
[2024-11-08 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:49][root][INFO] - Training Epoch: 2/10, step 364/574 completed (loss: 0.2404007613658905, acc: 0.8571428656578064)
[2024-11-08 03:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:49][root][INFO] - Training Epoch: 2/10, step 365/574 completed (loss: 0.261540949344635, acc: 0.8947368264198303)
[2024-11-08 03:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:50][root][INFO] - Training Epoch: 2/10, step 366/574 completed (loss: 0.15215858817100525, acc: 0.9545454382896423)
[2024-11-08 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:50][root][INFO] - Training Epoch: 2/10, step 367/574 completed (loss: 0.3269650638103485, acc: 0.8888888955116272)
[2024-11-08 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:50][root][INFO] - Training Epoch: 2/10, step 368/574 completed (loss: 0.2753105163574219, acc: 0.90625)
[2024-11-08 03:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:51][root][INFO] - Training Epoch: 2/10, step 369/574 completed (loss: 0.24781163036823273, acc: 0.8571428656578064)
[2024-11-08 03:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:52][root][INFO] - Training Epoch: 2/10, step 370/574 completed (loss: 0.5285124778747559, acc: 0.8571428656578064)
[2024-11-08 03:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:53][root][INFO] - Training Epoch: 2/10, step 371/574 completed (loss: 0.3354472815990448, acc: 0.7894737124443054)
[2024-11-08 03:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:54][root][INFO] - Training Epoch: 2/10, step 372/574 completed (loss: 0.23618151247501373, acc: 0.8636363744735718)
[2024-11-08 03:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:54][root][INFO] - Training Epoch: 2/10, step 373/574 completed (loss: 0.20655322074890137, acc: 0.8947368264198303)
[2024-11-08 03:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:54][root][INFO] - Training Epoch: 2/10, step 374/574 completed (loss: 0.2540510892868042, acc: 0.9090909361839294)
[2024-11-08 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:55][root][INFO] - Training Epoch: 2/10, step 375/574 completed (loss: 0.1791190505027771, acc: 0.931034505367279)
[2024-11-08 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:55][root][INFO] - Training Epoch: 2/10, step 376/574 completed (loss: 0.11762998253107071, acc: 0.9677419066429138)
[2024-11-08 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:56][root][INFO] - Training Epoch: 2/10, step 377/574 completed (loss: 0.4101954996585846, acc: 0.8571428656578064)
[2024-11-08 03:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:56][root][INFO] - Training Epoch: 2/10, step 378/574 completed (loss: 0.3618099093437195, acc: 0.8095238208770752)
[2024-11-08 03:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:57][root][INFO] - Training Epoch: 2/10, step 379/574 completed (loss: 0.19259510934352875, acc: 1.0)
[2024-11-08 03:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:58][root][INFO] - Training Epoch: 2/10, step 380/574 completed (loss: 0.2542837858200073, acc: 0.9090909361839294)
[2024-11-08 03:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:37:59][root][INFO] - Training Epoch: 2/10, step 381/574 completed (loss: 0.4053579568862915, acc: 0.8421052694320679)
[2024-11-08 03:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:00][root][INFO] - Training Epoch: 2/10, step 382/574 completed (loss: 0.20963811874389648, acc: 0.9090909361839294)
[2024-11-08 03:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:01][root][INFO] - Training Epoch: 2/10, step 383/574 completed (loss: 0.08828150480985641, acc: 0.9285714030265808)
[2024-11-08 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:01][root][INFO] - Training Epoch: 2/10, step 384/574 completed (loss: 0.061965230852365494, acc: 0.9666666388511658)
[2024-11-08 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:01][root][INFO] - Training Epoch: 2/10, step 385/574 completed (loss: 0.16596946120262146, acc: 0.9142857193946838)
[2024-11-08 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:02][root][INFO] - Training Epoch: 2/10, step 386/574 completed (loss: 0.1495344489812851, acc: 0.949999988079071)
[2024-11-08 03:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:02][root][INFO] - Training Epoch: 2/10, step 387/574 completed (loss: 0.24312056601047516, acc: 0.8947368264198303)
[2024-11-08 03:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:02][root][INFO] - Training Epoch: 2/10, step 388/574 completed (loss: 0.0535115972161293, acc: 1.0)
[2024-11-08 03:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:03][root][INFO] - Training Epoch: 2/10, step 389/574 completed (loss: 0.4083314836025238, acc: 0.8500000238418579)
[2024-11-08 03:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:03][root][INFO] - Training Epoch: 2/10, step 390/574 completed (loss: 0.2503443658351898, acc: 0.8636363744735718)
[2024-11-08 03:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:04][root][INFO] - Training Epoch: 2/10, step 391/574 completed (loss: 0.1553884893655777, acc: 0.9523809552192688)
[2024-11-08 03:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:04][root][INFO] - Training Epoch: 2/10, step 392/574 completed (loss: 0.45509955286979675, acc: 0.8421052694320679)
[2024-11-08 03:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:05][root][INFO] - Training Epoch: 2/10, step 393/574 completed (loss: 0.15300914645195007, acc: 0.9473684430122375)
[2024-11-08 03:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:05][root][INFO] - Training Epoch: 2/10, step 394/574 completed (loss: 0.5427598357200623, acc: 0.8181818127632141)
[2024-11-08 03:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:06][root][INFO] - Training Epoch: 2/10, step 395/574 completed (loss: 0.19327931106090546, acc: 0.9523809552192688)
[2024-11-08 03:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:06][root][INFO] - Training Epoch: 2/10, step 396/574 completed (loss: 0.2688092589378357, acc: 0.8620689511299133)
[2024-11-08 03:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:07][root][INFO] - Training Epoch: 2/10, step 397/574 completed (loss: 0.4483775496482849, acc: 0.9333333373069763)
[2024-11-08 03:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:07][root][INFO] - Training Epoch: 2/10, step 398/574 completed (loss: 0.2884569764137268, acc: 0.9047619104385376)
[2024-11-08 03:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:08][root][INFO] - Training Epoch: 2/10, step 399/574 completed (loss: 0.4157230854034424, acc: 0.8888888955116272)
[2024-11-08 03:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:08][root][INFO] - Training Epoch: 2/10, step 400/574 completed (loss: 0.2351701706647873, acc: 0.8636363744735718)
[2024-11-08 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:09][root][INFO] - Training Epoch: 2/10, step 401/574 completed (loss: 0.04508440941572189, acc: 1.0)
[2024-11-08 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:09][root][INFO] - Training Epoch: 2/10, step 402/574 completed (loss: 0.14378826320171356, acc: 0.9583333134651184)
[2024-11-08 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:10][root][INFO] - Training Epoch: 2/10, step 403/574 completed (loss: 0.13831232488155365, acc: 0.96875)
[2024-11-08 03:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:10][root][INFO] - Training Epoch: 2/10, step 404/574 completed (loss: 0.2652166783809662, acc: 0.9142857193946838)
[2024-11-08 03:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:10][root][INFO] - Training Epoch: 2/10, step 405/574 completed (loss: 0.2554088532924652, acc: 0.9230769276618958)
[2024-11-08 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:11][root][INFO] - Training Epoch: 2/10, step 406/574 completed (loss: 0.41266605257987976, acc: 0.8571428656578064)
[2024-11-08 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:11][root][INFO] - Training Epoch: 2/10, step 407/574 completed (loss: 0.3625207245349884, acc: 0.8421052694320679)
[2024-11-08 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:12][root][INFO] - Training Epoch: 2/10, step 408/574 completed (loss: 0.26611313223838806, acc: 0.8500000238418579)
[2024-11-08 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:12][root][INFO] - Training Epoch: 2/10, step 409/574 completed (loss: 0.289348840713501, acc: 0.9047619104385376)
[2024-11-08 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:12][root][INFO] - Training Epoch: 2/10, step 410/574 completed (loss: 0.21585144102573395, acc: 0.9545454382896423)
[2024-11-08 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:13][root][INFO] - Training Epoch: 2/10, step 411/574 completed (loss: 0.46774935722351074, acc: 0.8928571343421936)
[2024-11-08 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:13][root][INFO] - Training Epoch: 2/10, step 412/574 completed (loss: 0.14632956683635712, acc: 0.9629629850387573)
[2024-11-08 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:13][root][INFO] - Training Epoch: 2/10, step 413/574 completed (loss: 0.08149408549070358, acc: 0.9714285731315613)
[2024-11-08 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:14][root][INFO] - Training Epoch: 2/10, step 414/574 completed (loss: 0.23095253109931946, acc: 0.9230769276618958)
[2024-11-08 03:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:14][root][INFO] - Training Epoch: 2/10, step 415/574 completed (loss: 0.16401760280132294, acc: 0.9047619104385376)
[2024-11-08 03:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:14][root][INFO] - Training Epoch: 2/10, step 416/574 completed (loss: 0.10464382916688919, acc: 1.0)
[2024-11-08 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:15][root][INFO] - Training Epoch: 2/10, step 417/574 completed (loss: 0.19045019149780273, acc: 0.8947368264198303)
[2024-11-08 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:15][root][INFO] - Training Epoch: 2/10, step 418/574 completed (loss: 0.18449009954929352, acc: 0.9090909361839294)
[2024-11-08 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:16][root][INFO] - Training Epoch: 2/10, step 419/574 completed (loss: 0.47103655338287354, acc: 0.9047619104385376)
[2024-11-08 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:16][root][INFO] - Training Epoch: 2/10, step 420/574 completed (loss: 0.10496234148740768, acc: 1.0)
[2024-11-08 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:16][root][INFO] - Training Epoch: 2/10, step 421/574 completed (loss: 0.257707804441452, acc: 0.9354838728904724)
[2024-11-08 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:17][root][INFO] - Training Epoch: 2/10, step 422/574 completed (loss: 0.32251057028770447, acc: 0.8387096524238586)
[2024-11-08 03:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:17][root][INFO] - Training Epoch: 2/10, step 423/574 completed (loss: 0.4820467531681061, acc: 0.8461538553237915)
[2024-11-08 03:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:17][root][INFO] - Training Epoch: 2/10, step 424/574 completed (loss: 0.161360502243042, acc: 0.9047619104385376)
[2024-11-08 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:18][root][INFO] - Training Epoch: 2/10, step 425/574 completed (loss: 0.11497024446725845, acc: 1.0)
[2024-11-08 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:18][root][INFO] - Training Epoch: 2/10, step 426/574 completed (loss: 0.1978221982717514, acc: 0.9473684430122375)
[2024-11-08 03:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:03][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3010, device='cuda:0') eval_epoch_loss=tensor(0.2631, device='cuda:0') eval_epoch_acc=tensor(0.9105, device='cuda:0')
[2024-11-08 03:39:03][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:39:03][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:39:07][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_2_step_427_loss_0.2631354331970215/model.pt
[2024-11-08 03:39:07][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:39:07][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.2631354331970215
[2024-11-08 03:39:07][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9104732275009155
[2024-11-08 03:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:07][root][INFO] - Training Epoch: 2/10, step 427/574 completed (loss: 0.28025391697883606, acc: 0.9090909361839294)
[2024-11-08 03:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:08][root][INFO] - Training Epoch: 2/10, step 428/574 completed (loss: 0.1161118596792221, acc: 0.9523809552192688)
[2024-11-08 03:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:08][root][INFO] - Training Epoch: 2/10, step 429/574 completed (loss: 0.3168807327747345, acc: 0.9166666865348816)
[2024-11-08 03:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:08][root][INFO] - Training Epoch: 2/10, step 430/574 completed (loss: 0.009150223806500435, acc: 1.0)
[2024-11-08 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:09][root][INFO] - Training Epoch: 2/10, step 431/574 completed (loss: 0.06366203725337982, acc: 0.9677419066429138)
[2024-11-08 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:09][root][INFO] - Training Epoch: 2/10, step 432/574 completed (loss: 0.07759150862693787, acc: 0.9615384340286255)
[2024-11-08 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:10][root][INFO] - Training Epoch: 2/10, step 433/574 completed (loss: 0.10656221956014633, acc: 0.9523809552192688)
[2024-11-08 03:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:10][root][INFO] - Training Epoch: 2/10, step 434/574 completed (loss: 0.17847737669944763, acc: 0.9473684430122375)
[2024-11-08 03:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:10][root][INFO] - Training Epoch: 2/10, step 435/574 completed (loss: 0.3574272096157074, acc: 0.9473684430122375)
[2024-11-08 03:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:11][root][INFO] - Training Epoch: 2/10, step 436/574 completed (loss: 0.19523678719997406, acc: 0.9090909361839294)
[2024-11-08 03:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:11][root][INFO] - Training Epoch: 2/10, step 437/574 completed (loss: 0.0680907592177391, acc: 1.0)
[2024-11-08 03:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:11][root][INFO] - Training Epoch: 2/10, step 438/574 completed (loss: 0.19776898622512817, acc: 0.9545454382896423)
[2024-11-08 03:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:12][root][INFO] - Training Epoch: 2/10, step 439/574 completed (loss: 0.16830338537693024, acc: 0.9285714030265808)
[2024-11-08 03:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:12][root][INFO] - Training Epoch: 2/10, step 440/574 completed (loss: 0.08065136522054672, acc: 1.0)
[2024-11-08 03:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:13][root][INFO] - Training Epoch: 2/10, step 441/574 completed (loss: 0.6037688255310059, acc: 0.7894737124443054)
[2024-11-08 03:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:14][root][INFO] - Training Epoch: 2/10, step 442/574 completed (loss: 0.20275455713272095, acc: 0.9473684430122375)
[2024-11-08 03:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:15][root][INFO] - Training Epoch: 2/10, step 443/574 completed (loss: 0.2011670470237732, acc: 0.9545454382896423)
[2024-11-08 03:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:15][root][INFO] - Training Epoch: 2/10, step 444/574 completed (loss: 0.3451368808746338, acc: 0.8571428656578064)
[2024-11-08 03:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:16][root][INFO] - Training Epoch: 2/10, step 445/574 completed (loss: 0.05061953887343407, acc: 1.0)
[2024-11-08 03:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:16][root][INFO] - Training Epoch: 2/10, step 446/574 completed (loss: 0.27869775891304016, acc: 0.9354838728904724)
[2024-11-08 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:17][root][INFO] - Training Epoch: 2/10, step 447/574 completed (loss: 0.20834940671920776, acc: 0.9354838728904724)
[2024-11-08 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:17][root][INFO] - Training Epoch: 2/10, step 448/574 completed (loss: 0.06882362812757492, acc: 1.0)
[2024-11-08 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:18][root][INFO] - Training Epoch: 2/10, step 449/574 completed (loss: 0.060375750064849854, acc: 1.0)
[2024-11-08 03:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:18][root][INFO] - Training Epoch: 2/10, step 450/574 completed (loss: 0.07739560306072235, acc: 1.0)
[2024-11-08 03:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:18][root][INFO] - Training Epoch: 2/10, step 451/574 completed (loss: 0.12016593664884567, acc: 0.9473684430122375)
[2024-11-08 03:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:19][root][INFO] - Training Epoch: 2/10, step 452/574 completed (loss: 0.233279287815094, acc: 0.9545454382896423)
[2024-11-08 03:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:19][root][INFO] - Training Epoch: 2/10, step 453/574 completed (loss: 0.29949361085891724, acc: 0.8999999761581421)
[2024-11-08 03:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:20][root][INFO] - Training Epoch: 2/10, step 454/574 completed (loss: 0.053166210651397705, acc: 1.0)
[2024-11-08 03:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:20][root][INFO] - Training Epoch: 2/10, step 455/574 completed (loss: 0.18322694301605225, acc: 0.9354838728904724)
[2024-11-08 03:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:20][root][INFO] - Training Epoch: 2/10, step 456/574 completed (loss: 0.18979035317897797, acc: 0.949999988079071)
[2024-11-08 03:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:21][root][INFO] - Training Epoch: 2/10, step 457/574 completed (loss: 0.10904113203287125, acc: 0.949999988079071)
[2024-11-08 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:21][root][INFO] - Training Epoch: 2/10, step 458/574 completed (loss: 0.23997457325458527, acc: 0.8947368264198303)
[2024-11-08 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:21][root][INFO] - Training Epoch: 2/10, step 459/574 completed (loss: 0.23226839303970337, acc: 0.8636363744735718)
[2024-11-08 03:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:22][root][INFO] - Training Epoch: 2/10, step 460/574 completed (loss: 0.2372351586818695, acc: 0.949999988079071)
[2024-11-08 03:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:22][root][INFO] - Training Epoch: 2/10, step 461/574 completed (loss: 0.025094611570239067, acc: 1.0)
[2024-11-08 03:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:23][root][INFO] - Training Epoch: 2/10, step 462/574 completed (loss: 0.20187856256961823, acc: 0.9696969985961914)
[2024-11-08 03:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:23][root][INFO] - Training Epoch: 2/10, step 463/574 completed (loss: 0.2479938268661499, acc: 0.8888888955116272)
[2024-11-08 03:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:23][root][INFO] - Training Epoch: 2/10, step 464/574 completed (loss: 0.11224863678216934, acc: 0.9696969985961914)
[2024-11-08 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:24][root][INFO] - Training Epoch: 2/10, step 465/574 completed (loss: 0.09938773512840271, acc: 0.949999988079071)
[2024-11-08 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:24][root][INFO] - Training Epoch: 2/10, step 466/574 completed (loss: 0.06382694840431213, acc: 1.0)
[2024-11-08 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:25][root][INFO] - Training Epoch: 2/10, step 467/574 completed (loss: 0.27445560693740845, acc: 0.8947368264198303)
[2024-11-08 03:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:25][root][INFO] - Training Epoch: 2/10, step 468/574 completed (loss: 0.024291668087244034, acc: 1.0)
[2024-11-08 03:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:25][root][INFO] - Training Epoch: 2/10, step 469/574 completed (loss: 0.11138477921485901, acc: 0.9523809552192688)
[2024-11-08 03:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:26][root][INFO] - Training Epoch: 2/10, step 470/574 completed (loss: 0.07953683286905289, acc: 0.9583333134651184)
[2024-11-08 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:26][root][INFO] - Training Epoch: 2/10, step 471/574 completed (loss: 0.2173515409231186, acc: 0.8999999761581421)
[2024-11-08 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:27][root][INFO] - Training Epoch: 2/10, step 472/574 completed (loss: 0.538000762462616, acc: 0.7916666865348816)
[2024-11-08 03:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:27][root][INFO] - Training Epoch: 2/10, step 473/574 completed (loss: 0.20450197160243988, acc: 0.9523809552192688)
[2024-11-08 03:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:28][root][INFO] - Training Epoch: 2/10, step 474/574 completed (loss: 0.15899541974067688, acc: 0.8947368264198303)
[2024-11-08 03:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:28][root][INFO] - Training Epoch: 2/10, step 475/574 completed (loss: 0.28289496898651123, acc: 0.8500000238418579)
[2024-11-08 03:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:28][root][INFO] - Training Epoch: 2/10, step 476/574 completed (loss: 0.23968875408172607, acc: 0.8571428656578064)
[2024-11-08 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:29][root][INFO] - Training Epoch: 2/10, step 477/574 completed (loss: 0.6093271970748901, acc: 0.8636363744735718)
[2024-11-08 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:29][root][INFO] - Training Epoch: 2/10, step 478/574 completed (loss: 0.05812453106045723, acc: 1.0)
[2024-11-08 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:30][root][INFO] - Training Epoch: 2/10, step 479/574 completed (loss: 0.3426482081413269, acc: 0.8709677457809448)
[2024-11-08 03:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:30][root][INFO] - Training Epoch: 2/10, step 480/574 completed (loss: 0.19734077155590057, acc: 0.9354838728904724)
[2024-11-08 03:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:30][root][INFO] - Training Epoch: 2/10, step 481/574 completed (loss: 0.3576323091983795, acc: 0.9047619104385376)
[2024-11-08 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:31][root][INFO] - Training Epoch: 2/10, step 482/574 completed (loss: 0.22069215774536133, acc: 0.8947368264198303)
[2024-11-08 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:31][root][INFO] - Training Epoch: 2/10, step 483/574 completed (loss: 0.3249590992927551, acc: 0.9090909361839294)
[2024-11-08 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:32][root][INFO] - Training Epoch: 2/10, step 484/574 completed (loss: 0.23305486142635345, acc: 0.9090909361839294)
[2024-11-08 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:32][root][INFO] - Training Epoch: 2/10, step 485/574 completed (loss: 0.1536496877670288, acc: 0.9642857313156128)
[2024-11-08 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:33][root][INFO] - Training Epoch: 2/10, step 486/574 completed (loss: 0.31327927112579346, acc: 0.9259259104728699)
[2024-11-08 03:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:33][root][INFO] - Training Epoch: 2/10, step 487/574 completed (loss: 0.32382938265800476, acc: 0.9142857193946838)
[2024-11-08 03:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:33][root][INFO] - Training Epoch: 2/10, step 488/574 completed (loss: 0.5254419445991516, acc: 0.807692289352417)
[2024-11-08 03:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:34][root][INFO] - Training Epoch: 2/10, step 489/574 completed (loss: 0.14063508808612823, acc: 0.9523809552192688)
[2024-11-08 03:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:34][root][INFO] - Training Epoch: 2/10, step 490/574 completed (loss: 0.3073078691959381, acc: 0.8947368264198303)
[2024-11-08 03:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:35][root][INFO] - Training Epoch: 2/10, step 491/574 completed (loss: 0.24169518053531647, acc: 0.8947368264198303)
[2024-11-08 03:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:35][root][INFO] - Training Epoch: 2/10, step 492/574 completed (loss: 0.14984852075576782, acc: 0.9545454382896423)
[2024-11-08 03:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:36][root][INFO] - Training Epoch: 2/10, step 493/574 completed (loss: 0.4228666424751282, acc: 0.8571428656578064)
[2024-11-08 03:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:36][root][INFO] - Training Epoch: 2/10, step 494/574 completed (loss: 0.3808894157409668, acc: 0.875)
[2024-11-08 03:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:37][root][INFO] - Training Epoch: 2/10, step 495/574 completed (loss: 0.3523539900779724, acc: 0.8928571343421936)
[2024-11-08 03:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:37][root][INFO] - Training Epoch: 2/10, step 496/574 completed (loss: 0.256822407245636, acc: 0.8999999761581421)
[2024-11-08 03:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:38][root][INFO] - Training Epoch: 2/10, step 497/574 completed (loss: 0.2539713978767395, acc: 0.949999988079071)
[2024-11-08 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:38][root][INFO] - Training Epoch: 2/10, step 498/574 completed (loss: 0.37742340564727783, acc: 0.8947368264198303)
[2024-11-08 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:39][root][INFO] - Training Epoch: 2/10, step 499/574 completed (loss: 0.26184579730033875, acc: 0.8636363744735718)
[2024-11-08 03:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:39][root][INFO] - Training Epoch: 2/10, step 500/574 completed (loss: 0.6465195417404175, acc: 0.75)
[2024-11-08 03:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:40][root][INFO] - Training Epoch: 2/10, step 501/574 completed (loss: 0.10694867372512817, acc: 0.9583333134651184)
[2024-11-08 03:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:40][root][INFO] - Training Epoch: 2/10, step 502/574 completed (loss: 0.1043681725859642, acc: 0.9677419066429138)
[2024-11-08 03:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:41][root][INFO] - Training Epoch: 2/10, step 503/574 completed (loss: 0.2119627296924591, acc: 0.9032257795333862)
[2024-11-08 03:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:41][root][INFO] - Training Epoch: 2/10, step 504/574 completed (loss: 0.15782392024993896, acc: 0.95652174949646)
[2024-11-08 03:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:41][root][INFO] - Training Epoch: 2/10, step 505/574 completed (loss: 0.24825981259346008, acc: 0.8571428656578064)
[2024-11-08 03:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:42][root][INFO] - Training Epoch: 2/10, step 506/574 completed (loss: 0.34707462787628174, acc: 0.8947368264198303)
[2024-11-08 03:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:43][root][INFO] - Training Epoch: 2/10, step 507/574 completed (loss: 0.09127651154994965, acc: 0.949999988079071)
[2024-11-08 03:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:44][root][INFO] - Training Epoch: 2/10, step 508/574 completed (loss: 0.5221500396728516, acc: 0.75)
[2024-11-08 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:44][root][INFO] - Training Epoch: 2/10, step 509/574 completed (loss: 0.23588787019252777, acc: 0.9090909361839294)
[2024-11-08 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:44][root][INFO] - Training Epoch: 2/10, step 510/574 completed (loss: 0.04261719807982445, acc: 1.0)
[2024-11-08 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:45][root][INFO] - Training Epoch: 2/10, step 511/574 completed (loss: 0.20696265995502472, acc: 0.9666666388511658)
[2024-11-08 03:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:48][root][INFO] - Training Epoch: 2/10, step 512/574 completed (loss: 0.25580522418022156, acc: 0.9285714030265808)
[2024-11-08 03:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:50][root][INFO] - Training Epoch: 2/10, step 513/574 completed (loss: 0.2675122320652008, acc: 0.8571428656578064)
[2024-11-08 03:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:50][root][INFO] - Training Epoch: 2/10, step 514/574 completed (loss: 0.27858230471611023, acc: 0.8947368264198303)
[2024-11-08 03:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:51][root][INFO] - Training Epoch: 2/10, step 515/574 completed (loss: 0.23647749423980713, acc: 0.8999999761581421)
[2024-11-08 03:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:52][root][INFO] - Training Epoch: 2/10, step 516/574 completed (loss: 0.12076979875564575, acc: 0.9047619104385376)
[2024-11-08 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:52][root][INFO] - Training Epoch: 2/10, step 517/574 completed (loss: 0.13592314720153809, acc: 0.9545454382896423)
[2024-11-08 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:53][root][INFO] - Training Epoch: 2/10, step 518/574 completed (loss: 0.13546746969223022, acc: 0.9285714030265808)
[2024-11-08 03:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:53][root][INFO] - Training Epoch: 2/10, step 519/574 completed (loss: 0.2304757982492447, acc: 0.9259259104728699)
[2024-11-08 03:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:53][root][INFO] - Training Epoch: 2/10, step 520/574 completed (loss: 0.15904656052589417, acc: 0.9428571462631226)
[2024-11-08 03:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:55][root][INFO] - Training Epoch: 2/10, step 521/574 completed (loss: 0.242276132106781, acc: 0.9130434989929199)
[2024-11-08 03:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:55][root][INFO] - Training Epoch: 2/10, step 522/574 completed (loss: 0.09852363914251328, acc: 1.0)
[2024-11-08 03:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:56][root][INFO] - Training Epoch: 2/10, step 523/574 completed (loss: 0.4208913743495941, acc: 0.7894737124443054)
[2024-11-08 03:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:57][root][INFO] - Training Epoch: 2/10, step 524/574 completed (loss: 0.311545729637146, acc: 0.9090909361839294)
[2024-11-08 03:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:57][root][INFO] - Training Epoch: 2/10, step 525/574 completed (loss: 0.2412228286266327, acc: 0.8999999761581421)
[2024-11-08 03:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:58][root][INFO] - Training Epoch: 2/10, step 526/574 completed (loss: 0.2840063273906708, acc: 0.9090909361839294)
[2024-11-08 03:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:58][root][INFO] - Training Epoch: 2/10, step 527/574 completed (loss: 0.3525485396385193, acc: 0.939393937587738)
[2024-11-08 03:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:59][root][INFO] - Training Epoch: 2/10, step 528/574 completed (loss: 0.4765672981739044, acc: 0.8571428656578064)
[2024-11-08 03:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:59][root][INFO] - Training Epoch: 2/10, step 529/574 completed (loss: 0.12172971665859222, acc: 0.9523809552192688)
[2024-11-08 03:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:39:59][root][INFO] - Training Epoch: 2/10, step 530/574 completed (loss: 0.4686001241207123, acc: 0.8947368264198303)
[2024-11-08 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:00][root][INFO] - Training Epoch: 2/10, step 531/574 completed (loss: 0.35623854398727417, acc: 0.800000011920929)
[2024-11-08 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:00][root][INFO] - Training Epoch: 2/10, step 532/574 completed (loss: 0.5957847833633423, acc: 0.8095238208770752)
[2024-11-08 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:01][root][INFO] - Training Epoch: 2/10, step 533/574 completed (loss: 0.17341358959674835, acc: 0.9090909361839294)
[2024-11-08 03:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:01][root][INFO] - Training Epoch: 2/10, step 534/574 completed (loss: 0.23963813483715057, acc: 0.9285714030265808)
[2024-11-08 03:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:01][root][INFO] - Training Epoch: 2/10, step 535/574 completed (loss: 0.1519230604171753, acc: 0.9259259104728699)
[2024-11-08 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:02][root][INFO] - Training Epoch: 2/10, step 536/574 completed (loss: 0.526242196559906, acc: 0.8399999737739563)
[2024-11-08 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:02][root][INFO] - Training Epoch: 2/10, step 537/574 completed (loss: 0.2876364588737488, acc: 0.9047619104385376)
[2024-11-08 03:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:03][root][INFO] - Training Epoch: 2/10, step 538/574 completed (loss: 0.40050339698791504, acc: 0.8421052694320679)
[2024-11-08 03:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:04][root][INFO] - Training Epoch: 2/10, step 539/574 completed (loss: 0.19890812039375305, acc: 0.8999999761581421)
[2024-11-08 03:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:04][root][INFO] - Training Epoch: 2/10, step 540/574 completed (loss: 0.2679212689399719, acc: 0.8571428656578064)
[2024-11-08 03:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:04][root][INFO] - Training Epoch: 2/10, step 541/574 completed (loss: 0.14341050386428833, acc: 0.9523809552192688)
[2024-11-08 03:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:05][root][INFO] - Training Epoch: 2/10, step 542/574 completed (loss: 0.16786162555217743, acc: 0.939393937587738)
[2024-11-08 03:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:05][root][INFO] - Training Epoch: 2/10, step 543/574 completed (loss: 0.06935299932956696, acc: 0.9629629850387573)
[2024-11-08 03:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:05][root][INFO] - Training Epoch: 2/10, step 544/574 completed (loss: 0.2385217249393463, acc: 0.8787878751754761)
[2024-11-08 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:06][root][INFO] - Training Epoch: 2/10, step 545/574 completed (loss: 0.02585279382765293, acc: 1.0)
[2024-11-08 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:06][root][INFO] - Training Epoch: 2/10, step 546/574 completed (loss: 0.17084769904613495, acc: 0.8999999761581421)
[2024-11-08 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:07][root][INFO] - Training Epoch: 2/10, step 547/574 completed (loss: 0.1737125664949417, acc: 0.9523809552192688)
[2024-11-08 03:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:07][root][INFO] - Training Epoch: 2/10, step 548/574 completed (loss: 0.175211101770401, acc: 0.9523809552192688)
[2024-11-08 03:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:07][root][INFO] - Training Epoch: 2/10, step 549/574 completed (loss: 0.0636555626988411, acc: 1.0)
[2024-11-08 03:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:08][root][INFO] - Training Epoch: 2/10, step 550/574 completed (loss: 0.13687048852443695, acc: 0.9655172228813171)
[2024-11-08 03:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:08][root][INFO] - Training Epoch: 2/10, step 551/574 completed (loss: 0.12113670259714127, acc: 0.9677419066429138)
[2024-11-08 03:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:08][root][INFO] - Training Epoch: 2/10, step 552/574 completed (loss: 0.066340371966362, acc: 1.0)
[2024-11-08 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:09][root][INFO] - Training Epoch: 2/10, step 553/574 completed (loss: 0.41098514199256897, acc: 0.8571428656578064)
[2024-11-08 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:09][root][INFO] - Training Epoch: 2/10, step 554/574 completed (loss: 0.024699857458472252, acc: 1.0)
[2024-11-08 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:10][root][INFO] - Training Epoch: 2/10, step 555/574 completed (loss: 0.25321856141090393, acc: 0.8999999761581421)
[2024-11-08 03:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:10][root][INFO] - Training Epoch: 2/10, step 556/574 completed (loss: 0.18142294883728027, acc: 0.9523809552192688)
[2024-11-08 03:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:11][root][INFO] - Training Epoch: 2/10, step 557/574 completed (loss: 0.05156595632433891, acc: 1.0)
[2024-11-08 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:11][root][INFO] - Training Epoch: 2/10, step 558/574 completed (loss: 0.14401604235172272, acc: 0.9285714030265808)
[2024-11-08 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:11][root][INFO] - Training Epoch: 2/10, step 559/574 completed (loss: 0.1119273230433464, acc: 0.9629629850387573)
[2024-11-08 03:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:12][root][INFO] - Training Epoch: 2/10, step 560/574 completed (loss: 0.1791723072528839, acc: 0.939393937587738)
[2024-11-08 03:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:12][root][INFO] - Training Epoch: 2/10, step 561/574 completed (loss: 0.1596224159002304, acc: 1.0)
[2024-11-08 03:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:13][root][INFO] - Training Epoch: 2/10, step 562/574 completed (loss: 0.034817781299352646, acc: 1.0)
[2024-11-08 03:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:13][root][INFO] - Training Epoch: 2/10, step 563/574 completed (loss: 0.2848338782787323, acc: 0.8333333134651184)
[2024-11-08 03:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:14][root][INFO] - Training Epoch: 2/10, step 564/574 completed (loss: 0.35074731707572937, acc: 0.9545454382896423)
[2024-11-08 03:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:14][root][INFO] - Training Epoch: 2/10, step 565/574 completed (loss: 0.45046454668045044, acc: 0.800000011920929)
[2024-11-08 03:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:14][root][INFO] - Training Epoch: 2/10, step 566/574 completed (loss: 0.08543862402439117, acc: 0.9545454382896423)
[2024-11-08 03:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:15][root][INFO] - Training Epoch: 2/10, step 567/574 completed (loss: 0.10553810000419617, acc: 0.9629629850387573)
[2024-11-08 03:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:15][root][INFO] - Training Epoch: 2/10, step 568/574 completed (loss: 0.03282836079597473, acc: 1.0)
[2024-11-08 03:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:16][root][INFO] - Training Epoch: 2/10, step 569/574 completed (loss: 0.07072871923446655, acc: 1.0)
[2024-11-08 03:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:00][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2990, device='cuda:0') eval_epoch_loss=tensor(0.2616, device='cuda:0') eval_epoch_acc=tensor(0.9137, device='cuda:0')
[2024-11-08 03:41:00][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:41:00][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:41:03][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_2_step_570_loss_0.2616066038608551/model.pt
[2024-11-08 03:41:04][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:41:04][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.2616066038608551
[2024-11-08 03:41:04][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9137105941772461
[2024-11-08 03:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:04][root][INFO] - Training Epoch: 2/10, step 570/574 completed (loss: 0.004687374923378229, acc: 1.0)
[2024-11-08 03:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:04][root][INFO] - Training Epoch: 2/10, step 571/574 completed (loss: 0.2245510071516037, acc: 0.8421052694320679)
[2024-11-08 03:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:05][root][INFO] - Training Epoch: 2/10, step 572/574 completed (loss: 0.13895438611507416, acc: 0.9545454382896423)
[2024-11-08 03:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:05][root][INFO] - Training Epoch: 2/10, step 573/574 completed (loss: 0.5071654915809631, acc: 0.8500000238418579)
[2024-11-08 03:41:06][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.3522, train_epoch_loss=0.3017, epoch time 479.84302877634764s
[2024-11-08 03:41:06][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 03:41:06][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2024-11-08 03:41:06][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 03:41:06][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 1
[2024-11-08 03:41:06][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 03:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:06][root][INFO] - Training Epoch: 3/10, step 0/574 completed (loss: 0.18094588816165924, acc: 0.9285714030265808)
[2024-11-08 03:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:07][root][INFO] - Training Epoch: 3/10, step 1/574 completed (loss: 0.060947660356760025, acc: 1.0)
[2024-11-08 03:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:07][root][INFO] - Training Epoch: 3/10, step 2/574 completed (loss: 0.1610727608203888, acc: 0.9142857193946838)
[2024-11-08 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:08][root][INFO] - Training Epoch: 3/10, step 3/574 completed (loss: 0.16463246941566467, acc: 0.9523809552192688)
[2024-11-08 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:08][root][INFO] - Training Epoch: 3/10, step 4/574 completed (loss: 0.060572776943445206, acc: 1.0)
[2024-11-08 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:09][root][INFO] - Training Epoch: 3/10, step 5/574 completed (loss: 0.3038867115974426, acc: 0.9473684430122375)
[2024-11-08 03:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:09][root][INFO] - Training Epoch: 3/10, step 6/574 completed (loss: 0.025482922792434692, acc: 1.0)
[2024-11-08 03:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:09][root][INFO] - Training Epoch: 3/10, step 7/574 completed (loss: 0.13415643572807312, acc: 0.9523809552192688)
[2024-11-08 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:10][root][INFO] - Training Epoch: 3/10, step 8/574 completed (loss: 0.10903245955705643, acc: 0.9583333134651184)
[2024-11-08 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:10][root][INFO] - Training Epoch: 3/10, step 9/574 completed (loss: 0.1506204754114151, acc: 0.9354838728904724)
[2024-11-08 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:11][root][INFO] - Training Epoch: 3/10, step 10/574 completed (loss: 0.030766170471906662, acc: 1.0)
[2024-11-08 03:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:11][root][INFO] - Training Epoch: 3/10, step 11/574 completed (loss: 0.1565885990858078, acc: 0.9599999785423279)
[2024-11-08 03:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:11][root][INFO] - Training Epoch: 3/10, step 12/574 completed (loss: 0.08713620156049728, acc: 0.949999988079071)
[2024-11-08 03:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:12][root][INFO] - Training Epoch: 3/10, step 13/574 completed (loss: 0.3053976595401764, acc: 0.8421052694320679)
[2024-11-08 03:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:12][root][INFO] - Training Epoch: 3/10, step 14/574 completed (loss: 0.11003147065639496, acc: 0.95652174949646)
[2024-11-08 03:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:13][root][INFO] - Training Epoch: 3/10, step 15/574 completed (loss: 0.025749092921614647, acc: 1.0)
[2024-11-08 03:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:13][root][INFO] - Training Epoch: 3/10, step 16/574 completed (loss: 0.15872231125831604, acc: 0.9642857313156128)
[2024-11-08 03:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:14][root][INFO] - Training Epoch: 3/10, step 17/574 completed (loss: 0.10571151971817017, acc: 0.9642857313156128)
[2024-11-08 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:14][root][INFO] - Training Epoch: 3/10, step 18/574 completed (loss: 0.08246427029371262, acc: 1.0)
[2024-11-08 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:14][root][INFO] - Training Epoch: 3/10, step 19/574 completed (loss: 0.3355642855167389, acc: 0.8421052694320679)
[2024-11-08 03:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:15][root][INFO] - Training Epoch: 3/10, step 20/574 completed (loss: 0.23876619338989258, acc: 0.8947368264198303)
[2024-11-08 03:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:15][root][INFO] - Training Epoch: 3/10, step 21/574 completed (loss: 0.5844717621803284, acc: 0.8181818127632141)
[2024-11-08 03:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:16][root][INFO] - Training Epoch: 3/10, step 22/574 completed (loss: 0.26647835969924927, acc: 0.9523809552192688)
[2024-11-08 03:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:16][root][INFO] - Training Epoch: 3/10, step 23/574 completed (loss: 0.024772385135293007, acc: 1.0)
[2024-11-08 03:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:17][root][INFO] - Training Epoch: 3/10, step 24/574 completed (loss: 0.2792383134365082, acc: 0.9333333373069763)
[2024-11-08 03:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:17][root][INFO] - Training Epoch: 3/10, step 25/574 completed (loss: 0.29586732387542725, acc: 0.9047619104385376)
[2024-11-08 03:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:17][root][INFO] - Training Epoch: 3/10, step 26/574 completed (loss: 0.3764328360557556, acc: 0.8571428656578064)
[2024-11-08 03:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:19][root][INFO] - Training Epoch: 3/10, step 27/574 completed (loss: 0.3182734549045563, acc: 0.8947368264198303)
[2024-11-08 03:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:20][root][INFO] - Training Epoch: 3/10, step 28/574 completed (loss: 0.132445827126503, acc: 0.9545454382896423)
[2024-11-08 03:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:20][root][INFO] - Training Epoch: 3/10, step 29/574 completed (loss: 0.46962714195251465, acc: 0.800000011920929)
[2024-11-08 03:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:20][root][INFO] - Training Epoch: 3/10, step 30/574 completed (loss: 0.2926901578903198, acc: 0.9090909361839294)
[2024-11-08 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:21][root][INFO] - Training Epoch: 3/10, step 31/574 completed (loss: 0.197839617729187, acc: 0.931034505367279)
[2024-11-08 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:21][root][INFO] - Training Epoch: 3/10, step 32/574 completed (loss: 0.30536407232284546, acc: 0.9090909361839294)
[2024-11-08 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:22][root][INFO] - Training Epoch: 3/10, step 33/574 completed (loss: 0.12602181732654572, acc: 0.95652174949646)
[2024-11-08 03:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:22][root][INFO] - Training Epoch: 3/10, step 34/574 completed (loss: 0.05893273651599884, acc: 1.0)
[2024-11-08 03:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:23][root][INFO] - Training Epoch: 3/10, step 35/574 completed (loss: 0.22103673219680786, acc: 0.8947368264198303)
[2024-11-08 03:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:23][root][INFO] - Training Epoch: 3/10, step 36/574 completed (loss: 0.10898927599191666, acc: 1.0)
[2024-11-08 03:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:23][root][INFO] - Training Epoch: 3/10, step 37/574 completed (loss: 0.18844440579414368, acc: 0.9523809552192688)
[2024-11-08 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:24][root][INFO] - Training Epoch: 3/10, step 38/574 completed (loss: 0.01271430030465126, acc: 1.0)
[2024-11-08 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:24][root][INFO] - Training Epoch: 3/10, step 39/574 completed (loss: 0.17963720858097076, acc: 0.9285714030265808)
[2024-11-08 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:25][root][INFO] - Training Epoch: 3/10, step 40/574 completed (loss: 0.34055787324905396, acc: 0.8857142925262451)
[2024-11-08 03:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:25][root][INFO] - Training Epoch: 3/10, step 41/574 completed (loss: 0.21998202800750732, acc: 0.949999988079071)
[2024-11-08 03:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:26][root][INFO] - Training Epoch: 3/10, step 42/574 completed (loss: 0.1914396733045578, acc: 0.8999999761581421)
[2024-11-08 03:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:26][root][INFO] - Training Epoch: 3/10, step 43/574 completed (loss: 0.5791688561439514, acc: 0.7894737124443054)
[2024-11-08 03:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:27][root][INFO] - Training Epoch: 3/10, step 44/574 completed (loss: 0.220284566283226, acc: 0.9090909361839294)
[2024-11-08 03:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:27][root][INFO] - Training Epoch: 3/10, step 45/574 completed (loss: 0.18307320773601532, acc: 0.9047619104385376)
[2024-11-08 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:28][root][INFO] - Training Epoch: 3/10, step 46/574 completed (loss: 0.1265421211719513, acc: 0.9166666865348816)
[2024-11-08 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:28][root][INFO] - Training Epoch: 3/10, step 47/574 completed (loss: 0.06470061838626862, acc: 0.9677419066429138)
[2024-11-08 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:29][root][INFO] - Training Epoch: 3/10, step 48/574 completed (loss: 0.027741121128201485, acc: 1.0)
[2024-11-08 03:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:29][root][INFO] - Training Epoch: 3/10, step 49/574 completed (loss: 0.02065221592783928, acc: 1.0)
[2024-11-08 03:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:30][root][INFO] - Training Epoch: 3/10, step 50/574 completed (loss: 0.3247920274734497, acc: 0.9523809552192688)
[2024-11-08 03:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:30][root][INFO] - Training Epoch: 3/10, step 51/574 completed (loss: 0.2725611925125122, acc: 0.8947368264198303)
[2024-11-08 03:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:30][root][INFO] - Training Epoch: 3/10, step 52/574 completed (loss: 0.5035750865936279, acc: 0.8999999761581421)
[2024-11-08 03:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:31][root][INFO] - Training Epoch: 3/10, step 53/574 completed (loss: 0.4164232015609741, acc: 0.9047619104385376)
[2024-11-08 03:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:31][root][INFO] - Training Epoch: 3/10, step 54/574 completed (loss: 0.28560975193977356, acc: 0.9545454382896423)
[2024-11-08 03:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:32][root][INFO] - Training Epoch: 3/10, step 55/574 completed (loss: 0.020306434482336044, acc: 1.0)
[2024-11-08 03:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:36][root][INFO] - Training Epoch: 3/10, step 56/574 completed (loss: 0.21467356383800507, acc: 0.9047619104385376)
[2024-11-08 03:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:38][root][INFO] - Training Epoch: 3/10, step 57/574 completed (loss: 0.40891656279563904, acc: 0.8571428656578064)
[2024-11-08 03:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:39][root][INFO] - Training Epoch: 3/10, step 58/574 completed (loss: 0.17061299085617065, acc: 0.9473684430122375)
[2024-11-08 03:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:40][root][INFO] - Training Epoch: 3/10, step 59/574 completed (loss: 0.2633131146430969, acc: 0.9545454382896423)
[2024-11-08 03:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:40][root][INFO] - Training Epoch: 3/10, step 60/574 completed (loss: 0.41020867228507996, acc: 0.8421052694320679)
[2024-11-08 03:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:41][root][INFO] - Training Epoch: 3/10, step 61/574 completed (loss: 0.13061201572418213, acc: 0.9523809552192688)
[2024-11-08 03:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:41][root][INFO] - Training Epoch: 3/10, step 62/574 completed (loss: 0.10245189070701599, acc: 0.9722222089767456)
[2024-11-08 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:42][root][INFO] - Training Epoch: 3/10, step 63/574 completed (loss: 0.27531275153160095, acc: 0.8928571343421936)
[2024-11-08 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:42][root][INFO] - Training Epoch: 3/10, step 64/574 completed (loss: 0.45758065581321716, acc: 0.8571428656578064)
[2024-11-08 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:43][root][INFO] - Training Epoch: 3/10, step 65/574 completed (loss: 0.22362291812896729, acc: 0.8999999761581421)
[2024-11-08 03:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:43][root][INFO] - Training Epoch: 3/10, step 66/574 completed (loss: 0.3638926148414612, acc: 0.8421052694320679)
[2024-11-08 03:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:44][root][INFO] - Training Epoch: 3/10, step 67/574 completed (loss: 0.2983439862728119, acc: 0.949999988079071)
[2024-11-08 03:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:44][root][INFO] - Training Epoch: 3/10, step 68/574 completed (loss: 0.04891940578818321, acc: 1.0)
[2024-11-08 03:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:44][root][INFO] - Training Epoch: 3/10, step 69/574 completed (loss: 0.05241551995277405, acc: 0.9696969985961914)
[2024-11-08 03:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:45][root][INFO] - Training Epoch: 3/10, step 70/574 completed (loss: 0.1275912970304489, acc: 0.9629629850387573)
[2024-11-08 03:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:45][root][INFO] - Training Epoch: 3/10, step 71/574 completed (loss: 0.25739753246307373, acc: 0.8999999761581421)
[2024-11-08 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:46][root][INFO] - Training Epoch: 3/10, step 72/574 completed (loss: 0.10679591447114944, acc: 0.9523809552192688)
[2024-11-08 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:46][root][INFO] - Training Epoch: 3/10, step 73/574 completed (loss: 0.47045692801475525, acc: 0.7894737124443054)
[2024-11-08 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:47][root][INFO] - Training Epoch: 3/10, step 74/574 completed (loss: 0.25430068373680115, acc: 0.8500000238418579)
[2024-11-08 03:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:47][root][INFO] - Training Epoch: 3/10, step 75/574 completed (loss: 0.23400358855724335, acc: 0.9047619104385376)
[2024-11-08 03:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:48][root][INFO] - Training Epoch: 3/10, step 76/574 completed (loss: 0.1439937800168991, acc: 0.9090909361839294)
[2024-11-08 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:48][root][INFO] - Training Epoch: 3/10, step 77/574 completed (loss: 0.08815097063779831, acc: 0.9642857313156128)
[2024-11-08 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:48][root][INFO] - Training Epoch: 3/10, step 78/574 completed (loss: 0.061776090413331985, acc: 0.9629629850387573)
[2024-11-08 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:49][root][INFO] - Training Epoch: 3/10, step 79/574 completed (loss: 0.2684935927391052, acc: 0.9142857193946838)
[2024-11-08 03:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:49][root][INFO] - Training Epoch: 3/10, step 80/574 completed (loss: 0.04712825268507004, acc: 1.0)
[2024-11-08 03:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:50][root][INFO] - Training Epoch: 3/10, step 81/574 completed (loss: 0.19697821140289307, acc: 0.8571428656578064)
[2024-11-08 03:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:50][root][INFO] - Training Epoch: 3/10, step 82/574 completed (loss: 0.2667727470397949, acc: 0.9473684430122375)
[2024-11-08 03:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:51][root][INFO] - Training Epoch: 3/10, step 83/574 completed (loss: 0.10000261664390564, acc: 0.9473684430122375)
[2024-11-08 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:51][root][INFO] - Training Epoch: 3/10, step 84/574 completed (loss: 0.07381639629602432, acc: 0.9545454382896423)
[2024-11-08 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:51][root][INFO] - Training Epoch: 3/10, step 85/574 completed (loss: 0.11092974990606308, acc: 1.0)
[2024-11-08 03:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:52][root][INFO] - Training Epoch: 3/10, step 86/574 completed (loss: 0.04882828891277313, acc: 1.0)
[2024-11-08 03:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:53][root][INFO] - Training Epoch: 3/10, step 87/574 completed (loss: 0.281922847032547, acc: 0.807692289352417)
[2024-11-08 03:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:53][root][INFO] - Training Epoch: 3/10, step 88/574 completed (loss: 0.10069707781076431, acc: 0.9523809552192688)
[2024-11-08 03:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:55][root][INFO] - Training Epoch: 3/10, step 89/574 completed (loss: 0.3287174701690674, acc: 0.8421052694320679)
[2024-11-08 03:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:56][root][INFO] - Training Epoch: 3/10, step 90/574 completed (loss: 0.4950133264064789, acc: 0.7894737124443054)
[2024-11-08 03:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:57][root][INFO] - Training Epoch: 3/10, step 91/574 completed (loss: 0.2212466299533844, acc: 0.9090909361839294)
[2024-11-08 03:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:41:58][root][INFO] - Training Epoch: 3/10, step 92/574 completed (loss: 0.13330204784870148, acc: 0.9523809552192688)
[2024-11-08 03:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:00][root][INFO] - Training Epoch: 3/10, step 93/574 completed (loss: 0.6379544138908386, acc: 0.7307692170143127)
[2024-11-08 03:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:00][root][INFO] - Training Epoch: 3/10, step 94/574 completed (loss: 0.10923613607883453, acc: 0.9642857313156128)
[2024-11-08 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:01][root][INFO] - Training Epoch: 3/10, step 95/574 completed (loss: 0.26556843519210815, acc: 0.8571428656578064)
[2024-11-08 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:01][root][INFO] - Training Epoch: 3/10, step 96/574 completed (loss: 0.3570300340652466, acc: 0.8421052694320679)
[2024-11-08 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:02][root][INFO] - Training Epoch: 3/10, step 97/574 completed (loss: 0.19899877905845642, acc: 0.8999999761581421)
[2024-11-08 03:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:02][root][INFO] - Training Epoch: 3/10, step 98/574 completed (loss: 0.37579864263534546, acc: 0.8571428656578064)
[2024-11-08 03:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:03][root][INFO] - Training Epoch: 3/10, step 99/574 completed (loss: 0.30635568499565125, acc: 0.9090909361839294)
[2024-11-08 03:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:03][root][INFO] - Training Epoch: 3/10, step 100/574 completed (loss: 0.08520647138357162, acc: 0.9523809552192688)
[2024-11-08 03:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:04][root][INFO] - Training Epoch: 3/10, step 101/574 completed (loss: 0.11792697757482529, acc: 0.9599999785423279)
[2024-11-08 03:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:04][root][INFO] - Training Epoch: 3/10, step 102/574 completed (loss: 0.0345601961016655, acc: 1.0)
[2024-11-08 03:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:04][root][INFO] - Training Epoch: 3/10, step 103/574 completed (loss: 0.028353320434689522, acc: 1.0)
[2024-11-08 03:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:05][root][INFO] - Training Epoch: 3/10, step 104/574 completed (loss: 0.050475623458623886, acc: 1.0)
[2024-11-08 03:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:05][root][INFO] - Training Epoch: 3/10, step 105/574 completed (loss: 0.21359726786613464, acc: 0.8947368264198303)
[2024-11-08 03:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:06][root][INFO] - Training Epoch: 3/10, step 106/574 completed (loss: 0.25110653042793274, acc: 0.9090909361839294)
[2024-11-08 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:06][root][INFO] - Training Epoch: 3/10, step 107/574 completed (loss: 0.08012503385543823, acc: 0.9642857313156128)
[2024-11-08 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:07][root][INFO] - Training Epoch: 3/10, step 108/574 completed (loss: 0.04432239755988121, acc: 1.0)
[2024-11-08 03:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:07][root][INFO] - Training Epoch: 3/10, step 109/574 completed (loss: 0.32238537073135376, acc: 0.8571428656578064)
[2024-11-08 03:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:07][root][INFO] - Training Epoch: 3/10, step 110/574 completed (loss: 0.04087715968489647, acc: 1.0)
[2024-11-08 03:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:08][root][INFO] - Training Epoch: 3/10, step 111/574 completed (loss: 0.11868029832839966, acc: 0.9473684430122375)
[2024-11-08 03:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:09][root][INFO] - Training Epoch: 3/10, step 112/574 completed (loss: 0.026599375531077385, acc: 1.0)
[2024-11-08 03:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:09][root][INFO] - Training Epoch: 3/10, step 113/574 completed (loss: 0.022371714934706688, acc: 1.0)
[2024-11-08 03:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:10][root][INFO] - Training Epoch: 3/10, step 114/574 completed (loss: 0.025817591696977615, acc: 1.0)
[2024-11-08 03:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:10][root][INFO] - Training Epoch: 3/10, step 115/574 completed (loss: 0.06809063255786896, acc: 1.0)
[2024-11-08 03:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:10][root][INFO] - Training Epoch: 3/10, step 116/574 completed (loss: 0.18634693324565887, acc: 0.9200000166893005)
[2024-11-08 03:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:11][root][INFO] - Training Epoch: 3/10, step 117/574 completed (loss: 0.24761822819709778, acc: 0.8999999761581421)
[2024-11-08 03:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:11][root][INFO] - Training Epoch: 3/10, step 118/574 completed (loss: 0.1590641736984253, acc: 0.9473684430122375)
[2024-11-08 03:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:13][root][INFO] - Training Epoch: 3/10, step 119/574 completed (loss: 0.120620496571064, acc: 0.9090909361839294)
[2024-11-08 03:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:13][root][INFO] - Training Epoch: 3/10, step 120/574 completed (loss: 0.27630460262298584, acc: 0.8947368264198303)
[2024-11-08 03:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:14][root][INFO] - Training Epoch: 3/10, step 121/574 completed (loss: 0.23350538313388824, acc: 0.9090909361839294)
[2024-11-08 03:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:14][root][INFO] - Training Epoch: 3/10, step 122/574 completed (loss: 0.033703070133924484, acc: 1.0)
[2024-11-08 03:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:14][root][INFO] - Training Epoch: 3/10, step 123/574 completed (loss: 0.4303266406059265, acc: 0.8461538553237915)
[2024-11-08 03:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:15][root][INFO] - Training Epoch: 3/10, step 124/574 completed (loss: 0.2996240258216858, acc: 0.8999999761581421)
[2024-11-08 03:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:15][root][INFO] - Training Epoch: 3/10, step 125/574 completed (loss: 0.49554306268692017, acc: 0.8999999761581421)
[2024-11-08 03:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:16][root][INFO] - Training Epoch: 3/10, step 126/574 completed (loss: 0.27818477153778076, acc: 0.8947368264198303)
[2024-11-08 03:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:16][root][INFO] - Training Epoch: 3/10, step 127/574 completed (loss: 0.269778847694397, acc: 0.8636363744735718)
[2024-11-08 03:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:17][root][INFO] - Training Epoch: 3/10, step 128/574 completed (loss: 0.25836607813835144, acc: 0.8999999761581421)
[2024-11-08 03:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:17][root][INFO] - Training Epoch: 3/10, step 129/574 completed (loss: 0.06764733046293259, acc: 1.0)
[2024-11-08 03:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:18][root][INFO] - Training Epoch: 3/10, step 130/574 completed (loss: 0.14513473212718964, acc: 0.939393937587738)
[2024-11-08 03:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:18][root][INFO] - Training Epoch: 3/10, step 131/574 completed (loss: 0.2695651650428772, acc: 0.9259259104728699)
[2024-11-08 03:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:18][root][INFO] - Training Epoch: 3/10, step 132/574 completed (loss: 0.2882731258869171, acc: 0.9090909361839294)
[2024-11-08 03:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:19][root][INFO] - Training Epoch: 3/10, step 133/574 completed (loss: 0.3120217025279999, acc: 0.8500000238418579)
[2024-11-08 03:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:19][root][INFO] - Training Epoch: 3/10, step 134/574 completed (loss: 0.06415574997663498, acc: 0.949999988079071)
[2024-11-08 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:20][root][INFO] - Training Epoch: 3/10, step 135/574 completed (loss: 0.4986884891986847, acc: 0.7368420958518982)
[2024-11-08 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:20][root][INFO] - Training Epoch: 3/10, step 136/574 completed (loss: 0.18670901656150818, acc: 0.9545454382896423)
[2024-11-08 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:21][root][INFO] - Training Epoch: 3/10, step 137/574 completed (loss: 0.4984452724456787, acc: 0.8500000238418579)
[2024-11-08 03:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:21][root][INFO] - Training Epoch: 3/10, step 138/574 completed (loss: 0.18116943538188934, acc: 0.9545454382896423)
[2024-11-08 03:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:06][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2952, device='cuda:0') eval_epoch_loss=tensor(0.2587, device='cuda:0') eval_epoch_acc=tensor(0.9172, device='cuda:0')
[2024-11-08 03:43:06][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:43:06][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:43:09][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_3_step_139_loss_0.2586502730846405/model.pt
[2024-11-08 03:43:09][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:43:09][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.2586502730846405
[2024-11-08 03:43:09][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.9171586036682129
[2024-11-08 03:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:10][root][INFO] - Training Epoch: 3/10, step 139/574 completed (loss: 0.2261270135641098, acc: 0.939393937587738)
[2024-11-08 03:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:10][root][INFO] - Training Epoch: 3/10, step 140/574 completed (loss: 0.36048969626426697, acc: 0.8888888955116272)
[2024-11-08 03:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:11][root][INFO] - Training Epoch: 3/10, step 141/574 completed (loss: 0.19606707990169525, acc: 0.9090909361839294)
[2024-11-08 03:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:11][root][INFO] - Training Epoch: 3/10, step 142/574 completed (loss: 0.47268420457839966, acc: 0.8999999761581421)
[2024-11-08 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:12][root][INFO] - Training Epoch: 3/10, step 143/574 completed (loss: 0.2537834048271179, acc: 0.8999999761581421)
[2024-11-08 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:12][root][INFO] - Training Epoch: 3/10, step 144/574 completed (loss: 0.27754414081573486, acc: 0.8947368264198303)
[2024-11-08 03:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:13][root][INFO] - Training Epoch: 3/10, step 145/574 completed (loss: 0.23767124116420746, acc: 0.9090909361839294)
[2024-11-08 03:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:13][root][INFO] - Training Epoch: 3/10, step 146/574 completed (loss: 0.4932462275028229, acc: 0.8999999761581421)
[2024-11-08 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:14][root][INFO] - Training Epoch: 3/10, step 147/574 completed (loss: 0.18357309699058533, acc: 1.0)
[2024-11-08 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:14][root][INFO] - Training Epoch: 3/10, step 148/574 completed (loss: 0.2484005093574524, acc: 0.939393937587738)
[2024-11-08 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:15][root][INFO] - Training Epoch: 3/10, step 149/574 completed (loss: 0.16601698100566864, acc: 0.9259259104728699)
[2024-11-08 03:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:15][root][INFO] - Training Epoch: 3/10, step 150/574 completed (loss: 0.17754706740379333, acc: 0.9354838728904724)
[2024-11-08 03:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:15][root][INFO] - Training Epoch: 3/10, step 151/574 completed (loss: 0.21365217864513397, acc: 0.949999988079071)
[2024-11-08 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:16][root][INFO] - Training Epoch: 3/10, step 152/574 completed (loss: 0.11120037734508514, acc: 1.0)
[2024-11-08 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:16][root][INFO] - Training Epoch: 3/10, step 153/574 completed (loss: 0.14619839191436768, acc: 1.0)
[2024-11-08 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:17][root][INFO] - Training Epoch: 3/10, step 154/574 completed (loss: 0.0842069759964943, acc: 0.9545454382896423)
[2024-11-08 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:17][root][INFO] - Training Epoch: 3/10, step 155/574 completed (loss: 0.27578532695770264, acc: 0.8999999761581421)
[2024-11-08 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:17][root][INFO] - Training Epoch: 3/10, step 156/574 completed (loss: 0.20878595113754272, acc: 0.9545454382896423)
[2024-11-08 03:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:18][root][INFO] - Training Epoch: 3/10, step 157/574 completed (loss: 0.27061203122138977, acc: 0.9090909361839294)
[2024-11-08 03:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:21][root][INFO] - Training Epoch: 3/10, step 158/574 completed (loss: 0.38473737239837646, acc: 0.8500000238418579)
[2024-11-08 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:21][root][INFO] - Training Epoch: 3/10, step 159/574 completed (loss: 0.2203705757856369, acc: 0.8999999761581421)
[2024-11-08 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:22][root][INFO] - Training Epoch: 3/10, step 160/574 completed (loss: 0.3196238577365875, acc: 0.8421052694320679)
[2024-11-08 03:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:22][root][INFO] - Training Epoch: 3/10, step 161/574 completed (loss: 0.5735062956809998, acc: 0.8181818127632141)
[2024-11-08 03:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:23][root][INFO] - Training Epoch: 3/10, step 162/574 completed (loss: 0.3367420434951782, acc: 0.8500000238418579)
[2024-11-08 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:24][root][INFO] - Training Epoch: 3/10, step 163/574 completed (loss: 0.22603219747543335, acc: 0.9200000166893005)
[2024-11-08 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:24][root][INFO] - Training Epoch: 3/10, step 164/574 completed (loss: 0.11782386898994446, acc: 1.0)
[2024-11-08 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:25][root][INFO] - Training Epoch: 3/10, step 165/574 completed (loss: 0.1570262908935547, acc: 0.9444444179534912)
[2024-11-08 03:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:25][root][INFO] - Training Epoch: 3/10, step 166/574 completed (loss: 0.3395451009273529, acc: 0.9130434989929199)
[2024-11-08 03:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:25][root][INFO] - Training Epoch: 3/10, step 167/574 completed (loss: 0.14341489970684052, acc: 0.9047619104385376)
[2024-11-08 03:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:26][root][INFO] - Training Epoch: 3/10, step 168/574 completed (loss: 0.3075386583805084, acc: 0.9473684430122375)
[2024-11-08 03:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:26][root][INFO] - Training Epoch: 3/10, step 169/574 completed (loss: 0.36597540974617004, acc: 0.9090909361839294)
[2024-11-08 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:28][root][INFO] - Training Epoch: 3/10, step 170/574 completed (loss: 0.22811786830425262, acc: 0.8999999761581421)
[2024-11-08 03:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:28][root][INFO] - Training Epoch: 3/10, step 171/574 completed (loss: 0.051255617290735245, acc: 0.9545454382896423)
[2024-11-08 03:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:29][root][INFO] - Training Epoch: 3/10, step 172/574 completed (loss: 0.04445427656173706, acc: 0.9696969985961914)
[2024-11-08 03:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:29][root][INFO] - Training Epoch: 3/10, step 173/574 completed (loss: 0.07785365730524063, acc: 0.9666666388511658)
[2024-11-08 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:30][root][INFO] - Training Epoch: 3/10, step 174/574 completed (loss: 0.2611852288246155, acc: 0.8571428656578064)
[2024-11-08 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:31][root][INFO] - Training Epoch: 3/10, step 175/574 completed (loss: 0.285540908575058, acc: 0.9047619104385376)
[2024-11-08 03:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:31][root][INFO] - Training Epoch: 3/10, step 176/574 completed (loss: 0.24848401546478271, acc: 0.9473684430122375)
[2024-11-08 03:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:33][root][INFO] - Training Epoch: 3/10, step 177/574 completed (loss: 0.6169407963752747, acc: 0.9090909361839294)
[2024-11-08 03:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:34][root][INFO] - Training Epoch: 3/10, step 178/574 completed (loss: 0.2485562562942505, acc: 0.9473684430122375)
[2024-11-08 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:34][root][INFO] - Training Epoch: 3/10, step 179/574 completed (loss: 0.13848018646240234, acc: 0.9545454382896423)
[2024-11-08 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:34][root][INFO] - Training Epoch: 3/10, step 180/574 completed (loss: 0.00952867604792118, acc: 1.0)
[2024-11-08 03:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:35][root][INFO] - Training Epoch: 3/10, step 181/574 completed (loss: 0.014209591783583164, acc: 1.0)
[2024-11-08 03:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:35][root][INFO] - Training Epoch: 3/10, step 182/574 completed (loss: 0.11977764964103699, acc: 0.9714285731315613)
[2024-11-08 03:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:36][root][INFO] - Training Epoch: 3/10, step 183/574 completed (loss: 0.058624517172575, acc: 1.0)
[2024-11-08 03:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:36][root][INFO] - Training Epoch: 3/10, step 184/574 completed (loss: 0.16546432673931122, acc: 0.9047619104385376)
[2024-11-08 03:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:37][root][INFO] - Training Epoch: 3/10, step 185/574 completed (loss: 0.019928907975554466, acc: 1.0)
[2024-11-08 03:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:38][root][INFO] - Training Epoch: 3/10, step 186/574 completed (loss: 0.20597615838050842, acc: 0.8999999761581421)
[2024-11-08 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:38][root][INFO] - Training Epoch: 3/10, step 187/574 completed (loss: 0.19512778520584106, acc: 0.9047619104385376)
[2024-11-08 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:39][root][INFO] - Training Epoch: 3/10, step 188/574 completed (loss: 0.0729137510061264, acc: 1.0)
[2024-11-08 03:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:39][root][INFO] - Training Epoch: 3/10, step 189/574 completed (loss: 0.19837404787540436, acc: 0.9599999785423279)
[2024-11-08 03:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:40][root][INFO] - Training Epoch: 3/10, step 190/574 completed (loss: 0.38224202394485474, acc: 0.8500000238418579)
[2024-11-08 03:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:41][root][INFO] - Training Epoch: 3/10, step 191/574 completed (loss: 0.3847239911556244, acc: 0.761904776096344)
[2024-11-08 03:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:43][root][INFO] - Training Epoch: 3/10, step 192/574 completed (loss: 0.4700111448764801, acc: 0.8421052694320679)
[2024-11-08 03:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:44][root][INFO] - Training Epoch: 3/10, step 193/574 completed (loss: 0.14653165638446808, acc: 0.9545454382896423)
[2024-11-08 03:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:46][root][INFO] - Training Epoch: 3/10, step 194/574 completed (loss: 0.3111177384853363, acc: 0.9047619104385376)
[2024-11-08 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:47][root][INFO] - Training Epoch: 3/10, step 195/574 completed (loss: 0.16551688313484192, acc: 0.9166666865348816)
[2024-11-08 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:48][root][INFO] - Training Epoch: 3/10, step 196/574 completed (loss: 0.03369878605008125, acc: 1.0)
[2024-11-08 03:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:48][root][INFO] - Training Epoch: 3/10, step 197/574 completed (loss: 0.22544798254966736, acc: 0.9333333373069763)
[2024-11-08 03:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:49][root][INFO] - Training Epoch: 3/10, step 198/574 completed (loss: 0.0905367061495781, acc: 1.0)
[2024-11-08 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:49][root][INFO] - Training Epoch: 3/10, step 199/574 completed (loss: 0.16574101150035858, acc: 0.949999988079071)
[2024-11-08 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:50][root][INFO] - Training Epoch: 3/10, step 200/574 completed (loss: 0.048843078315258026, acc: 1.0)
[2024-11-08 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:50][root][INFO] - Training Epoch: 3/10, step 201/574 completed (loss: 0.1237042024731636, acc: 1.0)
[2024-11-08 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:50][root][INFO] - Training Epoch: 3/10, step 202/574 completed (loss: 0.13695098459720612, acc: 0.949999988079071)
[2024-11-08 03:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:51][root][INFO] - Training Epoch: 3/10, step 203/574 completed (loss: 0.33382314443588257, acc: 0.9130434989929199)
[2024-11-08 03:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:51][root][INFO] - Training Epoch: 3/10, step 204/574 completed (loss: 0.27340036630630493, acc: 0.9285714030265808)
[2024-11-08 03:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:52][root][INFO] - Training Epoch: 3/10, step 205/574 completed (loss: 0.21947482228279114, acc: 0.9047619104385376)
[2024-11-08 03:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:52][root][INFO] - Training Epoch: 3/10, step 206/574 completed (loss: 0.14277423918247223, acc: 0.9473684430122375)
[2024-11-08 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:53][root][INFO] - Training Epoch: 3/10, step 207/574 completed (loss: 0.3397862911224365, acc: 0.8421052694320679)
[2024-11-08 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:53][root][INFO] - Training Epoch: 3/10, step 208/574 completed (loss: 0.26189926266670227, acc: 0.9090909361839294)
[2024-11-08 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:54][root][INFO] - Training Epoch: 3/10, step 209/574 completed (loss: 0.16106657683849335, acc: 0.9047619104385376)
[2024-11-08 03:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:54][root][INFO] - Training Epoch: 3/10, step 210/574 completed (loss: 0.07436759769916534, acc: 0.9583333134651184)
[2024-11-08 03:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:55][root][INFO] - Training Epoch: 3/10, step 211/574 completed (loss: 0.08279251307249069, acc: 0.9677419066429138)
[2024-11-08 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:55][root][INFO] - Training Epoch: 3/10, step 212/574 completed (loss: 0.17188434302806854, acc: 0.9354838728904724)
[2024-11-08 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:55][root][INFO] - Training Epoch: 3/10, step 213/574 completed (loss: 0.22730742394924164, acc: 0.9230769276618958)
[2024-11-08 03:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:56][root][INFO] - Training Epoch: 3/10, step 214/574 completed (loss: 0.20708367228507996, acc: 0.9047619104385376)
[2024-11-08 03:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:57][root][INFO] - Training Epoch: 3/10, step 215/574 completed (loss: 0.06475339084863663, acc: 1.0)
[2024-11-08 03:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:57][root][INFO] - Training Epoch: 3/10, step 216/574 completed (loss: 0.08255641162395477, acc: 0.9473684430122375)
[2024-11-08 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:58][root][INFO] - Training Epoch: 3/10, step 217/574 completed (loss: 0.10914067924022675, acc: 0.9545454382896423)
[2024-11-08 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:59][root][INFO] - Training Epoch: 3/10, step 218/574 completed (loss: 0.15433365106582642, acc: 0.9523809552192688)
[2024-11-08 03:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:59][root][INFO] - Training Epoch: 3/10, step 219/574 completed (loss: 0.1241844967007637, acc: 0.9583333134651184)
[2024-11-08 03:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:43:59][root][INFO] - Training Epoch: 3/10, step 220/574 completed (loss: 0.04409129172563553, acc: 1.0)
[2024-11-08 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:00][root][INFO] - Training Epoch: 3/10, step 221/574 completed (loss: 0.02987528033554554, acc: 1.0)
[2024-11-08 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:00][root][INFO] - Training Epoch: 3/10, step 222/574 completed (loss: 0.05845523253083229, acc: 1.0)
[2024-11-08 03:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:01][root][INFO] - Training Epoch: 3/10, step 223/574 completed (loss: 0.2175556868314743, acc: 0.9047619104385376)
[2024-11-08 03:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:02][root][INFO] - Training Epoch: 3/10, step 224/574 completed (loss: 0.29543471336364746, acc: 0.8421052694320679)
[2024-11-08 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:03][root][INFO] - Training Epoch: 3/10, step 225/574 completed (loss: 0.1436532884836197, acc: 0.9473684430122375)
[2024-11-08 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:03][root][INFO] - Training Epoch: 3/10, step 226/574 completed (loss: 0.026246417313814163, acc: 1.0)
[2024-11-08 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:04][root][INFO] - Training Epoch: 3/10, step 227/574 completed (loss: 0.43294280767440796, acc: 0.9047619104385376)
[2024-11-08 03:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:04][root][INFO] - Training Epoch: 3/10, step 228/574 completed (loss: 0.24931591749191284, acc: 0.875)
[2024-11-08 03:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:05][root][INFO] - Training Epoch: 3/10, step 229/574 completed (loss: 0.29517942667007446, acc: 0.8695651888847351)
[2024-11-08 03:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:05][root][INFO] - Training Epoch: 3/10, step 230/574 completed (loss: 0.32683229446411133, acc: 0.9523809552192688)
[2024-11-08 03:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:05][root][INFO] - Training Epoch: 3/10, step 231/574 completed (loss: 0.3213457465171814, acc: 0.8421052694320679)
[2024-11-08 03:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:06][root][INFO] - Training Epoch: 3/10, step 232/574 completed (loss: 0.30384114384651184, acc: 0.8421052694320679)
[2024-11-08 03:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:07][root][INFO] - Training Epoch: 3/10, step 233/574 completed (loss: 0.5336123704910278, acc: 0.8181818127632141)
[2024-11-08 03:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:07][root][INFO] - Training Epoch: 3/10, step 234/574 completed (loss: 0.3081146776676178, acc: 0.8636363744735718)
[2024-11-08 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:08][root][INFO] - Training Epoch: 3/10, step 235/574 completed (loss: 0.13551035523414612, acc: 0.9285714030265808)
[2024-11-08 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:08][root][INFO] - Training Epoch: 3/10, step 236/574 completed (loss: 0.42169296741485596, acc: 0.8888888955116272)
[2024-11-08 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:09][root][INFO] - Training Epoch: 3/10, step 237/574 completed (loss: 0.4132291376590729, acc: 0.8787878751754761)
[2024-11-08 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:09][root][INFO] - Training Epoch: 3/10, step 238/574 completed (loss: 0.16145338118076324, acc: 0.9545454382896423)
[2024-11-08 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:10][root][INFO] - Training Epoch: 3/10, step 239/574 completed (loss: 0.3096749782562256, acc: 0.9047619104385376)
[2024-11-08 03:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:10][root][INFO] - Training Epoch: 3/10, step 240/574 completed (loss: 0.26445272564888, acc: 0.8947368264198303)
[2024-11-08 03:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:10][root][INFO] - Training Epoch: 3/10, step 241/574 completed (loss: 0.2792782187461853, acc: 0.9090909361839294)
[2024-11-08 03:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:11][root][INFO] - Training Epoch: 3/10, step 242/574 completed (loss: 0.18581950664520264, acc: 0.9473684430122375)
[2024-11-08 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:12][root][INFO] - Training Epoch: 3/10, step 243/574 completed (loss: 0.2484273463487625, acc: 0.9090909361839294)
[2024-11-08 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:12][root][INFO] - Training Epoch: 3/10, step 244/574 completed (loss: 0.26683923602104187, acc: 0.9285714030265808)
[2024-11-08 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:13][root][INFO] - Training Epoch: 3/10, step 245/574 completed (loss: 0.15144719183444977, acc: 0.9666666388511658)
[2024-11-08 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:13][root][INFO] - Training Epoch: 3/10, step 246/574 completed (loss: 0.19615770876407623, acc: 0.9428571462631226)
[2024-11-08 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:14][root][INFO] - Training Epoch: 3/10, step 247/574 completed (loss: 0.14600421488285065, acc: 0.95652174949646)
[2024-11-08 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:14][root][INFO] - Training Epoch: 3/10, step 248/574 completed (loss: 0.37161239981651306, acc: 0.9047619104385376)
[2024-11-08 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:14][root][INFO] - Training Epoch: 3/10, step 249/574 completed (loss: 0.27126652002334595, acc: 0.8947368264198303)
[2024-11-08 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:15][root][INFO] - Training Epoch: 3/10, step 250/574 completed (loss: 0.2993062436580658, acc: 0.8999999761581421)
[2024-11-08 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:15][root][INFO] - Training Epoch: 3/10, step 251/574 completed (loss: 0.4136067032814026, acc: 0.9047619104385376)
[2024-11-08 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:15][root][INFO] - Training Epoch: 3/10, step 252/574 completed (loss: 0.0779498890042305, acc: 0.9545454382896423)
[2024-11-08 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:16][root][INFO] - Training Epoch: 3/10, step 253/574 completed (loss: 0.1773931086063385, acc: 0.9285714030265808)
[2024-11-08 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:16][root][INFO] - Training Epoch: 3/10, step 254/574 completed (loss: 0.0789877399802208, acc: 0.9655172228813171)
[2024-11-08 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:16][root][INFO] - Training Epoch: 3/10, step 255/574 completed (loss: 0.08468631654977798, acc: 0.9714285731315613)
[2024-11-08 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:17][root][INFO] - Training Epoch: 3/10, step 256/574 completed (loss: 0.10911905020475388, acc: 0.9523809552192688)
[2024-11-08 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:17][root][INFO] - Training Epoch: 3/10, step 257/574 completed (loss: 0.21301482617855072, acc: 0.9047619104385376)
[2024-11-08 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:18][root][INFO] - Training Epoch: 3/10, step 258/574 completed (loss: 0.034335918724536896, acc: 1.0)
[2024-11-08 03:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:18][root][INFO] - Training Epoch: 3/10, step 259/574 completed (loss: 0.3232729136943817, acc: 0.9090909361839294)
[2024-11-08 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:19][root][INFO] - Training Epoch: 3/10, step 260/574 completed (loss: 0.09430066496133804, acc: 0.9473684430122375)
[2024-11-08 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:20][root][INFO] - Training Epoch: 3/10, step 261/574 completed (loss: 0.050599660724401474, acc: 1.0)
[2024-11-08 03:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:20][root][INFO] - Training Epoch: 3/10, step 262/574 completed (loss: 0.2385983169078827, acc: 0.9117646813392639)
[2024-11-08 03:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:20][root][INFO] - Training Epoch: 3/10, step 263/574 completed (loss: 0.14270304143428802, acc: 0.9545454382896423)
[2024-11-08 03:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:21][root][INFO] - Training Epoch: 3/10, step 264/574 completed (loss: 0.12863680720329285, acc: 0.949999988079071)
[2024-11-08 03:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:22][root][INFO] - Training Epoch: 3/10, step 265/574 completed (loss: 0.27829256653785706, acc: 0.8421052694320679)
[2024-11-08 03:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:23][root][INFO] - Training Epoch: 3/10, step 266/574 completed (loss: 0.2704874575138092, acc: 0.9130434989929199)
[2024-11-08 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:23][root][INFO] - Training Epoch: 3/10, step 267/574 completed (loss: 0.3143457770347595, acc: 0.8571428656578064)
[2024-11-08 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:24][root][INFO] - Training Epoch: 3/10, step 268/574 completed (loss: 0.10386382788419724, acc: 0.9655172228813171)
[2024-11-08 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:24][root][INFO] - Training Epoch: 3/10, step 269/574 completed (loss: 0.18462826311588287, acc: 0.8888888955116272)
[2024-11-08 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:24][root][INFO] - Training Epoch: 3/10, step 270/574 completed (loss: 0.1759009063243866, acc: 0.9615384340286255)
[2024-11-08 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:25][root][INFO] - Training Epoch: 3/10, step 271/574 completed (loss: 0.06396795809268951, acc: 0.9523809552192688)
[2024-11-08 03:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:25][root][INFO] - Training Epoch: 3/10, step 272/574 completed (loss: 0.02925323136150837, acc: 1.0)
[2024-11-08 03:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:25][root][INFO] - Training Epoch: 3/10, step 273/574 completed (loss: 0.08148591220378876, acc: 0.9473684430122375)
[2024-11-08 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:26][root][INFO] - Training Epoch: 3/10, step 274/574 completed (loss: 0.0997975766658783, acc: 0.9545454382896423)
[2024-11-08 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:26][root][INFO] - Training Epoch: 3/10, step 275/574 completed (loss: 0.05265161395072937, acc: 1.0)
[2024-11-08 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:27][root][INFO] - Training Epoch: 3/10, step 276/574 completed (loss: 0.24145635962486267, acc: 0.931034505367279)
[2024-11-08 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:27][root][INFO] - Training Epoch: 3/10, step 277/574 completed (loss: 0.07851595431566238, acc: 0.9629629850387573)
[2024-11-08 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:27][root][INFO] - Training Epoch: 3/10, step 278/574 completed (loss: 0.16791932284832, acc: 0.9047619104385376)
[2024-11-08 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:28][root][INFO] - Training Epoch: 3/10, step 279/574 completed (loss: 0.06159137561917305, acc: 1.0)
[2024-11-08 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:28][root][INFO] - Training Epoch: 3/10, step 280/574 completed (loss: 0.13016773760318756, acc: 0.9444444179534912)
[2024-11-08 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:29][root][INFO] - Training Epoch: 3/10, step 281/574 completed (loss: 0.10602910816669464, acc: 0.9545454382896423)
[2024-11-08 03:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:11][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2694, device='cuda:0') eval_epoch_loss=tensor(0.2385, device='cuda:0') eval_epoch_acc=tensor(0.9260, device='cuda:0')
[2024-11-08 03:45:11][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:45:11][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:45:15][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_3_step_282_loss_0.23851141333580017/model.pt
[2024-11-08 03:45:15][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:45:15][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 3 is 0.23851141333580017
[2024-11-08 03:45:15][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.9259733557701111
[2024-11-08 03:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:15][root][INFO] - Training Epoch: 3/10, step 282/574 completed (loss: 0.01770578697323799, acc: 1.0)
[2024-11-08 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:16][root][INFO] - Training Epoch: 3/10, step 283/574 completed (loss: 0.3932303786277771, acc: 0.8620689511299133)
[2024-11-08 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:16][root][INFO] - Training Epoch: 3/10, step 284/574 completed (loss: 0.24985232949256897, acc: 0.8666666746139526)
[2024-11-08 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:16][root][INFO] - Training Epoch: 3/10, step 285/574 completed (loss: 0.34920600056648254, acc: 0.8965517282485962)
[2024-11-08 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:17][root][INFO] - Training Epoch: 3/10, step 286/574 completed (loss: 0.48771652579307556, acc: 0.8571428656578064)
[2024-11-08 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:17][root][INFO] - Training Epoch: 3/10, step 287/574 completed (loss: 0.3016057312488556, acc: 0.8947368264198303)
[2024-11-08 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:18][root][INFO] - Training Epoch: 3/10, step 288/574 completed (loss: 0.39332595467567444, acc: 0.8421052694320679)
[2024-11-08 03:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:18][root][INFO] - Training Epoch: 3/10, step 289/574 completed (loss: 0.41012823581695557, acc: 0.8636363744735718)
[2024-11-08 03:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:18][root][INFO] - Training Epoch: 3/10, step 290/574 completed (loss: 0.3996363878250122, acc: 0.8181818127632141)
[2024-11-08 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:19][root][INFO] - Training Epoch: 3/10, step 291/574 completed (loss: 0.05089385807514191, acc: 0.9599999785423279)
[2024-11-08 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:19][root][INFO] - Training Epoch: 3/10, step 292/574 completed (loss: 0.17488008737564087, acc: 0.8666666746139526)
[2024-11-08 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:19][root][INFO] - Training Epoch: 3/10, step 293/574 completed (loss: 0.19253985583782196, acc: 0.9047619104385376)
[2024-11-08 03:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:20][root][INFO] - Training Epoch: 3/10, step 294/574 completed (loss: 0.18191692233085632, acc: 0.949999988079071)
[2024-11-08 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:21][root][INFO] - Training Epoch: 3/10, step 295/574 completed (loss: 0.30164802074432373, acc: 0.8947368264198303)
[2024-11-08 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:21][root][INFO] - Training Epoch: 3/10, step 296/574 completed (loss: 0.30504679679870605, acc: 0.8181818127632141)
[2024-11-08 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:22][root][INFO] - Training Epoch: 3/10, step 297/574 completed (loss: 0.6569157838821411, acc: 0.8571428656578064)
[2024-11-08 03:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:22][root][INFO] - Training Epoch: 3/10, step 298/574 completed (loss: 0.19557975232601166, acc: 0.9166666865348816)
[2024-11-08 03:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:22][root][INFO] - Training Epoch: 3/10, step 299/574 completed (loss: 0.10000430047512054, acc: 0.9677419066429138)
[2024-11-08 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:23][root][INFO] - Training Epoch: 3/10, step 300/574 completed (loss: 0.13473324477672577, acc: 0.9354838728904724)
[2024-11-08 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:23][root][INFO] - Training Epoch: 3/10, step 301/574 completed (loss: 0.1259365975856781, acc: 0.9615384340286255)
[2024-11-08 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:23][root][INFO] - Training Epoch: 3/10, step 302/574 completed (loss: 0.23076044023036957, acc: 0.9047619104385376)
[2024-11-08 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:24][root][INFO] - Training Epoch: 3/10, step 303/574 completed (loss: 0.1893700659275055, acc: 0.8947368264198303)
[2024-11-08 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:24][root][INFO] - Training Epoch: 3/10, step 304/574 completed (loss: 0.18092726171016693, acc: 0.8947368264198303)
[2024-11-08 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:25][root][INFO] - Training Epoch: 3/10, step 305/574 completed (loss: 0.17043405771255493, acc: 0.95652174949646)
[2024-11-08 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:25][root][INFO] - Training Epoch: 3/10, step 306/574 completed (loss: 0.059582509100437164, acc: 0.9545454382896423)
[2024-11-08 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:25][root][INFO] - Training Epoch: 3/10, step 307/574 completed (loss: 0.041914861649274826, acc: 1.0)
[2024-11-08 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:26][root][INFO] - Training Epoch: 3/10, step 308/574 completed (loss: 0.12439457327127457, acc: 0.9047619104385376)
[2024-11-08 03:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:26][root][INFO] - Training Epoch: 3/10, step 309/574 completed (loss: 0.25720131397247314, acc: 0.8947368264198303)
[2024-11-08 03:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:27][root][INFO] - Training Epoch: 3/10, step 310/574 completed (loss: 0.1648685485124588, acc: 0.9473684430122375)
[2024-11-08 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:27][root][INFO] - Training Epoch: 3/10, step 311/574 completed (loss: 0.2863522171974182, acc: 0.9090909361839294)
[2024-11-08 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:27][root][INFO] - Training Epoch: 3/10, step 312/574 completed (loss: 0.24187389016151428, acc: 0.9545454382896423)
[2024-11-08 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:28][root][INFO] - Training Epoch: 3/10, step 313/574 completed (loss: 0.06672488898038864, acc: 1.0)
[2024-11-08 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:28][root][INFO] - Training Epoch: 3/10, step 314/574 completed (loss: 0.13099582493305206, acc: 1.0)
[2024-11-08 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:28][root][INFO] - Training Epoch: 3/10, step 315/574 completed (loss: 0.16107304394245148, acc: 0.9714285731315613)
[2024-11-08 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:29][root][INFO] - Training Epoch: 3/10, step 316/574 completed (loss: 0.29193082451820374, acc: 0.8461538553237915)
[2024-11-08 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:29][root][INFO] - Training Epoch: 3/10, step 317/574 completed (loss: 0.05469876900315285, acc: 0.9523809552192688)
[2024-11-08 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:30][root][INFO] - Training Epoch: 3/10, step 318/574 completed (loss: 0.1300000101327896, acc: 0.8999999761581421)
[2024-11-08 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:30][root][INFO] - Training Epoch: 3/10, step 319/574 completed (loss: 0.022983521223068237, acc: 1.0)
[2024-11-08 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:30][root][INFO] - Training Epoch: 3/10, step 320/574 completed (loss: 0.1681494563817978, acc: 0.9523809552192688)
[2024-11-08 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:31][root][INFO] - Training Epoch: 3/10, step 321/574 completed (loss: 0.008576590567827225, acc: 1.0)
[2024-11-08 03:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:31][root][INFO] - Training Epoch: 3/10, step 322/574 completed (loss: 0.5291702151298523, acc: 0.782608687877655)
[2024-11-08 03:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:32][root][INFO] - Training Epoch: 3/10, step 323/574 completed (loss: 0.3289986848831177, acc: 0.9047619104385376)
[2024-11-08 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:32][root][INFO] - Training Epoch: 3/10, step 324/574 completed (loss: 0.6207157969474792, acc: 0.7894737124443054)
[2024-11-08 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:32][root][INFO] - Training Epoch: 3/10, step 325/574 completed (loss: 0.45418882369995117, acc: 0.8636363744735718)
[2024-11-08 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:33][root][INFO] - Training Epoch: 3/10, step 326/574 completed (loss: 0.3846638798713684, acc: 0.8421052694320679)
[2024-11-08 03:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:33][root][INFO] - Training Epoch: 3/10, step 327/574 completed (loss: 0.3513222634792328, acc: 0.875)
[2024-11-08 03:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:33][root][INFO] - Training Epoch: 3/10, step 328/574 completed (loss: 0.24498198926448822, acc: 0.931034505367279)
[2024-11-08 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:34][root][INFO] - Training Epoch: 3/10, step 329/574 completed (loss: 0.29320862889289856, acc: 0.9259259104728699)
[2024-11-08 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:34][root][INFO] - Training Epoch: 3/10, step 330/574 completed (loss: 0.13033215701580048, acc: 0.9523809552192688)
[2024-11-08 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:35][root][INFO] - Training Epoch: 3/10, step 331/574 completed (loss: 0.016365913674235344, acc: 1.0)
[2024-11-08 03:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:35][root][INFO] - Training Epoch: 3/10, step 332/574 completed (loss: 0.2700670063495636, acc: 0.9090909361839294)
[2024-11-08 03:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:35][root][INFO] - Training Epoch: 3/10, step 333/574 completed (loss: 0.6346150636672974, acc: 0.7894737124443054)
[2024-11-08 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:36][root][INFO] - Training Epoch: 3/10, step 334/574 completed (loss: 0.25263088941574097, acc: 0.9090909361839294)
[2024-11-08 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:36][root][INFO] - Training Epoch: 3/10, step 335/574 completed (loss: 0.48868343234062195, acc: 0.807692289352417)
[2024-11-08 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:37][root][INFO] - Training Epoch: 3/10, step 336/574 completed (loss: 0.5966722369194031, acc: 0.8333333134651184)
[2024-11-08 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:37][root][INFO] - Training Epoch: 3/10, step 337/574 completed (loss: 0.35511913895606995, acc: 0.8571428656578064)
[2024-11-08 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:37][root][INFO] - Training Epoch: 3/10, step 338/574 completed (loss: 0.2411442995071411, acc: 0.9130434989929199)
[2024-11-08 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:38][root][INFO] - Training Epoch: 3/10, step 339/574 completed (loss: 0.1439233422279358, acc: 1.0)
[2024-11-08 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:38][root][INFO] - Training Epoch: 3/10, step 340/574 completed (loss: 0.11792242527008057, acc: 0.9666666388511658)
[2024-11-08 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:38][root][INFO] - Training Epoch: 3/10, step 341/574 completed (loss: 0.12676703929901123, acc: 0.939393937587738)
[2024-11-08 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:39][root][INFO] - Training Epoch: 3/10, step 342/574 completed (loss: 0.23355357348918915, acc: 0.8999999761581421)
[2024-11-08 03:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:39][root][INFO] - Training Epoch: 3/10, step 343/574 completed (loss: 0.2891785502433777, acc: 0.8999999761581421)
[2024-11-08 03:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:39][root][INFO] - Training Epoch: 3/10, step 344/574 completed (loss: 0.07903198897838593, acc: 1.0)
[2024-11-08 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:40][root][INFO] - Training Epoch: 3/10, step 345/574 completed (loss: 0.1816588193178177, acc: 0.9545454382896423)
[2024-11-08 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:40][root][INFO] - Training Epoch: 3/10, step 346/574 completed (loss: 0.29277512431144714, acc: 0.8500000238418579)
[2024-11-08 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:41][root][INFO] - Training Epoch: 3/10, step 347/574 completed (loss: 0.1233777105808258, acc: 0.9166666865348816)
[2024-11-08 03:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:41][root][INFO] - Training Epoch: 3/10, step 348/574 completed (loss: 0.15764690935611725, acc: 0.95652174949646)
[2024-11-08 03:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:41][root][INFO] - Training Epoch: 3/10, step 349/574 completed (loss: 0.14195449650287628, acc: 0.8999999761581421)
[2024-11-08 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:42][root][INFO] - Training Epoch: 3/10, step 350/574 completed (loss: 0.3494759202003479, acc: 0.8421052694320679)
[2024-11-08 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:42][root][INFO] - Training Epoch: 3/10, step 351/574 completed (loss: 0.16828548908233643, acc: 1.0)
[2024-11-08 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:43][root][INFO] - Training Epoch: 3/10, step 352/574 completed (loss: 0.27503764629364014, acc: 0.8999999761581421)
[2024-11-08 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:43][root][INFO] - Training Epoch: 3/10, step 353/574 completed (loss: 0.1521967351436615, acc: 0.9599999785423279)
[2024-11-08 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:43][root][INFO] - Training Epoch: 3/10, step 354/574 completed (loss: 0.3158475458621979, acc: 0.9200000166893005)
[2024-11-08 03:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:44][root][INFO] - Training Epoch: 3/10, step 355/574 completed (loss: 0.33078494668006897, acc: 0.8571428656578064)
[2024-11-08 03:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:45][root][INFO] - Training Epoch: 3/10, step 356/574 completed (loss: 0.4837614893913269, acc: 0.800000011920929)
[2024-11-08 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:45][root][INFO] - Training Epoch: 3/10, step 357/574 completed (loss: 0.29409289360046387, acc: 0.9047619104385376)
[2024-11-08 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:45][root][INFO] - Training Epoch: 3/10, step 358/574 completed (loss: 0.29139989614486694, acc: 0.8636363744735718)
[2024-11-08 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:46][root][INFO] - Training Epoch: 3/10, step 359/574 completed (loss: 0.0170623529702425, acc: 1.0)
[2024-11-08 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:46][root][INFO] - Training Epoch: 3/10, step 360/574 completed (loss: 0.19388018548488617, acc: 0.8965517282485962)
[2024-11-08 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:47][root][INFO] - Training Epoch: 3/10, step 361/574 completed (loss: 0.13696439564228058, acc: 0.9428571462631226)
[2024-11-08 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:47][root][INFO] - Training Epoch: 3/10, step 362/574 completed (loss: 0.17465609312057495, acc: 0.9090909361839294)
[2024-11-08 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:47][root][INFO] - Training Epoch: 3/10, step 363/574 completed (loss: 0.25268301367759705, acc: 0.8947368264198303)
[2024-11-08 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:48][root][INFO] - Training Epoch: 3/10, step 364/574 completed (loss: 0.1963992863893509, acc: 0.9523809552192688)
[2024-11-08 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:48][root][INFO] - Training Epoch: 3/10, step 365/574 completed (loss: 0.09029413759708405, acc: 1.0)
[2024-11-08 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:48][root][INFO] - Training Epoch: 3/10, step 366/574 completed (loss: 0.053079091012477875, acc: 1.0)
[2024-11-08 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:49][root][INFO] - Training Epoch: 3/10, step 367/574 completed (loss: 0.2415243238210678, acc: 0.9629629850387573)
[2024-11-08 03:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:49][root][INFO] - Training Epoch: 3/10, step 368/574 completed (loss: 0.15258461236953735, acc: 0.96875)
[2024-11-08 03:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:49][root][INFO] - Training Epoch: 3/10, step 369/574 completed (loss: 0.1434869021177292, acc: 0.9523809552192688)
[2024-11-08 03:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:50][root][INFO] - Training Epoch: 3/10, step 370/574 completed (loss: 0.34986478090286255, acc: 0.8095238208770752)
[2024-11-08 03:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:52][root][INFO] - Training Epoch: 3/10, step 371/574 completed (loss: 0.269724577665329, acc: 0.8421052694320679)
[2024-11-08 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:52][root][INFO] - Training Epoch: 3/10, step 372/574 completed (loss: 0.07846289128065109, acc: 0.9545454382896423)
[2024-11-08 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:52][root][INFO] - Training Epoch: 3/10, step 373/574 completed (loss: 0.07955141365528107, acc: 1.0)
[2024-11-08 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:53][root][INFO] - Training Epoch: 3/10, step 374/574 completed (loss: 0.11333487182855606, acc: 0.9545454382896423)
[2024-11-08 03:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:53][root][INFO] - Training Epoch: 3/10, step 375/574 completed (loss: 0.09585302323102951, acc: 0.9655172228813171)
[2024-11-08 03:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:53][root][INFO] - Training Epoch: 3/10, step 376/574 completed (loss: 0.05145071819424629, acc: 0.9677419066429138)
[2024-11-08 03:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:54][root][INFO] - Training Epoch: 3/10, step 377/574 completed (loss: 0.28040796518325806, acc: 0.8571428656578064)
[2024-11-08 03:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:54][root][INFO] - Training Epoch: 3/10, step 378/574 completed (loss: 0.051359873265028, acc: 1.0)
[2024-11-08 03:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:55][root][INFO] - Training Epoch: 3/10, step 379/574 completed (loss: 0.08792027831077576, acc: 1.0)
[2024-11-08 03:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:56][root][INFO] - Training Epoch: 3/10, step 380/574 completed (loss: 0.3292176127433777, acc: 0.9090909361839294)
[2024-11-08 03:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:57][root][INFO] - Training Epoch: 3/10, step 381/574 completed (loss: 0.0781383290886879, acc: 1.0)
[2024-11-08 03:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:58][root][INFO] - Training Epoch: 3/10, step 382/574 completed (loss: 0.08112367242574692, acc: 0.9545454382896423)
[2024-11-08 03:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:58][root][INFO] - Training Epoch: 3/10, step 383/574 completed (loss: 0.008429484441876411, acc: 1.0)
[2024-11-08 03:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:59][root][INFO] - Training Epoch: 3/10, step 384/574 completed (loss: 0.029680877923965454, acc: 1.0)
[2024-11-08 03:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:45:59][root][INFO] - Training Epoch: 3/10, step 385/574 completed (loss: 0.007593177724629641, acc: 1.0)
[2024-11-08 03:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:00][root][INFO] - Training Epoch: 3/10, step 386/574 completed (loss: 0.06026318669319153, acc: 0.949999988079071)
[2024-11-08 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:00][root][INFO] - Training Epoch: 3/10, step 387/574 completed (loss: 0.0019704147707670927, acc: 1.0)
[2024-11-08 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:00][root][INFO] - Training Epoch: 3/10, step 388/574 completed (loss: 0.03537583351135254, acc: 1.0)
[2024-11-08 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:01][root][INFO] - Training Epoch: 3/10, step 389/574 completed (loss: 0.027470137923955917, acc: 1.0)
[2024-11-08 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:01][root][INFO] - Training Epoch: 3/10, step 390/574 completed (loss: 0.08144310861825943, acc: 0.9545454382896423)
[2024-11-08 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:01][root][INFO] - Training Epoch: 3/10, step 391/574 completed (loss: 0.20348705351352692, acc: 0.8571428656578064)
[2024-11-08 03:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:02][root][INFO] - Training Epoch: 3/10, step 392/574 completed (loss: 0.3376276195049286, acc: 0.8421052694320679)
[2024-11-08 03:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:03][root][INFO] - Training Epoch: 3/10, step 393/574 completed (loss: 0.07870321720838547, acc: 1.0)
[2024-11-08 03:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:03][root][INFO] - Training Epoch: 3/10, step 394/574 completed (loss: 0.2766329348087311, acc: 0.8636363744735718)
[2024-11-08 03:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:04][root][INFO] - Training Epoch: 3/10, step 395/574 completed (loss: 0.21943242847919464, acc: 0.9047619104385376)
[2024-11-08 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:04][root][INFO] - Training Epoch: 3/10, step 396/574 completed (loss: 0.041990675032138824, acc: 1.0)
[2024-11-08 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:04][root][INFO] - Training Epoch: 3/10, step 397/574 completed (loss: 0.17473359405994415, acc: 0.9333333373069763)
[2024-11-08 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:05][root][INFO] - Training Epoch: 3/10, step 398/574 completed (loss: 0.07131905853748322, acc: 0.9523809552192688)
[2024-11-08 03:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:05][root][INFO] - Training Epoch: 3/10, step 399/574 completed (loss: 0.028028011322021484, acc: 1.0)
[2024-11-08 03:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:06][root][INFO] - Training Epoch: 3/10, step 400/574 completed (loss: 0.2042972892522812, acc: 0.9090909361839294)
[2024-11-08 03:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:06][root][INFO] - Training Epoch: 3/10, step 401/574 completed (loss: 0.08116413652896881, acc: 0.9523809552192688)
[2024-11-08 03:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:07][root][INFO] - Training Epoch: 3/10, step 402/574 completed (loss: 0.013967548497021198, acc: 1.0)
[2024-11-08 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:07][root][INFO] - Training Epoch: 3/10, step 403/574 completed (loss: 0.07836096733808517, acc: 0.96875)
[2024-11-08 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:07][root][INFO] - Training Epoch: 3/10, step 404/574 completed (loss: 0.09941361844539642, acc: 0.9428571462631226)
[2024-11-08 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:08][root][INFO] - Training Epoch: 3/10, step 405/574 completed (loss: 0.23434299230575562, acc: 0.9230769276618958)
[2024-11-08 03:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:08][root][INFO] - Training Epoch: 3/10, step 406/574 completed (loss: 0.25198331475257874, acc: 0.8571428656578064)
[2024-11-08 03:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:08][root][INFO] - Training Epoch: 3/10, step 407/574 completed (loss: 0.04806837439537048, acc: 1.0)
[2024-11-08 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:09][root][INFO] - Training Epoch: 3/10, step 408/574 completed (loss: 0.024027664214372635, acc: 1.0)
[2024-11-08 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:09][root][INFO] - Training Epoch: 3/10, step 409/574 completed (loss: 0.14994780719280243, acc: 0.9523809552192688)
[2024-11-08 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:10][root][INFO] - Training Epoch: 3/10, step 410/574 completed (loss: 0.011737125925719738, acc: 1.0)
[2024-11-08 03:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:10][root][INFO] - Training Epoch: 3/10, step 411/574 completed (loss: 0.3587730824947357, acc: 0.8928571343421936)
[2024-11-08 03:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:10][root][INFO] - Training Epoch: 3/10, step 412/574 completed (loss: 0.07805291563272476, acc: 0.9629629850387573)
[2024-11-08 03:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:11][root][INFO] - Training Epoch: 3/10, step 413/574 completed (loss: 0.010725382715463638, acc: 1.0)
[2024-11-08 03:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:11][root][INFO] - Training Epoch: 3/10, step 414/574 completed (loss: 0.11893774569034576, acc: 0.9615384340286255)
[2024-11-08 03:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:11][root][INFO] - Training Epoch: 3/10, step 415/574 completed (loss: 0.0068694003857672215, acc: 1.0)
[2024-11-08 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:12][root][INFO] - Training Epoch: 3/10, step 416/574 completed (loss: 0.30824679136276245, acc: 0.9473684430122375)
[2024-11-08 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:12][root][INFO] - Training Epoch: 3/10, step 417/574 completed (loss: 0.037893690168857574, acc: 1.0)
[2024-11-08 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:13][root][INFO] - Training Epoch: 3/10, step 418/574 completed (loss: 0.06312025338411331, acc: 1.0)
[2024-11-08 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:13][root][INFO] - Training Epoch: 3/10, step 419/574 completed (loss: 0.18209488689899445, acc: 0.9047619104385376)
[2024-11-08 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:13][root][INFO] - Training Epoch: 3/10, step 420/574 completed (loss: 0.05937099829316139, acc: 1.0)
[2024-11-08 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:14][root][INFO] - Training Epoch: 3/10, step 421/574 completed (loss: 0.1521342545747757, acc: 0.9032257795333862)
[2024-11-08 03:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:14][root][INFO] - Training Epoch: 3/10, step 422/574 completed (loss: 0.2795780301094055, acc: 0.9032257795333862)
[2024-11-08 03:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:14][root][INFO] - Training Epoch: 3/10, step 423/574 completed (loss: 0.4100755453109741, acc: 0.8846153616905212)
[2024-11-08 03:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:15][root][INFO] - Training Epoch: 3/10, step 424/574 completed (loss: 0.07277747243642807, acc: 1.0)
[2024-11-08 03:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:46:57][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2697, device='cuda:0') eval_epoch_loss=tensor(0.2387, device='cuda:0') eval_epoch_acc=tensor(0.9201, device='cuda:0')
[2024-11-08 03:46:57][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:46:57][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:47:00][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_3_step_425_loss_0.23874583840370178/model.pt
[2024-11-08 03:47:01][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:01][root][INFO] - Training Epoch: 3/10, step 425/574 completed (loss: 0.19836150109767914, acc: 0.8947368264198303)
[2024-11-08 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:01][root][INFO] - Training Epoch: 3/10, step 426/574 completed (loss: 0.06241416186094284, acc: 0.9473684430122375)
[2024-11-08 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:02][root][INFO] - Training Epoch: 3/10, step 427/574 completed (loss: 0.2556843161582947, acc: 0.9090909361839294)
[2024-11-08 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:02][root][INFO] - Training Epoch: 3/10, step 428/574 completed (loss: 0.07352057099342346, acc: 0.9523809552192688)
[2024-11-08 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:02][root][INFO] - Training Epoch: 3/10, step 429/574 completed (loss: 0.02780322916805744, acc: 1.0)
[2024-11-08 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:03][root][INFO] - Training Epoch: 3/10, step 430/574 completed (loss: 0.10531961172819138, acc: 0.9677419066429138)
[2024-11-08 03:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:03][root][INFO] - Training Epoch: 3/10, step 431/574 completed (loss: 0.051864784210920334, acc: 0.9677419066429138)
[2024-11-08 03:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:03][root][INFO] - Training Epoch: 3/10, step 432/574 completed (loss: 0.1477065235376358, acc: 0.9230769276618958)
[2024-11-08 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:04][root][INFO] - Training Epoch: 3/10, step 433/574 completed (loss: 0.08615554124116898, acc: 1.0)
[2024-11-08 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:04][root][INFO] - Training Epoch: 3/10, step 434/574 completed (loss: 0.0439523383975029, acc: 1.0)
[2024-11-08 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:05][root][INFO] - Training Epoch: 3/10, step 435/574 completed (loss: 0.014880366623401642, acc: 1.0)
[2024-11-08 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:05][root][INFO] - Training Epoch: 3/10, step 436/574 completed (loss: 0.059622932225465775, acc: 1.0)
[2024-11-08 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:05][root][INFO] - Training Epoch: 3/10, step 437/574 completed (loss: 0.01061896700412035, acc: 1.0)
[2024-11-08 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:06][root][INFO] - Training Epoch: 3/10, step 438/574 completed (loss: 0.09820912033319473, acc: 0.9545454382896423)
[2024-11-08 03:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:06][root][INFO] - Training Epoch: 3/10, step 439/574 completed (loss: 0.10568100214004517, acc: 0.9642857313156128)
[2024-11-08 03:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:07][root][INFO] - Training Epoch: 3/10, step 440/574 completed (loss: 0.1036636009812355, acc: 0.9523809552192688)
[2024-11-08 03:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:08][root][INFO] - Training Epoch: 3/10, step 441/574 completed (loss: 0.5374830365180969, acc: 0.8947368264198303)
[2024-11-08 03:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:08][root][INFO] - Training Epoch: 3/10, step 442/574 completed (loss: 0.3219183385372162, acc: 0.8421052694320679)
[2024-11-08 03:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:09][root][INFO] - Training Epoch: 3/10, step 443/574 completed (loss: 0.12021371722221375, acc: 0.9545454382896423)
[2024-11-08 03:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:10][root][INFO] - Training Epoch: 3/10, step 444/574 completed (loss: 0.14956381916999817, acc: 0.9523809552192688)
[2024-11-08 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:10][root][INFO] - Training Epoch: 3/10, step 445/574 completed (loss: 0.010336914099752903, acc: 1.0)
[2024-11-08 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:10][root][INFO] - Training Epoch: 3/10, step 446/574 completed (loss: 0.23856474459171295, acc: 0.9677419066429138)
[2024-11-08 03:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:11][root][INFO] - Training Epoch: 3/10, step 447/574 completed (loss: 0.10458504408597946, acc: 0.9677419066429138)
[2024-11-08 03:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:11][root][INFO] - Training Epoch: 3/10, step 448/574 completed (loss: 0.04894934222102165, acc: 0.9677419066429138)
[2024-11-08 03:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:12][root][INFO] - Training Epoch: 3/10, step 449/574 completed (loss: 0.027639662846922874, acc: 1.0)
[2024-11-08 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:12][root][INFO] - Training Epoch: 3/10, step 450/574 completed (loss: 0.002857816871255636, acc: 1.0)
[2024-11-08 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:12][root][INFO] - Training Epoch: 3/10, step 451/574 completed (loss: 0.02814667858183384, acc: 1.0)
[2024-11-08 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:13][root][INFO] - Training Epoch: 3/10, step 452/574 completed (loss: 0.06660401076078415, acc: 0.9545454382896423)
[2024-11-08 03:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:13][root][INFO] - Training Epoch: 3/10, step 453/574 completed (loss: 0.3999713957309723, acc: 0.949999988079071)
[2024-11-08 03:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:13][root][INFO] - Training Epoch: 3/10, step 454/574 completed (loss: 0.20688152313232422, acc: 0.9090909361839294)
[2024-11-08 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:14][root][INFO] - Training Epoch: 3/10, step 455/574 completed (loss: 0.014526880346238613, acc: 1.0)
[2024-11-08 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:14][root][INFO] - Training Epoch: 3/10, step 456/574 completed (loss: 0.24454784393310547, acc: 0.8999999761581421)
[2024-11-08 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:14][root][INFO] - Training Epoch: 3/10, step 457/574 completed (loss: 0.0221015103161335, acc: 1.0)
[2024-11-08 03:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:15][root][INFO] - Training Epoch: 3/10, step 458/574 completed (loss: 0.1135423332452774, acc: 1.0)
[2024-11-08 03:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:15][root][INFO] - Training Epoch: 3/10, step 459/574 completed (loss: 0.19648490846157074, acc: 0.9090909361839294)
[2024-11-08 03:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:16][root][INFO] - Training Epoch: 3/10, step 460/574 completed (loss: 0.029557984322309494, acc: 1.0)
[2024-11-08 03:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:16][root][INFO] - Training Epoch: 3/10, step 461/574 completed (loss: 0.005406256299465895, acc: 1.0)
[2024-11-08 03:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:17][root][INFO] - Training Epoch: 3/10, step 462/574 completed (loss: 0.18726542592048645, acc: 0.9696969985961914)
[2024-11-08 03:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:17][root][INFO] - Training Epoch: 3/10, step 463/574 completed (loss: 0.0869700014591217, acc: 1.0)
[2024-11-08 03:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:17][root][INFO] - Training Epoch: 3/10, step 464/574 completed (loss: 0.02981334924697876, acc: 1.0)
[2024-11-08 03:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:18][root][INFO] - Training Epoch: 3/10, step 465/574 completed (loss: 0.014384426176548004, acc: 1.0)
[2024-11-08 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:18][root][INFO] - Training Epoch: 3/10, step 466/574 completed (loss: 0.06290708482265472, acc: 0.949999988079071)
[2024-11-08 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:18][root][INFO] - Training Epoch: 3/10, step 467/574 completed (loss: 0.08978379517793655, acc: 0.9473684430122375)
[2024-11-08 03:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:19][root][INFO] - Training Epoch: 3/10, step 468/574 completed (loss: 0.1708480566740036, acc: 0.949999988079071)
[2024-11-08 03:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:19][root][INFO] - Training Epoch: 3/10, step 469/574 completed (loss: 0.03380200266838074, acc: 1.0)
[2024-11-08 03:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:20][root][INFO] - Training Epoch: 3/10, step 470/574 completed (loss: 0.026657244190573692, acc: 1.0)
[2024-11-08 03:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:20][root][INFO] - Training Epoch: 3/10, step 471/574 completed (loss: 0.022664379328489304, acc: 1.0)
[2024-11-08 03:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:20][root][INFO] - Training Epoch: 3/10, step 472/574 completed (loss: 0.3313814103603363, acc: 0.9583333134651184)
[2024-11-08 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:21][root][INFO] - Training Epoch: 3/10, step 473/574 completed (loss: 0.2210969477891922, acc: 0.9523809552192688)
[2024-11-08 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:21][root][INFO] - Training Epoch: 3/10, step 474/574 completed (loss: 0.16659986972808838, acc: 0.9473684430122375)
[2024-11-08 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:22][root][INFO] - Training Epoch: 3/10, step 475/574 completed (loss: 0.1509631723165512, acc: 0.949999988079071)
[2024-11-08 03:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:22][root][INFO] - Training Epoch: 3/10, step 476/574 completed (loss: 0.047605037689208984, acc: 1.0)
[2024-11-08 03:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:22][root][INFO] - Training Epoch: 3/10, step 477/574 completed (loss: 0.17985974252223969, acc: 0.9545454382896423)
[2024-11-08 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:23][root][INFO] - Training Epoch: 3/10, step 478/574 completed (loss: 0.09572134912014008, acc: 0.9642857313156128)
[2024-11-08 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:23][root][INFO] - Training Epoch: 3/10, step 479/574 completed (loss: 0.05728717893362045, acc: 1.0)
[2024-11-08 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:23][root][INFO] - Training Epoch: 3/10, step 480/574 completed (loss: 0.08763276040554047, acc: 0.9677419066429138)
[2024-11-08 03:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:24][root][INFO] - Training Epoch: 3/10, step 481/574 completed (loss: 0.09904225915670395, acc: 1.0)
[2024-11-08 03:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:24][root][INFO] - Training Epoch: 3/10, step 482/574 completed (loss: 0.24021296203136444, acc: 0.8947368264198303)
[2024-11-08 03:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:25][root][INFO] - Training Epoch: 3/10, step 483/574 completed (loss: 0.2768591046333313, acc: 0.8636363744735718)
[2024-11-08 03:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:25][root][INFO] - Training Epoch: 3/10, step 484/574 completed (loss: 0.09707912802696228, acc: 0.9545454382896423)
[2024-11-08 03:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:26][root][INFO] - Training Epoch: 3/10, step 485/574 completed (loss: 0.03866817429661751, acc: 1.0)
[2024-11-08 03:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:26][root][INFO] - Training Epoch: 3/10, step 486/574 completed (loss: 0.22209444642066956, acc: 0.9259259104728699)
[2024-11-08 03:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:26][root][INFO] - Training Epoch: 3/10, step 487/574 completed (loss: 0.3560437262058258, acc: 0.9142857193946838)
[2024-11-08 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:27][root][INFO] - Training Epoch: 3/10, step 488/574 completed (loss: 0.35832661390304565, acc: 0.8846153616905212)
[2024-11-08 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:27][root][INFO] - Training Epoch: 3/10, step 489/574 completed (loss: 0.1031292974948883, acc: 0.9523809552192688)
[2024-11-08 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:28][root][INFO] - Training Epoch: 3/10, step 490/574 completed (loss: 0.06739693135023117, acc: 0.9473684430122375)
[2024-11-08 03:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:28][root][INFO] - Training Epoch: 3/10, step 491/574 completed (loss: 0.05008399486541748, acc: 1.0)
[2024-11-08 03:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:29][root][INFO] - Training Epoch: 3/10, step 492/574 completed (loss: 0.22028207778930664, acc: 0.9545454382896423)
[2024-11-08 03:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:29][root][INFO] - Training Epoch: 3/10, step 493/574 completed (loss: 0.22686104476451874, acc: 0.9047619104385376)
[2024-11-08 03:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:29][root][INFO] - Training Epoch: 3/10, step 494/574 completed (loss: 0.25065353512763977, acc: 0.875)
[2024-11-08 03:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:30][root][INFO] - Training Epoch: 3/10, step 495/574 completed (loss: 0.16681213676929474, acc: 0.9642857313156128)
[2024-11-08 03:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:30][root][INFO] - Training Epoch: 3/10, step 496/574 completed (loss: 0.33365875482559204, acc: 0.949999988079071)
[2024-11-08 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:31][root][INFO] - Training Epoch: 3/10, step 497/574 completed (loss: 0.17389518022537231, acc: 0.949999988079071)
[2024-11-08 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:31][root][INFO] - Training Epoch: 3/10, step 498/574 completed (loss: 0.9132403135299683, acc: 0.7894737124443054)
[2024-11-08 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:32][root][INFO] - Training Epoch: 3/10, step 499/574 completed (loss: 0.29740622639656067, acc: 0.8636363744735718)
[2024-11-08 03:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:32][root][INFO] - Training Epoch: 3/10, step 500/574 completed (loss: 0.44208431243896484, acc: 0.800000011920929)
[2024-11-08 03:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:33][root][INFO] - Training Epoch: 3/10, step 501/574 completed (loss: 0.07776839286088943, acc: 0.9583333134651184)
[2024-11-08 03:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:33][root][INFO] - Training Epoch: 3/10, step 502/574 completed (loss: 0.10096969455480576, acc: 0.9354838728904724)
[2024-11-08 03:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:33][root][INFO] - Training Epoch: 3/10, step 503/574 completed (loss: 0.10892032831907272, acc: 0.9677419066429138)
[2024-11-08 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:34][root][INFO] - Training Epoch: 3/10, step 504/574 completed (loss: 0.0723264142870903, acc: 1.0)
[2024-11-08 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:34][root][INFO] - Training Epoch: 3/10, step 505/574 completed (loss: 0.22590498626232147, acc: 0.9523809552192688)
[2024-11-08 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:35][root][INFO] - Training Epoch: 3/10, step 506/574 completed (loss: 0.42251917719841003, acc: 0.7894737124443054)
[2024-11-08 03:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:36][root][INFO] - Training Epoch: 3/10, step 507/574 completed (loss: 0.042570389807224274, acc: 1.0)
[2024-11-08 03:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:36][root][INFO] - Training Epoch: 3/10, step 508/574 completed (loss: 0.597544252872467, acc: 0.800000011920929)
[2024-11-08 03:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:37][root][INFO] - Training Epoch: 3/10, step 509/574 completed (loss: 0.1634824424982071, acc: 0.9090909361839294)
[2024-11-08 03:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:37][root][INFO] - Training Epoch: 3/10, step 510/574 completed (loss: 0.12155543267726898, acc: 0.9696969985961914)
[2024-11-08 03:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:37][root][INFO] - Training Epoch: 3/10, step 511/574 completed (loss: 0.15516510605812073, acc: 0.9333333373069763)
[2024-11-08 03:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:41][root][INFO] - Training Epoch: 3/10, step 512/574 completed (loss: 0.3127830922603607, acc: 0.8214285969734192)
[2024-11-08 03:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:42][root][INFO] - Training Epoch: 3/10, step 513/574 completed (loss: 0.2197384089231491, acc: 0.8571428656578064)
[2024-11-08 03:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:43][root][INFO] - Training Epoch: 3/10, step 514/574 completed (loss: 0.2599436044692993, acc: 0.8947368264198303)
[2024-11-08 03:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:43][root][INFO] - Training Epoch: 3/10, step 515/574 completed (loss: 0.15427261590957642, acc: 1.0)
[2024-11-08 03:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:44][root][INFO] - Training Epoch: 3/10, step 516/574 completed (loss: 0.10240073502063751, acc: 0.9523809552192688)
[2024-11-08 03:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:45][root][INFO] - Training Epoch: 3/10, step 517/574 completed (loss: 0.05372298136353493, acc: 1.0)
[2024-11-08 03:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:45][root][INFO] - Training Epoch: 3/10, step 518/574 completed (loss: 0.09793917089700699, acc: 1.0)
[2024-11-08 03:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:45][root][INFO] - Training Epoch: 3/10, step 519/574 completed (loss: 0.22404927015304565, acc: 0.9629629850387573)
[2024-11-08 03:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:46][root][INFO] - Training Epoch: 3/10, step 520/574 completed (loss: 0.10504225641489029, acc: 0.9428571462631226)
[2024-11-08 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:47][root][INFO] - Training Epoch: 3/10, step 521/574 completed (loss: 0.05598669499158859, acc: 1.0)
[2024-11-08 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:48][root][INFO] - Training Epoch: 3/10, step 522/574 completed (loss: 0.22875186800956726, acc: 0.9047619104385376)
[2024-11-08 03:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:48][root][INFO] - Training Epoch: 3/10, step 523/574 completed (loss: 0.30205580592155457, acc: 0.8947368264198303)
[2024-11-08 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:49][root][INFO] - Training Epoch: 3/10, step 524/574 completed (loss: 0.27534541487693787, acc: 0.9090909361839294)
[2024-11-08 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:49][root][INFO] - Training Epoch: 3/10, step 525/574 completed (loss: 0.06823822855949402, acc: 0.949999988079071)
[2024-11-08 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:50][root][INFO] - Training Epoch: 3/10, step 526/574 completed (loss: 0.13948699831962585, acc: 1.0)
[2024-11-08 03:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:50][root][INFO] - Training Epoch: 3/10, step 527/574 completed (loss: 0.18895766139030457, acc: 0.939393937587738)
[2024-11-08 03:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:50][root][INFO] - Training Epoch: 3/10, step 528/574 completed (loss: 0.2845419943332672, acc: 0.9285714030265808)
[2024-11-08 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:51][root][INFO] - Training Epoch: 3/10, step 529/574 completed (loss: 0.03953355923295021, acc: 1.0)
[2024-11-08 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:51][root][INFO] - Training Epoch: 3/10, step 530/574 completed (loss: 0.29170817136764526, acc: 0.8947368264198303)
[2024-11-08 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:51][root][INFO] - Training Epoch: 3/10, step 531/574 completed (loss: 0.29535776376724243, acc: 0.8999999761581421)
[2024-11-08 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:52][root][INFO] - Training Epoch: 3/10, step 532/574 completed (loss: 0.2891262471675873, acc: 0.9047619104385376)
[2024-11-08 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:52][root][INFO] - Training Epoch: 3/10, step 533/574 completed (loss: 0.055784739553928375, acc: 0.9545454382896423)
[2024-11-08 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:52][root][INFO] - Training Epoch: 3/10, step 534/574 completed (loss: 0.15116696059703827, acc: 0.9642857313156128)
[2024-11-08 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:53][root][INFO] - Training Epoch: 3/10, step 535/574 completed (loss: 0.03107997588813305, acc: 1.0)
[2024-11-08 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:53][root][INFO] - Training Epoch: 3/10, step 536/574 completed (loss: 0.16157817840576172, acc: 0.9200000166893005)
[2024-11-08 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:54][root][INFO] - Training Epoch: 3/10, step 537/574 completed (loss: 0.11630886793136597, acc: 0.9523809552192688)
[2024-11-08 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:54][root][INFO] - Training Epoch: 3/10, step 538/574 completed (loss: 0.2862025797367096, acc: 0.8947368264198303)
[2024-11-08 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:55][root][INFO] - Training Epoch: 3/10, step 539/574 completed (loss: 0.16589291393756866, acc: 0.949999988079071)
[2024-11-08 03:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:55][root][INFO] - Training Epoch: 3/10, step 540/574 completed (loss: 0.2808395028114319, acc: 0.8571428656578064)
[2024-11-08 03:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:55][root][INFO] - Training Epoch: 3/10, step 541/574 completed (loss: 0.0649072676897049, acc: 0.9523809552192688)
[2024-11-08 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:56][root][INFO] - Training Epoch: 3/10, step 542/574 completed (loss: 0.004822042770683765, acc: 1.0)
[2024-11-08 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:56][root][INFO] - Training Epoch: 3/10, step 543/574 completed (loss: 0.004136982839554548, acc: 1.0)
[2024-11-08 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:56][root][INFO] - Training Epoch: 3/10, step 544/574 completed (loss: 0.03153042495250702, acc: 1.0)
[2024-11-08 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:57][root][INFO] - Training Epoch: 3/10, step 545/574 completed (loss: 0.04592617228627205, acc: 0.949999988079071)
[2024-11-08 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:57][root][INFO] - Training Epoch: 3/10, step 546/574 completed (loss: 0.006790795363485813, acc: 1.0)
[2024-11-08 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:58][root][INFO] - Training Epoch: 3/10, step 547/574 completed (loss: 0.07625619322061539, acc: 1.0)
[2024-11-08 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:58][root][INFO] - Training Epoch: 3/10, step 548/574 completed (loss: 0.11601241677999496, acc: 0.9523809552192688)
[2024-11-08 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:58][root][INFO] - Training Epoch: 3/10, step 549/574 completed (loss: 0.001835478120483458, acc: 1.0)
[2024-11-08 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:59][root][INFO] - Training Epoch: 3/10, step 550/574 completed (loss: 0.001475786091759801, acc: 1.0)
[2024-11-08 03:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:47:59][root][INFO] - Training Epoch: 3/10, step 551/574 completed (loss: 0.04979029297828674, acc: 1.0)
[2024-11-08 03:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:00][root][INFO] - Training Epoch: 3/10, step 552/574 completed (loss: 0.08406141400337219, acc: 0.9629629850387573)
[2024-11-08 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:00][root][INFO] - Training Epoch: 3/10, step 553/574 completed (loss: 0.05880828574299812, acc: 1.0)
[2024-11-08 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:00][root][INFO] - Training Epoch: 3/10, step 554/574 completed (loss: 0.03877999261021614, acc: 1.0)
[2024-11-08 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:01][root][INFO] - Training Epoch: 3/10, step 555/574 completed (loss: 0.22513017058372498, acc: 0.949999988079071)
[2024-11-08 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:01][root][INFO] - Training Epoch: 3/10, step 556/574 completed (loss: 0.05724414810538292, acc: 0.9523809552192688)
[2024-11-08 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:01][root][INFO] - Training Epoch: 3/10, step 557/574 completed (loss: 0.007383363321423531, acc: 1.0)
[2024-11-08 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:02][root][INFO] - Training Epoch: 3/10, step 558/574 completed (loss: 0.10179237276315689, acc: 0.9642857313156128)
[2024-11-08 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:02][root][INFO] - Training Epoch: 3/10, step 559/574 completed (loss: 0.09247618168592453, acc: 0.9629629850387573)
[2024-11-08 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:03][root][INFO] - Training Epoch: 3/10, step 560/574 completed (loss: 0.12998804450035095, acc: 0.9696969985961914)
[2024-11-08 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:03][root][INFO] - Training Epoch: 3/10, step 561/574 completed (loss: 0.08180917799472809, acc: 1.0)
[2024-11-08 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:03][root][INFO] - Training Epoch: 3/10, step 562/574 completed (loss: 0.01579287275671959, acc: 1.0)
[2024-11-08 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:04][root][INFO] - Training Epoch: 3/10, step 563/574 completed (loss: 0.04069409891963005, acc: 1.0)
[2024-11-08 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:04][root][INFO] - Training Epoch: 3/10, step 564/574 completed (loss: 0.05534406751394272, acc: 0.9545454382896423)
[2024-11-08 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:04][root][INFO] - Training Epoch: 3/10, step 565/574 completed (loss: 0.25237929821014404, acc: 0.8999999761581421)
[2024-11-08 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:05][root][INFO] - Training Epoch: 3/10, step 566/574 completed (loss: 0.018917357549071312, acc: 1.0)
[2024-11-08 03:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:05][root][INFO] - Training Epoch: 3/10, step 567/574 completed (loss: 0.013765388168394566, acc: 1.0)
[2024-11-08 03:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:46][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2997, device='cuda:0') eval_epoch_loss=tensor(0.2621, device='cuda:0') eval_epoch_acc=tensor(0.9271, device='cuda:0')
[2024-11-08 03:48:46][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:48:46][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:48:50][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_3_step_568_loss_0.26209765672683716/model.pt
[2024-11-08 03:48:50][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:48:50][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 3 is 0.9270737171173096
[2024-11-08 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:50][root][INFO] - Training Epoch: 3/10, step 568/574 completed (loss: 0.04932398721575737, acc: 0.9677419066429138)
[2024-11-08 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:51][root][INFO] - Training Epoch: 3/10, step 569/574 completed (loss: 0.01312301866710186, acc: 1.0)
[2024-11-08 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:51][root][INFO] - Training Epoch: 3/10, step 570/574 completed (loss: 0.0024777764920145273, acc: 1.0)
[2024-11-08 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:52][root][INFO] - Training Epoch: 3/10, step 571/574 completed (loss: 0.007163085509091616, acc: 1.0)
[2024-11-08 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:52][root][INFO] - Training Epoch: 3/10, step 572/574 completed (loss: 0.011542932130396366, acc: 1.0)
[2024-11-08 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:52][root][INFO] - Training Epoch: 3/10, step 573/574 completed (loss: 0.22319531440734863, acc: 0.949999988079071)
[2024-11-08 03:48:53][slam_llm.utils.train_utils][INFO] - Epoch 3: train_perplexity=1.2038, train_epoch_loss=0.1854, epoch time 467.2710391599685s
[2024-11-08 03:48:53][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 03:48:53][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2024-11-08 03:48:53][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 03:48:53][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 2
[2024-11-08 03:48:53][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 03:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:54][root][INFO] - Training Epoch: 4/10, step 0/574 completed (loss: 0.03921401500701904, acc: 0.9642857313156128)
[2024-11-08 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:54][root][INFO] - Training Epoch: 4/10, step 1/574 completed (loss: 0.016651632264256477, acc: 1.0)
[2024-11-08 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:55][root][INFO] - Training Epoch: 4/10, step 2/574 completed (loss: 0.02026527002453804, acc: 1.0)
[2024-11-08 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:55][root][INFO] - Training Epoch: 4/10, step 3/574 completed (loss: 0.05449637398123741, acc: 1.0)
[2024-11-08 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:55][root][INFO] - Training Epoch: 4/10, step 4/574 completed (loss: 0.16354219615459442, acc: 0.9523809552192688)
[2024-11-08 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:56][root][INFO] - Training Epoch: 4/10, step 5/574 completed (loss: 0.30508026480674744, acc: 0.8947368264198303)
[2024-11-08 03:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:56][root][INFO] - Training Epoch: 4/10, step 6/574 completed (loss: 0.004755598027259111, acc: 1.0)
[2024-11-08 03:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:57][root][INFO] - Training Epoch: 4/10, step 7/574 completed (loss: 0.058626435697078705, acc: 1.0)
[2024-11-08 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:57][root][INFO] - Training Epoch: 4/10, step 8/574 completed (loss: 0.01649223081767559, acc: 1.0)
[2024-11-08 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:57][root][INFO] - Training Epoch: 4/10, step 9/574 completed (loss: 0.19890068471431732, acc: 0.9354838728904724)
[2024-11-08 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:58][root][INFO] - Training Epoch: 4/10, step 10/574 completed (loss: 0.012634491547942162, acc: 1.0)
[2024-11-08 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:58][root][INFO] - Training Epoch: 4/10, step 11/574 completed (loss: 0.11046011000871658, acc: 0.9599999785423279)
[2024-11-08 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:58][root][INFO] - Training Epoch: 4/10, step 12/574 completed (loss: 0.015780478715896606, acc: 1.0)
[2024-11-08 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:59][root][INFO] - Training Epoch: 4/10, step 13/574 completed (loss: 0.04036941006779671, acc: 1.0)
[2024-11-08 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:48:59][root][INFO] - Training Epoch: 4/10, step 14/574 completed (loss: 0.14264269173145294, acc: 0.95652174949646)
[2024-11-08 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:00][root][INFO] - Training Epoch: 4/10, step 15/574 completed (loss: 0.004032088443636894, acc: 1.0)
[2024-11-08 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:00][root][INFO] - Training Epoch: 4/10, step 16/574 completed (loss: 0.12065845727920532, acc: 0.9642857313156128)
[2024-11-08 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:01][root][INFO] - Training Epoch: 4/10, step 17/574 completed (loss: 0.039378222078084946, acc: 1.0)
[2024-11-08 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:01][root][INFO] - Training Epoch: 4/10, step 18/574 completed (loss: 0.1851314902305603, acc: 0.9090909361839294)
[2024-11-08 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:01][root][INFO] - Training Epoch: 4/10, step 19/574 completed (loss: 0.18617483973503113, acc: 0.9473684430122375)
[2024-11-08 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:02][root][INFO] - Training Epoch: 4/10, step 20/574 completed (loss: 0.28707370162010193, acc: 0.8947368264198303)
[2024-11-08 03:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:02][root][INFO] - Training Epoch: 4/10, step 21/574 completed (loss: 0.08564133942127228, acc: 1.0)
[2024-11-08 03:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:03][root][INFO] - Training Epoch: 4/10, step 22/574 completed (loss: 0.17827332019805908, acc: 0.9523809552192688)
[2024-11-08 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:03][root][INFO] - Training Epoch: 4/10, step 23/574 completed (loss: 0.0019984280224889517, acc: 1.0)
[2024-11-08 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:03][root][INFO] - Training Epoch: 4/10, step 24/574 completed (loss: 0.08945000171661377, acc: 0.9666666388511658)
[2024-11-08 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:04][root][INFO] - Training Epoch: 4/10, step 25/574 completed (loss: 0.27007564902305603, acc: 0.9047619104385376)
[2024-11-08 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:04][root][INFO] - Training Epoch: 4/10, step 26/574 completed (loss: 0.5048853754997253, acc: 0.8095238208770752)
[2024-11-08 03:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:06][root][INFO] - Training Epoch: 4/10, step 27/574 completed (loss: 0.04696885868906975, acc: 1.0)
[2024-11-08 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:06][root][INFO] - Training Epoch: 4/10, step 28/574 completed (loss: 0.01890847645699978, acc: 1.0)
[2024-11-08 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:07][root][INFO] - Training Epoch: 4/10, step 29/574 completed (loss: 0.2080036699771881, acc: 0.949999988079071)
[2024-11-08 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:07][root][INFO] - Training Epoch: 4/10, step 30/574 completed (loss: 0.07344415038824081, acc: 0.9545454382896423)
[2024-11-08 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:07][root][INFO] - Training Epoch: 4/10, step 31/574 completed (loss: 0.21228943765163422, acc: 0.9655172228813171)
[2024-11-08 03:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:08][root][INFO] - Training Epoch: 4/10, step 32/574 completed (loss: 0.20303615927696228, acc: 0.9090909361839294)
[2024-11-08 03:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:08][root][INFO] - Training Epoch: 4/10, step 33/574 completed (loss: 0.03566873446106911, acc: 1.0)
[2024-11-08 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:09][root][INFO] - Training Epoch: 4/10, step 34/574 completed (loss: 0.009230466559529305, acc: 1.0)
[2024-11-08 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:09][root][INFO] - Training Epoch: 4/10, step 35/574 completed (loss: 0.1361328661441803, acc: 0.9473684430122375)
[2024-11-08 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:10][root][INFO] - Training Epoch: 4/10, step 36/574 completed (loss: 0.2940191626548767, acc: 0.8999999761581421)
[2024-11-08 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:10][root][INFO] - Training Epoch: 4/10, step 37/574 completed (loss: 0.03673877194523811, acc: 1.0)
[2024-11-08 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:11][root][INFO] - Training Epoch: 4/10, step 38/574 completed (loss: 0.02399061992764473, acc: 1.0)
[2024-11-08 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:11][root][INFO] - Training Epoch: 4/10, step 39/574 completed (loss: 0.11994586884975433, acc: 0.9642857313156128)
[2024-11-08 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:12][root][INFO] - Training Epoch: 4/10, step 40/574 completed (loss: 0.20389097929000854, acc: 0.9428571462631226)
[2024-11-08 03:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:12][root][INFO] - Training Epoch: 4/10, step 41/574 completed (loss: 0.020908065140247345, acc: 1.0)
[2024-11-08 03:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:12][root][INFO] - Training Epoch: 4/10, step 42/574 completed (loss: 0.24686169624328613, acc: 0.8999999761581421)
[2024-11-08 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:13][root][INFO] - Training Epoch: 4/10, step 43/574 completed (loss: 0.29997387528419495, acc: 0.8947368264198303)
[2024-11-08 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:14][root][INFO] - Training Epoch: 4/10, step 44/574 completed (loss: 0.3530068099498749, acc: 0.8636363744735718)
[2024-11-08 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:14][root][INFO] - Training Epoch: 4/10, step 45/574 completed (loss: 0.24857938289642334, acc: 0.8571428656578064)
[2024-11-08 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:14][root][INFO] - Training Epoch: 4/10, step 46/574 completed (loss: 0.10429022461175919, acc: 1.0)
[2024-11-08 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:15][root][INFO] - Training Epoch: 4/10, step 47/574 completed (loss: 0.0129637960344553, acc: 1.0)
[2024-11-08 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:15][root][INFO] - Training Epoch: 4/10, step 48/574 completed (loss: 0.0099861491471529, acc: 1.0)
[2024-11-08 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:16][root][INFO] - Training Epoch: 4/10, step 49/574 completed (loss: 0.017752325162291527, acc: 1.0)
[2024-11-08 03:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:16][root][INFO] - Training Epoch: 4/10, step 50/574 completed (loss: 0.10819073766469955, acc: 0.9523809552192688)
[2024-11-08 03:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:16][root][INFO] - Training Epoch: 4/10, step 51/574 completed (loss: 0.21769559383392334, acc: 0.8947368264198303)
[2024-11-08 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:17][root][INFO] - Training Epoch: 4/10, step 52/574 completed (loss: 0.23087504506111145, acc: 0.8999999761581421)
[2024-11-08 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:17][root][INFO] - Training Epoch: 4/10, step 53/574 completed (loss: 0.10047827661037445, acc: 0.9523809552192688)
[2024-11-08 03:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:18][root][INFO] - Training Epoch: 4/10, step 54/574 completed (loss: 0.1822279542684555, acc: 0.9545454382896423)
[2024-11-08 03:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:18][root][INFO] - Training Epoch: 4/10, step 55/574 completed (loss: 0.024081775918602943, acc: 1.0)
[2024-11-08 03:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:22][root][INFO] - Training Epoch: 4/10, step 56/574 completed (loss: 0.12545958161354065, acc: 0.9523809552192688)
[2024-11-08 03:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:24][root][INFO] - Training Epoch: 4/10, step 57/574 completed (loss: 0.24886734783649445, acc: 0.9047619104385376)
[2024-11-08 03:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:25][root][INFO] - Training Epoch: 4/10, step 58/574 completed (loss: 0.06729064136743546, acc: 0.9473684430122375)
[2024-11-08 03:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:26][root][INFO] - Training Epoch: 4/10, step 59/574 completed (loss: 0.1299862414598465, acc: 0.9545454382896423)
[2024-11-08 03:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:27][root][INFO] - Training Epoch: 4/10, step 60/574 completed (loss: 0.09640026837587357, acc: 0.9473684430122375)
[2024-11-08 03:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:27][root][INFO] - Training Epoch: 4/10, step 61/574 completed (loss: 0.1315605342388153, acc: 0.9523809552192688)
[2024-11-08 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:28][root][INFO] - Training Epoch: 4/10, step 62/574 completed (loss: 0.09264521300792694, acc: 0.9722222089767456)
[2024-11-08 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:28][root][INFO] - Training Epoch: 4/10, step 63/574 completed (loss: 0.14874254167079926, acc: 0.9285714030265808)
[2024-11-08 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:29][root][INFO] - Training Epoch: 4/10, step 64/574 completed (loss: 0.22364450991153717, acc: 0.9047619104385376)
[2024-11-08 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:29][root][INFO] - Training Epoch: 4/10, step 65/574 completed (loss: 0.290189653635025, acc: 0.8999999761581421)
[2024-11-08 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:30][root][INFO] - Training Epoch: 4/10, step 66/574 completed (loss: 0.17780502140522003, acc: 0.8947368264198303)
[2024-11-08 03:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:30][root][INFO] - Training Epoch: 4/10, step 67/574 completed (loss: 0.15803202986717224, acc: 0.949999988079071)
[2024-11-08 03:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:30][root][INFO] - Training Epoch: 4/10, step 68/574 completed (loss: 0.008182692341506481, acc: 1.0)
[2024-11-08 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:31][root][INFO] - Training Epoch: 4/10, step 69/574 completed (loss: 0.006307668052613735, acc: 1.0)
[2024-11-08 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:31][root][INFO] - Training Epoch: 4/10, step 70/574 completed (loss: 0.044197216629981995, acc: 1.0)
[2024-11-08 03:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:32][root][INFO] - Training Epoch: 4/10, step 71/574 completed (loss: 0.10737380385398865, acc: 0.9666666388511658)
[2024-11-08 03:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:32][root][INFO] - Training Epoch: 4/10, step 72/574 completed (loss: 0.08057687431573868, acc: 0.9523809552192688)
[2024-11-08 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:33][root][INFO] - Training Epoch: 4/10, step 73/574 completed (loss: 0.4066583812236786, acc: 0.7894737124443054)
[2024-11-08 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:33][root][INFO] - Training Epoch: 4/10, step 74/574 completed (loss: 0.06046805530786514, acc: 1.0)
[2024-11-08 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:34][root][INFO] - Training Epoch: 4/10, step 75/574 completed (loss: 0.18805548548698425, acc: 0.9047619104385376)
[2024-11-08 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:34][root][INFO] - Training Epoch: 4/10, step 76/574 completed (loss: 0.13455834984779358, acc: 0.9545454382896423)
[2024-11-08 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:35][root][INFO] - Training Epoch: 4/10, step 77/574 completed (loss: 0.037439774721860886, acc: 1.0)
[2024-11-08 03:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:35][root][INFO] - Training Epoch: 4/10, step 78/574 completed (loss: 0.014692307449877262, acc: 1.0)
[2024-11-08 03:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:35][root][INFO] - Training Epoch: 4/10, step 79/574 completed (loss: 0.12830768525600433, acc: 0.9428571462631226)
[2024-11-08 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:36][root][INFO] - Training Epoch: 4/10, step 80/574 completed (loss: 0.010863670147955418, acc: 1.0)
[2024-11-08 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:36][root][INFO] - Training Epoch: 4/10, step 81/574 completed (loss: 0.0540589839220047, acc: 0.9523809552192688)
[2024-11-08 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:37][root][INFO] - Training Epoch: 4/10, step 82/574 completed (loss: 0.1981225311756134, acc: 0.9473684430122375)
[2024-11-08 03:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:37][root][INFO] - Training Epoch: 4/10, step 83/574 completed (loss: 0.022110678255558014, acc: 1.0)
[2024-11-08 03:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:37][root][INFO] - Training Epoch: 4/10, step 84/574 completed (loss: 0.008438844233751297, acc: 1.0)
[2024-11-08 03:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:38][root][INFO] - Training Epoch: 4/10, step 85/574 completed (loss: 0.14434754848480225, acc: 0.9523809552192688)
[2024-11-08 03:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:38][root][INFO] - Training Epoch: 4/10, step 86/574 completed (loss: 0.06741981953382492, acc: 1.0)
[2024-11-08 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:39][root][INFO] - Training Epoch: 4/10, step 87/574 completed (loss: 0.49880772829055786, acc: 0.8461538553237915)
[2024-11-08 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:40][root][INFO] - Training Epoch: 4/10, step 88/574 completed (loss: 0.051109082996845245, acc: 1.0)
[2024-11-08 03:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:41][root][INFO] - Training Epoch: 4/10, step 89/574 completed (loss: 0.25768136978149414, acc: 0.8947368264198303)
[2024-11-08 03:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:43][root][INFO] - Training Epoch: 4/10, step 90/574 completed (loss: 0.37573176622390747, acc: 0.8421052694320679)
[2024-11-08 03:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:44][root][INFO] - Training Epoch: 4/10, step 91/574 completed (loss: 0.15581659972667694, acc: 0.9090909361839294)
[2024-11-08 03:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:45][root][INFO] - Training Epoch: 4/10, step 92/574 completed (loss: 0.12041451781988144, acc: 0.9523809552192688)
[2024-11-08 03:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:46][root][INFO] - Training Epoch: 4/10, step 93/574 completed (loss: 0.541425347328186, acc: 0.807692289352417)
[2024-11-08 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:47][root][INFO] - Training Epoch: 4/10, step 94/574 completed (loss: 0.18811801075935364, acc: 0.8928571343421936)
[2024-11-08 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:47][root][INFO] - Training Epoch: 4/10, step 95/574 completed (loss: 0.0928998738527298, acc: 0.9523809552192688)
[2024-11-08 03:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:48][root][INFO] - Training Epoch: 4/10, step 96/574 completed (loss: 0.27458974719047546, acc: 0.8947368264198303)
[2024-11-08 03:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:48][root][INFO] - Training Epoch: 4/10, step 97/574 completed (loss: 0.4032876491546631, acc: 0.8999999761581421)
[2024-11-08 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:49][root][INFO] - Training Epoch: 4/10, step 98/574 completed (loss: 0.22493934631347656, acc: 0.9047619104385376)
[2024-11-08 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:49][root][INFO] - Training Epoch: 4/10, step 99/574 completed (loss: 0.1886039823293686, acc: 0.9090909361839294)
[2024-11-08 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:50][root][INFO] - Training Epoch: 4/10, step 100/574 completed (loss: 0.10468102991580963, acc: 0.9523809552192688)
[2024-11-08 03:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:50][root][INFO] - Training Epoch: 4/10, step 101/574 completed (loss: 0.04165889322757721, acc: 1.0)
[2024-11-08 03:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:50][root][INFO] - Training Epoch: 4/10, step 102/574 completed (loss: 0.05751519650220871, acc: 1.0)
[2024-11-08 03:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:51][root][INFO] - Training Epoch: 4/10, step 103/574 completed (loss: 0.04689024016261101, acc: 1.0)
[2024-11-08 03:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:51][root][INFO] - Training Epoch: 4/10, step 104/574 completed (loss: 0.07960974425077438, acc: 0.9545454382896423)
[2024-11-08 03:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:52][root][INFO] - Training Epoch: 4/10, step 105/574 completed (loss: 0.14904209971427917, acc: 0.9473684430122375)
[2024-11-08 03:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:52][root][INFO] - Training Epoch: 4/10, step 106/574 completed (loss: 0.09805016219615936, acc: 0.9545454382896423)
[2024-11-08 03:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:53][root][INFO] - Training Epoch: 4/10, step 107/574 completed (loss: 0.01151137612760067, acc: 1.0)
[2024-11-08 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:53][root][INFO] - Training Epoch: 4/10, step 108/574 completed (loss: 0.06573276221752167, acc: 0.9666666388511658)
[2024-11-08 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:53][root][INFO] - Training Epoch: 4/10, step 109/574 completed (loss: 0.04526830092072487, acc: 1.0)
[2024-11-08 03:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:54][root][INFO] - Training Epoch: 4/10, step 110/574 completed (loss: 0.011386305093765259, acc: 1.0)
[2024-11-08 03:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:54][root][INFO] - Training Epoch: 4/10, step 111/574 completed (loss: 0.12744586169719696, acc: 0.8947368264198303)
[2024-11-08 03:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:55][root][INFO] - Training Epoch: 4/10, step 112/574 completed (loss: 0.006866686511784792, acc: 1.0)
[2024-11-08 03:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:55][root][INFO] - Training Epoch: 4/10, step 113/574 completed (loss: 0.006473842076957226, acc: 1.0)
[2024-11-08 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:56][root][INFO] - Training Epoch: 4/10, step 114/574 completed (loss: 0.04619889333844185, acc: 1.0)
[2024-11-08 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:56][root][INFO] - Training Epoch: 4/10, step 115/574 completed (loss: 0.03625722974538803, acc: 1.0)
[2024-11-08 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:57][root][INFO] - Training Epoch: 4/10, step 116/574 completed (loss: 0.014798414893448353, acc: 1.0)
[2024-11-08 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:57][root][INFO] - Training Epoch: 4/10, step 117/574 completed (loss: 0.12434768676757812, acc: 0.8999999761581421)
[2024-11-08 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:58][root][INFO] - Training Epoch: 4/10, step 118/574 completed (loss: 0.0354640856385231, acc: 1.0)
[2024-11-08 03:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:59][root][INFO] - Training Epoch: 4/10, step 119/574 completed (loss: 0.011867481283843517, acc: 1.0)
[2024-11-08 03:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:49:59][root][INFO] - Training Epoch: 4/10, step 120/574 completed (loss: 0.008032050915062428, acc: 1.0)
[2024-11-08 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:00][root][INFO] - Training Epoch: 4/10, step 121/574 completed (loss: 0.16524627804756165, acc: 0.9545454382896423)
[2024-11-08 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:00][root][INFO] - Training Epoch: 4/10, step 122/574 completed (loss: 0.008866524323821068, acc: 1.0)
[2024-11-08 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:01][root][INFO] - Training Epoch: 4/10, step 123/574 completed (loss: 0.5010923743247986, acc: 0.8461538553237915)
[2024-11-08 03:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:01][root][INFO] - Training Epoch: 4/10, step 124/574 completed (loss: 0.17630133032798767, acc: 0.949999988079071)
[2024-11-08 03:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:02][root][INFO] - Training Epoch: 4/10, step 125/574 completed (loss: 0.35939785838127136, acc: 0.8999999761581421)
[2024-11-08 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:02][root][INFO] - Training Epoch: 4/10, step 126/574 completed (loss: 0.01749572902917862, acc: 1.0)
[2024-11-08 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:03][root][INFO] - Training Epoch: 4/10, step 127/574 completed (loss: 0.3370072543621063, acc: 0.7727272510528564)
[2024-11-08 03:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:03][root][INFO] - Training Epoch: 4/10, step 128/574 completed (loss: 0.10537102073431015, acc: 1.0)
[2024-11-08 03:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:03][root][INFO] - Training Epoch: 4/10, step 129/574 completed (loss: 0.013375138863921165, acc: 1.0)
[2024-11-08 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:04][root][INFO] - Training Epoch: 4/10, step 130/574 completed (loss: 0.21453391015529633, acc: 0.939393937587738)
[2024-11-08 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:04][root][INFO] - Training Epoch: 4/10, step 131/574 completed (loss: 0.10970283299684525, acc: 0.9629629850387573)
[2024-11-08 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:05][root][INFO] - Training Epoch: 4/10, step 132/574 completed (loss: 0.35837048292160034, acc: 0.8484848737716675)
[2024-11-08 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:05][root][INFO] - Training Epoch: 4/10, step 133/574 completed (loss: 0.11018548160791397, acc: 0.949999988079071)
[2024-11-08 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:06][root][INFO] - Training Epoch: 4/10, step 134/574 completed (loss: 0.17304067313671112, acc: 0.949999988079071)
[2024-11-08 03:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:06][root][INFO] - Training Epoch: 4/10, step 135/574 completed (loss: 0.37871208786964417, acc: 0.8947368264198303)
[2024-11-08 03:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:06][root][INFO] - Training Epoch: 4/10, step 136/574 completed (loss: 0.34730949997901917, acc: 0.9545454382896423)
[2024-11-08 03:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:51][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2622, device='cuda:0') eval_epoch_loss=tensor(0.2328, device='cuda:0') eval_epoch_acc=tensor(0.9348, device='cuda:0')
[2024-11-08 03:50:51][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:50:51][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:50:54][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_137_loss_0.23283101618289948/model.pt
[2024-11-08 03:50:54][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:50:54][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 4 is 0.23283101618289948
[2024-11-08 03:50:54][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.9348262548446655
[2024-11-08 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:55][root][INFO] - Training Epoch: 4/10, step 137/574 completed (loss: 0.3631432354450226, acc: 0.8999999761581421)
[2024-11-08 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:55][root][INFO] - Training Epoch: 4/10, step 138/574 completed (loss: 0.08394096791744232, acc: 0.9545454382896423)
[2024-11-08 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:56][root][INFO] - Training Epoch: 4/10, step 139/574 completed (loss: 0.11332346498966217, acc: 0.939393937587738)
[2024-11-08 03:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:56][root][INFO] - Training Epoch: 4/10, step 140/574 completed (loss: 0.05387483537197113, acc: 1.0)
[2024-11-08 03:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:57][root][INFO] - Training Epoch: 4/10, step 141/574 completed (loss: 0.10441148281097412, acc: 0.939393937587738)
[2024-11-08 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:57][root][INFO] - Training Epoch: 4/10, step 142/574 completed (loss: 0.33582913875579834, acc: 0.8999999761581421)
[2024-11-08 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:58][root][INFO] - Training Epoch: 4/10, step 143/574 completed (loss: 0.13018085062503815, acc: 0.8999999761581421)
[2024-11-08 03:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:58][root][INFO] - Training Epoch: 4/10, step 144/574 completed (loss: 0.03837893158197403, acc: 1.0)
[2024-11-08 03:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:59][root][INFO] - Training Epoch: 4/10, step 145/574 completed (loss: 0.4274156391620636, acc: 0.8636363744735718)
[2024-11-08 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:50:59][root][INFO] - Training Epoch: 4/10, step 146/574 completed (loss: 0.3306366801261902, acc: 0.949999988079071)
[2024-11-08 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:00][root][INFO] - Training Epoch: 4/10, step 147/574 completed (loss: 0.297160804271698, acc: 0.9090909361839294)
[2024-11-08 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:00][root][INFO] - Training Epoch: 4/10, step 148/574 completed (loss: 0.1576937586069107, acc: 0.939393937587738)
[2024-11-08 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:00][root][INFO] - Training Epoch: 4/10, step 149/574 completed (loss: 0.043166372925043106, acc: 1.0)
[2024-11-08 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:01][root][INFO] - Training Epoch: 4/10, step 150/574 completed (loss: 0.1308262050151825, acc: 0.9677419066429138)
[2024-11-08 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:01][root][INFO] - Training Epoch: 4/10, step 151/574 completed (loss: 0.2232407033443451, acc: 0.949999988079071)
[2024-11-08 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:02][root][INFO] - Training Epoch: 4/10, step 152/574 completed (loss: 0.004991746041923761, acc: 1.0)
[2024-11-08 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:02][root][INFO] - Training Epoch: 4/10, step 153/574 completed (loss: 0.5871438384056091, acc: 0.8947368264198303)
[2024-11-08 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:02][root][INFO] - Training Epoch: 4/10, step 154/574 completed (loss: 0.029886335134506226, acc: 1.0)
[2024-11-08 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:03][root][INFO] - Training Epoch: 4/10, step 155/574 completed (loss: 0.08202214539051056, acc: 1.0)
[2024-11-08 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:03][root][INFO] - Training Epoch: 4/10, step 156/574 completed (loss: 0.24620366096496582, acc: 0.9090909361839294)
[2024-11-08 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:04][root][INFO] - Training Epoch: 4/10, step 157/574 completed (loss: 0.2204350233078003, acc: 0.939393937587738)
[2024-11-08 03:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:06][root][INFO] - Training Epoch: 4/10, step 158/574 completed (loss: 0.4854513704776764, acc: 0.8500000238418579)
[2024-11-08 03:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:07][root][INFO] - Training Epoch: 4/10, step 159/574 completed (loss: 0.22659580409526825, acc: 0.8500000238418579)
[2024-11-08 03:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:07][root][INFO] - Training Epoch: 4/10, step 160/574 completed (loss: 0.28460660576820374, acc: 0.9473684430122375)
[2024-11-08 03:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:08][root][INFO] - Training Epoch: 4/10, step 161/574 completed (loss: 0.3114962577819824, acc: 0.8181818127632141)
[2024-11-08 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:09][root][INFO] - Training Epoch: 4/10, step 162/574 completed (loss: 0.40897655487060547, acc: 0.800000011920929)
[2024-11-08 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:09][root][INFO] - Training Epoch: 4/10, step 163/574 completed (loss: 0.2600620687007904, acc: 0.9200000166893005)
[2024-11-08 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:10][root][INFO] - Training Epoch: 4/10, step 164/574 completed (loss: 0.10930904746055603, acc: 0.9629629850387573)
[2024-11-08 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:10][root][INFO] - Training Epoch: 4/10, step 165/574 completed (loss: 0.4780762493610382, acc: 0.9166666865348816)
[2024-11-08 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:11][root][INFO] - Training Epoch: 4/10, step 166/574 completed (loss: 0.06768598407506943, acc: 0.95652174949646)
[2024-11-08 03:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:11][root][INFO] - Training Epoch: 4/10, step 167/574 completed (loss: 0.24968363344669342, acc: 0.9523809552192688)
[2024-11-08 03:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:12][root][INFO] - Training Epoch: 4/10, step 168/574 completed (loss: 0.10872869938611984, acc: 0.9473684430122375)
[2024-11-08 03:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:12][root][INFO] - Training Epoch: 4/10, step 169/574 completed (loss: 0.02423393540084362, acc: 1.0)
[2024-11-08 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:14][root][INFO] - Training Epoch: 4/10, step 170/574 completed (loss: 0.21297557651996613, acc: 0.949999988079071)
[2024-11-08 03:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:14][root][INFO] - Training Epoch: 4/10, step 171/574 completed (loss: 0.04132566973567009, acc: 1.0)
[2024-11-08 03:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:15][root][INFO] - Training Epoch: 4/10, step 172/574 completed (loss: 0.11149196326732635, acc: 0.939393937587738)
[2024-11-08 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:15][root][INFO] - Training Epoch: 4/10, step 173/574 completed (loss: 0.15409785509109497, acc: 0.9333333373069763)
[2024-11-08 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:16][root][INFO] - Training Epoch: 4/10, step 174/574 completed (loss: 0.06857599318027496, acc: 1.0)
[2024-11-08 03:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:16][root][INFO] - Training Epoch: 4/10, step 175/574 completed (loss: 0.1724410355091095, acc: 0.9523809552192688)
[2024-11-08 03:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:17][root][INFO] - Training Epoch: 4/10, step 176/574 completed (loss: 0.037185002118349075, acc: 1.0)
[2024-11-08 03:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:18][root][INFO] - Training Epoch: 4/10, step 177/574 completed (loss: 0.11694460362195969, acc: 0.9545454382896423)
[2024-11-08 03:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:19][root][INFO] - Training Epoch: 4/10, step 178/574 completed (loss: 0.29844292998313904, acc: 0.8947368264198303)
[2024-11-08 03:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:19][root][INFO] - Training Epoch: 4/10, step 179/574 completed (loss: 0.07011633366346359, acc: 0.9545454382896423)
[2024-11-08 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:20][root][INFO] - Training Epoch: 4/10, step 180/574 completed (loss: 0.004737111274152994, acc: 1.0)
[2024-11-08 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:20][root][INFO] - Training Epoch: 4/10, step 181/574 completed (loss: 0.014370729215443134, acc: 1.0)
[2024-11-08 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:21][root][INFO] - Training Epoch: 4/10, step 182/574 completed (loss: 0.6797045469284058, acc: 0.9428571462631226)
[2024-11-08 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:21][root][INFO] - Training Epoch: 4/10, step 183/574 completed (loss: 0.015745429322123528, acc: 1.0)
[2024-11-08 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:21][root][INFO] - Training Epoch: 4/10, step 184/574 completed (loss: 0.07742919027805328, acc: 1.0)
[2024-11-08 03:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:22][root][INFO] - Training Epoch: 4/10, step 185/574 completed (loss: 0.011284345760941505, acc: 1.0)
[2024-11-08 03:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:22][root][INFO] - Training Epoch: 4/10, step 186/574 completed (loss: 0.1277552992105484, acc: 0.8999999761581421)
[2024-11-08 03:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:23][root][INFO] - Training Epoch: 4/10, step 187/574 completed (loss: 0.1465163677930832, acc: 0.9047619104385376)
[2024-11-08 03:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:24][root][INFO] - Training Epoch: 4/10, step 188/574 completed (loss: 0.044009823352098465, acc: 1.0)
[2024-11-08 03:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:24][root][INFO] - Training Epoch: 4/10, step 189/574 completed (loss: 0.2864167094230652, acc: 0.9200000166893005)
[2024-11-08 03:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:25][root][INFO] - Training Epoch: 4/10, step 190/574 completed (loss: 0.29120364785194397, acc: 0.8500000238418579)
[2024-11-08 03:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:26][root][INFO] - Training Epoch: 4/10, step 191/574 completed (loss: 0.4831438958644867, acc: 0.8095238208770752)
[2024-11-08 03:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:28][root][INFO] - Training Epoch: 4/10, step 192/574 completed (loss: 0.3411695659160614, acc: 0.7894737124443054)
[2024-11-08 03:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:29][root][INFO] - Training Epoch: 4/10, step 193/574 completed (loss: 0.07077178359031677, acc: 1.0)
[2024-11-08 03:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:31][root][INFO] - Training Epoch: 4/10, step 194/574 completed (loss: 0.28877195715904236, acc: 0.9047619104385376)
[2024-11-08 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:32][root][INFO] - Training Epoch: 4/10, step 195/574 completed (loss: 0.16164416074752808, acc: 0.9166666865348816)
[2024-11-08 03:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:32][root][INFO] - Training Epoch: 4/10, step 196/574 completed (loss: 0.034322429448366165, acc: 1.0)
[2024-11-08 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:33][root][INFO] - Training Epoch: 4/10, step 197/574 completed (loss: 0.11472494900226593, acc: 0.9666666388511658)
[2024-11-08 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:33][root][INFO] - Training Epoch: 4/10, step 198/574 completed (loss: 0.024401996284723282, acc: 1.0)
[2024-11-08 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:34][root][INFO] - Training Epoch: 4/10, step 199/574 completed (loss: 0.08534838259220123, acc: 0.949999988079071)
[2024-11-08 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:34][root][INFO] - Training Epoch: 4/10, step 200/574 completed (loss: 0.030552491545677185, acc: 1.0)
[2024-11-08 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:35][root][INFO] - Training Epoch: 4/10, step 201/574 completed (loss: 0.1266351193189621, acc: 0.9090909361839294)
[2024-11-08 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:35][root][INFO] - Training Epoch: 4/10, step 202/574 completed (loss: 0.03400227427482605, acc: 1.0)
[2024-11-08 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:35][root][INFO] - Training Epoch: 4/10, step 203/574 completed (loss: 0.3767387568950653, acc: 0.8695651888847351)
[2024-11-08 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:36][root][INFO] - Training Epoch: 4/10, step 204/574 completed (loss: 0.06616643816232681, acc: 0.9642857313156128)
[2024-11-08 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:36][root][INFO] - Training Epoch: 4/10, step 205/574 completed (loss: 0.11935646086931229, acc: 0.9523809552192688)
[2024-11-08 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:37][root][INFO] - Training Epoch: 4/10, step 206/574 completed (loss: 0.13666963577270508, acc: 0.9473684430122375)
[2024-11-08 03:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:37][root][INFO] - Training Epoch: 4/10, step 207/574 completed (loss: 0.1316082626581192, acc: 0.9473684430122375)
[2024-11-08 03:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:37][root][INFO] - Training Epoch: 4/10, step 208/574 completed (loss: 0.12292402237653732, acc: 0.9545454382896423)
[2024-11-08 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:38][root][INFO] - Training Epoch: 4/10, step 209/574 completed (loss: 0.03419559448957443, acc: 1.0)
[2024-11-08 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:38][root][INFO] - Training Epoch: 4/10, step 210/574 completed (loss: 0.04347194731235504, acc: 1.0)
[2024-11-08 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:39][root][INFO] - Training Epoch: 4/10, step 211/574 completed (loss: 0.14015717804431915, acc: 0.9677419066429138)
[2024-11-08 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:39][root][INFO] - Training Epoch: 4/10, step 212/574 completed (loss: 0.06897620111703873, acc: 0.9677419066429138)
[2024-11-08 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:39][root][INFO] - Training Epoch: 4/10, step 213/574 completed (loss: 0.17384308576583862, acc: 0.9615384340286255)
[2024-11-08 03:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:41][root][INFO] - Training Epoch: 4/10, step 214/574 completed (loss: 0.13986507058143616, acc: 0.9523809552192688)
[2024-11-08 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:41][root][INFO] - Training Epoch: 4/10, step 215/574 completed (loss: 0.10824566334486008, acc: 0.9473684430122375)
[2024-11-08 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:41][root][INFO] - Training Epoch: 4/10, step 216/574 completed (loss: 0.03130747750401497, acc: 1.0)
[2024-11-08 03:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:42][root][INFO] - Training Epoch: 4/10, step 217/574 completed (loss: 0.015888206660747528, acc: 1.0)
[2024-11-08 03:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:43][root][INFO] - Training Epoch: 4/10, step 218/574 completed (loss: 0.050960324704647064, acc: 1.0)
[2024-11-08 03:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:43][root][INFO] - Training Epoch: 4/10, step 219/574 completed (loss: 0.0336940623819828, acc: 1.0)
[2024-11-08 03:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:44][root][INFO] - Training Epoch: 4/10, step 220/574 completed (loss: 0.015006480738520622, acc: 1.0)
[2024-11-08 03:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:44][root][INFO] - Training Epoch: 4/10, step 221/574 completed (loss: 0.007375786080956459, acc: 1.0)
[2024-11-08 03:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:45][root][INFO] - Training Epoch: 4/10, step 222/574 completed (loss: 0.010675609111785889, acc: 1.0)
[2024-11-08 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:46][root][INFO] - Training Epoch: 4/10, step 223/574 completed (loss: 0.037176065146923065, acc: 1.0)
[2024-11-08 03:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:46][root][INFO] - Training Epoch: 4/10, step 224/574 completed (loss: 0.14514869451522827, acc: 0.9473684430122375)
[2024-11-08 03:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:47][root][INFO] - Training Epoch: 4/10, step 225/574 completed (loss: 0.15396670997142792, acc: 0.8947368264198303)
[2024-11-08 03:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:48][root][INFO] - Training Epoch: 4/10, step 226/574 completed (loss: 0.19177666306495667, acc: 0.9090909361839294)
[2024-11-08 03:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:48][root][INFO] - Training Epoch: 4/10, step 227/574 completed (loss: 0.2251497507095337, acc: 0.9047619104385376)
[2024-11-08 03:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:48][root][INFO] - Training Epoch: 4/10, step 228/574 completed (loss: 0.414285272359848, acc: 0.875)
[2024-11-08 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:49][root][INFO] - Training Epoch: 4/10, step 229/574 completed (loss: 0.23325814306735992, acc: 0.9130434989929199)
[2024-11-08 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:49][root][INFO] - Training Epoch: 4/10, step 230/574 completed (loss: 0.12075072526931763, acc: 0.9523809552192688)
[2024-11-08 03:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:50][root][INFO] - Training Epoch: 4/10, step 231/574 completed (loss: 0.223888099193573, acc: 0.9473684430122375)
[2024-11-08 03:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:50][root][INFO] - Training Epoch: 4/10, step 232/574 completed (loss: 0.1490667760372162, acc: 0.8947368264198303)
[2024-11-08 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:51][root][INFO] - Training Epoch: 4/10, step 233/574 completed (loss: 0.3657914996147156, acc: 0.8636363744735718)
[2024-11-08 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:52][root][INFO] - Training Epoch: 4/10, step 234/574 completed (loss: 0.3531283140182495, acc: 0.8636363744735718)
[2024-11-08 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:52][root][INFO] - Training Epoch: 4/10, step 235/574 completed (loss: 0.11570409685373306, acc: 0.9642857313156128)
[2024-11-08 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:53][root][INFO] - Training Epoch: 4/10, step 236/574 completed (loss: 0.29761528968811035, acc: 0.9259259104728699)
[2024-11-08 03:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:53][root][INFO] - Training Epoch: 4/10, step 237/574 completed (loss: 0.15333126485347748, acc: 0.9090909361839294)
[2024-11-08 03:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:53][root][INFO] - Training Epoch: 4/10, step 238/574 completed (loss: 0.1583849936723709, acc: 0.9545454382896423)
[2024-11-08 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:54][root][INFO] - Training Epoch: 4/10, step 239/574 completed (loss: 0.251467764377594, acc: 0.9047619104385376)
[2024-11-08 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:54][root][INFO] - Training Epoch: 4/10, step 240/574 completed (loss: 0.11332175880670547, acc: 1.0)
[2024-11-08 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:55][root][INFO] - Training Epoch: 4/10, step 241/574 completed (loss: 0.01932200975716114, acc: 1.0)
[2024-11-08 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:56][root][INFO] - Training Epoch: 4/10, step 242/574 completed (loss: 0.19154596328735352, acc: 0.8947368264198303)
[2024-11-08 03:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:56][root][INFO] - Training Epoch: 4/10, step 243/574 completed (loss: 0.20682354271411896, acc: 0.9090909361839294)
[2024-11-08 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:57][root][INFO] - Training Epoch: 4/10, step 244/574 completed (loss: 0.004718591459095478, acc: 1.0)
[2024-11-08 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:57][root][INFO] - Training Epoch: 4/10, step 245/574 completed (loss: 0.10346533358097076, acc: 0.9666666388511658)
[2024-11-08 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:57][root][INFO] - Training Epoch: 4/10, step 246/574 completed (loss: 0.08142262697219849, acc: 0.9714285731315613)
[2024-11-08 03:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:58][root][INFO] - Training Epoch: 4/10, step 247/574 completed (loss: 0.4520852565765381, acc: 0.9130434989929199)
[2024-11-08 03:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:58][root][INFO] - Training Epoch: 4/10, step 248/574 completed (loss: 0.1506473571062088, acc: 0.9523809552192688)
[2024-11-08 03:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:59][root][INFO] - Training Epoch: 4/10, step 249/574 completed (loss: 0.2907577455043793, acc: 0.8421052694320679)
[2024-11-08 03:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:59][root][INFO] - Training Epoch: 4/10, step 250/574 completed (loss: 0.1924538016319275, acc: 0.949999988079071)
[2024-11-08 03:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:51:59][root][INFO] - Training Epoch: 4/10, step 251/574 completed (loss: 0.24762310087680817, acc: 0.9523809552192688)
[2024-11-08 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:00][root][INFO] - Training Epoch: 4/10, step 252/574 completed (loss: 0.017426254227757454, acc: 1.0)
[2024-11-08 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:00][root][INFO] - Training Epoch: 4/10, step 253/574 completed (loss: 0.02239171974360943, acc: 1.0)
[2024-11-08 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:01][root][INFO] - Training Epoch: 4/10, step 254/574 completed (loss: 0.02158922143280506, acc: 1.0)
[2024-11-08 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:01][root][INFO] - Training Epoch: 4/10, step 255/574 completed (loss: 0.05713354051113129, acc: 0.9714285731315613)
[2024-11-08 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:01][root][INFO] - Training Epoch: 4/10, step 256/574 completed (loss: 0.005721712950617075, acc: 1.0)
[2024-11-08 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:02][root][INFO] - Training Epoch: 4/10, step 257/574 completed (loss: 0.07394058257341385, acc: 0.9523809552192688)
[2024-11-08 03:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:02][root][INFO] - Training Epoch: 4/10, step 258/574 completed (loss: 0.009488856419920921, acc: 1.0)
[2024-11-08 03:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:03][root][INFO] - Training Epoch: 4/10, step 259/574 completed (loss: 0.1364292949438095, acc: 0.9090909361839294)
[2024-11-08 03:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:04][root][INFO] - Training Epoch: 4/10, step 260/574 completed (loss: 0.03005022369325161, acc: 1.0)
[2024-11-08 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:04][root][INFO] - Training Epoch: 4/10, step 261/574 completed (loss: 0.24045014381408691, acc: 0.9545454382896423)
[2024-11-08 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:05][root][INFO] - Training Epoch: 4/10, step 262/574 completed (loss: 0.22355401515960693, acc: 0.9411764740943909)
[2024-11-08 03:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:05][root][INFO] - Training Epoch: 4/10, step 263/574 completed (loss: 0.14226679503917694, acc: 0.9090909361839294)
[2024-11-08 03:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:05][root][INFO] - Training Epoch: 4/10, step 264/574 completed (loss: 0.07039709389209747, acc: 1.0)
[2024-11-08 03:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:07][root][INFO] - Training Epoch: 4/10, step 265/574 completed (loss: 0.2561873197555542, acc: 0.9473684430122375)
[2024-11-08 03:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:07][root][INFO] - Training Epoch: 4/10, step 266/574 completed (loss: 0.2124219387769699, acc: 0.9130434989929199)
[2024-11-08 03:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:08][root][INFO] - Training Epoch: 4/10, step 267/574 completed (loss: 0.23045694828033447, acc: 0.9047619104385376)
[2024-11-08 03:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:08][root][INFO] - Training Epoch: 4/10, step 268/574 completed (loss: 0.12115523219108582, acc: 0.9655172228813171)
[2024-11-08 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:09][root][INFO] - Training Epoch: 4/10, step 269/574 completed (loss: 0.32987067103385925, acc: 0.8888888955116272)
[2024-11-08 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:09][root][INFO] - Training Epoch: 4/10, step 270/574 completed (loss: 0.14737647771835327, acc: 0.9230769276618958)
[2024-11-08 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:10][root][INFO] - Training Epoch: 4/10, step 271/574 completed (loss: 0.06441371887922287, acc: 0.9523809552192688)
[2024-11-08 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:10][root][INFO] - Training Epoch: 4/10, step 272/574 completed (loss: 0.025706535205245018, acc: 1.0)
[2024-11-08 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:11][root][INFO] - Training Epoch: 4/10, step 273/574 completed (loss: 0.06551231443881989, acc: 0.9473684430122375)
[2024-11-08 03:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:11][root][INFO] - Training Epoch: 4/10, step 274/574 completed (loss: 0.10979822278022766, acc: 0.9545454382896423)
[2024-11-08 03:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:11][root][INFO] - Training Epoch: 4/10, step 275/574 completed (loss: 0.021312382072210312, acc: 1.0)
[2024-11-08 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:12][root][INFO] - Training Epoch: 4/10, step 276/574 completed (loss: 0.015945564955472946, acc: 1.0)
[2024-11-08 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:12][root][INFO] - Training Epoch: 4/10, step 277/574 completed (loss: 0.009628786705434322, acc: 1.0)
[2024-11-08 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:13][root][INFO] - Training Epoch: 4/10, step 278/574 completed (loss: 0.06580687314271927, acc: 1.0)
[2024-11-08 03:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:13][root][INFO] - Training Epoch: 4/10, step 279/574 completed (loss: 0.01011543720960617, acc: 1.0)
[2024-11-08 03:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:54][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2968, device='cuda:0') eval_epoch_loss=tensor(0.2599, device='cuda:0') eval_epoch_acc=tensor(0.9237, device='cuda:0')
[2024-11-08 03:52:54][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:52:54][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:52:58][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_280_loss_0.259921669960022/model.pt
[2024-11-08 03:52:58][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:58][root][INFO] - Training Epoch: 4/10, step 280/574 completed (loss: 0.1311073899269104, acc: 0.9444444179534912)
[2024-11-08 03:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:59][root][INFO] - Training Epoch: 4/10, step 281/574 completed (loss: 0.14138849079608917, acc: 0.9545454382896423)
[2024-11-08 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:59][root][INFO] - Training Epoch: 4/10, step 282/574 completed (loss: 0.006317343562841415, acc: 1.0)
[2024-11-08 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:52:59][root][INFO] - Training Epoch: 4/10, step 283/574 completed (loss: 0.41247814893722534, acc: 0.8965517282485962)
[2024-11-08 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:00][root][INFO] - Training Epoch: 4/10, step 284/574 completed (loss: 0.4735929071903229, acc: 0.9333333373069763)
[2024-11-08 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:00][root][INFO] - Training Epoch: 4/10, step 285/574 completed (loss: 0.45758292078971863, acc: 0.8620689511299133)
[2024-11-08 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:00][root][INFO] - Training Epoch: 4/10, step 286/574 completed (loss: 0.15561792254447937, acc: 0.9047619104385376)
[2024-11-08 03:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:01][root][INFO] - Training Epoch: 4/10, step 287/574 completed (loss: 0.6872625946998596, acc: 0.8421052694320679)
[2024-11-08 03:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:01][root][INFO] - Training Epoch: 4/10, step 288/574 completed (loss: 0.3341802656650543, acc: 0.9473684430122375)
[2024-11-08 03:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:02][root][INFO] - Training Epoch: 4/10, step 289/574 completed (loss: 0.4875435531139374, acc: 0.8636363744735718)
[2024-11-08 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:02][root][INFO] - Training Epoch: 4/10, step 290/574 completed (loss: 0.14522318542003632, acc: 1.0)
[2024-11-08 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:02][root][INFO] - Training Epoch: 4/10, step 291/574 completed (loss: 0.005535435862839222, acc: 1.0)
[2024-11-08 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:03][root][INFO] - Training Epoch: 4/10, step 292/574 completed (loss: 0.15825791656970978, acc: 0.9333333373069763)
[2024-11-08 03:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:03][root][INFO] - Training Epoch: 4/10, step 293/574 completed (loss: 0.14189735054969788, acc: 0.9523809552192688)
[2024-11-08 03:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:04][root][INFO] - Training Epoch: 4/10, step 294/574 completed (loss: 0.09080202877521515, acc: 1.0)
[2024-11-08 03:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:05][root][INFO] - Training Epoch: 4/10, step 295/574 completed (loss: 0.45083537697792053, acc: 0.8421052694320679)
[2024-11-08 03:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:05][root][INFO] - Training Epoch: 4/10, step 296/574 completed (loss: 0.14719702303409576, acc: 0.9545454382896423)
[2024-11-08 03:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:05][root][INFO] - Training Epoch: 4/10, step 297/574 completed (loss: 0.3675048053264618, acc: 0.9047619104385376)
[2024-11-08 03:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:06][root][INFO] - Training Epoch: 4/10, step 298/574 completed (loss: 0.17115478217601776, acc: 0.9583333134651184)
[2024-11-08 03:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:06][root][INFO] - Training Epoch: 4/10, step 299/574 completed (loss: 0.01625020243227482, acc: 1.0)
[2024-11-08 03:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:06][root][INFO] - Training Epoch: 4/10, step 300/574 completed (loss: 0.13149131834506989, acc: 0.9354838728904724)
[2024-11-08 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:07][root][INFO] - Training Epoch: 4/10, step 301/574 completed (loss: 0.11122166365385056, acc: 0.9615384340286255)
[2024-11-08 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:07][root][INFO] - Training Epoch: 4/10, step 302/574 completed (loss: 0.16339045763015747, acc: 0.9523809552192688)
[2024-11-08 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:08][root][INFO] - Training Epoch: 4/10, step 303/574 completed (loss: 0.059748560190200806, acc: 1.0)
[2024-11-08 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:08][root][INFO] - Training Epoch: 4/10, step 304/574 completed (loss: 0.17341667413711548, acc: 0.8947368264198303)
[2024-11-08 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:08][root][INFO] - Training Epoch: 4/10, step 305/574 completed (loss: 0.385219007730484, acc: 0.9130434989929199)
[2024-11-08 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:09][root][INFO] - Training Epoch: 4/10, step 306/574 completed (loss: 0.04245225712656975, acc: 1.0)
[2024-11-08 03:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:09][root][INFO] - Training Epoch: 4/10, step 307/574 completed (loss: 0.016009677201509476, acc: 1.0)
[2024-11-08 03:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:09][root][INFO] - Training Epoch: 4/10, step 308/574 completed (loss: 0.08624657988548279, acc: 0.9523809552192688)
[2024-11-08 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:10][root][INFO] - Training Epoch: 4/10, step 309/574 completed (loss: 0.1950017511844635, acc: 0.9473684430122375)
[2024-11-08 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:10][root][INFO] - Training Epoch: 4/10, step 310/574 completed (loss: 0.21582360565662384, acc: 0.9473684430122375)
[2024-11-08 03:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:11][root][INFO] - Training Epoch: 4/10, step 311/574 completed (loss: 0.25459814071655273, acc: 0.8636363744735718)
[2024-11-08 03:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:11][root][INFO] - Training Epoch: 4/10, step 312/574 completed (loss: 0.27577435970306396, acc: 0.9090909361839294)
[2024-11-08 03:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:12][root][INFO] - Training Epoch: 4/10, step 313/574 completed (loss: 0.03361576423048973, acc: 1.0)
[2024-11-08 03:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:12][root][INFO] - Training Epoch: 4/10, step 314/574 completed (loss: 0.04730169102549553, acc: 1.0)
[2024-11-08 03:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:12][root][INFO] - Training Epoch: 4/10, step 315/574 completed (loss: 0.19088120758533478, acc: 0.9142857193946838)
[2024-11-08 03:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:13][root][INFO] - Training Epoch: 4/10, step 316/574 completed (loss: 0.19979023933410645, acc: 0.9230769276618958)
[2024-11-08 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:13][root][INFO] - Training Epoch: 4/10, step 317/574 completed (loss: 0.06639323383569717, acc: 0.9523809552192688)
[2024-11-08 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:14][root][INFO] - Training Epoch: 4/10, step 318/574 completed (loss: 0.08808595687150955, acc: 1.0)
[2024-11-08 03:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:14][root][INFO] - Training Epoch: 4/10, step 319/574 completed (loss: 0.05473475903272629, acc: 0.949999988079071)
[2024-11-08 03:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:14][root][INFO] - Training Epoch: 4/10, step 320/574 completed (loss: 0.14733046293258667, acc: 0.9523809552192688)
[2024-11-08 03:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:15][root][INFO] - Training Epoch: 4/10, step 321/574 completed (loss: 0.00441348971799016, acc: 1.0)
[2024-11-08 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:15][root][INFO] - Training Epoch: 4/10, step 322/574 completed (loss: 0.5157730579376221, acc: 0.8260869383811951)
[2024-11-08 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:15][root][INFO] - Training Epoch: 4/10, step 323/574 completed (loss: 0.3680388033390045, acc: 0.9047619104385376)
[2024-11-08 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:16][root][INFO] - Training Epoch: 4/10, step 324/574 completed (loss: 0.49096524715423584, acc: 0.8421052694320679)
[2024-11-08 03:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:16][root][INFO] - Training Epoch: 4/10, step 325/574 completed (loss: 0.2965836822986603, acc: 0.9090909361839294)
[2024-11-08 03:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:17][root][INFO] - Training Epoch: 4/10, step 326/574 completed (loss: 0.26564690470695496, acc: 0.8947368264198303)
[2024-11-08 03:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:17][root][INFO] - Training Epoch: 4/10, step 327/574 completed (loss: 0.2861560881137848, acc: 0.9166666865348816)
[2024-11-08 03:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:17][root][INFO] - Training Epoch: 4/10, step 328/574 completed (loss: 0.12717905640602112, acc: 0.931034505367279)
[2024-11-08 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:18][root][INFO] - Training Epoch: 4/10, step 329/574 completed (loss: 0.18788297474384308, acc: 0.9259259104728699)
[2024-11-08 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:18][root][INFO] - Training Epoch: 4/10, step 330/574 completed (loss: 0.04475150257349014, acc: 1.0)
[2024-11-08 03:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:19][root][INFO] - Training Epoch: 4/10, step 331/574 completed (loss: 0.021469831466674805, acc: 1.0)
[2024-11-08 03:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:19][root][INFO] - Training Epoch: 4/10, step 332/574 completed (loss: 0.18725022673606873, acc: 0.9090909361839294)
[2024-11-08 03:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:20][root][INFO] - Training Epoch: 4/10, step 333/574 completed (loss: 0.382788747549057, acc: 0.8947368264198303)
[2024-11-08 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:20][root][INFO] - Training Epoch: 4/10, step 334/574 completed (loss: 0.05877002701163292, acc: 1.0)
[2024-11-08 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:20][root][INFO] - Training Epoch: 4/10, step 335/574 completed (loss: 0.1626674234867096, acc: 0.9615384340286255)
[2024-11-08 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:21][root][INFO] - Training Epoch: 4/10, step 336/574 completed (loss: 0.48972585797309875, acc: 0.7916666865348816)
[2024-11-08 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:21][root][INFO] - Training Epoch: 4/10, step 337/574 completed (loss: 0.5145252346992493, acc: 0.8095238208770752)
[2024-11-08 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:22][root][INFO] - Training Epoch: 4/10, step 338/574 completed (loss: 0.1861208826303482, acc: 0.8695651888847351)
[2024-11-08 03:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:22][root][INFO] - Training Epoch: 4/10, step 339/574 completed (loss: 0.1459120362997055, acc: 0.9545454382896423)
[2024-11-08 03:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:22][root][INFO] - Training Epoch: 4/10, step 340/574 completed (loss: 0.0452667661011219, acc: 0.9666666388511658)
[2024-11-08 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:23][root][INFO] - Training Epoch: 4/10, step 341/574 completed (loss: 0.022032465785741806, acc: 1.0)
[2024-11-08 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:23][root][INFO] - Training Epoch: 4/10, step 342/574 completed (loss: 0.08916672319173813, acc: 0.949999988079071)
[2024-11-08 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:23][root][INFO] - Training Epoch: 4/10, step 343/574 completed (loss: 0.16429772973060608, acc: 0.8999999761581421)
[2024-11-08 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:24][root][INFO] - Training Epoch: 4/10, step 344/574 completed (loss: 0.03226355090737343, acc: 1.0)
[2024-11-08 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:24][root][INFO] - Training Epoch: 4/10, step 345/574 completed (loss: 0.022219296544790268, acc: 1.0)
[2024-11-08 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:25][root][INFO] - Training Epoch: 4/10, step 346/574 completed (loss: 0.31098026037216187, acc: 0.8500000238418579)
[2024-11-08 03:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:25][root][INFO] - Training Epoch: 4/10, step 347/574 completed (loss: 0.0638049766421318, acc: 0.9583333134651184)
[2024-11-08 03:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:25][root][INFO] - Training Epoch: 4/10, step 348/574 completed (loss: 0.22613343596458435, acc: 0.9130434989929199)
[2024-11-08 03:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:26][root][INFO] - Training Epoch: 4/10, step 349/574 completed (loss: 0.07167572528123856, acc: 1.0)
[2024-11-08 03:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:26][root][INFO] - Training Epoch: 4/10, step 350/574 completed (loss: 0.44260671734809875, acc: 0.8421052694320679)
[2024-11-08 03:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:27][root][INFO] - Training Epoch: 4/10, step 351/574 completed (loss: 0.07693230360746384, acc: 1.0)
[2024-11-08 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:27][root][INFO] - Training Epoch: 4/10, step 352/574 completed (loss: 0.28946220874786377, acc: 0.8500000238418579)
[2024-11-08 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:28][root][INFO] - Training Epoch: 4/10, step 353/574 completed (loss: 0.13562917709350586, acc: 0.8799999952316284)
[2024-11-08 03:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:28][root][INFO] - Training Epoch: 4/10, step 354/574 completed (loss: 0.22163838148117065, acc: 0.9200000166893005)
[2024-11-08 03:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:28][root][INFO] - Training Epoch: 4/10, step 355/574 completed (loss: 0.26408612728118896, acc: 0.9047619104385376)
[2024-11-08 03:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:29][root][INFO] - Training Epoch: 4/10, step 356/574 completed (loss: 0.38582977652549744, acc: 0.8999999761581421)
[2024-11-08 03:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:30][root][INFO] - Training Epoch: 4/10, step 357/574 completed (loss: 0.1171870082616806, acc: 0.9047619104385376)
[2024-11-08 03:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:30][root][INFO] - Training Epoch: 4/10, step 358/574 completed (loss: 0.20155543088912964, acc: 0.9090909361839294)
[2024-11-08 03:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:30][root][INFO] - Training Epoch: 4/10, step 359/574 completed (loss: 0.0176386758685112, acc: 1.0)
[2024-11-08 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:31][root][INFO] - Training Epoch: 4/10, step 360/574 completed (loss: 0.04300342872738838, acc: 1.0)
[2024-11-08 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:31][root][INFO] - Training Epoch: 4/10, step 361/574 completed (loss: 0.11431968212127686, acc: 0.9428571462631226)
[2024-11-08 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:32][root][INFO] - Training Epoch: 4/10, step 362/574 completed (loss: 0.10212621837854385, acc: 0.9545454382896423)
[2024-11-08 03:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:32][root][INFO] - Training Epoch: 4/10, step 363/574 completed (loss: 0.09618601948022842, acc: 0.9473684430122375)
[2024-11-08 03:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:33][root][INFO] - Training Epoch: 4/10, step 364/574 completed (loss: 0.143667533993721, acc: 0.9523809552192688)
[2024-11-08 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:33][root][INFO] - Training Epoch: 4/10, step 365/574 completed (loss: 0.03506341204047203, acc: 1.0)
[2024-11-08 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:33][root][INFO] - Training Epoch: 4/10, step 366/574 completed (loss: 0.09289750456809998, acc: 1.0)
[2024-11-08 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:34][root][INFO] - Training Epoch: 4/10, step 367/574 completed (loss: 0.14365820586681366, acc: 0.9629629850387573)
[2024-11-08 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:34][root][INFO] - Training Epoch: 4/10, step 368/574 completed (loss: 0.17533284425735474, acc: 0.9375)
[2024-11-08 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:35][root][INFO] - Training Epoch: 4/10, step 369/574 completed (loss: 0.04537414386868477, acc: 1.0)
[2024-11-08 03:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:35][root][INFO] - Training Epoch: 4/10, step 370/574 completed (loss: 0.3410782217979431, acc: 0.8571428656578064)
[2024-11-08 03:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:37][root][INFO] - Training Epoch: 4/10, step 371/574 completed (loss: 0.3056480288505554, acc: 0.8947368264198303)
[2024-11-08 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:37][root][INFO] - Training Epoch: 4/10, step 372/574 completed (loss: 0.06869816035032272, acc: 0.9545454382896423)
[2024-11-08 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:38][root][INFO] - Training Epoch: 4/10, step 373/574 completed (loss: 0.03220634534955025, acc: 1.0)
[2024-11-08 03:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:38][root][INFO] - Training Epoch: 4/10, step 374/574 completed (loss: 0.08781517297029495, acc: 0.9545454382896423)
[2024-11-08 03:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:39][root][INFO] - Training Epoch: 4/10, step 375/574 completed (loss: 0.03870658576488495, acc: 1.0)
[2024-11-08 03:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:39][root][INFO] - Training Epoch: 4/10, step 376/574 completed (loss: 0.01065310463309288, acc: 1.0)
[2024-11-08 03:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:39][root][INFO] - Training Epoch: 4/10, step 377/574 completed (loss: 0.30001845955848694, acc: 0.9047619104385376)
[2024-11-08 03:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:40][root][INFO] - Training Epoch: 4/10, step 378/574 completed (loss: 0.027155080810189247, acc: 1.0)
[2024-11-08 03:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:41][root][INFO] - Training Epoch: 4/10, step 379/574 completed (loss: 0.028581036254763603, acc: 1.0)
[2024-11-08 03:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:41][root][INFO] - Training Epoch: 4/10, step 380/574 completed (loss: 0.18529552221298218, acc: 0.9090909361839294)
[2024-11-08 03:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:43][root][INFO] - Training Epoch: 4/10, step 381/574 completed (loss: 0.25144317746162415, acc: 0.8947368264198303)
[2024-11-08 03:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:44][root][INFO] - Training Epoch: 4/10, step 382/574 completed (loss: 0.019365064799785614, acc: 1.0)
[2024-11-08 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:44][root][INFO] - Training Epoch: 4/10, step 383/574 completed (loss: 0.00582472700625658, acc: 1.0)
[2024-11-08 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:45][root][INFO] - Training Epoch: 4/10, step 384/574 completed (loss: 0.0024601893965154886, acc: 1.0)
[2024-11-08 03:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:45][root][INFO] - Training Epoch: 4/10, step 385/574 completed (loss: 0.004693918861448765, acc: 1.0)
[2024-11-08 03:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:45][root][INFO] - Training Epoch: 4/10, step 386/574 completed (loss: 0.020805394276976585, acc: 1.0)
[2024-11-08 03:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:46][root][INFO] - Training Epoch: 4/10, step 387/574 completed (loss: 0.0014761630445718765, acc: 1.0)
[2024-11-08 03:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:46][root][INFO] - Training Epoch: 4/10, step 388/574 completed (loss: 0.010487904772162437, acc: 1.0)
[2024-11-08 03:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:47][root][INFO] - Training Epoch: 4/10, step 389/574 completed (loss: 0.009448601864278316, acc: 1.0)
[2024-11-08 03:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:47][root][INFO] - Training Epoch: 4/10, step 390/574 completed (loss: 0.10412322729825974, acc: 0.9090909361839294)
[2024-11-08 03:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:47][root][INFO] - Training Epoch: 4/10, step 391/574 completed (loss: 0.1333802044391632, acc: 0.9047619104385376)
[2024-11-08 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:48][root][INFO] - Training Epoch: 4/10, step 392/574 completed (loss: 0.14904309809207916, acc: 0.8947368264198303)
[2024-11-08 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:49][root][INFO] - Training Epoch: 4/10, step 393/574 completed (loss: 0.03974078595638275, acc: 1.0)
[2024-11-08 03:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:49][root][INFO] - Training Epoch: 4/10, step 394/574 completed (loss: 0.13813261687755585, acc: 0.9545454382896423)
[2024-11-08 03:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:50][root][INFO] - Training Epoch: 4/10, step 395/574 completed (loss: 0.0837506428360939, acc: 1.0)
[2024-11-08 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:50][root][INFO] - Training Epoch: 4/10, step 396/574 completed (loss: 0.0194769985973835, acc: 1.0)
[2024-11-08 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:50][root][INFO] - Training Epoch: 4/10, step 397/574 completed (loss: 0.26064446568489075, acc: 0.9333333373069763)
[2024-11-08 03:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:51][root][INFO] - Training Epoch: 4/10, step 398/574 completed (loss: 0.03171810507774353, acc: 1.0)
[2024-11-08 03:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:51][root][INFO] - Training Epoch: 4/10, step 399/574 completed (loss: 0.0015710883308202028, acc: 1.0)
[2024-11-08 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:52][root][INFO] - Training Epoch: 4/10, step 400/574 completed (loss: 0.12317299842834473, acc: 0.9545454382896423)
[2024-11-08 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:52][root][INFO] - Training Epoch: 4/10, step 401/574 completed (loss: 0.024452269077301025, acc: 1.0)
[2024-11-08 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:53][root][INFO] - Training Epoch: 4/10, step 402/574 completed (loss: 0.07412506639957428, acc: 0.9583333134651184)
[2024-11-08 03:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:53][root][INFO] - Training Epoch: 4/10, step 403/574 completed (loss: 0.013731922022998333, acc: 1.0)
[2024-11-08 03:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:53][root][INFO] - Training Epoch: 4/10, step 404/574 completed (loss: 0.1478295475244522, acc: 0.9428571462631226)
[2024-11-08 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:54][root][INFO] - Training Epoch: 4/10, step 405/574 completed (loss: 0.005884754937142134, acc: 1.0)
[2024-11-08 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:54][root][INFO] - Training Epoch: 4/10, step 406/574 completed (loss: 0.012816437520086765, acc: 1.0)
[2024-11-08 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:54][root][INFO] - Training Epoch: 4/10, step 407/574 completed (loss: 0.02275184728205204, acc: 1.0)
[2024-11-08 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:55][root][INFO] - Training Epoch: 4/10, step 408/574 completed (loss: 0.009079051204025745, acc: 1.0)
[2024-11-08 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:55][root][INFO] - Training Epoch: 4/10, step 409/574 completed (loss: 0.05804561451077461, acc: 0.9523809552192688)
[2024-11-08 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:56][root][INFO] - Training Epoch: 4/10, step 410/574 completed (loss: 0.0013075633905828, acc: 1.0)
[2024-11-08 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:56][root][INFO] - Training Epoch: 4/10, step 411/574 completed (loss: 0.29939332604408264, acc: 0.9285714030265808)
[2024-11-08 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:56][root][INFO] - Training Epoch: 4/10, step 412/574 completed (loss: 0.00266542611643672, acc: 1.0)
[2024-11-08 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:57][root][INFO] - Training Epoch: 4/10, step 413/574 completed (loss: 0.19143934547901154, acc: 0.9428571462631226)
[2024-11-08 03:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:57][root][INFO] - Training Epoch: 4/10, step 414/574 completed (loss: 0.009826879017055035, acc: 1.0)
[2024-11-08 03:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:57][root][INFO] - Training Epoch: 4/10, step 415/574 completed (loss: 0.0023062406107783318, acc: 1.0)
[2024-11-08 03:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:58][root][INFO] - Training Epoch: 4/10, step 416/574 completed (loss: 0.00480674859136343, acc: 1.0)
[2024-11-08 03:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:58][root][INFO] - Training Epoch: 4/10, step 417/574 completed (loss: 0.012463309802114964, acc: 1.0)
[2024-11-08 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:59][root][INFO] - Training Epoch: 4/10, step 418/574 completed (loss: 0.057771388441324234, acc: 0.9545454382896423)
[2024-11-08 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:53:59][root][INFO] - Training Epoch: 4/10, step 419/574 completed (loss: 0.06653295457363129, acc: 0.9523809552192688)
[2024-11-08 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:00][root][INFO] - Training Epoch: 4/10, step 420/574 completed (loss: 0.03810495883226395, acc: 1.0)
[2024-11-08 03:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:00][root][INFO] - Training Epoch: 4/10, step 421/574 completed (loss: 0.03155263140797615, acc: 1.0)
[2024-11-08 03:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:00][root][INFO] - Training Epoch: 4/10, step 422/574 completed (loss: 0.06972046196460724, acc: 0.9677419066429138)
[2024-11-08 03:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:42][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3006, device='cuda:0') eval_epoch_loss=tensor(0.2628, device='cuda:0') eval_epoch_acc=tensor(0.9359, device='cuda:0')
[2024-11-08 03:54:42][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:54:42][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:54:46][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_423_loss_0.2628248631954193/model.pt
[2024-11-08 03:54:46][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:54:46][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 4 is 0.9358890652656555
[2024-11-08 03:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:46][root][INFO] - Training Epoch: 4/10, step 423/574 completed (loss: 0.08449636399745941, acc: 0.9615384340286255)
[2024-11-08 03:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:46][root][INFO] - Training Epoch: 4/10, step 424/574 completed (loss: 0.0064139352180063725, acc: 1.0)
[2024-11-08 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:47][root][INFO] - Training Epoch: 4/10, step 425/574 completed (loss: 0.024467818439006805, acc: 1.0)
[2024-11-08 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:47][root][INFO] - Training Epoch: 4/10, step 426/574 completed (loss: 0.03860757127404213, acc: 1.0)
[2024-11-08 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:48][root][INFO] - Training Epoch: 4/10, step 427/574 completed (loss: 0.05905108153820038, acc: 0.9545454382896423)
[2024-11-08 03:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:48][root][INFO] - Training Epoch: 4/10, step 428/574 completed (loss: 0.00993400625884533, acc: 1.0)
[2024-11-08 03:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:48][root][INFO] - Training Epoch: 4/10, step 429/574 completed (loss: 0.04861583188176155, acc: 0.9583333134651184)
[2024-11-08 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:49][root][INFO] - Training Epoch: 4/10, step 430/574 completed (loss: 0.0022500287741422653, acc: 1.0)
[2024-11-08 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:49][root][INFO] - Training Epoch: 4/10, step 431/574 completed (loss: 0.005069543607532978, acc: 1.0)
[2024-11-08 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:50][root][INFO] - Training Epoch: 4/10, step 432/574 completed (loss: 0.004389122594147921, acc: 1.0)
[2024-11-08 03:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:50][root][INFO] - Training Epoch: 4/10, step 433/574 completed (loss: 0.03033129684627056, acc: 1.0)
[2024-11-08 03:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:51][root][INFO] - Training Epoch: 4/10, step 434/574 completed (loss: 0.002082103630527854, acc: 1.0)
[2024-11-08 03:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:51][root][INFO] - Training Epoch: 4/10, step 435/574 completed (loss: 0.0010263583390042186, acc: 1.0)
[2024-11-08 03:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:51][root][INFO] - Training Epoch: 4/10, step 436/574 completed (loss: 0.012453318573534489, acc: 1.0)
[2024-11-08 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:52][root][INFO] - Training Epoch: 4/10, step 437/574 completed (loss: 0.01778344251215458, acc: 1.0)
[2024-11-08 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:52][root][INFO] - Training Epoch: 4/10, step 438/574 completed (loss: 0.0033213903661817312, acc: 1.0)
[2024-11-08 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:53][root][INFO] - Training Epoch: 4/10, step 439/574 completed (loss: 0.02432830072939396, acc: 1.0)
[2024-11-08 03:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:53][root][INFO] - Training Epoch: 4/10, step 440/574 completed (loss: 0.05450858548283577, acc: 0.9523809552192688)
[2024-11-08 03:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:54][root][INFO] - Training Epoch: 4/10, step 441/574 completed (loss: 0.4078104794025421, acc: 0.8421052694320679)
[2024-11-08 03:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:55][root][INFO] - Training Epoch: 4/10, step 442/574 completed (loss: 0.15479496121406555, acc: 0.9473684430122375)
[2024-11-08 03:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:56][root][INFO] - Training Epoch: 4/10, step 443/574 completed (loss: 0.11689304560422897, acc: 0.9545454382896423)
[2024-11-08 03:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:56][root][INFO] - Training Epoch: 4/10, step 444/574 completed (loss: 0.00693304929882288, acc: 1.0)
[2024-11-08 03:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:57][root][INFO] - Training Epoch: 4/10, step 445/574 completed (loss: 0.006306102499365807, acc: 1.0)
[2024-11-08 03:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:57][root][INFO] - Training Epoch: 4/10, step 446/574 completed (loss: 0.19198399782180786, acc: 0.9677419066429138)
[2024-11-08 03:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:58][root][INFO] - Training Epoch: 4/10, step 447/574 completed (loss: 0.15013211965560913, acc: 0.9354838728904724)
[2024-11-08 03:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:58][root][INFO] - Training Epoch: 4/10, step 448/574 completed (loss: 0.029840584844350815, acc: 0.9677419066429138)
[2024-11-08 03:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:58][root][INFO] - Training Epoch: 4/10, step 449/574 completed (loss: 0.012284819968044758, acc: 1.0)
[2024-11-08 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:59][root][INFO] - Training Epoch: 4/10, step 450/574 completed (loss: 0.0013247306924313307, acc: 1.0)
[2024-11-08 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:54:59][root][INFO] - Training Epoch: 4/10, step 451/574 completed (loss: 0.03839436173439026, acc: 1.0)
[2024-11-08 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:00][root][INFO] - Training Epoch: 4/10, step 452/574 completed (loss: 0.005048563703894615, acc: 1.0)
[2024-11-08 03:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:00][root][INFO] - Training Epoch: 4/10, step 453/574 completed (loss: 0.151021808385849, acc: 0.949999988079071)
[2024-11-08 03:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:00][root][INFO] - Training Epoch: 4/10, step 454/574 completed (loss: 0.06864520907402039, acc: 0.9545454382896423)
[2024-11-08 03:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:01][root][INFO] - Training Epoch: 4/10, step 455/574 completed (loss: 0.06437364965677261, acc: 0.9677419066429138)
[2024-11-08 03:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:01][root][INFO] - Training Epoch: 4/10, step 456/574 completed (loss: 0.015198053792119026, acc: 1.0)
[2024-11-08 03:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:02][root][INFO] - Training Epoch: 4/10, step 457/574 completed (loss: 0.0029691539239138365, acc: 1.0)
[2024-11-08 03:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:02][root][INFO] - Training Epoch: 4/10, step 458/574 completed (loss: 0.03539068624377251, acc: 1.0)
[2024-11-08 03:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:03][root][INFO] - Training Epoch: 4/10, step 459/574 completed (loss: 0.06791142374277115, acc: 1.0)
[2024-11-08 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:03][root][INFO] - Training Epoch: 4/10, step 460/574 completed (loss: 0.07164368033409119, acc: 0.949999988079071)
[2024-11-08 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:03][root][INFO] - Training Epoch: 4/10, step 461/574 completed (loss: 0.0008531578350812197, acc: 1.0)
[2024-11-08 03:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:04][root][INFO] - Training Epoch: 4/10, step 462/574 completed (loss: 0.012211001478135586, acc: 1.0)
[2024-11-08 03:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:04][root][INFO] - Training Epoch: 4/10, step 463/574 completed (loss: 0.0761001780629158, acc: 0.9629629850387573)
[2024-11-08 03:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:05][root][INFO] - Training Epoch: 4/10, step 464/574 completed (loss: 0.034633636474609375, acc: 0.9696969985961914)
[2024-11-08 03:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:05][root][INFO] - Training Epoch: 4/10, step 465/574 completed (loss: 0.00889755692332983, acc: 1.0)
[2024-11-08 03:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:05][root][INFO] - Training Epoch: 4/10, step 466/574 completed (loss: 0.0031439196318387985, acc: 1.0)
[2024-11-08 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:06][root][INFO] - Training Epoch: 4/10, step 467/574 completed (loss: 0.04299720376729965, acc: 1.0)
[2024-11-08 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:06][root][INFO] - Training Epoch: 4/10, step 468/574 completed (loss: 0.04695308953523636, acc: 0.949999988079071)
[2024-11-08 03:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:07][root][INFO] - Training Epoch: 4/10, step 469/574 completed (loss: 0.00276772677898407, acc: 1.0)
[2024-11-08 03:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:07][root][INFO] - Training Epoch: 4/10, step 470/574 completed (loss: 0.014130696654319763, acc: 1.0)
[2024-11-08 03:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:08][root][INFO] - Training Epoch: 4/10, step 471/574 completed (loss: 0.020815033465623856, acc: 1.0)
[2024-11-08 03:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:08][root][INFO] - Training Epoch: 4/10, step 472/574 completed (loss: 0.04932166263461113, acc: 1.0)
[2024-11-08 03:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:09][root][INFO] - Training Epoch: 4/10, step 473/574 completed (loss: 0.19904492795467377, acc: 0.9523809552192688)
[2024-11-08 03:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:09][root][INFO] - Training Epoch: 4/10, step 474/574 completed (loss: 0.011691195890307426, acc: 1.0)
[2024-11-08 03:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:09][root][INFO] - Training Epoch: 4/10, step 475/574 completed (loss: 0.0838085263967514, acc: 0.949999988079071)
[2024-11-08 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:10][root][INFO] - Training Epoch: 4/10, step 476/574 completed (loss: 0.06856895983219147, acc: 0.9523809552192688)
[2024-11-08 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:10][root][INFO] - Training Epoch: 4/10, step 477/574 completed (loss: 0.03538193926215172, acc: 1.0)
[2024-11-08 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:11][root][INFO] - Training Epoch: 4/10, step 478/574 completed (loss: 0.009566697292029858, acc: 1.0)
[2024-11-08 03:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:11][root][INFO] - Training Epoch: 4/10, step 479/574 completed (loss: 0.032746586948633194, acc: 0.9677419066429138)
[2024-11-08 03:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:12][root][INFO] - Training Epoch: 4/10, step 480/574 completed (loss: 0.07318659871816635, acc: 0.9677419066429138)
[2024-11-08 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:12][root][INFO] - Training Epoch: 4/10, step 481/574 completed (loss: 0.235958531498909, acc: 0.9047619104385376)
[2024-11-08 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:12][root][INFO] - Training Epoch: 4/10, step 482/574 completed (loss: 0.16597679257392883, acc: 0.9473684430122375)
[2024-11-08 03:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:13][root][INFO] - Training Epoch: 4/10, step 483/574 completed (loss: 0.10259652137756348, acc: 0.9545454382896423)
[2024-11-08 03:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:13][root][INFO] - Training Epoch: 4/10, step 484/574 completed (loss: 0.051717277616262436, acc: 0.9545454382896423)
[2024-11-08 03:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:14][root][INFO] - Training Epoch: 4/10, step 485/574 completed (loss: 0.06570079922676086, acc: 0.9642857313156128)
[2024-11-08 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:14][root][INFO] - Training Epoch: 4/10, step 486/574 completed (loss: 0.044279541820287704, acc: 0.9629629850387573)
[2024-11-08 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:14][root][INFO] - Training Epoch: 4/10, step 487/574 completed (loss: 0.10145877301692963, acc: 0.9714285731315613)
[2024-11-08 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:15][root][INFO] - Training Epoch: 4/10, step 488/574 completed (loss: 0.3472091257572174, acc: 0.9230769276618958)
[2024-11-08 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:15][root][INFO] - Training Epoch: 4/10, step 489/574 completed (loss: 0.0628853291273117, acc: 0.9523809552192688)
[2024-11-08 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:16][root][INFO] - Training Epoch: 4/10, step 490/574 completed (loss: 0.014399482868611813, acc: 1.0)
[2024-11-08 03:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:16][root][INFO] - Training Epoch: 4/10, step 491/574 completed (loss: 0.01513725332915783, acc: 1.0)
[2024-11-08 03:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:17][root][INFO] - Training Epoch: 4/10, step 492/574 completed (loss: 0.010619723238050938, acc: 1.0)
[2024-11-08 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:17][root][INFO] - Training Epoch: 4/10, step 493/574 completed (loss: 0.2615751624107361, acc: 0.9047619104385376)
[2024-11-08 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:17][root][INFO] - Training Epoch: 4/10, step 494/574 completed (loss: 0.27123206853866577, acc: 0.9166666865348816)
[2024-11-08 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:18][root][INFO] - Training Epoch: 4/10, step 495/574 completed (loss: 0.03983267769217491, acc: 1.0)
[2024-11-08 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:18][root][INFO] - Training Epoch: 4/10, step 496/574 completed (loss: 0.20453603565692902, acc: 0.949999988079071)
[2024-11-08 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:19][root][INFO] - Training Epoch: 4/10, step 497/574 completed (loss: 0.10457909107208252, acc: 0.949999988079071)
[2024-11-08 03:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:19][root][INFO] - Training Epoch: 4/10, step 498/574 completed (loss: 0.2788529694080353, acc: 0.9473684430122375)
[2024-11-08 03:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:19][root][INFO] - Training Epoch: 4/10, step 499/574 completed (loss: 0.30649280548095703, acc: 0.8636363744735718)
[2024-11-08 03:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:20][root][INFO] - Training Epoch: 4/10, step 500/574 completed (loss: 0.0773060992360115, acc: 1.0)
[2024-11-08 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:20][root][INFO] - Training Epoch: 4/10, step 501/574 completed (loss: 0.030560649931430817, acc: 1.0)
[2024-11-08 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:20][root][INFO] - Training Epoch: 4/10, step 502/574 completed (loss: 0.06424521654844284, acc: 0.9677419066429138)
[2024-11-08 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:21][root][INFO] - Training Epoch: 4/10, step 503/574 completed (loss: 0.08665701001882553, acc: 0.9677419066429138)
[2024-11-08 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:21][root][INFO] - Training Epoch: 4/10, step 504/574 completed (loss: 0.01623142510652542, acc: 1.0)
[2024-11-08 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:22][root][INFO] - Training Epoch: 4/10, step 505/574 completed (loss: 0.16762426495552063, acc: 0.9047619104385376)
[2024-11-08 03:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:22][root][INFO] - Training Epoch: 4/10, step 506/574 completed (loss: 0.33492985367774963, acc: 0.8947368264198303)
[2024-11-08 03:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:23][root][INFO] - Training Epoch: 4/10, step 507/574 completed (loss: 0.07159009575843811, acc: 0.949999988079071)
[2024-11-08 03:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:24][root][INFO] - Training Epoch: 4/10, step 508/574 completed (loss: 0.5943590402603149, acc: 0.800000011920929)
[2024-11-08 03:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:24][root][INFO] - Training Epoch: 4/10, step 509/574 completed (loss: 0.05016232654452324, acc: 1.0)
[2024-11-08 03:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:24][root][INFO] - Training Epoch: 4/10, step 510/574 completed (loss: 0.02834896929562092, acc: 1.0)
[2024-11-08 03:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:25][root][INFO] - Training Epoch: 4/10, step 511/574 completed (loss: 0.04010331630706787, acc: 1.0)
[2024-11-08 03:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:28][root][INFO] - Training Epoch: 4/10, step 512/574 completed (loss: 0.2110757678747177, acc: 0.8928571343421936)
[2024-11-08 03:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:30][root][INFO] - Training Epoch: 4/10, step 513/574 completed (loss: 0.10672087967395782, acc: 0.9523809552192688)
[2024-11-08 03:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:30][root][INFO] - Training Epoch: 4/10, step 514/574 completed (loss: 0.3736495077610016, acc: 0.8421052694320679)
[2024-11-08 03:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:30][root][INFO] - Training Epoch: 4/10, step 515/574 completed (loss: 0.027806267142295837, acc: 1.0)
[2024-11-08 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:31][root][INFO] - Training Epoch: 4/10, step 516/574 completed (loss: 0.015956802293658257, acc: 1.0)
[2024-11-08 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:32][root][INFO] - Training Epoch: 4/10, step 517/574 completed (loss: 0.010783623903989792, acc: 1.0)
[2024-11-08 03:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:32][root][INFO] - Training Epoch: 4/10, step 518/574 completed (loss: 0.28989505767822266, acc: 0.9285714030265808)
[2024-11-08 03:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:32][root][INFO] - Training Epoch: 4/10, step 519/574 completed (loss: 0.17987129092216492, acc: 0.9259259104728699)
[2024-11-08 03:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:33][root][INFO] - Training Epoch: 4/10, step 520/574 completed (loss: 0.09371738135814667, acc: 0.9714285731315613)
[2024-11-08 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:34][root][INFO] - Training Epoch: 4/10, step 521/574 completed (loss: 0.168654665350914, acc: 0.9130434989929199)
[2024-11-08 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:35][root][INFO] - Training Epoch: 4/10, step 522/574 completed (loss: 0.07887966930866241, acc: 0.9523809552192688)
[2024-11-08 03:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:35][root][INFO] - Training Epoch: 4/10, step 523/574 completed (loss: 0.3073403835296631, acc: 0.8947368264198303)
[2024-11-08 03:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:36][root][INFO] - Training Epoch: 4/10, step 524/574 completed (loss: 0.22312265634536743, acc: 0.8636363744735718)
[2024-11-08 03:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:36][root][INFO] - Training Epoch: 4/10, step 525/574 completed (loss: 0.05050495266914368, acc: 1.0)
[2024-11-08 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:37][root][INFO] - Training Epoch: 4/10, step 526/574 completed (loss: 0.03571099415421486, acc: 1.0)
[2024-11-08 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:37][root][INFO] - Training Epoch: 4/10, step 527/574 completed (loss: 0.09110286831855774, acc: 0.9696969985961914)
[2024-11-08 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:37][root][INFO] - Training Epoch: 4/10, step 528/574 completed (loss: 0.21176600456237793, acc: 0.8928571343421936)
[2024-11-08 03:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:38][root][INFO] - Training Epoch: 4/10, step 529/574 completed (loss: 0.13077765703201294, acc: 0.9523809552192688)
[2024-11-08 03:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:38][root][INFO] - Training Epoch: 4/10, step 530/574 completed (loss: 0.206393763422966, acc: 0.9473684430122375)
[2024-11-08 03:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:39][root][INFO] - Training Epoch: 4/10, step 531/574 completed (loss: 0.13621051609516144, acc: 0.949999988079071)
[2024-11-08 03:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:39][root][INFO] - Training Epoch: 4/10, step 532/574 completed (loss: 0.20928780734539032, acc: 0.9523809552192688)
[2024-11-08 03:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:39][root][INFO] - Training Epoch: 4/10, step 533/574 completed (loss: 0.16834890842437744, acc: 0.9545454382896423)
[2024-11-08 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:40][root][INFO] - Training Epoch: 4/10, step 534/574 completed (loss: 0.08820829540491104, acc: 0.9642857313156128)
[2024-11-08 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:40][root][INFO] - Training Epoch: 4/10, step 535/574 completed (loss: 0.010873930528759956, acc: 1.0)
[2024-11-08 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:41][root][INFO] - Training Epoch: 4/10, step 536/574 completed (loss: 0.03264160081744194, acc: 1.0)
[2024-11-08 03:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:41][root][INFO] - Training Epoch: 4/10, step 537/574 completed (loss: 0.18064360320568085, acc: 0.9523809552192688)
[2024-11-08 03:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:42][root][INFO] - Training Epoch: 4/10, step 538/574 completed (loss: 0.30369335412979126, acc: 0.8947368264198303)
[2024-11-08 03:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:42][root][INFO] - Training Epoch: 4/10, step 539/574 completed (loss: 0.16842332482337952, acc: 0.8999999761581421)
[2024-11-08 03:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:42][root][INFO] - Training Epoch: 4/10, step 540/574 completed (loss: 0.27803054451942444, acc: 0.9523809552192688)
[2024-11-08 03:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:43][root][INFO] - Training Epoch: 4/10, step 541/574 completed (loss: 0.07420840859413147, acc: 0.9523809552192688)
[2024-11-08 03:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:43][root][INFO] - Training Epoch: 4/10, step 542/574 completed (loss: 0.0025502133648842573, acc: 1.0)
[2024-11-08 03:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:44][root][INFO] - Training Epoch: 4/10, step 543/574 completed (loss: 0.003338004695251584, acc: 1.0)
[2024-11-08 03:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:44][root][INFO] - Training Epoch: 4/10, step 544/574 completed (loss: 0.007395459339022636, acc: 1.0)
[2024-11-08 03:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:44][root][INFO] - Training Epoch: 4/10, step 545/574 completed (loss: 0.004407518543303013, acc: 1.0)
[2024-11-08 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:45][root][INFO] - Training Epoch: 4/10, step 546/574 completed (loss: 0.01180062722414732, acc: 1.0)
[2024-11-08 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:45][root][INFO] - Training Epoch: 4/10, step 547/574 completed (loss: 0.07170464098453522, acc: 0.9523809552192688)
[2024-11-08 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:46][root][INFO] - Training Epoch: 4/10, step 548/574 completed (loss: 0.003778788261115551, acc: 1.0)
[2024-11-08 03:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:46][root][INFO] - Training Epoch: 4/10, step 549/574 completed (loss: 0.0012172887800261378, acc: 1.0)
[2024-11-08 03:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:46][root][INFO] - Training Epoch: 4/10, step 550/574 completed (loss: 0.0015528200892731547, acc: 1.0)
[2024-11-08 03:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:47][root][INFO] - Training Epoch: 4/10, step 551/574 completed (loss: 0.008392782881855965, acc: 1.0)
[2024-11-08 03:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:47][root][INFO] - Training Epoch: 4/10, step 552/574 completed (loss: 0.008299238048493862, acc: 1.0)
[2024-11-08 03:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:48][root][INFO] - Training Epoch: 4/10, step 553/574 completed (loss: 0.0077073113061487675, acc: 1.0)
[2024-11-08 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:48][root][INFO] - Training Epoch: 4/10, step 554/574 completed (loss: 0.0035524540580809116, acc: 1.0)
[2024-11-08 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:48][root][INFO] - Training Epoch: 4/10, step 555/574 completed (loss: 0.06883758306503296, acc: 0.949999988079071)
[2024-11-08 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:49][root][INFO] - Training Epoch: 4/10, step 556/574 completed (loss: 0.03743162378668785, acc: 1.0)
[2024-11-08 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:49][root][INFO] - Training Epoch: 4/10, step 557/574 completed (loss: 0.0017412066226825118, acc: 1.0)
[2024-11-08 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:50][root][INFO] - Training Epoch: 4/10, step 558/574 completed (loss: 0.0018934282707050443, acc: 1.0)
[2024-11-08 03:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:50][root][INFO] - Training Epoch: 4/10, step 559/574 completed (loss: 0.05249118059873581, acc: 0.9629629850387573)
[2024-11-08 03:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:50][root][INFO] - Training Epoch: 4/10, step 560/574 completed (loss: 0.04483896493911743, acc: 0.9696969985961914)
[2024-11-08 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:51][root][INFO] - Training Epoch: 4/10, step 561/574 completed (loss: 0.059753939509391785, acc: 1.0)
[2024-11-08 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:51][root][INFO] - Training Epoch: 4/10, step 562/574 completed (loss: 0.007284809369593859, acc: 1.0)
[2024-11-08 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:52][root][INFO] - Training Epoch: 4/10, step 563/574 completed (loss: 0.013324987143278122, acc: 1.0)
[2024-11-08 03:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:52][root][INFO] - Training Epoch: 4/10, step 564/574 completed (loss: 0.07194376736879349, acc: 0.9545454382896423)
[2024-11-08 03:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:52][root][INFO] - Training Epoch: 4/10, step 565/574 completed (loss: 0.2026636302471161, acc: 0.949999988079071)
[2024-11-08 03:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3372, device='cuda:0') eval_epoch_loss=tensor(0.2906, device='cuda:0') eval_epoch_acc=tensor(0.9267, device='cuda:0')
[2024-11-08 03:56:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:56:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:56:37][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_566_loss_0.2905721068382263/model.pt
[2024-11-08 03:56:37][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:37][root][INFO] - Training Epoch: 4/10, step 566/574 completed (loss: 0.002643507206812501, acc: 1.0)
[2024-11-08 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:37][root][INFO] - Training Epoch: 4/10, step 567/574 completed (loss: 0.0020918692462146282, acc: 1.0)
[2024-11-08 03:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:38][root][INFO] - Training Epoch: 4/10, step 568/574 completed (loss: 0.008187229745090008, acc: 1.0)
[2024-11-08 03:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:38][root][INFO] - Training Epoch: 4/10, step 569/574 completed (loss: 0.0011210906086489558, acc: 1.0)
[2024-11-08 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:39][root][INFO] - Training Epoch: 4/10, step 570/574 completed (loss: 0.001336022512987256, acc: 1.0)
[2024-11-08 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:39][root][INFO] - Training Epoch: 4/10, step 571/574 completed (loss: 0.005119347479194403, acc: 1.0)
[2024-11-08 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:40][root][INFO] - Training Epoch: 4/10, step 572/574 completed (loss: 0.022857511416077614, acc: 1.0)
[2024-11-08 03:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:40][root][INFO] - Training Epoch: 4/10, step 573/574 completed (loss: 0.16267700493335724, acc: 0.8999999761581421)
[2024-11-08 03:56:40][slam_llm.utils.train_utils][INFO] - Epoch 4: train_perplexity=1.1371, train_epoch_loss=0.1285, epoch time 467.6139791700989s
[2024-11-08 03:56:40][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 03:56:40][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2024-11-08 03:56:40][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 03:56:40][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2024-11-08 03:56:40][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 03:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:41][root][INFO] - Training Epoch: 5/10, step 0/574 completed (loss: 0.10992158949375153, acc: 0.9642857313156128)
[2024-11-08 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:42][root][INFO] - Training Epoch: 5/10, step 1/574 completed (loss: 0.013833080418407917, acc: 1.0)
[2024-11-08 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:42][root][INFO] - Training Epoch: 5/10, step 2/574 completed (loss: 0.06402165442705154, acc: 0.9714285731315613)
[2024-11-08 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:43][root][INFO] - Training Epoch: 5/10, step 3/574 completed (loss: 0.047260113060474396, acc: 1.0)
[2024-11-08 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:43][root][INFO] - Training Epoch: 5/10, step 4/574 completed (loss: 0.05032418295741081, acc: 0.9523809552192688)
[2024-11-08 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:43][root][INFO] - Training Epoch: 5/10, step 5/574 completed (loss: 0.2730741798877716, acc: 0.9473684430122375)
[2024-11-08 03:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:44][root][INFO] - Training Epoch: 5/10, step 6/574 completed (loss: 0.0019170184386894107, acc: 1.0)
[2024-11-08 03:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:44][root][INFO] - Training Epoch: 5/10, step 7/574 completed (loss: 0.004455604590475559, acc: 1.0)
[2024-11-08 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:45][root][INFO] - Training Epoch: 5/10, step 8/574 completed (loss: 0.019184505566954613, acc: 1.0)
[2024-11-08 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:45][root][INFO] - Training Epoch: 5/10, step 9/574 completed (loss: 0.021577216684818268, acc: 1.0)
[2024-11-08 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:46][root][INFO] - Training Epoch: 5/10, step 10/574 completed (loss: 0.004532466176897287, acc: 1.0)
[2024-11-08 03:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:46][root][INFO] - Training Epoch: 5/10, step 11/574 completed (loss: 0.07635457068681717, acc: 0.9599999785423279)
[2024-11-08 03:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:47][root][INFO] - Training Epoch: 5/10, step 12/574 completed (loss: 0.27405664324760437, acc: 0.8999999761581421)
[2024-11-08 03:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:47][root][INFO] - Training Epoch: 5/10, step 13/574 completed (loss: 0.006152203772217035, acc: 1.0)
[2024-11-08 03:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:48][root][INFO] - Training Epoch: 5/10, step 14/574 completed (loss: 0.03260045126080513, acc: 1.0)
[2024-11-08 03:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:48][root][INFO] - Training Epoch: 5/10, step 15/574 completed (loss: 0.034765247255563736, acc: 0.9545454382896423)
[2024-11-08 03:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:48][root][INFO] - Training Epoch: 5/10, step 16/574 completed (loss: 0.03678493946790695, acc: 0.9642857313156128)
[2024-11-08 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:49][root][INFO] - Training Epoch: 5/10, step 17/574 completed (loss: 0.0932970643043518, acc: 0.9642857313156128)
[2024-11-08 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:49][root][INFO] - Training Epoch: 5/10, step 18/574 completed (loss: 0.1392819732427597, acc: 0.9090909361839294)
[2024-11-08 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:50][root][INFO] - Training Epoch: 5/10, step 19/574 completed (loss: 0.3538850247859955, acc: 0.8947368264198303)
[2024-11-08 03:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:50][root][INFO] - Training Epoch: 5/10, step 20/574 completed (loss: 0.12312397360801697, acc: 0.9473684430122375)
[2024-11-08 03:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:50][root][INFO] - Training Epoch: 5/10, step 21/574 completed (loss: 0.17175084352493286, acc: 0.9090909361839294)
[2024-11-08 03:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:51][root][INFO] - Training Epoch: 5/10, step 22/574 completed (loss: 0.16488496959209442, acc: 0.9523809552192688)
[2024-11-08 03:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:51][root][INFO] - Training Epoch: 5/10, step 23/574 completed (loss: 0.07620193809270859, acc: 0.9523809552192688)
[2024-11-08 03:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:52][root][INFO] - Training Epoch: 5/10, step 24/574 completed (loss: 0.04258478060364723, acc: 1.0)
[2024-11-08 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:52][root][INFO] - Training Epoch: 5/10, step 25/574 completed (loss: 0.12233301252126694, acc: 0.9047619104385376)
[2024-11-08 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:52][root][INFO] - Training Epoch: 5/10, step 26/574 completed (loss: 0.29852887988090515, acc: 0.9047619104385376)
[2024-11-08 03:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:54][root][INFO] - Training Epoch: 5/10, step 27/574 completed (loss: 0.047702379524707794, acc: 1.0)
[2024-11-08 03:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:54][root][INFO] - Training Epoch: 5/10, step 28/574 completed (loss: 0.03994506224989891, acc: 1.0)
[2024-11-08 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:55][root][INFO] - Training Epoch: 5/10, step 29/574 completed (loss: 0.10525691509246826, acc: 0.949999988079071)
[2024-11-08 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:55][root][INFO] - Training Epoch: 5/10, step 30/574 completed (loss: 0.11421740800142288, acc: 0.9545454382896423)
[2024-11-08 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:56][root][INFO] - Training Epoch: 5/10, step 31/574 completed (loss: 0.09302043169736862, acc: 0.9655172228813171)
[2024-11-08 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:56][root][INFO] - Training Epoch: 5/10, step 32/574 completed (loss: 0.11711489409208298, acc: 0.9696969985961914)
[2024-11-08 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:56][root][INFO] - Training Epoch: 5/10, step 33/574 completed (loss: 0.020809926092624664, acc: 1.0)
[2024-11-08 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:57][root][INFO] - Training Epoch: 5/10, step 34/574 completed (loss: 0.15095768868923187, acc: 0.9047619104385376)
[2024-11-08 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:57][root][INFO] - Training Epoch: 5/10, step 35/574 completed (loss: 0.6229088306427002, acc: 0.8947368264198303)
[2024-11-08 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:58][root][INFO] - Training Epoch: 5/10, step 36/574 completed (loss: 0.08463732153177261, acc: 0.949999988079071)
[2024-11-08 03:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:58][root][INFO] - Training Epoch: 5/10, step 37/574 completed (loss: 0.010481725446879864, acc: 1.0)
[2024-11-08 03:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:59][root][INFO] - Training Epoch: 5/10, step 38/574 completed (loss: 0.04307936504483223, acc: 0.9545454382896423)
[2024-11-08 03:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:59][root][INFO] - Training Epoch: 5/10, step 39/574 completed (loss: 0.07018841803073883, acc: 0.9642857313156128)
[2024-11-08 03:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:56:59][root][INFO] - Training Epoch: 5/10, step 40/574 completed (loss: 0.31136342883110046, acc: 0.8857142925262451)
[2024-11-08 03:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:00][root][INFO] - Training Epoch: 5/10, step 41/574 completed (loss: 0.4230789542198181, acc: 0.949999988079071)
[2024-11-08 03:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:00][root][INFO] - Training Epoch: 5/10, step 42/574 completed (loss: 0.3322388529777527, acc: 0.8999999761581421)
[2024-11-08 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:01][root][INFO] - Training Epoch: 5/10, step 43/574 completed (loss: 0.40019455552101135, acc: 0.8947368264198303)
[2024-11-08 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:01][root][INFO] - Training Epoch: 5/10, step 44/574 completed (loss: 0.3568834066390991, acc: 0.9090909361839294)
[2024-11-08 03:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:02][root][INFO] - Training Epoch: 5/10, step 45/574 completed (loss: 0.06453283131122589, acc: 1.0)
[2024-11-08 03:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:02][root][INFO] - Training Epoch: 5/10, step 46/574 completed (loss: 0.1897439956665039, acc: 0.9583333134651184)
[2024-11-08 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:03][root][INFO] - Training Epoch: 5/10, step 47/574 completed (loss: 0.032486818730831146, acc: 1.0)
[2024-11-08 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:03][root][INFO] - Training Epoch: 5/10, step 48/574 completed (loss: 0.059842709451913834, acc: 0.9714285731315613)
[2024-11-08 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:04][root][INFO] - Training Epoch: 5/10, step 49/574 completed (loss: 0.01593807153403759, acc: 1.0)
[2024-11-08 03:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:04][root][INFO] - Training Epoch: 5/10, step 50/574 completed (loss: 0.03617090359330177, acc: 1.0)
[2024-11-08 03:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:05][root][INFO] - Training Epoch: 5/10, step 51/574 completed (loss: 0.352430522441864, acc: 0.8947368264198303)
[2024-11-08 03:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:05][root][INFO] - Training Epoch: 5/10, step 52/574 completed (loss: 0.21733108162879944, acc: 0.949999988079071)
[2024-11-08 03:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:06][root][INFO] - Training Epoch: 5/10, step 53/574 completed (loss: 0.22087547183036804, acc: 0.9047619104385376)
[2024-11-08 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:06][root][INFO] - Training Epoch: 5/10, step 54/574 completed (loss: 0.20925790071487427, acc: 0.9545454382896423)
[2024-11-08 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:06][root][INFO] - Training Epoch: 5/10, step 55/574 completed (loss: 0.01576325111091137, acc: 1.0)
[2024-11-08 03:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:11][root][INFO] - Training Epoch: 5/10, step 56/574 completed (loss: 0.3125373423099518, acc: 0.8095238208770752)
[2024-11-08 03:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:12][root][INFO] - Training Epoch: 5/10, step 57/574 completed (loss: 0.11709592491388321, acc: 0.9523809552192688)
[2024-11-08 03:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:13][root][INFO] - Training Epoch: 5/10, step 58/574 completed (loss: 0.08513827621936798, acc: 0.9473684430122375)
[2024-11-08 03:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:14][root][INFO] - Training Epoch: 5/10, step 59/574 completed (loss: 0.10833606123924255, acc: 0.9545454382896423)
[2024-11-08 03:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:15][root][INFO] - Training Epoch: 5/10, step 60/574 completed (loss: 0.2562702000141144, acc: 0.9473684430122375)
[2024-11-08 03:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:16][root][INFO] - Training Epoch: 5/10, step 61/574 completed (loss: 0.0796075239777565, acc: 1.0)
[2024-11-08 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:16][root][INFO] - Training Epoch: 5/10, step 62/574 completed (loss: 0.017030026763677597, acc: 1.0)
[2024-11-08 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:16][root][INFO] - Training Epoch: 5/10, step 63/574 completed (loss: 0.06307198852300644, acc: 0.9642857313156128)
[2024-11-08 03:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:17][root][INFO] - Training Epoch: 5/10, step 64/574 completed (loss: 0.18812765181064606, acc: 0.9523809552192688)
[2024-11-08 03:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:17][root][INFO] - Training Epoch: 5/10, step 65/574 completed (loss: 0.1081547960639, acc: 1.0)
[2024-11-08 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:18][root][INFO] - Training Epoch: 5/10, step 66/574 completed (loss: 0.157171368598938, acc: 0.9473684430122375)
[2024-11-08 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:18][root][INFO] - Training Epoch: 5/10, step 67/574 completed (loss: 0.21203669905662537, acc: 0.8999999761581421)
[2024-11-08 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:19][root][INFO] - Training Epoch: 5/10, step 68/574 completed (loss: 0.05995345488190651, acc: 0.9545454382896423)
[2024-11-08 03:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:19][root][INFO] - Training Epoch: 5/10, step 69/574 completed (loss: 0.003206901252269745, acc: 1.0)
[2024-11-08 03:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:19][root][INFO] - Training Epoch: 5/10, step 70/574 completed (loss: 0.08504131436347961, acc: 0.9259259104728699)
[2024-11-08 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:20][root][INFO] - Training Epoch: 5/10, step 71/574 completed (loss: 0.10362633317708969, acc: 0.9666666388511658)
[2024-11-08 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:20][root][INFO] - Training Epoch: 5/10, step 72/574 completed (loss: 0.06619168817996979, acc: 0.9523809552192688)
[2024-11-08 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:21][root][INFO] - Training Epoch: 5/10, step 73/574 completed (loss: 0.32463908195495605, acc: 0.8947368264198303)
[2024-11-08 03:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:21][root][INFO] - Training Epoch: 5/10, step 74/574 completed (loss: 0.10494079440832138, acc: 0.949999988079071)
[2024-11-08 03:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:22][root][INFO] - Training Epoch: 5/10, step 75/574 completed (loss: 0.14900630712509155, acc: 0.9523809552192688)
[2024-11-08 03:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:22][root][INFO] - Training Epoch: 5/10, step 76/574 completed (loss: 0.07891098409891129, acc: 0.9545454382896423)
[2024-11-08 03:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:23][root][INFO] - Training Epoch: 5/10, step 77/574 completed (loss: 0.0018731120508164167, acc: 1.0)
[2024-11-08 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:23][root][INFO] - Training Epoch: 5/10, step 78/574 completed (loss: 0.023640785366296768, acc: 1.0)
[2024-11-08 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:23][root][INFO] - Training Epoch: 5/10, step 79/574 completed (loss: 0.05412972345948219, acc: 1.0)
[2024-11-08 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:24][root][INFO] - Training Epoch: 5/10, step 80/574 completed (loss: 0.03701518848538399, acc: 1.0)
[2024-11-08 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:24][root][INFO] - Training Epoch: 5/10, step 81/574 completed (loss: 0.0893990695476532, acc: 0.9523809552192688)
[2024-11-08 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:25][root][INFO] - Training Epoch: 5/10, step 82/574 completed (loss: 0.12599213421344757, acc: 0.9473684430122375)
[2024-11-08 03:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:25][root][INFO] - Training Epoch: 5/10, step 83/574 completed (loss: 0.3601618707180023, acc: 0.8947368264198303)
[2024-11-08 03:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:26][root][INFO] - Training Epoch: 5/10, step 84/574 completed (loss: 0.010956278070807457, acc: 1.0)
[2024-11-08 03:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:26][root][INFO] - Training Epoch: 5/10, step 85/574 completed (loss: 0.1803932785987854, acc: 0.8571428656578064)
[2024-11-08 03:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:26][root][INFO] - Training Epoch: 5/10, step 86/574 completed (loss: 0.009129020385444164, acc: 1.0)
[2024-11-08 03:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:27][root][INFO] - Training Epoch: 5/10, step 87/574 completed (loss: 0.3054719567298889, acc: 0.8461538553237915)
[2024-11-08 03:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:28][root][INFO] - Training Epoch: 5/10, step 88/574 completed (loss: 0.22252441942691803, acc: 0.9523809552192688)
[2024-11-08 03:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:29][root][INFO] - Training Epoch: 5/10, step 89/574 completed (loss: 0.24817994236946106, acc: 0.9473684430122375)
[2024-11-08 03:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:30][root][INFO] - Training Epoch: 5/10, step 90/574 completed (loss: 0.3303084671497345, acc: 0.8421052694320679)
[2024-11-08 03:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:32][root][INFO] - Training Epoch: 5/10, step 91/574 completed (loss: 0.10826028138399124, acc: 0.9545454382896423)
[2024-11-08 03:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:33][root][INFO] - Training Epoch: 5/10, step 92/574 completed (loss: 0.10951407998800278, acc: 0.9523809552192688)
[2024-11-08 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:34][root][INFO] - Training Epoch: 5/10, step 93/574 completed (loss: 0.4577568769454956, acc: 0.8461538553237915)
[2024-11-08 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:35][root][INFO] - Training Epoch: 5/10, step 94/574 completed (loss: 0.2611025273799896, acc: 0.9642857313156128)
[2024-11-08 03:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:35][root][INFO] - Training Epoch: 5/10, step 95/574 completed (loss: 0.15418584644794464, acc: 0.9523809552192688)
[2024-11-08 03:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:36][root][INFO] - Training Epoch: 5/10, step 96/574 completed (loss: 0.22026227414608002, acc: 0.8947368264198303)
[2024-11-08 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:36][root][INFO] - Training Epoch: 5/10, step 97/574 completed (loss: 0.17992834746837616, acc: 0.8999999761581421)
[2024-11-08 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:36][root][INFO] - Training Epoch: 5/10, step 98/574 completed (loss: 0.39826980233192444, acc: 0.8571428656578064)
[2024-11-08 03:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:37][root][INFO] - Training Epoch: 5/10, step 99/574 completed (loss: 0.31605270504951477, acc: 0.9090909361839294)
[2024-11-08 03:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:37][root][INFO] - Training Epoch: 5/10, step 100/574 completed (loss: 0.06701334565877914, acc: 0.9523809552192688)
[2024-11-08 03:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:38][root][INFO] - Training Epoch: 5/10, step 101/574 completed (loss: 0.049479976296424866, acc: 0.9599999785423279)
[2024-11-08 03:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:38][root][INFO] - Training Epoch: 5/10, step 102/574 completed (loss: 0.11006704717874527, acc: 0.9523809552192688)
[2024-11-08 03:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:38][root][INFO] - Training Epoch: 5/10, step 103/574 completed (loss: 0.005696296691894531, acc: 1.0)
[2024-11-08 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:39][root][INFO] - Training Epoch: 5/10, step 104/574 completed (loss: 0.07777388393878937, acc: 0.9545454382896423)
[2024-11-08 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:39][root][INFO] - Training Epoch: 5/10, step 105/574 completed (loss: 0.00691990228369832, acc: 1.0)
[2024-11-08 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:40][root][INFO] - Training Epoch: 5/10, step 106/574 completed (loss: 0.07626346498727798, acc: 1.0)
[2024-11-08 03:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:40][root][INFO] - Training Epoch: 5/10, step 107/574 completed (loss: 0.018819626420736313, acc: 1.0)
[2024-11-08 03:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:40][root][INFO] - Training Epoch: 5/10, step 108/574 completed (loss: 0.009755118750035763, acc: 1.0)
[2024-11-08 03:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:41][root][INFO] - Training Epoch: 5/10, step 109/574 completed (loss: 0.12938086688518524, acc: 0.9523809552192688)
[2024-11-08 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:41][root][INFO] - Training Epoch: 5/10, step 110/574 completed (loss: 0.06229894980788231, acc: 1.0)
[2024-11-08 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:42][root][INFO] - Training Epoch: 5/10, step 111/574 completed (loss: 0.17779946327209473, acc: 0.9473684430122375)
[2024-11-08 03:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:42][root][INFO] - Training Epoch: 5/10, step 112/574 completed (loss: 0.010242927819490433, acc: 1.0)
[2024-11-08 03:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:43][root][INFO] - Training Epoch: 5/10, step 113/574 completed (loss: 0.07368607819080353, acc: 0.9473684430122375)
[2024-11-08 03:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:43][root][INFO] - Training Epoch: 5/10, step 114/574 completed (loss: 0.00593762518838048, acc: 1.0)
[2024-11-08 03:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:43][root][INFO] - Training Epoch: 5/10, step 115/574 completed (loss: 0.002233995357528329, acc: 1.0)
[2024-11-08 03:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:44][root][INFO] - Training Epoch: 5/10, step 116/574 completed (loss: 0.03056451864540577, acc: 1.0)
[2024-11-08 03:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:44][root][INFO] - Training Epoch: 5/10, step 117/574 completed (loss: 0.10821410268545151, acc: 0.949999988079071)
[2024-11-08 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:45][root][INFO] - Training Epoch: 5/10, step 118/574 completed (loss: 0.03277195245027542, acc: 1.0)
[2024-11-08 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:46][root][INFO] - Training Epoch: 5/10, step 119/574 completed (loss: 0.013505621813237667, acc: 1.0)
[2024-11-08 03:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:46][root][INFO] - Training Epoch: 5/10, step 120/574 completed (loss: 0.05423879623413086, acc: 1.0)
[2024-11-08 03:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:47][root][INFO] - Training Epoch: 5/10, step 121/574 completed (loss: 0.05602551996707916, acc: 1.0)
[2024-11-08 03:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:47][root][INFO] - Training Epoch: 5/10, step 122/574 completed (loss: 0.0037203559186309576, acc: 1.0)
[2024-11-08 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:48][root][INFO] - Training Epoch: 5/10, step 123/574 completed (loss: 0.5802362561225891, acc: 0.8461538553237915)
[2024-11-08 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:48][root][INFO] - Training Epoch: 5/10, step 124/574 completed (loss: 0.25323426723480225, acc: 0.8999999761581421)
[2024-11-08 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:49][root][INFO] - Training Epoch: 5/10, step 125/574 completed (loss: 0.20110006630420685, acc: 0.8999999761581421)
[2024-11-08 03:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:49][root][INFO] - Training Epoch: 5/10, step 126/574 completed (loss: 0.052182018756866455, acc: 1.0)
[2024-11-08 03:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:50][root][INFO] - Training Epoch: 5/10, step 127/574 completed (loss: 0.17695127427577972, acc: 0.9090909361839294)
[2024-11-08 03:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:50][root][INFO] - Training Epoch: 5/10, step 128/574 completed (loss: 0.07298130542039871, acc: 1.0)
[2024-11-08 03:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:51][root][INFO] - Training Epoch: 5/10, step 129/574 completed (loss: 0.02186495251953602, acc: 1.0)
[2024-11-08 03:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:51][root][INFO] - Training Epoch: 5/10, step 130/574 completed (loss: 0.10851819068193436, acc: 0.9696969985961914)
[2024-11-08 03:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:52][root][INFO] - Training Epoch: 5/10, step 131/574 completed (loss: 0.16419877111911774, acc: 0.9259259104728699)
[2024-11-08 03:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:52][root][INFO] - Training Epoch: 5/10, step 132/574 completed (loss: 0.14671114087104797, acc: 0.9696969985961914)
[2024-11-08 03:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:52][root][INFO] - Training Epoch: 5/10, step 133/574 completed (loss: 0.20364251732826233, acc: 0.8999999761581421)
[2024-11-08 03:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:53][root][INFO] - Training Epoch: 5/10, step 134/574 completed (loss: 0.007960223592817783, acc: 1.0)
[2024-11-08 03:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:37][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3263, device='cuda:0') eval_epoch_loss=tensor(0.2824, device='cuda:0') eval_epoch_acc=tensor(0.9170, device='cuda:0')
[2024-11-08 03:58:37][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 03:58:37][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 03:58:40][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_5_step_135_loss_0.28240713477134705/model.pt
[2024-11-08 03:58:40][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 03:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:41][root][INFO] - Training Epoch: 5/10, step 135/574 completed (loss: 0.43469199538230896, acc: 0.8947368264198303)
[2024-11-08 03:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:41][root][INFO] - Training Epoch: 5/10, step 136/574 completed (loss: 0.06459884345531464, acc: 1.0)
[2024-11-08 03:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:42][root][INFO] - Training Epoch: 5/10, step 137/574 completed (loss: 0.5165324807167053, acc: 0.8500000238418579)
[2024-11-08 03:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:42][root][INFO] - Training Epoch: 5/10, step 138/574 completed (loss: 0.12259319424629211, acc: 0.9090909361839294)
[2024-11-08 03:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:42][root][INFO] - Training Epoch: 5/10, step 139/574 completed (loss: 0.1191292330622673, acc: 0.9696969985961914)
[2024-11-08 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:43][root][INFO] - Training Epoch: 5/10, step 140/574 completed (loss: 0.15536795556545258, acc: 0.9259259104728699)
[2024-11-08 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:43][root][INFO] - Training Epoch: 5/10, step 141/574 completed (loss: 0.14635683596134186, acc: 0.939393937587738)
[2024-11-08 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:44][root][INFO] - Training Epoch: 5/10, step 142/574 completed (loss: 0.2420925348997116, acc: 0.8999999761581421)
[2024-11-08 03:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:44][root][INFO] - Training Epoch: 5/10, step 143/574 completed (loss: 0.14074596762657166, acc: 0.949999988079071)
[2024-11-08 03:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:45][root][INFO] - Training Epoch: 5/10, step 144/574 completed (loss: 0.027506209909915924, acc: 1.0)
[2024-11-08 03:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:45][root][INFO] - Training Epoch: 5/10, step 145/574 completed (loss: 0.11018940806388855, acc: 0.9545454382896423)
[2024-11-08 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:46][root][INFO] - Training Epoch: 5/10, step 146/574 completed (loss: 0.10606175661087036, acc: 0.949999988079071)
[2024-11-08 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:46][root][INFO] - Training Epoch: 5/10, step 147/574 completed (loss: 0.18946057558059692, acc: 0.9545454382896423)
[2024-11-08 03:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:47][root][INFO] - Training Epoch: 5/10, step 148/574 completed (loss: 0.22599023580551147, acc: 0.9696969985961914)
[2024-11-08 03:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:47][root][INFO] - Training Epoch: 5/10, step 149/574 completed (loss: 0.05230757221579552, acc: 0.9629629850387573)
[2024-11-08 03:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:48][root][INFO] - Training Epoch: 5/10, step 150/574 completed (loss: 0.05541005730628967, acc: 0.9677419066429138)
[2024-11-08 03:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:48][root][INFO] - Training Epoch: 5/10, step 151/574 completed (loss: 0.25292056798934937, acc: 0.8999999761581421)
[2024-11-08 03:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:49][root][INFO] - Training Epoch: 5/10, step 152/574 completed (loss: 0.014176120981574059, acc: 1.0)
[2024-11-08 03:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:49][root][INFO] - Training Epoch: 5/10, step 153/574 completed (loss: 0.02766546793282032, acc: 1.0)
[2024-11-08 03:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:49][root][INFO] - Training Epoch: 5/10, step 154/574 completed (loss: 0.11425595730543137, acc: 0.9545454382896423)
[2024-11-08 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:50][root][INFO] - Training Epoch: 5/10, step 155/574 completed (loss: 0.05406096577644348, acc: 1.0)
[2024-11-08 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:50][root][INFO] - Training Epoch: 5/10, step 156/574 completed (loss: 0.33710142970085144, acc: 0.8636363744735718)
[2024-11-08 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:51][root][INFO] - Training Epoch: 5/10, step 157/574 completed (loss: 0.25656330585479736, acc: 0.9090909361839294)
[2024-11-08 03:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:54][root][INFO] - Training Epoch: 5/10, step 158/574 completed (loss: 0.3035934865474701, acc: 0.8999999761581421)
[2024-11-08 03:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:54][root][INFO] - Training Epoch: 5/10, step 159/574 completed (loss: 0.18407724797725677, acc: 0.8999999761581421)
[2024-11-08 03:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:55][root][INFO] - Training Epoch: 5/10, step 160/574 completed (loss: 0.2063918262720108, acc: 0.9473684430122375)
[2024-11-08 03:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:55][root][INFO] - Training Epoch: 5/10, step 161/574 completed (loss: 0.19212032854557037, acc: 0.9545454382896423)
[2024-11-08 03:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:56][root][INFO] - Training Epoch: 5/10, step 162/574 completed (loss: 0.1880200356245041, acc: 0.949999988079071)
[2024-11-08 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:57][root][INFO] - Training Epoch: 5/10, step 163/574 completed (loss: 0.2230646312236786, acc: 0.9200000166893005)
[2024-11-08 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:57][root][INFO] - Training Epoch: 5/10, step 164/574 completed (loss: 0.0606125071644783, acc: 0.9629629850387573)
[2024-11-08 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:58][root][INFO] - Training Epoch: 5/10, step 165/574 completed (loss: 0.1367468535900116, acc: 0.9722222089767456)
[2024-11-08 03:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:58][root][INFO] - Training Epoch: 5/10, step 166/574 completed (loss: 0.07865510135889053, acc: 0.95652174949646)
[2024-11-08 03:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:59][root][INFO] - Training Epoch: 5/10, step 167/574 completed (loss: 0.11438827216625214, acc: 0.9523809552192688)
[2024-11-08 03:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:59][root][INFO] - Training Epoch: 5/10, step 168/574 completed (loss: 0.0012150993570685387, acc: 1.0)
[2024-11-08 03:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:58:59][root][INFO] - Training Epoch: 5/10, step 169/574 completed (loss: 0.1282888501882553, acc: 0.9545454382896423)
[2024-11-08 03:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:01][root][INFO] - Training Epoch: 5/10, step 170/574 completed (loss: 0.05233662202954292, acc: 0.949999988079071)
[2024-11-08 03:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:02][root][INFO] - Training Epoch: 5/10, step 171/574 completed (loss: 0.0092507004737854, acc: 1.0)
[2024-11-08 03:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:02][root][INFO] - Training Epoch: 5/10, step 172/574 completed (loss: 0.025914527475833893, acc: 1.0)
[2024-11-08 03:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:03][root][INFO] - Training Epoch: 5/10, step 173/574 completed (loss: 0.29165536165237427, acc: 0.8999999761581421)
[2024-11-08 03:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:03][root][INFO] - Training Epoch: 5/10, step 174/574 completed (loss: 0.07259459048509598, acc: 0.9523809552192688)
[2024-11-08 03:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:04][root][INFO] - Training Epoch: 5/10, step 175/574 completed (loss: 0.12904532253742218, acc: 0.9047619104385376)
[2024-11-08 03:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:04][root][INFO] - Training Epoch: 5/10, step 176/574 completed (loss: 0.0933796763420105, acc: 0.9473684430122375)
[2024-11-08 03:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:06][root][INFO] - Training Epoch: 5/10, step 177/574 completed (loss: 0.11468686908483505, acc: 0.9090909361839294)
[2024-11-08 03:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:07][root][INFO] - Training Epoch: 5/10, step 178/574 completed (loss: 0.010722038336098194, acc: 1.0)
[2024-11-08 03:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:07][root][INFO] - Training Epoch: 5/10, step 179/574 completed (loss: 0.0054047298617661, acc: 1.0)
[2024-11-08 03:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:07][root][INFO] - Training Epoch: 5/10, step 180/574 completed (loss: 0.0015535970451310277, acc: 1.0)
[2024-11-08 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:08][root][INFO] - Training Epoch: 5/10, step 181/574 completed (loss: 0.0005130373756401241, acc: 1.0)
[2024-11-08 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:08][root][INFO] - Training Epoch: 5/10, step 182/574 completed (loss: 0.007728440221399069, acc: 1.0)
[2024-11-08 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:09][root][INFO] - Training Epoch: 5/10, step 183/574 completed (loss: 0.021352441981434822, acc: 1.0)
[2024-11-08 03:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:09][root][INFO] - Training Epoch: 5/10, step 184/574 completed (loss: 0.011013769544661045, acc: 1.0)
[2024-11-08 03:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:10][root][INFO] - Training Epoch: 5/10, step 185/574 completed (loss: 0.0012816613307222724, acc: 1.0)
[2024-11-08 03:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:10][root][INFO] - Training Epoch: 5/10, step 186/574 completed (loss: 0.019484588876366615, acc: 1.0)
[2024-11-08 03:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:11][root][INFO] - Training Epoch: 5/10, step 187/574 completed (loss: 0.040509868413209915, acc: 1.0)
[2024-11-08 03:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:11][root][INFO] - Training Epoch: 5/10, step 188/574 completed (loss: 0.18002226948738098, acc: 0.9545454382896423)
[2024-11-08 03:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:12][root][INFO] - Training Epoch: 5/10, step 189/574 completed (loss: 0.240768164396286, acc: 0.8799999952316284)
[2024-11-08 03:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:13][root][INFO] - Training Epoch: 5/10, step 190/574 completed (loss: 0.31337037682533264, acc: 0.8999999761581421)
[2024-11-08 03:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:14][root][INFO] - Training Epoch: 5/10, step 191/574 completed (loss: 0.28780534863471985, acc: 0.9047619104385376)
[2024-11-08 03:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:15][root][INFO] - Training Epoch: 5/10, step 192/574 completed (loss: 0.31813135743141174, acc: 0.8947368264198303)
[2024-11-08 03:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:17][root][INFO] - Training Epoch: 5/10, step 193/574 completed (loss: 0.35499832034111023, acc: 0.9090909361839294)
[2024-11-08 03:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:18][root][INFO] - Training Epoch: 5/10, step 194/574 completed (loss: 0.25260359048843384, acc: 0.9047619104385376)
[2024-11-08 03:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:20][root][INFO] - Training Epoch: 5/10, step 195/574 completed (loss: 0.07278761267662048, acc: 0.9583333134651184)
[2024-11-08 03:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:20][root][INFO] - Training Epoch: 5/10, step 196/574 completed (loss: 0.07214784622192383, acc: 0.9677419066429138)
[2024-11-08 03:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:21][root][INFO] - Training Epoch: 5/10, step 197/574 completed (loss: 0.16543516516685486, acc: 0.9333333373069763)
[2024-11-08 03:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:21][root][INFO] - Training Epoch: 5/10, step 198/574 completed (loss: 0.02608291245996952, acc: 1.0)
[2024-11-08 03:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:21][root][INFO] - Training Epoch: 5/10, step 199/574 completed (loss: 0.1365201473236084, acc: 0.949999988079071)
[2024-11-08 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:22][root][INFO] - Training Epoch: 5/10, step 200/574 completed (loss: 0.11239512264728546, acc: 0.9473684430122375)
[2024-11-08 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:22][root][INFO] - Training Epoch: 5/10, step 201/574 completed (loss: 0.09051784873008728, acc: 0.9545454382896423)
[2024-11-08 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:23][root][INFO] - Training Epoch: 5/10, step 202/574 completed (loss: 0.09206617623567581, acc: 0.949999988079071)
[2024-11-08 03:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:23][root][INFO] - Training Epoch: 5/10, step 203/574 completed (loss: 0.21799246966838837, acc: 0.9130434989929199)
[2024-11-08 03:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:23][root][INFO] - Training Epoch: 5/10, step 204/574 completed (loss: 0.019520657137036324, acc: 1.0)
[2024-11-08 03:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:24][root][INFO] - Training Epoch: 5/10, step 205/574 completed (loss: 0.06396980583667755, acc: 0.9523809552192688)
[2024-11-08 03:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:24][root][INFO] - Training Epoch: 5/10, step 206/574 completed (loss: 0.047327034175395966, acc: 1.0)
[2024-11-08 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:25][root][INFO] - Training Epoch: 5/10, step 207/574 completed (loss: 0.12370821088552475, acc: 0.8947368264198303)
[2024-11-08 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:25][root][INFO] - Training Epoch: 5/10, step 208/574 completed (loss: 0.03106582723557949, acc: 1.0)
[2024-11-08 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:26][root][INFO] - Training Epoch: 5/10, step 209/574 completed (loss: 0.04657408967614174, acc: 1.0)
[2024-11-08 03:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:26][root][INFO] - Training Epoch: 5/10, step 210/574 completed (loss: 0.03321140632033348, acc: 1.0)
[2024-11-08 03:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:26][root][INFO] - Training Epoch: 5/10, step 211/574 completed (loss: 0.05205276980996132, acc: 0.9677419066429138)
[2024-11-08 03:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:27][root][INFO] - Training Epoch: 5/10, step 212/574 completed (loss: 0.13044369220733643, acc: 0.9354838728904724)
[2024-11-08 03:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:27][root][INFO] - Training Epoch: 5/10, step 213/574 completed (loss: 0.3579317629337311, acc: 0.9615384340286255)
[2024-11-08 03:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:28][root][INFO] - Training Epoch: 5/10, step 214/574 completed (loss: 0.05379219725728035, acc: 0.9523809552192688)
[2024-11-08 03:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:29][root][INFO] - Training Epoch: 5/10, step 215/574 completed (loss: 0.05157751590013504, acc: 1.0)
[2024-11-08 03:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:29][root][INFO] - Training Epoch: 5/10, step 216/574 completed (loss: 0.05652254447340965, acc: 1.0)
[2024-11-08 03:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:30][root][INFO] - Training Epoch: 5/10, step 217/574 completed (loss: 0.02320042811334133, acc: 1.0)
[2024-11-08 03:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:30][root][INFO] - Training Epoch: 5/10, step 218/574 completed (loss: 0.015512350015342236, acc: 1.0)
[2024-11-08 03:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:31][root][INFO] - Training Epoch: 5/10, step 219/574 completed (loss: 0.0681329295039177, acc: 0.9583333134651184)
[2024-11-08 03:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:31][root][INFO] - Training Epoch: 5/10, step 220/574 completed (loss: 0.034630123525857925, acc: 1.0)
[2024-11-08 03:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:31][root][INFO] - Training Epoch: 5/10, step 221/574 completed (loss: 0.3399113118648529, acc: 0.9677419066429138)
[2024-11-08 03:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:32][root][INFO] - Training Epoch: 5/10, step 222/574 completed (loss: 0.07672926038503647, acc: 0.9615384340286255)
[2024-11-08 03:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:33][root][INFO] - Training Epoch: 5/10, step 223/574 completed (loss: 0.06965859234333038, acc: 0.9523809552192688)
[2024-11-08 03:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:34][root][INFO] - Training Epoch: 5/10, step 224/574 completed (loss: 0.0781853049993515, acc: 1.0)
[2024-11-08 03:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:34][root][INFO] - Training Epoch: 5/10, step 225/574 completed (loss: 0.023930484429001808, acc: 1.0)
[2024-11-08 03:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:35][root][INFO] - Training Epoch: 5/10, step 226/574 completed (loss: 0.029067223891615868, acc: 1.0)
[2024-11-08 03:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:35][root][INFO] - Training Epoch: 5/10, step 227/574 completed (loss: 0.2495412528514862, acc: 0.9523809552192688)
[2024-11-08 03:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:36][root][INFO] - Training Epoch: 5/10, step 228/574 completed (loss: 0.15074880421161652, acc: 0.9166666865348816)
[2024-11-08 03:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:36][root][INFO] - Training Epoch: 5/10, step 229/574 completed (loss: 0.3871375322341919, acc: 0.8695651888847351)
[2024-11-08 03:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:36][root][INFO] - Training Epoch: 5/10, step 230/574 completed (loss: 0.2974000573158264, acc: 0.9047619104385376)
[2024-11-08 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:37][root][INFO] - Training Epoch: 5/10, step 231/574 completed (loss: 0.27828550338745117, acc: 0.8947368264198303)
[2024-11-08 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:37][root][INFO] - Training Epoch: 5/10, step 232/574 completed (loss: 0.2846437394618988, acc: 0.8947368264198303)
[2024-11-08 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:38][root][INFO] - Training Epoch: 5/10, step 233/574 completed (loss: 0.5748631358146667, acc: 0.8181818127632141)
[2024-11-08 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:39][root][INFO] - Training Epoch: 5/10, step 234/574 completed (loss: 0.3218897879123688, acc: 0.8181818127632141)
[2024-11-08 03:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:39][root][INFO] - Training Epoch: 5/10, step 235/574 completed (loss: 0.08915319293737411, acc: 0.9285714030265808)
[2024-11-08 03:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:39][root][INFO] - Training Epoch: 5/10, step 236/574 completed (loss: 0.0799926146864891, acc: 0.9629629850387573)
[2024-11-08 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:40][root][INFO] - Training Epoch: 5/10, step 237/574 completed (loss: 0.1938912719488144, acc: 0.9090909361839294)
[2024-11-08 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:40][root][INFO] - Training Epoch: 5/10, step 238/574 completed (loss: 0.12100253254175186, acc: 0.9545454382896423)
[2024-11-08 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:40][root][INFO] - Training Epoch: 5/10, step 239/574 completed (loss: 0.16240951418876648, acc: 0.9047619104385376)
[2024-11-08 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:41][root][INFO] - Training Epoch: 5/10, step 240/574 completed (loss: 0.40129148960113525, acc: 0.8947368264198303)
[2024-11-08 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:41][root][INFO] - Training Epoch: 5/10, step 241/574 completed (loss: 0.03383486717939377, acc: 1.0)
[2024-11-08 03:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:42][root][INFO] - Training Epoch: 5/10, step 242/574 completed (loss: 0.35282444953918457, acc: 0.8421052694320679)
[2024-11-08 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:43][root][INFO] - Training Epoch: 5/10, step 243/574 completed (loss: 0.06758839637041092, acc: 1.0)
[2024-11-08 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:43][root][INFO] - Training Epoch: 5/10, step 244/574 completed (loss: 0.020003417506814003, acc: 1.0)
[2024-11-08 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:44][root][INFO] - Training Epoch: 5/10, step 245/574 completed (loss: 0.13795293867588043, acc: 0.9333333373069763)
[2024-11-08 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:44][root][INFO] - Training Epoch: 5/10, step 246/574 completed (loss: 0.012741584330797195, acc: 1.0)
[2024-11-08 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:44][root][INFO] - Training Epoch: 5/10, step 247/574 completed (loss: 0.02266448549926281, acc: 1.0)
[2024-11-08 03:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:45][root][INFO] - Training Epoch: 5/10, step 248/574 completed (loss: 0.2948502004146576, acc: 0.9523809552192688)
[2024-11-08 03:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:45][root][INFO] - Training Epoch: 5/10, step 249/574 completed (loss: 0.03803497925400734, acc: 1.0)
[2024-11-08 03:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:46][root][INFO] - Training Epoch: 5/10, step 250/574 completed (loss: 0.19242919981479645, acc: 0.8999999761581421)
[2024-11-08 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:46][root][INFO] - Training Epoch: 5/10, step 251/574 completed (loss: 0.1101498007774353, acc: 0.9523809552192688)
[2024-11-08 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:46][root][INFO] - Training Epoch: 5/10, step 252/574 completed (loss: 0.013664099387824535, acc: 1.0)
[2024-11-08 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:47][root][INFO] - Training Epoch: 5/10, step 253/574 completed (loss: 0.009925462305545807, acc: 1.0)
[2024-11-08 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:47][root][INFO] - Training Epoch: 5/10, step 254/574 completed (loss: 0.04214004799723625, acc: 0.9655172228813171)
[2024-11-08 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:47][root][INFO] - Training Epoch: 5/10, step 255/574 completed (loss: 0.09624172002077103, acc: 0.9714285731315613)
[2024-11-08 03:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:48][root][INFO] - Training Epoch: 5/10, step 256/574 completed (loss: 0.005950615741312504, acc: 1.0)
[2024-11-08 03:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:48][root][INFO] - Training Epoch: 5/10, step 257/574 completed (loss: 0.1286991536617279, acc: 0.9523809552192688)
[2024-11-08 03:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:49][root][INFO] - Training Epoch: 5/10, step 258/574 completed (loss: 0.0706668347120285, acc: 0.9473684430122375)
[2024-11-08 03:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:49][root][INFO] - Training Epoch: 5/10, step 259/574 completed (loss: 0.11319153755903244, acc: 0.9545454382896423)
[2024-11-08 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:50][root][INFO] - Training Epoch: 5/10, step 260/574 completed (loss: 0.01459482777863741, acc: 1.0)
[2024-11-08 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:51][root][INFO] - Training Epoch: 5/10, step 261/574 completed (loss: 0.03454574942588806, acc: 1.0)
[2024-11-08 03:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:51][root][INFO] - Training Epoch: 5/10, step 262/574 completed (loss: 0.08124948292970657, acc: 0.970588207244873)
[2024-11-08 03:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:51][root][INFO] - Training Epoch: 5/10, step 263/574 completed (loss: 0.03167730197310448, acc: 1.0)
[2024-11-08 03:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:52][root][INFO] - Training Epoch: 5/10, step 264/574 completed (loss: 0.04452236741781235, acc: 1.0)
[2024-11-08 03:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:53][root][INFO] - Training Epoch: 5/10, step 265/574 completed (loss: 0.1378350555896759, acc: 0.9473684430122375)
[2024-11-08 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:53][root][INFO] - Training Epoch: 5/10, step 266/574 completed (loss: 0.14401239156723022, acc: 0.95652174949646)
[2024-11-08 03:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:54][root][INFO] - Training Epoch: 5/10, step 267/574 completed (loss: 0.20878571271896362, acc: 0.9047619104385376)
[2024-11-08 03:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:54][root][INFO] - Training Epoch: 5/10, step 268/574 completed (loss: 0.1348980814218521, acc: 0.931034505367279)
[2024-11-08 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:55][root][INFO] - Training Epoch: 5/10, step 269/574 completed (loss: 0.1524231731891632, acc: 0.9629629850387573)
[2024-11-08 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:55][root][INFO] - Training Epoch: 5/10, step 270/574 completed (loss: 0.07292553782463074, acc: 0.9230769276618958)
[2024-11-08 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:56][root][INFO] - Training Epoch: 5/10, step 271/574 completed (loss: 0.00588560476899147, acc: 1.0)
[2024-11-08 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:56][root][INFO] - Training Epoch: 5/10, step 272/574 completed (loss: 0.008904553018510342, acc: 1.0)
[2024-11-08 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:56][root][INFO] - Training Epoch: 5/10, step 273/574 completed (loss: 0.058948349207639694, acc: 1.0)
[2024-11-08 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:57][root][INFO] - Training Epoch: 5/10, step 274/574 completed (loss: 0.014517530798912048, acc: 1.0)
[2024-11-08 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:57][root][INFO] - Training Epoch: 5/10, step 275/574 completed (loss: 0.08934752643108368, acc: 0.9523809552192688)
[2024-11-08 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:58][root][INFO] - Training Epoch: 5/10, step 276/574 completed (loss: 0.08248972147703171, acc: 0.931034505367279)
[2024-11-08 03:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:58][root][INFO] - Training Epoch: 5/10, step 277/574 completed (loss: 0.040403272956609726, acc: 1.0)
[2024-11-08 03:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 03:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:41][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2869, device='cuda:0') eval_epoch_loss=tensor(0.2522, device='cuda:0') eval_epoch_acc=tensor(0.9284, device='cuda:0')
[2024-11-08 04:00:41][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:00:41][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:00:44][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_5_step_278_loss_0.2522341012954712/model.pt
[2024-11-08 04:00:44][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:45][root][INFO] - Training Epoch: 5/10, step 278/574 completed (loss: 0.08195102959871292, acc: 0.9523809552192688)
[2024-11-08 04:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:45][root][INFO] - Training Epoch: 5/10, step 279/574 completed (loss: 0.007839336059987545, acc: 1.0)
[2024-11-08 04:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:46][root][INFO] - Training Epoch: 5/10, step 280/574 completed (loss: 0.01464686170220375, acc: 1.0)
[2024-11-08 04:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:46][root][INFO] - Training Epoch: 5/10, step 281/574 completed (loss: 0.021741079166531563, acc: 1.0)
[2024-11-08 04:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:47][root][INFO] - Training Epoch: 5/10, step 282/574 completed (loss: 0.0031989533454179764, acc: 1.0)
[2024-11-08 04:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:47][root][INFO] - Training Epoch: 5/10, step 283/574 completed (loss: 0.022948550060391426, acc: 1.0)
[2024-11-08 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:48][root][INFO] - Training Epoch: 5/10, step 284/574 completed (loss: 0.08177769929170609, acc: 0.9333333373069763)
[2024-11-08 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:48][root][INFO] - Training Epoch: 5/10, step 285/574 completed (loss: 0.10348372161388397, acc: 0.9655172228813171)
[2024-11-08 04:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:49][root][INFO] - Training Epoch: 5/10, step 286/574 completed (loss: 0.11349381506443024, acc: 0.9523809552192688)
[2024-11-08 04:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:49][root][INFO] - Training Epoch: 5/10, step 287/574 completed (loss: 0.1607493907213211, acc: 0.8947368264198303)
[2024-11-08 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:50][root][INFO] - Training Epoch: 5/10, step 288/574 completed (loss: 0.1188669428229332, acc: 0.9473684430122375)
[2024-11-08 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:50][root][INFO] - Training Epoch: 5/10, step 289/574 completed (loss: 0.12044462561607361, acc: 0.9545454382896423)
[2024-11-08 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:51][root][INFO] - Training Epoch: 5/10, step 290/574 completed (loss: 0.05569842830300331, acc: 0.9545454382896423)
[2024-11-08 04:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:51][root][INFO] - Training Epoch: 5/10, step 291/574 completed (loss: 0.0032598653342574835, acc: 1.0)
[2024-11-08 04:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:51][root][INFO] - Training Epoch: 5/10, step 292/574 completed (loss: 0.1133372113108635, acc: 0.9666666388511658)
[2024-11-08 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:52][root][INFO] - Training Epoch: 5/10, step 293/574 completed (loss: 0.050882283598184586, acc: 1.0)
[2024-11-08 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:53][root][INFO] - Training Epoch: 5/10, step 294/574 completed (loss: 0.1800769567489624, acc: 0.8999999761581421)
[2024-11-08 04:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:53][root][INFO] - Training Epoch: 5/10, step 295/574 completed (loss: 0.3976523280143738, acc: 0.8947368264198303)
[2024-11-08 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:54][root][INFO] - Training Epoch: 5/10, step 296/574 completed (loss: 0.05033828318119049, acc: 1.0)
[2024-11-08 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:54][root][INFO] - Training Epoch: 5/10, step 297/574 completed (loss: 0.3188801407814026, acc: 0.9047619104385376)
[2024-11-08 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:55][root][INFO] - Training Epoch: 5/10, step 298/574 completed (loss: 0.18024809658527374, acc: 0.9166666865348816)
[2024-11-08 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:55][root][INFO] - Training Epoch: 5/10, step 299/574 completed (loss: 0.046531859785318375, acc: 0.9677419066429138)
[2024-11-08 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:56][root][INFO] - Training Epoch: 5/10, step 300/574 completed (loss: 0.058028653264045715, acc: 0.9677419066429138)
[2024-11-08 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:56][root][INFO] - Training Epoch: 5/10, step 301/574 completed (loss: 0.12846148014068604, acc: 0.9615384340286255)
[2024-11-08 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:57][root][INFO] - Training Epoch: 5/10, step 302/574 completed (loss: 0.15913550555706024, acc: 0.9523809552192688)
[2024-11-08 04:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:57][root][INFO] - Training Epoch: 5/10, step 303/574 completed (loss: 0.05422279238700867, acc: 0.9473684430122375)
[2024-11-08 04:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:57][root][INFO] - Training Epoch: 5/10, step 304/574 completed (loss: 0.03602682054042816, acc: 1.0)
[2024-11-08 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:58][root][INFO] - Training Epoch: 5/10, step 305/574 completed (loss: 0.21449151635169983, acc: 0.95652174949646)
[2024-11-08 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:58][root][INFO] - Training Epoch: 5/10, step 306/574 completed (loss: 0.1294076293706894, acc: 0.9545454382896423)
[2024-11-08 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:59][root][INFO] - Training Epoch: 5/10, step 307/574 completed (loss: 0.005109831225126982, acc: 1.0)
[2024-11-08 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:59][root][INFO] - Training Epoch: 5/10, step 308/574 completed (loss: 0.018922436982393265, acc: 1.0)
[2024-11-08 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:00:59][root][INFO] - Training Epoch: 5/10, step 309/574 completed (loss: 0.38874492049217224, acc: 0.8421052694320679)
[2024-11-08 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:00][root][INFO] - Training Epoch: 5/10, step 310/574 completed (loss: 0.09883370995521545, acc: 0.9473684430122375)
[2024-11-08 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:00][root][INFO] - Training Epoch: 5/10, step 311/574 completed (loss: 0.19312267005443573, acc: 0.9545454382896423)
[2024-11-08 04:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:01][root][INFO] - Training Epoch: 5/10, step 312/574 completed (loss: 0.26357725262641907, acc: 0.9090909361839294)
[2024-11-08 04:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:01][root][INFO] - Training Epoch: 5/10, step 313/574 completed (loss: 0.03634526580572128, acc: 1.0)
[2024-11-08 04:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:02][root][INFO] - Training Epoch: 5/10, step 314/574 completed (loss: 0.05114077776670456, acc: 1.0)
[2024-11-08 04:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:02][root][INFO] - Training Epoch: 5/10, step 315/574 completed (loss: 0.1460602879524231, acc: 0.9428571462631226)
[2024-11-08 04:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:02][root][INFO] - Training Epoch: 5/10, step 316/574 completed (loss: 0.14755436778068542, acc: 0.9230769276618958)
[2024-11-08 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:03][root][INFO] - Training Epoch: 5/10, step 317/574 completed (loss: 0.00680968165397644, acc: 1.0)
[2024-11-08 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:03][root][INFO] - Training Epoch: 5/10, step 318/574 completed (loss: 0.0521409697830677, acc: 1.0)
[2024-11-08 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:04][root][INFO] - Training Epoch: 5/10, step 319/574 completed (loss: 0.005891323555260897, acc: 1.0)
[2024-11-08 04:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:04][root][INFO] - Training Epoch: 5/10, step 320/574 completed (loss: 0.06765404343605042, acc: 0.9523809552192688)
[2024-11-08 04:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:04][root][INFO] - Training Epoch: 5/10, step 321/574 completed (loss: 0.009193903766572475, acc: 1.0)
[2024-11-08 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:05][root][INFO] - Training Epoch: 5/10, step 322/574 completed (loss: 0.3577500283718109, acc: 0.8695651888847351)
[2024-11-08 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:05][root][INFO] - Training Epoch: 5/10, step 323/574 completed (loss: 0.21040603518486023, acc: 0.9047619104385376)
[2024-11-08 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:06][root][INFO] - Training Epoch: 5/10, step 324/574 completed (loss: 0.4040704667568207, acc: 0.7894737124443054)
[2024-11-08 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:06][root][INFO] - Training Epoch: 5/10, step 325/574 completed (loss: 0.4536162316799164, acc: 0.7727272510528564)
[2024-11-08 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:06][root][INFO] - Training Epoch: 5/10, step 326/574 completed (loss: 0.27548161149024963, acc: 0.8947368264198303)
[2024-11-08 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:07][root][INFO] - Training Epoch: 5/10, step 327/574 completed (loss: 0.37273016571998596, acc: 0.8333333134651184)
[2024-11-08 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:07][root][INFO] - Training Epoch: 5/10, step 328/574 completed (loss: 0.23245306313037872, acc: 0.931034505367279)
[2024-11-08 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:08][root][INFO] - Training Epoch: 5/10, step 329/574 completed (loss: 0.027407445013523102, acc: 1.0)
[2024-11-08 04:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:08][root][INFO] - Training Epoch: 5/10, step 330/574 completed (loss: 0.08399371057748795, acc: 0.9523809552192688)
[2024-11-08 04:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:08][root][INFO] - Training Epoch: 5/10, step 331/574 completed (loss: 0.03272086754441261, acc: 1.0)
[2024-11-08 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:09][root][INFO] - Training Epoch: 5/10, step 332/574 completed (loss: 0.05253398418426514, acc: 1.0)
[2024-11-08 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:09][root][INFO] - Training Epoch: 5/10, step 333/574 completed (loss: 0.3233996331691742, acc: 0.9473684430122375)
[2024-11-08 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:10][root][INFO] - Training Epoch: 5/10, step 334/574 completed (loss: 0.06527812033891678, acc: 1.0)
[2024-11-08 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:10][root][INFO] - Training Epoch: 5/10, step 335/574 completed (loss: 0.0774756669998169, acc: 1.0)
[2024-11-08 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:10][root][INFO] - Training Epoch: 5/10, step 336/574 completed (loss: 0.33748766779899597, acc: 0.875)
[2024-11-08 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:11][root][INFO] - Training Epoch: 5/10, step 337/574 completed (loss: 0.1535310596227646, acc: 1.0)
[2024-11-08 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:11][root][INFO] - Training Epoch: 5/10, step 338/574 completed (loss: 0.28106239438056946, acc: 0.9130434989929199)
[2024-11-08 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:12][root][INFO] - Training Epoch: 5/10, step 339/574 completed (loss: 0.0593736432492733, acc: 0.9545454382896423)
[2024-11-08 04:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:12][root][INFO] - Training Epoch: 5/10, step 340/574 completed (loss: 0.01594715379178524, acc: 1.0)
[2024-11-08 04:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:12][root][INFO] - Training Epoch: 5/10, step 341/574 completed (loss: 0.04977517947554588, acc: 0.9696969985961914)
[2024-11-08 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:13][root][INFO] - Training Epoch: 5/10, step 342/574 completed (loss: 0.02697927877306938, acc: 1.0)
[2024-11-08 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:13][root][INFO] - Training Epoch: 5/10, step 343/574 completed (loss: 0.11178450286388397, acc: 0.949999988079071)
[2024-11-08 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:14][root][INFO] - Training Epoch: 5/10, step 344/574 completed (loss: 0.004384965170174837, acc: 1.0)
[2024-11-08 04:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:14][root][INFO] - Training Epoch: 5/10, step 345/574 completed (loss: 0.08374257385730743, acc: 0.9545454382896423)
[2024-11-08 04:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:14][root][INFO] - Training Epoch: 5/10, step 346/574 completed (loss: 0.011475794948637486, acc: 1.0)
[2024-11-08 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:15][root][INFO] - Training Epoch: 5/10, step 347/574 completed (loss: 0.004888927098363638, acc: 1.0)
[2024-11-08 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:15][root][INFO] - Training Epoch: 5/10, step 348/574 completed (loss: 0.1285615861415863, acc: 0.95652174949646)
[2024-11-08 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:16][root][INFO] - Training Epoch: 5/10, step 349/574 completed (loss: 0.024125467985868454, acc: 1.0)
[2024-11-08 04:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:16][root][INFO] - Training Epoch: 5/10, step 350/574 completed (loss: 0.260830819606781, acc: 0.8947368264198303)
[2024-11-08 04:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:16][root][INFO] - Training Epoch: 5/10, step 351/574 completed (loss: 0.07774832844734192, acc: 1.0)
[2024-11-08 04:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:17][root][INFO] - Training Epoch: 5/10, step 352/574 completed (loss: 0.17829973995685577, acc: 0.8999999761581421)
[2024-11-08 04:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:17][root][INFO] - Training Epoch: 5/10, step 353/574 completed (loss: 0.19367368519306183, acc: 0.9599999785423279)
[2024-11-08 04:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:18][root][INFO] - Training Epoch: 5/10, step 354/574 completed (loss: 0.15110152959823608, acc: 0.9200000166893005)
[2024-11-08 04:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:18][root][INFO] - Training Epoch: 5/10, step 355/574 completed (loss: 0.4177607595920563, acc: 0.8571428656578064)
[2024-11-08 04:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:19][root][INFO] - Training Epoch: 5/10, step 356/574 completed (loss: 0.1476375162601471, acc: 1.0)
[2024-11-08 04:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:19][root][INFO] - Training Epoch: 5/10, step 357/574 completed (loss: 0.08491413295269012, acc: 0.9523809552192688)
[2024-11-08 04:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:20][root][INFO] - Training Epoch: 5/10, step 358/574 completed (loss: 0.2578602731227875, acc: 0.9090909361839294)
[2024-11-08 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:20][root][INFO] - Training Epoch: 5/10, step 359/574 completed (loss: 0.06516863405704498, acc: 0.9642857313156128)
[2024-11-08 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:20][root][INFO] - Training Epoch: 5/10, step 360/574 completed (loss: 0.03807235509157181, acc: 1.0)
[2024-11-08 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:21][root][INFO] - Training Epoch: 5/10, step 361/574 completed (loss: 0.056709520518779755, acc: 0.9714285731315613)
[2024-11-08 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:21][root][INFO] - Training Epoch: 5/10, step 362/574 completed (loss: 0.16252213716506958, acc: 0.9090909361839294)
[2024-11-08 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:21][root][INFO] - Training Epoch: 5/10, step 363/574 completed (loss: 0.0032709084916859865, acc: 1.0)
[2024-11-08 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:22][root][INFO] - Training Epoch: 5/10, step 364/574 completed (loss: 0.2067756950855255, acc: 0.8571428656578064)
[2024-11-08 04:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:22][root][INFO] - Training Epoch: 5/10, step 365/574 completed (loss: 0.02481800690293312, acc: 1.0)
[2024-11-08 04:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:22][root][INFO] - Training Epoch: 5/10, step 366/574 completed (loss: 0.02724982611835003, acc: 1.0)
[2024-11-08 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:23][root][INFO] - Training Epoch: 5/10, step 367/574 completed (loss: 0.033355727791786194, acc: 1.0)
[2024-11-08 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:23][root][INFO] - Training Epoch: 5/10, step 368/574 completed (loss: 0.07315671443939209, acc: 0.96875)
[2024-11-08 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:23][root][INFO] - Training Epoch: 5/10, step 369/574 completed (loss: 0.20665568113327026, acc: 0.9047619104385376)
[2024-11-08 04:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:24][root][INFO] - Training Epoch: 5/10, step 370/574 completed (loss: 0.24704615771770477, acc: 0.9047619104385376)
[2024-11-08 04:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:26][root][INFO] - Training Epoch: 5/10, step 371/574 completed (loss: 0.36178648471832275, acc: 0.8421052694320679)
[2024-11-08 04:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:26][root][INFO] - Training Epoch: 5/10, step 372/574 completed (loss: 0.34216398000717163, acc: 0.9545454382896423)
[2024-11-08 04:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:26][root][INFO] - Training Epoch: 5/10, step 373/574 completed (loss: 0.025194557383656502, acc: 1.0)
[2024-11-08 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:27][root][INFO] - Training Epoch: 5/10, step 374/574 completed (loss: 0.09612536430358887, acc: 0.9545454382896423)
[2024-11-08 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:27][root][INFO] - Training Epoch: 5/10, step 375/574 completed (loss: 0.10376646369695663, acc: 0.9655172228813171)
[2024-11-08 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:28][root][INFO] - Training Epoch: 5/10, step 376/574 completed (loss: 0.003209377871826291, acc: 1.0)
[2024-11-08 04:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:28][root][INFO] - Training Epoch: 5/10, step 377/574 completed (loss: 0.12384668737649918, acc: 0.9523809552192688)
[2024-11-08 04:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:28][root][INFO] - Training Epoch: 5/10, step 378/574 completed (loss: 0.07050248235464096, acc: 1.0)
[2024-11-08 04:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:29][root][INFO] - Training Epoch: 5/10, step 379/574 completed (loss: 0.059077080339193344, acc: 1.0)
[2024-11-08 04:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:30][root][INFO] - Training Epoch: 5/10, step 380/574 completed (loss: 0.17980404198169708, acc: 0.9545454382896423)
[2024-11-08 04:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:31][root][INFO] - Training Epoch: 5/10, step 381/574 completed (loss: 0.035385675728321075, acc: 1.0)
[2024-11-08 04:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:32][root][INFO] - Training Epoch: 5/10, step 382/574 completed (loss: 0.06499569118022919, acc: 0.9545454382896423)
[2024-11-08 04:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:33][root][INFO] - Training Epoch: 5/10, step 383/574 completed (loss: 0.05577182397246361, acc: 0.9642857313156128)
[2024-11-08 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:33][root][INFO] - Training Epoch: 5/10, step 384/574 completed (loss: 0.016747355461120605, acc: 1.0)
[2024-11-08 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:33][root][INFO] - Training Epoch: 5/10, step 385/574 completed (loss: 0.004104586783796549, acc: 1.0)
[2024-11-08 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:34][root][INFO] - Training Epoch: 5/10, step 386/574 completed (loss: 0.0144609110429883, acc: 1.0)
[2024-11-08 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:34][root][INFO] - Training Epoch: 5/10, step 387/574 completed (loss: 0.020348524674773216, acc: 1.0)
[2024-11-08 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:35][root][INFO] - Training Epoch: 5/10, step 388/574 completed (loss: 0.002514804946258664, acc: 1.0)
[2024-11-08 04:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:35][root][INFO] - Training Epoch: 5/10, step 389/574 completed (loss: 0.010425493121147156, acc: 1.0)
[2024-11-08 04:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:35][root][INFO] - Training Epoch: 5/10, step 390/574 completed (loss: 0.030951358377933502, acc: 1.0)
[2024-11-08 04:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:36][root][INFO] - Training Epoch: 5/10, step 391/574 completed (loss: 0.19921892881393433, acc: 0.9047619104385376)
[2024-11-08 04:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:36][root][INFO] - Training Epoch: 5/10, step 392/574 completed (loss: 0.24683614075183868, acc: 0.8947368264198303)
[2024-11-08 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:37][root][INFO] - Training Epoch: 5/10, step 393/574 completed (loss: 0.17257091403007507, acc: 0.9473684430122375)
[2024-11-08 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:37][root][INFO] - Training Epoch: 5/10, step 394/574 completed (loss: 0.22539931535720825, acc: 0.9090909361839294)
[2024-11-08 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:38][root][INFO] - Training Epoch: 5/10, step 395/574 completed (loss: 0.25983259081840515, acc: 0.9523809552192688)
[2024-11-08 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:38][root][INFO] - Training Epoch: 5/10, step 396/574 completed (loss: 0.018529267981648445, acc: 1.0)
[2024-11-08 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:39][root][INFO] - Training Epoch: 5/10, step 397/574 completed (loss: 0.1057129055261612, acc: 0.9333333373069763)
[2024-11-08 04:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:39][root][INFO] - Training Epoch: 5/10, step 398/574 completed (loss: 0.026952054351568222, acc: 1.0)
[2024-11-08 04:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:39][root][INFO] - Training Epoch: 5/10, step 399/574 completed (loss: 0.004391711205244064, acc: 1.0)
[2024-11-08 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:40][root][INFO] - Training Epoch: 5/10, step 400/574 completed (loss: 0.12086537480354309, acc: 0.9545454382896423)
[2024-11-08 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:41][root][INFO] - Training Epoch: 5/10, step 401/574 completed (loss: 0.0841042622923851, acc: 0.9523809552192688)
[2024-11-08 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:41][root][INFO] - Training Epoch: 5/10, step 402/574 completed (loss: 0.013696424663066864, acc: 1.0)
[2024-11-08 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:41][root][INFO] - Training Epoch: 5/10, step 403/574 completed (loss: 0.010162847116589546, acc: 1.0)
[2024-11-08 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:42][root][INFO] - Training Epoch: 5/10, step 404/574 completed (loss: 0.05233159288764, acc: 0.9714285731315613)
[2024-11-08 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:42][root][INFO] - Training Epoch: 5/10, step 405/574 completed (loss: 0.12295754998922348, acc: 0.9615384340286255)
[2024-11-08 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:42][root][INFO] - Training Epoch: 5/10, step 406/574 completed (loss: 0.009587181732058525, acc: 1.0)
[2024-11-08 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:43][root][INFO] - Training Epoch: 5/10, step 407/574 completed (loss: 0.00847694557160139, acc: 1.0)
[2024-11-08 04:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:43][root][INFO] - Training Epoch: 5/10, step 408/574 completed (loss: 0.006318129599094391, acc: 1.0)
[2024-11-08 04:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:43][root][INFO] - Training Epoch: 5/10, step 409/574 completed (loss: 0.022131288424134254, acc: 1.0)
[2024-11-08 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:44][root][INFO] - Training Epoch: 5/10, step 410/574 completed (loss: 0.0035834291484206915, acc: 1.0)
[2024-11-08 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:44][root][INFO] - Training Epoch: 5/10, step 411/574 completed (loss: 0.06569177657365799, acc: 0.9642857313156128)
[2024-11-08 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:45][root][INFO] - Training Epoch: 5/10, step 412/574 completed (loss: 0.11608719080686569, acc: 0.9629629850387573)
[2024-11-08 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:45][root][INFO] - Training Epoch: 5/10, step 413/574 completed (loss: 0.04506933316588402, acc: 0.9714285731315613)
[2024-11-08 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:45][root][INFO] - Training Epoch: 5/10, step 414/574 completed (loss: 0.03425078094005585, acc: 0.9615384340286255)
[2024-11-08 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:46][root][INFO] - Training Epoch: 5/10, step 415/574 completed (loss: 0.023668749257922173, acc: 1.0)
[2024-11-08 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:46][root][INFO] - Training Epoch: 5/10, step 416/574 completed (loss: 0.006402940489351749, acc: 1.0)
[2024-11-08 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:46][root][INFO] - Training Epoch: 5/10, step 417/574 completed (loss: 0.005831177346408367, acc: 1.0)
[2024-11-08 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:47][root][INFO] - Training Epoch: 5/10, step 418/574 completed (loss: 0.020400362089276314, acc: 1.0)
[2024-11-08 04:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:47][root][INFO] - Training Epoch: 5/10, step 419/574 completed (loss: 0.010127967223525047, acc: 1.0)
[2024-11-08 04:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:47][root][INFO] - Training Epoch: 5/10, step 420/574 completed (loss: 0.018466869369149208, acc: 1.0)
[2024-11-08 04:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:30][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3078, device='cuda:0') eval_epoch_loss=tensor(0.2684, device='cuda:0') eval_epoch_acc=tensor(0.9339, device='cuda:0')
[2024-11-08 04:02:30][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:02:30][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:02:34][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_5_step_421_loss_0.26836898922920227/model.pt
[2024-11-08 04:02:34][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:34][root][INFO] - Training Epoch: 5/10, step 421/574 completed (loss: 0.03259584307670593, acc: 1.0)
[2024-11-08 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:35][root][INFO] - Training Epoch: 5/10, step 422/574 completed (loss: 0.08029244095087051, acc: 0.9677419066429138)
[2024-11-08 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:35][root][INFO] - Training Epoch: 5/10, step 423/574 completed (loss: 0.0059641096740961075, acc: 1.0)
[2024-11-08 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:36][root][INFO] - Training Epoch: 5/10, step 424/574 completed (loss: 0.061494968831539154, acc: 0.9523809552192688)
[2024-11-08 04:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:36][root][INFO] - Training Epoch: 5/10, step 425/574 completed (loss: 0.06785522401332855, acc: 0.9473684430122375)
[2024-11-08 04:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:36][root][INFO] - Training Epoch: 5/10, step 426/574 completed (loss: 0.02065531350672245, acc: 1.0)
[2024-11-08 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:37][root][INFO] - Training Epoch: 5/10, step 427/574 completed (loss: 0.023421810939908028, acc: 1.0)
[2024-11-08 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:37][root][INFO] - Training Epoch: 5/10, step 428/574 completed (loss: 0.004509711172431707, acc: 1.0)
[2024-11-08 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:38][root][INFO] - Training Epoch: 5/10, step 429/574 completed (loss: 0.07229221612215042, acc: 0.9583333134651184)
[2024-11-08 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:38][root][INFO] - Training Epoch: 5/10, step 430/574 completed (loss: 0.0011718120658770204, acc: 1.0)
[2024-11-08 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:39][root][INFO] - Training Epoch: 5/10, step 431/574 completed (loss: 0.000793439510744065, acc: 1.0)
[2024-11-08 04:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:39][root][INFO] - Training Epoch: 5/10, step 432/574 completed (loss: 0.0027630154509097338, acc: 1.0)
[2024-11-08 04:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:39][root][INFO] - Training Epoch: 5/10, step 433/574 completed (loss: 0.003983075264841318, acc: 1.0)
[2024-11-08 04:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:40][root][INFO] - Training Epoch: 5/10, step 434/574 completed (loss: 0.09507837891578674, acc: 0.9473684430122375)
[2024-11-08 04:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:40][root][INFO] - Training Epoch: 5/10, step 435/574 completed (loss: 0.0054664453491568565, acc: 1.0)
[2024-11-08 04:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:41][root][INFO] - Training Epoch: 5/10, step 436/574 completed (loss: 0.05277376249432564, acc: 1.0)
[2024-11-08 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:41][root][INFO] - Training Epoch: 5/10, step 437/574 completed (loss: 0.004412277601659298, acc: 1.0)
[2024-11-08 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:41][root][INFO] - Training Epoch: 5/10, step 438/574 completed (loss: 0.02731338143348694, acc: 1.0)
[2024-11-08 04:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:42][root][INFO] - Training Epoch: 5/10, step 439/574 completed (loss: 0.08110002428293228, acc: 0.9642857313156128)
[2024-11-08 04:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:43][root][INFO] - Training Epoch: 5/10, step 440/574 completed (loss: 0.0267795417457819, acc: 1.0)
[2024-11-08 04:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:44][root][INFO] - Training Epoch: 5/10, step 441/574 completed (loss: 0.12211531400680542, acc: 1.0)
[2024-11-08 04:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:44][root][INFO] - Training Epoch: 5/10, step 442/574 completed (loss: 0.267164021730423, acc: 0.8947368264198303)
[2024-11-08 04:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:45][root][INFO] - Training Epoch: 5/10, step 443/574 completed (loss: 0.06751124560832977, acc: 1.0)
[2024-11-08 04:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:45][root][INFO] - Training Epoch: 5/10, step 444/574 completed (loss: 0.00895287562161684, acc: 1.0)
[2024-11-08 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:46][root][INFO] - Training Epoch: 5/10, step 445/574 completed (loss: 0.007991361431777477, acc: 1.0)
[2024-11-08 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:47][root][INFO] - Training Epoch: 5/10, step 446/574 completed (loss: 0.193540558218956, acc: 0.9677419066429138)
[2024-11-08 04:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:47][root][INFO] - Training Epoch: 5/10, step 447/574 completed (loss: 0.17434021830558777, acc: 0.9354838728904724)
[2024-11-08 04:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:47][root][INFO] - Training Epoch: 5/10, step 448/574 completed (loss: 0.00384496059268713, acc: 1.0)
[2024-11-08 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:48][root][INFO] - Training Epoch: 5/10, step 449/574 completed (loss: 0.004433245398104191, acc: 1.0)
[2024-11-08 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:48][root][INFO] - Training Epoch: 5/10, step 450/574 completed (loss: 0.003361741779372096, acc: 1.0)
[2024-11-08 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:48][root][INFO] - Training Epoch: 5/10, step 451/574 completed (loss: 0.03997518867254257, acc: 1.0)
[2024-11-08 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:49][root][INFO] - Training Epoch: 5/10, step 452/574 completed (loss: 0.0016844019992277026, acc: 1.0)
[2024-11-08 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:49][root][INFO] - Training Epoch: 5/10, step 453/574 completed (loss: 0.016239816322922707, acc: 1.0)
[2024-11-08 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:50][root][INFO] - Training Epoch: 5/10, step 454/574 completed (loss: 0.003009401261806488, acc: 1.0)
[2024-11-08 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:50][root][INFO] - Training Epoch: 5/10, step 455/574 completed (loss: 0.0015609704423695803, acc: 1.0)
[2024-11-08 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:50][root][INFO] - Training Epoch: 5/10, step 456/574 completed (loss: 0.27238067984580994, acc: 0.8999999761581421)
[2024-11-08 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:51][root][INFO] - Training Epoch: 5/10, step 457/574 completed (loss: 0.00959896482527256, acc: 1.0)
[2024-11-08 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:51][root][INFO] - Training Epoch: 5/10, step 458/574 completed (loss: 0.07304045557975769, acc: 0.9473684430122375)
[2024-11-08 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:52][root][INFO] - Training Epoch: 5/10, step 459/574 completed (loss: 0.02157198078930378, acc: 1.0)
[2024-11-08 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:52][root][INFO] - Training Epoch: 5/10, step 460/574 completed (loss: 0.028492677956819534, acc: 1.0)
[2024-11-08 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:52][root][INFO] - Training Epoch: 5/10, step 461/574 completed (loss: 0.0019443436758592725, acc: 1.0)
[2024-11-08 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:53][root][INFO] - Training Epoch: 5/10, step 462/574 completed (loss: 0.003444642759859562, acc: 1.0)
[2024-11-08 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:53][root][INFO] - Training Epoch: 5/10, step 463/574 completed (loss: 0.11750883609056473, acc: 0.9629629850387573)
[2024-11-08 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:54][root][INFO] - Training Epoch: 5/10, step 464/574 completed (loss: 0.00658138282597065, acc: 1.0)
[2024-11-08 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:54][root][INFO] - Training Epoch: 5/10, step 465/574 completed (loss: 0.014368449337780476, acc: 1.0)
[2024-11-08 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:55][root][INFO] - Training Epoch: 5/10, step 466/574 completed (loss: 0.007093979511409998, acc: 1.0)
[2024-11-08 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:55][root][INFO] - Training Epoch: 5/10, step 467/574 completed (loss: 0.0059243980795145035, acc: 1.0)
[2024-11-08 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:55][root][INFO] - Training Epoch: 5/10, step 468/574 completed (loss: 0.16395360231399536, acc: 0.949999988079071)
[2024-11-08 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:56][root][INFO] - Training Epoch: 5/10, step 469/574 completed (loss: 0.03719242289662361, acc: 0.9523809552192688)
[2024-11-08 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:56][root][INFO] - Training Epoch: 5/10, step 470/574 completed (loss: 0.0035288287326693535, acc: 1.0)
[2024-11-08 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:57][root][INFO] - Training Epoch: 5/10, step 471/574 completed (loss: 0.007221338339149952, acc: 1.0)
[2024-11-08 04:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:57][root][INFO] - Training Epoch: 5/10, step 472/574 completed (loss: 0.04018525779247284, acc: 1.0)
[2024-11-08 04:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:58][root][INFO] - Training Epoch: 5/10, step 473/574 completed (loss: 0.1146315187215805, acc: 0.9523809552192688)
[2024-11-08 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:58][root][INFO] - Training Epoch: 5/10, step 474/574 completed (loss: 0.06003084406256676, acc: 1.0)
[2024-11-08 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:58][root][INFO] - Training Epoch: 5/10, step 475/574 completed (loss: 0.018418656662106514, acc: 1.0)
[2024-11-08 04:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:59][root][INFO] - Training Epoch: 5/10, step 476/574 completed (loss: 0.002830210141837597, acc: 1.0)
[2024-11-08 04:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:02:59][root][INFO] - Training Epoch: 5/10, step 477/574 completed (loss: 0.12955456972122192, acc: 0.9545454382896423)
[2024-11-08 04:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:00][root][INFO] - Training Epoch: 5/10, step 478/574 completed (loss: 0.04402533918619156, acc: 1.0)
[2024-11-08 04:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:00][root][INFO] - Training Epoch: 5/10, step 479/574 completed (loss: 0.03640660271048546, acc: 1.0)
[2024-11-08 04:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:01][root][INFO] - Training Epoch: 5/10, step 480/574 completed (loss: 0.04206316918134689, acc: 0.9677419066429138)
[2024-11-08 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:01][root][INFO] - Training Epoch: 5/10, step 481/574 completed (loss: 0.032277822494506836, acc: 1.0)
[2024-11-08 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:01][root][INFO] - Training Epoch: 5/10, step 482/574 completed (loss: 0.1654336005449295, acc: 0.8947368264198303)
[2024-11-08 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:02][root][INFO] - Training Epoch: 5/10, step 483/574 completed (loss: 0.13127537071704865, acc: 0.9090909361839294)
[2024-11-08 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:02][root][INFO] - Training Epoch: 5/10, step 484/574 completed (loss: 0.00462675653398037, acc: 1.0)
[2024-11-08 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:03][root][INFO] - Training Epoch: 5/10, step 485/574 completed (loss: 0.0037805831525474787, acc: 1.0)
[2024-11-08 04:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:03][root][INFO] - Training Epoch: 5/10, step 486/574 completed (loss: 0.012507732026278973, acc: 1.0)
[2024-11-08 04:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:03][root][INFO] - Training Epoch: 5/10, step 487/574 completed (loss: 0.1344146579504013, acc: 0.9714285731315613)
[2024-11-08 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:04][root][INFO] - Training Epoch: 5/10, step 488/574 completed (loss: 0.15654125809669495, acc: 0.8846153616905212)
[2024-11-08 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:04][root][INFO] - Training Epoch: 5/10, step 489/574 completed (loss: 0.10360699892044067, acc: 0.9523809552192688)
[2024-11-08 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:05][root][INFO] - Training Epoch: 5/10, step 490/574 completed (loss: 0.011748839169740677, acc: 1.0)
[2024-11-08 04:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:05][root][INFO] - Training Epoch: 5/10, step 491/574 completed (loss: 0.0060609327629208565, acc: 1.0)
[2024-11-08 04:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:05][root][INFO] - Training Epoch: 5/10, step 492/574 completed (loss: 0.015049468725919724, acc: 1.0)
[2024-11-08 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:06][root][INFO] - Training Epoch: 5/10, step 493/574 completed (loss: 0.1399899274110794, acc: 0.9523809552192688)
[2024-11-08 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:06][root][INFO] - Training Epoch: 5/10, step 494/574 completed (loss: 0.22028225660324097, acc: 0.9583333134651184)
[2024-11-08 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:06][root][INFO] - Training Epoch: 5/10, step 495/574 completed (loss: 0.6015644073486328, acc: 0.9285714030265808)
[2024-11-08 04:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:07][root][INFO] - Training Epoch: 5/10, step 496/574 completed (loss: 0.01760786399245262, acc: 1.0)
[2024-11-08 04:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:07][root][INFO] - Training Epoch: 5/10, step 497/574 completed (loss: 0.1937592625617981, acc: 0.949999988079071)
[2024-11-08 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:08][root][INFO] - Training Epoch: 5/10, step 498/574 completed (loss: 0.18055015802383423, acc: 0.9473684430122375)
[2024-11-08 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:08][root][INFO] - Training Epoch: 5/10, step 499/574 completed (loss: 0.24433563649654388, acc: 0.9545454382896423)
[2024-11-08 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:09][root][INFO] - Training Epoch: 5/10, step 500/574 completed (loss: 0.1660183221101761, acc: 0.949999988079071)
[2024-11-08 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:09][root][INFO] - Training Epoch: 5/10, step 501/574 completed (loss: 0.1117461696267128, acc: 0.9583333134651184)
[2024-11-08 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:09][root][INFO] - Training Epoch: 5/10, step 502/574 completed (loss: 0.07158111035823822, acc: 0.9677419066429138)
[2024-11-08 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:10][root][INFO] - Training Epoch: 5/10, step 503/574 completed (loss: 0.0932842567563057, acc: 0.9677419066429138)
[2024-11-08 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:10][root][INFO] - Training Epoch: 5/10, step 504/574 completed (loss: 0.09211646020412445, acc: 0.95652174949646)
[2024-11-08 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:10][root][INFO] - Training Epoch: 5/10, step 505/574 completed (loss: 0.16259045898914337, acc: 0.9523809552192688)
[2024-11-08 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:11][root][INFO] - Training Epoch: 5/10, step 506/574 completed (loss: 0.3271005153656006, acc: 0.8947368264198303)
[2024-11-08 04:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:12][root][INFO] - Training Epoch: 5/10, step 507/574 completed (loss: 0.01959245279431343, acc: 1.0)
[2024-11-08 04:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:12][root][INFO] - Training Epoch: 5/10, step 508/574 completed (loss: 0.29699426889419556, acc: 0.8500000238418579)
[2024-11-08 04:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:13][root][INFO] - Training Epoch: 5/10, step 509/574 completed (loss: 0.346524715423584, acc: 0.9090909361839294)
[2024-11-08 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:13][root][INFO] - Training Epoch: 5/10, step 510/574 completed (loss: 0.020764382556080818, acc: 1.0)
[2024-11-08 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:13][root][INFO] - Training Epoch: 5/10, step 511/574 completed (loss: 0.07465996593236923, acc: 0.9666666388511658)
[2024-11-08 04:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:17][root][INFO] - Training Epoch: 5/10, step 512/574 completed (loss: 0.13426028192043304, acc: 0.9285714030265808)
[2024-11-08 04:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:18][root][INFO] - Training Epoch: 5/10, step 513/574 completed (loss: 0.05570809543132782, acc: 1.0)
[2024-11-08 04:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:18][root][INFO] - Training Epoch: 5/10, step 514/574 completed (loss: 0.18555647134780884, acc: 0.8947368264198303)
[2024-11-08 04:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:19][root][INFO] - Training Epoch: 5/10, step 515/574 completed (loss: 0.13845166563987732, acc: 0.949999988079071)
[2024-11-08 04:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:20][root][INFO] - Training Epoch: 5/10, step 516/574 completed (loss: 0.03199780359864235, acc: 1.0)
[2024-11-08 04:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:20][root][INFO] - Training Epoch: 5/10, step 517/574 completed (loss: 0.019527507945895195, acc: 1.0)
[2024-11-08 04:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:20][root][INFO] - Training Epoch: 5/10, step 518/574 completed (loss: 0.2651272118091583, acc: 0.9285714030265808)
[2024-11-08 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:21][root][INFO] - Training Epoch: 5/10, step 519/574 completed (loss: 0.13432961702346802, acc: 0.9629629850387573)
[2024-11-08 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:21][root][INFO] - Training Epoch: 5/10, step 520/574 completed (loss: 0.11034957319498062, acc: 0.9428571462631226)
[2024-11-08 04:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:23][root][INFO] - Training Epoch: 5/10, step 521/574 completed (loss: 0.02942206896841526, acc: 1.0)
[2024-11-08 04:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:23][root][INFO] - Training Epoch: 5/10, step 522/574 completed (loss: 0.06244003027677536, acc: 1.0)
[2024-11-08 04:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:23][root][INFO] - Training Epoch: 5/10, step 523/574 completed (loss: 0.18579010665416718, acc: 0.9473684430122375)
[2024-11-08 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:24][root][INFO] - Training Epoch: 5/10, step 524/574 completed (loss: 0.15017570555210114, acc: 0.9545454382896423)
[2024-11-08 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:25][root][INFO] - Training Epoch: 5/10, step 525/574 completed (loss: 0.14569994807243347, acc: 0.949999988079071)
[2024-11-08 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:25][root][INFO] - Training Epoch: 5/10, step 526/574 completed (loss: 0.09804877638816833, acc: 0.9545454382896423)
[2024-11-08 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:25][root][INFO] - Training Epoch: 5/10, step 527/574 completed (loss: 0.07474768906831741, acc: 0.9696969985961914)
[2024-11-08 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:26][root][INFO] - Training Epoch: 5/10, step 528/574 completed (loss: 0.17294301092624664, acc: 0.9285714030265808)
[2024-11-08 04:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:26][root][INFO] - Training Epoch: 5/10, step 529/574 completed (loss: 0.025652088224887848, acc: 1.0)
[2024-11-08 04:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:26][root][INFO] - Training Epoch: 5/10, step 530/574 completed (loss: 0.4009096324443817, acc: 0.8947368264198303)
[2024-11-08 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:27][root][INFO] - Training Epoch: 5/10, step 531/574 completed (loss: 0.20689436793327332, acc: 0.8999999761581421)
[2024-11-08 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:27][root][INFO] - Training Epoch: 5/10, step 532/574 completed (loss: 0.11385330557823181, acc: 0.9523809552192688)
[2024-11-08 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:28][root][INFO] - Training Epoch: 5/10, step 533/574 completed (loss: 0.030667394399642944, acc: 1.0)
[2024-11-08 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:28][root][INFO] - Training Epoch: 5/10, step 534/574 completed (loss: 0.1440984010696411, acc: 0.9642857313156128)
[2024-11-08 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:28][root][INFO] - Training Epoch: 5/10, step 535/574 completed (loss: 0.011926366947591305, acc: 1.0)
[2024-11-08 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:29][root][INFO] - Training Epoch: 5/10, step 536/574 completed (loss: 0.07576403766870499, acc: 0.9599999785423279)
[2024-11-08 04:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:29][root][INFO] - Training Epoch: 5/10, step 537/574 completed (loss: 0.02526290901005268, acc: 1.0)
[2024-11-08 04:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:30][root][INFO] - Training Epoch: 5/10, step 538/574 completed (loss: 0.14262787997722626, acc: 0.9473684430122375)
[2024-11-08 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:30][root][INFO] - Training Epoch: 5/10, step 539/574 completed (loss: 0.19577524065971375, acc: 0.949999988079071)
[2024-11-08 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:30][root][INFO] - Training Epoch: 5/10, step 540/574 completed (loss: 0.22874753177165985, acc: 0.9047619104385376)
[2024-11-08 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:31][root][INFO] - Training Epoch: 5/10, step 541/574 completed (loss: 0.11791357398033142, acc: 0.9523809552192688)
[2024-11-08 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:31][root][INFO] - Training Epoch: 5/10, step 542/574 completed (loss: 0.0018725490663200617, acc: 1.0)
[2024-11-08 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:32][root][INFO] - Training Epoch: 5/10, step 543/574 completed (loss: 0.033796828240156174, acc: 0.9629629850387573)
[2024-11-08 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:32][root][INFO] - Training Epoch: 5/10, step 544/574 completed (loss: 0.006346867419779301, acc: 1.0)
[2024-11-08 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:32][root][INFO] - Training Epoch: 5/10, step 545/574 completed (loss: 0.01492752879858017, acc: 1.0)
[2024-11-08 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:33][root][INFO] - Training Epoch: 5/10, step 546/574 completed (loss: 0.0038888573180884123, acc: 1.0)
[2024-11-08 04:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:33][root][INFO] - Training Epoch: 5/10, step 547/574 completed (loss: 0.02767481468617916, acc: 1.0)
[2024-11-08 04:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:33][root][INFO] - Training Epoch: 5/10, step 548/574 completed (loss: 0.0017956692026928067, acc: 1.0)
[2024-11-08 04:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:34][root][INFO] - Training Epoch: 5/10, step 549/574 completed (loss: 0.003014043904840946, acc: 1.0)
[2024-11-08 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:34][root][INFO] - Training Epoch: 5/10, step 550/574 completed (loss: 0.0019782015588134527, acc: 1.0)
[2024-11-08 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:34][root][INFO] - Training Epoch: 5/10, step 551/574 completed (loss: 0.015200022608041763, acc: 1.0)
[2024-11-08 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:35][root][INFO] - Training Epoch: 5/10, step 552/574 completed (loss: 0.03210929036140442, acc: 0.9629629850387573)
[2024-11-08 04:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:35][root][INFO] - Training Epoch: 5/10, step 553/574 completed (loss: 0.08571655303239822, acc: 0.9523809552192688)
[2024-11-08 04:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:35][root][INFO] - Training Epoch: 5/10, step 554/574 completed (loss: 0.0023671118542551994, acc: 1.0)
[2024-11-08 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:36][root][INFO] - Training Epoch: 5/10, step 555/574 completed (loss: 0.03343779966235161, acc: 1.0)
[2024-11-08 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:36][root][INFO] - Training Epoch: 5/10, step 556/574 completed (loss: 0.009453235194087029, acc: 1.0)
[2024-11-08 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:36][root][INFO] - Training Epoch: 5/10, step 557/574 completed (loss: 0.001399387139827013, acc: 1.0)
[2024-11-08 04:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:37][root][INFO] - Training Epoch: 5/10, step 558/574 completed (loss: 0.17801986634731293, acc: 0.9642857313156128)
[2024-11-08 04:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:37][root][INFO] - Training Epoch: 5/10, step 559/574 completed (loss: 0.05095062404870987, acc: 0.9629629850387573)
[2024-11-08 04:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:38][root][INFO] - Training Epoch: 5/10, step 560/574 completed (loss: 0.011764316819608212, acc: 1.0)
[2024-11-08 04:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:38][root][INFO] - Training Epoch: 5/10, step 561/574 completed (loss: 0.025146815925836563, acc: 1.0)
[2024-11-08 04:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:38][root][INFO] - Training Epoch: 5/10, step 562/574 completed (loss: 0.14675085246562958, acc: 0.9523809552192688)
[2024-11-08 04:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:39][root][INFO] - Training Epoch: 5/10, step 563/574 completed (loss: 0.010180632583796978, acc: 1.0)
[2024-11-08 04:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:21][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2905, device='cuda:0') eval_epoch_loss=tensor(0.2550, device='cuda:0') eval_epoch_acc=tensor(0.9322, device='cuda:0')
[2024-11-08 04:04:21][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:04:21][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:04:25][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_5_step_564_loss_0.25502029061317444/model.pt
[2024-11-08 04:04:25][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:25][root][INFO] - Training Epoch: 5/10, step 564/574 completed (loss: 0.0031448574736714363, acc: 1.0)
[2024-11-08 04:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:25][root][INFO] - Training Epoch: 5/10, step 565/574 completed (loss: 0.03248602896928787, acc: 1.0)
[2024-11-08 04:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:26][root][INFO] - Training Epoch: 5/10, step 566/574 completed (loss: 0.015172825194895267, acc: 1.0)
[2024-11-08 04:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:26][root][INFO] - Training Epoch: 5/10, step 567/574 completed (loss: 0.008200136944651604, acc: 1.0)
[2024-11-08 04:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:27][root][INFO] - Training Epoch: 5/10, step 568/574 completed (loss: 0.015643060207366943, acc: 1.0)
[2024-11-08 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:27][root][INFO] - Training Epoch: 5/10, step 569/574 completed (loss: 0.0020490591414272785, acc: 1.0)
[2024-11-08 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:27][root][INFO] - Training Epoch: 5/10, step 570/574 completed (loss: 0.0021071217488497496, acc: 1.0)
[2024-11-08 04:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:28][root][INFO] - Training Epoch: 5/10, step 571/574 completed (loss: 0.11866109073162079, acc: 0.9473684430122375)
[2024-11-08 04:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:28][root][INFO] - Training Epoch: 5/10, step 572/574 completed (loss: 0.06727350503206253, acc: 0.9545454382896423)
[2024-11-08 04:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:29][root][INFO] - Training Epoch: 5/10, step 573/574 completed (loss: 0.060187242925167084, acc: 1.0)
[2024-11-08 04:04:29][slam_llm.utils.train_utils][INFO] - Epoch 5: train_perplexity=1.1151, train_epoch_loss=0.1090, epoch time 468.82175563648343s
[2024-11-08 04:04:29][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 04:04:29][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2024-11-08 04:04:29][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 04:04:29][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 4
[2024-11-08 04:04:29][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:30][root][INFO] - Training Epoch: 6/10, step 0/574 completed (loss: 0.007970944978296757, acc: 1.0)
[2024-11-08 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:31][root][INFO] - Training Epoch: 6/10, step 1/574 completed (loss: 0.0038359910249710083, acc: 1.0)
[2024-11-08 04:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:31][root][INFO] - Training Epoch: 6/10, step 2/574 completed (loss: 0.027964303269982338, acc: 0.9714285731315613)
[2024-11-08 04:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:32][root][INFO] - Training Epoch: 6/10, step 3/574 completed (loss: 0.03327062726020813, acc: 1.0)
[2024-11-08 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:32][root][INFO] - Training Epoch: 6/10, step 4/574 completed (loss: 0.026161687448620796, acc: 1.0)
[2024-11-08 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:32][root][INFO] - Training Epoch: 6/10, step 5/574 completed (loss: 0.23602598905563354, acc: 0.9473684430122375)
[2024-11-08 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:33][root][INFO] - Training Epoch: 6/10, step 6/574 completed (loss: 0.002048676600679755, acc: 1.0)
[2024-11-08 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:33][root][INFO] - Training Epoch: 6/10, step 7/574 completed (loss: 0.03152162581682205, acc: 1.0)
[2024-11-08 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:34][root][INFO] - Training Epoch: 6/10, step 8/574 completed (loss: 0.008906100876629353, acc: 1.0)
[2024-11-08 04:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:34][root][INFO] - Training Epoch: 6/10, step 9/574 completed (loss: 0.11639314889907837, acc: 0.9032257795333862)
[2024-11-08 04:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:34][root][INFO] - Training Epoch: 6/10, step 10/574 completed (loss: 0.008212070912122726, acc: 1.0)
[2024-11-08 04:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:35][root][INFO] - Training Epoch: 6/10, step 11/574 completed (loss: 0.03850855678319931, acc: 0.9599999785423279)
[2024-11-08 04:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:35][root][INFO] - Training Epoch: 6/10, step 12/574 completed (loss: 0.035626962780952454, acc: 1.0)
[2024-11-08 04:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:36][root][INFO] - Training Epoch: 6/10, step 13/574 completed (loss: 0.08354735374450684, acc: 0.9473684430122375)
[2024-11-08 04:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:36][root][INFO] - Training Epoch: 6/10, step 14/574 completed (loss: 0.005619664676487446, acc: 1.0)
[2024-11-08 04:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:37][root][INFO] - Training Epoch: 6/10, step 15/574 completed (loss: 0.0011904439888894558, acc: 1.0)
[2024-11-08 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:37][root][INFO] - Training Epoch: 6/10, step 16/574 completed (loss: 0.01248631440103054, acc: 1.0)
[2024-11-08 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:37][root][INFO] - Training Epoch: 6/10, step 17/574 completed (loss: 0.05362192541360855, acc: 0.9642857313156128)
[2024-11-08 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:38][root][INFO] - Training Epoch: 6/10, step 18/574 completed (loss: 0.17043957114219666, acc: 0.9545454382896423)
[2024-11-08 04:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:38][root][INFO] - Training Epoch: 6/10, step 19/574 completed (loss: 0.19112077355384827, acc: 0.9473684430122375)
[2024-11-08 04:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:38][root][INFO] - Training Epoch: 6/10, step 20/574 completed (loss: 0.040885258466005325, acc: 1.0)
[2024-11-08 04:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:39][root][INFO] - Training Epoch: 6/10, step 21/574 completed (loss: 0.025986291468143463, acc: 1.0)
[2024-11-08 04:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:39][root][INFO] - Training Epoch: 6/10, step 22/574 completed (loss: 0.0142044797539711, acc: 1.0)
[2024-11-08 04:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:39][root][INFO] - Training Epoch: 6/10, step 23/574 completed (loss: 0.004596991464495659, acc: 1.0)
[2024-11-08 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:40][root][INFO] - Training Epoch: 6/10, step 24/574 completed (loss: 0.021847551688551903, acc: 1.0)
[2024-11-08 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:40][root][INFO] - Training Epoch: 6/10, step 25/574 completed (loss: 0.1492632031440735, acc: 0.9047619104385376)
[2024-11-08 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:41][root][INFO] - Training Epoch: 6/10, step 26/574 completed (loss: 0.131813645362854, acc: 0.9523809552192688)
[2024-11-08 04:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:42][root][INFO] - Training Epoch: 6/10, step 27/574 completed (loss: 0.1037771925330162, acc: 0.9473684430122375)
[2024-11-08 04:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:43][root][INFO] - Training Epoch: 6/10, step 28/574 completed (loss: 0.0031322201248258352, acc: 1.0)
[2024-11-08 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:43][root][INFO] - Training Epoch: 6/10, step 29/574 completed (loss: 0.15620920062065125, acc: 0.949999988079071)
[2024-11-08 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:43][root][INFO] - Training Epoch: 6/10, step 30/574 completed (loss: 0.030727064236998558, acc: 1.0)
[2024-11-08 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:44][root][INFO] - Training Epoch: 6/10, step 31/574 completed (loss: 0.023213081061840057, acc: 1.0)
[2024-11-08 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:44][root][INFO] - Training Epoch: 6/10, step 32/574 completed (loss: 0.04960118979215622, acc: 1.0)
[2024-11-08 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:44][root][INFO] - Training Epoch: 6/10, step 33/574 completed (loss: 0.006672251503914595, acc: 1.0)
[2024-11-08 04:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:45][root][INFO] - Training Epoch: 6/10, step 34/574 completed (loss: 0.0851767435669899, acc: 0.9523809552192688)
[2024-11-08 04:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:45][root][INFO] - Training Epoch: 6/10, step 35/574 completed (loss: 0.0025216739159077406, acc: 1.0)
[2024-11-08 04:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:46][root][INFO] - Training Epoch: 6/10, step 36/574 completed (loss: 0.06257515400648117, acc: 0.949999988079071)
[2024-11-08 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:46][root][INFO] - Training Epoch: 6/10, step 37/574 completed (loss: 0.0033193316776305437, acc: 1.0)
[2024-11-08 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:46][root][INFO] - Training Epoch: 6/10, step 38/574 completed (loss: 0.013438109308481216, acc: 1.0)
[2024-11-08 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:47][root][INFO] - Training Epoch: 6/10, step 39/574 completed (loss: 0.13906723260879517, acc: 0.9642857313156128)
[2024-11-08 04:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:47][root][INFO] - Training Epoch: 6/10, step 40/574 completed (loss: 0.3906131386756897, acc: 0.9142857193946838)
[2024-11-08 04:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:48][root][INFO] - Training Epoch: 6/10, step 41/574 completed (loss: 0.015339547768235207, acc: 1.0)
[2024-11-08 04:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:48][root][INFO] - Training Epoch: 6/10, step 42/574 completed (loss: 0.10562925040721893, acc: 1.0)
[2024-11-08 04:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:49][root][INFO] - Training Epoch: 6/10, step 43/574 completed (loss: 0.19184119999408722, acc: 0.8947368264198303)
[2024-11-08 04:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:49][root][INFO] - Training Epoch: 6/10, step 44/574 completed (loss: 0.17424316704273224, acc: 0.9090909361839294)
[2024-11-08 04:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:50][root][INFO] - Training Epoch: 6/10, step 45/574 completed (loss: 0.2862655222415924, acc: 0.9047619104385376)
[2024-11-08 04:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:50][root][INFO] - Training Epoch: 6/10, step 46/574 completed (loss: 0.12768112123012543, acc: 0.9166666865348816)
[2024-11-08 04:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:50][root][INFO] - Training Epoch: 6/10, step 47/574 completed (loss: 0.0033112072851508856, acc: 1.0)
[2024-11-08 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:51][root][INFO] - Training Epoch: 6/10, step 48/574 completed (loss: 0.05617602542042732, acc: 0.9714285731315613)
[2024-11-08 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:51][root][INFO] - Training Epoch: 6/10, step 49/574 completed (loss: 0.04043591395020485, acc: 0.9599999785423279)
[2024-11-08 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:51][root][INFO] - Training Epoch: 6/10, step 50/574 completed (loss: 0.030282389372587204, acc: 1.0)
[2024-11-08 04:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:52][root][INFO] - Training Epoch: 6/10, step 51/574 completed (loss: 0.27575522661209106, acc: 0.8421052694320679)
[2024-11-08 04:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:52][root][INFO] - Training Epoch: 6/10, step 52/574 completed (loss: 0.21147224307060242, acc: 0.949999988079071)
[2024-11-08 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:53][root][INFO] - Training Epoch: 6/10, step 53/574 completed (loss: 0.14153006672859192, acc: 0.9523809552192688)
[2024-11-08 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:53][root][INFO] - Training Epoch: 6/10, step 54/574 completed (loss: 0.19948042929172516, acc: 0.9545454382896423)
[2024-11-08 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:54][root][INFO] - Training Epoch: 6/10, step 55/574 completed (loss: 0.013252356089651585, acc: 1.0)
[2024-11-08 04:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:04:58][root][INFO] - Training Epoch: 6/10, step 56/574 completed (loss: 0.2702294886112213, acc: 0.9523809552192688)
[2024-11-08 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:00][root][INFO] - Training Epoch: 6/10, step 57/574 completed (loss: 0.12478671967983246, acc: 0.9523809552192688)
[2024-11-08 04:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:00][root][INFO] - Training Epoch: 6/10, step 58/574 completed (loss: 0.016055839136242867, acc: 1.0)
[2024-11-08 04:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:01][root][INFO] - Training Epoch: 6/10, step 59/574 completed (loss: 0.06800883263349533, acc: 1.0)
[2024-11-08 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:02][root][INFO] - Training Epoch: 6/10, step 60/574 completed (loss: 0.11766081303358078, acc: 0.9473684430122375)
[2024-11-08 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:03][root][INFO] - Training Epoch: 6/10, step 61/574 completed (loss: 0.16479900479316711, acc: 0.9523809552192688)
[2024-11-08 04:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:03][root][INFO] - Training Epoch: 6/10, step 62/574 completed (loss: 0.030071739107370377, acc: 1.0)
[2024-11-08 04:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:03][root][INFO] - Training Epoch: 6/10, step 63/574 completed (loss: 0.20018520951271057, acc: 0.9642857313156128)
[2024-11-08 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:04][root][INFO] - Training Epoch: 6/10, step 64/574 completed (loss: 0.07142384350299835, acc: 0.9523809552192688)
[2024-11-08 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:04][root][INFO] - Training Epoch: 6/10, step 65/574 completed (loss: 0.14253871142864227, acc: 0.8999999761581421)
[2024-11-08 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:04][root][INFO] - Training Epoch: 6/10, step 66/574 completed (loss: 0.06641162931919098, acc: 1.0)
[2024-11-08 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:05][root][INFO] - Training Epoch: 6/10, step 67/574 completed (loss: 0.0818401500582695, acc: 1.0)
[2024-11-08 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:05][root][INFO] - Training Epoch: 6/10, step 68/574 completed (loss: 0.01429713424295187, acc: 1.0)
[2024-11-08 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:06][root][INFO] - Training Epoch: 6/10, step 69/574 completed (loss: 0.0171540305018425, acc: 1.0)
[2024-11-08 04:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:06][root][INFO] - Training Epoch: 6/10, step 70/574 completed (loss: 0.053501326590776443, acc: 1.0)
[2024-11-08 04:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:06][root][INFO] - Training Epoch: 6/10, step 71/574 completed (loss: 0.061340197920799255, acc: 0.9666666388511658)
[2024-11-08 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:07][root][INFO] - Training Epoch: 6/10, step 72/574 completed (loss: 0.048892486840486526, acc: 0.9523809552192688)
[2024-11-08 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:07][root][INFO] - Training Epoch: 6/10, step 73/574 completed (loss: 0.13181689381599426, acc: 0.9473684430122375)
[2024-11-08 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:08][root][INFO] - Training Epoch: 6/10, step 74/574 completed (loss: 0.08202295005321503, acc: 0.949999988079071)
[2024-11-08 04:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:08][root][INFO] - Training Epoch: 6/10, step 75/574 completed (loss: 0.009163505397737026, acc: 1.0)
[2024-11-08 04:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:08][root][INFO] - Training Epoch: 6/10, step 76/574 completed (loss: 0.02405300736427307, acc: 1.0)
[2024-11-08 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:09][root][INFO] - Training Epoch: 6/10, step 77/574 completed (loss: 0.0009757233783602715, acc: 1.0)
[2024-11-08 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:09][root][INFO] - Training Epoch: 6/10, step 78/574 completed (loss: 0.0011106798192486167, acc: 1.0)
[2024-11-08 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:09][root][INFO] - Training Epoch: 6/10, step 79/574 completed (loss: 0.023111047223210335, acc: 1.0)
[2024-11-08 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:10][root][INFO] - Training Epoch: 6/10, step 80/574 completed (loss: 0.006367940921336412, acc: 1.0)
[2024-11-08 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:10][root][INFO] - Training Epoch: 6/10, step 81/574 completed (loss: 0.4181922674179077, acc: 0.9523809552192688)
[2024-11-08 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:11][root][INFO] - Training Epoch: 6/10, step 82/574 completed (loss: 0.09142417460680008, acc: 0.9473684430122375)
[2024-11-08 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:11][root][INFO] - Training Epoch: 6/10, step 83/574 completed (loss: 0.007034485228359699, acc: 1.0)
[2024-11-08 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:11][root][INFO] - Training Epoch: 6/10, step 84/574 completed (loss: 0.03302643820643425, acc: 1.0)
[2024-11-08 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:12][root][INFO] - Training Epoch: 6/10, step 85/574 completed (loss: 0.004165844060480595, acc: 1.0)
[2024-11-08 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:12][root][INFO] - Training Epoch: 6/10, step 86/574 completed (loss: 0.03707396611571312, acc: 1.0)
[2024-11-08 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:13][root][INFO] - Training Epoch: 6/10, step 87/574 completed (loss: 0.14557990431785583, acc: 0.8846153616905212)
[2024-11-08 04:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:13][root][INFO] - Training Epoch: 6/10, step 88/574 completed (loss: 0.16807810962200165, acc: 0.9523809552192688)
[2024-11-08 04:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:15][root][INFO] - Training Epoch: 6/10, step 89/574 completed (loss: 0.21043498814105988, acc: 0.9473684430122375)
[2024-11-08 04:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:16][root][INFO] - Training Epoch: 6/10, step 90/574 completed (loss: 0.0641724169254303, acc: 0.9473684430122375)
[2024-11-08 04:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:17][root][INFO] - Training Epoch: 6/10, step 91/574 completed (loss: 0.05319203436374664, acc: 0.9545454382896423)
[2024-11-08 04:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:18][root][INFO] - Training Epoch: 6/10, step 92/574 completed (loss: 0.07860992848873138, acc: 1.0)
[2024-11-08 04:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:20][root][INFO] - Training Epoch: 6/10, step 93/574 completed (loss: 0.2801739275455475, acc: 0.8461538553237915)
[2024-11-08 04:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:20][root][INFO] - Training Epoch: 6/10, step 94/574 completed (loss: 0.07688753306865692, acc: 0.9642857313156128)
[2024-11-08 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:21][root][INFO] - Training Epoch: 6/10, step 95/574 completed (loss: 0.11192280054092407, acc: 0.9523809552192688)
[2024-11-08 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:21][root][INFO] - Training Epoch: 6/10, step 96/574 completed (loss: 0.04737952724099159, acc: 1.0)
[2024-11-08 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:22][root][INFO] - Training Epoch: 6/10, step 97/574 completed (loss: 0.10848303139209747, acc: 0.949999988079071)
[2024-11-08 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:22][root][INFO] - Training Epoch: 6/10, step 98/574 completed (loss: 0.14386241137981415, acc: 0.9047619104385376)
[2024-11-08 04:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:23][root][INFO] - Training Epoch: 6/10, step 99/574 completed (loss: 0.5119429230690002, acc: 0.8636363744735718)
[2024-11-08 04:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:23][root][INFO] - Training Epoch: 6/10, step 100/574 completed (loss: 0.019244704395532608, acc: 1.0)
[2024-11-08 04:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:24][root][INFO] - Training Epoch: 6/10, step 101/574 completed (loss: 0.06026981770992279, acc: 0.9599999785423279)
[2024-11-08 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:24][root][INFO] - Training Epoch: 6/10, step 102/574 completed (loss: 0.00811089389026165, acc: 1.0)
[2024-11-08 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:24][root][INFO] - Training Epoch: 6/10, step 103/574 completed (loss: 0.000499503395985812, acc: 1.0)
[2024-11-08 04:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:25][root][INFO] - Training Epoch: 6/10, step 104/574 completed (loss: 0.07273434847593307, acc: 0.9545454382896423)
[2024-11-08 04:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:25][root][INFO] - Training Epoch: 6/10, step 105/574 completed (loss: 0.008831056766211987, acc: 1.0)
[2024-11-08 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:26][root][INFO] - Training Epoch: 6/10, step 106/574 completed (loss: 0.08113127201795578, acc: 0.9545454382896423)
[2024-11-08 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:26][root][INFO] - Training Epoch: 6/10, step 107/574 completed (loss: 0.043153829872608185, acc: 0.9642857313156128)
[2024-11-08 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:27][root][INFO] - Training Epoch: 6/10, step 108/574 completed (loss: 0.0030708436388522387, acc: 1.0)
[2024-11-08 04:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:27][root][INFO] - Training Epoch: 6/10, step 109/574 completed (loss: 0.027395764365792274, acc: 1.0)
[2024-11-08 04:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:27][root][INFO] - Training Epoch: 6/10, step 110/574 completed (loss: 0.0018635060405358672, acc: 1.0)
[2024-11-08 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:28][root][INFO] - Training Epoch: 6/10, step 111/574 completed (loss: 0.029960086569190025, acc: 1.0)
[2024-11-08 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:28][root][INFO] - Training Epoch: 6/10, step 112/574 completed (loss: 0.01024863962084055, acc: 1.0)
[2024-11-08 04:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:29][root][INFO] - Training Epoch: 6/10, step 113/574 completed (loss: 0.006407297682017088, acc: 1.0)
[2024-11-08 04:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:29][root][INFO] - Training Epoch: 6/10, step 114/574 completed (loss: 0.17435093224048615, acc: 0.9545454382896423)
[2024-11-08 04:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:30][root][INFO] - Training Epoch: 6/10, step 115/574 completed (loss: 0.0028309833724051714, acc: 1.0)
[2024-11-08 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:30][root][INFO] - Training Epoch: 6/10, step 116/574 completed (loss: 0.16192474961280823, acc: 0.9599999785423279)
[2024-11-08 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:31][root][INFO] - Training Epoch: 6/10, step 117/574 completed (loss: 0.1450943946838379, acc: 0.949999988079071)
[2024-11-08 04:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:31][root][INFO] - Training Epoch: 6/10, step 118/574 completed (loss: 0.02002638392150402, acc: 1.0)
[2024-11-08 04:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:32][root][INFO] - Training Epoch: 6/10, step 119/574 completed (loss: 0.012054302729666233, acc: 1.0)
[2024-11-08 04:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:33][root][INFO] - Training Epoch: 6/10, step 120/574 completed (loss: 0.0036787993740290403, acc: 1.0)
[2024-11-08 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:33][root][INFO] - Training Epoch: 6/10, step 121/574 completed (loss: 0.01887475699186325, acc: 1.0)
[2024-11-08 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:34][root][INFO] - Training Epoch: 6/10, step 122/574 completed (loss: 0.0034972205758094788, acc: 1.0)
[2024-11-08 04:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:34][root][INFO] - Training Epoch: 6/10, step 123/574 completed (loss: 0.09802459180355072, acc: 0.9615384340286255)
[2024-11-08 04:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:35][root][INFO] - Training Epoch: 6/10, step 124/574 completed (loss: 0.11791360378265381, acc: 0.8999999761581421)
[2024-11-08 04:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:35][root][INFO] - Training Epoch: 6/10, step 125/574 completed (loss: 0.178160160779953, acc: 0.8999999761581421)
[2024-11-08 04:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:35][root][INFO] - Training Epoch: 6/10, step 126/574 completed (loss: 0.003134964033961296, acc: 1.0)
[2024-11-08 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:36][root][INFO] - Training Epoch: 6/10, step 127/574 completed (loss: 0.2605297863483429, acc: 0.9090909361839294)
[2024-11-08 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:37][root][INFO] - Training Epoch: 6/10, step 128/574 completed (loss: 0.029501548036932945, acc: 1.0)
[2024-11-08 04:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:37][root][INFO] - Training Epoch: 6/10, step 129/574 completed (loss: 0.0027218458708375692, acc: 1.0)
[2024-11-08 04:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:38][root][INFO] - Training Epoch: 6/10, step 130/574 completed (loss: 0.13008004426956177, acc: 0.9696969985961914)
[2024-11-08 04:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:38][root][INFO] - Training Epoch: 6/10, step 131/574 completed (loss: 0.12783828377723694, acc: 0.9259259104728699)
[2024-11-08 04:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:38][root][INFO] - Training Epoch: 6/10, step 132/574 completed (loss: 0.1723688393831253, acc: 0.9696969985961914)
[2024-11-08 04:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:22][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3152, device='cuda:0') eval_epoch_loss=tensor(0.2740, device='cuda:0') eval_epoch_acc=tensor(0.9278, device='cuda:0')
[2024-11-08 04:06:22][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:06:22][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:06:26][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_6_step_133_loss_0.2740265429019928/model.pt
[2024-11-08 04:06:26][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:26][root][INFO] - Training Epoch: 6/10, step 133/574 completed (loss: 0.008257107809185982, acc: 1.0)
[2024-11-08 04:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:27][root][INFO] - Training Epoch: 6/10, step 134/574 completed (loss: 0.0010016020387411118, acc: 1.0)
[2024-11-08 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:27][root][INFO] - Training Epoch: 6/10, step 135/574 completed (loss: 0.12367813289165497, acc: 0.9473684430122375)
[2024-11-08 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:27][root][INFO] - Training Epoch: 6/10, step 136/574 completed (loss: 0.38861706852912903, acc: 0.9090909361839294)
[2024-11-08 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:28][root][INFO] - Training Epoch: 6/10, step 137/574 completed (loss: 0.2786058783531189, acc: 0.949999988079071)
[2024-11-08 04:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:28][root][INFO] - Training Epoch: 6/10, step 138/574 completed (loss: 0.01083444058895111, acc: 1.0)
[2024-11-08 04:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:28][root][INFO] - Training Epoch: 6/10, step 139/574 completed (loss: 0.003170286538079381, acc: 1.0)
[2024-11-08 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:29][root][INFO] - Training Epoch: 6/10, step 140/574 completed (loss: 0.03866779804229736, acc: 1.0)
[2024-11-08 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:29][root][INFO] - Training Epoch: 6/10, step 141/574 completed (loss: 0.02923276647925377, acc: 1.0)
[2024-11-08 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:30][root][INFO] - Training Epoch: 6/10, step 142/574 completed (loss: 0.07037300616502762, acc: 1.0)
[2024-11-08 04:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:31][root][INFO] - Training Epoch: 6/10, step 143/574 completed (loss: 0.13639496266841888, acc: 0.949999988079071)
[2024-11-08 04:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:31][root][INFO] - Training Epoch: 6/10, step 144/574 completed (loss: 0.03497970849275589, acc: 1.0)
[2024-11-08 04:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:31][root][INFO] - Training Epoch: 6/10, step 145/574 completed (loss: 0.011572212912142277, acc: 1.0)
[2024-11-08 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:32][root][INFO] - Training Epoch: 6/10, step 146/574 completed (loss: 0.08227737247943878, acc: 1.0)
[2024-11-08 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:32][root][INFO] - Training Epoch: 6/10, step 147/574 completed (loss: 0.023956410586833954, acc: 1.0)
[2024-11-08 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:33][root][INFO] - Training Epoch: 6/10, step 148/574 completed (loss: 0.12554122507572174, acc: 0.9696969985961914)
[2024-11-08 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:33][root][INFO] - Training Epoch: 6/10, step 149/574 completed (loss: 0.00435003312304616, acc: 1.0)
[2024-11-08 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:34][root][INFO] - Training Epoch: 6/10, step 150/574 completed (loss: 0.11655518412590027, acc: 0.9677419066429138)
[2024-11-08 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:34][root][INFO] - Training Epoch: 6/10, step 151/574 completed (loss: 0.10394400358200073, acc: 0.949999988079071)
[2024-11-08 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:34][root][INFO] - Training Epoch: 6/10, step 152/574 completed (loss: 0.0920904278755188, acc: 0.949999988079071)
[2024-11-08 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:35][root][INFO] - Training Epoch: 6/10, step 153/574 completed (loss: 0.010512272827327251, acc: 1.0)
[2024-11-08 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:35][root][INFO] - Training Epoch: 6/10, step 154/574 completed (loss: 0.003311281092464924, acc: 1.0)
[2024-11-08 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:36][root][INFO] - Training Epoch: 6/10, step 155/574 completed (loss: 0.13019824028015137, acc: 0.949999988079071)
[2024-11-08 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:36][root][INFO] - Training Epoch: 6/10, step 156/574 completed (loss: 0.3844524621963501, acc: 0.8636363744735718)
[2024-11-08 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:36][root][INFO] - Training Epoch: 6/10, step 157/574 completed (loss: 0.2849563956260681, acc: 0.9090909361839294)
[2024-11-08 04:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:39][root][INFO] - Training Epoch: 6/10, step 158/574 completed (loss: 0.1638876050710678, acc: 0.949999988079071)
[2024-11-08 04:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:39][root][INFO] - Training Epoch: 6/10, step 159/574 completed (loss: 0.09360170364379883, acc: 0.949999988079071)
[2024-11-08 04:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:40][root][INFO] - Training Epoch: 6/10, step 160/574 completed (loss: 0.13690176606178284, acc: 0.9473684430122375)
[2024-11-08 04:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:41][root][INFO] - Training Epoch: 6/10, step 161/574 completed (loss: 0.17170573770999908, acc: 0.9545454382896423)
[2024-11-08 04:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:42][root][INFO] - Training Epoch: 6/10, step 162/574 completed (loss: 0.23535342514514923, acc: 0.949999988079071)
[2024-11-08 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:42][root][INFO] - Training Epoch: 6/10, step 163/574 completed (loss: 0.15847550332546234, acc: 0.9599999785423279)
[2024-11-08 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:42][root][INFO] - Training Epoch: 6/10, step 164/574 completed (loss: 0.06666427105665207, acc: 0.9629629850387573)
[2024-11-08 04:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:43][root][INFO] - Training Epoch: 6/10, step 165/574 completed (loss: 0.15260885655879974, acc: 0.9444444179534912)
[2024-11-08 04:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:43][root][INFO] - Training Epoch: 6/10, step 166/574 completed (loss: 0.005178662482649088, acc: 1.0)
[2024-11-08 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:44][root][INFO] - Training Epoch: 6/10, step 167/574 completed (loss: 0.12258763611316681, acc: 0.9047619104385376)
[2024-11-08 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:44][root][INFO] - Training Epoch: 6/10, step 168/574 completed (loss: 0.00393730727955699, acc: 1.0)
[2024-11-08 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:45][root][INFO] - Training Epoch: 6/10, step 169/574 completed (loss: 0.06771888583898544, acc: 0.9545454382896423)
[2024-11-08 04:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:47][root][INFO] - Training Epoch: 6/10, step 170/574 completed (loss: 0.10769964754581451, acc: 0.949999988079071)
[2024-11-08 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:47][root][INFO] - Training Epoch: 6/10, step 171/574 completed (loss: 0.038875844329595566, acc: 1.0)
[2024-11-08 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:47][root][INFO] - Training Epoch: 6/10, step 172/574 completed (loss: 0.061154428869485855, acc: 0.9696969985961914)
[2024-11-08 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:48][root][INFO] - Training Epoch: 6/10, step 173/574 completed (loss: 0.05691159889101982, acc: 0.9666666388511658)
[2024-11-08 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:49][root][INFO] - Training Epoch: 6/10, step 174/574 completed (loss: 0.09679047018289566, acc: 0.9523809552192688)
[2024-11-08 04:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:49][root][INFO] - Training Epoch: 6/10, step 175/574 completed (loss: 0.05801057815551758, acc: 1.0)
[2024-11-08 04:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:50][root][INFO] - Training Epoch: 6/10, step 176/574 completed (loss: 0.008531168103218079, acc: 1.0)
[2024-11-08 04:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:51][root][INFO] - Training Epoch: 6/10, step 177/574 completed (loss: 0.1279308944940567, acc: 0.9545454382896423)
[2024-11-08 04:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:52][root][INFO] - Training Epoch: 6/10, step 178/574 completed (loss: 0.1685231775045395, acc: 0.9473684430122375)
[2024-11-08 04:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:52][root][INFO] - Training Epoch: 6/10, step 179/574 completed (loss: 0.001433818368241191, acc: 1.0)
[2024-11-08 04:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:53][root][INFO] - Training Epoch: 6/10, step 180/574 completed (loss: 0.013876743614673615, acc: 1.0)
[2024-11-08 04:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:53][root][INFO] - Training Epoch: 6/10, step 181/574 completed (loss: 0.0006077197031117976, acc: 1.0)
[2024-11-08 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:54][root][INFO] - Training Epoch: 6/10, step 182/574 completed (loss: 0.04778784140944481, acc: 0.9714285731315613)
[2024-11-08 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:54][root][INFO] - Training Epoch: 6/10, step 183/574 completed (loss: 0.003341358620673418, acc: 1.0)
[2024-11-08 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:55][root][INFO] - Training Epoch: 6/10, step 184/574 completed (loss: 0.10566234588623047, acc: 0.9523809552192688)
[2024-11-08 04:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:55][root][INFO] - Training Epoch: 6/10, step 185/574 completed (loss: 0.003908908925950527, acc: 1.0)
[2024-11-08 04:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:56][root][INFO] - Training Epoch: 6/10, step 186/574 completed (loss: 0.3764283359050751, acc: 0.8999999761581421)
[2024-11-08 04:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:56][root][INFO] - Training Epoch: 6/10, step 187/574 completed (loss: 0.053042348474264145, acc: 1.0)
[2024-11-08 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:57][root][INFO] - Training Epoch: 6/10, step 188/574 completed (loss: 0.0022910917177796364, acc: 1.0)
[2024-11-08 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:57][root][INFO] - Training Epoch: 6/10, step 189/574 completed (loss: 0.12991154193878174, acc: 0.9200000166893005)
[2024-11-08 04:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:06:58][root][INFO] - Training Epoch: 6/10, step 190/574 completed (loss: 0.2018168419599533, acc: 0.949999988079071)
[2024-11-08 04:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:00][root][INFO] - Training Epoch: 6/10, step 191/574 completed (loss: 0.2814137935638428, acc: 0.9047619104385376)
[2024-11-08 04:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:01][root][INFO] - Training Epoch: 6/10, step 192/574 completed (loss: 0.4454052150249481, acc: 0.8421052694320679)
[2024-11-08 04:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:02][root][INFO] - Training Epoch: 6/10, step 193/574 completed (loss: 0.059649016708135605, acc: 1.0)
[2024-11-08 04:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:04][root][INFO] - Training Epoch: 6/10, step 194/574 completed (loss: 0.25585559010505676, acc: 0.9523809552192688)
[2024-11-08 04:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:06][root][INFO] - Training Epoch: 6/10, step 195/574 completed (loss: 0.0842900350689888, acc: 0.9583333134651184)
[2024-11-08 04:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:06][root][INFO] - Training Epoch: 6/10, step 196/574 completed (loss: 0.004514288157224655, acc: 1.0)
[2024-11-08 04:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:06][root][INFO] - Training Epoch: 6/10, step 197/574 completed (loss: 0.023450564593076706, acc: 1.0)
[2024-11-08 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:07][root][INFO] - Training Epoch: 6/10, step 198/574 completed (loss: 0.18053045868873596, acc: 0.949999988079071)
[2024-11-08 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:07][root][INFO] - Training Epoch: 6/10, step 199/574 completed (loss: 0.00849960744380951, acc: 1.0)
[2024-11-08 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:08][root][INFO] - Training Epoch: 6/10, step 200/574 completed (loss: 0.0053435759618878365, acc: 1.0)
[2024-11-08 04:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:08][root][INFO] - Training Epoch: 6/10, step 201/574 completed (loss: 0.0878734216094017, acc: 0.9545454382896423)
[2024-11-08 04:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:09][root][INFO] - Training Epoch: 6/10, step 202/574 completed (loss: 0.025859598070383072, acc: 1.0)
[2024-11-08 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:09][root][INFO] - Training Epoch: 6/10, step 203/574 completed (loss: 0.021780192852020264, acc: 1.0)
[2024-11-08 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:09][root][INFO] - Training Epoch: 6/10, step 204/574 completed (loss: 0.010129092261195183, acc: 1.0)
[2024-11-08 04:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:10][root][INFO] - Training Epoch: 6/10, step 205/574 completed (loss: 0.07365763932466507, acc: 0.9523809552192688)
[2024-11-08 04:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:10][root][INFO] - Training Epoch: 6/10, step 206/574 completed (loss: 0.06516324728727341, acc: 1.0)
[2024-11-08 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:11][root][INFO] - Training Epoch: 6/10, step 207/574 completed (loss: 0.03343229368329048, acc: 1.0)
[2024-11-08 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:11][root][INFO] - Training Epoch: 6/10, step 208/574 completed (loss: 0.2595042586326599, acc: 0.9090909361839294)
[2024-11-08 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:12][root][INFO] - Training Epoch: 6/10, step 209/574 completed (loss: 0.1348518580198288, acc: 0.9523809552192688)
[2024-11-08 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:12][root][INFO] - Training Epoch: 6/10, step 210/574 completed (loss: 0.03721429035067558, acc: 0.9583333134651184)
[2024-11-08 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:13][root][INFO] - Training Epoch: 6/10, step 211/574 completed (loss: 0.1529555320739746, acc: 0.9354838728904724)
[2024-11-08 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:13][root][INFO] - Training Epoch: 6/10, step 212/574 completed (loss: 0.02666471339762211, acc: 1.0)
[2024-11-08 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:14][root][INFO] - Training Epoch: 6/10, step 213/574 completed (loss: 0.29326131939888, acc: 0.9615384340286255)
[2024-11-08 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:15][root][INFO] - Training Epoch: 6/10, step 214/574 completed (loss: 0.014417629688978195, acc: 1.0)
[2024-11-08 04:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:15][root][INFO] - Training Epoch: 6/10, step 215/574 completed (loss: 0.04170958697795868, acc: 1.0)
[2024-11-08 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:16][root][INFO] - Training Epoch: 6/10, step 216/574 completed (loss: 0.005239839665591717, acc: 1.0)
[2024-11-08 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:17][root][INFO] - Training Epoch: 6/10, step 217/574 completed (loss: 0.0350002646446228, acc: 0.9545454382896423)
[2024-11-08 04:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:17][root][INFO] - Training Epoch: 6/10, step 218/574 completed (loss: 0.1148190125823021, acc: 0.9523809552192688)
[2024-11-08 04:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:18][root][INFO] - Training Epoch: 6/10, step 219/574 completed (loss: 0.02730361558496952, acc: 1.0)
[2024-11-08 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:18][root][INFO] - Training Epoch: 6/10, step 220/574 completed (loss: 0.009178750216960907, acc: 1.0)
[2024-11-08 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:18][root][INFO] - Training Epoch: 6/10, step 221/574 completed (loss: 0.002459367038682103, acc: 1.0)
[2024-11-08 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:19][root][INFO] - Training Epoch: 6/10, step 222/574 completed (loss: 0.165262371301651, acc: 0.9615384340286255)
[2024-11-08 04:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:20][root][INFO] - Training Epoch: 6/10, step 223/574 completed (loss: 0.0880064144730568, acc: 0.9523809552192688)
[2024-11-08 04:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:21][root][INFO] - Training Epoch: 6/10, step 224/574 completed (loss: 0.1441623717546463, acc: 0.9473684430122375)
[2024-11-08 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:21][root][INFO] - Training Epoch: 6/10, step 225/574 completed (loss: 0.0050947461277246475, acc: 1.0)
[2024-11-08 04:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:22][root][INFO] - Training Epoch: 6/10, step 226/574 completed (loss: 0.06171711906790733, acc: 0.9545454382896423)
[2024-11-08 04:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:22][root][INFO] - Training Epoch: 6/10, step 227/574 completed (loss: 0.18199273943901062, acc: 0.9523809552192688)
[2024-11-08 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:23][root][INFO] - Training Epoch: 6/10, step 228/574 completed (loss: 0.2109258770942688, acc: 0.875)
[2024-11-08 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:23][root][INFO] - Training Epoch: 6/10, step 229/574 completed (loss: 0.25978419184684753, acc: 0.9130434989929199)
[2024-11-08 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:24][root][INFO] - Training Epoch: 6/10, step 230/574 completed (loss: 0.1059313416481018, acc: 0.9523809552192688)
[2024-11-08 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:24][root][INFO] - Training Epoch: 6/10, step 231/574 completed (loss: 0.18581391870975494, acc: 0.9473684430122375)
[2024-11-08 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:25][root][INFO] - Training Epoch: 6/10, step 232/574 completed (loss: 0.060901910066604614, acc: 0.9473684430122375)
[2024-11-08 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:25][root][INFO] - Training Epoch: 6/10, step 233/574 completed (loss: 0.21225929260253906, acc: 0.9090909361839294)
[2024-11-08 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:26][root][INFO] - Training Epoch: 6/10, step 234/574 completed (loss: 0.19353999197483063, acc: 0.9545454382896423)
[2024-11-08 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:27][root][INFO] - Training Epoch: 6/10, step 235/574 completed (loss: 0.025985000655055046, acc: 1.0)
[2024-11-08 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:27][root][INFO] - Training Epoch: 6/10, step 236/574 completed (loss: 0.06471502035856247, acc: 0.9629629850387573)
[2024-11-08 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:27][root][INFO] - Training Epoch: 6/10, step 237/574 completed (loss: 0.30174556374549866, acc: 0.9090909361839294)
[2024-11-08 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:28][root][INFO] - Training Epoch: 6/10, step 238/574 completed (loss: 0.07866105437278748, acc: 1.0)
[2024-11-08 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:28][root][INFO] - Training Epoch: 6/10, step 239/574 completed (loss: 0.012480180710554123, acc: 1.0)
[2024-11-08 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:29][root][INFO] - Training Epoch: 6/10, step 240/574 completed (loss: 0.09379477798938751, acc: 1.0)
[2024-11-08 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:29][root][INFO] - Training Epoch: 6/10, step 241/574 completed (loss: 0.08690761774778366, acc: 0.9545454382896423)
[2024-11-08 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:30][root][INFO] - Training Epoch: 6/10, step 242/574 completed (loss: 0.08150428533554077, acc: 0.9473684430122375)
[2024-11-08 04:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:31][root][INFO] - Training Epoch: 6/10, step 243/574 completed (loss: 0.24795569479465485, acc: 0.9090909361839294)
[2024-11-08 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:31][root][INFO] - Training Epoch: 6/10, step 244/574 completed (loss: 0.004361755680292845, acc: 1.0)
[2024-11-08 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:31][root][INFO] - Training Epoch: 6/10, step 245/574 completed (loss: 0.17182184755802155, acc: 0.9333333373069763)
[2024-11-08 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:32][root][INFO] - Training Epoch: 6/10, step 246/574 completed (loss: 0.1065644770860672, acc: 0.9428571462631226)
[2024-11-08 04:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:32][root][INFO] - Training Epoch: 6/10, step 247/574 completed (loss: 0.013258971273899078, acc: 1.0)
[2024-11-08 04:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:32][root][INFO] - Training Epoch: 6/10, step 248/574 completed (loss: 0.2492820918560028, acc: 0.9047619104385376)
[2024-11-08 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:33][root][INFO] - Training Epoch: 6/10, step 249/574 completed (loss: 0.0833268016576767, acc: 0.9473684430122375)
[2024-11-08 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:33][root][INFO] - Training Epoch: 6/10, step 250/574 completed (loss: 0.16259071230888367, acc: 0.949999988079071)
[2024-11-08 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:34][root][INFO] - Training Epoch: 6/10, step 251/574 completed (loss: 0.08963501453399658, acc: 0.9047619104385376)
[2024-11-08 04:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:34][root][INFO] - Training Epoch: 6/10, step 252/574 completed (loss: 0.0008890565950423479, acc: 1.0)
[2024-11-08 04:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:34][root][INFO] - Training Epoch: 6/10, step 253/574 completed (loss: 0.002600299660116434, acc: 1.0)
[2024-11-08 04:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:35][root][INFO] - Training Epoch: 6/10, step 254/574 completed (loss: 0.007947493344545364, acc: 1.0)
[2024-11-08 04:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:35][root][INFO] - Training Epoch: 6/10, step 255/574 completed (loss: 0.10762374103069305, acc: 0.9714285731315613)
[2024-11-08 04:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:35][root][INFO] - Training Epoch: 6/10, step 256/574 completed (loss: 0.003086282406002283, acc: 1.0)
[2024-11-08 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:36][root][INFO] - Training Epoch: 6/10, step 257/574 completed (loss: 0.08277268707752228, acc: 0.9523809552192688)
[2024-11-08 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:36][root][INFO] - Training Epoch: 6/10, step 258/574 completed (loss: 0.028126845136284828, acc: 1.0)
[2024-11-08 04:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:37][root][INFO] - Training Epoch: 6/10, step 259/574 completed (loss: 0.08390498161315918, acc: 0.9545454382896423)
[2024-11-08 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:38][root][INFO] - Training Epoch: 6/10, step 260/574 completed (loss: 0.07967756688594818, acc: 0.9473684430122375)
[2024-11-08 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:38][root][INFO] - Training Epoch: 6/10, step 261/574 completed (loss: 0.12100109457969666, acc: 0.9090909361839294)
[2024-11-08 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:39][root][INFO] - Training Epoch: 6/10, step 262/574 completed (loss: 0.15945735573768616, acc: 0.9411764740943909)
[2024-11-08 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:39][root][INFO] - Training Epoch: 6/10, step 263/574 completed (loss: 0.03028586320579052, acc: 1.0)
[2024-11-08 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:40][root][INFO] - Training Epoch: 6/10, step 264/574 completed (loss: 0.023431207984685898, acc: 1.0)
[2024-11-08 04:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:41][root][INFO] - Training Epoch: 6/10, step 265/574 completed (loss: 0.05058044195175171, acc: 1.0)
[2024-11-08 04:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:41][root][INFO] - Training Epoch: 6/10, step 266/574 completed (loss: 0.01102647464722395, acc: 1.0)
[2024-11-08 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:42][root][INFO] - Training Epoch: 6/10, step 267/574 completed (loss: 0.051811058074235916, acc: 1.0)
[2024-11-08 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:43][root][INFO] - Training Epoch: 6/10, step 268/574 completed (loss: 0.03200793266296387, acc: 1.0)
[2024-11-08 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:43][root][INFO] - Training Epoch: 6/10, step 269/574 completed (loss: 0.024794183671474457, acc: 1.0)
[2024-11-08 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:43][root][INFO] - Training Epoch: 6/10, step 270/574 completed (loss: 0.08146549016237259, acc: 0.9230769276618958)
[2024-11-08 04:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:44][root][INFO] - Training Epoch: 6/10, step 271/574 completed (loss: 0.004296142607927322, acc: 1.0)
[2024-11-08 04:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:44][root][INFO] - Training Epoch: 6/10, step 272/574 completed (loss: 0.025582781061530113, acc: 1.0)
[2024-11-08 04:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:45][root][INFO] - Training Epoch: 6/10, step 273/574 completed (loss: 0.08290509879589081, acc: 0.9473684430122375)
[2024-11-08 04:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:45][root][INFO] - Training Epoch: 6/10, step 274/574 completed (loss: 0.017552873119711876, acc: 1.0)
[2024-11-08 04:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:46][root][INFO] - Training Epoch: 6/10, step 275/574 completed (loss: 0.06359285116195679, acc: 0.9523809552192688)
[2024-11-08 04:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:28][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4135, device='cuda:0') eval_epoch_loss=tensor(0.3461, device='cuda:0') eval_epoch_acc=tensor(0.9200, device='cuda:0')
[2024-11-08 04:08:28][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:08:28][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:08:31][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_6_step_276_loss_0.3460935056209564/model.pt
[2024-11-08 04:08:31][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:32][root][INFO] - Training Epoch: 6/10, step 276/574 completed (loss: 0.001173037220723927, acc: 1.0)
[2024-11-08 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:32][root][INFO] - Training Epoch: 6/10, step 277/574 completed (loss: 0.06743811070919037, acc: 0.9629629850387573)
[2024-11-08 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:33][root][INFO] - Training Epoch: 6/10, step 278/574 completed (loss: 0.0451052263379097, acc: 0.9523809552192688)
[2024-11-08 04:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:33][root][INFO] - Training Epoch: 6/10, step 279/574 completed (loss: 0.0003373814979568124, acc: 1.0)
[2024-11-08 04:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:34][root][INFO] - Training Epoch: 6/10, step 280/574 completed (loss: 0.03900044411420822, acc: 1.0)
[2024-11-08 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:34][root][INFO] - Training Epoch: 6/10, step 281/574 completed (loss: 0.1267051100730896, acc: 0.9545454382896423)
[2024-11-08 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:34][root][INFO] - Training Epoch: 6/10, step 282/574 completed (loss: 0.002945356070995331, acc: 1.0)
[2024-11-08 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:35][root][INFO] - Training Epoch: 6/10, step 283/574 completed (loss: 0.18964898586273193, acc: 0.931034505367279)
[2024-11-08 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:35][root][INFO] - Training Epoch: 6/10, step 284/574 completed (loss: 0.250759482383728, acc: 0.9333333373069763)
[2024-11-08 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:36][root][INFO] - Training Epoch: 6/10, step 285/574 completed (loss: 0.23402167856693268, acc: 0.931034505367279)
[2024-11-08 04:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:36][root][INFO] - Training Epoch: 6/10, step 286/574 completed (loss: 0.09328512102365494, acc: 0.9523809552192688)
[2024-11-08 04:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:36][root][INFO] - Training Epoch: 6/10, step 287/574 completed (loss: 0.12318319827318192, acc: 0.9473684430122375)
[2024-11-08 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:37][root][INFO] - Training Epoch: 6/10, step 288/574 completed (loss: 0.15589337050914764, acc: 0.9473684430122375)
[2024-11-08 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:37][root][INFO] - Training Epoch: 6/10, step 289/574 completed (loss: 0.01662112958729267, acc: 1.0)
[2024-11-08 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:38][root][INFO] - Training Epoch: 6/10, step 290/574 completed (loss: 0.011840328574180603, acc: 1.0)
[2024-11-08 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:38][root][INFO] - Training Epoch: 6/10, step 291/574 completed (loss: 0.011391584761440754, acc: 1.0)
[2024-11-08 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:39][root][INFO] - Training Epoch: 6/10, step 292/574 completed (loss: 0.07262644916772842, acc: 0.9666666388511658)
[2024-11-08 04:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:39][root][INFO] - Training Epoch: 6/10, step 293/574 completed (loss: 0.12384265661239624, acc: 0.9523809552192688)
[2024-11-08 04:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:40][root][INFO] - Training Epoch: 6/10, step 294/574 completed (loss: 0.02685694396495819, acc: 1.0)
[2024-11-08 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:41][root][INFO] - Training Epoch: 6/10, step 295/574 completed (loss: 0.21550579369068146, acc: 0.9473684430122375)
[2024-11-08 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:41][root][INFO] - Training Epoch: 6/10, step 296/574 completed (loss: 0.3113902807235718, acc: 0.9090909361839294)
[2024-11-08 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:41][root][INFO] - Training Epoch: 6/10, step 297/574 completed (loss: 0.24764949083328247, acc: 0.9047619104385376)
[2024-11-08 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:42][root][INFO] - Training Epoch: 6/10, step 298/574 completed (loss: 0.03574179857969284, acc: 1.0)
[2024-11-08 04:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:42][root][INFO] - Training Epoch: 6/10, step 299/574 completed (loss: 0.03548727557063103, acc: 1.0)
[2024-11-08 04:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:43][root][INFO] - Training Epoch: 6/10, step 300/574 completed (loss: 0.033080533146858215, acc: 1.0)
[2024-11-08 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:43][root][INFO] - Training Epoch: 6/10, step 301/574 completed (loss: 0.01930704526603222, acc: 1.0)
[2024-11-08 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:43][root][INFO] - Training Epoch: 6/10, step 302/574 completed (loss: 0.07151026278734207, acc: 0.9523809552192688)
[2024-11-08 04:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:44][root][INFO] - Training Epoch: 6/10, step 303/574 completed (loss: 0.013280338607728481, acc: 1.0)
[2024-11-08 04:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:44][root][INFO] - Training Epoch: 6/10, step 304/574 completed (loss: 0.19390714168548584, acc: 0.9473684430122375)
[2024-11-08 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:45][root][INFO] - Training Epoch: 6/10, step 305/574 completed (loss: 0.06078197807073593, acc: 1.0)
[2024-11-08 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:45][root][INFO] - Training Epoch: 6/10, step 306/574 completed (loss: 0.13503339886665344, acc: 0.9545454382896423)
[2024-11-08 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:45][root][INFO] - Training Epoch: 6/10, step 307/574 completed (loss: 0.015043786726891994, acc: 1.0)
[2024-11-08 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:46][root][INFO] - Training Epoch: 6/10, step 308/574 completed (loss: 0.009630138985812664, acc: 1.0)
[2024-11-08 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:46][root][INFO] - Training Epoch: 6/10, step 309/574 completed (loss: 0.05948849022388458, acc: 1.0)
[2024-11-08 04:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:47][root][INFO] - Training Epoch: 6/10, step 310/574 completed (loss: 0.14622551202774048, acc: 0.9473684430122375)
[2024-11-08 04:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:47][root][INFO] - Training Epoch: 6/10, step 311/574 completed (loss: 0.04888931289315224, acc: 1.0)
[2024-11-08 04:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:48][root][INFO] - Training Epoch: 6/10, step 312/574 completed (loss: 0.12862202525138855, acc: 0.9545454382896423)
[2024-11-08 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:48][root][INFO] - Training Epoch: 6/10, step 313/574 completed (loss: 0.06542067974805832, acc: 0.9642857313156128)
[2024-11-08 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:48][root][INFO] - Training Epoch: 6/10, step 314/574 completed (loss: 0.02589975669980049, acc: 1.0)
[2024-11-08 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:49][root][INFO] - Training Epoch: 6/10, step 315/574 completed (loss: 0.16833141446113586, acc: 0.9428571462631226)
[2024-11-08 04:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:49][root][INFO] - Training Epoch: 6/10, step 316/574 completed (loss: 0.09771515429019928, acc: 0.9615384340286255)
[2024-11-08 04:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:50][root][INFO] - Training Epoch: 6/10, step 317/574 completed (loss: 0.015688005834817886, acc: 1.0)
[2024-11-08 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:50][root][INFO] - Training Epoch: 6/10, step 318/574 completed (loss: 0.03207062557339668, acc: 1.0)
[2024-11-08 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:50][root][INFO] - Training Epoch: 6/10, step 319/574 completed (loss: 0.0003323711862321943, acc: 1.0)
[2024-11-08 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:51][root][INFO] - Training Epoch: 6/10, step 320/574 completed (loss: 0.07977642118930817, acc: 0.9523809552192688)
[2024-11-08 04:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:51][root][INFO] - Training Epoch: 6/10, step 321/574 completed (loss: 0.004049146547913551, acc: 1.0)
[2024-11-08 04:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:52][root][INFO] - Training Epoch: 6/10, step 322/574 completed (loss: 0.4332648515701294, acc: 0.8260869383811951)
[2024-11-08 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:52][root][INFO] - Training Epoch: 6/10, step 323/574 completed (loss: 0.41749218106269836, acc: 0.9047619104385376)
[2024-11-08 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:52][root][INFO] - Training Epoch: 6/10, step 324/574 completed (loss: 0.3431051969528198, acc: 0.7894737124443054)
[2024-11-08 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:53][root][INFO] - Training Epoch: 6/10, step 325/574 completed (loss: 0.35076960921287537, acc: 0.9090909361839294)
[2024-11-08 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:53][root][INFO] - Training Epoch: 6/10, step 326/574 completed (loss: 0.33798378705978394, acc: 0.8947368264198303)
[2024-11-08 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:54][root][INFO] - Training Epoch: 6/10, step 327/574 completed (loss: 0.25546297430992126, acc: 0.875)
[2024-11-08 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:54][root][INFO] - Training Epoch: 6/10, step 328/574 completed (loss: 0.020308876410126686, acc: 1.0)
[2024-11-08 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:54][root][INFO] - Training Epoch: 6/10, step 329/574 completed (loss: 0.11699080467224121, acc: 0.9629629850387573)
[2024-11-08 04:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:55][root][INFO] - Training Epoch: 6/10, step 330/574 completed (loss: 0.1308022439479828, acc: 0.9523809552192688)
[2024-11-08 04:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:55][root][INFO] - Training Epoch: 6/10, step 331/574 completed (loss: 0.06260453164577484, acc: 1.0)
[2024-11-08 04:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:56][root][INFO] - Training Epoch: 6/10, step 332/574 completed (loss: 0.22328868508338928, acc: 0.9090909361839294)
[2024-11-08 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:56][root][INFO] - Training Epoch: 6/10, step 333/574 completed (loss: 0.2737148702144623, acc: 0.8947368264198303)
[2024-11-08 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:57][root][INFO] - Training Epoch: 6/10, step 334/574 completed (loss: 0.015225685201585293, acc: 1.0)
[2024-11-08 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:57][root][INFO] - Training Epoch: 6/10, step 335/574 completed (loss: 0.18481500446796417, acc: 0.9615384340286255)
[2024-11-08 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:58][root][INFO] - Training Epoch: 6/10, step 336/574 completed (loss: 0.3215887248516083, acc: 0.875)
[2024-11-08 04:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:58][root][INFO] - Training Epoch: 6/10, step 337/574 completed (loss: 0.3037380278110504, acc: 0.8095238208770752)
[2024-11-08 04:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:58][root][INFO] - Training Epoch: 6/10, step 338/574 completed (loss: 0.084029920399189, acc: 1.0)
[2024-11-08 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:59][root][INFO] - Training Epoch: 6/10, step 339/574 completed (loss: 0.12799346446990967, acc: 0.9545454382896423)
[2024-11-08 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:08:59][root][INFO] - Training Epoch: 6/10, step 340/574 completed (loss: 0.02611459046602249, acc: 1.0)
[2024-11-08 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:00][root][INFO] - Training Epoch: 6/10, step 341/574 completed (loss: 0.029463954269886017, acc: 1.0)
[2024-11-08 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:00][root][INFO] - Training Epoch: 6/10, step 342/574 completed (loss: 0.05131075531244278, acc: 0.949999988079071)
[2024-11-08 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:00][root][INFO] - Training Epoch: 6/10, step 343/574 completed (loss: 0.45941370725631714, acc: 0.8500000238418579)
[2024-11-08 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:01][root][INFO] - Training Epoch: 6/10, step 344/574 completed (loss: 0.05441473051905632, acc: 0.9473684430122375)
[2024-11-08 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:01][root][INFO] - Training Epoch: 6/10, step 345/574 completed (loss: 0.003826847765594721, acc: 1.0)
[2024-11-08 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:02][root][INFO] - Training Epoch: 6/10, step 346/574 completed (loss: 0.08293571323156357, acc: 0.949999988079071)
[2024-11-08 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:02][root][INFO] - Training Epoch: 6/10, step 347/574 completed (loss: 0.03453619033098221, acc: 1.0)
[2024-11-08 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:03][root][INFO] - Training Epoch: 6/10, step 348/574 completed (loss: 0.02834499441087246, acc: 1.0)
[2024-11-08 04:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:03][root][INFO] - Training Epoch: 6/10, step 349/574 completed (loss: 0.164988175034523, acc: 0.949999988079071)
[2024-11-08 04:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:03][root][INFO] - Training Epoch: 6/10, step 350/574 completed (loss: 0.20425593852996826, acc: 0.9473684430122375)
[2024-11-08 04:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:04][root][INFO] - Training Epoch: 6/10, step 351/574 completed (loss: 0.13387982547283173, acc: 0.9090909361839294)
[2024-11-08 04:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:04][root][INFO] - Training Epoch: 6/10, step 352/574 completed (loss: 0.222032830119133, acc: 0.8999999761581421)
[2024-11-08 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:05][root][INFO] - Training Epoch: 6/10, step 353/574 completed (loss: 0.12978851795196533, acc: 0.9599999785423279)
[2024-11-08 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:05][root][INFO] - Training Epoch: 6/10, step 354/574 completed (loss: 0.5036185383796692, acc: 0.9200000166893005)
[2024-11-08 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:06][root][INFO] - Training Epoch: 6/10, step 355/574 completed (loss: 0.04964916408061981, acc: 1.0)
[2024-11-08 04:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:06][root][INFO] - Training Epoch: 6/10, step 356/574 completed (loss: 0.235317200422287, acc: 0.8500000238418579)
[2024-11-08 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:07][root][INFO] - Training Epoch: 6/10, step 357/574 completed (loss: 0.018722891807556152, acc: 1.0)
[2024-11-08 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:07][root][INFO] - Training Epoch: 6/10, step 358/574 completed (loss: 0.20803876221179962, acc: 0.9090909361839294)
[2024-11-08 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:08][root][INFO] - Training Epoch: 6/10, step 359/574 completed (loss: 0.038957078009843826, acc: 1.0)
[2024-11-08 04:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:08][root][INFO] - Training Epoch: 6/10, step 360/574 completed (loss: 0.09333401173353195, acc: 0.9655172228813171)
[2024-11-08 04:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:08][root][INFO] - Training Epoch: 6/10, step 361/574 completed (loss: 0.3105922341346741, acc: 0.8857142925262451)
[2024-11-08 04:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:09][root][INFO] - Training Epoch: 6/10, step 362/574 completed (loss: 0.09078299254179001, acc: 0.9545454382896423)
[2024-11-08 04:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:09][root][INFO] - Training Epoch: 6/10, step 363/574 completed (loss: 0.051554907113313675, acc: 0.9473684430122375)
[2024-11-08 04:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:10][root][INFO] - Training Epoch: 6/10, step 364/574 completed (loss: 0.08062585443258286, acc: 0.9523809552192688)
[2024-11-08 04:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:10][root][INFO] - Training Epoch: 6/10, step 365/574 completed (loss: 0.03619922325015068, acc: 1.0)
[2024-11-08 04:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:10][root][INFO] - Training Epoch: 6/10, step 366/574 completed (loss: 0.05589869245886803, acc: 0.9545454382896423)
[2024-11-08 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:11][root][INFO] - Training Epoch: 6/10, step 367/574 completed (loss: 0.11313113570213318, acc: 0.9629629850387573)
[2024-11-08 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:11][root][INFO] - Training Epoch: 6/10, step 368/574 completed (loss: 0.05300498008728027, acc: 0.96875)
[2024-11-08 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:12][root][INFO] - Training Epoch: 6/10, step 369/574 completed (loss: 0.025264935567975044, acc: 1.0)
[2024-11-08 04:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:12][root][INFO] - Training Epoch: 6/10, step 370/574 completed (loss: 0.18104936182498932, acc: 0.9047619104385376)
[2024-11-08 04:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:14][root][INFO] - Training Epoch: 6/10, step 371/574 completed (loss: 0.3529333174228668, acc: 0.8947368264198303)
[2024-11-08 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:14][root][INFO] - Training Epoch: 6/10, step 372/574 completed (loss: 0.1557687520980835, acc: 0.9545454382896423)
[2024-11-08 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:15][root][INFO] - Training Epoch: 6/10, step 373/574 completed (loss: 0.020247934386134148, acc: 1.0)
[2024-11-08 04:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:15][root][INFO] - Training Epoch: 6/10, step 374/574 completed (loss: 0.21654067933559418, acc: 0.8636363744735718)
[2024-11-08 04:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:15][root][INFO] - Training Epoch: 6/10, step 375/574 completed (loss: 0.09233470261096954, acc: 0.9655172228813171)
[2024-11-08 04:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:16][root][INFO] - Training Epoch: 6/10, step 376/574 completed (loss: 0.03443199023604393, acc: 1.0)
[2024-11-08 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:16][root][INFO] - Training Epoch: 6/10, step 377/574 completed (loss: 0.046836357563734055, acc: 1.0)
[2024-11-08 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:16][root][INFO] - Training Epoch: 6/10, step 378/574 completed (loss: 0.05129304900765419, acc: 1.0)
[2024-11-08 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:17][root][INFO] - Training Epoch: 6/10, step 379/574 completed (loss: 0.018228374421596527, acc: 1.0)
[2024-11-08 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:18][root][INFO] - Training Epoch: 6/10, step 380/574 completed (loss: 0.2261669635772705, acc: 0.9090909361839294)
[2024-11-08 04:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:19][root][INFO] - Training Epoch: 6/10, step 381/574 completed (loss: 0.042104750871658325, acc: 1.0)
[2024-11-08 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:20][root][INFO] - Training Epoch: 6/10, step 382/574 completed (loss: 0.006446563173085451, acc: 1.0)
[2024-11-08 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:21][root][INFO] - Training Epoch: 6/10, step 383/574 completed (loss: 0.17618654668331146, acc: 0.9642857313156128)
[2024-11-08 04:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:21][root][INFO] - Training Epoch: 6/10, step 384/574 completed (loss: 0.0013554188190028071, acc: 1.0)
[2024-11-08 04:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:21][root][INFO] - Training Epoch: 6/10, step 385/574 completed (loss: 0.01705780252814293, acc: 1.0)
[2024-11-08 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:22][root][INFO] - Training Epoch: 6/10, step 386/574 completed (loss: 0.02024829015135765, acc: 1.0)
[2024-11-08 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:22][root][INFO] - Training Epoch: 6/10, step 387/574 completed (loss: 0.0010401994222775102, acc: 1.0)
[2024-11-08 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:22][root][INFO] - Training Epoch: 6/10, step 388/574 completed (loss: 0.008227908052504063, acc: 1.0)
[2024-11-08 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:23][root][INFO] - Training Epoch: 6/10, step 389/574 completed (loss: 0.01553359441459179, acc: 1.0)
[2024-11-08 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:23][root][INFO] - Training Epoch: 6/10, step 390/574 completed (loss: 0.06319986283779144, acc: 0.9545454382896423)
[2024-11-08 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:24][root][INFO] - Training Epoch: 6/10, step 391/574 completed (loss: 0.22086191177368164, acc: 0.9523809552192688)
[2024-11-08 04:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:24][root][INFO] - Training Epoch: 6/10, step 392/574 completed (loss: 0.0789041742682457, acc: 1.0)
[2024-11-08 04:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:25][root][INFO] - Training Epoch: 6/10, step 393/574 completed (loss: 0.011749602854251862, acc: 1.0)
[2024-11-08 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:25][root][INFO] - Training Epoch: 6/10, step 394/574 completed (loss: 0.1300804615020752, acc: 0.9545454382896423)
[2024-11-08 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:26][root][INFO] - Training Epoch: 6/10, step 395/574 completed (loss: 0.1288224160671234, acc: 0.9523809552192688)
[2024-11-08 04:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:26][root][INFO] - Training Epoch: 6/10, step 396/574 completed (loss: 0.033265355974435806, acc: 1.0)
[2024-11-08 04:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:26][root][INFO] - Training Epoch: 6/10, step 397/574 completed (loss: 0.02979235164821148, acc: 0.9666666388511658)
[2024-11-08 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:27][root][INFO] - Training Epoch: 6/10, step 398/574 completed (loss: 0.024945978075265884, acc: 1.0)
[2024-11-08 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:27][root][INFO] - Training Epoch: 6/10, step 399/574 completed (loss: 0.011199387721717358, acc: 1.0)
[2024-11-08 04:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:28][root][INFO] - Training Epoch: 6/10, step 400/574 completed (loss: 0.030023861676454544, acc: 1.0)
[2024-11-08 04:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:28][root][INFO] - Training Epoch: 6/10, step 401/574 completed (loss: 0.004611729644238949, acc: 1.0)
[2024-11-08 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:29][root][INFO] - Training Epoch: 6/10, step 402/574 completed (loss: 0.004066119436174631, acc: 1.0)
[2024-11-08 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:29][root][INFO] - Training Epoch: 6/10, step 403/574 completed (loss: 0.003113214625045657, acc: 1.0)
[2024-11-08 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:30][root][INFO] - Training Epoch: 6/10, step 404/574 completed (loss: 0.05521750450134277, acc: 0.9714285731315613)
[2024-11-08 04:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:30][root][INFO] - Training Epoch: 6/10, step 405/574 completed (loss: 0.003846860257908702, acc: 1.0)
[2024-11-08 04:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:30][root][INFO] - Training Epoch: 6/10, step 406/574 completed (loss: 0.12741167843341827, acc: 0.9523809552192688)
[2024-11-08 04:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:31][root][INFO] - Training Epoch: 6/10, step 407/574 completed (loss: 0.23487450182437897, acc: 0.9473684430122375)
[2024-11-08 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:31][root][INFO] - Training Epoch: 6/10, step 408/574 completed (loss: 0.00278085027821362, acc: 1.0)
[2024-11-08 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:31][root][INFO] - Training Epoch: 6/10, step 409/574 completed (loss: 0.029090162366628647, acc: 1.0)
[2024-11-08 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:32][root][INFO] - Training Epoch: 6/10, step 410/574 completed (loss: 0.0009241041261702776, acc: 1.0)
[2024-11-08 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:32][root][INFO] - Training Epoch: 6/10, step 411/574 completed (loss: 0.03388766199350357, acc: 1.0)
[2024-11-08 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:33][root][INFO] - Training Epoch: 6/10, step 412/574 completed (loss: 0.0006402828730642796, acc: 1.0)
[2024-11-08 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:33][root][INFO] - Training Epoch: 6/10, step 413/574 completed (loss: 0.005318388808518648, acc: 1.0)
[2024-11-08 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:33][root][INFO] - Training Epoch: 6/10, step 414/574 completed (loss: 0.006701900623738766, acc: 1.0)
[2024-11-08 04:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:34][root][INFO] - Training Epoch: 6/10, step 415/574 completed (loss: 0.001142723485827446, acc: 1.0)
[2024-11-08 04:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:34][root][INFO] - Training Epoch: 6/10, step 416/574 completed (loss: 0.002939108293503523, acc: 1.0)
[2024-11-08 04:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:35][root][INFO] - Training Epoch: 6/10, step 417/574 completed (loss: 0.030703965574502945, acc: 1.0)
[2024-11-08 04:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:35][root][INFO] - Training Epoch: 6/10, step 418/574 completed (loss: 0.026392756029963493, acc: 1.0)
[2024-11-08 04:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:17][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3256, device='cuda:0') eval_epoch_loss=tensor(0.2819, device='cuda:0') eval_epoch_acc=tensor(0.9288, device='cuda:0')
[2024-11-08 04:10:17][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:10:17][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:10:21][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_6_step_419_loss_0.2818506360054016/model.pt
[2024-11-08 04:10:21][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:22][root][INFO] - Training Epoch: 6/10, step 419/574 completed (loss: 0.03682269528508186, acc: 0.9523809552192688)
[2024-11-08 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:22][root][INFO] - Training Epoch: 6/10, step 420/574 completed (loss: 0.07945294678211212, acc: 0.9583333134651184)
[2024-11-08 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:22][root][INFO] - Training Epoch: 6/10, step 421/574 completed (loss: 0.05016317963600159, acc: 0.9677419066429138)
[2024-11-08 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:23][root][INFO] - Training Epoch: 6/10, step 422/574 completed (loss: 0.12029297649860382, acc: 0.9677419066429138)
[2024-11-08 04:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:23][root][INFO] - Training Epoch: 6/10, step 423/574 completed (loss: 0.29071810841560364, acc: 0.9230769276618958)
[2024-11-08 04:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:23][root][INFO] - Training Epoch: 6/10, step 424/574 completed (loss: 0.007592473644763231, acc: 1.0)
[2024-11-08 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:24][root][INFO] - Training Epoch: 6/10, step 425/574 completed (loss: 0.003573151770979166, acc: 1.0)
[2024-11-08 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:24][root][INFO] - Training Epoch: 6/10, step 426/574 completed (loss: 0.07385528087615967, acc: 0.9473684430122375)
[2024-11-08 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:25][root][INFO] - Training Epoch: 6/10, step 427/574 completed (loss: 0.028411122038960457, acc: 1.0)
[2024-11-08 04:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:25][root][INFO] - Training Epoch: 6/10, step 428/574 completed (loss: 0.02893618494272232, acc: 1.0)
[2024-11-08 04:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:25][root][INFO] - Training Epoch: 6/10, step 429/574 completed (loss: 0.01033613458275795, acc: 1.0)
[2024-11-08 04:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:26][root][INFO] - Training Epoch: 6/10, step 430/574 completed (loss: 0.017756076529622078, acc: 1.0)
[2024-11-08 04:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:26][root][INFO] - Training Epoch: 6/10, step 431/574 completed (loss: 0.0022797463461756706, acc: 1.0)
[2024-11-08 04:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:26][root][INFO] - Training Epoch: 6/10, step 432/574 completed (loss: 0.0006125984364189208, acc: 1.0)
[2024-11-08 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:27][root][INFO] - Training Epoch: 6/10, step 433/574 completed (loss: 0.0051932912319898605, acc: 1.0)
[2024-11-08 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:27][root][INFO] - Training Epoch: 6/10, step 434/574 completed (loss: 0.009076064452528954, acc: 1.0)
[2024-11-08 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:28][root][INFO] - Training Epoch: 6/10, step 435/574 completed (loss: 0.01806003227829933, acc: 1.0)
[2024-11-08 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:28][root][INFO] - Training Epoch: 6/10, step 436/574 completed (loss: 0.00237565697170794, acc: 1.0)
[2024-11-08 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:29][root][INFO] - Training Epoch: 6/10, step 437/574 completed (loss: 0.0009645030368119478, acc: 1.0)
[2024-11-08 04:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:29][root][INFO] - Training Epoch: 6/10, step 438/574 completed (loss: 0.021647624671459198, acc: 1.0)
[2024-11-08 04:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:30][root][INFO] - Training Epoch: 6/10, step 439/574 completed (loss: 0.005546696484088898, acc: 1.0)
[2024-11-08 04:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:30][root][INFO] - Training Epoch: 6/10, step 440/574 completed (loss: 0.015181739814579487, acc: 1.0)
[2024-11-08 04:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:31][root][INFO] - Training Epoch: 6/10, step 441/574 completed (loss: 0.10623033344745636, acc: 1.0)
[2024-11-08 04:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:32][root][INFO] - Training Epoch: 6/10, step 442/574 completed (loss: 0.10392407327890396, acc: 0.9473684430122375)
[2024-11-08 04:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:33][root][INFO] - Training Epoch: 6/10, step 443/574 completed (loss: 0.04200451821088791, acc: 0.9545454382896423)
[2024-11-08 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:33][root][INFO] - Training Epoch: 6/10, step 444/574 completed (loss: 0.017719488590955734, acc: 1.0)
[2024-11-08 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:34][root][INFO] - Training Epoch: 6/10, step 445/574 completed (loss: 0.0032031021546572447, acc: 1.0)
[2024-11-08 04:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:34][root][INFO] - Training Epoch: 6/10, step 446/574 completed (loss: 0.1220223605632782, acc: 0.9677419066429138)
[2024-11-08 04:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:34][root][INFO] - Training Epoch: 6/10, step 447/574 completed (loss: 0.1803094893693924, acc: 0.9354838728904724)
[2024-11-08 04:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:35][root][INFO] - Training Epoch: 6/10, step 448/574 completed (loss: 0.014378616586327553, acc: 1.0)
[2024-11-08 04:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:35][root][INFO] - Training Epoch: 6/10, step 449/574 completed (loss: 0.001059850910678506, acc: 1.0)
[2024-11-08 04:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:36][root][INFO] - Training Epoch: 6/10, step 450/574 completed (loss: 0.00037878178409300745, acc: 1.0)
[2024-11-08 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:36][root][INFO] - Training Epoch: 6/10, step 451/574 completed (loss: 0.018246915191411972, acc: 1.0)
[2024-11-08 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:36][root][INFO] - Training Epoch: 6/10, step 452/574 completed (loss: 0.00030994179542176425, acc: 1.0)
[2024-11-08 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:37][root][INFO] - Training Epoch: 6/10, step 453/574 completed (loss: 0.0024128358345478773, acc: 1.0)
[2024-11-08 04:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:37][root][INFO] - Training Epoch: 6/10, step 454/574 completed (loss: 0.0462486632168293, acc: 1.0)
[2024-11-08 04:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:37][root][INFO] - Training Epoch: 6/10, step 455/574 completed (loss: 0.0370602123439312, acc: 0.9677419066429138)
[2024-11-08 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:38][root][INFO] - Training Epoch: 6/10, step 456/574 completed (loss: 0.019170567393302917, acc: 1.0)
[2024-11-08 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:38][root][INFO] - Training Epoch: 6/10, step 457/574 completed (loss: 0.01266943197697401, acc: 1.0)
[2024-11-08 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:39][root][INFO] - Training Epoch: 6/10, step 458/574 completed (loss: 0.003463990520685911, acc: 1.0)
[2024-11-08 04:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:39][root][INFO] - Training Epoch: 6/10, step 459/574 completed (loss: 0.009130540303885937, acc: 1.0)
[2024-11-08 04:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:39][root][INFO] - Training Epoch: 6/10, step 460/574 completed (loss: 0.01762772910296917, acc: 1.0)
[2024-11-08 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:40][root][INFO] - Training Epoch: 6/10, step 461/574 completed (loss: 0.022872494533658028, acc: 1.0)
[2024-11-08 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:40][root][INFO] - Training Epoch: 6/10, step 462/574 completed (loss: 0.020460084080696106, acc: 1.0)
[2024-11-08 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:40][root][INFO] - Training Epoch: 6/10, step 463/574 completed (loss: 0.013314676471054554, acc: 1.0)
[2024-11-08 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:41][root][INFO] - Training Epoch: 6/10, step 464/574 completed (loss: 0.00404174393042922, acc: 1.0)
[2024-11-08 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:41][root][INFO] - Training Epoch: 6/10, step 465/574 completed (loss: 0.0909939780831337, acc: 0.949999988079071)
[2024-11-08 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:42][root][INFO] - Training Epoch: 6/10, step 466/574 completed (loss: 0.007104402873665094, acc: 1.0)
[2024-11-08 04:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:42][root][INFO] - Training Epoch: 6/10, step 467/574 completed (loss: 0.005456394981592894, acc: 1.0)
[2024-11-08 04:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:42][root][INFO] - Training Epoch: 6/10, step 468/574 completed (loss: 0.0029919552616775036, acc: 1.0)
[2024-11-08 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:43][root][INFO] - Training Epoch: 6/10, step 469/574 completed (loss: 0.00016913507715798914, acc: 1.0)
[2024-11-08 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:43][root][INFO] - Training Epoch: 6/10, step 470/574 completed (loss: 0.023490263149142265, acc: 1.0)
[2024-11-08 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:44][root][INFO] - Training Epoch: 6/10, step 471/574 completed (loss: 0.016076816245913506, acc: 1.0)
[2024-11-08 04:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:44][root][INFO] - Training Epoch: 6/10, step 472/574 completed (loss: 0.00934823788702488, acc: 1.0)
[2024-11-08 04:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:45][root][INFO] - Training Epoch: 6/10, step 473/574 completed (loss: 0.007058687042444944, acc: 1.0)
[2024-11-08 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:45][root][INFO] - Training Epoch: 6/10, step 474/574 completed (loss: 0.01753098890185356, acc: 1.0)
[2024-11-08 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:45][root][INFO] - Training Epoch: 6/10, step 475/574 completed (loss: 0.03699900954961777, acc: 1.0)
[2024-11-08 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:46][root][INFO] - Training Epoch: 6/10, step 476/574 completed (loss: 0.01910424791276455, acc: 1.0)
[2024-11-08 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:46][root][INFO] - Training Epoch: 6/10, step 477/574 completed (loss: 0.10115714371204376, acc: 0.9090909361839294)
[2024-11-08 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:46][root][INFO] - Training Epoch: 6/10, step 478/574 completed (loss: 0.0016492769354954362, acc: 1.0)
[2024-11-08 04:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:47][root][INFO] - Training Epoch: 6/10, step 479/574 completed (loss: 0.005764803849160671, acc: 1.0)
[2024-11-08 04:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:47][root][INFO] - Training Epoch: 6/10, step 480/574 completed (loss: 0.1800733506679535, acc: 0.9677419066429138)
[2024-11-08 04:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:48][root][INFO] - Training Epoch: 6/10, step 481/574 completed (loss: 0.04639105871319771, acc: 1.0)
[2024-11-08 04:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:48][root][INFO] - Training Epoch: 6/10, step 482/574 completed (loss: 0.13506260514259338, acc: 0.9473684430122375)
[2024-11-08 04:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:48][root][INFO] - Training Epoch: 6/10, step 483/574 completed (loss: 0.04784958064556122, acc: 1.0)
[2024-11-08 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:49][root][INFO] - Training Epoch: 6/10, step 484/574 completed (loss: 0.026181288063526154, acc: 1.0)
[2024-11-08 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:49][root][INFO] - Training Epoch: 6/10, step 485/574 completed (loss: 0.0030333902686834335, acc: 1.0)
[2024-11-08 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:49][root][INFO] - Training Epoch: 6/10, step 486/574 completed (loss: 0.023971233516931534, acc: 1.0)
[2024-11-08 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:50][root][INFO] - Training Epoch: 6/10, step 487/574 completed (loss: 0.06380900740623474, acc: 0.9714285731315613)
[2024-11-08 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:50][root][INFO] - Training Epoch: 6/10, step 488/574 completed (loss: 0.07639724761247635, acc: 0.9615384340286255)
[2024-11-08 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:51][root][INFO] - Training Epoch: 6/10, step 489/574 completed (loss: 0.045242030173540115, acc: 0.9523809552192688)
[2024-11-08 04:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:51][root][INFO] - Training Epoch: 6/10, step 490/574 completed (loss: 0.01714899390935898, acc: 1.0)
[2024-11-08 04:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:51][root][INFO] - Training Epoch: 6/10, step 491/574 completed (loss: 0.27968084812164307, acc: 0.8947368264198303)
[2024-11-08 04:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:52][root][INFO] - Training Epoch: 6/10, step 492/574 completed (loss: 0.03801657259464264, acc: 1.0)
[2024-11-08 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:52][root][INFO] - Training Epoch: 6/10, step 493/574 completed (loss: 0.01274839323014021, acc: 1.0)
[2024-11-08 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:53][root][INFO] - Training Epoch: 6/10, step 494/574 completed (loss: 0.10133183002471924, acc: 0.9166666865348816)
[2024-11-08 04:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:53][root][INFO] - Training Epoch: 6/10, step 495/574 completed (loss: 0.2116023153066635, acc: 0.9642857313156128)
[2024-11-08 04:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:53][root][INFO] - Training Epoch: 6/10, step 496/574 completed (loss: 0.17622068524360657, acc: 0.949999988079071)
[2024-11-08 04:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:54][root][INFO] - Training Epoch: 6/10, step 497/574 completed (loss: 0.02428283914923668, acc: 1.0)
[2024-11-08 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:54][root][INFO] - Training Epoch: 6/10, step 498/574 completed (loss: 0.09568557888269424, acc: 0.9473684430122375)
[2024-11-08 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:55][root][INFO] - Training Epoch: 6/10, step 499/574 completed (loss: 0.19843611121177673, acc: 0.9090909361839294)
[2024-11-08 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:55][root][INFO] - Training Epoch: 6/10, step 500/574 completed (loss: 0.3675270974636078, acc: 0.8999999761581421)
[2024-11-08 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:55][root][INFO] - Training Epoch: 6/10, step 501/574 completed (loss: 0.002286585746333003, acc: 1.0)
[2024-11-08 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:56][root][INFO] - Training Epoch: 6/10, step 502/574 completed (loss: 0.0043294732458889484, acc: 1.0)
[2024-11-08 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:56][root][INFO] - Training Epoch: 6/10, step 503/574 completed (loss: 0.017314007505774498, acc: 1.0)
[2024-11-08 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:57][root][INFO] - Training Epoch: 6/10, step 504/574 completed (loss: 0.005288335960358381, acc: 1.0)
[2024-11-08 04:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:57][root][INFO] - Training Epoch: 6/10, step 505/574 completed (loss: 0.09343338757753372, acc: 0.9047619104385376)
[2024-11-08 04:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:57][root][INFO] - Training Epoch: 6/10, step 506/574 completed (loss: 0.22217324376106262, acc: 0.8947368264198303)
[2024-11-08 04:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:58][root][INFO] - Training Epoch: 6/10, step 507/574 completed (loss: 0.00869551207870245, acc: 1.0)
[2024-11-08 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:59][root][INFO] - Training Epoch: 6/10, step 508/574 completed (loss: 0.628850519657135, acc: 0.800000011920929)
[2024-11-08 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:10:59][root][INFO] - Training Epoch: 6/10, step 509/574 completed (loss: 0.02577175945043564, acc: 1.0)
[2024-11-08 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:00][root][INFO] - Training Epoch: 6/10, step 510/574 completed (loss: 0.001355100772343576, acc: 1.0)
[2024-11-08 04:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:00][root][INFO] - Training Epoch: 6/10, step 511/574 completed (loss: 0.012692037038505077, acc: 1.0)
[2024-11-08 04:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:04][root][INFO] - Training Epoch: 6/10, step 512/574 completed (loss: 0.0832962617278099, acc: 0.9642857313156128)
[2024-11-08 04:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:05][root][INFO] - Training Epoch: 6/10, step 513/574 completed (loss: 0.016729146242141724, acc: 1.0)
[2024-11-08 04:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:05][root][INFO] - Training Epoch: 6/10, step 514/574 completed (loss: 0.15068519115447998, acc: 0.9473684430122375)
[2024-11-08 04:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:06][root][INFO] - Training Epoch: 6/10, step 515/574 completed (loss: 0.00525062158703804, acc: 1.0)
[2024-11-08 04:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:07][root][INFO] - Training Epoch: 6/10, step 516/574 completed (loss: 0.009376494213938713, acc: 1.0)
[2024-11-08 04:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:07][root][INFO] - Training Epoch: 6/10, step 517/574 completed (loss: 0.09697019308805466, acc: 0.9545454382896423)
[2024-11-08 04:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:08][root][INFO] - Training Epoch: 6/10, step 518/574 completed (loss: 0.07273560017347336, acc: 0.9642857313156128)
[2024-11-08 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:08][root][INFO] - Training Epoch: 6/10, step 519/574 completed (loss: 0.15038283169269562, acc: 0.9629629850387573)
[2024-11-08 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:08][root][INFO] - Training Epoch: 6/10, step 520/574 completed (loss: 0.018217187374830246, acc: 1.0)
[2024-11-08 04:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:10][root][INFO] - Training Epoch: 6/10, step 521/574 completed (loss: 0.050072167068719864, acc: 0.95652174949646)
[2024-11-08 04:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:10][root][INFO] - Training Epoch: 6/10, step 522/574 completed (loss: 0.2900053560733795, acc: 0.9047619104385376)
[2024-11-08 04:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:11][root][INFO] - Training Epoch: 6/10, step 523/574 completed (loss: 0.0469348169863224, acc: 1.0)
[2024-11-08 04:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:12][root][INFO] - Training Epoch: 6/10, step 524/574 completed (loss: 0.24099741876125336, acc: 0.9090909361839294)
[2024-11-08 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:12][root][INFO] - Training Epoch: 6/10, step 525/574 completed (loss: 0.04428624361753464, acc: 0.949999988079071)
[2024-11-08 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:12][root][INFO] - Training Epoch: 6/10, step 526/574 completed (loss: 0.05308084562420845, acc: 1.0)
[2024-11-08 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:13][root][INFO] - Training Epoch: 6/10, step 527/574 completed (loss: 0.1633288413286209, acc: 0.939393937587738)
[2024-11-08 04:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:13][root][INFO] - Training Epoch: 6/10, step 528/574 completed (loss: 0.11498618125915527, acc: 0.9642857313156128)
[2024-11-08 04:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:13][root][INFO] - Training Epoch: 6/10, step 529/574 completed (loss: 0.012727306224405766, acc: 1.0)
[2024-11-08 04:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:14][root][INFO] - Training Epoch: 6/10, step 530/574 completed (loss: 0.23527109622955322, acc: 0.8947368264198303)
[2024-11-08 04:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:14][root][INFO] - Training Epoch: 6/10, step 531/574 completed (loss: 0.09278862178325653, acc: 0.949999988079071)
[2024-11-08 04:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:15][root][INFO] - Training Epoch: 6/10, step 532/574 completed (loss: 0.38570302724838257, acc: 0.9047619104385376)
[2024-11-08 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:15][root][INFO] - Training Epoch: 6/10, step 533/574 completed (loss: 0.041295021772384644, acc: 1.0)
[2024-11-08 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:15][root][INFO] - Training Epoch: 6/10, step 534/574 completed (loss: 0.5012624859809875, acc: 0.9285714030265808)
[2024-11-08 04:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:16][root][INFO] - Training Epoch: 6/10, step 535/574 completed (loss: 0.019545897841453552, acc: 1.0)
[2024-11-08 04:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:16][root][INFO] - Training Epoch: 6/10, step 536/574 completed (loss: 0.08046650886535645, acc: 0.9599999785423279)
[2024-11-08 04:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:17][root][INFO] - Training Epoch: 6/10, step 537/574 completed (loss: 0.0886397659778595, acc: 0.9523809552192688)
[2024-11-08 04:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:17][root][INFO] - Training Epoch: 6/10, step 538/574 completed (loss: 0.34824633598327637, acc: 0.8421052694320679)
[2024-11-08 04:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:18][root][INFO] - Training Epoch: 6/10, step 539/574 completed (loss: 0.2076307237148285, acc: 0.949999988079071)
[2024-11-08 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:18][root][INFO] - Training Epoch: 6/10, step 540/574 completed (loss: 0.19008702039718628, acc: 0.9523809552192688)
[2024-11-08 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:18][root][INFO] - Training Epoch: 6/10, step 541/574 completed (loss: 0.004508716519922018, acc: 1.0)
[2024-11-08 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:19][root][INFO] - Training Epoch: 6/10, step 542/574 completed (loss: 0.07905086129903793, acc: 0.939393937587738)
[2024-11-08 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:19][root][INFO] - Training Epoch: 6/10, step 543/574 completed (loss: 0.006187424995005131, acc: 1.0)
[2024-11-08 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:19][root][INFO] - Training Epoch: 6/10, step 544/574 completed (loss: 0.1263674795627594, acc: 0.9696969985961914)
[2024-11-08 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:20][root][INFO] - Training Epoch: 6/10, step 545/574 completed (loss: 0.0340479351580143, acc: 1.0)
[2024-11-08 04:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:20][root][INFO] - Training Epoch: 6/10, step 546/574 completed (loss: 0.020134560763835907, acc: 1.0)
[2024-11-08 04:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:20][root][INFO] - Training Epoch: 6/10, step 547/574 completed (loss: 0.13698919117450714, acc: 0.9523809552192688)
[2024-11-08 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:21][root][INFO] - Training Epoch: 6/10, step 548/574 completed (loss: 0.04336515814065933, acc: 1.0)
[2024-11-08 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:21][root][INFO] - Training Epoch: 6/10, step 549/574 completed (loss: 0.007842623628675938, acc: 1.0)
[2024-11-08 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:21][root][INFO] - Training Epoch: 6/10, step 550/574 completed (loss: 0.009196560829877853, acc: 1.0)
[2024-11-08 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:22][root][INFO] - Training Epoch: 6/10, step 551/574 completed (loss: 0.03887200728058815, acc: 1.0)
[2024-11-08 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:22][root][INFO] - Training Epoch: 6/10, step 552/574 completed (loss: 0.004826860502362251, acc: 1.0)
[2024-11-08 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:23][root][INFO] - Training Epoch: 6/10, step 553/574 completed (loss: 0.005102460738271475, acc: 1.0)
[2024-11-08 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:23][root][INFO] - Training Epoch: 6/10, step 554/574 completed (loss: 0.0005996610270813107, acc: 1.0)
[2024-11-08 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:23][root][INFO] - Training Epoch: 6/10, step 555/574 completed (loss: 0.059318751096725464, acc: 1.0)
[2024-11-08 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:24][root][INFO] - Training Epoch: 6/10, step 556/574 completed (loss: 0.03932250663638115, acc: 1.0)
[2024-11-08 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:24][root][INFO] - Training Epoch: 6/10, step 557/574 completed (loss: 0.004820132162421942, acc: 1.0)
[2024-11-08 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:24][root][INFO] - Training Epoch: 6/10, step 558/574 completed (loss: 0.3836812376976013, acc: 0.8928571343421936)
[2024-11-08 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:25][root][INFO] - Training Epoch: 6/10, step 559/574 completed (loss: 0.1778101921081543, acc: 0.9629629850387573)
[2024-11-08 04:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:25][root][INFO] - Training Epoch: 6/10, step 560/574 completed (loss: 0.011776327155530453, acc: 1.0)
[2024-11-08 04:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:25][root][INFO] - Training Epoch: 6/10, step 561/574 completed (loss: 0.021855011582374573, acc: 1.0)
[2024-11-08 04:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:07][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3821, device='cuda:0') eval_epoch_loss=tensor(0.3236, device='cuda:0') eval_epoch_acc=tensor(0.9114, device='cuda:0')
[2024-11-08 04:12:07][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:12:07][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:12:11][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_6_step_562_loss_0.32359760999679565/model.pt
[2024-11-08 04:12:11][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:11][root][INFO] - Training Epoch: 6/10, step 562/574 completed (loss: 0.0036485265009105206, acc: 1.0)
[2024-11-08 04:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:11][root][INFO] - Training Epoch: 6/10, step 563/574 completed (loss: 0.016169842332601547, acc: 1.0)
[2024-11-08 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:12][root][INFO] - Training Epoch: 6/10, step 564/574 completed (loss: 0.1355554312467575, acc: 0.9545454382896423)
[2024-11-08 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:12][root][INFO] - Training Epoch: 6/10, step 565/574 completed (loss: 0.1477234810590744, acc: 0.8999999761581421)
[2024-11-08 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:13][root][INFO] - Training Epoch: 6/10, step 566/574 completed (loss: 0.10862117260694504, acc: 0.9090909361839294)
[2024-11-08 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:13][root][INFO] - Training Epoch: 6/10, step 567/574 completed (loss: 0.03746461495757103, acc: 1.0)
[2024-11-08 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:13][root][INFO] - Training Epoch: 6/10, step 568/574 completed (loss: 0.4684560298919678, acc: 0.9032257795333862)
[2024-11-08 04:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:14][root][INFO] - Training Epoch: 6/10, step 569/574 completed (loss: 0.0028521076310425997, acc: 1.0)
[2024-11-08 04:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:14][root][INFO] - Training Epoch: 6/10, step 570/574 completed (loss: 0.007209294941276312, acc: 1.0)
[2024-11-08 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:15][root][INFO] - Training Epoch: 6/10, step 571/574 completed (loss: 0.003207871690392494, acc: 1.0)
[2024-11-08 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:15][root][INFO] - Training Epoch: 6/10, step 572/574 completed (loss: 0.01317136362195015, acc: 1.0)
[2024-11-08 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:16][root][INFO] - Training Epoch: 6/10, step 573/574 completed (loss: 0.021931707859039307, acc: 1.0)
[2024-11-08 04:12:16][slam_llm.utils.train_utils][INFO] - Epoch 6: train_perplexity=1.0907, train_epoch_loss=0.0868, epoch time 466.76882673986256s
[2024-11-08 04:12:16][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 04:12:16][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2024-11-08 04:12:16][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 04:12:16][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 5
[2024-11-08 04:12:16][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 04:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:17][root][INFO] - Training Epoch: 7/10, step 0/574 completed (loss: 0.5870911478996277, acc: 0.8571428656578064)
[2024-11-08 04:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:17][root][INFO] - Training Epoch: 7/10, step 1/574 completed (loss: 0.04477475583553314, acc: 1.0)
[2024-11-08 04:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:18][root][INFO] - Training Epoch: 7/10, step 2/574 completed (loss: 0.07804302126169205, acc: 0.9714285731315613)
[2024-11-08 04:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:18][root][INFO] - Training Epoch: 7/10, step 3/574 completed (loss: 0.01494126208126545, acc: 1.0)
[2024-11-08 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:19][root][INFO] - Training Epoch: 7/10, step 4/574 completed (loss: 0.008460314013063908, acc: 1.0)
[2024-11-08 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:19][root][INFO] - Training Epoch: 7/10, step 5/574 completed (loss: 0.14049644768238068, acc: 0.9473684430122375)
[2024-11-08 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:19][root][INFO] - Training Epoch: 7/10, step 6/574 completed (loss: 0.007906044833362103, acc: 1.0)
[2024-11-08 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:20][root][INFO] - Training Epoch: 7/10, step 7/574 completed (loss: 0.1737828254699707, acc: 0.9523809552192688)
[2024-11-08 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:20][root][INFO] - Training Epoch: 7/10, step 8/574 completed (loss: 0.006404349580407143, acc: 1.0)
[2024-11-08 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:21][root][INFO] - Training Epoch: 7/10, step 9/574 completed (loss: 0.0872674360871315, acc: 1.0)
[2024-11-08 04:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:21][root][INFO] - Training Epoch: 7/10, step 10/574 completed (loss: 0.00662236986681819, acc: 1.0)
[2024-11-08 04:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:21][root][INFO] - Training Epoch: 7/10, step 11/574 completed (loss: 0.021206524223089218, acc: 1.0)
[2024-11-08 04:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:22][root][INFO] - Training Epoch: 7/10, step 12/574 completed (loss: 0.025762874633073807, acc: 1.0)
[2024-11-08 04:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:22][root][INFO] - Training Epoch: 7/10, step 13/574 completed (loss: 0.016190195456147194, acc: 1.0)
[2024-11-08 04:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:23][root][INFO] - Training Epoch: 7/10, step 14/574 completed (loss: 0.010146590881049633, acc: 1.0)
[2024-11-08 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:23][root][INFO] - Training Epoch: 7/10, step 15/574 completed (loss: 0.016329072415828705, acc: 1.0)
[2024-11-08 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:23][root][INFO] - Training Epoch: 7/10, step 16/574 completed (loss: 0.10076596587896347, acc: 0.9642857313156128)
[2024-11-08 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:24][root][INFO] - Training Epoch: 7/10, step 17/574 completed (loss: 0.03514557331800461, acc: 1.0)
[2024-11-08 04:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:24][root][INFO] - Training Epoch: 7/10, step 18/574 completed (loss: 0.010099176317453384, acc: 1.0)
[2024-11-08 04:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:25][root][INFO] - Training Epoch: 7/10, step 19/574 completed (loss: 0.11551646143198013, acc: 0.9473684430122375)
[2024-11-08 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:25][root][INFO] - Training Epoch: 7/10, step 20/574 completed (loss: 0.03792475163936615, acc: 1.0)
[2024-11-08 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:25][root][INFO] - Training Epoch: 7/10, step 21/574 completed (loss: 0.5702897906303406, acc: 0.8636363744735718)
[2024-11-08 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:26][root][INFO] - Training Epoch: 7/10, step 22/574 completed (loss: 0.040725357830524445, acc: 1.0)
[2024-11-08 04:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:26][root][INFO] - Training Epoch: 7/10, step 23/574 completed (loss: 0.0009191856952384114, acc: 1.0)
[2024-11-08 04:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:26][root][INFO] - Training Epoch: 7/10, step 24/574 completed (loss: 0.08445194363594055, acc: 0.9666666388511658)
[2024-11-08 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:27][root][INFO] - Training Epoch: 7/10, step 25/574 completed (loss: 0.05340725556015968, acc: 1.0)
[2024-11-08 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:27][root][INFO] - Training Epoch: 7/10, step 26/574 completed (loss: 0.13513733446598053, acc: 0.9047619104385376)
[2024-11-08 04:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:29][root][INFO] - Training Epoch: 7/10, step 27/574 completed (loss: 0.00998940784484148, acc: 1.0)
[2024-11-08 04:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:29][root][INFO] - Training Epoch: 7/10, step 28/574 completed (loss: 0.04659094661474228, acc: 1.0)
[2024-11-08 04:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:30][root][INFO] - Training Epoch: 7/10, step 29/574 completed (loss: 0.08112377673387527, acc: 0.949999988079071)
[2024-11-08 04:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:30][root][INFO] - Training Epoch: 7/10, step 30/574 completed (loss: 0.008340073749423027, acc: 1.0)
[2024-11-08 04:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:31][root][INFO] - Training Epoch: 7/10, step 31/574 completed (loss: 0.07417058199644089, acc: 0.931034505367279)
[2024-11-08 04:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:31][root][INFO] - Training Epoch: 7/10, step 32/574 completed (loss: 0.824858546257019, acc: 0.9090909361839294)
[2024-11-08 04:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:31][root][INFO] - Training Epoch: 7/10, step 33/574 completed (loss: 0.007911304011940956, acc: 1.0)
[2024-11-08 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:32][root][INFO] - Training Epoch: 7/10, step 34/574 completed (loss: 0.028185231611132622, acc: 1.0)
[2024-11-08 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:32][root][INFO] - Training Epoch: 7/10, step 35/574 completed (loss: 0.00834553875029087, acc: 1.0)
[2024-11-08 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:33][root][INFO] - Training Epoch: 7/10, step 36/574 completed (loss: 0.19447621703147888, acc: 0.949999988079071)
[2024-11-08 04:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:33][root][INFO] - Training Epoch: 7/10, step 37/574 completed (loss: 0.08766297250986099, acc: 0.9523809552192688)
[2024-11-08 04:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:33][root][INFO] - Training Epoch: 7/10, step 38/574 completed (loss: 0.01689717173576355, acc: 1.0)
[2024-11-08 04:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:34][root][INFO] - Training Epoch: 7/10, step 39/574 completed (loss: 0.36857205629348755, acc: 0.8928571343421936)
[2024-11-08 04:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:34][root][INFO] - Training Epoch: 7/10, step 40/574 completed (loss: 0.4879353642463684, acc: 0.8571428656578064)
[2024-11-08 04:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:35][root][INFO] - Training Epoch: 7/10, step 41/574 completed (loss: 0.16581538319587708, acc: 0.8999999761581421)
[2024-11-08 04:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:35][root][INFO] - Training Epoch: 7/10, step 42/574 completed (loss: 0.23216703534126282, acc: 0.949999988079071)
[2024-11-08 04:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:36][root][INFO] - Training Epoch: 7/10, step 43/574 completed (loss: 0.06581749767065048, acc: 1.0)
[2024-11-08 04:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:36][root][INFO] - Training Epoch: 7/10, step 44/574 completed (loss: 0.24432650208473206, acc: 0.9090909361839294)
[2024-11-08 04:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:37][root][INFO] - Training Epoch: 7/10, step 45/574 completed (loss: 0.06430049985647202, acc: 0.9523809552192688)
[2024-11-08 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:37][root][INFO] - Training Epoch: 7/10, step 46/574 completed (loss: 0.09620171785354614, acc: 0.9583333134651184)
[2024-11-08 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:38][root][INFO] - Training Epoch: 7/10, step 47/574 completed (loss: 0.018314825370907784, acc: 1.0)
[2024-11-08 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:38][root][INFO] - Training Epoch: 7/10, step 48/574 completed (loss: 0.12200360745191574, acc: 0.9428571462631226)
[2024-11-08 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:38][root][INFO] - Training Epoch: 7/10, step 49/574 completed (loss: 0.018259776756167412, acc: 1.0)
[2024-11-08 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:39][root][INFO] - Training Epoch: 7/10, step 50/574 completed (loss: 0.03486233577132225, acc: 1.0)
[2024-11-08 04:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:39][root][INFO] - Training Epoch: 7/10, step 51/574 completed (loss: 0.020380202680826187, acc: 1.0)
[2024-11-08 04:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:39][root][INFO] - Training Epoch: 7/10, step 52/574 completed (loss: 0.08407894521951675, acc: 0.949999988079071)
[2024-11-08 04:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:40][root][INFO] - Training Epoch: 7/10, step 53/574 completed (loss: 0.0629650354385376, acc: 1.0)
[2024-11-08 04:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:40][root][INFO] - Training Epoch: 7/10, step 54/574 completed (loss: 0.10944729298353195, acc: 0.9545454382896423)
[2024-11-08 04:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:41][root][INFO] - Training Epoch: 7/10, step 55/574 completed (loss: 0.005274042021483183, acc: 1.0)
[2024-11-08 04:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:45][root][INFO] - Training Epoch: 7/10, step 56/574 completed (loss: 0.19156710803508759, acc: 0.9047619104385376)
[2024-11-08 04:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:47][root][INFO] - Training Epoch: 7/10, step 57/574 completed (loss: 0.2767362594604492, acc: 0.9523809552192688)
[2024-11-08 04:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:48][root][INFO] - Training Epoch: 7/10, step 58/574 completed (loss: 0.06828866899013519, acc: 0.9473684430122375)
[2024-11-08 04:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:48][root][INFO] - Training Epoch: 7/10, step 59/574 completed (loss: 0.1238986998796463, acc: 0.9090909361839294)
[2024-11-08 04:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:49][root][INFO] - Training Epoch: 7/10, step 60/574 completed (loss: 0.17111846804618835, acc: 0.9473684430122375)
[2024-11-08 04:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:50][root][INFO] - Training Epoch: 7/10, step 61/574 completed (loss: 0.037765003740787506, acc: 1.0)
[2024-11-08 04:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:50][root][INFO] - Training Epoch: 7/10, step 62/574 completed (loss: 0.027635768055915833, acc: 1.0)
[2024-11-08 04:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:51][root][INFO] - Training Epoch: 7/10, step 63/574 completed (loss: 0.25394824147224426, acc: 0.8928571343421936)
[2024-11-08 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:51][root][INFO] - Training Epoch: 7/10, step 64/574 completed (loss: 0.023001376539468765, acc: 1.0)
[2024-11-08 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:51][root][INFO] - Training Epoch: 7/10, step 65/574 completed (loss: 0.04445866867899895, acc: 1.0)
[2024-11-08 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:52][root][INFO] - Training Epoch: 7/10, step 66/574 completed (loss: 0.10105985403060913, acc: 0.9473684430122375)
[2024-11-08 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:52][root][INFO] - Training Epoch: 7/10, step 67/574 completed (loss: 0.08885841071605682, acc: 0.949999988079071)
[2024-11-08 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:53][root][INFO] - Training Epoch: 7/10, step 68/574 completed (loss: 0.0028979815542697906, acc: 1.0)
[2024-11-08 04:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:53][root][INFO] - Training Epoch: 7/10, step 69/574 completed (loss: 0.019164055585861206, acc: 1.0)
[2024-11-08 04:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:53][root][INFO] - Training Epoch: 7/10, step 70/574 completed (loss: 0.006811236497014761, acc: 1.0)
[2024-11-08 04:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:54][root][INFO] - Training Epoch: 7/10, step 71/574 completed (loss: 0.021549087017774582, acc: 1.0)
[2024-11-08 04:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:54][root][INFO] - Training Epoch: 7/10, step 72/574 completed (loss: 0.1472802311182022, acc: 0.9523809552192688)
[2024-11-08 04:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:54][root][INFO] - Training Epoch: 7/10, step 73/574 completed (loss: 0.058353669941425323, acc: 1.0)
[2024-11-08 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:55][root][INFO] - Training Epoch: 7/10, step 74/574 completed (loss: 0.05536729842424393, acc: 0.949999988079071)
[2024-11-08 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:55][root][INFO] - Training Epoch: 7/10, step 75/574 completed (loss: 0.06821442395448685, acc: 0.9523809552192688)
[2024-11-08 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:56][root][INFO] - Training Epoch: 7/10, step 76/574 completed (loss: 0.014395692385733128, acc: 1.0)
[2024-11-08 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:56][root][INFO] - Training Epoch: 7/10, step 77/574 completed (loss: 0.002842677989974618, acc: 1.0)
[2024-11-08 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:57][root][INFO] - Training Epoch: 7/10, step 78/574 completed (loss: 0.0019350963411852717, acc: 1.0)
[2024-11-08 04:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:57][root][INFO] - Training Epoch: 7/10, step 79/574 completed (loss: 0.021401191130280495, acc: 1.0)
[2024-11-08 04:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:57][root][INFO] - Training Epoch: 7/10, step 80/574 completed (loss: 0.004674126394093037, acc: 1.0)
[2024-11-08 04:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:58][root][INFO] - Training Epoch: 7/10, step 81/574 completed (loss: 0.009373841807246208, acc: 1.0)
[2024-11-08 04:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:58][root][INFO] - Training Epoch: 7/10, step 82/574 completed (loss: 0.01949036307632923, acc: 1.0)
[2024-11-08 04:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:59][root][INFO] - Training Epoch: 7/10, step 83/574 completed (loss: 0.13702428340911865, acc: 0.9473684430122375)
[2024-11-08 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:59][root][INFO] - Training Epoch: 7/10, step 84/574 completed (loss: 0.006818207912147045, acc: 1.0)
[2024-11-08 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:12:59][root][INFO] - Training Epoch: 7/10, step 85/574 completed (loss: 0.0018941857852041721, acc: 1.0)
[2024-11-08 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:00][root][INFO] - Training Epoch: 7/10, step 86/574 completed (loss: 0.014668474905192852, acc: 1.0)
[2024-11-08 04:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:00][root][INFO] - Training Epoch: 7/10, step 87/574 completed (loss: 0.3366020917892456, acc: 0.8846153616905212)
[2024-11-08 04:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:01][root][INFO] - Training Epoch: 7/10, step 88/574 completed (loss: 0.11747554689645767, acc: 0.9523809552192688)
[2024-11-08 04:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:02][root][INFO] - Training Epoch: 7/10, step 89/574 completed (loss: 0.11249390244483948, acc: 0.9473684430122375)
[2024-11-08 04:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:04][root][INFO] - Training Epoch: 7/10, step 90/574 completed (loss: 0.19521848857402802, acc: 0.9473684430122375)
[2024-11-08 04:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:05][root][INFO] - Training Epoch: 7/10, step 91/574 completed (loss: 0.04735422134399414, acc: 0.9545454382896423)
[2024-11-08 04:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:06][root][INFO] - Training Epoch: 7/10, step 92/574 completed (loss: 0.1583532989025116, acc: 0.9523809552192688)
[2024-11-08 04:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:07][root][INFO] - Training Epoch: 7/10, step 93/574 completed (loss: 0.6562981009483337, acc: 0.807692289352417)
[2024-11-08 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:08][root][INFO] - Training Epoch: 7/10, step 94/574 completed (loss: 0.13350601494312286, acc: 0.9285714030265808)
[2024-11-08 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:08][root][INFO] - Training Epoch: 7/10, step 95/574 completed (loss: 0.07578251510858536, acc: 1.0)
[2024-11-08 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:09][root][INFO] - Training Epoch: 7/10, step 96/574 completed (loss: 0.12993834912776947, acc: 0.9473684430122375)
[2024-11-08 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:09][root][INFO] - Training Epoch: 7/10, step 97/574 completed (loss: 0.13756707310676575, acc: 0.949999988079071)
[2024-11-08 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:10][root][INFO] - Training Epoch: 7/10, step 98/574 completed (loss: 0.12993231415748596, acc: 0.9047619104385376)
[2024-11-08 04:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:10][root][INFO] - Training Epoch: 7/10, step 99/574 completed (loss: 0.12108859419822693, acc: 0.9545454382896423)
[2024-11-08 04:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:10][root][INFO] - Training Epoch: 7/10, step 100/574 completed (loss: 0.04099627584218979, acc: 1.0)
[2024-11-08 04:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:11][root][INFO] - Training Epoch: 7/10, step 101/574 completed (loss: 0.026764879003167152, acc: 1.0)
[2024-11-08 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:11][root][INFO] - Training Epoch: 7/10, step 102/574 completed (loss: 0.06820843368768692, acc: 1.0)
[2024-11-08 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:12][root][INFO] - Training Epoch: 7/10, step 103/574 completed (loss: 0.10776074230670929, acc: 0.9473684430122375)
[2024-11-08 04:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:12][root][INFO] - Training Epoch: 7/10, step 104/574 completed (loss: 0.06068187952041626, acc: 0.9545454382896423)
[2024-11-08 04:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:12][root][INFO] - Training Epoch: 7/10, step 105/574 completed (loss: 0.10113774240016937, acc: 0.9473684430122375)
[2024-11-08 04:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:13][root][INFO] - Training Epoch: 7/10, step 106/574 completed (loss: 0.1618526726961136, acc: 0.9545454382896423)
[2024-11-08 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:13][root][INFO] - Training Epoch: 7/10, step 107/574 completed (loss: 0.002063715597614646, acc: 1.0)
[2024-11-08 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:13][root][INFO] - Training Epoch: 7/10, step 108/574 completed (loss: 0.003849813248962164, acc: 1.0)
[2024-11-08 04:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:14][root][INFO] - Training Epoch: 7/10, step 109/574 completed (loss: 0.011639932170510292, acc: 1.0)
[2024-11-08 04:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:14][root][INFO] - Training Epoch: 7/10, step 110/574 completed (loss: 0.0075338431634008884, acc: 1.0)
[2024-11-08 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:15][root][INFO] - Training Epoch: 7/10, step 111/574 completed (loss: 0.12735901772975922, acc: 0.9473684430122375)
[2024-11-08 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:15][root][INFO] - Training Epoch: 7/10, step 112/574 completed (loss: 0.0047060418874025345, acc: 1.0)
[2024-11-08 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:16][root][INFO] - Training Epoch: 7/10, step 113/574 completed (loss: 0.015666091814637184, acc: 1.0)
[2024-11-08 04:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:16][root][INFO] - Training Epoch: 7/10, step 114/574 completed (loss: 0.0013765048934146762, acc: 1.0)
[2024-11-08 04:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:17][root][INFO] - Training Epoch: 7/10, step 115/574 completed (loss: 0.047806382179260254, acc: 0.9615384340286255)
[2024-11-08 04:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:17][root][INFO] - Training Epoch: 7/10, step 116/574 completed (loss: 0.06436607241630554, acc: 0.9599999785423279)
[2024-11-08 04:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:18][root][INFO] - Training Epoch: 7/10, step 117/574 completed (loss: 0.034309886395931244, acc: 1.0)
[2024-11-08 04:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:18][root][INFO] - Training Epoch: 7/10, step 118/574 completed (loss: 0.14345724880695343, acc: 0.9473684430122375)
[2024-11-08 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:19][root][INFO] - Training Epoch: 7/10, step 119/574 completed (loss: 0.035419221967458725, acc: 1.0)
[2024-11-08 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:20][root][INFO] - Training Epoch: 7/10, step 120/574 completed (loss: 0.005963629111647606, acc: 1.0)
[2024-11-08 04:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:20][root][INFO] - Training Epoch: 7/10, step 121/574 completed (loss: 0.04205918312072754, acc: 1.0)
[2024-11-08 04:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:21][root][INFO] - Training Epoch: 7/10, step 122/574 completed (loss: 0.001948177465237677, acc: 1.0)
[2024-11-08 04:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:21][root][INFO] - Training Epoch: 7/10, step 123/574 completed (loss: 0.2443491816520691, acc: 0.8846153616905212)
[2024-11-08 04:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:21][root][INFO] - Training Epoch: 7/10, step 124/574 completed (loss: 0.2590906023979187, acc: 0.8999999761581421)
[2024-11-08 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:22][root][INFO] - Training Epoch: 7/10, step 125/574 completed (loss: 0.19501647353172302, acc: 0.949999988079071)
[2024-11-08 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:22][root][INFO] - Training Epoch: 7/10, step 126/574 completed (loss: 0.008015752770006657, acc: 1.0)
[2024-11-08 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:23][root][INFO] - Training Epoch: 7/10, step 127/574 completed (loss: 0.11017724871635437, acc: 0.9545454382896423)
[2024-11-08 04:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:23][root][INFO] - Training Epoch: 7/10, step 128/574 completed (loss: 0.10523448884487152, acc: 0.8999999761581421)
[2024-11-08 04:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:24][root][INFO] - Training Epoch: 7/10, step 129/574 completed (loss: 0.005445975344628096, acc: 1.0)
[2024-11-08 04:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:24][root][INFO] - Training Epoch: 7/10, step 130/574 completed (loss: 0.11809287965297699, acc: 0.939393937587738)
[2024-11-08 04:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:05][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3298, device='cuda:0') eval_epoch_loss=tensor(0.2850, device='cuda:0') eval_epoch_acc=tensor(0.9306, device='cuda:0')
[2024-11-08 04:14:05][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:14:05][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:14:10][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_7_step_131_loss_0.28501802682876587/model.pt
[2024-11-08 04:14:10][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:10][root][INFO] - Training Epoch: 7/10, step 131/574 completed (loss: 0.20080019533634186, acc: 0.9259259104728699)
[2024-11-08 04:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:11][root][INFO] - Training Epoch: 7/10, step 132/574 completed (loss: 0.3031393587589264, acc: 0.939393937587738)
[2024-11-08 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:11][root][INFO] - Training Epoch: 7/10, step 133/574 completed (loss: 0.17748841643333435, acc: 0.949999988079071)
[2024-11-08 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:11][root][INFO] - Training Epoch: 7/10, step 134/574 completed (loss: 0.022447558119893074, acc: 1.0)
[2024-11-08 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:12][root][INFO] - Training Epoch: 7/10, step 135/574 completed (loss: 0.19028189778327942, acc: 0.9473684430122375)
[2024-11-08 04:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:12][root][INFO] - Training Epoch: 7/10, step 136/574 completed (loss: 0.023482544347643852, acc: 1.0)
[2024-11-08 04:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:13][root][INFO] - Training Epoch: 7/10, step 137/574 completed (loss: 0.2510093152523041, acc: 0.8999999761581421)
[2024-11-08 04:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:13][root][INFO] - Training Epoch: 7/10, step 138/574 completed (loss: 0.026762360706925392, acc: 1.0)
[2024-11-08 04:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:13][root][INFO] - Training Epoch: 7/10, step 139/574 completed (loss: 0.06315750628709793, acc: 0.9696969985961914)
[2024-11-08 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:14][root][INFO] - Training Epoch: 7/10, step 140/574 completed (loss: 0.14072266221046448, acc: 0.9629629850387573)
[2024-11-08 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:14][root][INFO] - Training Epoch: 7/10, step 141/574 completed (loss: 0.05272037535905838, acc: 0.9696969985961914)
[2024-11-08 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:14][root][INFO] - Training Epoch: 7/10, step 142/574 completed (loss: 0.049894049763679504, acc: 1.0)
[2024-11-08 04:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:15][root][INFO] - Training Epoch: 7/10, step 143/574 completed (loss: 0.03558831661939621, acc: 1.0)
[2024-11-08 04:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:16][root][INFO] - Training Epoch: 7/10, step 144/574 completed (loss: 0.10948996990919113, acc: 0.9473684430122375)
[2024-11-08 04:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:16][root][INFO] - Training Epoch: 7/10, step 145/574 completed (loss: 0.013940808363258839, acc: 1.0)
[2024-11-08 04:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:17][root][INFO] - Training Epoch: 7/10, step 146/574 completed (loss: 0.035177771002054214, acc: 1.0)
[2024-11-08 04:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:17][root][INFO] - Training Epoch: 7/10, step 147/574 completed (loss: 0.3626630902290344, acc: 0.9090909361839294)
[2024-11-08 04:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:18][root][INFO] - Training Epoch: 7/10, step 148/574 completed (loss: 0.02260538563132286, acc: 1.0)
[2024-11-08 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:18][root][INFO] - Training Epoch: 7/10, step 149/574 completed (loss: 0.04226546362042427, acc: 1.0)
[2024-11-08 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:18][root][INFO] - Training Epoch: 7/10, step 150/574 completed (loss: 0.07808860391378403, acc: 0.9677419066429138)
[2024-11-08 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:19][root][INFO] - Training Epoch: 7/10, step 151/574 completed (loss: 0.022661428898572922, acc: 1.0)
[2024-11-08 04:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:19][root][INFO] - Training Epoch: 7/10, step 152/574 completed (loss: 0.0014514256035909057, acc: 1.0)
[2024-11-08 04:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:20][root][INFO] - Training Epoch: 7/10, step 153/574 completed (loss: 0.0598413459956646, acc: 1.0)
[2024-11-08 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:20][root][INFO] - Training Epoch: 7/10, step 154/574 completed (loss: 0.0016991418087854981, acc: 1.0)
[2024-11-08 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:20][root][INFO] - Training Epoch: 7/10, step 155/574 completed (loss: 0.03384123742580414, acc: 1.0)
[2024-11-08 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:21][root][INFO] - Training Epoch: 7/10, step 156/574 completed (loss: 0.30919668078422546, acc: 0.9090909361839294)
[2024-11-08 04:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:21][root][INFO] - Training Epoch: 7/10, step 157/574 completed (loss: 0.39516568183898926, acc: 0.9090909361839294)
[2024-11-08 04:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:24][root][INFO] - Training Epoch: 7/10, step 158/574 completed (loss: 0.31789523363113403, acc: 0.949999988079071)
[2024-11-08 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:24][root][INFO] - Training Epoch: 7/10, step 159/574 completed (loss: 0.08289356529712677, acc: 1.0)
[2024-11-08 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:25][root][INFO] - Training Epoch: 7/10, step 160/574 completed (loss: 0.10567530989646912, acc: 0.9473684430122375)
[2024-11-08 04:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:25][root][INFO] - Training Epoch: 7/10, step 161/574 completed (loss: 0.11968434602022171, acc: 0.9545454382896423)
[2024-11-08 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:26][root][INFO] - Training Epoch: 7/10, step 162/574 completed (loss: 0.29143813252449036, acc: 0.8999999761581421)
[2024-11-08 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:27][root][INFO] - Training Epoch: 7/10, step 163/574 completed (loss: 0.1042858436703682, acc: 0.9599999785423279)
[2024-11-08 04:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:27][root][INFO] - Training Epoch: 7/10, step 164/574 completed (loss: 0.03817105293273926, acc: 1.0)
[2024-11-08 04:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:27][root][INFO] - Training Epoch: 7/10, step 165/574 completed (loss: 0.15621508657932281, acc: 0.9722222089767456)
[2024-11-08 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:28][root][INFO] - Training Epoch: 7/10, step 166/574 completed (loss: 0.010751323774456978, acc: 1.0)
[2024-11-08 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:28][root][INFO] - Training Epoch: 7/10, step 167/574 completed (loss: 0.06134704127907753, acc: 0.9523809552192688)
[2024-11-08 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:29][root][INFO] - Training Epoch: 7/10, step 168/574 completed (loss: 0.0009143204661086202, acc: 1.0)
[2024-11-08 04:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:29][root][INFO] - Training Epoch: 7/10, step 169/574 completed (loss: 0.03805556520819664, acc: 1.0)
[2024-11-08 04:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:31][root][INFO] - Training Epoch: 7/10, step 170/574 completed (loss: 0.01691373437643051, acc: 1.0)
[2024-11-08 04:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:31][root][INFO] - Training Epoch: 7/10, step 171/574 completed (loss: 0.011324319988489151, acc: 1.0)
[2024-11-08 04:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:32][root][INFO] - Training Epoch: 7/10, step 172/574 completed (loss: 0.0029629101045429707, acc: 1.0)
[2024-11-08 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:32][root][INFO] - Training Epoch: 7/10, step 173/574 completed (loss: 0.1570783108472824, acc: 0.9666666388511658)
[2024-11-08 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:33][root][INFO] - Training Epoch: 7/10, step 174/574 completed (loss: 0.22461307048797607, acc: 0.9523809552192688)
[2024-11-08 04:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:33][root][INFO] - Training Epoch: 7/10, step 175/574 completed (loss: 0.07566843181848526, acc: 1.0)
[2024-11-08 04:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:34][root][INFO] - Training Epoch: 7/10, step 176/574 completed (loss: 0.06173446774482727, acc: 0.9473684430122375)
[2024-11-08 04:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:35][root][INFO] - Training Epoch: 7/10, step 177/574 completed (loss: 0.07049954682588577, acc: 0.9545454382896423)
[2024-11-08 04:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:36][root][INFO] - Training Epoch: 7/10, step 178/574 completed (loss: 0.0815652459859848, acc: 0.9473684430122375)
[2024-11-08 04:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:36][root][INFO] - Training Epoch: 7/10, step 179/574 completed (loss: 0.09056653827428818, acc: 0.9545454382896423)
[2024-11-08 04:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:37][root][INFO] - Training Epoch: 7/10, step 180/574 completed (loss: 0.0006612218567170203, acc: 1.0)
[2024-11-08 04:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:37][root][INFO] - Training Epoch: 7/10, step 181/574 completed (loss: 0.0003665061085484922, acc: 1.0)
[2024-11-08 04:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:38][root][INFO] - Training Epoch: 7/10, step 182/574 completed (loss: 0.03464466333389282, acc: 1.0)
[2024-11-08 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:38][root][INFO] - Training Epoch: 7/10, step 183/574 completed (loss: 0.005682284943759441, acc: 1.0)
[2024-11-08 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:39][root][INFO] - Training Epoch: 7/10, step 184/574 completed (loss: 0.0024121631868183613, acc: 1.0)
[2024-11-08 04:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:39][root][INFO] - Training Epoch: 7/10, step 185/574 completed (loss: 0.0018151121912524104, acc: 1.0)
[2024-11-08 04:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:40][root][INFO] - Training Epoch: 7/10, step 186/574 completed (loss: 0.017488982528448105, acc: 1.0)
[2024-11-08 04:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:40][root][INFO] - Training Epoch: 7/10, step 187/574 completed (loss: 0.01763315126299858, acc: 1.0)
[2024-11-08 04:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:41][root][INFO] - Training Epoch: 7/10, step 188/574 completed (loss: 0.008790748193860054, acc: 1.0)
[2024-11-08 04:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:41][root][INFO] - Training Epoch: 7/10, step 189/574 completed (loss: 0.1874343454837799, acc: 0.9599999785423279)
[2024-11-08 04:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:42][root][INFO] - Training Epoch: 7/10, step 190/574 completed (loss: 0.13361655175685883, acc: 0.949999988079071)
[2024-11-08 04:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:43][root][INFO] - Training Epoch: 7/10, step 191/574 completed (loss: 0.162274569272995, acc: 0.9523809552192688)
[2024-11-08 04:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:45][root][INFO] - Training Epoch: 7/10, step 192/574 completed (loss: 0.2820512354373932, acc: 0.8947368264198303)
[2024-11-08 04:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:46][root][INFO] - Training Epoch: 7/10, step 193/574 completed (loss: 0.05502503737807274, acc: 1.0)
[2024-11-08 04:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:48][root][INFO] - Training Epoch: 7/10, step 194/574 completed (loss: 0.23471219837665558, acc: 0.9523809552192688)
[2024-11-08 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:49][root][INFO] - Training Epoch: 7/10, step 195/574 completed (loss: 0.03907577320933342, acc: 0.9583333134651184)
[2024-11-08 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:50][root][INFO] - Training Epoch: 7/10, step 196/574 completed (loss: 0.001024126075208187, acc: 1.0)
[2024-11-08 04:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:50][root][INFO] - Training Epoch: 7/10, step 197/574 completed (loss: 0.010409733280539513, acc: 1.0)
[2024-11-08 04:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:50][root][INFO] - Training Epoch: 7/10, step 198/574 completed (loss: 0.007388978265225887, acc: 1.0)
[2024-11-08 04:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:51][root][INFO] - Training Epoch: 7/10, step 199/574 completed (loss: 0.009847508743405342, acc: 1.0)
[2024-11-08 04:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:51][root][INFO] - Training Epoch: 7/10, step 200/574 completed (loss: 0.009093096479773521, acc: 1.0)
[2024-11-08 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:52][root][INFO] - Training Epoch: 7/10, step 201/574 completed (loss: 0.09868397563695908, acc: 0.9545454382896423)
[2024-11-08 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:52][root][INFO] - Training Epoch: 7/10, step 202/574 completed (loss: 0.013606865890324116, acc: 1.0)
[2024-11-08 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:53][root][INFO] - Training Epoch: 7/10, step 203/574 completed (loss: 0.01165800355374813, acc: 1.0)
[2024-11-08 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:53][root][INFO] - Training Epoch: 7/10, step 204/574 completed (loss: 0.0074892728589475155, acc: 1.0)
[2024-11-08 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:54][root][INFO] - Training Epoch: 7/10, step 205/574 completed (loss: 0.21506141126155853, acc: 0.9523809552192688)
[2024-11-08 04:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:54][root][INFO] - Training Epoch: 7/10, step 206/574 completed (loss: 0.0430331714451313, acc: 1.0)
[2024-11-08 04:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:54][root][INFO] - Training Epoch: 7/10, step 207/574 completed (loss: 0.023511972278356552, acc: 1.0)
[2024-11-08 04:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:55][root][INFO] - Training Epoch: 7/10, step 208/574 completed (loss: 0.022006353363394737, acc: 1.0)
[2024-11-08 04:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:55][root][INFO] - Training Epoch: 7/10, step 209/574 completed (loss: 0.07169686257839203, acc: 0.9523809552192688)
[2024-11-08 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:56][root][INFO] - Training Epoch: 7/10, step 210/574 completed (loss: 0.02111058123409748, acc: 1.0)
[2024-11-08 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:56][root][INFO] - Training Epoch: 7/10, step 211/574 completed (loss: 0.019939914345741272, acc: 1.0)
[2024-11-08 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:56][root][INFO] - Training Epoch: 7/10, step 212/574 completed (loss: 0.052675772458314896, acc: 0.9677419066429138)
[2024-11-08 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:57][root][INFO] - Training Epoch: 7/10, step 213/574 completed (loss: 0.12249621003866196, acc: 0.9615384340286255)
[2024-11-08 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:58][root][INFO] - Training Epoch: 7/10, step 214/574 completed (loss: 0.0020993889775127172, acc: 1.0)
[2024-11-08 04:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:58][root][INFO] - Training Epoch: 7/10, step 215/574 completed (loss: 0.019864404574036598, acc: 1.0)
[2024-11-08 04:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:14:59][root][INFO] - Training Epoch: 7/10, step 216/574 completed (loss: 0.009023523889482021, acc: 1.0)
[2024-11-08 04:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:00][root][INFO] - Training Epoch: 7/10, step 217/574 completed (loss: 0.001973893726244569, acc: 1.0)
[2024-11-08 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:00][root][INFO] - Training Epoch: 7/10, step 218/574 completed (loss: 0.08456123620271683, acc: 0.9523809552192688)
[2024-11-08 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:00][root][INFO] - Training Epoch: 7/10, step 219/574 completed (loss: 0.015615797601640224, acc: 1.0)
[2024-11-08 04:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:01][root][INFO] - Training Epoch: 7/10, step 220/574 completed (loss: 0.03342590853571892, acc: 1.0)
[2024-11-08 04:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:01][root][INFO] - Training Epoch: 7/10, step 221/574 completed (loss: 0.008763749152421951, acc: 1.0)
[2024-11-08 04:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:02][root][INFO] - Training Epoch: 7/10, step 222/574 completed (loss: 0.020779050886631012, acc: 1.0)
[2024-11-08 04:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:03][root][INFO] - Training Epoch: 7/10, step 223/574 completed (loss: 0.013076229020953178, acc: 1.0)
[2024-11-08 04:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:03][root][INFO] - Training Epoch: 7/10, step 224/574 completed (loss: 0.03670719265937805, acc: 1.0)
[2024-11-08 04:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:04][root][INFO] - Training Epoch: 7/10, step 225/574 completed (loss: 0.012461810372769833, acc: 1.0)
[2024-11-08 04:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:05][root][INFO] - Training Epoch: 7/10, step 226/574 completed (loss: 0.033786725252866745, acc: 1.0)
[2024-11-08 04:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:05][root][INFO] - Training Epoch: 7/10, step 227/574 completed (loss: 0.05107571929693222, acc: 1.0)
[2024-11-08 04:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:05][root][INFO] - Training Epoch: 7/10, step 228/574 completed (loss: 0.3490654528141022, acc: 0.875)
[2024-11-08 04:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:06][root][INFO] - Training Epoch: 7/10, step 229/574 completed (loss: 0.21518155932426453, acc: 0.95652174949646)
[2024-11-08 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:06][root][INFO] - Training Epoch: 7/10, step 230/574 completed (loss: 0.05599595233798027, acc: 1.0)
[2024-11-08 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:07][root][INFO] - Training Epoch: 7/10, step 231/574 completed (loss: 0.05418739095330238, acc: 1.0)
[2024-11-08 04:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:07][root][INFO] - Training Epoch: 7/10, step 232/574 completed (loss: 0.13536767661571503, acc: 0.8947368264198303)
[2024-11-08 04:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:08][root][INFO] - Training Epoch: 7/10, step 233/574 completed (loss: 0.13425347208976746, acc: 0.9090909361839294)
[2024-11-08 04:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:08][root][INFO] - Training Epoch: 7/10, step 234/574 completed (loss: 0.16514836251735687, acc: 0.9090909361839294)
[2024-11-08 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:09][root][INFO] - Training Epoch: 7/10, step 235/574 completed (loss: 0.022562410682439804, acc: 1.0)
[2024-11-08 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:09][root][INFO] - Training Epoch: 7/10, step 236/574 completed (loss: 0.10916315764188766, acc: 0.9629629850387573)
[2024-11-08 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:10][root][INFO] - Training Epoch: 7/10, step 237/574 completed (loss: 0.1403321623802185, acc: 0.939393937587738)
[2024-11-08 04:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:10][root][INFO] - Training Epoch: 7/10, step 238/574 completed (loss: 0.1929343342781067, acc: 0.9545454382896423)
[2024-11-08 04:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:10][root][INFO] - Training Epoch: 7/10, step 239/574 completed (loss: 0.029940256848931313, acc: 1.0)
[2024-11-08 04:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:11][root][INFO] - Training Epoch: 7/10, step 240/574 completed (loss: 0.07378167659044266, acc: 0.9473684430122375)
[2024-11-08 04:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:11][root][INFO] - Training Epoch: 7/10, step 241/574 completed (loss: 0.018831048160791397, acc: 1.0)
[2024-11-08 04:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:12][root][INFO] - Training Epoch: 7/10, step 242/574 completed (loss: 0.16427117586135864, acc: 0.8947368264198303)
[2024-11-08 04:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:13][root][INFO] - Training Epoch: 7/10, step 243/574 completed (loss: 0.040672268718481064, acc: 1.0)
[2024-11-08 04:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:13][root][INFO] - Training Epoch: 7/10, step 244/574 completed (loss: 0.001271720277145505, acc: 1.0)
[2024-11-08 04:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:14][root][INFO] - Training Epoch: 7/10, step 245/574 completed (loss: 0.14761191606521606, acc: 0.9333333373069763)
[2024-11-08 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:14][root][INFO] - Training Epoch: 7/10, step 246/574 completed (loss: 0.0014611012302339077, acc: 1.0)
[2024-11-08 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:14][root][INFO] - Training Epoch: 7/10, step 247/574 completed (loss: 0.006255873013287783, acc: 1.0)
[2024-11-08 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:15][root][INFO] - Training Epoch: 7/10, step 248/574 completed (loss: 0.13908490538597107, acc: 0.9523809552192688)
[2024-11-08 04:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:15][root][INFO] - Training Epoch: 7/10, step 249/574 completed (loss: 0.057308442890644073, acc: 0.9473684430122375)
[2024-11-08 04:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:15][root][INFO] - Training Epoch: 7/10, step 250/574 completed (loss: 0.055252838879823685, acc: 0.949999988079071)
[2024-11-08 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:16][root][INFO] - Training Epoch: 7/10, step 251/574 completed (loss: 0.3088269829750061, acc: 0.9523809552192688)
[2024-11-08 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:16][root][INFO] - Training Epoch: 7/10, step 252/574 completed (loss: 0.0023841143120080233, acc: 1.0)
[2024-11-08 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:17][root][INFO] - Training Epoch: 7/10, step 253/574 completed (loss: 0.01565992459654808, acc: 1.0)
[2024-11-08 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:17][root][INFO] - Training Epoch: 7/10, step 254/574 completed (loss: 0.011153149418532848, acc: 1.0)
[2024-11-08 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:17][root][INFO] - Training Epoch: 7/10, step 255/574 completed (loss: 0.0010854835854843259, acc: 1.0)
[2024-11-08 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:18][root][INFO] - Training Epoch: 7/10, step 256/574 completed (loss: 0.005573953967541456, acc: 1.0)
[2024-11-08 04:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:18][root][INFO] - Training Epoch: 7/10, step 257/574 completed (loss: 0.037374090403318405, acc: 1.0)
[2024-11-08 04:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:18][root][INFO] - Training Epoch: 7/10, step 258/574 completed (loss: 0.0047024209052324295, acc: 1.0)
[2024-11-08 04:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:19][root][INFO] - Training Epoch: 7/10, step 259/574 completed (loss: 0.18288344144821167, acc: 0.9545454382896423)
[2024-11-08 04:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:20][root][INFO] - Training Epoch: 7/10, step 260/574 completed (loss: 0.0012300534872338176, acc: 1.0)
[2024-11-08 04:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:21][root][INFO] - Training Epoch: 7/10, step 261/574 completed (loss: 0.004872343968600035, acc: 1.0)
[2024-11-08 04:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:21][root][INFO] - Training Epoch: 7/10, step 262/574 completed (loss: 0.015457479283213615, acc: 1.0)
[2024-11-08 04:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:21][root][INFO] - Training Epoch: 7/10, step 263/574 completed (loss: 0.007183592300862074, acc: 1.0)
[2024-11-08 04:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:22][root][INFO] - Training Epoch: 7/10, step 264/574 completed (loss: 0.10615935176610947, acc: 0.949999988079071)
[2024-11-08 04:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:23][root][INFO] - Training Epoch: 7/10, step 265/574 completed (loss: 0.06760396808385849, acc: 1.0)
[2024-11-08 04:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:23][root][INFO] - Training Epoch: 7/10, step 266/574 completed (loss: 0.13039004802703857, acc: 0.95652174949646)
[2024-11-08 04:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:24][root][INFO] - Training Epoch: 7/10, step 267/574 completed (loss: 0.045691754668951035, acc: 1.0)
[2024-11-08 04:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:25][root][INFO] - Training Epoch: 7/10, step 268/574 completed (loss: 0.08982521295547485, acc: 0.9655172228813171)
[2024-11-08 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:25][root][INFO] - Training Epoch: 7/10, step 269/574 completed (loss: 0.04919924959540367, acc: 0.9629629850387573)
[2024-11-08 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:25][root][INFO] - Training Epoch: 7/10, step 270/574 completed (loss: 0.14763833582401276, acc: 0.9615384340286255)
[2024-11-08 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:26][root][INFO] - Training Epoch: 7/10, step 271/574 completed (loss: 0.08547283709049225, acc: 0.9523809552192688)
[2024-11-08 04:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:26][root][INFO] - Training Epoch: 7/10, step 272/574 completed (loss: 0.0029107534792274237, acc: 1.0)
[2024-11-08 04:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:26][root][INFO] - Training Epoch: 7/10, step 273/574 completed (loss: 0.0012568766251206398, acc: 1.0)
[2024-11-08 04:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:09][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3834, device='cuda:0') eval_epoch_loss=tensor(0.3245, device='cuda:0') eval_epoch_acc=tensor(0.9345, device='cuda:0')
[2024-11-08 04:16:09][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:16:09][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:16:13][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_7_step_274_loss_0.32451409101486206/model.pt
[2024-11-08 04:16:13][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:13][root][INFO] - Training Epoch: 7/10, step 274/574 completed (loss: 0.03783496841788292, acc: 1.0)
[2024-11-08 04:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:13][root][INFO] - Training Epoch: 7/10, step 275/574 completed (loss: 0.004477610345929861, acc: 1.0)
[2024-11-08 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:14][root][INFO] - Training Epoch: 7/10, step 276/574 completed (loss: 0.04464976117014885, acc: 0.9655172228813171)
[2024-11-08 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:14][root][INFO] - Training Epoch: 7/10, step 277/574 completed (loss: 0.0012796414084732533, acc: 1.0)
[2024-11-08 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:15][root][INFO] - Training Epoch: 7/10, step 278/574 completed (loss: 0.0027551567181944847, acc: 1.0)
[2024-11-08 04:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:15][root][INFO] - Training Epoch: 7/10, step 279/574 completed (loss: 0.0030786653514951468, acc: 1.0)
[2024-11-08 04:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:15][root][INFO] - Training Epoch: 7/10, step 280/574 completed (loss: 0.1406361162662506, acc: 0.8888888955116272)
[2024-11-08 04:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:16][root][INFO] - Training Epoch: 7/10, step 281/574 completed (loss: 0.08186855912208557, acc: 0.9545454382896423)
[2024-11-08 04:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:16][root][INFO] - Training Epoch: 7/10, step 282/574 completed (loss: 0.0034884484484791756, acc: 1.0)
[2024-11-08 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:17][root][INFO] - Training Epoch: 7/10, step 283/574 completed (loss: 0.007349250838160515, acc: 1.0)
[2024-11-08 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:17][root][INFO] - Training Epoch: 7/10, step 284/574 completed (loss: 0.2761358916759491, acc: 0.8999999761581421)
[2024-11-08 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:18][root][INFO] - Training Epoch: 7/10, step 285/574 completed (loss: 0.3969758152961731, acc: 0.8965517282485962)
[2024-11-08 04:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:18][root][INFO] - Training Epoch: 7/10, step 286/574 completed (loss: 0.0174102820456028, acc: 1.0)
[2024-11-08 04:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:19][root][INFO] - Training Epoch: 7/10, step 287/574 completed (loss: 0.003970184363424778, acc: 1.0)
[2024-11-08 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:19][root][INFO] - Training Epoch: 7/10, step 288/574 completed (loss: 0.00897352397441864, acc: 1.0)
[2024-11-08 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:19][root][INFO] - Training Epoch: 7/10, step 289/574 completed (loss: 0.40164193511009216, acc: 0.9545454382896423)
[2024-11-08 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:20][root][INFO] - Training Epoch: 7/10, step 290/574 completed (loss: 0.04791032150387764, acc: 1.0)
[2024-11-08 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:20][root][INFO] - Training Epoch: 7/10, step 291/574 completed (loss: 0.0038166248705238104, acc: 1.0)
[2024-11-08 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:21][root][INFO] - Training Epoch: 7/10, step 292/574 completed (loss: 0.0541379414498806, acc: 0.9666666388511658)
[2024-11-08 04:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:21][root][INFO] - Training Epoch: 7/10, step 293/574 completed (loss: 0.12169001251459122, acc: 0.9523809552192688)
[2024-11-08 04:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:22][root][INFO] - Training Epoch: 7/10, step 294/574 completed (loss: 0.008337803184986115, acc: 1.0)
[2024-11-08 04:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:23][root][INFO] - Training Epoch: 7/10, step 295/574 completed (loss: 0.45705780386924744, acc: 0.8421052694320679)
[2024-11-08 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:23][root][INFO] - Training Epoch: 7/10, step 296/574 completed (loss: 0.0846128910779953, acc: 0.9090909361839294)
[2024-11-08 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:24][root][INFO] - Training Epoch: 7/10, step 297/574 completed (loss: 0.05769408494234085, acc: 1.0)
[2024-11-08 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:24][root][INFO] - Training Epoch: 7/10, step 298/574 completed (loss: 0.06330423057079315, acc: 1.0)
[2024-11-08 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:24][root][INFO] - Training Epoch: 7/10, step 299/574 completed (loss: 0.03732125461101532, acc: 0.9677419066429138)
[2024-11-08 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:25][root][INFO] - Training Epoch: 7/10, step 300/574 completed (loss: 0.027649251744151115, acc: 1.0)
[2024-11-08 04:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:25][root][INFO] - Training Epoch: 7/10, step 301/574 completed (loss: 0.12480980902910233, acc: 0.9230769276618958)
[2024-11-08 04:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:25][root][INFO] - Training Epoch: 7/10, step 302/574 completed (loss: 0.07712002843618393, acc: 0.9523809552192688)
[2024-11-08 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:26][root][INFO] - Training Epoch: 7/10, step 303/574 completed (loss: 0.0036392011679708958, acc: 1.0)
[2024-11-08 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:26][root][INFO] - Training Epoch: 7/10, step 304/574 completed (loss: 0.007414941675961018, acc: 1.0)
[2024-11-08 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:27][root][INFO] - Training Epoch: 7/10, step 305/574 completed (loss: 0.05357583239674568, acc: 0.95652174949646)
[2024-11-08 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:27][root][INFO] - Training Epoch: 7/10, step 306/574 completed (loss: 0.030421216040849686, acc: 1.0)
[2024-11-08 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:27][root][INFO] - Training Epoch: 7/10, step 307/574 completed (loss: 0.019805951043963432, acc: 1.0)
[2024-11-08 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:28][root][INFO] - Training Epoch: 7/10, step 308/574 completed (loss: 0.014409530907869339, acc: 1.0)
[2024-11-08 04:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:28][root][INFO] - Training Epoch: 7/10, step 309/574 completed (loss: 0.21662774682044983, acc: 0.8947368264198303)
[2024-11-08 04:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:29][root][INFO] - Training Epoch: 7/10, step 310/574 completed (loss: 0.31321418285369873, acc: 0.9473684430122375)
[2024-11-08 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:29][root][INFO] - Training Epoch: 7/10, step 311/574 completed (loss: 0.022537710145115852, acc: 1.0)
[2024-11-08 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:29][root][INFO] - Training Epoch: 7/10, step 312/574 completed (loss: 0.04945439472794533, acc: 1.0)
[2024-11-08 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:30][root][INFO] - Training Epoch: 7/10, step 313/574 completed (loss: 0.024652039632201195, acc: 1.0)
[2024-11-08 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:30][root][INFO] - Training Epoch: 7/10, step 314/574 completed (loss: 0.13368409872055054, acc: 0.9629629850387573)
[2024-11-08 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:30][root][INFO] - Training Epoch: 7/10, step 315/574 completed (loss: 0.06954172253608704, acc: 0.9714285731315613)
[2024-11-08 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:31][root][INFO] - Training Epoch: 7/10, step 316/574 completed (loss: 0.09429128468036652, acc: 0.9615384340286255)
[2024-11-08 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:31][root][INFO] - Training Epoch: 7/10, step 317/574 completed (loss: 0.0036358919460326433, acc: 1.0)
[2024-11-08 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:32][root][INFO] - Training Epoch: 7/10, step 318/574 completed (loss: 0.04689009115099907, acc: 1.0)
[2024-11-08 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:32][root][INFO] - Training Epoch: 7/10, step 319/574 completed (loss: 0.011020591482520103, acc: 1.0)
[2024-11-08 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:32][root][INFO] - Training Epoch: 7/10, step 320/574 completed (loss: 0.015649545937776566, acc: 1.0)
[2024-11-08 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:33][root][INFO] - Training Epoch: 7/10, step 321/574 completed (loss: 0.007892348803579807, acc: 1.0)
[2024-11-08 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:33][root][INFO] - Training Epoch: 7/10, step 322/574 completed (loss: 0.2840977907180786, acc: 0.9130434989929199)
[2024-11-08 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:33][root][INFO] - Training Epoch: 7/10, step 323/574 completed (loss: 0.12534448504447937, acc: 0.9523809552192688)
[2024-11-08 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:34][root][INFO] - Training Epoch: 7/10, step 324/574 completed (loss: 0.16533982753753662, acc: 0.8947368264198303)
[2024-11-08 04:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:34][root][INFO] - Training Epoch: 7/10, step 325/574 completed (loss: 0.1549091935157776, acc: 0.9545454382896423)
[2024-11-08 04:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:35][root][INFO] - Training Epoch: 7/10, step 326/574 completed (loss: 0.4797838628292084, acc: 0.8421052694320679)
[2024-11-08 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:35][root][INFO] - Training Epoch: 7/10, step 327/574 completed (loss: 0.1587040275335312, acc: 0.9166666865348816)
[2024-11-08 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:35][root][INFO] - Training Epoch: 7/10, step 328/574 completed (loss: 0.3316972553730011, acc: 0.931034505367279)
[2024-11-08 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:36][root][INFO] - Training Epoch: 7/10, step 329/574 completed (loss: 0.011193484999239445, acc: 1.0)
[2024-11-08 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:36][root][INFO] - Training Epoch: 7/10, step 330/574 completed (loss: 0.011891807429492474, acc: 1.0)
[2024-11-08 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:36][root][INFO] - Training Epoch: 7/10, step 331/574 completed (loss: 0.00549117149785161, acc: 1.0)
[2024-11-08 04:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:37][root][INFO] - Training Epoch: 7/10, step 332/574 completed (loss: 0.023409835994243622, acc: 1.0)
[2024-11-08 04:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:37][root][INFO] - Training Epoch: 7/10, step 333/574 completed (loss: 0.10989827662706375, acc: 0.9473684430122375)
[2024-11-08 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:38][root][INFO] - Training Epoch: 7/10, step 334/574 completed (loss: 0.051321517676115036, acc: 0.9545454382896423)
[2024-11-08 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:38][root][INFO] - Training Epoch: 7/10, step 335/574 completed (loss: 0.061165984719991684, acc: 0.9615384340286255)
[2024-11-08 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:39][root][INFO] - Training Epoch: 7/10, step 336/574 completed (loss: 0.2572791278362274, acc: 0.8333333134651184)
[2024-11-08 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:39][root][INFO] - Training Epoch: 7/10, step 337/574 completed (loss: 0.19286905229091644, acc: 0.9523809552192688)
[2024-11-08 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:39][root][INFO] - Training Epoch: 7/10, step 338/574 completed (loss: 0.10861381143331528, acc: 0.95652174949646)
[2024-11-08 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:40][root][INFO] - Training Epoch: 7/10, step 339/574 completed (loss: 0.0288752019405365, acc: 1.0)
[2024-11-08 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:40][root][INFO] - Training Epoch: 7/10, step 340/574 completed (loss: 0.0026993684004992247, acc: 1.0)
[2024-11-08 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:40][root][INFO] - Training Epoch: 7/10, step 341/574 completed (loss: 0.003970570396631956, acc: 1.0)
[2024-11-08 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:41][root][INFO] - Training Epoch: 7/10, step 342/574 completed (loss: 0.01290431059896946, acc: 1.0)
[2024-11-08 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:41][root][INFO] - Training Epoch: 7/10, step 343/574 completed (loss: 0.139908105134964, acc: 0.949999988079071)
[2024-11-08 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:42][root][INFO] - Training Epoch: 7/10, step 344/574 completed (loss: 0.0036699038464576006, acc: 1.0)
[2024-11-08 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:42][root][INFO] - Training Epoch: 7/10, step 345/574 completed (loss: 0.12462437152862549, acc: 0.9545454382896423)
[2024-11-08 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:42][root][INFO] - Training Epoch: 7/10, step 346/574 completed (loss: 0.057298291474580765, acc: 1.0)
[2024-11-08 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:43][root][INFO] - Training Epoch: 7/10, step 347/574 completed (loss: 0.05886739119887352, acc: 0.9583333134651184)
[2024-11-08 04:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:43][root][INFO] - Training Epoch: 7/10, step 348/574 completed (loss: 0.010272661224007607, acc: 1.0)
[2024-11-08 04:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:44][root][INFO] - Training Epoch: 7/10, step 349/574 completed (loss: 0.11145986616611481, acc: 0.949999988079071)
[2024-11-08 04:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:44][root][INFO] - Training Epoch: 7/10, step 350/574 completed (loss: 0.07230477035045624, acc: 1.0)
[2024-11-08 04:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:45][root][INFO] - Training Epoch: 7/10, step 351/574 completed (loss: 0.050815414637327194, acc: 0.9545454382896423)
[2024-11-08 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:45][root][INFO] - Training Epoch: 7/10, step 352/574 completed (loss: 0.2079465091228485, acc: 0.949999988079071)
[2024-11-08 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:45][root][INFO] - Training Epoch: 7/10, step 353/574 completed (loss: 0.10087039321660995, acc: 0.9599999785423279)
[2024-11-08 04:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:46][root][INFO] - Training Epoch: 7/10, step 354/574 completed (loss: 0.06829439848661423, acc: 0.9599999785423279)
[2024-11-08 04:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:46][root][INFO] - Training Epoch: 7/10, step 355/574 completed (loss: 0.274030864238739, acc: 0.9523809552192688)
[2024-11-08 04:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:47][root][INFO] - Training Epoch: 7/10, step 356/574 completed (loss: 0.11371459811925888, acc: 0.949999988079071)
[2024-11-08 04:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:47][root][INFO] - Training Epoch: 7/10, step 357/574 completed (loss: 0.1371065378189087, acc: 0.9523809552192688)
[2024-11-08 04:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:48][root][INFO] - Training Epoch: 7/10, step 358/574 completed (loss: 0.03946372866630554, acc: 1.0)
[2024-11-08 04:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:48][root][INFO] - Training Epoch: 7/10, step 359/574 completed (loss: 0.12352706491947174, acc: 0.9642857313156128)
[2024-11-08 04:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:49][root][INFO] - Training Epoch: 7/10, step 360/574 completed (loss: 0.010805698111653328, acc: 1.0)
[2024-11-08 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:49][root][INFO] - Training Epoch: 7/10, step 361/574 completed (loss: 0.1515044867992401, acc: 0.9428571462631226)
[2024-11-08 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:49][root][INFO] - Training Epoch: 7/10, step 362/574 completed (loss: 0.17949070036411285, acc: 0.9545454382896423)
[2024-11-08 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:50][root][INFO] - Training Epoch: 7/10, step 363/574 completed (loss: 0.002752165775746107, acc: 1.0)
[2024-11-08 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:50][root][INFO] - Training Epoch: 7/10, step 364/574 completed (loss: 0.020728005096316338, acc: 1.0)
[2024-11-08 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:50][root][INFO] - Training Epoch: 7/10, step 365/574 completed (loss: 0.06431540846824646, acc: 0.9473684430122375)
[2024-11-08 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:51][root][INFO] - Training Epoch: 7/10, step 366/574 completed (loss: 0.04986552149057388, acc: 0.9545454382896423)
[2024-11-08 04:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:51][root][INFO] - Training Epoch: 7/10, step 367/574 completed (loss: 0.19762907922267914, acc: 0.9629629850387573)
[2024-11-08 04:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:51][root][INFO] - Training Epoch: 7/10, step 368/574 completed (loss: 0.03725164756178856, acc: 0.96875)
[2024-11-08 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:52][root][INFO] - Training Epoch: 7/10, step 369/574 completed (loss: 0.02322332002222538, acc: 1.0)
[2024-11-08 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:53][root][INFO] - Training Epoch: 7/10, step 370/574 completed (loss: 0.054651372134685516, acc: 1.0)
[2024-11-08 04:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:54][root][INFO] - Training Epoch: 7/10, step 371/574 completed (loss: 0.19502101838588715, acc: 0.9473684430122375)
[2024-11-08 04:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:54][root][INFO] - Training Epoch: 7/10, step 372/574 completed (loss: 0.10841061919927597, acc: 0.9090909361839294)
[2024-11-08 04:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:55][root][INFO] - Training Epoch: 7/10, step 373/574 completed (loss: 0.00847491156309843, acc: 1.0)
[2024-11-08 04:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:55][root][INFO] - Training Epoch: 7/10, step 374/574 completed (loss: 0.03458002582192421, acc: 1.0)
[2024-11-08 04:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:56][root][INFO] - Training Epoch: 7/10, step 375/574 completed (loss: 0.014155499637126923, acc: 1.0)
[2024-11-08 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:56][root][INFO] - Training Epoch: 7/10, step 376/574 completed (loss: 0.0071879499591887, acc: 1.0)
[2024-11-08 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:56][root][INFO] - Training Epoch: 7/10, step 377/574 completed (loss: 0.003343353047966957, acc: 1.0)
[2024-11-08 04:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:57][root][INFO] - Training Epoch: 7/10, step 378/574 completed (loss: 0.2596057057380676, acc: 0.9523809552192688)
[2024-11-08 04:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:58][root][INFO] - Training Epoch: 7/10, step 379/574 completed (loss: 0.01084053609520197, acc: 1.0)
[2024-11-08 04:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:16:58][root][INFO] - Training Epoch: 7/10, step 380/574 completed (loss: 0.12131217122077942, acc: 0.9545454382896423)
[2024-11-08 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:00][root][INFO] - Training Epoch: 7/10, step 381/574 completed (loss: 0.010104344226419926, acc: 1.0)
[2024-11-08 04:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:01][root][INFO] - Training Epoch: 7/10, step 382/574 completed (loss: 0.029248416423797607, acc: 1.0)
[2024-11-08 04:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:01][root][INFO] - Training Epoch: 7/10, step 383/574 completed (loss: 0.02381640300154686, acc: 1.0)
[2024-11-08 04:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:01][root][INFO] - Training Epoch: 7/10, step 384/574 completed (loss: 0.06807374954223633, acc: 0.9666666388511658)
[2024-11-08 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:02][root][INFO] - Training Epoch: 7/10, step 385/574 completed (loss: 0.001303170109167695, acc: 1.0)
[2024-11-08 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:02][root][INFO] - Training Epoch: 7/10, step 386/574 completed (loss: 0.006085800938308239, acc: 1.0)
[2024-11-08 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:03][root][INFO] - Training Epoch: 7/10, step 387/574 completed (loss: 0.0010751361260190606, acc: 1.0)
[2024-11-08 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:03][root][INFO] - Training Epoch: 7/10, step 388/574 completed (loss: 0.005515750031918287, acc: 1.0)
[2024-11-08 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:03][root][INFO] - Training Epoch: 7/10, step 389/574 completed (loss: 0.0018190644914284348, acc: 1.0)
[2024-11-08 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:04][root][INFO] - Training Epoch: 7/10, step 390/574 completed (loss: 0.04972540959715843, acc: 0.9545454382896423)
[2024-11-08 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:04][root][INFO] - Training Epoch: 7/10, step 391/574 completed (loss: 0.055016033351421356, acc: 1.0)
[2024-11-08 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:04][root][INFO] - Training Epoch: 7/10, step 392/574 completed (loss: 0.039295174181461334, acc: 1.0)
[2024-11-08 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:05][root][INFO] - Training Epoch: 7/10, step 393/574 completed (loss: 0.000594829092733562, acc: 1.0)
[2024-11-08 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:06][root][INFO] - Training Epoch: 7/10, step 394/574 completed (loss: 0.19906778633594513, acc: 0.9090909361839294)
[2024-11-08 04:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:06][root][INFO] - Training Epoch: 7/10, step 395/574 completed (loss: 0.04104255884885788, acc: 1.0)
[2024-11-08 04:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:06][root][INFO] - Training Epoch: 7/10, step 396/574 completed (loss: 0.008775890804827213, acc: 1.0)
[2024-11-08 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:07][root][INFO] - Training Epoch: 7/10, step 397/574 completed (loss: 0.17347171902656555, acc: 0.9333333373069763)
[2024-11-08 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:07][root][INFO] - Training Epoch: 7/10, step 398/574 completed (loss: 0.037235043942928314, acc: 1.0)
[2024-11-08 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:08][root][INFO] - Training Epoch: 7/10, step 399/574 completed (loss: 0.007643071934580803, acc: 1.0)
[2024-11-08 04:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:08][root][INFO] - Training Epoch: 7/10, step 400/574 completed (loss: 0.011248305439949036, acc: 1.0)
[2024-11-08 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:09][root][INFO] - Training Epoch: 7/10, step 401/574 completed (loss: 0.16086305677890778, acc: 0.9523809552192688)
[2024-11-08 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:09][root][INFO] - Training Epoch: 7/10, step 402/574 completed (loss: 0.012720312923192978, acc: 1.0)
[2024-11-08 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:10][root][INFO] - Training Epoch: 7/10, step 403/574 completed (loss: 0.02671801671385765, acc: 0.96875)
[2024-11-08 04:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:10][root][INFO] - Training Epoch: 7/10, step 404/574 completed (loss: 0.06804682314395905, acc: 0.9714285731315613)
[2024-11-08 04:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:10][root][INFO] - Training Epoch: 7/10, step 405/574 completed (loss: 0.004759965930134058, acc: 1.0)
[2024-11-08 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:11][root][INFO] - Training Epoch: 7/10, step 406/574 completed (loss: 0.04815823584794998, acc: 0.9523809552192688)
[2024-11-08 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:11][root][INFO] - Training Epoch: 7/10, step 407/574 completed (loss: 0.030313963070511818, acc: 1.0)
[2024-11-08 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:12][root][INFO] - Training Epoch: 7/10, step 408/574 completed (loss: 0.002977609634399414, acc: 1.0)
[2024-11-08 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:12][root][INFO] - Training Epoch: 7/10, step 409/574 completed (loss: 0.11480624228715897, acc: 0.9523809552192688)
[2024-11-08 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:12][root][INFO] - Training Epoch: 7/10, step 410/574 completed (loss: 0.004958466161042452, acc: 1.0)
[2024-11-08 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:13][root][INFO] - Training Epoch: 7/10, step 411/574 completed (loss: 0.042060889303684235, acc: 1.0)
[2024-11-08 04:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:13][root][INFO] - Training Epoch: 7/10, step 412/574 completed (loss: 0.0011678999289870262, acc: 1.0)
[2024-11-08 04:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:13][root][INFO] - Training Epoch: 7/10, step 413/574 completed (loss: 0.0078094517812132835, acc: 1.0)
[2024-11-08 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:14][root][INFO] - Training Epoch: 7/10, step 414/574 completed (loss: 0.0022565037943422794, acc: 1.0)
[2024-11-08 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:14][root][INFO] - Training Epoch: 7/10, step 415/574 completed (loss: 0.01724080555140972, acc: 1.0)
[2024-11-08 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:14][root][INFO] - Training Epoch: 7/10, step 416/574 completed (loss: 0.0043850732035934925, acc: 1.0)
[2024-11-08 04:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:17:56][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3448, device='cuda:0') eval_epoch_loss=tensor(0.2963, device='cuda:0') eval_epoch_acc=tensor(0.9318, device='cuda:0')
[2024-11-08 04:17:56][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:17:56][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:18:00][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_7_step_417_loss_0.29627588391304016/model.pt
[2024-11-08 04:18:00][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:00][root][INFO] - Training Epoch: 7/10, step 417/574 completed (loss: 0.01470929104834795, acc: 1.0)
[2024-11-08 04:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:01][root][INFO] - Training Epoch: 7/10, step 418/574 completed (loss: 0.002865390619263053, acc: 1.0)
[2024-11-08 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:01][root][INFO] - Training Epoch: 7/10, step 419/574 completed (loss: 0.16035854816436768, acc: 0.9523809552192688)
[2024-11-08 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:01][root][INFO] - Training Epoch: 7/10, step 420/574 completed (loss: 0.054593298584222794, acc: 1.0)
[2024-11-08 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:02][root][INFO] - Training Epoch: 7/10, step 421/574 completed (loss: 0.025173751637339592, acc: 1.0)
[2024-11-08 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:02][root][INFO] - Training Epoch: 7/10, step 422/574 completed (loss: 0.18071778118610382, acc: 0.9354838728904724)
[2024-11-08 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:02][root][INFO] - Training Epoch: 7/10, step 423/574 completed (loss: 0.013163534924387932, acc: 1.0)
[2024-11-08 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:03][root][INFO] - Training Epoch: 7/10, step 424/574 completed (loss: 0.09928270429372787, acc: 0.9523809552192688)
[2024-11-08 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:03][root][INFO] - Training Epoch: 7/10, step 425/574 completed (loss: 0.003612530417740345, acc: 1.0)
[2024-11-08 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:04][root][INFO] - Training Epoch: 7/10, step 426/574 completed (loss: 0.1874643713235855, acc: 0.9473684430122375)
[2024-11-08 04:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:04][root][INFO] - Training Epoch: 7/10, step 427/574 completed (loss: 0.03505392372608185, acc: 1.0)
[2024-11-08 04:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:04][root][INFO] - Training Epoch: 7/10, step 428/574 completed (loss: 0.005680442787706852, acc: 1.0)
[2024-11-08 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:05][root][INFO] - Training Epoch: 7/10, step 429/574 completed (loss: 0.037111394107341766, acc: 0.9583333134651184)
[2024-11-08 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:05][root][INFO] - Training Epoch: 7/10, step 430/574 completed (loss: 0.0005033178022131324, acc: 1.0)
[2024-11-08 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:06][root][INFO] - Training Epoch: 7/10, step 431/574 completed (loss: 0.004306438844650984, acc: 1.0)
[2024-11-08 04:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:06][root][INFO] - Training Epoch: 7/10, step 432/574 completed (loss: 0.001466232817620039, acc: 1.0)
[2024-11-08 04:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:06][root][INFO] - Training Epoch: 7/10, step 433/574 completed (loss: 0.004301312379539013, acc: 1.0)
[2024-11-08 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:07][root][INFO] - Training Epoch: 7/10, step 434/574 completed (loss: 0.00259550497867167, acc: 1.0)
[2024-11-08 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:07][root][INFO] - Training Epoch: 7/10, step 435/574 completed (loss: 0.013122295029461384, acc: 1.0)
[2024-11-08 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:08][root][INFO] - Training Epoch: 7/10, step 436/574 completed (loss: 0.004107191227376461, acc: 1.0)
[2024-11-08 04:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:08][root][INFO] - Training Epoch: 7/10, step 437/574 completed (loss: 0.0005902870325371623, acc: 1.0)
[2024-11-08 04:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:09][root][INFO] - Training Epoch: 7/10, step 438/574 completed (loss: 0.04034629091620445, acc: 1.0)
[2024-11-08 04:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:09][root][INFO] - Training Epoch: 7/10, step 439/574 completed (loss: 0.008706102147698402, acc: 1.0)
[2024-11-08 04:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:10][root][INFO] - Training Epoch: 7/10, step 440/574 completed (loss: 0.03791150450706482, acc: 1.0)
[2024-11-08 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:11][root][INFO] - Training Epoch: 7/10, step 441/574 completed (loss: 0.21113957464694977, acc: 0.9473684430122375)
[2024-11-08 04:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:11][root][INFO] - Training Epoch: 7/10, step 442/574 completed (loss: 0.21605971455574036, acc: 0.8947368264198303)
[2024-11-08 04:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:12][root][INFO] - Training Epoch: 7/10, step 443/574 completed (loss: 0.06422588229179382, acc: 0.9545454382896423)
[2024-11-08 04:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:13][root][INFO] - Training Epoch: 7/10, step 444/574 completed (loss: 0.01835579425096512, acc: 1.0)
[2024-11-08 04:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:13][root][INFO] - Training Epoch: 7/10, step 445/574 completed (loss: 0.026105089113116264, acc: 1.0)
[2024-11-08 04:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:14][root][INFO] - Training Epoch: 7/10, step 446/574 completed (loss: 0.1395871490240097, acc: 0.9677419066429138)
[2024-11-08 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:14][root][INFO] - Training Epoch: 7/10, step 447/574 completed (loss: 0.1223742738366127, acc: 0.9677419066429138)
[2024-11-08 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:14][root][INFO] - Training Epoch: 7/10, step 448/574 completed (loss: 0.002399849472567439, acc: 1.0)
[2024-11-08 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:15][root][INFO] - Training Epoch: 7/10, step 449/574 completed (loss: 0.004210476763546467, acc: 1.0)
[2024-11-08 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:15][root][INFO] - Training Epoch: 7/10, step 450/574 completed (loss: 0.00023053295444697142, acc: 1.0)
[2024-11-08 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:16][root][INFO] - Training Epoch: 7/10, step 451/574 completed (loss: 0.02930993214249611, acc: 1.0)
[2024-11-08 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:16][root][INFO] - Training Epoch: 7/10, step 452/574 completed (loss: 0.0015514534898102283, acc: 1.0)
[2024-11-08 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:17][root][INFO] - Training Epoch: 7/10, step 453/574 completed (loss: 0.4362577795982361, acc: 0.8999999761581421)
[2024-11-08 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:17][root][INFO] - Training Epoch: 7/10, step 454/574 completed (loss: 0.005967490375041962, acc: 1.0)
[2024-11-08 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:17][root][INFO] - Training Epoch: 7/10, step 455/574 completed (loss: 0.24578818678855896, acc: 0.9677419066429138)
[2024-11-08 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:18][root][INFO] - Training Epoch: 7/10, step 456/574 completed (loss: 0.035239920020103455, acc: 1.0)
[2024-11-08 04:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:18][root][INFO] - Training Epoch: 7/10, step 457/574 completed (loss: 0.0016279083210974932, acc: 1.0)
[2024-11-08 04:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:19][root][INFO] - Training Epoch: 7/10, step 458/574 completed (loss: 0.007110870908945799, acc: 1.0)
[2024-11-08 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:19][root][INFO] - Training Epoch: 7/10, step 459/574 completed (loss: 0.09641670435667038, acc: 0.9090909361839294)
[2024-11-08 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:19][root][INFO] - Training Epoch: 7/10, step 460/574 completed (loss: 0.008129911497235298, acc: 1.0)
[2024-11-08 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:20][root][INFO] - Training Epoch: 7/10, step 461/574 completed (loss: 0.000842514680698514, acc: 1.0)
[2024-11-08 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:20][root][INFO] - Training Epoch: 7/10, step 462/574 completed (loss: 0.05603179335594177, acc: 0.9696969985961914)
[2024-11-08 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:21][root][INFO] - Training Epoch: 7/10, step 463/574 completed (loss: 0.02535644732415676, acc: 1.0)
[2024-11-08 04:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:21][root][INFO] - Training Epoch: 7/10, step 464/574 completed (loss: 0.001344130840152502, acc: 1.0)
[2024-11-08 04:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:21][root][INFO] - Training Epoch: 7/10, step 465/574 completed (loss: 0.00756014883518219, acc: 1.0)
[2024-11-08 04:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:22][root][INFO] - Training Epoch: 7/10, step 466/574 completed (loss: 0.002704496495425701, acc: 1.0)
[2024-11-08 04:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:22][root][INFO] - Training Epoch: 7/10, step 467/574 completed (loss: 0.003946094773709774, acc: 1.0)
[2024-11-08 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:23][root][INFO] - Training Epoch: 7/10, step 468/574 completed (loss: 0.0012527127983048558, acc: 1.0)
[2024-11-08 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:23][root][INFO] - Training Epoch: 7/10, step 469/574 completed (loss: 0.0006145888473838568, acc: 1.0)
[2024-11-08 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:24][root][INFO] - Training Epoch: 7/10, step 470/574 completed (loss: 0.0065908064134418964, acc: 1.0)
[2024-11-08 04:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:24][root][INFO] - Training Epoch: 7/10, step 471/574 completed (loss: 0.0027721200603991747, acc: 1.0)
[2024-11-08 04:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:24][root][INFO] - Training Epoch: 7/10, step 472/574 completed (loss: 0.25272831320762634, acc: 0.875)
[2024-11-08 04:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:25][root][INFO] - Training Epoch: 7/10, step 473/574 completed (loss: 0.029893815517425537, acc: 1.0)
[2024-11-08 04:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:25][root][INFO] - Training Epoch: 7/10, step 474/574 completed (loss: 0.06687770038843155, acc: 1.0)
[2024-11-08 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:26][root][INFO] - Training Epoch: 7/10, step 475/574 completed (loss: 0.17867228388786316, acc: 0.949999988079071)
[2024-11-08 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:26][root][INFO] - Training Epoch: 7/10, step 476/574 completed (loss: 0.006845978554338217, acc: 1.0)
[2024-11-08 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:27][root][INFO] - Training Epoch: 7/10, step 477/574 completed (loss: 0.060078904032707214, acc: 1.0)
[2024-11-08 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:27][root][INFO] - Training Epoch: 7/10, step 478/574 completed (loss: 0.0025926746893674135, acc: 1.0)
[2024-11-08 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:28][root][INFO] - Training Epoch: 7/10, step 479/574 completed (loss: 0.002643587999045849, acc: 1.0)
[2024-11-08 04:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:28][root][INFO] - Training Epoch: 7/10, step 480/574 completed (loss: 0.08341848105192184, acc: 0.9354838728904724)
[2024-11-08 04:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:28][root][INFO] - Training Epoch: 7/10, step 481/574 completed (loss: 0.1393873393535614, acc: 0.9047619104385376)
[2024-11-08 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:29][root][INFO] - Training Epoch: 7/10, step 482/574 completed (loss: 0.08378155529499054, acc: 0.9473684430122375)
[2024-11-08 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:29][root][INFO] - Training Epoch: 7/10, step 483/574 completed (loss: 0.16959500312805176, acc: 0.9090909361839294)
[2024-11-08 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:30][root][INFO] - Training Epoch: 7/10, step 484/574 completed (loss: 0.007181164808571339, acc: 1.0)
[2024-11-08 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:30][root][INFO] - Training Epoch: 7/10, step 485/574 completed (loss: 0.007981999777257442, acc: 1.0)
[2024-11-08 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:31][root][INFO] - Training Epoch: 7/10, step 486/574 completed (loss: 0.03295419365167618, acc: 1.0)
[2024-11-08 04:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:31][root][INFO] - Training Epoch: 7/10, step 487/574 completed (loss: 0.05966360867023468, acc: 0.9714285731315613)
[2024-11-08 04:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:31][root][INFO] - Training Epoch: 7/10, step 488/574 completed (loss: 0.08126167207956314, acc: 0.9615384340286255)
[2024-11-08 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:32][root][INFO] - Training Epoch: 7/10, step 489/574 completed (loss: 0.07606367021799088, acc: 0.9523809552192688)
[2024-11-08 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:32][root][INFO] - Training Epoch: 7/10, step 490/574 completed (loss: 0.015212650410830975, acc: 1.0)
[2024-11-08 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:33][root][INFO] - Training Epoch: 7/10, step 491/574 completed (loss: 0.026667797937989235, acc: 1.0)
[2024-11-08 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:33][root][INFO] - Training Epoch: 7/10, step 492/574 completed (loss: 0.0022724608425050974, acc: 1.0)
[2024-11-08 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:33][root][INFO] - Training Epoch: 7/10, step 493/574 completed (loss: 0.0036694665905088186, acc: 1.0)
[2024-11-08 04:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:34][root][INFO] - Training Epoch: 7/10, step 494/574 completed (loss: 0.26874613761901855, acc: 0.9166666865348816)
[2024-11-08 04:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:34][root][INFO] - Training Epoch: 7/10, step 495/574 completed (loss: 0.1532590240240097, acc: 0.9642857313156128)
[2024-11-08 04:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:35][root][INFO] - Training Epoch: 7/10, step 496/574 completed (loss: 0.036915890872478485, acc: 1.0)
[2024-11-08 04:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:35][root][INFO] - Training Epoch: 7/10, step 497/574 completed (loss: 0.06961014121770859, acc: 0.949999988079071)
[2024-11-08 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:36][root][INFO] - Training Epoch: 7/10, step 498/574 completed (loss: 0.03843079134821892, acc: 1.0)
[2024-11-08 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:36][root][INFO] - Training Epoch: 7/10, step 499/574 completed (loss: 0.0011846399866044521, acc: 1.0)
[2024-11-08 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:37][root][INFO] - Training Epoch: 7/10, step 500/574 completed (loss: 0.02040679194033146, acc: 1.0)
[2024-11-08 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:37][root][INFO] - Training Epoch: 7/10, step 501/574 completed (loss: 0.001868602936156094, acc: 1.0)
[2024-11-08 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:37][root][INFO] - Training Epoch: 7/10, step 502/574 completed (loss: 0.0004658836987800896, acc: 1.0)
[2024-11-08 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:38][root][INFO] - Training Epoch: 7/10, step 503/574 completed (loss: 0.016776638105511665, acc: 1.0)
[2024-11-08 04:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:38][root][INFO] - Training Epoch: 7/10, step 504/574 completed (loss: 0.023989135399460793, acc: 1.0)
[2024-11-08 04:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:38][root][INFO] - Training Epoch: 7/10, step 505/574 completed (loss: 0.005907710641622543, acc: 1.0)
[2024-11-08 04:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:39][root][INFO] - Training Epoch: 7/10, step 506/574 completed (loss: 0.18946720659732819, acc: 0.9473684430122375)
[2024-11-08 04:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:40][root][INFO] - Training Epoch: 7/10, step 507/574 completed (loss: 0.0025266557931900024, acc: 1.0)
[2024-11-08 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:40][root][INFO] - Training Epoch: 7/10, step 508/574 completed (loss: 0.12404876947402954, acc: 0.949999988079071)
[2024-11-08 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:41][root][INFO] - Training Epoch: 7/10, step 509/574 completed (loss: 0.31530138850212097, acc: 0.9090909361839294)
[2024-11-08 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:41][root][INFO] - Training Epoch: 7/10, step 510/574 completed (loss: 0.013591507449746132, acc: 1.0)
[2024-11-08 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:42][root][INFO] - Training Epoch: 7/10, step 511/574 completed (loss: 0.17832276225090027, acc: 0.9666666388511658)
[2024-11-08 04:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:45][root][INFO] - Training Epoch: 7/10, step 512/574 completed (loss: 0.3054288923740387, acc: 0.8928571343421936)
[2024-11-08 04:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:46][root][INFO] - Training Epoch: 7/10, step 513/574 completed (loss: 0.04860101267695427, acc: 0.9523809552192688)
[2024-11-08 04:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:47][root][INFO] - Training Epoch: 7/10, step 514/574 completed (loss: 0.25178343057632446, acc: 0.9473684430122375)
[2024-11-08 04:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:47][root][INFO] - Training Epoch: 7/10, step 515/574 completed (loss: 0.002633277792483568, acc: 1.0)
[2024-11-08 04:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:48][root][INFO] - Training Epoch: 7/10, step 516/574 completed (loss: 0.2892259955406189, acc: 0.9523809552192688)
[2024-11-08 04:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:49][root][INFO] - Training Epoch: 7/10, step 517/574 completed (loss: 0.012211752124130726, acc: 1.0)
[2024-11-08 04:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:49][root][INFO] - Training Epoch: 7/10, step 518/574 completed (loss: 0.03812602534890175, acc: 0.9642857313156128)
[2024-11-08 04:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:49][root][INFO] - Training Epoch: 7/10, step 519/574 completed (loss: 0.0942750945687294, acc: 0.9629629850387573)
[2024-11-08 04:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:50][root][INFO] - Training Epoch: 7/10, step 520/574 completed (loss: 0.2130965143442154, acc: 0.9142857193946838)
[2024-11-08 04:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:51][root][INFO] - Training Epoch: 7/10, step 521/574 completed (loss: 0.17185457050800323, acc: 0.95652174949646)
[2024-11-08 04:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:52][root][INFO] - Training Epoch: 7/10, step 522/574 completed (loss: 0.013865597546100616, acc: 1.0)
[2024-11-08 04:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:52][root][INFO] - Training Epoch: 7/10, step 523/574 completed (loss: 0.022385017946362495, acc: 1.0)
[2024-11-08 04:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:53][root][INFO] - Training Epoch: 7/10, step 524/574 completed (loss: 0.3530588746070862, acc: 0.8636363744735718)
[2024-11-08 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:53][root][INFO] - Training Epoch: 7/10, step 525/574 completed (loss: 0.05439625307917595, acc: 0.949999988079071)
[2024-11-08 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:54][root][INFO] - Training Epoch: 7/10, step 526/574 completed (loss: 0.0757509395480156, acc: 1.0)
[2024-11-08 04:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:54][root][INFO] - Training Epoch: 7/10, step 527/574 completed (loss: 0.0841158926486969, acc: 0.9696969985961914)
[2024-11-08 04:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:54][root][INFO] - Training Epoch: 7/10, step 528/574 completed (loss: 0.11717313528060913, acc: 0.9285714030265808)
[2024-11-08 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:55][root][INFO] - Training Epoch: 7/10, step 529/574 completed (loss: 0.030106117948889732, acc: 1.0)
[2024-11-08 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:55][root][INFO] - Training Epoch: 7/10, step 530/574 completed (loss: 0.3162791132926941, acc: 0.8947368264198303)
[2024-11-08 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:56][root][INFO] - Training Epoch: 7/10, step 531/574 completed (loss: 0.05876075103878975, acc: 1.0)
[2024-11-08 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:56][root][INFO] - Training Epoch: 7/10, step 532/574 completed (loss: 0.17644241452217102, acc: 0.9047619104385376)
[2024-11-08 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:56][root][INFO] - Training Epoch: 7/10, step 533/574 completed (loss: 0.0441771000623703, acc: 1.0)
[2024-11-08 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:57][root][INFO] - Training Epoch: 7/10, step 534/574 completed (loss: 0.058795906603336334, acc: 0.9642857313156128)
[2024-11-08 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:57][root][INFO] - Training Epoch: 7/10, step 535/574 completed (loss: 0.023836739361286163, acc: 1.0)
[2024-11-08 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:58][root][INFO] - Training Epoch: 7/10, step 536/574 completed (loss: 0.01368320919573307, acc: 1.0)
[2024-11-08 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:58][root][INFO] - Training Epoch: 7/10, step 537/574 completed (loss: 0.008219925686717033, acc: 1.0)
[2024-11-08 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:58][root][INFO] - Training Epoch: 7/10, step 538/574 completed (loss: 0.08753911405801773, acc: 1.0)
[2024-11-08 04:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:59][root][INFO] - Training Epoch: 7/10, step 539/574 completed (loss: 0.13024863600730896, acc: 0.949999988079071)
[2024-11-08 04:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:18:59][root][INFO] - Training Epoch: 7/10, step 540/574 completed (loss: 0.07101744413375854, acc: 1.0)
[2024-11-08 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:00][root][INFO] - Training Epoch: 7/10, step 541/574 completed (loss: 0.034416742622852325, acc: 1.0)
[2024-11-08 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:00][root][INFO] - Training Epoch: 7/10, step 542/574 completed (loss: 0.0020611174404621124, acc: 1.0)
[2024-11-08 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:01][root][INFO] - Training Epoch: 7/10, step 543/574 completed (loss: 0.0023039476945996284, acc: 1.0)
[2024-11-08 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:01][root][INFO] - Training Epoch: 7/10, step 544/574 completed (loss: 0.005279428791254759, acc: 1.0)
[2024-11-08 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:01][root][INFO] - Training Epoch: 7/10, step 545/574 completed (loss: 0.006945070810616016, acc: 1.0)
[2024-11-08 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:02][root][INFO] - Training Epoch: 7/10, step 546/574 completed (loss: 0.000592318014241755, acc: 1.0)
[2024-11-08 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:02][root][INFO] - Training Epoch: 7/10, step 547/574 completed (loss: 0.08039122819900513, acc: 0.9523809552192688)
[2024-11-08 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:02][root][INFO] - Training Epoch: 7/10, step 548/574 completed (loss: 0.016759829595685005, acc: 1.0)
[2024-11-08 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:03][root][INFO] - Training Epoch: 7/10, step 549/574 completed (loss: 0.005688217002898455, acc: 1.0)
[2024-11-08 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:03][root][INFO] - Training Epoch: 7/10, step 550/574 completed (loss: 0.001488995272666216, acc: 1.0)
[2024-11-08 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:03][root][INFO] - Training Epoch: 7/10, step 551/574 completed (loss: 0.004196980502456427, acc: 1.0)
[2024-11-08 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:04][root][INFO] - Training Epoch: 7/10, step 552/574 completed (loss: 0.0006403929437510669, acc: 1.0)
[2024-11-08 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:04][root][INFO] - Training Epoch: 7/10, step 553/574 completed (loss: 0.020108995959162712, acc: 1.0)
[2024-11-08 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:04][root][INFO] - Training Epoch: 7/10, step 554/574 completed (loss: 0.0018313071923330426, acc: 1.0)
[2024-11-08 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:05][root][INFO] - Training Epoch: 7/10, step 555/574 completed (loss: 0.009824692271649837, acc: 1.0)
[2024-11-08 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:05][root][INFO] - Training Epoch: 7/10, step 556/574 completed (loss: 0.00041306563070975244, acc: 1.0)
[2024-11-08 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:05][root][INFO] - Training Epoch: 7/10, step 557/574 completed (loss: 0.0883110761642456, acc: 0.9545454382896423)
[2024-11-08 04:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:06][root][INFO] - Training Epoch: 7/10, step 558/574 completed (loss: 0.01872420683503151, acc: 1.0)
[2024-11-08 04:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:06][root][INFO] - Training Epoch: 7/10, step 559/574 completed (loss: 0.0012082952307537198, acc: 1.0)
[2024-11-08 04:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:48][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3182, device='cuda:0') eval_epoch_loss=tensor(0.2762, device='cuda:0') eval_epoch_acc=tensor(0.9270, device='cuda:0')
[2024-11-08 04:19:48][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:19:48][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:19:52][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_7_step_560_loss_0.2762417495250702/model.pt
[2024-11-08 04:19:52][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:52][root][INFO] - Training Epoch: 7/10, step 560/574 completed (loss: 0.011931807734072208, acc: 1.0)
[2024-11-08 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:52][root][INFO] - Training Epoch: 7/10, step 561/574 completed (loss: 0.04014137014746666, acc: 1.0)
[2024-11-08 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:53][root][INFO] - Training Epoch: 7/10, step 562/574 completed (loss: 0.008671963587403297, acc: 1.0)
[2024-11-08 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:53][root][INFO] - Training Epoch: 7/10, step 563/574 completed (loss: 0.0151662677526474, acc: 1.0)
[2024-11-08 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:54][root][INFO] - Training Epoch: 7/10, step 564/574 completed (loss: 0.0010708236368373036, acc: 1.0)
[2024-11-08 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:54][root][INFO] - Training Epoch: 7/10, step 565/574 completed (loss: 0.023434214293956757, acc: 1.0)
[2024-11-08 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:54][root][INFO] - Training Epoch: 7/10, step 566/574 completed (loss: 0.0028275849763303995, acc: 1.0)
[2024-11-08 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:55][root][INFO] - Training Epoch: 7/10, step 567/574 completed (loss: 0.004226382821798325, acc: 1.0)
[2024-11-08 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:55][root][INFO] - Training Epoch: 7/10, step 568/574 completed (loss: 0.004797609522938728, acc: 1.0)
[2024-11-08 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:55][root][INFO] - Training Epoch: 7/10, step 569/574 completed (loss: 0.0014176142867654562, acc: 1.0)
[2024-11-08 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:56][root][INFO] - Training Epoch: 7/10, step 570/574 completed (loss: 0.0008390884613618255, acc: 1.0)
[2024-11-08 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:56][root][INFO] - Training Epoch: 7/10, step 571/574 completed (loss: 0.15121956169605255, acc: 0.9473684430122375)
[2024-11-08 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:56][root][INFO] - Training Epoch: 7/10, step 572/574 completed (loss: 0.038934849202632904, acc: 1.0)
[2024-11-08 04:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:57][root][INFO] - Training Epoch: 7/10, step 573/574 completed (loss: 0.004507369361817837, acc: 1.0)
[2024-11-08 04:19:57][slam_llm.utils.train_utils][INFO] - Epoch 7: train_perplexity=1.0794, train_epoch_loss=0.0764, epoch time 461.3544965106994s
[2024-11-08 04:19:57][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 04:19:57][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2024-11-08 04:19:57][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 04:19:57][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2024-11-08 04:19:57][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:58][root][INFO] - Training Epoch: 8/10, step 0/574 completed (loss: 0.004973430186510086, acc: 1.0)
[2024-11-08 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:59][root][INFO] - Training Epoch: 8/10, step 1/574 completed (loss: 0.02657429315149784, acc: 0.9629629850387573)
[2024-11-08 04:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:59][root][INFO] - Training Epoch: 8/10, step 2/574 completed (loss: 0.015911724418401718, acc: 1.0)
[2024-11-08 04:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:19:59][root][INFO] - Training Epoch: 8/10, step 3/574 completed (loss: 0.03085695579648018, acc: 1.0)
[2024-11-08 04:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:00][root][INFO] - Training Epoch: 8/10, step 4/574 completed (loss: 0.03409443795681, acc: 1.0)
[2024-11-08 04:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:00][root][INFO] - Training Epoch: 8/10, step 5/574 completed (loss: 0.009252977557480335, acc: 1.0)
[2024-11-08 04:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:01][root][INFO] - Training Epoch: 8/10, step 6/574 completed (loss: 0.004303367808461189, acc: 1.0)
[2024-11-08 04:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:01][root][INFO] - Training Epoch: 8/10, step 7/574 completed (loss: 0.01090200711041689, acc: 1.0)
[2024-11-08 04:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:01][root][INFO] - Training Epoch: 8/10, step 8/574 completed (loss: 0.018848510459065437, acc: 1.0)
[2024-11-08 04:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:02][root][INFO] - Training Epoch: 8/10, step 9/574 completed (loss: 0.006654801312834024, acc: 1.0)
[2024-11-08 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:02][root][INFO] - Training Epoch: 8/10, step 10/574 completed (loss: 0.0024470200296491385, acc: 1.0)
[2024-11-08 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:02][root][INFO] - Training Epoch: 8/10, step 11/574 completed (loss: 0.032163724303245544, acc: 1.0)
[2024-11-08 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:03][root][INFO] - Training Epoch: 8/10, step 12/574 completed (loss: 0.0006729998858645558, acc: 1.0)
[2024-11-08 04:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:03][root][INFO] - Training Epoch: 8/10, step 13/574 completed (loss: 0.04591239243745804, acc: 1.0)
[2024-11-08 04:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:03][root][INFO] - Training Epoch: 8/10, step 14/574 completed (loss: 0.06489342451095581, acc: 0.95652174949646)
[2024-11-08 04:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:04][root][INFO] - Training Epoch: 8/10, step 15/574 completed (loss: 0.004826858639717102, acc: 1.0)
[2024-11-08 04:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:04][root][INFO] - Training Epoch: 8/10, step 16/574 completed (loss: 0.08333796262741089, acc: 0.9642857313156128)
[2024-11-08 04:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:05][root][INFO] - Training Epoch: 8/10, step 17/574 completed (loss: 0.02832481637597084, acc: 1.0)
[2024-11-08 04:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:05][root][INFO] - Training Epoch: 8/10, step 18/574 completed (loss: 0.008185413666069508, acc: 1.0)
[2024-11-08 04:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:06][root][INFO] - Training Epoch: 8/10, step 19/574 completed (loss: 0.14876693487167358, acc: 0.9473684430122375)
[2024-11-08 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:06][root][INFO] - Training Epoch: 8/10, step 20/574 completed (loss: 0.06840389966964722, acc: 0.9473684430122375)
[2024-11-08 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:07][root][INFO] - Training Epoch: 8/10, step 21/574 completed (loss: 0.015068700537085533, acc: 1.0)
[2024-11-08 04:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:07][root][INFO] - Training Epoch: 8/10, step 22/574 completed (loss: 0.03833107277750969, acc: 1.0)
[2024-11-08 04:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:07][root][INFO] - Training Epoch: 8/10, step 23/574 completed (loss: 0.0006189647247083485, acc: 1.0)
[2024-11-08 04:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:08][root][INFO] - Training Epoch: 8/10, step 24/574 completed (loss: 0.005191747564822435, acc: 1.0)
[2024-11-08 04:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:08][root][INFO] - Training Epoch: 8/10, step 25/574 completed (loss: 0.19442172348499298, acc: 0.9047619104385376)
[2024-11-08 04:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:09][root][INFO] - Training Epoch: 8/10, step 26/574 completed (loss: 0.07862760871648788, acc: 1.0)
[2024-11-08 04:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:10][root][INFO] - Training Epoch: 8/10, step 27/574 completed (loss: 0.03139244019985199, acc: 1.0)
[2024-11-08 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:11][root][INFO] - Training Epoch: 8/10, step 28/574 completed (loss: 0.0037157342303544283, acc: 1.0)
[2024-11-08 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:11][root][INFO] - Training Epoch: 8/10, step 29/574 completed (loss: 0.060791026800870895, acc: 0.949999988079071)
[2024-11-08 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:12][root][INFO] - Training Epoch: 8/10, step 30/574 completed (loss: 0.0012039270950481296, acc: 1.0)
[2024-11-08 04:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:12][root][INFO] - Training Epoch: 8/10, step 31/574 completed (loss: 0.011968162842094898, acc: 1.0)
[2024-11-08 04:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:12][root][INFO] - Training Epoch: 8/10, step 32/574 completed (loss: 0.06818526238203049, acc: 0.9696969985961914)
[2024-11-08 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:13][root][INFO] - Training Epoch: 8/10, step 33/574 completed (loss: 0.0021870345808565617, acc: 1.0)
[2024-11-08 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:13][root][INFO] - Training Epoch: 8/10, step 34/574 completed (loss: 0.007384599652141333, acc: 1.0)
[2024-11-08 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:14][root][INFO] - Training Epoch: 8/10, step 35/574 completed (loss: 0.0019070780836045742, acc: 1.0)
[2024-11-08 04:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:14][root][INFO] - Training Epoch: 8/10, step 36/574 completed (loss: 0.015008571557700634, acc: 1.0)
[2024-11-08 04:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:15][root][INFO] - Training Epoch: 8/10, step 37/574 completed (loss: 0.07148591428995132, acc: 0.9523809552192688)
[2024-11-08 04:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:15][root][INFO] - Training Epoch: 8/10, step 38/574 completed (loss: 0.0007915672031231225, acc: 1.0)
[2024-11-08 04:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:15][root][INFO] - Training Epoch: 8/10, step 39/574 completed (loss: 0.0558464340865612, acc: 0.9642857313156128)
[2024-11-08 04:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:16][root][INFO] - Training Epoch: 8/10, step 40/574 completed (loss: 0.21772658824920654, acc: 0.9142857193946838)
[2024-11-08 04:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:16][root][INFO] - Training Epoch: 8/10, step 41/574 completed (loss: 0.2751556634902954, acc: 0.949999988079071)
[2024-11-08 04:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:17][root][INFO] - Training Epoch: 8/10, step 42/574 completed (loss: 0.02389875054359436, acc: 1.0)
[2024-11-08 04:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:17][root][INFO] - Training Epoch: 8/10, step 43/574 completed (loss: 0.17079444229602814, acc: 0.8947368264198303)
[2024-11-08 04:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:18][root][INFO] - Training Epoch: 8/10, step 44/574 completed (loss: 0.02039717137813568, acc: 1.0)
[2024-11-08 04:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:19][root][INFO] - Training Epoch: 8/10, step 45/574 completed (loss: 0.0115824518725276, acc: 1.0)
[2024-11-08 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:19][root][INFO] - Training Epoch: 8/10, step 46/574 completed (loss: 0.07650268822908401, acc: 0.9583333134651184)
[2024-11-08 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:19][root][INFO] - Training Epoch: 8/10, step 47/574 completed (loss: 0.0006551245460286736, acc: 1.0)
[2024-11-08 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:20][root][INFO] - Training Epoch: 8/10, step 48/574 completed (loss: 0.007601891178637743, acc: 1.0)
[2024-11-08 04:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:20][root][INFO] - Training Epoch: 8/10, step 49/574 completed (loss: 0.0010457991156727076, acc: 1.0)
[2024-11-08 04:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:21][root][INFO] - Training Epoch: 8/10, step 50/574 completed (loss: 0.3959767818450928, acc: 0.9523809552192688)
[2024-11-08 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:21][root][INFO] - Training Epoch: 8/10, step 51/574 completed (loss: 0.024466050788760185, acc: 1.0)
[2024-11-08 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:21][root][INFO] - Training Epoch: 8/10, step 52/574 completed (loss: 0.11560319364070892, acc: 0.949999988079071)
[2024-11-08 04:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:22][root][INFO] - Training Epoch: 8/10, step 53/574 completed (loss: 0.002485796110704541, acc: 1.0)
[2024-11-08 04:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:22][root][INFO] - Training Epoch: 8/10, step 54/574 completed (loss: 0.2368592619895935, acc: 0.9545454382896423)
[2024-11-08 04:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:23][root][INFO] - Training Epoch: 8/10, step 55/574 completed (loss: 0.001734356046654284, acc: 1.0)
[2024-11-08 04:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:27][root][INFO] - Training Epoch: 8/10, step 56/574 completed (loss: 0.0036091392394155264, acc: 1.0)
[2024-11-08 04:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:29][root][INFO] - Training Epoch: 8/10, step 57/574 completed (loss: 0.2689889371395111, acc: 0.9047619104385376)
[2024-11-08 04:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:30][root][INFO] - Training Epoch: 8/10, step 58/574 completed (loss: 0.004332647658884525, acc: 1.0)
[2024-11-08 04:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:31][root][INFO] - Training Epoch: 8/10, step 59/574 completed (loss: 0.017433257773518562, acc: 1.0)
[2024-11-08 04:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:31][root][INFO] - Training Epoch: 8/10, step 60/574 completed (loss: 0.05174008756875992, acc: 1.0)
[2024-11-08 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:32][root][INFO] - Training Epoch: 8/10, step 61/574 completed (loss: 0.012636159546673298, acc: 1.0)
[2024-11-08 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:32][root][INFO] - Training Epoch: 8/10, step 62/574 completed (loss: 0.21764150261878967, acc: 0.9722222089767456)
[2024-11-08 04:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:33][root][INFO] - Training Epoch: 8/10, step 63/574 completed (loss: 0.16938960552215576, acc: 0.9642857313156128)
[2024-11-08 04:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:33][root][INFO] - Training Epoch: 8/10, step 64/574 completed (loss: 0.005521445535123348, acc: 1.0)
[2024-11-08 04:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:34][root][INFO] - Training Epoch: 8/10, step 65/574 completed (loss: 0.00650415662676096, acc: 1.0)
[2024-11-08 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:34][root][INFO] - Training Epoch: 8/10, step 66/574 completed (loss: 0.13812761008739471, acc: 0.9473684430122375)
[2024-11-08 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:34][root][INFO] - Training Epoch: 8/10, step 67/574 completed (loss: 0.06808143109083176, acc: 0.949999988079071)
[2024-11-08 04:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:35][root][INFO] - Training Epoch: 8/10, step 68/574 completed (loss: 0.0027826642617583275, acc: 1.0)
[2024-11-08 04:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:35][root][INFO] - Training Epoch: 8/10, step 69/574 completed (loss: 0.12899652123451233, acc: 0.9696969985961914)
[2024-11-08 04:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:36][root][INFO] - Training Epoch: 8/10, step 70/574 completed (loss: 0.04606429859995842, acc: 0.9629629850387573)
[2024-11-08 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:36][root][INFO] - Training Epoch: 8/10, step 71/574 completed (loss: 0.18331554532051086, acc: 0.9666666388511658)
[2024-11-08 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:36][root][INFO] - Training Epoch: 8/10, step 72/574 completed (loss: 0.03410344570875168, acc: 1.0)
[2024-11-08 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:37][root][INFO] - Training Epoch: 8/10, step 73/574 completed (loss: 0.0026046528946608305, acc: 1.0)
[2024-11-08 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:37][root][INFO] - Training Epoch: 8/10, step 74/574 completed (loss: 0.0025970283895730972, acc: 1.0)
[2024-11-08 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:38][root][INFO] - Training Epoch: 8/10, step 75/574 completed (loss: 0.01642775535583496, acc: 1.0)
[2024-11-08 04:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:38][root][INFO] - Training Epoch: 8/10, step 76/574 completed (loss: 0.025560641661286354, acc: 1.0)
[2024-11-08 04:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:39][root][INFO] - Training Epoch: 8/10, step 77/574 completed (loss: 0.01823490671813488, acc: 1.0)
[2024-11-08 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:39][root][INFO] - Training Epoch: 8/10, step 78/574 completed (loss: 0.03423011675477028, acc: 1.0)
[2024-11-08 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:39][root][INFO] - Training Epoch: 8/10, step 79/574 completed (loss: 0.025019114837050438, acc: 1.0)
[2024-11-08 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:40][root][INFO] - Training Epoch: 8/10, step 80/574 completed (loss: 0.01589050516486168, acc: 1.0)
[2024-11-08 04:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:40][root][INFO] - Training Epoch: 8/10, step 81/574 completed (loss: 0.31162476539611816, acc: 0.9523809552192688)
[2024-11-08 04:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:40][root][INFO] - Training Epoch: 8/10, step 82/574 completed (loss: 0.004524415358901024, acc: 1.0)
[2024-11-08 04:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:41][root][INFO] - Training Epoch: 8/10, step 83/574 completed (loss: 0.012762583792209625, acc: 1.0)
[2024-11-08 04:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:41][root][INFO] - Training Epoch: 8/10, step 84/574 completed (loss: 0.005088247358798981, acc: 1.0)
[2024-11-08 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:42][root][INFO] - Training Epoch: 8/10, step 85/574 completed (loss: 0.00309767690487206, acc: 1.0)
[2024-11-08 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:42][root][INFO] - Training Epoch: 8/10, step 86/574 completed (loss: 0.006979431491345167, acc: 1.0)
[2024-11-08 04:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:43][root][INFO] - Training Epoch: 8/10, step 87/574 completed (loss: 0.1304214894771576, acc: 0.9615384340286255)
[2024-11-08 04:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:43][root][INFO] - Training Epoch: 8/10, step 88/574 completed (loss: 0.006593816448003054, acc: 1.0)
[2024-11-08 04:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:45][root][INFO] - Training Epoch: 8/10, step 89/574 completed (loss: 0.5032036304473877, acc: 0.8947368264198303)
[2024-11-08 04:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:46][root][INFO] - Training Epoch: 8/10, step 90/574 completed (loss: 0.04261043667793274, acc: 1.0)
[2024-11-08 04:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:47][root][INFO] - Training Epoch: 8/10, step 91/574 completed (loss: 0.15878605842590332, acc: 0.9545454382896423)
[2024-11-08 04:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:49][root][INFO] - Training Epoch: 8/10, step 92/574 completed (loss: 0.03845791518688202, acc: 1.0)
[2024-11-08 04:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:50][root][INFO] - Training Epoch: 8/10, step 93/574 completed (loss: 0.16775304079055786, acc: 0.9230769276618958)
[2024-11-08 04:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:51][root][INFO] - Training Epoch: 8/10, step 94/574 completed (loss: 0.08604636043310165, acc: 0.9642857313156128)
[2024-11-08 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:51][root][INFO] - Training Epoch: 8/10, step 95/574 completed (loss: 0.0936536192893982, acc: 0.9047619104385376)
[2024-11-08 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:51][root][INFO] - Training Epoch: 8/10, step 96/574 completed (loss: 0.010545574128627777, acc: 1.0)
[2024-11-08 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:52][root][INFO] - Training Epoch: 8/10, step 97/574 completed (loss: 0.10603034496307373, acc: 0.949999988079071)
[2024-11-08 04:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:52][root][INFO] - Training Epoch: 8/10, step 98/574 completed (loss: 0.03400009498000145, acc: 1.0)
[2024-11-08 04:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:53][root][INFO] - Training Epoch: 8/10, step 99/574 completed (loss: 0.0637013390660286, acc: 0.9545454382896423)
[2024-11-08 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:53][root][INFO] - Training Epoch: 8/10, step 100/574 completed (loss: 0.034249309450387955, acc: 1.0)
[2024-11-08 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:53][root][INFO] - Training Epoch: 8/10, step 101/574 completed (loss: 0.020369045436382294, acc: 1.0)
[2024-11-08 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:54][root][INFO] - Training Epoch: 8/10, step 102/574 completed (loss: 0.003230116795748472, acc: 1.0)
[2024-11-08 04:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:54][root][INFO] - Training Epoch: 8/10, step 103/574 completed (loss: 0.002660430036485195, acc: 1.0)
[2024-11-08 04:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:54][root][INFO] - Training Epoch: 8/10, step 104/574 completed (loss: 0.0368223637342453, acc: 1.0)
[2024-11-08 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:55][root][INFO] - Training Epoch: 8/10, step 105/574 completed (loss: 0.2646653354167938, acc: 0.8947368264198303)
[2024-11-08 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:55][root][INFO] - Training Epoch: 8/10, step 106/574 completed (loss: 0.10801710933446884, acc: 0.9545454382896423)
[2024-11-08 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:56][root][INFO] - Training Epoch: 8/10, step 107/574 completed (loss: 0.002368653891608119, acc: 1.0)
[2024-11-08 04:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:56][root][INFO] - Training Epoch: 8/10, step 108/574 completed (loss: 0.004340505693107843, acc: 1.0)
[2024-11-08 04:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:56][root][INFO] - Training Epoch: 8/10, step 109/574 completed (loss: 0.03159873187541962, acc: 1.0)
[2024-11-08 04:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:57][root][INFO] - Training Epoch: 8/10, step 110/574 completed (loss: 0.2687879204750061, acc: 0.9523809552192688)
[2024-11-08 04:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:57][root][INFO] - Training Epoch: 8/10, step 111/574 completed (loss: 0.128327414393425, acc: 0.9473684430122375)
[2024-11-08 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:58][root][INFO] - Training Epoch: 8/10, step 112/574 completed (loss: 0.006254452746361494, acc: 1.0)
[2024-11-08 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:58][root][INFO] - Training Epoch: 8/10, step 113/574 completed (loss: 0.0010720612481236458, acc: 1.0)
[2024-11-08 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:59][root][INFO] - Training Epoch: 8/10, step 114/574 completed (loss: 0.0006808103644289076, acc: 1.0)
[2024-11-08 04:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:20:59][root][INFO] - Training Epoch: 8/10, step 115/574 completed (loss: 0.005735471379011869, acc: 1.0)
[2024-11-08 04:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:00][root][INFO] - Training Epoch: 8/10, step 116/574 completed (loss: 0.02432691864669323, acc: 1.0)
[2024-11-08 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:00][root][INFO] - Training Epoch: 8/10, step 117/574 completed (loss: 0.10928231477737427, acc: 0.949999988079071)
[2024-11-08 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:00][root][INFO] - Training Epoch: 8/10, step 118/574 completed (loss: 0.02517816424369812, acc: 1.0)
[2024-11-08 04:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:01][root][INFO] - Training Epoch: 8/10, step 119/574 completed (loss: 0.03072626143693924, acc: 1.0)
[2024-11-08 04:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:02][root][INFO] - Training Epoch: 8/10, step 120/574 completed (loss: 0.00244214478880167, acc: 1.0)
[2024-11-08 04:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:02][root][INFO] - Training Epoch: 8/10, step 121/574 completed (loss: 0.02748998999595642, acc: 1.0)
[2024-11-08 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:03][root][INFO] - Training Epoch: 8/10, step 122/574 completed (loss: 0.018077118322253227, acc: 1.0)
[2024-11-08 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:03][root][INFO] - Training Epoch: 8/10, step 123/574 completed (loss: 0.4256313443183899, acc: 0.9230769276618958)
[2024-11-08 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:04][root][INFO] - Training Epoch: 8/10, step 124/574 completed (loss: 0.031694360077381134, acc: 1.0)
[2024-11-08 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:04][root][INFO] - Training Epoch: 8/10, step 125/574 completed (loss: 0.08493977040052414, acc: 0.949999988079071)
[2024-11-08 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:04][root][INFO] - Training Epoch: 8/10, step 126/574 completed (loss: 0.0751071497797966, acc: 1.0)
[2024-11-08 04:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:05][root][INFO] - Training Epoch: 8/10, step 127/574 completed (loss: 0.06993991881608963, acc: 1.0)
[2024-11-08 04:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:05][root][INFO] - Training Epoch: 8/10, step 128/574 completed (loss: 0.06868283450603485, acc: 1.0)
[2024-11-08 04:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:48][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3060, device='cuda:0') eval_epoch_loss=tensor(0.2670, device='cuda:0') eval_epoch_acc=tensor(0.9293, device='cuda:0')
[2024-11-08 04:21:48][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:21:48][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:21:52][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_8_step_129_loss_0.26698097586631775/model.pt
[2024-11-08 04:21:52][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:52][root][INFO] - Training Epoch: 8/10, step 129/574 completed (loss: 0.0033926290925592184, acc: 1.0)
[2024-11-08 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:53][root][INFO] - Training Epoch: 8/10, step 130/574 completed (loss: 0.13455002009868622, acc: 0.9696969985961914)
[2024-11-08 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:53][root][INFO] - Training Epoch: 8/10, step 131/574 completed (loss: 0.0616716705262661, acc: 0.9629629850387573)
[2024-11-08 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:53][root][INFO] - Training Epoch: 8/10, step 132/574 completed (loss: 0.1902545690536499, acc: 0.939393937587738)
[2024-11-08 04:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:54][root][INFO] - Training Epoch: 8/10, step 133/574 completed (loss: 0.031251467764377594, acc: 1.0)
[2024-11-08 04:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:54][root][INFO] - Training Epoch: 8/10, step 134/574 completed (loss: 0.09993910789489746, acc: 0.949999988079071)
[2024-11-08 04:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:55][root][INFO] - Training Epoch: 8/10, step 135/574 completed (loss: 0.043338313698768616, acc: 1.0)
[2024-11-08 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:55][root][INFO] - Training Epoch: 8/10, step 136/574 completed (loss: 0.0004805934731848538, acc: 1.0)
[2024-11-08 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:55][root][INFO] - Training Epoch: 8/10, step 137/574 completed (loss: 0.3884056508541107, acc: 0.8999999761581421)
[2024-11-08 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:56][root][INFO] - Training Epoch: 8/10, step 138/574 completed (loss: 0.0844646617770195, acc: 0.9545454382896423)
[2024-11-08 04:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:56][root][INFO] - Training Epoch: 8/10, step 139/574 completed (loss: 0.0034386743791401386, acc: 1.0)
[2024-11-08 04:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:56][root][INFO] - Training Epoch: 8/10, step 140/574 completed (loss: 0.026692315936088562, acc: 1.0)
[2024-11-08 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:57][root][INFO] - Training Epoch: 8/10, step 141/574 completed (loss: 0.012917142361402512, acc: 1.0)
[2024-11-08 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:57][root][INFO] - Training Epoch: 8/10, step 142/574 completed (loss: 0.1596764475107193, acc: 0.8999999761581421)
[2024-11-08 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:58][root][INFO] - Training Epoch: 8/10, step 143/574 completed (loss: 0.05084723234176636, acc: 0.949999988079071)
[2024-11-08 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:58][root][INFO] - Training Epoch: 8/10, step 144/574 completed (loss: 0.0037033911794424057, acc: 1.0)
[2024-11-08 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:59][root][INFO] - Training Epoch: 8/10, step 145/574 completed (loss: 0.10916206985712051, acc: 0.9545454382896423)
[2024-11-08 04:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:21:59][root][INFO] - Training Epoch: 8/10, step 146/574 completed (loss: 0.49932998418807983, acc: 0.8999999761581421)
[2024-11-08 04:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:00][root][INFO] - Training Epoch: 8/10, step 147/574 completed (loss: 0.07193473726511002, acc: 0.9545454382896423)
[2024-11-08 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:00][root][INFO] - Training Epoch: 8/10, step 148/574 completed (loss: 0.01943071559071541, acc: 1.0)
[2024-11-08 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:00][root][INFO] - Training Epoch: 8/10, step 149/574 completed (loss: 0.004971902351826429, acc: 1.0)
[2024-11-08 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:01][root][INFO] - Training Epoch: 8/10, step 150/574 completed (loss: 0.01534487958997488, acc: 1.0)
[2024-11-08 04:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:01][root][INFO] - Training Epoch: 8/10, step 151/574 completed (loss: 0.004715175833553076, acc: 1.0)
[2024-11-08 04:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:02][root][INFO] - Training Epoch: 8/10, step 152/574 completed (loss: 0.006005650851875544, acc: 1.0)
[2024-11-08 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:02][root][INFO] - Training Epoch: 8/10, step 153/574 completed (loss: 0.052647363394498825, acc: 0.9473684430122375)
[2024-11-08 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:02][root][INFO] - Training Epoch: 8/10, step 154/574 completed (loss: 0.0020705400966107845, acc: 1.0)
[2024-11-08 04:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:03][root][INFO] - Training Epoch: 8/10, step 155/574 completed (loss: 0.04171113297343254, acc: 1.0)
[2024-11-08 04:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:03][root][INFO] - Training Epoch: 8/10, step 156/574 completed (loss: 0.18927830457687378, acc: 0.9545454382896423)
[2024-11-08 04:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:04][root][INFO] - Training Epoch: 8/10, step 157/574 completed (loss: 0.33453935384750366, acc: 0.9090909361839294)
[2024-11-08 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:06][root][INFO] - Training Epoch: 8/10, step 158/574 completed (loss: 0.31745436787605286, acc: 0.8999999761581421)
[2024-11-08 04:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:07][root][INFO] - Training Epoch: 8/10, step 159/574 completed (loss: 0.39652949571609497, acc: 0.8500000238418579)
[2024-11-08 04:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:07][root][INFO] - Training Epoch: 8/10, step 160/574 completed (loss: 0.024296823889017105, acc: 1.0)
[2024-11-08 04:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:08][root][INFO] - Training Epoch: 8/10, step 161/574 completed (loss: 0.26338765025138855, acc: 0.9545454382896423)
[2024-11-08 04:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:09][root][INFO] - Training Epoch: 8/10, step 162/574 completed (loss: 0.15396687388420105, acc: 0.949999988079071)
[2024-11-08 04:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:09][root][INFO] - Training Epoch: 8/10, step 163/574 completed (loss: 0.18196488916873932, acc: 0.9599999785423279)
[2024-11-08 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:10][root][INFO] - Training Epoch: 8/10, step 164/574 completed (loss: 0.04030932858586311, acc: 1.0)
[2024-11-08 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:10][root][INFO] - Training Epoch: 8/10, step 165/574 completed (loss: 0.12472447007894516, acc: 0.9722222089767456)
[2024-11-08 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:11][root][INFO] - Training Epoch: 8/10, step 166/574 completed (loss: 0.0978287011384964, acc: 0.95652174949646)
[2024-11-08 04:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:11][root][INFO] - Training Epoch: 8/10, step 167/574 completed (loss: 0.014502119272947311, acc: 1.0)
[2024-11-08 04:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:11][root][INFO] - Training Epoch: 8/10, step 168/574 completed (loss: 0.0018844002624973655, acc: 1.0)
[2024-11-08 04:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:12][root][INFO] - Training Epoch: 8/10, step 169/574 completed (loss: 0.0022050628904253244, acc: 1.0)
[2024-11-08 04:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:13][root][INFO] - Training Epoch: 8/10, step 170/574 completed (loss: 0.07932307571172714, acc: 0.949999988079071)
[2024-11-08 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:14][root][INFO] - Training Epoch: 8/10, step 171/574 completed (loss: 0.04797281324863434, acc: 0.9545454382896423)
[2024-11-08 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:14][root][INFO] - Training Epoch: 8/10, step 172/574 completed (loss: 0.007427709177136421, acc: 1.0)
[2024-11-08 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:15][root][INFO] - Training Epoch: 8/10, step 173/574 completed (loss: 0.008581801317632198, acc: 1.0)
[2024-11-08 04:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:15][root][INFO] - Training Epoch: 8/10, step 174/574 completed (loss: 0.05437394976615906, acc: 1.0)
[2024-11-08 04:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:16][root][INFO] - Training Epoch: 8/10, step 175/574 completed (loss: 0.09623882919549942, acc: 0.9523809552192688)
[2024-11-08 04:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:16][root][INFO] - Training Epoch: 8/10, step 176/574 completed (loss: 0.0051790266297757626, acc: 1.0)
[2024-11-08 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:17][root][INFO] - Training Epoch: 8/10, step 177/574 completed (loss: 0.03453492000699043, acc: 1.0)
[2024-11-08 04:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:18][root][INFO] - Training Epoch: 8/10, step 178/574 completed (loss: 0.10275501012802124, acc: 0.9473684430122375)
[2024-11-08 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:19][root][INFO] - Training Epoch: 8/10, step 179/574 completed (loss: 0.056619253009557724, acc: 0.9545454382896423)
[2024-11-08 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:19][root][INFO] - Training Epoch: 8/10, step 180/574 completed (loss: 0.07779739797115326, acc: 0.9642857313156128)
[2024-11-08 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:20][root][INFO] - Training Epoch: 8/10, step 181/574 completed (loss: 0.0020577108953148127, acc: 1.0)
[2024-11-08 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:20][root][INFO] - Training Epoch: 8/10, step 182/574 completed (loss: 0.08150500059127808, acc: 0.9714285731315613)
[2024-11-08 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:20][root][INFO] - Training Epoch: 8/10, step 183/574 completed (loss: 0.012640247121453285, acc: 1.0)
[2024-11-08 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:21][root][INFO] - Training Epoch: 8/10, step 184/574 completed (loss: 0.011363704688847065, acc: 1.0)
[2024-11-08 04:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:21][root][INFO] - Training Epoch: 8/10, step 185/574 completed (loss: 0.013505036942660809, acc: 1.0)
[2024-11-08 04:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:22][root][INFO] - Training Epoch: 8/10, step 186/574 completed (loss: 0.01326528750360012, acc: 1.0)
[2024-11-08 04:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:22][root][INFO] - Training Epoch: 8/10, step 187/574 completed (loss: 0.012616355903446674, acc: 1.0)
[2024-11-08 04:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:23][root][INFO] - Training Epoch: 8/10, step 188/574 completed (loss: 0.0014209881192073226, acc: 1.0)
[2024-11-08 04:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:23][root][INFO] - Training Epoch: 8/10, step 189/574 completed (loss: 0.09924224019050598, acc: 0.9200000166893005)
[2024-11-08 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:24][root][INFO] - Training Epoch: 8/10, step 190/574 completed (loss: 0.058668650686740875, acc: 0.949999988079071)
[2024-11-08 04:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:25][root][INFO] - Training Epoch: 8/10, step 191/574 completed (loss: 0.36790814995765686, acc: 0.9047619104385376)
[2024-11-08 04:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:27][root][INFO] - Training Epoch: 8/10, step 192/574 completed (loss: 0.4398709237575531, acc: 0.8421052694320679)
[2024-11-08 04:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:28][root][INFO] - Training Epoch: 8/10, step 193/574 completed (loss: 0.008512168191373348, acc: 1.0)
[2024-11-08 04:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:30][root][INFO] - Training Epoch: 8/10, step 194/574 completed (loss: 0.043070606887340546, acc: 1.0)
[2024-11-08 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:31][root][INFO] - Training Epoch: 8/10, step 195/574 completed (loss: 0.11977066844701767, acc: 0.9583333134651184)
[2024-11-08 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:32][root][INFO] - Training Epoch: 8/10, step 196/574 completed (loss: 0.00028599437791854143, acc: 1.0)
[2024-11-08 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:32][root][INFO] - Training Epoch: 8/10, step 197/574 completed (loss: 0.10133162885904312, acc: 0.9333333373069763)
[2024-11-08 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:32][root][INFO] - Training Epoch: 8/10, step 198/574 completed (loss: 0.02988537587225437, acc: 1.0)
[2024-11-08 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:33][root][INFO] - Training Epoch: 8/10, step 199/574 completed (loss: 0.009007868357002735, acc: 1.0)
[2024-11-08 04:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:33][root][INFO] - Training Epoch: 8/10, step 200/574 completed (loss: 0.0008875492494553328, acc: 1.0)
[2024-11-08 04:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:34][root][INFO] - Training Epoch: 8/10, step 201/574 completed (loss: 0.042025092989206314, acc: 1.0)
[2024-11-08 04:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:34][root][INFO] - Training Epoch: 8/10, step 202/574 completed (loss: 0.07610093057155609, acc: 0.949999988079071)
[2024-11-08 04:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:34][root][INFO] - Training Epoch: 8/10, step 203/574 completed (loss: 0.09152998775243759, acc: 0.95652174949646)
[2024-11-08 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:35][root][INFO] - Training Epoch: 8/10, step 204/574 completed (loss: 0.01143196877092123, acc: 1.0)
[2024-11-08 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:35][root][INFO] - Training Epoch: 8/10, step 205/574 completed (loss: 0.15826936066150665, acc: 0.9523809552192688)
[2024-11-08 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:36][root][INFO] - Training Epoch: 8/10, step 206/574 completed (loss: 0.01100499089807272, acc: 1.0)
[2024-11-08 04:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:36][root][INFO] - Training Epoch: 8/10, step 207/574 completed (loss: 0.007684336509555578, acc: 1.0)
[2024-11-08 04:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:36][root][INFO] - Training Epoch: 8/10, step 208/574 completed (loss: 0.006573238875716925, acc: 1.0)
[2024-11-08 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:37][root][INFO] - Training Epoch: 8/10, step 209/574 completed (loss: 0.020618081092834473, acc: 1.0)
[2024-11-08 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:37][root][INFO] - Training Epoch: 8/10, step 210/574 completed (loss: 0.02558695711195469, acc: 1.0)
[2024-11-08 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:38][root][INFO] - Training Epoch: 8/10, step 211/574 completed (loss: 0.007568697445094585, acc: 1.0)
[2024-11-08 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:38][root][INFO] - Training Epoch: 8/10, step 212/574 completed (loss: 0.10909215360879898, acc: 0.9677419066429138)
[2024-11-08 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:39][root][INFO] - Training Epoch: 8/10, step 213/574 completed (loss: 0.2314402461051941, acc: 0.9615384340286255)
[2024-11-08 04:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:40][root][INFO] - Training Epoch: 8/10, step 214/574 completed (loss: 0.00923894438892603, acc: 1.0)
[2024-11-08 04:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:40][root][INFO] - Training Epoch: 8/10, step 215/574 completed (loss: 0.0352645069360733, acc: 1.0)
[2024-11-08 04:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:41][root][INFO] - Training Epoch: 8/10, step 216/574 completed (loss: 0.0012408123584464192, acc: 1.0)
[2024-11-08 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:41][root][INFO] - Training Epoch: 8/10, step 217/574 completed (loss: 0.002026689937338233, acc: 1.0)
[2024-11-08 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:42][root][INFO] - Training Epoch: 8/10, step 218/574 completed (loss: 0.006386259570717812, acc: 1.0)
[2024-11-08 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:42][root][INFO] - Training Epoch: 8/10, step 219/574 completed (loss: 0.00509917177259922, acc: 1.0)
[2024-11-08 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:43][root][INFO] - Training Epoch: 8/10, step 220/574 completed (loss: 0.019279824569821358, acc: 1.0)
[2024-11-08 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:43][root][INFO] - Training Epoch: 8/10, step 221/574 completed (loss: 0.0011175074614584446, acc: 1.0)
[2024-11-08 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:44][root][INFO] - Training Epoch: 8/10, step 222/574 completed (loss: 0.0016000195173546672, acc: 1.0)
[2024-11-08 04:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:45][root][INFO] - Training Epoch: 8/10, step 223/574 completed (loss: 0.07420798391103745, acc: 0.9523809552192688)
[2024-11-08 04:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:45][root][INFO] - Training Epoch: 8/10, step 224/574 completed (loss: 0.08328071236610413, acc: 1.0)
[2024-11-08 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:46][root][INFO] - Training Epoch: 8/10, step 225/574 completed (loss: 0.002926107496023178, acc: 1.0)
[2024-11-08 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:47][root][INFO] - Training Epoch: 8/10, step 226/574 completed (loss: 0.018773691728711128, acc: 1.0)
[2024-11-08 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:47][root][INFO] - Training Epoch: 8/10, step 227/574 completed (loss: 0.12897565960884094, acc: 0.9523809552192688)
[2024-11-08 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:47][root][INFO] - Training Epoch: 8/10, step 228/574 completed (loss: 0.31023213267326355, acc: 0.875)
[2024-11-08 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:48][root][INFO] - Training Epoch: 8/10, step 229/574 completed (loss: 0.3102796673774719, acc: 0.8260869383811951)
[2024-11-08 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:48][root][INFO] - Training Epoch: 8/10, step 230/574 completed (loss: 0.06807450950145721, acc: 1.0)
[2024-11-08 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:49][root][INFO] - Training Epoch: 8/10, step 231/574 completed (loss: 0.5086711049079895, acc: 0.8421052694320679)
[2024-11-08 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:49][root][INFO] - Training Epoch: 8/10, step 232/574 completed (loss: 0.023988232016563416, acc: 1.0)
[2024-11-08 04:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:50][root][INFO] - Training Epoch: 8/10, step 233/574 completed (loss: 0.1470336765050888, acc: 0.9545454382896423)
[2024-11-08 04:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:51][root][INFO] - Training Epoch: 8/10, step 234/574 completed (loss: 0.24182818830013275, acc: 0.9545454382896423)
[2024-11-08 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:51][root][INFO] - Training Epoch: 8/10, step 235/574 completed (loss: 0.09888464957475662, acc: 0.9642857313156128)
[2024-11-08 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:52][root][INFO] - Training Epoch: 8/10, step 236/574 completed (loss: 0.07410327345132828, acc: 0.9259259104728699)
[2024-11-08 04:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:52][root][INFO] - Training Epoch: 8/10, step 237/574 completed (loss: 0.09845577925443649, acc: 0.9090909361839294)
[2024-11-08 04:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:52][root][INFO] - Training Epoch: 8/10, step 238/574 completed (loss: 0.07439642399549484, acc: 0.9545454382896423)
[2024-11-08 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:53][root][INFO] - Training Epoch: 8/10, step 239/574 completed (loss: 0.16734343767166138, acc: 0.9523809552192688)
[2024-11-08 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:53][root][INFO] - Training Epoch: 8/10, step 240/574 completed (loss: 0.4302390217781067, acc: 0.9473684430122375)
[2024-11-08 04:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:54][root][INFO] - Training Epoch: 8/10, step 241/574 completed (loss: 0.023076975718140602, acc: 1.0)
[2024-11-08 04:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:55][root][INFO] - Training Epoch: 8/10, step 242/574 completed (loss: 0.01337489951401949, acc: 1.0)
[2024-11-08 04:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:55][root][INFO] - Training Epoch: 8/10, step 243/574 completed (loss: 0.22451753914356232, acc: 0.9545454382896423)
[2024-11-08 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:56][root][INFO] - Training Epoch: 8/10, step 244/574 completed (loss: 0.0013874329160898924, acc: 1.0)
[2024-11-08 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:56][root][INFO] - Training Epoch: 8/10, step 245/574 completed (loss: 0.12185044586658478, acc: 0.9666666388511658)
[2024-11-08 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:57][root][INFO] - Training Epoch: 8/10, step 246/574 completed (loss: 0.031240537762641907, acc: 1.0)
[2024-11-08 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:57][root][INFO] - Training Epoch: 8/10, step 247/574 completed (loss: 0.006976414006203413, acc: 1.0)
[2024-11-08 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:58][root][INFO] - Training Epoch: 8/10, step 248/574 completed (loss: 0.009449087083339691, acc: 1.0)
[2024-11-08 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:58][root][INFO] - Training Epoch: 8/10, step 249/574 completed (loss: 0.02549971640110016, acc: 1.0)
[2024-11-08 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:58][root][INFO] - Training Epoch: 8/10, step 250/574 completed (loss: 0.014642680063843727, acc: 1.0)
[2024-11-08 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:59][root][INFO] - Training Epoch: 8/10, step 251/574 completed (loss: 0.02303379587829113, acc: 1.0)
[2024-11-08 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:22:59][root][INFO] - Training Epoch: 8/10, step 252/574 completed (loss: 0.0025972251314669847, acc: 1.0)
[2024-11-08 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:00][root][INFO] - Training Epoch: 8/10, step 253/574 completed (loss: 0.007259946782141924, acc: 1.0)
[2024-11-08 04:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:00][root][INFO] - Training Epoch: 8/10, step 254/574 completed (loss: 0.0008506372687406838, acc: 1.0)
[2024-11-08 04:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:01][root][INFO] - Training Epoch: 8/10, step 255/574 completed (loss: 0.021421043202280998, acc: 1.0)
[2024-11-08 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:01][root][INFO] - Training Epoch: 8/10, step 256/574 completed (loss: 0.0023823310621082783, acc: 1.0)
[2024-11-08 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:01][root][INFO] - Training Epoch: 8/10, step 257/574 completed (loss: 0.013217568397521973, acc: 1.0)
[2024-11-08 04:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:02][root][INFO] - Training Epoch: 8/10, step 258/574 completed (loss: 0.0015360498800873756, acc: 1.0)
[2024-11-08 04:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:03][root][INFO] - Training Epoch: 8/10, step 259/574 completed (loss: 0.0026043441612273455, acc: 1.0)
[2024-11-08 04:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:04][root][INFO] - Training Epoch: 8/10, step 260/574 completed (loss: 0.004136846866458654, acc: 1.0)
[2024-11-08 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:04][root][INFO] - Training Epoch: 8/10, step 261/574 completed (loss: 0.0045598517172038555, acc: 1.0)
[2024-11-08 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:04][root][INFO] - Training Epoch: 8/10, step 262/574 completed (loss: 0.06050238013267517, acc: 0.970588207244873)
[2024-11-08 04:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:05][root][INFO] - Training Epoch: 8/10, step 263/574 completed (loss: 0.29864931106567383, acc: 0.9090909361839294)
[2024-11-08 04:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:05][root][INFO] - Training Epoch: 8/10, step 264/574 completed (loss: 0.10724939405918121, acc: 0.949999988079071)
[2024-11-08 04:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:07][root][INFO] - Training Epoch: 8/10, step 265/574 completed (loss: 0.2017112374305725, acc: 0.9473684430122375)
[2024-11-08 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:07][root][INFO] - Training Epoch: 8/10, step 266/574 completed (loss: 0.06568405777215958, acc: 0.95652174949646)
[2024-11-08 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:08][root][INFO] - Training Epoch: 8/10, step 267/574 completed (loss: 0.03974391892552376, acc: 1.0)
[2024-11-08 04:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:08][root][INFO] - Training Epoch: 8/10, step 268/574 completed (loss: 0.06998180598020554, acc: 0.9655172228813171)
[2024-11-08 04:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:09][root][INFO] - Training Epoch: 8/10, step 269/574 completed (loss: 0.0004835462605115026, acc: 1.0)
[2024-11-08 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:09][root][INFO] - Training Epoch: 8/10, step 270/574 completed (loss: 0.013758874498307705, acc: 1.0)
[2024-11-08 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:10][root][INFO] - Training Epoch: 8/10, step 271/574 completed (loss: 0.00032922509126365185, acc: 1.0)
[2024-11-08 04:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:52][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3247, device='cuda:0') eval_epoch_loss=tensor(0.2812, device='cuda:0') eval_epoch_acc=tensor(0.9270, device='cuda:0')
[2024-11-08 04:23:52][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:23:52][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:23:56][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_8_step_272_loss_0.2811983823776245/model.pt
[2024-11-08 04:23:56][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:56][root][INFO] - Training Epoch: 8/10, step 272/574 completed (loss: 0.005934140644967556, acc: 1.0)
[2024-11-08 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:57][root][INFO] - Training Epoch: 8/10, step 273/574 completed (loss: 0.0006038201972842216, acc: 1.0)
[2024-11-08 04:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:57][root][INFO] - Training Epoch: 8/10, step 274/574 completed (loss: 0.0016691884957253933, acc: 1.0)
[2024-11-08 04:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:57][root][INFO] - Training Epoch: 8/10, step 275/574 completed (loss: 0.011589220724999905, acc: 1.0)
[2024-11-08 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:58][root][INFO] - Training Epoch: 8/10, step 276/574 completed (loss: 0.06330820173025131, acc: 0.9655172228813171)
[2024-11-08 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:58][root][INFO] - Training Epoch: 8/10, step 277/574 completed (loss: 0.0294649600982666, acc: 1.0)
[2024-11-08 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:59][root][INFO] - Training Epoch: 8/10, step 278/574 completed (loss: 0.13761037588119507, acc: 0.9523809552192688)
[2024-11-08 04:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:59][root][INFO] - Training Epoch: 8/10, step 279/574 completed (loss: 0.010284423828125, acc: 1.0)
[2024-11-08 04:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:23:59][root][INFO] - Training Epoch: 8/10, step 280/574 completed (loss: 0.014283471740782261, acc: 1.0)
[2024-11-08 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:00][root][INFO] - Training Epoch: 8/10, step 281/574 completed (loss: 0.038602497428655624, acc: 0.9545454382896423)
[2024-11-08 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:00][root][INFO] - Training Epoch: 8/10, step 282/574 completed (loss: 0.0010840685572475195, acc: 1.0)
[2024-11-08 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:01][root][INFO] - Training Epoch: 8/10, step 283/574 completed (loss: 0.25526294112205505, acc: 0.9655172228813171)
[2024-11-08 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:01][root][INFO] - Training Epoch: 8/10, step 284/574 completed (loss: 0.18343600630760193, acc: 0.9666666388511658)
[2024-11-08 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:01][root][INFO] - Training Epoch: 8/10, step 285/574 completed (loss: 0.028991568833589554, acc: 1.0)
[2024-11-08 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:02][root][INFO] - Training Epoch: 8/10, step 286/574 completed (loss: 0.3373624384403229, acc: 0.9523809552192688)
[2024-11-08 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:02][root][INFO] - Training Epoch: 8/10, step 287/574 completed (loss: 0.10976409912109375, acc: 0.9473684430122375)
[2024-11-08 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:03][root][INFO] - Training Epoch: 8/10, step 288/574 completed (loss: 0.0795525535941124, acc: 1.0)
[2024-11-08 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:03][root][INFO] - Training Epoch: 8/10, step 289/574 completed (loss: 0.034915581345558167, acc: 1.0)
[2024-11-08 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:03][root][INFO] - Training Epoch: 8/10, step 290/574 completed (loss: 0.009238763712346554, acc: 1.0)
[2024-11-08 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:04][root][INFO] - Training Epoch: 8/10, step 291/574 completed (loss: 0.08146257698535919, acc: 0.9599999785423279)
[2024-11-08 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:04][root][INFO] - Training Epoch: 8/10, step 292/574 completed (loss: 0.09642557054758072, acc: 0.9666666388511658)
[2024-11-08 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:04][root][INFO] - Training Epoch: 8/10, step 293/574 completed (loss: 0.076737180352211, acc: 0.9523809552192688)
[2024-11-08 04:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:05][root][INFO] - Training Epoch: 8/10, step 294/574 completed (loss: 0.018482808023691177, acc: 1.0)
[2024-11-08 04:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:06][root][INFO] - Training Epoch: 8/10, step 295/574 completed (loss: 0.3385961949825287, acc: 0.8947368264198303)
[2024-11-08 04:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:06][root][INFO] - Training Epoch: 8/10, step 296/574 completed (loss: 0.0011636712588369846, acc: 1.0)
[2024-11-08 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:07][root][INFO] - Training Epoch: 8/10, step 297/574 completed (loss: 0.10155860334634781, acc: 0.9523809552192688)
[2024-11-08 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:07][root][INFO] - Training Epoch: 8/10, step 298/574 completed (loss: 0.08413497358560562, acc: 0.9583333134651184)
[2024-11-08 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:07][root][INFO] - Training Epoch: 8/10, step 299/574 completed (loss: 0.0007202455890364945, acc: 1.0)
[2024-11-08 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:08][root][INFO] - Training Epoch: 8/10, step 300/574 completed (loss: 0.00506869051605463, acc: 1.0)
[2024-11-08 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:08][root][INFO] - Training Epoch: 8/10, step 301/574 completed (loss: 0.011789601296186447, acc: 1.0)
[2024-11-08 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:09][root][INFO] - Training Epoch: 8/10, step 302/574 completed (loss: 0.006219840608537197, acc: 1.0)
[2024-11-08 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:09][root][INFO] - Training Epoch: 8/10, step 303/574 completed (loss: 0.1630171686410904, acc: 0.9473684430122375)
[2024-11-08 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:09][root][INFO] - Training Epoch: 8/10, step 304/574 completed (loss: 0.0015351938782259822, acc: 1.0)
[2024-11-08 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:10][root][INFO] - Training Epoch: 8/10, step 305/574 completed (loss: 0.05308874696493149, acc: 0.95652174949646)
[2024-11-08 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:10][root][INFO] - Training Epoch: 8/10, step 306/574 completed (loss: 0.005116707179695368, acc: 1.0)
[2024-11-08 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:10][root][INFO] - Training Epoch: 8/10, step 307/574 completed (loss: 0.025945965200662613, acc: 1.0)
[2024-11-08 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:11][root][INFO] - Training Epoch: 8/10, step 308/574 completed (loss: 0.03882171958684921, acc: 1.0)
[2024-11-08 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:11][root][INFO] - Training Epoch: 8/10, step 309/574 completed (loss: 0.050224706530570984, acc: 1.0)
[2024-11-08 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:12][root][INFO] - Training Epoch: 8/10, step 310/574 completed (loss: 0.0017280066385865211, acc: 1.0)
[2024-11-08 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:12][root][INFO] - Training Epoch: 8/10, step 311/574 completed (loss: 0.01837897300720215, acc: 1.0)
[2024-11-08 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:12][root][INFO] - Training Epoch: 8/10, step 312/574 completed (loss: 0.07347355037927628, acc: 0.9545454382896423)
[2024-11-08 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:13][root][INFO] - Training Epoch: 8/10, step 313/574 completed (loss: 0.0015643775695934892, acc: 1.0)
[2024-11-08 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:13][root][INFO] - Training Epoch: 8/10, step 314/574 completed (loss: 0.02600140869617462, acc: 1.0)
[2024-11-08 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:13][root][INFO] - Training Epoch: 8/10, step 315/574 completed (loss: 0.028877854347229004, acc: 1.0)
[2024-11-08 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:14][root][INFO] - Training Epoch: 8/10, step 316/574 completed (loss: 0.10205110162496567, acc: 0.9230769276618958)
[2024-11-08 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:14][root][INFO] - Training Epoch: 8/10, step 317/574 completed (loss: 0.06199756637215614, acc: 0.9523809552192688)
[2024-11-08 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:15][root][INFO] - Training Epoch: 8/10, step 318/574 completed (loss: 0.012083632871508598, acc: 1.0)
[2024-11-08 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:15][root][INFO] - Training Epoch: 8/10, step 319/574 completed (loss: 0.002027126494795084, acc: 1.0)
[2024-11-08 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:15][root][INFO] - Training Epoch: 8/10, step 320/574 completed (loss: 0.010437725111842155, acc: 1.0)
[2024-11-08 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:16][root][INFO] - Training Epoch: 8/10, step 321/574 completed (loss: 0.001746808411553502, acc: 1.0)
[2024-11-08 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:16][root][INFO] - Training Epoch: 8/10, step 322/574 completed (loss: 0.2587222158908844, acc: 0.9130434989929199)
[2024-11-08 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:16][root][INFO] - Training Epoch: 8/10, step 323/574 completed (loss: 0.09960905462503433, acc: 0.9523809552192688)
[2024-11-08 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:17][root][INFO] - Training Epoch: 8/10, step 324/574 completed (loss: 0.2889956831932068, acc: 0.8421052694320679)
[2024-11-08 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:17][root][INFO] - Training Epoch: 8/10, step 325/574 completed (loss: 0.3148273527622223, acc: 0.8181818127632141)
[2024-11-08 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:18][root][INFO] - Training Epoch: 8/10, step 326/574 completed (loss: 0.49411940574645996, acc: 0.8421052694320679)
[2024-11-08 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:18][root][INFO] - Training Epoch: 8/10, step 327/574 completed (loss: 0.35855209827423096, acc: 0.875)
[2024-11-08 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:18][root][INFO] - Training Epoch: 8/10, step 328/574 completed (loss: 0.023019451647996902, acc: 1.0)
[2024-11-08 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:19][root][INFO] - Training Epoch: 8/10, step 329/574 completed (loss: 0.006611391436308622, acc: 1.0)
[2024-11-08 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:19][root][INFO] - Training Epoch: 8/10, step 330/574 completed (loss: 0.033258575946092606, acc: 1.0)
[2024-11-08 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:20][root][INFO] - Training Epoch: 8/10, step 331/574 completed (loss: 0.0016106325201690197, acc: 1.0)
[2024-11-08 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:20][root][INFO] - Training Epoch: 8/10, step 332/574 completed (loss: 0.12929461896419525, acc: 0.9545454382896423)
[2024-11-08 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:21][root][INFO] - Training Epoch: 8/10, step 333/574 completed (loss: 0.023207100108265877, acc: 1.0)
[2024-11-08 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:21][root][INFO] - Training Epoch: 8/10, step 334/574 completed (loss: 0.11517922580242157, acc: 0.9545454382896423)
[2024-11-08 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:21][root][INFO] - Training Epoch: 8/10, step 335/574 completed (loss: 0.10343575477600098, acc: 0.9615384340286255)
[2024-11-08 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:22][root][INFO] - Training Epoch: 8/10, step 336/574 completed (loss: 0.22416603565216064, acc: 0.9166666865348816)
[2024-11-08 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:22][root][INFO] - Training Epoch: 8/10, step 337/574 completed (loss: 0.1382681280374527, acc: 0.9047619104385376)
[2024-11-08 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:23][root][INFO] - Training Epoch: 8/10, step 338/574 completed (loss: 0.23838511109352112, acc: 0.8695651888847351)
[2024-11-08 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:23][root][INFO] - Training Epoch: 8/10, step 339/574 completed (loss: 0.02814636193215847, acc: 1.0)
[2024-11-08 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:23][root][INFO] - Training Epoch: 8/10, step 340/574 completed (loss: 0.0007918939809314907, acc: 1.0)
[2024-11-08 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:24][root][INFO] - Training Epoch: 8/10, step 341/574 completed (loss: 0.025122003629803658, acc: 1.0)
[2024-11-08 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:24][root][INFO] - Training Epoch: 8/10, step 342/574 completed (loss: 0.009553582407534122, acc: 1.0)
[2024-11-08 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:25][root][INFO] - Training Epoch: 8/10, step 343/574 completed (loss: 0.11994246393442154, acc: 0.949999988079071)
[2024-11-08 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:25][root][INFO] - Training Epoch: 8/10, step 344/574 completed (loss: 0.001128541654907167, acc: 1.0)
[2024-11-08 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:25][root][INFO] - Training Epoch: 8/10, step 345/574 completed (loss: 0.034376587718725204, acc: 1.0)
[2024-11-08 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:26][root][INFO] - Training Epoch: 8/10, step 346/574 completed (loss: 0.011925594881176949, acc: 1.0)
[2024-11-08 04:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:26][root][INFO] - Training Epoch: 8/10, step 347/574 completed (loss: 0.0014063523849472404, acc: 1.0)
[2024-11-08 04:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:26][root][INFO] - Training Epoch: 8/10, step 348/574 completed (loss: 0.012443766929209232, acc: 1.0)
[2024-11-08 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:27][root][INFO] - Training Epoch: 8/10, step 349/574 completed (loss: 0.29197436571121216, acc: 0.8999999761581421)
[2024-11-08 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:27][root][INFO] - Training Epoch: 8/10, step 350/574 completed (loss: 0.11021285504102707, acc: 0.9473684430122375)
[2024-11-08 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:28][root][INFO] - Training Epoch: 8/10, step 351/574 completed (loss: 0.0177494864910841, acc: 1.0)
[2024-11-08 04:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:28][root][INFO] - Training Epoch: 8/10, step 352/574 completed (loss: 0.05959522724151611, acc: 1.0)
[2024-11-08 04:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:29][root][INFO] - Training Epoch: 8/10, step 353/574 completed (loss: 0.09193790704011917, acc: 0.9599999785423279)
[2024-11-08 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:29][root][INFO] - Training Epoch: 8/10, step 354/574 completed (loss: 0.07289232313632965, acc: 0.9599999785423279)
[2024-11-08 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:29][root][INFO] - Training Epoch: 8/10, step 355/574 completed (loss: 0.0684138759970665, acc: 0.9523809552192688)
[2024-11-08 04:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:30][root][INFO] - Training Epoch: 8/10, step 356/574 completed (loss: 0.20027756690979004, acc: 0.949999988079071)
[2024-11-08 04:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:30][root][INFO] - Training Epoch: 8/10, step 357/574 completed (loss: 0.07342848181724548, acc: 0.9523809552192688)
[2024-11-08 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:31][root][INFO] - Training Epoch: 8/10, step 358/574 completed (loss: 0.12347884476184845, acc: 0.9090909361839294)
[2024-11-08 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:31][root][INFO] - Training Epoch: 8/10, step 359/574 completed (loss: 0.00037782106664963067, acc: 1.0)
[2024-11-08 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:32][root][INFO] - Training Epoch: 8/10, step 360/574 completed (loss: 0.005199171137064695, acc: 1.0)
[2024-11-08 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:32][root][INFO] - Training Epoch: 8/10, step 361/574 completed (loss: 0.09611453860998154, acc: 0.9714285731315613)
[2024-11-08 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:32][root][INFO] - Training Epoch: 8/10, step 362/574 completed (loss: 0.09312577545642853, acc: 0.9545454382896423)
[2024-11-08 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:33][root][INFO] - Training Epoch: 8/10, step 363/574 completed (loss: 0.03934990242123604, acc: 1.0)
[2024-11-08 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:33][root][INFO] - Training Epoch: 8/10, step 364/574 completed (loss: 0.13637998700141907, acc: 0.9047619104385376)
[2024-11-08 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:33][root][INFO] - Training Epoch: 8/10, step 365/574 completed (loss: 0.10891496390104294, acc: 0.9473684430122375)
[2024-11-08 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:34][root][INFO] - Training Epoch: 8/10, step 366/574 completed (loss: 0.0007792077958583832, acc: 1.0)
[2024-11-08 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:34][root][INFO] - Training Epoch: 8/10, step 367/574 completed (loss: 0.03370232135057449, acc: 1.0)
[2024-11-08 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:35][root][INFO] - Training Epoch: 8/10, step 368/574 completed (loss: 0.02790433168411255, acc: 1.0)
[2024-11-08 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:35][root][INFO] - Training Epoch: 8/10, step 369/574 completed (loss: 0.01382779236882925, acc: 1.0)
[2024-11-08 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:36][root][INFO] - Training Epoch: 8/10, step 370/574 completed (loss: 0.35415372252464294, acc: 0.9047619104385376)
[2024-11-08 04:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:37][root][INFO] - Training Epoch: 8/10, step 371/574 completed (loss: 0.09069249033927917, acc: 0.9473684430122375)
[2024-11-08 04:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:37][root][INFO] - Training Epoch: 8/10, step 372/574 completed (loss: 0.0010474142618477345, acc: 1.0)
[2024-11-08 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:38][root][INFO] - Training Epoch: 8/10, step 373/574 completed (loss: 0.17587342858314514, acc: 0.9473684430122375)
[2024-11-08 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:38][root][INFO] - Training Epoch: 8/10, step 374/574 completed (loss: 0.03593156859278679, acc: 0.9545454382896423)
[2024-11-08 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:39][root][INFO] - Training Epoch: 8/10, step 375/574 completed (loss: 0.0008753736619837582, acc: 1.0)
[2024-11-08 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:39][root][INFO] - Training Epoch: 8/10, step 376/574 completed (loss: 0.004351361189037561, acc: 1.0)
[2024-11-08 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:39][root][INFO] - Training Epoch: 8/10, step 377/574 completed (loss: 0.015376782044768333, acc: 1.0)
[2024-11-08 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:40][root][INFO] - Training Epoch: 8/10, step 378/574 completed (loss: 0.00869873259216547, acc: 1.0)
[2024-11-08 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:41][root][INFO] - Training Epoch: 8/10, step 379/574 completed (loss: 0.10253256559371948, acc: 0.8947368264198303)
[2024-11-08 04:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:41][root][INFO] - Training Epoch: 8/10, step 380/574 completed (loss: 0.15701787173748016, acc: 0.8636363744735718)
[2024-11-08 04:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:43][root][INFO] - Training Epoch: 8/10, step 381/574 completed (loss: 0.037958353757858276, acc: 1.0)
[2024-11-08 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:44][root][INFO] - Training Epoch: 8/10, step 382/574 completed (loss: 0.027411889284849167, acc: 1.0)
[2024-11-08 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:44][root][INFO] - Training Epoch: 8/10, step 383/574 completed (loss: 0.01803397201001644, acc: 1.0)
[2024-11-08 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:44][root][INFO] - Training Epoch: 8/10, step 384/574 completed (loss: 0.010059865191578865, acc: 1.0)
[2024-11-08 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:45][root][INFO] - Training Epoch: 8/10, step 385/574 completed (loss: 0.0026787503156811, acc: 1.0)
[2024-11-08 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:45][root][INFO] - Training Epoch: 8/10, step 386/574 completed (loss: 0.007479344494640827, acc: 1.0)
[2024-11-08 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:45][root][INFO] - Training Epoch: 8/10, step 387/574 completed (loss: 0.0033909736666828394, acc: 1.0)
[2024-11-08 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:46][root][INFO] - Training Epoch: 8/10, step 388/574 completed (loss: 0.009013022296130657, acc: 1.0)
[2024-11-08 04:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:46][root][INFO] - Training Epoch: 8/10, step 389/574 completed (loss: 0.006543944124132395, acc: 1.0)
[2024-11-08 04:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:46][root][INFO] - Training Epoch: 8/10, step 390/574 completed (loss: 0.10199350863695145, acc: 0.9545454382896423)
[2024-11-08 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:47][root][INFO] - Training Epoch: 8/10, step 391/574 completed (loss: 0.03968873620033264, acc: 0.9523809552192688)
[2024-11-08 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:47][root][INFO] - Training Epoch: 8/10, step 392/574 completed (loss: 0.06755071133375168, acc: 0.9473684430122375)
[2024-11-08 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:48][root][INFO] - Training Epoch: 8/10, step 393/574 completed (loss: 0.010411032475531101, acc: 1.0)
[2024-11-08 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:48][root][INFO] - Training Epoch: 8/10, step 394/574 completed (loss: 0.04995490610599518, acc: 1.0)
[2024-11-08 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:49][root][INFO] - Training Epoch: 8/10, step 395/574 completed (loss: 0.20887774229049683, acc: 0.9047619104385376)
[2024-11-08 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:49][root][INFO] - Training Epoch: 8/10, step 396/574 completed (loss: 0.0013214407954365015, acc: 1.0)
[2024-11-08 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:50][root][INFO] - Training Epoch: 8/10, step 397/574 completed (loss: 0.05647631362080574, acc: 0.9666666388511658)
[2024-11-08 04:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:50][root][INFO] - Training Epoch: 8/10, step 398/574 completed (loss: 0.29999789595603943, acc: 0.9523809552192688)
[2024-11-08 04:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:50][root][INFO] - Training Epoch: 8/10, step 399/574 completed (loss: 0.009105097502470016, acc: 1.0)
[2024-11-08 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:51][root][INFO] - Training Epoch: 8/10, step 400/574 completed (loss: 0.03461260348558426, acc: 1.0)
[2024-11-08 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:52][root][INFO] - Training Epoch: 8/10, step 401/574 completed (loss: 0.00039722229121252894, acc: 1.0)
[2024-11-08 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:52][root][INFO] - Training Epoch: 8/10, step 402/574 completed (loss: 0.0018228166736662388, acc: 1.0)
[2024-11-08 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:52][root][INFO] - Training Epoch: 8/10, step 403/574 completed (loss: 0.06699629127979279, acc: 0.96875)
[2024-11-08 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:53][root][INFO] - Training Epoch: 8/10, step 404/574 completed (loss: 0.13912121951580048, acc: 0.9428571462631226)
[2024-11-08 04:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:53][root][INFO] - Training Epoch: 8/10, step 405/574 completed (loss: 0.007416078820824623, acc: 1.0)
[2024-11-08 04:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:53][root][INFO] - Training Epoch: 8/10, step 406/574 completed (loss: 0.018909800797700882, acc: 1.0)
[2024-11-08 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:54][root][INFO] - Training Epoch: 8/10, step 407/574 completed (loss: 0.14017152786254883, acc: 0.8947368264198303)
[2024-11-08 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:54][root][INFO] - Training Epoch: 8/10, step 408/574 completed (loss: 0.003748436691239476, acc: 1.0)
[2024-11-08 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:55][root][INFO] - Training Epoch: 8/10, step 409/574 completed (loss: 0.0010213104542344809, acc: 1.0)
[2024-11-08 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:55][root][INFO] - Training Epoch: 8/10, step 410/574 completed (loss: 0.0008322313660755754, acc: 1.0)
[2024-11-08 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:55][root][INFO] - Training Epoch: 8/10, step 411/574 completed (loss: 0.2477833777666092, acc: 0.9642857313156128)
[2024-11-08 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:56][root][INFO] - Training Epoch: 8/10, step 412/574 completed (loss: 0.0021575805731117725, acc: 1.0)
[2024-11-08 04:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:56][root][INFO] - Training Epoch: 8/10, step 413/574 completed (loss: 0.0017934616189450026, acc: 1.0)
[2024-11-08 04:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:56][root][INFO] - Training Epoch: 8/10, step 414/574 completed (loss: 0.13540002703666687, acc: 0.9615384340286255)
[2024-11-08 04:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:39][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3557, device='cuda:0') eval_epoch_loss=tensor(0.3043, device='cuda:0') eval_epoch_acc=tensor(0.9250, device='cuda:0')
[2024-11-08 04:25:39][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:25:39][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:25:42][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_8_step_415_loss_0.3043113946914673/model.pt
[2024-11-08 04:25:42][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:43][root][INFO] - Training Epoch: 8/10, step 415/574 completed (loss: 0.005551579408347607, acc: 1.0)
[2024-11-08 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:43][root][INFO] - Training Epoch: 8/10, step 416/574 completed (loss: 0.004816541448235512, acc: 1.0)
[2024-11-08 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:43][root][INFO] - Training Epoch: 8/10, step 417/574 completed (loss: 0.0008386120316572487, acc: 1.0)
[2024-11-08 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:44][root][INFO] - Training Epoch: 8/10, step 418/574 completed (loss: 0.03009331040084362, acc: 1.0)
[2024-11-08 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:44][root][INFO] - Training Epoch: 8/10, step 419/574 completed (loss: 0.01307551097124815, acc: 1.0)
[2024-11-08 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:45][root][INFO] - Training Epoch: 8/10, step 420/574 completed (loss: 0.0723496749997139, acc: 1.0)
[2024-11-08 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:45][root][INFO] - Training Epoch: 8/10, step 421/574 completed (loss: 0.01602349616587162, acc: 1.0)
[2024-11-08 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:45][root][INFO] - Training Epoch: 8/10, step 422/574 completed (loss: 0.015786338597536087, acc: 1.0)
[2024-11-08 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:46][root][INFO] - Training Epoch: 8/10, step 423/574 completed (loss: 0.010690122842788696, acc: 1.0)
[2024-11-08 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:46][root][INFO] - Training Epoch: 8/10, step 424/574 completed (loss: 0.004682048223912716, acc: 1.0)
[2024-11-08 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:46][root][INFO] - Training Epoch: 8/10, step 425/574 completed (loss: 0.005504343658685684, acc: 1.0)
[2024-11-08 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:47][root][INFO] - Training Epoch: 8/10, step 426/574 completed (loss: 0.007331328932195902, acc: 1.0)
[2024-11-08 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:47][root][INFO] - Training Epoch: 8/10, step 427/574 completed (loss: 0.0678710788488388, acc: 1.0)
[2024-11-08 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:48][root][INFO] - Training Epoch: 8/10, step 428/574 completed (loss: 0.003952542319893837, acc: 1.0)
[2024-11-08 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:48][root][INFO] - Training Epoch: 8/10, step 429/574 completed (loss: 0.0015157629968598485, acc: 1.0)
[2024-11-08 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:48][root][INFO] - Training Epoch: 8/10, step 430/574 completed (loss: 0.0006981866317801178, acc: 1.0)
[2024-11-08 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:49][root][INFO] - Training Epoch: 8/10, step 431/574 completed (loss: 0.0006785598234273493, acc: 1.0)
[2024-11-08 04:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:49][root][INFO] - Training Epoch: 8/10, step 432/574 completed (loss: 0.002741836477071047, acc: 1.0)
[2024-11-08 04:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:49][root][INFO] - Training Epoch: 8/10, step 433/574 completed (loss: 0.0036100149154663086, acc: 1.0)
[2024-11-08 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:50][root][INFO] - Training Epoch: 8/10, step 434/574 completed (loss: 0.00137997861020267, acc: 1.0)
[2024-11-08 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:50][root][INFO] - Training Epoch: 8/10, step 435/574 completed (loss: 0.000643512699753046, acc: 1.0)
[2024-11-08 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:51][root][INFO] - Training Epoch: 8/10, step 436/574 completed (loss: 0.07407928258180618, acc: 0.9090909361839294)
[2024-11-08 04:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:51][root][INFO] - Training Epoch: 8/10, step 437/574 completed (loss: 0.0028799341525882483, acc: 1.0)
[2024-11-08 04:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:51][root][INFO] - Training Epoch: 8/10, step 438/574 completed (loss: 0.0005849251174367964, acc: 1.0)
[2024-11-08 04:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:52][root][INFO] - Training Epoch: 8/10, step 439/574 completed (loss: 0.3029839098453522, acc: 0.8928571343421936)
[2024-11-08 04:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:52][root][INFO] - Training Epoch: 8/10, step 440/574 completed (loss: 0.045228082686662674, acc: 0.9523809552192688)
[2024-11-08 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:53][root][INFO] - Training Epoch: 8/10, step 441/574 completed (loss: 0.1774676889181137, acc: 0.9473684430122375)
[2024-11-08 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:54][root][INFO] - Training Epoch: 8/10, step 442/574 completed (loss: 0.07994566857814789, acc: 0.9473684430122375)
[2024-11-08 04:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:55][root][INFO] - Training Epoch: 8/10, step 443/574 completed (loss: 0.07034566253423691, acc: 1.0)
[2024-11-08 04:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:55][root][INFO] - Training Epoch: 8/10, step 444/574 completed (loss: 0.009934679605066776, acc: 1.0)
[2024-11-08 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:56][root][INFO] - Training Epoch: 8/10, step 445/574 completed (loss: 0.0005430559976957738, acc: 1.0)
[2024-11-08 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:56][root][INFO] - Training Epoch: 8/10, step 446/574 completed (loss: 0.2093174159526825, acc: 0.9677419066429138)
[2024-11-08 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:57][root][INFO] - Training Epoch: 8/10, step 447/574 completed (loss: 0.019291233271360397, acc: 1.0)
[2024-11-08 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:57][root][INFO] - Training Epoch: 8/10, step 448/574 completed (loss: 0.003075497457757592, acc: 1.0)
[2024-11-08 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:57][root][INFO] - Training Epoch: 8/10, step 449/574 completed (loss: 0.02975848875939846, acc: 1.0)
[2024-11-08 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:58][root][INFO] - Training Epoch: 8/10, step 450/574 completed (loss: 0.0005528691690415144, acc: 1.0)
[2024-11-08 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:58][root][INFO] - Training Epoch: 8/10, step 451/574 completed (loss: 0.05644775182008743, acc: 0.9473684430122375)
[2024-11-08 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:58][root][INFO] - Training Epoch: 8/10, step 452/574 completed (loss: 0.004302604123950005, acc: 1.0)
[2024-11-08 04:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:59][root][INFO] - Training Epoch: 8/10, step 453/574 completed (loss: 0.02639756165444851, acc: 1.0)
[2024-11-08 04:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:25:59][root][INFO] - Training Epoch: 8/10, step 454/574 completed (loss: 0.0034031986724585295, acc: 1.0)
[2024-11-08 04:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:00][root][INFO] - Training Epoch: 8/10, step 455/574 completed (loss: 0.0007197902305051684, acc: 1.0)
[2024-11-08 04:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:00][root][INFO] - Training Epoch: 8/10, step 456/574 completed (loss: 0.02835015393793583, acc: 1.0)
[2024-11-08 04:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:00][root][INFO] - Training Epoch: 8/10, step 457/574 completed (loss: 0.03386884182691574, acc: 1.0)
[2024-11-08 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:01][root][INFO] - Training Epoch: 8/10, step 458/574 completed (loss: 0.012341754510998726, acc: 1.0)
[2024-11-08 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:01][root][INFO] - Training Epoch: 8/10, step 459/574 completed (loss: 0.031862773001194, acc: 1.0)
[2024-11-08 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:02][root][INFO] - Training Epoch: 8/10, step 460/574 completed (loss: 0.024768315255641937, acc: 1.0)
[2024-11-08 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:02][root][INFO] - Training Epoch: 8/10, step 461/574 completed (loss: 0.0016928102122619748, acc: 1.0)
[2024-11-08 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:02][root][INFO] - Training Epoch: 8/10, step 462/574 completed (loss: 0.005433656740933657, acc: 1.0)
[2024-11-08 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:03][root][INFO] - Training Epoch: 8/10, step 463/574 completed (loss: 0.010987023822963238, acc: 1.0)
[2024-11-08 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:03][root][INFO] - Training Epoch: 8/10, step 464/574 completed (loss: 0.004150785505771637, acc: 1.0)
[2024-11-08 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:03][root][INFO] - Training Epoch: 8/10, step 465/574 completed (loss: 0.0018477020785212517, acc: 1.0)
[2024-11-08 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:04][root][INFO] - Training Epoch: 8/10, step 466/574 completed (loss: 0.3129245936870575, acc: 0.949999988079071)
[2024-11-08 04:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:04][root][INFO] - Training Epoch: 8/10, step 467/574 completed (loss: 0.005180508363991976, acc: 1.0)
[2024-11-08 04:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:04][root][INFO] - Training Epoch: 8/10, step 468/574 completed (loss: 0.03524158522486687, acc: 1.0)
[2024-11-08 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:05][root][INFO] - Training Epoch: 8/10, step 469/574 completed (loss: 0.011004313826560974, acc: 1.0)
[2024-11-08 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:05][root][INFO] - Training Epoch: 8/10, step 470/574 completed (loss: 0.055127162486314774, acc: 0.9583333134651184)
[2024-11-08 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:05][root][INFO] - Training Epoch: 8/10, step 471/574 completed (loss: 0.108513742685318, acc: 0.9666666388511658)
[2024-11-08 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:06][root][INFO] - Training Epoch: 8/10, step 472/574 completed (loss: 0.24944989383220673, acc: 0.9583333134651184)
[2024-11-08 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:06][root][INFO] - Training Epoch: 8/10, step 473/574 completed (loss: 0.03236844763159752, acc: 1.0)
[2024-11-08 04:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:07][root][INFO] - Training Epoch: 8/10, step 474/574 completed (loss: 0.0010130030568689108, acc: 1.0)
[2024-11-08 04:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:07][root][INFO] - Training Epoch: 8/10, step 475/574 completed (loss: 0.035669900476932526, acc: 1.0)
[2024-11-08 04:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:08][root][INFO] - Training Epoch: 8/10, step 476/574 completed (loss: 0.03400936350226402, acc: 1.0)
[2024-11-08 04:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:08][root][INFO] - Training Epoch: 8/10, step 477/574 completed (loss: 0.02678479067981243, acc: 1.0)
[2024-11-08 04:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:09][root][INFO] - Training Epoch: 8/10, step 478/574 completed (loss: 0.08314496278762817, acc: 0.9642857313156128)
[2024-11-08 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:09][root][INFO] - Training Epoch: 8/10, step 479/574 completed (loss: 0.18280942738056183, acc: 0.9354838728904724)
[2024-11-08 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:09][root][INFO] - Training Epoch: 8/10, step 480/574 completed (loss: 0.06028221547603607, acc: 0.9677419066429138)
[2024-11-08 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:10][root][INFO] - Training Epoch: 8/10, step 481/574 completed (loss: 0.07853718101978302, acc: 0.9523809552192688)
[2024-11-08 04:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:10][root][INFO] - Training Epoch: 8/10, step 482/574 completed (loss: 0.09166761487722397, acc: 0.9473684430122375)
[2024-11-08 04:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:11][root][INFO] - Training Epoch: 8/10, step 483/574 completed (loss: 0.11026608198881149, acc: 0.9545454382896423)
[2024-11-08 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:11][root][INFO] - Training Epoch: 8/10, step 484/574 completed (loss: 0.15782180428504944, acc: 0.9545454382896423)
[2024-11-08 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:11][root][INFO] - Training Epoch: 8/10, step 485/574 completed (loss: 0.0007576900534331799, acc: 1.0)
[2024-11-08 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:12][root][INFO] - Training Epoch: 8/10, step 486/574 completed (loss: 0.06074449047446251, acc: 0.9629629850387573)
[2024-11-08 04:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:12][root][INFO] - Training Epoch: 8/10, step 487/574 completed (loss: 0.03497685119509697, acc: 1.0)
[2024-11-08 04:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:12][root][INFO] - Training Epoch: 8/10, step 488/574 completed (loss: 0.1361534297466278, acc: 0.9230769276618958)
[2024-11-08 04:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:13][root][INFO] - Training Epoch: 8/10, step 489/574 completed (loss: 0.021075241267681122, acc: 1.0)
[2024-11-08 04:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:13][root][INFO] - Training Epoch: 8/10, step 490/574 completed (loss: 0.00902851577848196, acc: 1.0)
[2024-11-08 04:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:14][root][INFO] - Training Epoch: 8/10, step 491/574 completed (loss: 0.0009100561728700995, acc: 1.0)
[2024-11-08 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:14][root][INFO] - Training Epoch: 8/10, step 492/574 completed (loss: 0.05199373885989189, acc: 1.0)
[2024-11-08 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:14][root][INFO] - Training Epoch: 8/10, step 493/574 completed (loss: 0.26701080799102783, acc: 0.9523809552192688)
[2024-11-08 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:15][root][INFO] - Training Epoch: 8/10, step 494/574 completed (loss: 0.23283064365386963, acc: 0.9583333134651184)
[2024-11-08 04:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:15][root][INFO] - Training Epoch: 8/10, step 495/574 completed (loss: 0.01349554117769003, acc: 1.0)
[2024-11-08 04:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:15][root][INFO] - Training Epoch: 8/10, step 496/574 completed (loss: 0.04862038418650627, acc: 1.0)
[2024-11-08 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:16][root][INFO] - Training Epoch: 8/10, step 497/574 completed (loss: 0.014139831066131592, acc: 1.0)
[2024-11-08 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:16][root][INFO] - Training Epoch: 8/10, step 498/574 completed (loss: 0.05656532198190689, acc: 0.9473684430122375)
[2024-11-08 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:17][root][INFO] - Training Epoch: 8/10, step 499/574 completed (loss: 0.057807765901088715, acc: 1.0)
[2024-11-08 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:17][root][INFO] - Training Epoch: 8/10, step 500/574 completed (loss: 0.03245920687913895, acc: 1.0)
[2024-11-08 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:18][root][INFO] - Training Epoch: 8/10, step 501/574 completed (loss: 0.00435625808313489, acc: 1.0)
[2024-11-08 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:18][root][INFO] - Training Epoch: 8/10, step 502/574 completed (loss: 0.0027221087366342545, acc: 1.0)
[2024-11-08 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:18][root][INFO] - Training Epoch: 8/10, step 503/574 completed (loss: 0.03895040601491928, acc: 1.0)
[2024-11-08 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:19][root][INFO] - Training Epoch: 8/10, step 504/574 completed (loss: 0.03614437207579613, acc: 1.0)
[2024-11-08 04:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:19][root][INFO] - Training Epoch: 8/10, step 505/574 completed (loss: 0.05895717442035675, acc: 0.9523809552192688)
[2024-11-08 04:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:19][root][INFO] - Training Epoch: 8/10, step 506/574 completed (loss: 0.2567614018917084, acc: 0.9473684430122375)
[2024-11-08 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:20][root][INFO] - Training Epoch: 8/10, step 507/574 completed (loss: 0.008086142130196095, acc: 1.0)
[2024-11-08 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:21][root][INFO] - Training Epoch: 8/10, step 508/574 completed (loss: 0.21692538261413574, acc: 0.949999988079071)
[2024-11-08 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:21][root][INFO] - Training Epoch: 8/10, step 509/574 completed (loss: 0.0076621342450380325, acc: 1.0)
[2024-11-08 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:21][root][INFO] - Training Epoch: 8/10, step 510/574 completed (loss: 0.004672727081924677, acc: 1.0)
[2024-11-08 04:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:22][root][INFO] - Training Epoch: 8/10, step 511/574 completed (loss: 0.0028677028603851795, acc: 1.0)
[2024-11-08 04:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:26][root][INFO] - Training Epoch: 8/10, step 512/574 completed (loss: 0.1358751654624939, acc: 0.9285714030265808)
[2024-11-08 04:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:27][root][INFO] - Training Epoch: 8/10, step 513/574 completed (loss: 0.12388531118631363, acc: 0.9047619104385376)
[2024-11-08 04:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:27][root][INFO] - Training Epoch: 8/10, step 514/574 completed (loss: 0.17792968451976776, acc: 0.9473684430122375)
[2024-11-08 04:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:27][root][INFO] - Training Epoch: 8/10, step 515/574 completed (loss: 0.0015365735162049532, acc: 1.0)
[2024-11-08 04:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:28][root][INFO] - Training Epoch: 8/10, step 516/574 completed (loss: 0.01981036365032196, acc: 1.0)
[2024-11-08 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:29][root][INFO] - Training Epoch: 8/10, step 517/574 completed (loss: 0.0015819754917174578, acc: 1.0)
[2024-11-08 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:29][root][INFO] - Training Epoch: 8/10, step 518/574 completed (loss: 0.11119230091571808, acc: 0.9642857313156128)
[2024-11-08 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:30][root][INFO] - Training Epoch: 8/10, step 519/574 completed (loss: 0.07866570353507996, acc: 0.9629629850387573)
[2024-11-08 04:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:30][root][INFO] - Training Epoch: 8/10, step 520/574 completed (loss: 0.03794915974140167, acc: 0.9714285731315613)
[2024-11-08 04:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:31][root][INFO] - Training Epoch: 8/10, step 521/574 completed (loss: 0.007680973969399929, acc: 1.0)
[2024-11-08 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:32][root][INFO] - Training Epoch: 8/10, step 522/574 completed (loss: 0.05915132164955139, acc: 0.9523809552192688)
[2024-11-08 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:32][root][INFO] - Training Epoch: 8/10, step 523/574 completed (loss: 0.015837645158171654, acc: 1.0)
[2024-11-08 04:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:33][root][INFO] - Training Epoch: 8/10, step 524/574 completed (loss: 0.13700200617313385, acc: 0.9545454382896423)
[2024-11-08 04:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:33][root][INFO] - Training Epoch: 8/10, step 525/574 completed (loss: 0.0076547241769731045, acc: 1.0)
[2024-11-08 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:34][root][INFO] - Training Epoch: 8/10, step 526/574 completed (loss: 0.013504663482308388, acc: 1.0)
[2024-11-08 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:34][root][INFO] - Training Epoch: 8/10, step 527/574 completed (loss: 0.023964375257492065, acc: 1.0)
[2024-11-08 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:35][root][INFO] - Training Epoch: 8/10, step 528/574 completed (loss: 0.02594558335840702, acc: 1.0)
[2024-11-08 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:35][root][INFO] - Training Epoch: 8/10, step 529/574 completed (loss: 0.003952657803893089, acc: 1.0)
[2024-11-08 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:35][root][INFO] - Training Epoch: 8/10, step 530/574 completed (loss: 0.28186294436454773, acc: 0.8947368264198303)
[2024-11-08 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:36][root][INFO] - Training Epoch: 8/10, step 531/574 completed (loss: 0.16350147128105164, acc: 0.8999999761581421)
[2024-11-08 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:36][root][INFO] - Training Epoch: 8/10, step 532/574 completed (loss: 0.25276708602905273, acc: 0.9523809552192688)
[2024-11-08 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:36][root][INFO] - Training Epoch: 8/10, step 533/574 completed (loss: 0.012038761749863625, acc: 1.0)
[2024-11-08 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:37][root][INFO] - Training Epoch: 8/10, step 534/574 completed (loss: 0.047719378024339676, acc: 0.9642857313156128)
[2024-11-08 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:37][root][INFO] - Training Epoch: 8/10, step 535/574 completed (loss: 0.006994117051362991, acc: 1.0)
[2024-11-08 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:37][root][INFO] - Training Epoch: 8/10, step 536/574 completed (loss: 0.06296276301145554, acc: 0.9599999785423279)
[2024-11-08 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:38][root][INFO] - Training Epoch: 8/10, step 537/574 completed (loss: 0.05898239463567734, acc: 0.9523809552192688)
[2024-11-08 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:38][root][INFO] - Training Epoch: 8/10, step 538/574 completed (loss: 0.35371139645576477, acc: 0.9473684430122375)
[2024-11-08 04:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:39][root][INFO] - Training Epoch: 8/10, step 539/574 completed (loss: 0.07264818996191025, acc: 0.949999988079071)
[2024-11-08 04:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:39][root][INFO] - Training Epoch: 8/10, step 540/574 completed (loss: 0.29447141289711, acc: 0.9047619104385376)
[2024-11-08 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:40][root][INFO] - Training Epoch: 8/10, step 541/574 completed (loss: 0.018962185829877853, acc: 1.0)
[2024-11-08 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:40][root][INFO] - Training Epoch: 8/10, step 542/574 completed (loss: 0.0004768354701809585, acc: 1.0)
[2024-11-08 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:40][root][INFO] - Training Epoch: 8/10, step 543/574 completed (loss: 0.0014791110297665, acc: 1.0)
[2024-11-08 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:41][root][INFO] - Training Epoch: 8/10, step 544/574 completed (loss: 0.005084365140646696, acc: 1.0)
[2024-11-08 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:41][root][INFO] - Training Epoch: 8/10, step 545/574 completed (loss: 0.0002585299080237746, acc: 1.0)
[2024-11-08 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:41][root][INFO] - Training Epoch: 8/10, step 546/574 completed (loss: 0.003547336906194687, acc: 1.0)
[2024-11-08 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:42][root][INFO] - Training Epoch: 8/10, step 547/574 completed (loss: 0.10493046045303345, acc: 0.9523809552192688)
[2024-11-08 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:42][root][INFO] - Training Epoch: 8/10, step 548/574 completed (loss: 0.01368397194892168, acc: 1.0)
[2024-11-08 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:43][root][INFO] - Training Epoch: 8/10, step 549/574 completed (loss: 0.0035305104684084654, acc: 1.0)
[2024-11-08 04:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:43][root][INFO] - Training Epoch: 8/10, step 550/574 completed (loss: 0.0013294414384290576, acc: 1.0)
[2024-11-08 04:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:43][root][INFO] - Training Epoch: 8/10, step 551/574 completed (loss: 0.0016872887499630451, acc: 1.0)
[2024-11-08 04:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:44][root][INFO] - Training Epoch: 8/10, step 552/574 completed (loss: 0.0008199568837881088, acc: 1.0)
[2024-11-08 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:44][root][INFO] - Training Epoch: 8/10, step 553/574 completed (loss: 0.0036356516648083925, acc: 1.0)
[2024-11-08 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:44][root][INFO] - Training Epoch: 8/10, step 554/574 completed (loss: 0.002522330032661557, acc: 1.0)
[2024-11-08 04:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:45][root][INFO] - Training Epoch: 8/10, step 555/574 completed (loss: 0.0055425213649868965, acc: 1.0)
[2024-11-08 04:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:45][root][INFO] - Training Epoch: 8/10, step 556/574 completed (loss: 0.0013501355424523354, acc: 1.0)
[2024-11-08 04:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:46][root][INFO] - Training Epoch: 8/10, step 557/574 completed (loss: 0.00030288606649264693, acc: 1.0)
[2024-11-08 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:27][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3440, device='cuda:0') eval_epoch_loss=tensor(0.2957, device='cuda:0') eval_epoch_acc=tensor(0.9301, device='cuda:0')
[2024-11-08 04:27:27][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:27:27][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:27:31][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_8_step_558_loss_0.29568254947662354/model.pt
[2024-11-08 04:27:31][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:31][root][INFO] - Training Epoch: 8/10, step 558/574 completed (loss: 0.0029351157136261463, acc: 1.0)
[2024-11-08 04:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:31][root][INFO] - Training Epoch: 8/10, step 559/574 completed (loss: 0.03140224516391754, acc: 0.9629629850387573)
[2024-11-08 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:32][root][INFO] - Training Epoch: 8/10, step 560/574 completed (loss: 0.01923043467104435, acc: 1.0)
[2024-11-08 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:32][root][INFO] - Training Epoch: 8/10, step 561/574 completed (loss: 0.03754669800400734, acc: 1.0)
[2024-11-08 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:33][root][INFO] - Training Epoch: 8/10, step 562/574 completed (loss: 0.03214885666966438, acc: 1.0)
[2024-11-08 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:33][root][INFO] - Training Epoch: 8/10, step 563/574 completed (loss: 0.027555644512176514, acc: 1.0)
[2024-11-08 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:34][root][INFO] - Training Epoch: 8/10, step 564/574 completed (loss: 0.001163679757155478, acc: 1.0)
[2024-11-08 04:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:34][root][INFO] - Training Epoch: 8/10, step 565/574 completed (loss: 0.02519276738166809, acc: 1.0)
[2024-11-08 04:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:34][root][INFO] - Training Epoch: 8/10, step 566/574 completed (loss: 0.0075358375906944275, acc: 1.0)
[2024-11-08 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:35][root][INFO] - Training Epoch: 8/10, step 567/574 completed (loss: 0.00031294612563215196, acc: 1.0)
[2024-11-08 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:35][root][INFO] - Training Epoch: 8/10, step 568/574 completed (loss: 0.01233426108956337, acc: 1.0)
[2024-11-08 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:36][root][INFO] - Training Epoch: 8/10, step 569/574 completed (loss: 0.0008365589892491698, acc: 1.0)
[2024-11-08 04:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:36][root][INFO] - Training Epoch: 8/10, step 570/574 completed (loss: 0.0068110572174191475, acc: 1.0)
[2024-11-08 04:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:36][root][INFO] - Training Epoch: 8/10, step 571/574 completed (loss: 0.014339563436806202, acc: 1.0)
[2024-11-08 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:37][root][INFO] - Training Epoch: 8/10, step 572/574 completed (loss: 0.004918503575026989, acc: 1.0)
[2024-11-08 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:37][root][INFO] - Training Epoch: 8/10, step 573/574 completed (loss: 0.0793764591217041, acc: 0.949999988079071)
[2024-11-08 04:27:38][slam_llm.utils.train_utils][INFO] - Epoch 8: train_perplexity=1.0681, train_epoch_loss=0.0659, epoch time 460.3471999950707s
[2024-11-08 04:27:38][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 04:27:38][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 12 GB
[2024-11-08 04:27:38][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 04:27:38][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 7
[2024-11-08 04:27:38][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 04:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:39][root][INFO] - Training Epoch: 9/10, step 0/574 completed (loss: 0.0011217182036489248, acc: 1.0)
[2024-11-08 04:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:39][root][INFO] - Training Epoch: 9/10, step 1/574 completed (loss: 0.0044172597117722034, acc: 1.0)
[2024-11-08 04:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:39][root][INFO] - Training Epoch: 9/10, step 2/574 completed (loss: 0.019757786765694618, acc: 1.0)
[2024-11-08 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:40][root][INFO] - Training Epoch: 9/10, step 3/574 completed (loss: 0.006746835540980101, acc: 1.0)
[2024-11-08 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:40][root][INFO] - Training Epoch: 9/10, step 4/574 completed (loss: 0.02012629248201847, acc: 1.0)
[2024-11-08 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:41][root][INFO] - Training Epoch: 9/10, step 5/574 completed (loss: 0.00653995294123888, acc: 1.0)
[2024-11-08 04:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:41][root][INFO] - Training Epoch: 9/10, step 6/574 completed (loss: 0.0001429702970199287, acc: 1.0)
[2024-11-08 04:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:41][root][INFO] - Training Epoch: 9/10, step 7/574 completed (loss: 0.005475421901792288, acc: 1.0)
[2024-11-08 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:42][root][INFO] - Training Epoch: 9/10, step 8/574 completed (loss: 0.0017397621413692832, acc: 1.0)
[2024-11-08 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:42][root][INFO] - Training Epoch: 9/10, step 9/574 completed (loss: 0.0900547206401825, acc: 0.9677419066429138)
[2024-11-08 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:43][root][INFO] - Training Epoch: 9/10, step 10/574 completed (loss: 0.0020605376921594143, acc: 1.0)
[2024-11-08 04:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:43][root][INFO] - Training Epoch: 9/10, step 11/574 completed (loss: 0.0008742452482692897, acc: 1.0)
[2024-11-08 04:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:43][root][INFO] - Training Epoch: 9/10, step 12/574 completed (loss: 0.00237709260545671, acc: 1.0)
[2024-11-08 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:44][root][INFO] - Training Epoch: 9/10, step 13/574 completed (loss: 0.00797789916396141, acc: 1.0)
[2024-11-08 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:44][root][INFO] - Training Epoch: 9/10, step 14/574 completed (loss: 0.0015129599487408996, acc: 1.0)
[2024-11-08 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:45][root][INFO] - Training Epoch: 9/10, step 15/574 completed (loss: 0.0004903806839138269, acc: 1.0)
[2024-11-08 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:45][root][INFO] - Training Epoch: 9/10, step 16/574 completed (loss: 0.0003341349947731942, acc: 1.0)
[2024-11-08 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:45][root][INFO] - Training Epoch: 9/10, step 17/574 completed (loss: 0.001998600782826543, acc: 1.0)
[2024-11-08 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:46][root][INFO] - Training Epoch: 9/10, step 18/574 completed (loss: 0.0790242850780487, acc: 0.9545454382896423)
[2024-11-08 04:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:46][root][INFO] - Training Epoch: 9/10, step 19/574 completed (loss: 0.0685393363237381, acc: 1.0)
[2024-11-08 04:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:47][root][INFO] - Training Epoch: 9/10, step 20/574 completed (loss: 0.07732175290584564, acc: 0.9473684430122375)
[2024-11-08 04:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:47][root][INFO] - Training Epoch: 9/10, step 21/574 completed (loss: 0.003187000984326005, acc: 1.0)
[2024-11-08 04:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:47][root][INFO] - Training Epoch: 9/10, step 22/574 completed (loss: 0.005935856606811285, acc: 1.0)
[2024-11-08 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:48][root][INFO] - Training Epoch: 9/10, step 23/574 completed (loss: 0.00025235716020688415, acc: 1.0)
[2024-11-08 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:48][root][INFO] - Training Epoch: 9/10, step 24/574 completed (loss: 0.18319827318191528, acc: 0.9333333373069763)
[2024-11-08 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:48][root][INFO] - Training Epoch: 9/10, step 25/574 completed (loss: 0.026204921305179596, acc: 1.0)
[2024-11-08 04:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:49][root][INFO] - Training Epoch: 9/10, step 26/574 completed (loss: 0.1447135955095291, acc: 0.9523809552192688)
[2024-11-08 04:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:51][root][INFO] - Training Epoch: 9/10, step 27/574 completed (loss: 0.17392957210540771, acc: 0.9473684430122375)
[2024-11-08 04:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:51][root][INFO] - Training Epoch: 9/10, step 28/574 completed (loss: 0.09615811705589294, acc: 0.9545454382896423)
[2024-11-08 04:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:51][root][INFO] - Training Epoch: 9/10, step 29/574 completed (loss: 0.009226586669683456, acc: 1.0)
[2024-11-08 04:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:52][root][INFO] - Training Epoch: 9/10, step 30/574 completed (loss: 0.039926979690790176, acc: 0.9545454382896423)
[2024-11-08 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:52][root][INFO] - Training Epoch: 9/10, step 31/574 completed (loss: 0.033109892159700394, acc: 0.9655172228813171)
[2024-11-08 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:52][root][INFO] - Training Epoch: 9/10, step 32/574 completed (loss: 0.018041621893644333, acc: 1.0)
[2024-11-08 04:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:53][root][INFO] - Training Epoch: 9/10, step 33/574 completed (loss: 0.0007155992789193988, acc: 1.0)
[2024-11-08 04:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:53][root][INFO] - Training Epoch: 9/10, step 34/574 completed (loss: 0.13309882581233978, acc: 0.9523809552192688)
[2024-11-08 04:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:53][root][INFO] - Training Epoch: 9/10, step 35/574 completed (loss: 0.006232231389731169, acc: 1.0)
[2024-11-08 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:54][root][INFO] - Training Epoch: 9/10, step 36/574 completed (loss: 0.0017569022020325065, acc: 1.0)
[2024-11-08 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:54][root][INFO] - Training Epoch: 9/10, step 37/574 completed (loss: 0.002314099809154868, acc: 1.0)
[2024-11-08 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:55][root][INFO] - Training Epoch: 9/10, step 38/574 completed (loss: 0.0006798689719289541, acc: 1.0)
[2024-11-08 04:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:55][root][INFO] - Training Epoch: 9/10, step 39/574 completed (loss: 0.100941002368927, acc: 0.9642857313156128)
[2024-11-08 04:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:55][root][INFO] - Training Epoch: 9/10, step 40/574 completed (loss: 0.16585145890712738, acc: 0.9428571462631226)
[2024-11-08 04:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:56][root][INFO] - Training Epoch: 9/10, step 41/574 completed (loss: 0.0008831190061755478, acc: 1.0)
[2024-11-08 04:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:56][root][INFO] - Training Epoch: 9/10, step 42/574 completed (loss: 0.07410808652639389, acc: 0.949999988079071)
[2024-11-08 04:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:57][root][INFO] - Training Epoch: 9/10, step 43/574 completed (loss: 0.03805068880319595, acc: 1.0)
[2024-11-08 04:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:57][root][INFO] - Training Epoch: 9/10, step 44/574 completed (loss: 0.08460597693920135, acc: 0.9545454382896423)
[2024-11-08 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:58][root][INFO] - Training Epoch: 9/10, step 45/574 completed (loss: 0.17095382511615753, acc: 0.9523809552192688)
[2024-11-08 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:58][root][INFO] - Training Epoch: 9/10, step 46/574 completed (loss: 0.1265103965997696, acc: 0.9583333134651184)
[2024-11-08 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:59][root][INFO] - Training Epoch: 9/10, step 47/574 completed (loss: 0.011456567794084549, acc: 1.0)
[2024-11-08 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:59][root][INFO] - Training Epoch: 9/10, step 48/574 completed (loss: 0.030934004113078117, acc: 0.9714285731315613)
[2024-11-08 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:27:59][root][INFO] - Training Epoch: 9/10, step 49/574 completed (loss: 0.001826753723435104, acc: 1.0)
[2024-11-08 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:00][root][INFO] - Training Epoch: 9/10, step 50/574 completed (loss: 0.0019215093925595284, acc: 1.0)
[2024-11-08 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:00][root][INFO] - Training Epoch: 9/10, step 51/574 completed (loss: 0.0030087607447057962, acc: 1.0)
[2024-11-08 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:00][root][INFO] - Training Epoch: 9/10, step 52/574 completed (loss: 0.013085046783089638, acc: 1.0)
[2024-11-08 04:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:01][root][INFO] - Training Epoch: 9/10, step 53/574 completed (loss: 0.0006395586533471942, acc: 1.0)
[2024-11-08 04:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:01][root][INFO] - Training Epoch: 9/10, step 54/574 completed (loss: 0.13736535608768463, acc: 0.9545454382896423)
[2024-11-08 04:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:02][root][INFO] - Training Epoch: 9/10, step 55/574 completed (loss: 0.002671100664883852, acc: 1.0)
[2024-11-08 04:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:06][root][INFO] - Training Epoch: 9/10, step 56/574 completed (loss: 0.015485427342355251, acc: 1.0)
[2024-11-08 04:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:08][root][INFO] - Training Epoch: 9/10, step 57/574 completed (loss: 0.09012708067893982, acc: 0.9523809552192688)
[2024-11-08 04:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:09][root][INFO] - Training Epoch: 9/10, step 58/574 completed (loss: 0.0014415897894650698, acc: 1.0)
[2024-11-08 04:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:09][root][INFO] - Training Epoch: 9/10, step 59/574 completed (loss: 0.1417962610721588, acc: 0.9090909361839294)
[2024-11-08 04:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:10][root][INFO] - Training Epoch: 9/10, step 60/574 completed (loss: 0.0005970639758743346, acc: 1.0)
[2024-11-08 04:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:11][root][INFO] - Training Epoch: 9/10, step 61/574 completed (loss: 0.04362640902400017, acc: 1.0)
[2024-11-08 04:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:11][root][INFO] - Training Epoch: 9/10, step 62/574 completed (loss: 0.012177768163383007, acc: 1.0)
[2024-11-08 04:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:11][root][INFO] - Training Epoch: 9/10, step 63/574 completed (loss: 0.007108776364475489, acc: 1.0)
[2024-11-08 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:12][root][INFO] - Training Epoch: 9/10, step 64/574 completed (loss: 0.010104464367032051, acc: 1.0)
[2024-11-08 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:12][root][INFO] - Training Epoch: 9/10, step 65/574 completed (loss: 0.020461851730942726, acc: 1.0)
[2024-11-08 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:13][root][INFO] - Training Epoch: 9/10, step 66/574 completed (loss: 0.12159647792577744, acc: 0.9473684430122375)
[2024-11-08 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:13][root][INFO] - Training Epoch: 9/10, step 67/574 completed (loss: 0.008196448907256126, acc: 1.0)
[2024-11-08 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:14][root][INFO] - Training Epoch: 9/10, step 68/574 completed (loss: 0.005678245332092047, acc: 1.0)
[2024-11-08 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:14][root][INFO] - Training Epoch: 9/10, step 69/574 completed (loss: 0.0018182037165388465, acc: 1.0)
[2024-11-08 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:14][root][INFO] - Training Epoch: 9/10, step 70/574 completed (loss: 0.0854276716709137, acc: 0.9629629850387573)
[2024-11-08 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:15][root][INFO] - Training Epoch: 9/10, step 71/574 completed (loss: 0.09001792222261429, acc: 0.9666666388511658)
[2024-11-08 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:15][root][INFO] - Training Epoch: 9/10, step 72/574 completed (loss: 0.1704213172197342, acc: 0.9047619104385376)
[2024-11-08 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:15][root][INFO] - Training Epoch: 9/10, step 73/574 completed (loss: 0.15605506300926208, acc: 0.8947368264198303)
[2024-11-08 04:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:16][root][INFO] - Training Epoch: 9/10, step 74/574 completed (loss: 0.10929137468338013, acc: 0.949999988079071)
[2024-11-08 04:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:16][root][INFO] - Training Epoch: 9/10, step 75/574 completed (loss: 0.010682915337383747, acc: 1.0)
[2024-11-08 04:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:17][root][INFO] - Training Epoch: 9/10, step 76/574 completed (loss: 0.020641829818487167, acc: 1.0)
[2024-11-08 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:17][root][INFO] - Training Epoch: 9/10, step 77/574 completed (loss: 0.007065538316965103, acc: 1.0)
[2024-11-08 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:17][root][INFO] - Training Epoch: 9/10, step 78/574 completed (loss: 0.1041046753525734, acc: 0.9629629850387573)
[2024-11-08 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:18][root][INFO] - Training Epoch: 9/10, step 79/574 completed (loss: 0.003444841830059886, acc: 1.0)
[2024-11-08 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:18][root][INFO] - Training Epoch: 9/10, step 80/574 completed (loss: 0.0020079014357179403, acc: 1.0)
[2024-11-08 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:19][root][INFO] - Training Epoch: 9/10, step 81/574 completed (loss: 0.0011543647851794958, acc: 1.0)
[2024-11-08 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:19][root][INFO] - Training Epoch: 9/10, step 82/574 completed (loss: 0.004249710589647293, acc: 1.0)
[2024-11-08 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:19][root][INFO] - Training Epoch: 9/10, step 83/574 completed (loss: 0.002915463875979185, acc: 1.0)
[2024-11-08 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:20][root][INFO] - Training Epoch: 9/10, step 84/574 completed (loss: 0.03973497077822685, acc: 1.0)
[2024-11-08 04:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:20][root][INFO] - Training Epoch: 9/10, step 85/574 completed (loss: 0.03334002569317818, acc: 1.0)
[2024-11-08 04:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:20][root][INFO] - Training Epoch: 9/10, step 86/574 completed (loss: 0.040374282747507095, acc: 1.0)
[2024-11-08 04:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:21][root][INFO] - Training Epoch: 9/10, step 87/574 completed (loss: 0.07481428980827332, acc: 0.9615384340286255)
[2024-11-08 04:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:21][root][INFO] - Training Epoch: 9/10, step 88/574 completed (loss: 0.0637916550040245, acc: 0.9523809552192688)
[2024-11-08 04:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:23][root][INFO] - Training Epoch: 9/10, step 89/574 completed (loss: 0.2490866333246231, acc: 0.8947368264198303)
[2024-11-08 04:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:24][root][INFO] - Training Epoch: 9/10, step 90/574 completed (loss: 0.1682453751564026, acc: 0.9473684430122375)
[2024-11-08 04:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:26][root][INFO] - Training Epoch: 9/10, step 91/574 completed (loss: 0.07622203230857849, acc: 0.9545454382896423)
[2024-11-08 04:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:27][root][INFO] - Training Epoch: 9/10, step 92/574 completed (loss: 0.05608349293470383, acc: 1.0)
[2024-11-08 04:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:28][root][INFO] - Training Epoch: 9/10, step 93/574 completed (loss: 0.29192888736724854, acc: 0.9230769276618958)
[2024-11-08 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:29][root][INFO] - Training Epoch: 9/10, step 94/574 completed (loss: 0.2184523344039917, acc: 0.9642857313156128)
[2024-11-08 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:29][root][INFO] - Training Epoch: 9/10, step 95/574 completed (loss: 0.029620500281453133, acc: 1.0)
[2024-11-08 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:30][root][INFO] - Training Epoch: 9/10, step 96/574 completed (loss: 0.010977008379995823, acc: 1.0)
[2024-11-08 04:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:30][root][INFO] - Training Epoch: 9/10, step 97/574 completed (loss: 0.08494772017002106, acc: 1.0)
[2024-11-08 04:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:31][root][INFO] - Training Epoch: 9/10, step 98/574 completed (loss: 0.21161356568336487, acc: 0.8571428656578064)
[2024-11-08 04:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:31][root][INFO] - Training Epoch: 9/10, step 99/574 completed (loss: 0.007639072369784117, acc: 1.0)
[2024-11-08 04:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:31][root][INFO] - Training Epoch: 9/10, step 100/574 completed (loss: 0.11751393973827362, acc: 0.9523809552192688)
[2024-11-08 04:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:32][root][INFO] - Training Epoch: 9/10, step 101/574 completed (loss: 0.14734721183776855, acc: 0.9599999785423279)
[2024-11-08 04:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:32][root][INFO] - Training Epoch: 9/10, step 102/574 completed (loss: 0.0033750012516975403, acc: 1.0)
[2024-11-08 04:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:33][root][INFO] - Training Epoch: 9/10, step 103/574 completed (loss: 0.007107540965080261, acc: 1.0)
[2024-11-08 04:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:33][root][INFO] - Training Epoch: 9/10, step 104/574 completed (loss: 0.010296943597495556, acc: 1.0)
[2024-11-08 04:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:34][root][INFO] - Training Epoch: 9/10, step 105/574 completed (loss: 0.0011873405892401934, acc: 1.0)
[2024-11-08 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:34][root][INFO] - Training Epoch: 9/10, step 106/574 completed (loss: 0.15343745052814484, acc: 0.9545454382896423)
[2024-11-08 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:34][root][INFO] - Training Epoch: 9/10, step 107/574 completed (loss: 0.0027397319208830595, acc: 1.0)
[2024-11-08 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:35][root][INFO] - Training Epoch: 9/10, step 108/574 completed (loss: 0.025818930938839912, acc: 1.0)
[2024-11-08 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:35][root][INFO] - Training Epoch: 9/10, step 109/574 completed (loss: 0.18175379931926727, acc: 0.9523809552192688)
[2024-11-08 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:36][root][INFO] - Training Epoch: 9/10, step 110/574 completed (loss: 0.003297274000942707, acc: 1.0)
[2024-11-08 04:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:36][root][INFO] - Training Epoch: 9/10, step 111/574 completed (loss: 0.008379093371331692, acc: 1.0)
[2024-11-08 04:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:37][root][INFO] - Training Epoch: 9/10, step 112/574 completed (loss: 0.012095022946596146, acc: 1.0)
[2024-11-08 04:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:37][root][INFO] - Training Epoch: 9/10, step 113/574 completed (loss: 0.0011141002178192139, acc: 1.0)
[2024-11-08 04:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:37][root][INFO] - Training Epoch: 9/10, step 114/574 completed (loss: 0.0031731536146253347, acc: 1.0)
[2024-11-08 04:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:38][root][INFO] - Training Epoch: 9/10, step 115/574 completed (loss: 0.00385342282243073, acc: 1.0)
[2024-11-08 04:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:38][root][INFO] - Training Epoch: 9/10, step 116/574 completed (loss: 0.0016009750543162227, acc: 1.0)
[2024-11-08 04:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:39][root][INFO] - Training Epoch: 9/10, step 117/574 completed (loss: 0.003049304708838463, acc: 1.0)
[2024-11-08 04:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:39][root][INFO] - Training Epoch: 9/10, step 118/574 completed (loss: 0.0008001801907084882, acc: 1.0)
[2024-11-08 04:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:40][root][INFO] - Training Epoch: 9/10, step 119/574 completed (loss: 0.010110476985573769, acc: 1.0)
[2024-11-08 04:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:41][root][INFO] - Training Epoch: 9/10, step 120/574 completed (loss: 0.000769547710660845, acc: 1.0)
[2024-11-08 04:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:41][root][INFO] - Training Epoch: 9/10, step 121/574 completed (loss: 0.026725010946393013, acc: 1.0)
[2024-11-08 04:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:42][root][INFO] - Training Epoch: 9/10, step 122/574 completed (loss: 0.0034445871133357286, acc: 1.0)
[2024-11-08 04:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:42][root][INFO] - Training Epoch: 9/10, step 123/574 completed (loss: 0.03343332186341286, acc: 1.0)
[2024-11-08 04:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:42][root][INFO] - Training Epoch: 9/10, step 124/574 completed (loss: 0.026872137561440468, acc: 1.0)
[2024-11-08 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:43][root][INFO] - Training Epoch: 9/10, step 125/574 completed (loss: 0.1822715699672699, acc: 0.949999988079071)
[2024-11-08 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:43][root][INFO] - Training Epoch: 9/10, step 126/574 completed (loss: 0.08261595666408539, acc: 0.9473684430122375)
[2024-11-08 04:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:25][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3124, device='cuda:0') eval_epoch_loss=tensor(0.2718, device='cuda:0') eval_epoch_acc=tensor(0.9351, device='cuda:0')
[2024-11-08 04:29:25][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:29:25][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:29:29][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_9_step_127_loss_0.2718285024166107/model.pt
[2024-11-08 04:29:29][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:29][root][INFO] - Training Epoch: 9/10, step 127/574 completed (loss: 0.14313556253910065, acc: 0.9090909361839294)
[2024-11-08 04:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:30][root][INFO] - Training Epoch: 9/10, step 128/574 completed (loss: 0.021741434931755066, acc: 1.0)
[2024-11-08 04:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:30][root][INFO] - Training Epoch: 9/10, step 129/574 completed (loss: 0.0025290122721344233, acc: 1.0)
[2024-11-08 04:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:31][root][INFO] - Training Epoch: 9/10, step 130/574 completed (loss: 0.034178443253040314, acc: 0.9696969985961914)
[2024-11-08 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:31][root][INFO] - Training Epoch: 9/10, step 131/574 completed (loss: 0.1274290382862091, acc: 1.0)
[2024-11-08 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:31][root][INFO] - Training Epoch: 9/10, step 132/574 completed (loss: 0.1252823770046234, acc: 0.939393937587738)
[2024-11-08 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:32][root][INFO] - Training Epoch: 9/10, step 133/574 completed (loss: 0.008240626193583012, acc: 1.0)
[2024-11-08 04:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:32][root][INFO] - Training Epoch: 9/10, step 134/574 completed (loss: 0.0005148693453520536, acc: 1.0)
[2024-11-08 04:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:32][root][INFO] - Training Epoch: 9/10, step 135/574 completed (loss: 0.07146769762039185, acc: 0.9473684430122375)
[2024-11-08 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:33][root][INFO] - Training Epoch: 9/10, step 136/574 completed (loss: 0.0008293894934467971, acc: 1.0)
[2024-11-08 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:33][root][INFO] - Training Epoch: 9/10, step 137/574 completed (loss: 0.2538208067417145, acc: 0.949999988079071)
[2024-11-08 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:33][root][INFO] - Training Epoch: 9/10, step 138/574 completed (loss: 0.015573554672300816, acc: 1.0)
[2024-11-08 04:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:34][root][INFO] - Training Epoch: 9/10, step 139/574 completed (loss: 0.003758709877729416, acc: 1.0)
[2024-11-08 04:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:34][root][INFO] - Training Epoch: 9/10, step 140/574 completed (loss: 0.037256259471178055, acc: 0.9629629850387573)
[2024-11-08 04:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:35][root][INFO] - Training Epoch: 9/10, step 141/574 completed (loss: 0.09314008057117462, acc: 0.9696969985961914)
[2024-11-08 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:35][root][INFO] - Training Epoch: 9/10, step 142/574 completed (loss: 0.15565048158168793, acc: 0.949999988079071)
[2024-11-08 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:36][root][INFO] - Training Epoch: 9/10, step 143/574 completed (loss: 0.01744261011481285, acc: 1.0)
[2024-11-08 04:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:36][root][INFO] - Training Epoch: 9/10, step 144/574 completed (loss: 0.014931154437363148, acc: 1.0)
[2024-11-08 04:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:37][root][INFO] - Training Epoch: 9/10, step 145/574 completed (loss: 0.0013526534894481301, acc: 1.0)
[2024-11-08 04:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:37][root][INFO] - Training Epoch: 9/10, step 146/574 completed (loss: 0.025219956412911415, acc: 1.0)
[2024-11-08 04:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:38][root][INFO] - Training Epoch: 9/10, step 147/574 completed (loss: 0.007670379709452391, acc: 1.0)
[2024-11-08 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:38][root][INFO] - Training Epoch: 9/10, step 148/574 completed (loss: 0.020023852586746216, acc: 1.0)
[2024-11-08 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:38][root][INFO] - Training Epoch: 9/10, step 149/574 completed (loss: 0.003560112789273262, acc: 1.0)
[2024-11-08 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:39][root][INFO] - Training Epoch: 9/10, step 150/574 completed (loss: 0.010504284873604774, acc: 1.0)
[2024-11-08 04:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:39][root][INFO] - Training Epoch: 9/10, step 151/574 completed (loss: 0.00952657125890255, acc: 1.0)
[2024-11-08 04:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:39][root][INFO] - Training Epoch: 9/10, step 152/574 completed (loss: 0.002560727298259735, acc: 1.0)
[2024-11-08 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:40][root][INFO] - Training Epoch: 9/10, step 153/574 completed (loss: 0.01457994431257248, acc: 1.0)
[2024-11-08 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:40][root][INFO] - Training Epoch: 9/10, step 154/574 completed (loss: 0.002248788019642234, acc: 1.0)
[2024-11-08 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:40][root][INFO] - Training Epoch: 9/10, step 155/574 completed (loss: 0.04045684263110161, acc: 1.0)
[2024-11-08 04:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:41][root][INFO] - Training Epoch: 9/10, step 156/574 completed (loss: 0.06897354125976562, acc: 1.0)
[2024-11-08 04:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:41][root][INFO] - Training Epoch: 9/10, step 157/574 completed (loss: 0.38208678364753723, acc: 0.8787878751754761)
[2024-11-08 04:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:44][root][INFO] - Training Epoch: 9/10, step 158/574 completed (loss: 0.15461240708827972, acc: 0.949999988079071)
[2024-11-08 04:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:44][root][INFO] - Training Epoch: 9/10, step 159/574 completed (loss: 0.01841430924832821, acc: 1.0)
[2024-11-08 04:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:45][root][INFO] - Training Epoch: 9/10, step 160/574 completed (loss: 0.011875257827341557, acc: 1.0)
[2024-11-08 04:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:46][root][INFO] - Training Epoch: 9/10, step 161/574 completed (loss: 0.1782693862915039, acc: 0.9545454382896423)
[2024-11-08 04:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:46][root][INFO] - Training Epoch: 9/10, step 162/574 completed (loss: 0.1705756038427353, acc: 0.949999988079071)
[2024-11-08 04:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:47][root][INFO] - Training Epoch: 9/10, step 163/574 completed (loss: 0.07174396514892578, acc: 1.0)
[2024-11-08 04:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:47][root][INFO] - Training Epoch: 9/10, step 164/574 completed (loss: 0.05103633552789688, acc: 0.9629629850387573)
[2024-11-08 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:48][root][INFO] - Training Epoch: 9/10, step 165/574 completed (loss: 0.15236490964889526, acc: 0.9722222089767456)
[2024-11-08 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:48][root][INFO] - Training Epoch: 9/10, step 166/574 completed (loss: 0.00626319320872426, acc: 1.0)
[2024-11-08 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:49][root][INFO] - Training Epoch: 9/10, step 167/574 completed (loss: 0.015584571287035942, acc: 1.0)
[2024-11-08 04:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:49][root][INFO] - Training Epoch: 9/10, step 168/574 completed (loss: 0.0035423808731138706, acc: 1.0)
[2024-11-08 04:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:50][root][INFO] - Training Epoch: 9/10, step 169/574 completed (loss: 0.00032180972630158067, acc: 1.0)
[2024-11-08 04:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:51][root][INFO] - Training Epoch: 9/10, step 170/574 completed (loss: 0.0012766264844685793, acc: 1.0)
[2024-11-08 04:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:52][root][INFO] - Training Epoch: 9/10, step 171/574 completed (loss: 0.0044044749811291695, acc: 1.0)
[2024-11-08 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:52][root][INFO] - Training Epoch: 9/10, step 172/574 completed (loss: 0.0015319663798436522, acc: 1.0)
[2024-11-08 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:53][root][INFO] - Training Epoch: 9/10, step 173/574 completed (loss: 0.1079077199101448, acc: 0.9666666388511658)
[2024-11-08 04:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:53][root][INFO] - Training Epoch: 9/10, step 174/574 completed (loss: 0.12672488391399384, acc: 0.9523809552192688)
[2024-11-08 04:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:54][root][INFO] - Training Epoch: 9/10, step 175/574 completed (loss: 0.015615216456353664, acc: 1.0)
[2024-11-08 04:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:54][root][INFO] - Training Epoch: 9/10, step 176/574 completed (loss: 0.004083608277142048, acc: 1.0)
[2024-11-08 04:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:56][root][INFO] - Training Epoch: 9/10, step 177/574 completed (loss: 0.042670849710702896, acc: 1.0)
[2024-11-08 04:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:57][root][INFO] - Training Epoch: 9/10, step 178/574 completed (loss: 0.004850487690418959, acc: 1.0)
[2024-11-08 04:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:57][root][INFO] - Training Epoch: 9/10, step 179/574 completed (loss: 0.00022669088502880186, acc: 1.0)
[2024-11-08 04:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:57][root][INFO] - Training Epoch: 9/10, step 180/574 completed (loss: 0.0017375218449160457, acc: 1.0)
[2024-11-08 04:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:58][root][INFO] - Training Epoch: 9/10, step 181/574 completed (loss: 0.0061533828265964985, acc: 1.0)
[2024-11-08 04:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:58][root][INFO] - Training Epoch: 9/10, step 182/574 completed (loss: 0.010663933120667934, acc: 1.0)
[2024-11-08 04:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:59][root][INFO] - Training Epoch: 9/10, step 183/574 completed (loss: 0.0032097662333399057, acc: 1.0)
[2024-11-08 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:59][root][INFO] - Training Epoch: 9/10, step 184/574 completed (loss: 0.00039094939711503685, acc: 1.0)
[2024-11-08 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:29:59][root][INFO] - Training Epoch: 9/10, step 185/574 completed (loss: 0.01253172941505909, acc: 1.0)
[2024-11-08 04:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:00][root][INFO] - Training Epoch: 9/10, step 186/574 completed (loss: 0.03215422481298447, acc: 1.0)
[2024-11-08 04:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:01][root][INFO] - Training Epoch: 9/10, step 187/574 completed (loss: 0.03482836112380028, acc: 1.0)
[2024-11-08 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:01][root][INFO] - Training Epoch: 9/10, step 188/574 completed (loss: 0.03308510780334473, acc: 1.0)
[2024-11-08 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:02][root][INFO] - Training Epoch: 9/10, step 189/574 completed (loss: 0.002776025328785181, acc: 1.0)
[2024-11-08 04:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:02][root][INFO] - Training Epoch: 9/10, step 190/574 completed (loss: 0.062399595975875854, acc: 0.949999988079071)
[2024-11-08 04:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:04][root][INFO] - Training Epoch: 9/10, step 191/574 completed (loss: 0.1953122764825821, acc: 0.9047619104385376)
[2024-11-08 04:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:05][root][INFO] - Training Epoch: 9/10, step 192/574 completed (loss: 0.1929168701171875, acc: 0.8947368264198303)
[2024-11-08 04:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:06][root][INFO] - Training Epoch: 9/10, step 193/574 completed (loss: 0.05840037018060684, acc: 0.9545454382896423)
[2024-11-08 04:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:08][root][INFO] - Training Epoch: 9/10, step 194/574 completed (loss: 0.2942116856575012, acc: 0.9047619104385376)
[2024-11-08 04:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:10][root][INFO] - Training Epoch: 9/10, step 195/574 completed (loss: 0.028189657256007195, acc: 1.0)
[2024-11-08 04:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:10][root][INFO] - Training Epoch: 9/10, step 196/574 completed (loss: 0.00025539781199768186, acc: 1.0)
[2024-11-08 04:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:11][root][INFO] - Training Epoch: 9/10, step 197/574 completed (loss: 0.01690511777997017, acc: 1.0)
[2024-11-08 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:11][root][INFO] - Training Epoch: 9/10, step 198/574 completed (loss: 0.006615969352424145, acc: 1.0)
[2024-11-08 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:11][root][INFO] - Training Epoch: 9/10, step 199/574 completed (loss: 0.001077106106095016, acc: 1.0)
[2024-11-08 04:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:12][root][INFO] - Training Epoch: 9/10, step 200/574 completed (loss: 0.08986063301563263, acc: 0.9473684430122375)
[2024-11-08 04:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:12][root][INFO] - Training Epoch: 9/10, step 201/574 completed (loss: 0.280992716550827, acc: 0.9090909361839294)
[2024-11-08 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:13][root][INFO] - Training Epoch: 9/10, step 202/574 completed (loss: 0.07599766552448273, acc: 0.949999988079071)
[2024-11-08 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:13][root][INFO] - Training Epoch: 9/10, step 203/574 completed (loss: 0.005838753189891577, acc: 1.0)
[2024-11-08 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:14][root][INFO] - Training Epoch: 9/10, step 204/574 completed (loss: 0.27789735794067383, acc: 0.9642857313156128)
[2024-11-08 04:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:14][root][INFO] - Training Epoch: 9/10, step 205/574 completed (loss: 0.017653813585639, acc: 1.0)
[2024-11-08 04:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:15][root][INFO] - Training Epoch: 9/10, step 206/574 completed (loss: 0.0010793376713991165, acc: 1.0)
[2024-11-08 04:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:15][root][INFO] - Training Epoch: 9/10, step 207/574 completed (loss: 0.0005885213031433523, acc: 1.0)
[2024-11-08 04:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:15][root][INFO] - Training Epoch: 9/10, step 208/574 completed (loss: 0.21591812372207642, acc: 0.9545454382896423)
[2024-11-08 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:16][root][INFO] - Training Epoch: 9/10, step 209/574 completed (loss: 0.03740870952606201, acc: 1.0)
[2024-11-08 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:16][root][INFO] - Training Epoch: 9/10, step 210/574 completed (loss: 0.00250205653719604, acc: 1.0)
[2024-11-08 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:17][root][INFO] - Training Epoch: 9/10, step 211/574 completed (loss: 0.008516227826476097, acc: 1.0)
[2024-11-08 04:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:17][root][INFO] - Training Epoch: 9/10, step 212/574 completed (loss: 0.002584836445748806, acc: 1.0)
[2024-11-08 04:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:18][root][INFO] - Training Epoch: 9/10, step 213/574 completed (loss: 0.16740156710147858, acc: 0.9615384340286255)
[2024-11-08 04:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:19][root][INFO] - Training Epoch: 9/10, step 214/574 completed (loss: 0.06949124485254288, acc: 0.9523809552192688)
[2024-11-08 04:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:19][root][INFO] - Training Epoch: 9/10, step 215/574 completed (loss: 0.10781718790531158, acc: 0.9473684430122375)
[2024-11-08 04:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:19][root][INFO] - Training Epoch: 9/10, step 216/574 completed (loss: 0.1925138682126999, acc: 0.9473684430122375)
[2024-11-08 04:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:20][root][INFO] - Training Epoch: 9/10, step 217/574 completed (loss: 0.0004089252033736557, acc: 1.0)
[2024-11-08 04:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:21][root][INFO] - Training Epoch: 9/10, step 218/574 completed (loss: 0.07267015427350998, acc: 1.0)
[2024-11-08 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:21][root][INFO] - Training Epoch: 9/10, step 219/574 completed (loss: 0.0028378318529576063, acc: 1.0)
[2024-11-08 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:21][root][INFO] - Training Epoch: 9/10, step 220/574 completed (loss: 0.04337171092629433, acc: 0.9677419066429138)
[2024-11-08 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:22][root][INFO] - Training Epoch: 9/10, step 221/574 completed (loss: 0.048605505377054214, acc: 0.9677419066429138)
[2024-11-08 04:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:22][root][INFO] - Training Epoch: 9/10, step 222/574 completed (loss: 0.07483701407909393, acc: 0.9615384340286255)
[2024-11-08 04:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:23][root][INFO] - Training Epoch: 9/10, step 223/574 completed (loss: 0.23166465759277344, acc: 0.9523809552192688)
[2024-11-08 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:24][root][INFO] - Training Epoch: 9/10, step 224/574 completed (loss: 0.013920892030000687, acc: 1.0)
[2024-11-08 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:25][root][INFO] - Training Epoch: 9/10, step 225/574 completed (loss: 0.05605726316571236, acc: 0.9473684430122375)
[2024-11-08 04:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:25][root][INFO] - Training Epoch: 9/10, step 226/574 completed (loss: 0.040410201996564865, acc: 1.0)
[2024-11-08 04:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:25][root][INFO] - Training Epoch: 9/10, step 227/574 completed (loss: 0.061189763247966766, acc: 0.9523809552192688)
[2024-11-08 04:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:26][root][INFO] - Training Epoch: 9/10, step 228/574 completed (loss: 0.14623506367206573, acc: 0.9583333134651184)
[2024-11-08 04:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:26][root][INFO] - Training Epoch: 9/10, step 229/574 completed (loss: 0.2670929431915283, acc: 0.9130434989929199)
[2024-11-08 04:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:27][root][INFO] - Training Epoch: 9/10, step 230/574 completed (loss: 0.16462714970111847, acc: 0.9523809552192688)
[2024-11-08 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:27][root][INFO] - Training Epoch: 9/10, step 231/574 completed (loss: 0.019171444699168205, acc: 1.0)
[2024-11-08 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:28][root][INFO] - Training Epoch: 9/10, step 232/574 completed (loss: 0.014996202662587166, acc: 1.0)
[2024-11-08 04:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:28][root][INFO] - Training Epoch: 9/10, step 233/574 completed (loss: 0.048529576510190964, acc: 1.0)
[2024-11-08 04:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:29][root][INFO] - Training Epoch: 9/10, step 234/574 completed (loss: 0.14763914048671722, acc: 0.9545454382896423)
[2024-11-08 04:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:29][root][INFO] - Training Epoch: 9/10, step 235/574 completed (loss: 0.03113528899848461, acc: 1.0)
[2024-11-08 04:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:30][root][INFO] - Training Epoch: 9/10, step 236/574 completed (loss: 0.04272972047328949, acc: 1.0)
[2024-11-08 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:30][root][INFO] - Training Epoch: 9/10, step 237/574 completed (loss: 0.007238299120217562, acc: 1.0)
[2024-11-08 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:30][root][INFO] - Training Epoch: 9/10, step 238/574 completed (loss: 0.05089219659566879, acc: 0.9545454382896423)
[2024-11-08 04:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:31][root][INFO] - Training Epoch: 9/10, step 239/574 completed (loss: 0.05781576782464981, acc: 0.9523809552192688)
[2024-11-08 04:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:31][root][INFO] - Training Epoch: 9/10, step 240/574 completed (loss: 0.0060443030670285225, acc: 1.0)
[2024-11-08 04:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:31][root][INFO] - Training Epoch: 9/10, step 241/574 completed (loss: 0.006003186106681824, acc: 1.0)
[2024-11-08 04:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:32][root][INFO] - Training Epoch: 9/10, step 242/574 completed (loss: 0.02454494684934616, acc: 1.0)
[2024-11-08 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:33][root][INFO] - Training Epoch: 9/10, step 243/574 completed (loss: 0.051274433732032776, acc: 0.9545454382896423)
[2024-11-08 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:34][root][INFO] - Training Epoch: 9/10, step 244/574 completed (loss: 0.049958717077970505, acc: 0.9642857313156128)
[2024-11-08 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:34][root][INFO] - Training Epoch: 9/10, step 245/574 completed (loss: 0.0453394390642643, acc: 1.0)
[2024-11-08 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:34][root][INFO] - Training Epoch: 9/10, step 246/574 completed (loss: 0.002922977553680539, acc: 1.0)
[2024-11-08 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:35][root][INFO] - Training Epoch: 9/10, step 247/574 completed (loss: 0.006987881381064653, acc: 1.0)
[2024-11-08 04:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:35][root][INFO] - Training Epoch: 9/10, step 248/574 completed (loss: 0.04761834442615509, acc: 0.9523809552192688)
[2024-11-08 04:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:35][root][INFO] - Training Epoch: 9/10, step 249/574 completed (loss: 0.0028906683437526226, acc: 1.0)
[2024-11-08 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:36][root][INFO] - Training Epoch: 9/10, step 250/574 completed (loss: 0.043625470250844955, acc: 1.0)
[2024-11-08 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:36][root][INFO] - Training Epoch: 9/10, step 251/574 completed (loss: 0.013479547575116158, acc: 1.0)
[2024-11-08 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:37][root][INFO] - Training Epoch: 9/10, step 252/574 completed (loss: 0.00259625562466681, acc: 1.0)
[2024-11-08 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:37][root][INFO] - Training Epoch: 9/10, step 253/574 completed (loss: 0.0007560738595202565, acc: 1.0)
[2024-11-08 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:37][root][INFO] - Training Epoch: 9/10, step 254/574 completed (loss: 0.00041830280679278076, acc: 1.0)
[2024-11-08 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:38][root][INFO] - Training Epoch: 9/10, step 255/574 completed (loss: 0.0044587282463908195, acc: 1.0)
[2024-11-08 04:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:38][root][INFO] - Training Epoch: 9/10, step 256/574 completed (loss: 0.002248026430606842, acc: 1.0)
[2024-11-08 04:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:38][root][INFO] - Training Epoch: 9/10, step 257/574 completed (loss: 0.0006482814787887037, acc: 1.0)
[2024-11-08 04:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:39][root][INFO] - Training Epoch: 9/10, step 258/574 completed (loss: 0.0015187229728326201, acc: 1.0)
[2024-11-08 04:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:40][root][INFO] - Training Epoch: 9/10, step 259/574 completed (loss: 0.04418559744954109, acc: 0.9545454382896423)
[2024-11-08 04:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:40][root][INFO] - Training Epoch: 9/10, step 260/574 completed (loss: 0.004625273868441582, acc: 1.0)
[2024-11-08 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:41][root][INFO] - Training Epoch: 9/10, step 261/574 completed (loss: 0.048735592514276505, acc: 0.9545454382896423)
[2024-11-08 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:41][root][INFO] - Training Epoch: 9/10, step 262/574 completed (loss: 0.022907668724656105, acc: 0.970588207244873)
[2024-11-08 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:42][root][INFO] - Training Epoch: 9/10, step 263/574 completed (loss: 0.002255787141621113, acc: 1.0)
[2024-11-08 04:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:42][root][INFO] - Training Epoch: 9/10, step 264/574 completed (loss: 0.02173832803964615, acc: 1.0)
[2024-11-08 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:43][root][INFO] - Training Epoch: 9/10, step 265/574 completed (loss: 0.1429634392261505, acc: 0.8947368264198303)
[2024-11-08 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:44][root][INFO] - Training Epoch: 9/10, step 266/574 completed (loss: 0.030683239921927452, acc: 1.0)
[2024-11-08 04:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:44][root][INFO] - Training Epoch: 9/10, step 267/574 completed (loss: 0.02060975320637226, acc: 1.0)
[2024-11-08 04:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:45][root][INFO] - Training Epoch: 9/10, step 268/574 completed (loss: 0.058398038148880005, acc: 0.9655172228813171)
[2024-11-08 04:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:45][root][INFO] - Training Epoch: 9/10, step 269/574 completed (loss: 0.007297653239220381, acc: 1.0)
[2024-11-08 04:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:26][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3633, device='cuda:0') eval_epoch_loss=tensor(0.3099, device='cuda:0') eval_epoch_acc=tensor(0.9341, device='cuda:0')
[2024-11-08 04:31:26][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:31:26][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:31:30][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_9_step_270_loss_0.3099229633808136/model.pt
[2024-11-08 04:31:30][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:30][root][INFO] - Training Epoch: 9/10, step 270/574 completed (loss: 0.21756361424922943, acc: 0.9615384340286255)
[2024-11-08 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:31][root][INFO] - Training Epoch: 9/10, step 271/574 completed (loss: 0.00023600447457283735, acc: 1.0)
[2024-11-08 04:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:31][root][INFO] - Training Epoch: 9/10, step 272/574 completed (loss: 0.0004854426661040634, acc: 1.0)
[2024-11-08 04:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:32][root][INFO] - Training Epoch: 9/10, step 273/574 completed (loss: 0.0064446814358234406, acc: 1.0)
[2024-11-08 04:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:32][root][INFO] - Training Epoch: 9/10, step 274/574 completed (loss: 0.004260377958416939, acc: 1.0)
[2024-11-08 04:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:32][root][INFO] - Training Epoch: 9/10, step 275/574 completed (loss: 0.06937643885612488, acc: 0.9523809552192688)
[2024-11-08 04:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:33][root][INFO] - Training Epoch: 9/10, step 276/574 completed (loss: 0.0004258002736605704, acc: 1.0)
[2024-11-08 04:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:33][root][INFO] - Training Epoch: 9/10, step 277/574 completed (loss: 0.027015240862965584, acc: 1.0)
[2024-11-08 04:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:33][root][INFO] - Training Epoch: 9/10, step 278/574 completed (loss: 0.0036508182529360056, acc: 1.0)
[2024-11-08 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:34][root][INFO] - Training Epoch: 9/10, step 279/574 completed (loss: 0.01838766410946846, acc: 1.0)
[2024-11-08 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:34][root][INFO] - Training Epoch: 9/10, step 280/574 completed (loss: 0.0006097583100199699, acc: 1.0)
[2024-11-08 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:35][root][INFO] - Training Epoch: 9/10, step 281/574 completed (loss: 0.003233951283618808, acc: 1.0)
[2024-11-08 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:35][root][INFO] - Training Epoch: 9/10, step 282/574 completed (loss: 0.0009176957537420094, acc: 1.0)
[2024-11-08 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:36][root][INFO] - Training Epoch: 9/10, step 283/574 completed (loss: 0.01542374212294817, acc: 1.0)
[2024-11-08 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:36][root][INFO] - Training Epoch: 9/10, step 284/574 completed (loss: 0.009944156743586063, acc: 1.0)
[2024-11-08 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:36][root][INFO] - Training Epoch: 9/10, step 285/574 completed (loss: 0.03566933050751686, acc: 1.0)
[2024-11-08 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:37][root][INFO] - Training Epoch: 9/10, step 286/574 completed (loss: 0.01747634820640087, acc: 1.0)
[2024-11-08 04:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:37][root][INFO] - Training Epoch: 9/10, step 287/574 completed (loss: 0.012593108229339123, acc: 1.0)
[2024-11-08 04:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:37][root][INFO] - Training Epoch: 9/10, step 288/574 completed (loss: 0.03508663550019264, acc: 1.0)
[2024-11-08 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:38][root][INFO] - Training Epoch: 9/10, step 289/574 completed (loss: 0.11624132841825485, acc: 0.9090909361839294)
[2024-11-08 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:38][root][INFO] - Training Epoch: 9/10, step 290/574 completed (loss: 0.0023810595739632845, acc: 1.0)
[2024-11-08 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:39][root][INFO] - Training Epoch: 9/10, step 291/574 completed (loss: 0.001972629688680172, acc: 1.0)
[2024-11-08 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:39][root][INFO] - Training Epoch: 9/10, step 292/574 completed (loss: 0.09779689460992813, acc: 0.9666666388511658)
[2024-11-08 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:39][root][INFO] - Training Epoch: 9/10, step 293/574 completed (loss: 0.032116033136844635, acc: 1.0)
[2024-11-08 04:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:40][root][INFO] - Training Epoch: 9/10, step 294/574 completed (loss: 0.0037889580707997084, acc: 1.0)
[2024-11-08 04:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:41][root][INFO] - Training Epoch: 9/10, step 295/574 completed (loss: 0.16237716376781464, acc: 0.9473684430122375)
[2024-11-08 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:41][root][INFO] - Training Epoch: 9/10, step 296/574 completed (loss: 0.0006679862854070961, acc: 1.0)
[2024-11-08 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:42][root][INFO] - Training Epoch: 9/10, step 297/574 completed (loss: 0.052112430334091187, acc: 0.9523809552192688)
[2024-11-08 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:42][root][INFO] - Training Epoch: 9/10, step 298/574 completed (loss: 0.026648394763469696, acc: 1.0)
[2024-11-08 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:42][root][INFO] - Training Epoch: 9/10, step 299/574 completed (loss: 0.0002009818417718634, acc: 1.0)
[2024-11-08 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:43][root][INFO] - Training Epoch: 9/10, step 300/574 completed (loss: 0.11568774282932281, acc: 0.9677419066429138)
[2024-11-08 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:43][root][INFO] - Training Epoch: 9/10, step 301/574 completed (loss: 0.003965127747505903, acc: 1.0)
[2024-11-08 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:43][root][INFO] - Training Epoch: 9/10, step 302/574 completed (loss: 0.020300649106502533, acc: 1.0)
[2024-11-08 04:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:44][root][INFO] - Training Epoch: 9/10, step 303/574 completed (loss: 0.00451866677030921, acc: 1.0)
[2024-11-08 04:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:44][root][INFO] - Training Epoch: 9/10, step 304/574 completed (loss: 0.03488495573401451, acc: 1.0)
[2024-11-08 04:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:45][root][INFO] - Training Epoch: 9/10, step 305/574 completed (loss: 0.07176750898361206, acc: 0.95652174949646)
[2024-11-08 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:45][root][INFO] - Training Epoch: 9/10, step 306/574 completed (loss: 0.0134985176846385, acc: 1.0)
[2024-11-08 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:45][root][INFO] - Training Epoch: 9/10, step 307/574 completed (loss: 0.006796491798013449, acc: 1.0)
[2024-11-08 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:46][root][INFO] - Training Epoch: 9/10, step 308/574 completed (loss: 0.008036058396100998, acc: 1.0)
[2024-11-08 04:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:46][root][INFO] - Training Epoch: 9/10, step 309/574 completed (loss: 0.006132932845503092, acc: 1.0)
[2024-11-08 04:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:47][root][INFO] - Training Epoch: 9/10, step 310/574 completed (loss: 0.11000754684209824, acc: 0.9473684430122375)
[2024-11-08 04:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:47][root][INFO] - Training Epoch: 9/10, step 311/574 completed (loss: 0.052247676998376846, acc: 1.0)
[2024-11-08 04:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:47][root][INFO] - Training Epoch: 9/10, step 312/574 completed (loss: 0.6437912583351135, acc: 0.8636363744735718)
[2024-11-08 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:48][root][INFO] - Training Epoch: 9/10, step 313/574 completed (loss: 0.05661496892571449, acc: 0.9642857313156128)
[2024-11-08 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:48][root][INFO] - Training Epoch: 9/10, step 314/574 completed (loss: 0.0036686051171272993, acc: 1.0)
[2024-11-08 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:49][root][INFO] - Training Epoch: 9/10, step 315/574 completed (loss: 0.005212163552641869, acc: 1.0)
[2024-11-08 04:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:49][root][INFO] - Training Epoch: 9/10, step 316/574 completed (loss: 0.02817869372665882, acc: 1.0)
[2024-11-08 04:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:49][root][INFO] - Training Epoch: 9/10, step 317/574 completed (loss: 0.0074953921139240265, acc: 1.0)
[2024-11-08 04:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:50][root][INFO] - Training Epoch: 9/10, step 318/574 completed (loss: 0.01932879537343979, acc: 1.0)
[2024-11-08 04:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:50][root][INFO] - Training Epoch: 9/10, step 319/574 completed (loss: 0.004090249538421631, acc: 1.0)
[2024-11-08 04:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:51][root][INFO] - Training Epoch: 9/10, step 320/574 completed (loss: 0.12376359850168228, acc: 0.9523809552192688)
[2024-11-08 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:51][root][INFO] - Training Epoch: 9/10, step 321/574 completed (loss: 0.01691681332886219, acc: 1.0)
[2024-11-08 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:51][root][INFO] - Training Epoch: 9/10, step 322/574 completed (loss: 0.14520162343978882, acc: 0.95652174949646)
[2024-11-08 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:52][root][INFO] - Training Epoch: 9/10, step 323/574 completed (loss: 0.10100717842578888, acc: 0.9523809552192688)
[2024-11-08 04:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:52][root][INFO] - Training Epoch: 9/10, step 324/574 completed (loss: 0.1652960628271103, acc: 0.9473684430122375)
[2024-11-08 04:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:53][root][INFO] - Training Epoch: 9/10, step 325/574 completed (loss: 0.10604484379291534, acc: 0.9545454382896423)
[2024-11-08 04:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:53][root][INFO] - Training Epoch: 9/10, step 326/574 completed (loss: 0.13832400739192963, acc: 0.9473684430122375)
[2024-11-08 04:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:53][root][INFO] - Training Epoch: 9/10, step 327/574 completed (loss: 0.06845296174287796, acc: 0.9583333134651184)
[2024-11-08 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:54][root][INFO] - Training Epoch: 9/10, step 328/574 completed (loss: 0.18078145384788513, acc: 0.931034505367279)
[2024-11-08 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:54][root][INFO] - Training Epoch: 9/10, step 329/574 completed (loss: 0.10226131975650787, acc: 0.9629629850387573)
[2024-11-08 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:55][root][INFO] - Training Epoch: 9/10, step 330/574 completed (loss: 0.002381173660978675, acc: 1.0)
[2024-11-08 04:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:55][root][INFO] - Training Epoch: 9/10, step 331/574 completed (loss: 0.1481199413537979, acc: 0.9473684430122375)
[2024-11-08 04:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:56][root][INFO] - Training Epoch: 9/10, step 332/574 completed (loss: 0.03180733695626259, acc: 1.0)
[2024-11-08 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:56][root][INFO] - Training Epoch: 9/10, step 333/574 completed (loss: 0.09052599221467972, acc: 0.9473684430122375)
[2024-11-08 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:56][root][INFO] - Training Epoch: 9/10, step 334/574 completed (loss: 0.004764378070831299, acc: 1.0)
[2024-11-08 04:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:57][root][INFO] - Training Epoch: 9/10, step 335/574 completed (loss: 0.39801546931266785, acc: 0.8846153616905212)
[2024-11-08 04:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:57][root][INFO] - Training Epoch: 9/10, step 336/574 completed (loss: 0.16537289321422577, acc: 0.9583333134651184)
[2024-11-08 04:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:58][root][INFO] - Training Epoch: 9/10, step 337/574 completed (loss: 0.006810683757066727, acc: 1.0)
[2024-11-08 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:58][root][INFO] - Training Epoch: 9/10, step 338/574 completed (loss: 0.1753312647342682, acc: 0.95652174949646)
[2024-11-08 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:58][root][INFO] - Training Epoch: 9/10, step 339/574 completed (loss: 0.1260794997215271, acc: 0.9545454382896423)
[2024-11-08 04:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:59][root][INFO] - Training Epoch: 9/10, step 340/574 completed (loss: 0.011541632935404778, acc: 1.0)
[2024-11-08 04:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:31:59][root][INFO] - Training Epoch: 9/10, step 341/574 completed (loss: 0.018747663125395775, acc: 1.0)
[2024-11-08 04:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:00][root][INFO] - Training Epoch: 9/10, step 342/574 completed (loss: 0.03736632317304611, acc: 1.0)
[2024-11-08 04:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:00][root][INFO] - Training Epoch: 9/10, step 343/574 completed (loss: 0.17544589936733246, acc: 0.949999988079071)
[2024-11-08 04:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:01][root][INFO] - Training Epoch: 9/10, step 344/574 completed (loss: 0.005650192033499479, acc: 1.0)
[2024-11-08 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:01][root][INFO] - Training Epoch: 9/10, step 345/574 completed (loss: 0.10272655636072159, acc: 0.9545454382896423)
[2024-11-08 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:01][root][INFO] - Training Epoch: 9/10, step 346/574 completed (loss: 0.36648720502853394, acc: 0.8999999761581421)
[2024-11-08 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:02][root][INFO] - Training Epoch: 9/10, step 347/574 completed (loss: 0.025624191388487816, acc: 1.0)
[2024-11-08 04:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:02][root][INFO] - Training Epoch: 9/10, step 348/574 completed (loss: 0.03151600435376167, acc: 1.0)
[2024-11-08 04:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:03][root][INFO] - Training Epoch: 9/10, step 349/574 completed (loss: 0.03343218192458153, acc: 1.0)
[2024-11-08 04:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:03][root][INFO] - Training Epoch: 9/10, step 350/574 completed (loss: 0.0416179783642292, acc: 1.0)
[2024-11-08 04:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:03][root][INFO] - Training Epoch: 9/10, step 351/574 completed (loss: 0.04381191357970238, acc: 0.9545454382896423)
[2024-11-08 04:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:04][root][INFO] - Training Epoch: 9/10, step 352/574 completed (loss: 0.07860206812620163, acc: 0.949999988079071)
[2024-11-08 04:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:04][root][INFO] - Training Epoch: 9/10, step 353/574 completed (loss: 0.00833306647837162, acc: 1.0)
[2024-11-08 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:05][root][INFO] - Training Epoch: 9/10, step 354/574 completed (loss: 0.040885161608457565, acc: 0.9599999785423279)
[2024-11-08 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:05][root][INFO] - Training Epoch: 9/10, step 355/574 completed (loss: 0.10999351739883423, acc: 0.9523809552192688)
[2024-11-08 04:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:06][root][INFO] - Training Epoch: 9/10, step 356/574 completed (loss: 0.0583876371383667, acc: 0.949999988079071)
[2024-11-08 04:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:06][root][INFO] - Training Epoch: 9/10, step 357/574 completed (loss: 0.004780132323503494, acc: 1.0)
[2024-11-08 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:07][root][INFO] - Training Epoch: 9/10, step 358/574 completed (loss: 0.03960668295621872, acc: 1.0)
[2024-11-08 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:07][root][INFO] - Training Epoch: 9/10, step 359/574 completed (loss: 0.003446981543675065, acc: 1.0)
[2024-11-08 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:08][root][INFO] - Training Epoch: 9/10, step 360/574 completed (loss: 0.049839723855257034, acc: 0.9655172228813171)
[2024-11-08 04:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:08][root][INFO] - Training Epoch: 9/10, step 361/574 completed (loss: 0.1360316425561905, acc: 0.9714285731315613)
[2024-11-08 04:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:08][root][INFO] - Training Epoch: 9/10, step 362/574 completed (loss: 0.059209611266851425, acc: 1.0)
[2024-11-08 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:09][root][INFO] - Training Epoch: 9/10, step 363/574 completed (loss: 0.007992955856025219, acc: 1.0)
[2024-11-08 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:09][root][INFO] - Training Epoch: 9/10, step 364/574 completed (loss: 0.10449091345071793, acc: 0.9523809552192688)
[2024-11-08 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:10][root][INFO] - Training Epoch: 9/10, step 365/574 completed (loss: 0.0018553759437054396, acc: 1.0)
[2024-11-08 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:10][root][INFO] - Training Epoch: 9/10, step 366/574 completed (loss: 0.00582302687689662, acc: 1.0)
[2024-11-08 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:10][root][INFO] - Training Epoch: 9/10, step 367/574 completed (loss: 0.014213914051651955, acc: 1.0)
[2024-11-08 04:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:11][root][INFO] - Training Epoch: 9/10, step 368/574 completed (loss: 0.22875888645648956, acc: 0.90625)
[2024-11-08 04:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:11][root][INFO] - Training Epoch: 9/10, step 369/574 completed (loss: 0.021148238331079483, acc: 1.0)
[2024-11-08 04:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:12][root][INFO] - Training Epoch: 9/10, step 370/574 completed (loss: 0.08875326812267303, acc: 0.9523809552192688)
[2024-11-08 04:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:13][root][INFO] - Training Epoch: 9/10, step 371/574 completed (loss: 0.2114076167345047, acc: 0.9473684430122375)
[2024-11-08 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:14][root][INFO] - Training Epoch: 9/10, step 372/574 completed (loss: 0.0367705374956131, acc: 0.9545454382896423)
[2024-11-08 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:14][root][INFO] - Training Epoch: 9/10, step 373/574 completed (loss: 0.018689794465899467, acc: 1.0)
[2024-11-08 04:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:15][root][INFO] - Training Epoch: 9/10, step 374/574 completed (loss: 0.009720701724290848, acc: 1.0)
[2024-11-08 04:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:15][root][INFO] - Training Epoch: 9/10, step 375/574 completed (loss: 0.00582089414820075, acc: 1.0)
[2024-11-08 04:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:16][root][INFO] - Training Epoch: 9/10, step 376/574 completed (loss: 0.006243898533284664, acc: 1.0)
[2024-11-08 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:16][root][INFO] - Training Epoch: 9/10, step 377/574 completed (loss: 0.005264829844236374, acc: 1.0)
[2024-11-08 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:17][root][INFO] - Training Epoch: 9/10, step 378/574 completed (loss: 0.006859833840280771, acc: 1.0)
[2024-11-08 04:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:17][root][INFO] - Training Epoch: 9/10, step 379/574 completed (loss: 0.15925508737564087, acc: 0.9473684430122375)
[2024-11-08 04:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:18][root][INFO] - Training Epoch: 9/10, step 380/574 completed (loss: 0.02583727054297924, acc: 1.0)
[2024-11-08 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:20][root][INFO] - Training Epoch: 9/10, step 381/574 completed (loss: 0.03413320705294609, acc: 1.0)
[2024-11-08 04:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:20][root][INFO] - Training Epoch: 9/10, step 382/574 completed (loss: 0.009161378256976604, acc: 1.0)
[2024-11-08 04:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:21][root][INFO] - Training Epoch: 9/10, step 383/574 completed (loss: 0.007437081076204777, acc: 1.0)
[2024-11-08 04:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:21][root][INFO] - Training Epoch: 9/10, step 384/574 completed (loss: 0.00043081524199806154, acc: 1.0)
[2024-11-08 04:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:22][root][INFO] - Training Epoch: 9/10, step 385/574 completed (loss: 0.0028760579880326986, acc: 1.0)
[2024-11-08 04:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:22][root][INFO] - Training Epoch: 9/10, step 386/574 completed (loss: 0.000690347864292562, acc: 1.0)
[2024-11-08 04:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:22][root][INFO] - Training Epoch: 9/10, step 387/574 completed (loss: 0.00022918051399756223, acc: 1.0)
[2024-11-08 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:23][root][INFO] - Training Epoch: 9/10, step 388/574 completed (loss: 0.00046431971713900566, acc: 1.0)
[2024-11-08 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:23][root][INFO] - Training Epoch: 9/10, step 389/574 completed (loss: 0.0011247318470850587, acc: 1.0)
[2024-11-08 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:24][root][INFO] - Training Epoch: 9/10, step 390/574 completed (loss: 0.02565595507621765, acc: 1.0)
[2024-11-08 04:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:24][root][INFO] - Training Epoch: 9/10, step 391/574 completed (loss: 0.04875540733337402, acc: 1.0)
[2024-11-08 04:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:25][root][INFO] - Training Epoch: 9/10, step 392/574 completed (loss: 0.09908957779407501, acc: 0.9473684430122375)
[2024-11-08 04:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:25][root][INFO] - Training Epoch: 9/10, step 393/574 completed (loss: 0.12366466969251633, acc: 0.9473684430122375)
[2024-11-08 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:26][root][INFO] - Training Epoch: 9/10, step 394/574 completed (loss: 0.39591190218925476, acc: 0.9090909361839294)
[2024-11-08 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:26][root][INFO] - Training Epoch: 9/10, step 395/574 completed (loss: 0.014708979055285454, acc: 1.0)
[2024-11-08 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:27][root][INFO] - Training Epoch: 9/10, step 396/574 completed (loss: 0.015751156955957413, acc: 1.0)
[2024-11-08 04:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:27][root][INFO] - Training Epoch: 9/10, step 397/574 completed (loss: 0.13162869215011597, acc: 0.9333333373069763)
[2024-11-08 04:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:28][root][INFO] - Training Epoch: 9/10, step 398/574 completed (loss: 0.024936238303780556, acc: 1.0)
[2024-11-08 04:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:28][root][INFO] - Training Epoch: 9/10, step 399/574 completed (loss: 0.07463113963603973, acc: 0.9444444179534912)
[2024-11-08 04:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:29][root][INFO] - Training Epoch: 9/10, step 400/574 completed (loss: 0.018462209030985832, acc: 1.0)
[2024-11-08 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:29][root][INFO] - Training Epoch: 9/10, step 401/574 completed (loss: 0.09462981671094894, acc: 0.9523809552192688)
[2024-11-08 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:29][root][INFO] - Training Epoch: 9/10, step 402/574 completed (loss: 0.012758574448525906, acc: 1.0)
[2024-11-08 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:30][root][INFO] - Training Epoch: 9/10, step 403/574 completed (loss: 0.005922939162701368, acc: 1.0)
[2024-11-08 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:30][root][INFO] - Training Epoch: 9/10, step 404/574 completed (loss: 0.029303818941116333, acc: 1.0)
[2024-11-08 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:31][root][INFO] - Training Epoch: 9/10, step 405/574 completed (loss: 0.034212760627269745, acc: 1.0)
[2024-11-08 04:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:31][root][INFO] - Training Epoch: 9/10, step 406/574 completed (loss: 0.5431196689605713, acc: 0.9523809552192688)
[2024-11-08 04:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:32][root][INFO] - Training Epoch: 9/10, step 407/574 completed (loss: 0.06962478905916214, acc: 0.9473684430122375)
[2024-11-08 04:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:32][root][INFO] - Training Epoch: 9/10, step 408/574 completed (loss: 0.37029922008514404, acc: 0.949999988079071)
[2024-11-08 04:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:32][root][INFO] - Training Epoch: 9/10, step 409/574 completed (loss: 0.011576991528272629, acc: 1.0)
[2024-11-08 04:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:33][root][INFO] - Training Epoch: 9/10, step 410/574 completed (loss: 0.013823763467371464, acc: 1.0)
[2024-11-08 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:33][root][INFO] - Training Epoch: 9/10, step 411/574 completed (loss: 0.397900253534317, acc: 0.8571428656578064)
[2024-11-08 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:34][root][INFO] - Training Epoch: 9/10, step 412/574 completed (loss: 0.3237779438495636, acc: 0.8888888955116272)
[2024-11-08 04:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:17][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3652, device='cuda:0') eval_epoch_loss=tensor(0.3113, device='cuda:0') eval_epoch_acc=tensor(0.9204, device='cuda:0')
[2024-11-08 04:33:17][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:33:17][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:33:21][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_9_step_413_loss_0.3113119900226593/model.pt
[2024-11-08 04:33:21][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:21][root][INFO] - Training Epoch: 9/10, step 413/574 completed (loss: 0.08289656043052673, acc: 0.9714285731315613)
[2024-11-08 04:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:22][root][INFO] - Training Epoch: 9/10, step 414/574 completed (loss: 0.013210262171924114, acc: 1.0)
[2024-11-08 04:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:22][root][INFO] - Training Epoch: 9/10, step 415/574 completed (loss: 0.04105363413691521, acc: 1.0)
[2024-11-08 04:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:23][root][INFO] - Training Epoch: 9/10, step 416/574 completed (loss: 0.014331497251987457, acc: 1.0)
[2024-11-08 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:23][root][INFO] - Training Epoch: 9/10, step 417/574 completed (loss: 0.19559772312641144, acc: 0.9473684430122375)
[2024-11-08 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:24][root][INFO] - Training Epoch: 9/10, step 418/574 completed (loss: 0.024577712640166283, acc: 1.0)
[2024-11-08 04:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:24][root][INFO] - Training Epoch: 9/10, step 419/574 completed (loss: 0.06277389824390411, acc: 1.0)
[2024-11-08 04:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:24][root][INFO] - Training Epoch: 9/10, step 420/574 completed (loss: 0.08338136225938797, acc: 1.0)
[2024-11-08 04:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:25][root][INFO] - Training Epoch: 9/10, step 421/574 completed (loss: 0.3192531168460846, acc: 0.8387096524238586)
[2024-11-08 04:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:25][root][INFO] - Training Epoch: 9/10, step 422/574 completed (loss: 0.7040198445320129, acc: 0.8709677457809448)
[2024-11-08 04:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:26][root][INFO] - Training Epoch: 9/10, step 423/574 completed (loss: 0.13712824881076813, acc: 0.9615384340286255)
[2024-11-08 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:26][root][INFO] - Training Epoch: 9/10, step 424/574 completed (loss: 0.12238927185535431, acc: 0.9523809552192688)
[2024-11-08 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:26][root][INFO] - Training Epoch: 9/10, step 425/574 completed (loss: 0.03749978542327881, acc: 1.0)
[2024-11-08 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:27][root][INFO] - Training Epoch: 9/10, step 426/574 completed (loss: 0.030304647982120514, acc: 1.0)
[2024-11-08 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:27][root][INFO] - Training Epoch: 9/10, step 427/574 completed (loss: 0.10643550753593445, acc: 0.9545454382896423)
[2024-11-08 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:28][root][INFO] - Training Epoch: 9/10, step 428/574 completed (loss: 0.14489908516407013, acc: 0.9047619104385376)
[2024-11-08 04:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:28][root][INFO] - Training Epoch: 9/10, step 429/574 completed (loss: 0.002761384705081582, acc: 1.0)
[2024-11-08 04:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:29][root][INFO] - Training Epoch: 9/10, step 430/574 completed (loss: 0.6343881487846375, acc: 0.9032257795333862)
[2024-11-08 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:29][root][INFO] - Training Epoch: 9/10, step 431/574 completed (loss: 0.11498323827981949, acc: 0.9677419066429138)
[2024-11-08 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:29][root][INFO] - Training Epoch: 9/10, step 432/574 completed (loss: 0.09723515063524246, acc: 0.9615384340286255)
[2024-11-08 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:30][root][INFO] - Training Epoch: 9/10, step 433/574 completed (loss: 0.027316808700561523, acc: 1.0)
[2024-11-08 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:30][root][INFO] - Training Epoch: 9/10, step 434/574 completed (loss: 0.006922171451151371, acc: 1.0)
[2024-11-08 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:30][root][INFO] - Training Epoch: 9/10, step 435/574 completed (loss: 0.27482661604881287, acc: 0.9473684430122375)
[2024-11-08 04:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:31][root][INFO] - Training Epoch: 9/10, step 436/574 completed (loss: 0.01192128099501133, acc: 1.0)
[2024-11-08 04:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:31][root][INFO] - Training Epoch: 9/10, step 437/574 completed (loss: 0.11769048124551773, acc: 0.9523809552192688)
[2024-11-08 04:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:32][root][INFO] - Training Epoch: 9/10, step 438/574 completed (loss: 0.004649574868381023, acc: 1.0)
[2024-11-08 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:32][root][INFO] - Training Epoch: 9/10, step 439/574 completed (loss: 0.04852232336997986, acc: 1.0)
[2024-11-08 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:33][root][INFO] - Training Epoch: 9/10, step 440/574 completed (loss: 0.03297331929206848, acc: 1.0)
[2024-11-08 04:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:34][root][INFO] - Training Epoch: 9/10, step 441/574 completed (loss: 0.08062909543514252, acc: 0.9473684430122375)
[2024-11-08 04:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:34][root][INFO] - Training Epoch: 9/10, step 442/574 completed (loss: 0.2083820104598999, acc: 0.9473684430122375)
[2024-11-08 04:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:35][root][INFO] - Training Epoch: 9/10, step 443/574 completed (loss: 0.08643374592065811, acc: 0.9090909361839294)
[2024-11-08 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:36][root][INFO] - Training Epoch: 9/10, step 444/574 completed (loss: 0.011189363896846771, acc: 1.0)
[2024-11-08 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:36][root][INFO] - Training Epoch: 9/10, step 445/574 completed (loss: 0.014465787447988987, acc: 1.0)
[2024-11-08 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:37][root][INFO] - Training Epoch: 9/10, step 446/574 completed (loss: 0.14554361999034882, acc: 0.9354838728904724)
[2024-11-08 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:37][root][INFO] - Training Epoch: 9/10, step 447/574 completed (loss: 0.33921724557876587, acc: 0.9032257795333862)
[2024-11-08 04:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:38][root][INFO] - Training Epoch: 9/10, step 448/574 completed (loss: 0.18930357694625854, acc: 0.9354838728904724)
[2024-11-08 04:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:38][root][INFO] - Training Epoch: 9/10, step 449/574 completed (loss: 0.004185794852674007, acc: 1.0)
[2024-11-08 04:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:39][root][INFO] - Training Epoch: 9/10, step 450/574 completed (loss: 0.05440182611346245, acc: 0.949999988079071)
[2024-11-08 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:39][root][INFO] - Training Epoch: 9/10, step 451/574 completed (loss: 0.003114453749731183, acc: 1.0)
[2024-11-08 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:39][root][INFO] - Training Epoch: 9/10, step 452/574 completed (loss: 0.005254472605884075, acc: 1.0)
[2024-11-08 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:40][root][INFO] - Training Epoch: 9/10, step 453/574 completed (loss: 0.06683462858200073, acc: 0.949999988079071)
[2024-11-08 04:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:40][root][INFO] - Training Epoch: 9/10, step 454/574 completed (loss: 0.7150722146034241, acc: 0.9545454382896423)
[2024-11-08 04:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:40][root][INFO] - Training Epoch: 9/10, step 455/574 completed (loss: 0.0689108818769455, acc: 1.0)
[2024-11-08 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:41][root][INFO] - Training Epoch: 9/10, step 456/574 completed (loss: 0.011185531504452229, acc: 1.0)
[2024-11-08 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:41][root][INFO] - Training Epoch: 9/10, step 457/574 completed (loss: 0.11812832206487656, acc: 0.8999999761581421)
[2024-11-08 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:42][root][INFO] - Training Epoch: 9/10, step 458/574 completed (loss: 0.04132095351815224, acc: 1.0)
[2024-11-08 04:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:42][root][INFO] - Training Epoch: 9/10, step 459/574 completed (loss: 0.010569745674729347, acc: 1.0)
[2024-11-08 04:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:42][root][INFO] - Training Epoch: 9/10, step 460/574 completed (loss: 0.03894152492284775, acc: 1.0)
[2024-11-08 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:43][root][INFO] - Training Epoch: 9/10, step 461/574 completed (loss: 0.3178088963031769, acc: 0.9545454382896423)
[2024-11-08 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:43][root][INFO] - Training Epoch: 9/10, step 462/574 completed (loss: 0.02415182814002037, acc: 1.0)
[2024-11-08 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:44][root][INFO] - Training Epoch: 9/10, step 463/574 completed (loss: 0.13779768347740173, acc: 0.9629629850387573)
[2024-11-08 04:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:44][root][INFO] - Training Epoch: 9/10, step 464/574 completed (loss: 0.20885223150253296, acc: 0.9696969985961914)
[2024-11-08 04:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:44][root][INFO] - Training Epoch: 9/10, step 465/574 completed (loss: 0.0036529998760670424, acc: 1.0)
[2024-11-08 04:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:45][root][INFO] - Training Epoch: 9/10, step 466/574 completed (loss: 0.03158038109540939, acc: 1.0)
[2024-11-08 04:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:45][root][INFO] - Training Epoch: 9/10, step 467/574 completed (loss: 0.013356759212911129, acc: 1.0)
[2024-11-08 04:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:45][root][INFO] - Training Epoch: 9/10, step 468/574 completed (loss: 0.0033757840283215046, acc: 1.0)
[2024-11-08 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:46][root][INFO] - Training Epoch: 9/10, step 469/574 completed (loss: 0.012473453767597675, acc: 1.0)
[2024-11-08 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:46][root][INFO] - Training Epoch: 9/10, step 470/574 completed (loss: 0.048423755913972855, acc: 1.0)
[2024-11-08 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:47][root][INFO] - Training Epoch: 9/10, step 471/574 completed (loss: 0.008308938704431057, acc: 1.0)
[2024-11-08 04:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:47][root][INFO] - Training Epoch: 9/10, step 472/574 completed (loss: 0.053138162940740585, acc: 0.9583333134651184)
[2024-11-08 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:48][root][INFO] - Training Epoch: 9/10, step 473/574 completed (loss: 0.028149783611297607, acc: 1.0)
[2024-11-08 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:48][root][INFO] - Training Epoch: 9/10, step 474/574 completed (loss: 0.06671638041734695, acc: 0.9473684430122375)
[2024-11-08 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:49][root][INFO] - Training Epoch: 9/10, step 475/574 completed (loss: 0.018657561391592026, acc: 1.0)
[2024-11-08 04:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:49][root][INFO] - Training Epoch: 9/10, step 476/574 completed (loss: 0.03259851410984993, acc: 1.0)
[2024-11-08 04:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:50][root][INFO] - Training Epoch: 9/10, step 477/574 completed (loss: 0.01657663658261299, acc: 1.0)
[2024-11-08 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:50][root][INFO] - Training Epoch: 9/10, step 478/574 completed (loss: 0.06640166789293289, acc: 0.9642857313156128)
[2024-11-08 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:50][root][INFO] - Training Epoch: 9/10, step 479/574 completed (loss: 0.007959638722240925, acc: 1.0)
[2024-11-08 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:51][root][INFO] - Training Epoch: 9/10, step 480/574 completed (loss: 0.54952073097229, acc: 0.9677419066429138)
[2024-11-08 04:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:51][root][INFO] - Training Epoch: 9/10, step 481/574 completed (loss: 0.01888039894402027, acc: 1.0)
[2024-11-08 04:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:51][root][INFO] - Training Epoch: 9/10, step 482/574 completed (loss: 0.3762103021144867, acc: 0.8947368264198303)
[2024-11-08 04:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:52][root][INFO] - Training Epoch: 9/10, step 483/574 completed (loss: 0.035043880343437195, acc: 1.0)
[2024-11-08 04:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:52][root][INFO] - Training Epoch: 9/10, step 484/574 completed (loss: 0.004466948099434376, acc: 1.0)
[2024-11-08 04:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:53][root][INFO] - Training Epoch: 9/10, step 485/574 completed (loss: 0.03337680175900459, acc: 1.0)
[2024-11-08 04:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:53][root][INFO] - Training Epoch: 9/10, step 486/574 completed (loss: 0.01871705800294876, acc: 1.0)
[2024-11-08 04:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:53][root][INFO] - Training Epoch: 9/10, step 487/574 completed (loss: 0.15420687198638916, acc: 0.9428571462631226)
[2024-11-08 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:54][root][INFO] - Training Epoch: 9/10, step 488/574 completed (loss: 0.061190590262413025, acc: 1.0)
[2024-11-08 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:54][root][INFO] - Training Epoch: 9/10, step 489/574 completed (loss: 0.018968746066093445, acc: 1.0)
[2024-11-08 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:55][root][INFO] - Training Epoch: 9/10, step 490/574 completed (loss: 0.04644970968365669, acc: 1.0)
[2024-11-08 04:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:55][root][INFO] - Training Epoch: 9/10, step 491/574 completed (loss: 0.004332795739173889, acc: 1.0)
[2024-11-08 04:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:55][root][INFO] - Training Epoch: 9/10, step 492/574 completed (loss: 0.007366691250354052, acc: 1.0)
[2024-11-08 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:56][root][INFO] - Training Epoch: 9/10, step 493/574 completed (loss: 0.03256155923008919, acc: 1.0)
[2024-11-08 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:56][root][INFO] - Training Epoch: 9/10, step 494/574 completed (loss: 0.15238086879253387, acc: 0.9583333134651184)
[2024-11-08 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:57][root][INFO] - Training Epoch: 9/10, step 495/574 completed (loss: 0.037680454552173615, acc: 1.0)
[2024-11-08 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:57][root][INFO] - Training Epoch: 9/10, step 496/574 completed (loss: 0.1329699009656906, acc: 0.949999988079071)
[2024-11-08 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:57][root][INFO] - Training Epoch: 9/10, step 497/574 completed (loss: 0.02132246270775795, acc: 1.0)
[2024-11-08 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:58][root][INFO] - Training Epoch: 9/10, step 498/574 completed (loss: 0.05910170078277588, acc: 1.0)
[2024-11-08 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:58][root][INFO] - Training Epoch: 9/10, step 499/574 completed (loss: 0.0654916912317276, acc: 0.9545454382896423)
[2024-11-08 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:59][root][INFO] - Training Epoch: 9/10, step 500/574 completed (loss: 0.04336532577872276, acc: 1.0)
[2024-11-08 04:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:59][root][INFO] - Training Epoch: 9/10, step 501/574 completed (loss: 0.00436449097469449, acc: 1.0)
[2024-11-08 04:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:33:59][root][INFO] - Training Epoch: 9/10, step 502/574 completed (loss: 0.07511475682258606, acc: 0.9677419066429138)
[2024-11-08 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:00][root][INFO] - Training Epoch: 9/10, step 503/574 completed (loss: 0.01162697747349739, acc: 1.0)
[2024-11-08 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:00][root][INFO] - Training Epoch: 9/10, step 504/574 completed (loss: 0.0028269400354474783, acc: 1.0)
[2024-11-08 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:01][root][INFO] - Training Epoch: 9/10, step 505/574 completed (loss: 0.03042786754667759, acc: 1.0)
[2024-11-08 04:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:01][root][INFO] - Training Epoch: 9/10, step 506/574 completed (loss: 0.28278666734695435, acc: 0.9473684430122375)
[2024-11-08 04:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:02][root][INFO] - Training Epoch: 9/10, step 507/574 completed (loss: 0.0023592496290802956, acc: 1.0)
[2024-11-08 04:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:03][root][INFO] - Training Epoch: 9/10, step 508/574 completed (loss: 0.25734707713127136, acc: 0.949999988079071)
[2024-11-08 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:03][root][INFO] - Training Epoch: 9/10, step 509/574 completed (loss: 0.02939855307340622, acc: 1.0)
[2024-11-08 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:04][root][INFO] - Training Epoch: 9/10, step 510/574 completed (loss: 0.0018835460068657994, acc: 1.0)
[2024-11-08 04:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:04][root][INFO] - Training Epoch: 9/10, step 511/574 completed (loss: 0.038922496140003204, acc: 0.9666666388511658)
[2024-11-08 04:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:08][root][INFO] - Training Epoch: 9/10, step 512/574 completed (loss: 0.08209451287984848, acc: 0.9642857313156128)
[2024-11-08 04:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:09][root][INFO] - Training Epoch: 9/10, step 513/574 completed (loss: 0.004295537248253822, acc: 1.0)
[2024-11-08 04:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:09][root][INFO] - Training Epoch: 9/10, step 514/574 completed (loss: 0.23363490402698517, acc: 0.9473684430122375)
[2024-11-08 04:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:10][root][INFO] - Training Epoch: 9/10, step 515/574 completed (loss: 0.0030985395424067974, acc: 1.0)
[2024-11-08 04:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:11][root][INFO] - Training Epoch: 9/10, step 516/574 completed (loss: 0.012381221167743206, acc: 1.0)
[2024-11-08 04:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:11][root][INFO] - Training Epoch: 9/10, step 517/574 completed (loss: 0.14808224141597748, acc: 0.9545454382896423)
[2024-11-08 04:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:11][root][INFO] - Training Epoch: 9/10, step 518/574 completed (loss: 0.04462062940001488, acc: 0.9642857313156128)
[2024-11-08 04:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:12][root][INFO] - Training Epoch: 9/10, step 519/574 completed (loss: 0.04065113887190819, acc: 1.0)
[2024-11-08 04:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:12][root][INFO] - Training Epoch: 9/10, step 520/574 completed (loss: 0.021760519593954086, acc: 1.0)
[2024-11-08 04:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:14][root][INFO] - Training Epoch: 9/10, step 521/574 completed (loss: 0.03623978793621063, acc: 1.0)
[2024-11-08 04:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:14][root][INFO] - Training Epoch: 9/10, step 522/574 completed (loss: 0.012361383065581322, acc: 1.0)
[2024-11-08 04:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:15][root][INFO] - Training Epoch: 9/10, step 523/574 completed (loss: 0.09130904078483582, acc: 0.9473684430122375)
[2024-11-08 04:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:15][root][INFO] - Training Epoch: 9/10, step 524/574 completed (loss: 0.04737532511353493, acc: 1.0)
[2024-11-08 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:16][root][INFO] - Training Epoch: 9/10, step 525/574 completed (loss: 0.008851406164467335, acc: 1.0)
[2024-11-08 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:16][root][INFO] - Training Epoch: 9/10, step 526/574 completed (loss: 0.08316431194543839, acc: 0.9545454382896423)
[2024-11-08 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:17][root][INFO] - Training Epoch: 9/10, step 527/574 completed (loss: 0.007413296960294247, acc: 1.0)
[2024-11-08 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:17][root][INFO] - Training Epoch: 9/10, step 528/574 completed (loss: 0.07499720901250839, acc: 0.9642857313156128)
[2024-11-08 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:17][root][INFO] - Training Epoch: 9/10, step 529/574 completed (loss: 0.0029362214263528585, acc: 1.0)
[2024-11-08 04:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:18][root][INFO] - Training Epoch: 9/10, step 530/574 completed (loss: 0.40536123514175415, acc: 0.8421052694320679)
[2024-11-08 04:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:18][root][INFO] - Training Epoch: 9/10, step 531/574 completed (loss: 0.17476731538772583, acc: 0.949999988079071)
[2024-11-08 04:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:19][root][INFO] - Training Epoch: 9/10, step 532/574 completed (loss: 0.17838247120380402, acc: 0.9523809552192688)
[2024-11-08 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:19][root][INFO] - Training Epoch: 9/10, step 533/574 completed (loss: 0.05623224750161171, acc: 0.9545454382896423)
[2024-11-08 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:20][root][INFO] - Training Epoch: 9/10, step 534/574 completed (loss: 0.04362032189965248, acc: 1.0)
[2024-11-08 04:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:20][root][INFO] - Training Epoch: 9/10, step 535/574 completed (loss: 0.002248636446893215, acc: 1.0)
[2024-11-08 04:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:20][root][INFO] - Training Epoch: 9/10, step 536/574 completed (loss: 0.006966354325413704, acc: 1.0)
[2024-11-08 04:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:21][root][INFO] - Training Epoch: 9/10, step 537/574 completed (loss: 0.008576120249927044, acc: 1.0)
[2024-11-08 04:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:21][root][INFO] - Training Epoch: 9/10, step 538/574 completed (loss: 0.10495689511299133, acc: 0.9473684430122375)
[2024-11-08 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:22][root][INFO] - Training Epoch: 9/10, step 539/574 completed (loss: 0.1905943900346756, acc: 0.8999999761581421)
[2024-11-08 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:22][root][INFO] - Training Epoch: 9/10, step 540/574 completed (loss: 0.18620088696479797, acc: 0.9523809552192688)
[2024-11-08 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:23][root][INFO] - Training Epoch: 9/10, step 541/574 completed (loss: 0.019159335643053055, acc: 1.0)
[2024-11-08 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:23][root][INFO] - Training Epoch: 9/10, step 542/574 completed (loss: 0.001796641736291349, acc: 1.0)
[2024-11-08 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:24][root][INFO] - Training Epoch: 9/10, step 543/574 completed (loss: 0.004313597455620766, acc: 1.0)
[2024-11-08 04:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:24][root][INFO] - Training Epoch: 9/10, step 544/574 completed (loss: 0.002643560990691185, acc: 1.0)
[2024-11-08 04:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:25][root][INFO] - Training Epoch: 9/10, step 545/574 completed (loss: 0.043919216841459274, acc: 0.949999988079071)
[2024-11-08 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:25][root][INFO] - Training Epoch: 9/10, step 546/574 completed (loss: 0.0009959404123947024, acc: 1.0)
[2024-11-08 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:26][root][INFO] - Training Epoch: 9/10, step 547/574 completed (loss: 0.06091094762086868, acc: 0.9523809552192688)
[2024-11-08 04:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:26][root][INFO] - Training Epoch: 9/10, step 548/574 completed (loss: 0.02879156358540058, acc: 1.0)
[2024-11-08 04:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:26][root][INFO] - Training Epoch: 9/10, step 549/574 completed (loss: 0.004149056971073151, acc: 1.0)
[2024-11-08 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:27][root][INFO] - Training Epoch: 9/10, step 550/574 completed (loss: 0.016322031617164612, acc: 1.0)
[2024-11-08 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:27][root][INFO] - Training Epoch: 9/10, step 551/574 completed (loss: 0.016634369269013405, acc: 1.0)
[2024-11-08 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:28][root][INFO] - Training Epoch: 9/10, step 552/574 completed (loss: 0.0013971400912851095, acc: 1.0)
[2024-11-08 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:28][root][INFO] - Training Epoch: 9/10, step 553/574 completed (loss: 0.02387787401676178, acc: 1.0)
[2024-11-08 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:29][root][INFO] - Training Epoch: 9/10, step 554/574 completed (loss: 0.0014216869603842497, acc: 1.0)
[2024-11-08 04:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:29][root][INFO] - Training Epoch: 9/10, step 555/574 completed (loss: 0.010577814653515816, acc: 1.0)
[2024-11-08 04:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:14][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3778, device='cuda:0') eval_epoch_loss=tensor(0.3205, device='cuda:0') eval_epoch_acc=tensor(0.9266, device='cuda:0')
[2024-11-08 04:35:14][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:35:14][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:35:17][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_9_step_556_loss_0.3205111026763916/model.pt
[2024-11-08 04:35:17][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:18][root][INFO] - Training Epoch: 9/10, step 556/574 completed (loss: 0.004576250445097685, acc: 1.0)
[2024-11-08 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:18][root][INFO] - Training Epoch: 9/10, step 557/574 completed (loss: 0.000995668931864202, acc: 1.0)
[2024-11-08 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:19][root][INFO] - Training Epoch: 9/10, step 558/574 completed (loss: 0.009208460338413715, acc: 1.0)
[2024-11-08 04:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:19][root][INFO] - Training Epoch: 9/10, step 559/574 completed (loss: 0.05114830285310745, acc: 0.9629629850387573)
[2024-11-08 04:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:19][root][INFO] - Training Epoch: 9/10, step 560/574 completed (loss: 0.011963467113673687, acc: 1.0)
[2024-11-08 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:20][root][INFO] - Training Epoch: 9/10, step 561/574 completed (loss: 0.03666176274418831, acc: 1.0)
[2024-11-08 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:20][root][INFO] - Training Epoch: 9/10, step 562/574 completed (loss: 0.0005183418979868293, acc: 1.0)
[2024-11-08 04:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:21][root][INFO] - Training Epoch: 9/10, step 563/574 completed (loss: 0.0012662762310355902, acc: 1.0)
[2024-11-08 04:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:21][root][INFO] - Training Epoch: 9/10, step 564/574 completed (loss: 0.011730789206922054, acc: 1.0)
[2024-11-08 04:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:22][root][INFO] - Training Epoch: 9/10, step 565/574 completed (loss: 0.006587568670511246, acc: 1.0)
[2024-11-08 04:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:22][root][INFO] - Training Epoch: 9/10, step 566/574 completed (loss: 0.010343716479837894, acc: 1.0)
[2024-11-08 04:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:23][root][INFO] - Training Epoch: 9/10, step 567/574 completed (loss: 0.00477849505841732, acc: 1.0)
[2024-11-08 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:23][root][INFO] - Training Epoch: 9/10, step 568/574 completed (loss: 0.004532319493591785, acc: 1.0)
[2024-11-08 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:23][root][INFO] - Training Epoch: 9/10, step 569/574 completed (loss: 0.004016627557575703, acc: 1.0)
[2024-11-08 04:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:24][root][INFO] - Training Epoch: 9/10, step 570/574 completed (loss: 0.0048911189660429955, acc: 1.0)
[2024-11-08 04:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:24][root][INFO] - Training Epoch: 9/10, step 571/574 completed (loss: 0.0017057216027751565, acc: 1.0)
[2024-11-08 04:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:25][root][INFO] - Training Epoch: 9/10, step 572/574 completed (loss: 0.003049842081964016, acc: 1.0)
[2024-11-08 04:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:25][root][INFO] - Training Epoch: 9/10, step 573/574 completed (loss: 0.1465463787317276, acc: 0.949999988079071)
[2024-11-08 04:35:26][slam_llm.utils.train_utils][INFO] - Epoch 9: train_perplexity=1.0649, train_epoch_loss=0.0629, epoch time 468.12055987119675s
[2024-11-08 04:35:26][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 04:35:26][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 14 GB
[2024-11-08 04:35:26][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 04:35:26][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 7
[2024-11-08 04:35:26][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 04:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:27][root][INFO] - Training Epoch: 10/10, step 0/574 completed (loss: 0.03259552642703056, acc: 0.9642857313156128)
[2024-11-08 04:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:27][root][INFO] - Training Epoch: 10/10, step 1/574 completed (loss: 0.005392559338361025, acc: 1.0)
[2024-11-08 04:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:28][root][INFO] - Training Epoch: 10/10, step 2/574 completed (loss: 0.02181096002459526, acc: 1.0)
[2024-11-08 04:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:28][root][INFO] - Training Epoch: 10/10, step 3/574 completed (loss: 0.005083328112959862, acc: 1.0)
[2024-11-08 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:29][root][INFO] - Training Epoch: 10/10, step 4/574 completed (loss: 0.10733160376548767, acc: 0.9523809552192688)
[2024-11-08 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:29][root][INFO] - Training Epoch: 10/10, step 5/574 completed (loss: 0.14907485246658325, acc: 0.8947368264198303)
[2024-11-08 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:30][root][INFO] - Training Epoch: 10/10, step 6/574 completed (loss: 0.001491443021222949, acc: 1.0)
[2024-11-08 04:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:30][root][INFO] - Training Epoch: 10/10, step 7/574 completed (loss: 0.017232442274689674, acc: 1.0)
[2024-11-08 04:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:30][root][INFO] - Training Epoch: 10/10, step 8/574 completed (loss: 0.0018464107997715473, acc: 1.0)
[2024-11-08 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:31][root][INFO] - Training Epoch: 10/10, step 9/574 completed (loss: 0.07045631110668182, acc: 0.9354838728904724)
[2024-11-08 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:31][root][INFO] - Training Epoch: 10/10, step 10/574 completed (loss: 0.0030401821713894606, acc: 1.0)
[2024-11-08 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:32][root][INFO] - Training Epoch: 10/10, step 11/574 completed (loss: 0.07964025437831879, acc: 0.9599999785423279)
[2024-11-08 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:32][root][INFO] - Training Epoch: 10/10, step 12/574 completed (loss: 0.013678026385605335, acc: 1.0)
[2024-11-08 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:33][root][INFO] - Training Epoch: 10/10, step 13/574 completed (loss: 0.0033189149107784033, acc: 1.0)
[2024-11-08 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:33][root][INFO] - Training Epoch: 10/10, step 14/574 completed (loss: 0.07851976156234741, acc: 0.95652174949646)
[2024-11-08 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:33][root][INFO] - Training Epoch: 10/10, step 15/574 completed (loss: 0.00965805258601904, acc: 1.0)
[2024-11-08 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:34][root][INFO] - Training Epoch: 10/10, step 16/574 completed (loss: 0.0008342904620803893, acc: 1.0)
[2024-11-08 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:34][root][INFO] - Training Epoch: 10/10, step 17/574 completed (loss: 0.007025298196822405, acc: 1.0)
[2024-11-08 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:35][root][INFO] - Training Epoch: 10/10, step 18/574 completed (loss: 0.00368204852566123, acc: 1.0)
[2024-11-08 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:35][root][INFO] - Training Epoch: 10/10, step 19/574 completed (loss: 0.01171169150620699, acc: 1.0)
[2024-11-08 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:35][root][INFO] - Training Epoch: 10/10, step 20/574 completed (loss: 0.03383728861808777, acc: 1.0)
[2024-11-08 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:36][root][INFO] - Training Epoch: 10/10, step 21/574 completed (loss: 0.11712049692869186, acc: 0.9545454382896423)
[2024-11-08 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:36][root][INFO] - Training Epoch: 10/10, step 22/574 completed (loss: 0.032164447009563446, acc: 1.0)
[2024-11-08 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:36][root][INFO] - Training Epoch: 10/10, step 23/574 completed (loss: 0.0005364626995287836, acc: 1.0)
[2024-11-08 04:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:37][root][INFO] - Training Epoch: 10/10, step 24/574 completed (loss: 0.0026229366194456816, acc: 1.0)
[2024-11-08 04:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:37][root][INFO] - Training Epoch: 10/10, step 25/574 completed (loss: 0.1906926929950714, acc: 0.9523809552192688)
[2024-11-08 04:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:38][root][INFO] - Training Epoch: 10/10, step 26/574 completed (loss: 0.18876264989376068, acc: 0.9523809552192688)
[2024-11-08 04:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:39][root][INFO] - Training Epoch: 10/10, step 27/574 completed (loss: 0.011721196584403515, acc: 1.0)
[2024-11-08 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:40][root][INFO] - Training Epoch: 10/10, step 28/574 completed (loss: 0.004599455278366804, acc: 1.0)
[2024-11-08 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:40][root][INFO] - Training Epoch: 10/10, step 29/574 completed (loss: 0.2235521823167801, acc: 0.949999988079071)
[2024-11-08 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:41][root][INFO] - Training Epoch: 10/10, step 30/574 completed (loss: 0.0007465272792614996, acc: 1.0)
[2024-11-08 04:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:41][root][INFO] - Training Epoch: 10/10, step 31/574 completed (loss: 0.0007578260265290737, acc: 1.0)
[2024-11-08 04:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:41][root][INFO] - Training Epoch: 10/10, step 32/574 completed (loss: 0.0011753595899790525, acc: 1.0)
[2024-11-08 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:42][root][INFO] - Training Epoch: 10/10, step 33/574 completed (loss: 0.0016020659822970629, acc: 1.0)
[2024-11-08 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:42][root][INFO] - Training Epoch: 10/10, step 34/574 completed (loss: 0.03876306861639023, acc: 0.9523809552192688)
[2024-11-08 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:43][root][INFO] - Training Epoch: 10/10, step 35/574 completed (loss: 0.0014548951294273138, acc: 1.0)
[2024-11-08 04:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:43][root][INFO] - Training Epoch: 10/10, step 36/574 completed (loss: 0.08847378194332123, acc: 0.949999988079071)
[2024-11-08 04:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:44][root][INFO] - Training Epoch: 10/10, step 37/574 completed (loss: 0.0378485843539238, acc: 1.0)
[2024-11-08 04:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:44][root][INFO] - Training Epoch: 10/10, step 38/574 completed (loss: 0.0005553847877308726, acc: 1.0)
[2024-11-08 04:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:44][root][INFO] - Training Epoch: 10/10, step 39/574 completed (loss: 0.02760387770831585, acc: 1.0)
[2024-11-08 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:45][root][INFO] - Training Epoch: 10/10, step 40/574 completed (loss: 0.1384744644165039, acc: 0.9714285731315613)
[2024-11-08 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:45][root][INFO] - Training Epoch: 10/10, step 41/574 completed (loss: 0.005410742945969105, acc: 1.0)
[2024-11-08 04:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:46][root][INFO] - Training Epoch: 10/10, step 42/574 completed (loss: 0.11708325147628784, acc: 0.949999988079071)
[2024-11-08 04:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:46][root][INFO] - Training Epoch: 10/10, step 43/574 completed (loss: 0.005439542233943939, acc: 1.0)
[2024-11-08 04:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:47][root][INFO] - Training Epoch: 10/10, step 44/574 completed (loss: 0.016456937417387962, acc: 1.0)
[2024-11-08 04:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:47][root][INFO] - Training Epoch: 10/10, step 45/574 completed (loss: 0.1721697598695755, acc: 0.9523809552192688)
[2024-11-08 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:48][root][INFO] - Training Epoch: 10/10, step 46/574 completed (loss: 0.059458598494529724, acc: 0.9583333134651184)
[2024-11-08 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:48][root][INFO] - Training Epoch: 10/10, step 47/574 completed (loss: 0.01451896969228983, acc: 1.0)
[2024-11-08 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:49][root][INFO] - Training Epoch: 10/10, step 48/574 completed (loss: 0.01072798203676939, acc: 1.0)
[2024-11-08 04:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:49][root][INFO] - Training Epoch: 10/10, step 49/574 completed (loss: 0.0033743921667337418, acc: 1.0)
[2024-11-08 04:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:50][root][INFO] - Training Epoch: 10/10, step 50/574 completed (loss: 0.002712516114115715, acc: 1.0)
[2024-11-08 04:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:50][root][INFO] - Training Epoch: 10/10, step 51/574 completed (loss: 0.34218910336494446, acc: 0.9473684430122375)
[2024-11-08 04:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:50][root][INFO] - Training Epoch: 10/10, step 52/574 completed (loss: 0.10388483107089996, acc: 0.949999988079071)
[2024-11-08 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:51][root][INFO] - Training Epoch: 10/10, step 53/574 completed (loss: 0.01950306072831154, acc: 1.0)
[2024-11-08 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:51][root][INFO] - Training Epoch: 10/10, step 54/574 completed (loss: 0.029587997123599052, acc: 1.0)
[2024-11-08 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:52][root][INFO] - Training Epoch: 10/10, step 55/574 completed (loss: 0.002959192031994462, acc: 1.0)
[2024-11-08 04:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:56][root][INFO] - Training Epoch: 10/10, step 56/574 completed (loss: 0.21801263093948364, acc: 0.8571428656578064)
[2024-11-08 04:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:58][root][INFO] - Training Epoch: 10/10, step 57/574 completed (loss: 0.018079813569784164, acc: 1.0)
[2024-11-08 04:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:58][root][INFO] - Training Epoch: 10/10, step 58/574 completed (loss: 0.0073110186494886875, acc: 1.0)
[2024-11-08 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:35:59][root][INFO] - Training Epoch: 10/10, step 59/574 completed (loss: 0.009982584044337273, acc: 1.0)
[2024-11-08 04:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:00][root][INFO] - Training Epoch: 10/10, step 60/574 completed (loss: 0.006704988423734903, acc: 1.0)
[2024-11-08 04:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:01][root][INFO] - Training Epoch: 10/10, step 61/574 completed (loss: 0.02841666154563427, acc: 1.0)
[2024-11-08 04:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:01][root][INFO] - Training Epoch: 10/10, step 62/574 completed (loss: 0.001063489355146885, acc: 1.0)
[2024-11-08 04:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:01][root][INFO] - Training Epoch: 10/10, step 63/574 completed (loss: 0.07920771837234497, acc: 0.9642857313156128)
[2024-11-08 04:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:02][root][INFO] - Training Epoch: 10/10, step 64/574 completed (loss: 0.016433997079730034, acc: 1.0)
[2024-11-08 04:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:02][root][INFO] - Training Epoch: 10/10, step 65/574 completed (loss: 0.034104906022548676, acc: 1.0)
[2024-11-08 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:03][root][INFO] - Training Epoch: 10/10, step 66/574 completed (loss: 0.04227633774280548, acc: 1.0)
[2024-11-08 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:03][root][INFO] - Training Epoch: 10/10, step 67/574 completed (loss: 0.04582549259066582, acc: 1.0)
[2024-11-08 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:04][root][INFO] - Training Epoch: 10/10, step 68/574 completed (loss: 0.02581808716058731, acc: 1.0)
[2024-11-08 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:04][root][INFO] - Training Epoch: 10/10, step 69/574 completed (loss: 0.0019382017198950052, acc: 1.0)
[2024-11-08 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:04][root][INFO] - Training Epoch: 10/10, step 70/574 completed (loss: 0.0022628691513091326, acc: 1.0)
[2024-11-08 04:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:05][root][INFO] - Training Epoch: 10/10, step 71/574 completed (loss: 0.04653935506939888, acc: 1.0)
[2024-11-08 04:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:05][root][INFO] - Training Epoch: 10/10, step 72/574 completed (loss: 0.0006218043272383511, acc: 1.0)
[2024-11-08 04:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:06][root][INFO] - Training Epoch: 10/10, step 73/574 completed (loss: 0.1027415543794632, acc: 0.9473684430122375)
[2024-11-08 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:06][root][INFO] - Training Epoch: 10/10, step 74/574 completed (loss: 0.19940701127052307, acc: 0.949999988079071)
[2024-11-08 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:07][root][INFO] - Training Epoch: 10/10, step 75/574 completed (loss: 0.00597985927015543, acc: 1.0)
[2024-11-08 04:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:07][root][INFO] - Training Epoch: 10/10, step 76/574 completed (loss: 0.02465157024562359, acc: 1.0)
[2024-11-08 04:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:07][root][INFO] - Training Epoch: 10/10, step 77/574 completed (loss: 0.0010895811719819903, acc: 1.0)
[2024-11-08 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:08][root][INFO] - Training Epoch: 10/10, step 78/574 completed (loss: 0.0009809317998588085, acc: 1.0)
[2024-11-08 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:08][root][INFO] - Training Epoch: 10/10, step 79/574 completed (loss: 0.06741569191217422, acc: 0.9714285731315613)
[2024-11-08 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:09][root][INFO] - Training Epoch: 10/10, step 80/574 completed (loss: 0.001165744150057435, acc: 1.0)
[2024-11-08 04:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:09][root][INFO] - Training Epoch: 10/10, step 81/574 completed (loss: 0.000917227822355926, acc: 1.0)
[2024-11-08 04:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:10][root][INFO] - Training Epoch: 10/10, step 82/574 completed (loss: 0.16796839237213135, acc: 0.9473684430122375)
[2024-11-08 04:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:10][root][INFO] - Training Epoch: 10/10, step 83/574 completed (loss: 0.0009630762506276369, acc: 1.0)
[2024-11-08 04:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:10][root][INFO] - Training Epoch: 10/10, step 84/574 completed (loss: 0.02092347852885723, acc: 1.0)
[2024-11-08 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:11][root][INFO] - Training Epoch: 10/10, step 85/574 completed (loss: 0.0005885980790480971, acc: 1.0)
[2024-11-08 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:11][root][INFO] - Training Epoch: 10/10, step 86/574 completed (loss: 0.0008352853474207222, acc: 1.0)
[2024-11-08 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:12][root][INFO] - Training Epoch: 10/10, step 87/574 completed (loss: 0.015761952847242355, acc: 1.0)
[2024-11-08 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:12][root][INFO] - Training Epoch: 10/10, step 88/574 completed (loss: 0.02158915065228939, acc: 1.0)
[2024-11-08 04:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:14][root][INFO] - Training Epoch: 10/10, step 89/574 completed (loss: 0.19387337565422058, acc: 0.8947368264198303)
[2024-11-08 04:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:15][root][INFO] - Training Epoch: 10/10, step 90/574 completed (loss: 0.004031885415315628, acc: 1.0)
[2024-11-08 04:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:16][root][INFO] - Training Epoch: 10/10, step 91/574 completed (loss: 0.1075311154127121, acc: 0.9545454382896423)
[2024-11-08 04:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:17][root][INFO] - Training Epoch: 10/10, step 92/574 completed (loss: 0.04757952690124512, acc: 1.0)
[2024-11-08 04:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:19][root][INFO] - Training Epoch: 10/10, step 93/574 completed (loss: 0.3301706314086914, acc: 0.8846153616905212)
[2024-11-08 04:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:19][root][INFO] - Training Epoch: 10/10, step 94/574 completed (loss: 0.02693558670580387, acc: 1.0)
[2024-11-08 04:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:20][root][INFO] - Training Epoch: 10/10, step 95/574 completed (loss: 0.02328566089272499, acc: 1.0)
[2024-11-08 04:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:20][root][INFO] - Training Epoch: 10/10, step 96/574 completed (loss: 0.016446758061647415, acc: 1.0)
[2024-11-08 04:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:21][root][INFO] - Training Epoch: 10/10, step 97/574 completed (loss: 0.05319083482027054, acc: 1.0)
[2024-11-08 04:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:21][root][INFO] - Training Epoch: 10/10, step 98/574 completed (loss: 0.010787463746964931, acc: 1.0)
[2024-11-08 04:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:22][root][INFO] - Training Epoch: 10/10, step 99/574 completed (loss: 0.2049265056848526, acc: 0.9090909361839294)
[2024-11-08 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:22][root][INFO] - Training Epoch: 10/10, step 100/574 completed (loss: 0.02199367620050907, acc: 1.0)
[2024-11-08 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:22][root][INFO] - Training Epoch: 10/10, step 101/574 completed (loss: 0.008283314295113087, acc: 1.0)
[2024-11-08 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:23][root][INFO] - Training Epoch: 10/10, step 102/574 completed (loss: 0.01602613367140293, acc: 1.0)
[2024-11-08 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:23][root][INFO] - Training Epoch: 10/10, step 103/574 completed (loss: 0.0009765119175426662, acc: 1.0)
[2024-11-08 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:24][root][INFO] - Training Epoch: 10/10, step 104/574 completed (loss: 0.003613819368183613, acc: 1.0)
[2024-11-08 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:24][root][INFO] - Training Epoch: 10/10, step 105/574 completed (loss: 0.1272982656955719, acc: 0.9473684430122375)
[2024-11-08 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:24][root][INFO] - Training Epoch: 10/10, step 106/574 completed (loss: 0.07512062788009644, acc: 0.9545454382896423)
[2024-11-08 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:25][root][INFO] - Training Epoch: 10/10, step 107/574 completed (loss: 0.0011537909740582108, acc: 1.0)
[2024-11-08 04:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:25][root][INFO] - Training Epoch: 10/10, step 108/574 completed (loss: 0.005319391842931509, acc: 1.0)
[2024-11-08 04:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:26][root][INFO] - Training Epoch: 10/10, step 109/574 completed (loss: 0.01129885483533144, acc: 1.0)
[2024-11-08 04:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:26][root][INFO] - Training Epoch: 10/10, step 110/574 completed (loss: 0.0009118487359955907, acc: 1.0)
[2024-11-08 04:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:27][root][INFO] - Training Epoch: 10/10, step 111/574 completed (loss: 0.005543687846511602, acc: 1.0)
[2024-11-08 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:27][root][INFO] - Training Epoch: 10/10, step 112/574 completed (loss: 0.009044983424246311, acc: 1.0)
[2024-11-08 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:28][root][INFO] - Training Epoch: 10/10, step 113/574 completed (loss: 0.008138672448694706, acc: 1.0)
[2024-11-08 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:28][root][INFO] - Training Epoch: 10/10, step 114/574 completed (loss: 0.0016432828269898891, acc: 1.0)
[2024-11-08 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:28][root][INFO] - Training Epoch: 10/10, step 115/574 completed (loss: 0.003723239991813898, acc: 1.0)
[2024-11-08 04:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:29][root][INFO] - Training Epoch: 10/10, step 116/574 completed (loss: 0.0476677268743515, acc: 0.9599999785423279)
[2024-11-08 04:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:29][root][INFO] - Training Epoch: 10/10, step 117/574 completed (loss: 0.018818186596035957, acc: 1.0)
[2024-11-08 04:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:30][root][INFO] - Training Epoch: 10/10, step 118/574 completed (loss: 0.0008729756809771061, acc: 1.0)
[2024-11-08 04:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:31][root][INFO] - Training Epoch: 10/10, step 119/574 completed (loss: 0.02567565254867077, acc: 1.0)
[2024-11-08 04:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:31][root][INFO] - Training Epoch: 10/10, step 120/574 completed (loss: 0.007807896938174963, acc: 1.0)
[2024-11-08 04:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:32][root][INFO] - Training Epoch: 10/10, step 121/574 completed (loss: 0.16478852927684784, acc: 0.9545454382896423)
[2024-11-08 04:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:32][root][INFO] - Training Epoch: 10/10, step 122/574 completed (loss: 0.003948545083403587, acc: 1.0)
[2024-11-08 04:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:33][root][INFO] - Training Epoch: 10/10, step 123/574 completed (loss: 0.03226953372359276, acc: 1.0)
[2024-11-08 04:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:33][root][INFO] - Training Epoch: 10/10, step 124/574 completed (loss: 0.007865140214562416, acc: 1.0)
[2024-11-08 04:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:15][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3918, device='cuda:0') eval_epoch_loss=tensor(0.3306, device='cuda:0') eval_epoch_acc=tensor(0.9306, device='cuda:0')
[2024-11-08 04:37:15][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:37:15][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:37:18][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_125_loss_0.3306262493133545/model.pt
[2024-11-08 04:37:18][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:19][root][INFO] - Training Epoch: 10/10, step 125/574 completed (loss: 0.032853204756975174, acc: 1.0)
[2024-11-08 04:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:19][root][INFO] - Training Epoch: 10/10, step 126/574 completed (loss: 0.005292913876473904, acc: 1.0)
[2024-11-08 04:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:20][root][INFO] - Training Epoch: 10/10, step 127/574 completed (loss: 0.08788257092237473, acc: 0.9545454382896423)
[2024-11-08 04:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:20][root][INFO] - Training Epoch: 10/10, step 128/574 completed (loss: 0.0030659197364002466, acc: 1.0)
[2024-11-08 04:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:21][root][INFO] - Training Epoch: 10/10, step 129/574 completed (loss: 0.03387252241373062, acc: 1.0)
[2024-11-08 04:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:21][root][INFO] - Training Epoch: 10/10, step 130/574 completed (loss: 0.10825273394584656, acc: 0.9696969985961914)
[2024-11-08 04:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:21][root][INFO] - Training Epoch: 10/10, step 131/574 completed (loss: 0.19509105384349823, acc: 0.9259259104728699)
[2024-11-08 04:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:22][root][INFO] - Training Epoch: 10/10, step 132/574 completed (loss: 0.1546729952096939, acc: 0.9696969985961914)
[2024-11-08 04:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:22][root][INFO] - Training Epoch: 10/10, step 133/574 completed (loss: 0.008864444680511951, acc: 1.0)
[2024-11-08 04:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:22][root][INFO] - Training Epoch: 10/10, step 134/574 completed (loss: 0.005937495268881321, acc: 1.0)
[2024-11-08 04:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:23][root][INFO] - Training Epoch: 10/10, step 135/574 completed (loss: 0.06843777000904083, acc: 0.9473684430122375)
[2024-11-08 04:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:23][root][INFO] - Training Epoch: 10/10, step 136/574 completed (loss: 0.13125981390476227, acc: 0.9090909361839294)
[2024-11-08 04:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:24][root][INFO] - Training Epoch: 10/10, step 137/574 completed (loss: 0.273551881313324, acc: 0.8999999761581421)
[2024-11-08 04:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:24][root][INFO] - Training Epoch: 10/10, step 138/574 completed (loss: 0.003393098944798112, acc: 1.0)
[2024-11-08 04:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:24][root][INFO] - Training Epoch: 10/10, step 139/574 completed (loss: 0.0354396291077137, acc: 0.9696969985961914)
[2024-11-08 04:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:25][root][INFO] - Training Epoch: 10/10, step 140/574 completed (loss: 0.04753239452838898, acc: 0.9629629850387573)
[2024-11-08 04:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:25][root][INFO] - Training Epoch: 10/10, step 141/574 completed (loss: 0.3190804421901703, acc: 0.9696969985961914)
[2024-11-08 04:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:25][root][INFO] - Training Epoch: 10/10, step 142/574 completed (loss: 0.0015162162017077208, acc: 1.0)
[2024-11-08 04:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:26][root][INFO] - Training Epoch: 10/10, step 143/574 completed (loss: 0.37061387300491333, acc: 0.949999988079071)
[2024-11-08 04:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:27][root][INFO] - Training Epoch: 10/10, step 144/574 completed (loss: 0.15013986825942993, acc: 0.9473684430122375)
[2024-11-08 04:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:27][root][INFO] - Training Epoch: 10/10, step 145/574 completed (loss: 0.1124868094921112, acc: 0.9545454382896423)
[2024-11-08 04:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:28][root][INFO] - Training Epoch: 10/10, step 146/574 completed (loss: 0.0033305648248642683, acc: 1.0)
[2024-11-08 04:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:28][root][INFO] - Training Epoch: 10/10, step 147/574 completed (loss: 0.11858814209699631, acc: 0.9090909361839294)
[2024-11-08 04:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:28][root][INFO] - Training Epoch: 10/10, step 148/574 completed (loss: 0.0007132586906664073, acc: 1.0)
[2024-11-08 04:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:29][root][INFO] - Training Epoch: 10/10, step 149/574 completed (loss: 0.004643904976546764, acc: 1.0)
[2024-11-08 04:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:29][root][INFO] - Training Epoch: 10/10, step 150/574 completed (loss: 0.004143447615206242, acc: 1.0)
[2024-11-08 04:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:29][root][INFO] - Training Epoch: 10/10, step 151/574 completed (loss: 0.04376889020204544, acc: 1.0)
[2024-11-08 04:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:30][root][INFO] - Training Epoch: 10/10, step 152/574 completed (loss: 0.016300104558467865, acc: 1.0)
[2024-11-08 04:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:30][root][INFO] - Training Epoch: 10/10, step 153/574 completed (loss: 0.004127216525375843, acc: 1.0)
[2024-11-08 04:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:31][root][INFO] - Training Epoch: 10/10, step 154/574 completed (loss: 0.0008283128845505416, acc: 1.0)
[2024-11-08 04:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:31][root][INFO] - Training Epoch: 10/10, step 155/574 completed (loss: 0.013593743555247784, acc: 1.0)
[2024-11-08 04:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:31][root][INFO] - Training Epoch: 10/10, step 156/574 completed (loss: 0.2771367132663727, acc: 0.9545454382896423)
[2024-11-08 04:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:32][root][INFO] - Training Epoch: 10/10, step 157/574 completed (loss: 0.34442025423049927, acc: 0.8787878751754761)
[2024-11-08 04:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:34][root][INFO] - Training Epoch: 10/10, step 158/574 completed (loss: 0.26002225279808044, acc: 0.949999988079071)
[2024-11-08 04:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:35][root][INFO] - Training Epoch: 10/10, step 159/574 completed (loss: 0.12050040066242218, acc: 0.949999988079071)
[2024-11-08 04:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:35][root][INFO] - Training Epoch: 10/10, step 160/574 completed (loss: 0.06060919165611267, acc: 1.0)
[2024-11-08 04:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:36][root][INFO] - Training Epoch: 10/10, step 161/574 completed (loss: 0.1606149822473526, acc: 0.9545454382896423)
[2024-11-08 04:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:37][root][INFO] - Training Epoch: 10/10, step 162/574 completed (loss: 0.25174644589424133, acc: 0.949999988079071)
[2024-11-08 04:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:37][root][INFO] - Training Epoch: 10/10, step 163/574 completed (loss: 0.22665727138519287, acc: 0.9599999785423279)
[2024-11-08 04:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:38][root][INFO] - Training Epoch: 10/10, step 164/574 completed (loss: 0.02825615182518959, acc: 1.0)
[2024-11-08 04:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:38][root][INFO] - Training Epoch: 10/10, step 165/574 completed (loss: 0.13610942661762238, acc: 0.9444444179534912)
[2024-11-08 04:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:39][root][INFO] - Training Epoch: 10/10, step 166/574 completed (loss: 0.002636222168803215, acc: 1.0)
[2024-11-08 04:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:39][root][INFO] - Training Epoch: 10/10, step 167/574 completed (loss: 0.09107018262147903, acc: 0.9523809552192688)
[2024-11-08 04:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:40][root][INFO] - Training Epoch: 10/10, step 168/574 completed (loss: 0.0005197413847781718, acc: 1.0)
[2024-11-08 04:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:40][root][INFO] - Training Epoch: 10/10, step 169/574 completed (loss: 0.004774685949087143, acc: 1.0)
[2024-11-08 04:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:42][root][INFO] - Training Epoch: 10/10, step 170/574 completed (loss: 0.008465087972581387, acc: 1.0)
[2024-11-08 04:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:42][root][INFO] - Training Epoch: 10/10, step 171/574 completed (loss: 0.22799862921237946, acc: 0.9090909361839294)
[2024-11-08 04:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:42][root][INFO] - Training Epoch: 10/10, step 172/574 completed (loss: 0.007170836441218853, acc: 1.0)
[2024-11-08 04:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:43][root][INFO] - Training Epoch: 10/10, step 173/574 completed (loss: 0.006949956063181162, acc: 1.0)
[2024-11-08 04:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:44][root][INFO] - Training Epoch: 10/10, step 174/574 completed (loss: 0.09835867583751678, acc: 1.0)
[2024-11-08 04:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:44][root][INFO] - Training Epoch: 10/10, step 175/574 completed (loss: 0.10654674470424652, acc: 0.9523809552192688)
[2024-11-08 04:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:44][root][INFO] - Training Epoch: 10/10, step 176/574 completed (loss: 0.0007724965107627213, acc: 1.0)
[2024-11-08 04:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:46][root][INFO] - Training Epoch: 10/10, step 177/574 completed (loss: 0.00801035389304161, acc: 1.0)
[2024-11-08 04:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:47][root][INFO] - Training Epoch: 10/10, step 178/574 completed (loss: 0.24487920105457306, acc: 0.9473684430122375)
[2024-11-08 04:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:47][root][INFO] - Training Epoch: 10/10, step 179/574 completed (loss: 0.0006342417327687144, acc: 1.0)
[2024-11-08 04:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:47][root][INFO] - Training Epoch: 10/10, step 180/574 completed (loss: 0.0005156918778084219, acc: 1.0)
[2024-11-08 04:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:48][root][INFO] - Training Epoch: 10/10, step 181/574 completed (loss: 0.0003667107957880944, acc: 1.0)
[2024-11-08 04:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:48][root][INFO] - Training Epoch: 10/10, step 182/574 completed (loss: 0.020787786692380905, acc: 1.0)
[2024-11-08 04:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:49][root][INFO] - Training Epoch: 10/10, step 183/574 completed (loss: 0.00038108922308310866, acc: 1.0)
[2024-11-08 04:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:49][root][INFO] - Training Epoch: 10/10, step 184/574 completed (loss: 0.0006216547335498035, acc: 1.0)
[2024-11-08 04:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:50][root][INFO] - Training Epoch: 10/10, step 185/574 completed (loss: 0.019984984770417213, acc: 1.0)
[2024-11-08 04:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:50][root][INFO] - Training Epoch: 10/10, step 186/574 completed (loss: 0.015240689739584923, acc: 1.0)
[2024-11-08 04:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:51][root][INFO] - Training Epoch: 10/10, step 187/574 completed (loss: 0.001971284858882427, acc: 1.0)
[2024-11-08 04:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:51][root][INFO] - Training Epoch: 10/10, step 188/574 completed (loss: 0.0023961474653333426, acc: 1.0)
[2024-11-08 04:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:52][root][INFO] - Training Epoch: 10/10, step 189/574 completed (loss: 0.33177539706230164, acc: 0.9200000166893005)
[2024-11-08 04:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:53][root][INFO] - Training Epoch: 10/10, step 190/574 completed (loss: 0.020200934261083603, acc: 1.0)
[2024-11-08 04:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:54][root][INFO] - Training Epoch: 10/10, step 191/574 completed (loss: 0.05519850552082062, acc: 1.0)
[2024-11-08 04:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:55][root][INFO] - Training Epoch: 10/10, step 192/574 completed (loss: 0.2834495007991791, acc: 0.9473684430122375)
[2024-11-08 04:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:56][root][INFO] - Training Epoch: 10/10, step 193/574 completed (loss: 0.007018832489848137, acc: 1.0)
[2024-11-08 04:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:37:58][root][INFO] - Training Epoch: 10/10, step 194/574 completed (loss: 0.0593150295317173, acc: 0.9523809552192688)
[2024-11-08 04:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:00][root][INFO] - Training Epoch: 10/10, step 195/574 completed (loss: 0.027266094461083412, acc: 1.0)
[2024-11-08 04:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:00][root][INFO] - Training Epoch: 10/10, step 196/574 completed (loss: 0.0004161308170296252, acc: 1.0)
[2024-11-08 04:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:00][root][INFO] - Training Epoch: 10/10, step 197/574 completed (loss: 0.003890473861247301, acc: 1.0)
[2024-11-08 04:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:01][root][INFO] - Training Epoch: 10/10, step 198/574 completed (loss: 0.0027538167778402567, acc: 1.0)
[2024-11-08 04:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:01][root][INFO] - Training Epoch: 10/10, step 199/574 completed (loss: 0.016891932114958763, acc: 1.0)
[2024-11-08 04:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:02][root][INFO] - Training Epoch: 10/10, step 200/574 completed (loss: 0.020791837945580482, acc: 1.0)
[2024-11-08 04:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:02][root][INFO] - Training Epoch: 10/10, step 201/574 completed (loss: 0.007399548776447773, acc: 1.0)
[2024-11-08 04:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:03][root][INFO] - Training Epoch: 10/10, step 202/574 completed (loss: 0.0017034709453582764, acc: 1.0)
[2024-11-08 04:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:03][root][INFO] - Training Epoch: 10/10, step 203/574 completed (loss: 0.013568374328315258, acc: 1.0)
[2024-11-08 04:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:03][root][INFO] - Training Epoch: 10/10, step 204/574 completed (loss: 0.006482292432337999, acc: 1.0)
[2024-11-08 04:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:04][root][INFO] - Training Epoch: 10/10, step 205/574 completed (loss: 0.0009446619660593569, acc: 1.0)
[2024-11-08 04:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:04][root][INFO] - Training Epoch: 10/10, step 206/574 completed (loss: 0.004430131986737251, acc: 1.0)
[2024-11-08 04:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:05][root][INFO] - Training Epoch: 10/10, step 207/574 completed (loss: 0.10561306774616241, acc: 0.9473684430122375)
[2024-11-08 04:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:05][root][INFO] - Training Epoch: 10/10, step 208/574 completed (loss: 0.03492973372340202, acc: 1.0)
[2024-11-08 04:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:06][root][INFO] - Training Epoch: 10/10, step 209/574 completed (loss: 0.003271169262006879, acc: 1.0)
[2024-11-08 04:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:06][root][INFO] - Training Epoch: 10/10, step 210/574 completed (loss: 0.05789009854197502, acc: 0.9583333134651184)
[2024-11-08 04:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:07][root][INFO] - Training Epoch: 10/10, step 211/574 completed (loss: 0.03921150043606758, acc: 0.9677419066429138)
[2024-11-08 04:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:07][root][INFO] - Training Epoch: 10/10, step 212/574 completed (loss: 0.010063396766781807, acc: 1.0)
[2024-11-08 04:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:08][root][INFO] - Training Epoch: 10/10, step 213/574 completed (loss: 0.2072492241859436, acc: 0.9615384340286255)
[2024-11-08 04:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:09][root][INFO] - Training Epoch: 10/10, step 214/574 completed (loss: 0.0006322021945379674, acc: 1.0)
[2024-11-08 04:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:09][root][INFO] - Training Epoch: 10/10, step 215/574 completed (loss: 0.0016852980479598045, acc: 1.0)
[2024-11-08 04:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:09][root][INFO] - Training Epoch: 10/10, step 216/574 completed (loss: 0.008250479586422443, acc: 1.0)
[2024-11-08 04:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:10][root][INFO] - Training Epoch: 10/10, step 217/574 completed (loss: 0.0005223742336966097, acc: 1.0)
[2024-11-08 04:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:11][root][INFO] - Training Epoch: 10/10, step 218/574 completed (loss: 0.02883649431169033, acc: 1.0)
[2024-11-08 04:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:11][root][INFO] - Training Epoch: 10/10, step 219/574 completed (loss: 0.0017225873889401555, acc: 1.0)
[2024-11-08 04:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:11][root][INFO] - Training Epoch: 10/10, step 220/574 completed (loss: 0.0034609439317137003, acc: 1.0)
[2024-11-08 04:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:12][root][INFO] - Training Epoch: 10/10, step 221/574 completed (loss: 0.0005815702024847269, acc: 1.0)
[2024-11-08 04:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:12][root][INFO] - Training Epoch: 10/10, step 222/574 completed (loss: 0.09874004870653152, acc: 0.9615384340286255)
[2024-11-08 04:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:13][root][INFO] - Training Epoch: 10/10, step 223/574 completed (loss: 0.004053119104355574, acc: 1.0)
[2024-11-08 04:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:14][root][INFO] - Training Epoch: 10/10, step 224/574 completed (loss: 0.06444549560546875, acc: 0.9473684430122375)
[2024-11-08 04:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:15][root][INFO] - Training Epoch: 10/10, step 225/574 completed (loss: 0.00025229272432625294, acc: 1.0)
[2024-11-08 04:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:15][root][INFO] - Training Epoch: 10/10, step 226/574 completed (loss: 0.005212796851992607, acc: 1.0)
[2024-11-08 04:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:16][root][INFO] - Training Epoch: 10/10, step 227/574 completed (loss: 0.0063925073482096195, acc: 1.0)
[2024-11-08 04:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:16][root][INFO] - Training Epoch: 10/10, step 228/574 completed (loss: 0.2117442488670349, acc: 0.9583333134651184)
[2024-11-08 04:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:16][root][INFO] - Training Epoch: 10/10, step 229/574 completed (loss: 0.41867595911026, acc: 0.8695651888847351)
[2024-11-08 04:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:17][root][INFO] - Training Epoch: 10/10, step 230/574 completed (loss: 0.15338371694087982, acc: 0.9523809552192688)
[2024-11-08 04:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:17][root][INFO] - Training Epoch: 10/10, step 231/574 completed (loss: 0.14871402084827423, acc: 0.9473684430122375)
[2024-11-08 04:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:18][root][INFO] - Training Epoch: 10/10, step 232/574 completed (loss: 0.1844455599784851, acc: 0.9473684430122375)
[2024-11-08 04:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:18][root][INFO] - Training Epoch: 10/10, step 233/574 completed (loss: 0.04033523425459862, acc: 1.0)
[2024-11-08 04:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:19][root][INFO] - Training Epoch: 10/10, step 234/574 completed (loss: 0.19333934783935547, acc: 0.9545454382896423)
[2024-11-08 04:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:20][root][INFO] - Training Epoch: 10/10, step 235/574 completed (loss: 0.01885438896715641, acc: 1.0)
[2024-11-08 04:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:20][root][INFO] - Training Epoch: 10/10, step 236/574 completed (loss: 0.17656947672367096, acc: 0.9629629850387573)
[2024-11-08 04:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:20][root][INFO] - Training Epoch: 10/10, step 237/574 completed (loss: 0.09261735528707504, acc: 0.9696969985961914)
[2024-11-08 04:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:21][root][INFO] - Training Epoch: 10/10, step 238/574 completed (loss: 0.055767692625522614, acc: 1.0)
[2024-11-08 04:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:21][root][INFO] - Training Epoch: 10/10, step 239/574 completed (loss: 0.0019866626244038343, acc: 1.0)
[2024-11-08 04:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:21][root][INFO] - Training Epoch: 10/10, step 240/574 completed (loss: 0.2943136692047119, acc: 0.8947368264198303)
[2024-11-08 04:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:22][root][INFO] - Training Epoch: 10/10, step 241/574 completed (loss: 0.04360843077301979, acc: 0.9545454382896423)
[2024-11-08 04:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:23][root][INFO] - Training Epoch: 10/10, step 242/574 completed (loss: 0.030820118263363838, acc: 1.0)
[2024-11-08 04:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:23][root][INFO] - Training Epoch: 10/10, step 243/574 completed (loss: 0.01541281957179308, acc: 1.0)
[2024-11-08 04:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:24][root][INFO] - Training Epoch: 10/10, step 244/574 completed (loss: 0.005389110185205936, acc: 1.0)
[2024-11-08 04:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:24][root][INFO] - Training Epoch: 10/10, step 245/574 completed (loss: 0.32411399483680725, acc: 0.8999999761581421)
[2024-11-08 04:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:24][root][INFO] - Training Epoch: 10/10, step 246/574 completed (loss: 0.005775167606770992, acc: 1.0)
[2024-11-08 04:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:25][root][INFO] - Training Epoch: 10/10, step 247/574 completed (loss: 0.12877057492733002, acc: 0.95652174949646)
[2024-11-08 04:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:25][root][INFO] - Training Epoch: 10/10, step 248/574 completed (loss: 0.017852116376161575, acc: 1.0)
[2024-11-08 04:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:26][root][INFO] - Training Epoch: 10/10, step 249/574 completed (loss: 0.036963194608688354, acc: 1.0)
[2024-11-08 04:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:26][root][INFO] - Training Epoch: 10/10, step 250/574 completed (loss: 0.16297513246536255, acc: 0.8999999761581421)
[2024-11-08 04:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:26][root][INFO] - Training Epoch: 10/10, step 251/574 completed (loss: 0.04914483428001404, acc: 1.0)
[2024-11-08 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:27][root][INFO] - Training Epoch: 10/10, step 252/574 completed (loss: 0.10198196768760681, acc: 0.9545454382896423)
[2024-11-08 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:27][root][INFO] - Training Epoch: 10/10, step 253/574 completed (loss: 0.11867030709981918, acc: 0.9285714030265808)
[2024-11-08 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:28][root][INFO] - Training Epoch: 10/10, step 254/574 completed (loss: 0.0009264655527658761, acc: 1.0)
[2024-11-08 04:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:28][root][INFO] - Training Epoch: 10/10, step 255/574 completed (loss: 0.004374521318823099, acc: 1.0)
[2024-11-08 04:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:28][root][INFO] - Training Epoch: 10/10, step 256/574 completed (loss: 0.0014904469717293978, acc: 1.0)
[2024-11-08 04:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:29][root][INFO] - Training Epoch: 10/10, step 257/574 completed (loss: 0.0006918865838088095, acc: 1.0)
[2024-11-08 04:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:29][root][INFO] - Training Epoch: 10/10, step 258/574 completed (loss: 0.0011025459971278906, acc: 1.0)
[2024-11-08 04:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:30][root][INFO] - Training Epoch: 10/10, step 259/574 completed (loss: 0.03300165757536888, acc: 1.0)
[2024-11-08 04:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:31][root][INFO] - Training Epoch: 10/10, step 260/574 completed (loss: 0.006358038168400526, acc: 1.0)
[2024-11-08 04:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:31][root][INFO] - Training Epoch: 10/10, step 261/574 completed (loss: 0.0032417632173746824, acc: 1.0)
[2024-11-08 04:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:31][root][INFO] - Training Epoch: 10/10, step 262/574 completed (loss: 0.11751892417669296, acc: 0.970588207244873)
[2024-11-08 04:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:32][root][INFO] - Training Epoch: 10/10, step 263/574 completed (loss: 0.0675027072429657, acc: 1.0)
[2024-11-08 04:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:32][root][INFO] - Training Epoch: 10/10, step 264/574 completed (loss: 0.016796154901385307, acc: 1.0)
[2024-11-08 04:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:33][root][INFO] - Training Epoch: 10/10, step 265/574 completed (loss: 0.055450111627578735, acc: 0.9473684430122375)
[2024-11-08 04:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:34][root][INFO] - Training Epoch: 10/10, step 266/574 completed (loss: 0.0009258118807338178, acc: 1.0)
[2024-11-08 04:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:34][root][INFO] - Training Epoch: 10/10, step 267/574 completed (loss: 0.07832223922014236, acc: 0.9523809552192688)
[2024-11-08 04:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:16][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3719, device='cuda:0') eval_epoch_loss=tensor(0.3162, device='cuda:0') eval_epoch_acc=tensor(0.9281, device='cuda:0')
[2024-11-08 04:39:16][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:39:16][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:39:19][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_268_loss_0.3162238597869873/model.pt
[2024-11-08 04:39:19][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:20][root][INFO] - Training Epoch: 10/10, step 268/574 completed (loss: 0.10179537534713745, acc: 0.931034505367279)
[2024-11-08 04:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:20][root][INFO] - Training Epoch: 10/10, step 269/574 completed (loss: 0.006833038758486509, acc: 1.0)
[2024-11-08 04:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:21][root][INFO] - Training Epoch: 10/10, step 270/574 completed (loss: 0.02000744268298149, acc: 1.0)
[2024-11-08 04:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:21][root][INFO] - Training Epoch: 10/10, step 271/574 completed (loss: 0.004171273671090603, acc: 1.0)
[2024-11-08 04:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:21][root][INFO] - Training Epoch: 10/10, step 272/574 completed (loss: 0.0005974927917122841, acc: 1.0)
[2024-11-08 04:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:22][root][INFO] - Training Epoch: 10/10, step 273/574 completed (loss: 0.02371021918952465, acc: 1.0)
[2024-11-08 04:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:22][root][INFO] - Training Epoch: 10/10, step 274/574 completed (loss: 0.07839219272136688, acc: 0.9090909361839294)
[2024-11-08 04:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:23][root][INFO] - Training Epoch: 10/10, step 275/574 completed (loss: 0.00200484786182642, acc: 1.0)
[2024-11-08 04:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:23][root][INFO] - Training Epoch: 10/10, step 276/574 completed (loss: 0.06115404888987541, acc: 0.9655172228813171)
[2024-11-08 04:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:23][root][INFO] - Training Epoch: 10/10, step 277/574 completed (loss: 0.0005944297881796956, acc: 1.0)
[2024-11-08 04:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:24][root][INFO] - Training Epoch: 10/10, step 278/574 completed (loss: 0.0011768247932195663, acc: 1.0)
[2024-11-08 04:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:24][root][INFO] - Training Epoch: 10/10, step 279/574 completed (loss: 0.00011508598981890827, acc: 1.0)
[2024-11-08 04:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:24][root][INFO] - Training Epoch: 10/10, step 280/574 completed (loss: 0.16685637831687927, acc: 0.9444444179534912)
[2024-11-08 04:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:25][root][INFO] - Training Epoch: 10/10, step 281/574 completed (loss: 0.0009237877675332129, acc: 1.0)
[2024-11-08 04:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:25][root][INFO] - Training Epoch: 10/10, step 282/574 completed (loss: 0.000851876218803227, acc: 1.0)
[2024-11-08 04:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:26][root][INFO] - Training Epoch: 10/10, step 283/574 completed (loss: 0.16625449061393738, acc: 0.9655172228813171)
[2024-11-08 04:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:26][root][INFO] - Training Epoch: 10/10, step 284/574 completed (loss: 0.0857473835349083, acc: 0.9666666388511658)
[2024-11-08 04:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:27][root][INFO] - Training Epoch: 10/10, step 285/574 completed (loss: 0.02328803576529026, acc: 1.0)
[2024-11-08 04:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:27][root][INFO] - Training Epoch: 10/10, step 286/574 completed (loss: 0.20620793104171753, acc: 0.9047619104385376)
[2024-11-08 04:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:27][root][INFO] - Training Epoch: 10/10, step 287/574 completed (loss: 0.27234721183776855, acc: 0.9473684430122375)
[2024-11-08 04:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:28][root][INFO] - Training Epoch: 10/10, step 288/574 completed (loss: 0.0074655101634562016, acc: 1.0)
[2024-11-08 04:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:28][root][INFO] - Training Epoch: 10/10, step 289/574 completed (loss: 0.008571178652346134, acc: 1.0)
[2024-11-08 04:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:28][root][INFO] - Training Epoch: 10/10, step 290/574 completed (loss: 0.00887711439281702, acc: 1.0)
[2024-11-08 04:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:29][root][INFO] - Training Epoch: 10/10, step 291/574 completed (loss: 0.03367852792143822, acc: 0.9599999785423279)
[2024-11-08 04:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:29][root][INFO] - Training Epoch: 10/10, step 292/574 completed (loss: 0.004485397133976221, acc: 1.0)
[2024-11-08 04:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:29][root][INFO] - Training Epoch: 10/10, step 293/574 completed (loss: 0.005010966211557388, acc: 1.0)
[2024-11-08 04:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:30][root][INFO] - Training Epoch: 10/10, step 294/574 completed (loss: 0.01328408531844616, acc: 1.0)
[2024-11-08 04:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:31][root][INFO] - Training Epoch: 10/10, step 295/574 completed (loss: 0.004247714299708605, acc: 1.0)
[2024-11-08 04:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:31][root][INFO] - Training Epoch: 10/10, step 296/574 completed (loss: 0.04785742983222008, acc: 0.9545454382896423)
[2024-11-08 04:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:32][root][INFO] - Training Epoch: 10/10, step 297/574 completed (loss: 0.12394677847623825, acc: 0.9523809552192688)
[2024-11-08 04:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:32][root][INFO] - Training Epoch: 10/10, step 298/574 completed (loss: 0.005643306765705347, acc: 1.0)
[2024-11-08 04:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:32][root][INFO] - Training Epoch: 10/10, step 299/574 completed (loss: 0.0019686627201735973, acc: 1.0)
[2024-11-08 04:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:33][root][INFO] - Training Epoch: 10/10, step 300/574 completed (loss: 0.013272891752421856, acc: 1.0)
[2024-11-08 04:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:33][root][INFO] - Training Epoch: 10/10, step 301/574 completed (loss: 0.04445093870162964, acc: 0.9615384340286255)
[2024-11-08 04:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:33][root][INFO] - Training Epoch: 10/10, step 302/574 completed (loss: 0.001388296252116561, acc: 1.0)
[2024-11-08 04:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:34][root][INFO] - Training Epoch: 10/10, step 303/574 completed (loss: 0.012373086996376514, acc: 1.0)
[2024-11-08 04:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:34][root][INFO] - Training Epoch: 10/10, step 304/574 completed (loss: 0.012823482044041157, acc: 1.0)
[2024-11-08 04:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:35][root][INFO] - Training Epoch: 10/10, step 305/574 completed (loss: 0.02125025913119316, acc: 1.0)
[2024-11-08 04:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:35][root][INFO] - Training Epoch: 10/10, step 306/574 completed (loss: 0.3978781998157501, acc: 0.9090909361839294)
[2024-11-08 04:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:35][root][INFO] - Training Epoch: 10/10, step 307/574 completed (loss: 0.023564878851175308, acc: 1.0)
[2024-11-08 04:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:36][root][INFO] - Training Epoch: 10/10, step 308/574 completed (loss: 0.002131290966644883, acc: 1.0)
[2024-11-08 04:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:36][root][INFO] - Training Epoch: 10/10, step 309/574 completed (loss: 0.1351328045129776, acc: 0.9473684430122375)
[2024-11-08 04:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:37][root][INFO] - Training Epoch: 10/10, step 310/574 completed (loss: 0.025838643312454224, acc: 1.0)
[2024-11-08 04:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:37][root][INFO] - Training Epoch: 10/10, step 311/574 completed (loss: 0.08150570839643478, acc: 0.9545454382896423)
[2024-11-08 04:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:37][root][INFO] - Training Epoch: 10/10, step 312/574 completed (loss: 0.014137192629277706, acc: 1.0)
[2024-11-08 04:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:38][root][INFO] - Training Epoch: 10/10, step 313/574 completed (loss: 0.0026930510066449642, acc: 1.0)
[2024-11-08 04:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:38][root][INFO] - Training Epoch: 10/10, step 314/574 completed (loss: 0.1311367005109787, acc: 0.9629629850387573)
[2024-11-08 04:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:38][root][INFO] - Training Epoch: 10/10, step 315/574 completed (loss: 0.06109149754047394, acc: 0.9714285731315613)
[2024-11-08 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:39][root][INFO] - Training Epoch: 10/10, step 316/574 completed (loss: 0.10466893017292023, acc: 0.9615384340286255)
[2024-11-08 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:39][root][INFO] - Training Epoch: 10/10, step 317/574 completed (loss: 0.2977774143218994, acc: 0.9523809552192688)
[2024-11-08 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:40][root][INFO] - Training Epoch: 10/10, step 318/574 completed (loss: 0.12069560587406158, acc: 0.949999988079071)
[2024-11-08 04:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:40][root][INFO] - Training Epoch: 10/10, step 319/574 completed (loss: 0.0010565011762082577, acc: 1.0)
[2024-11-08 04:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:40][root][INFO] - Training Epoch: 10/10, step 320/574 completed (loss: 0.007194001227617264, acc: 1.0)
[2024-11-08 04:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:41][root][INFO] - Training Epoch: 10/10, step 321/574 completed (loss: 0.0009169146651402116, acc: 1.0)
[2024-11-08 04:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:41][root][INFO] - Training Epoch: 10/10, step 322/574 completed (loss: 0.28551235795021057, acc: 0.8695651888847351)
[2024-11-08 04:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:41][root][INFO] - Training Epoch: 10/10, step 323/574 completed (loss: 0.3158412277698517, acc: 0.9047619104385376)
[2024-11-08 04:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:42][root][INFO] - Training Epoch: 10/10, step 324/574 completed (loss: 0.11630408465862274, acc: 0.9473684430122375)
[2024-11-08 04:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:42][root][INFO] - Training Epoch: 10/10, step 325/574 completed (loss: 0.16132943332195282, acc: 0.9090909361839294)
[2024-11-08 04:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:42][root][INFO] - Training Epoch: 10/10, step 326/574 completed (loss: 0.21772648394107819, acc: 0.8947368264198303)
[2024-11-08 04:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:43][root][INFO] - Training Epoch: 10/10, step 327/574 completed (loss: 0.1389923095703125, acc: 0.9166666865348816)
[2024-11-08 04:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:43][root][INFO] - Training Epoch: 10/10, step 328/574 completed (loss: 0.01082379836589098, acc: 1.0)
[2024-11-08 04:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:44][root][INFO] - Training Epoch: 10/10, step 329/574 completed (loss: 0.036278560757637024, acc: 0.9629629850387573)
[2024-11-08 04:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:44][root][INFO] - Training Epoch: 10/10, step 330/574 completed (loss: 0.30053481459617615, acc: 0.9523809552192688)
[2024-11-08 04:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:44][root][INFO] - Training Epoch: 10/10, step 331/574 completed (loss: 0.13511566817760468, acc: 0.9473684430122375)
[2024-11-08 04:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:45][root][INFO] - Training Epoch: 10/10, step 332/574 completed (loss: 0.06526221334934235, acc: 1.0)
[2024-11-08 04:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:45][root][INFO] - Training Epoch: 10/10, step 333/574 completed (loss: 0.05746151879429817, acc: 0.9473684430122375)
[2024-11-08 04:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:45][root][INFO] - Training Epoch: 10/10, step 334/574 completed (loss: 0.4598517417907715, acc: 0.9090909361839294)
[2024-11-08 04:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:46][root][INFO] - Training Epoch: 10/10, step 335/574 completed (loss: 0.044301386922597885, acc: 0.9615384340286255)
[2024-11-08 04:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:46][root][INFO] - Training Epoch: 10/10, step 336/574 completed (loss: 0.11023584753274918, acc: 0.9583333134651184)
[2024-11-08 04:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:47][root][INFO] - Training Epoch: 10/10, step 337/574 completed (loss: 0.10062362998723984, acc: 0.9047619104385376)
[2024-11-08 04:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:47][root][INFO] - Training Epoch: 10/10, step 338/574 completed (loss: 0.22782626748085022, acc: 0.8695651888847351)
[2024-11-08 04:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:47][root][INFO] - Training Epoch: 10/10, step 339/574 completed (loss: 0.01516058761626482, acc: 1.0)
[2024-11-08 04:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:48][root][INFO] - Training Epoch: 10/10, step 340/574 completed (loss: 0.005622120574116707, acc: 1.0)
[2024-11-08 04:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:48][root][INFO] - Training Epoch: 10/10, step 341/574 completed (loss: 0.02058016136288643, acc: 1.0)
[2024-11-08 04:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:48][root][INFO] - Training Epoch: 10/10, step 342/574 completed (loss: 0.003701488021761179, acc: 1.0)
[2024-11-08 04:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:49][root][INFO] - Training Epoch: 10/10, step 343/574 completed (loss: 0.06801360845565796, acc: 1.0)
[2024-11-08 04:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:49][root][INFO] - Training Epoch: 10/10, step 344/574 completed (loss: 0.0037523319479078054, acc: 1.0)
[2024-11-08 04:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:50][root][INFO] - Training Epoch: 10/10, step 345/574 completed (loss: 0.04641995206475258, acc: 1.0)
[2024-11-08 04:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:50][root][INFO] - Training Epoch: 10/10, step 346/574 completed (loss: 0.2255939245223999, acc: 0.949999988079071)
[2024-11-08 04:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:50][root][INFO] - Training Epoch: 10/10, step 347/574 completed (loss: 0.05549945309758186, acc: 0.9583333134651184)
[2024-11-08 04:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:51][root][INFO] - Training Epoch: 10/10, step 348/574 completed (loss: 0.04819302633404732, acc: 0.95652174949646)
[2024-11-08 04:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:51][root][INFO] - Training Epoch: 10/10, step 349/574 completed (loss: 0.005121572874486446, acc: 1.0)
[2024-11-08 04:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:52][root][INFO] - Training Epoch: 10/10, step 350/574 completed (loss: 0.01079894695430994, acc: 1.0)
[2024-11-08 04:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:52][root][INFO] - Training Epoch: 10/10, step 351/574 completed (loss: 0.15010493993759155, acc: 0.9090909361839294)
[2024-11-08 04:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:52][root][INFO] - Training Epoch: 10/10, step 352/574 completed (loss: 0.04694483429193497, acc: 1.0)
[2024-11-08 04:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:53][root][INFO] - Training Epoch: 10/10, step 353/574 completed (loss: 0.0727841928601265, acc: 0.9599999785423279)
[2024-11-08 04:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:53][root][INFO] - Training Epoch: 10/10, step 354/574 completed (loss: 0.0403953455388546, acc: 1.0)
[2024-11-08 04:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:53][root][INFO] - Training Epoch: 10/10, step 355/574 completed (loss: 0.0385257788002491, acc: 1.0)
[2024-11-08 04:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:54][root][INFO] - Training Epoch: 10/10, step 356/574 completed (loss: 0.09257934987545013, acc: 0.949999988079071)
[2024-11-08 04:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:55][root][INFO] - Training Epoch: 10/10, step 357/574 completed (loss: 0.0035008687991648912, acc: 1.0)
[2024-11-08 04:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:55][root][INFO] - Training Epoch: 10/10, step 358/574 completed (loss: 0.0850309357047081, acc: 0.9545454382896423)
[2024-11-08 04:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:55][root][INFO] - Training Epoch: 10/10, step 359/574 completed (loss: 0.09443330019712448, acc: 0.9642857313156128)
[2024-11-08 04:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:56][root][INFO] - Training Epoch: 10/10, step 360/574 completed (loss: 0.0069185649044811726, acc: 1.0)
[2024-11-08 04:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:56][root][INFO] - Training Epoch: 10/10, step 361/574 completed (loss: 0.2370045781135559, acc: 0.9142857193946838)
[2024-11-08 04:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:57][root][INFO] - Training Epoch: 10/10, step 362/574 completed (loss: 0.05579356104135513, acc: 0.9545454382896423)
[2024-11-08 04:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:57][root][INFO] - Training Epoch: 10/10, step 363/574 completed (loss: 0.05451546236872673, acc: 1.0)
[2024-11-08 04:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:57][root][INFO] - Training Epoch: 10/10, step 364/574 completed (loss: 0.010844055563211441, acc: 1.0)
[2024-11-08 04:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:58][root][INFO] - Training Epoch: 10/10, step 365/574 completed (loss: 0.007919726893305779, acc: 1.0)
[2024-11-08 04:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:58][root][INFO] - Training Epoch: 10/10, step 366/574 completed (loss: 0.0018067742930725217, acc: 1.0)
[2024-11-08 04:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:58][root][INFO] - Training Epoch: 10/10, step 367/574 completed (loss: 0.0100769167765975, acc: 1.0)
[2024-11-08 04:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:59][root][INFO] - Training Epoch: 10/10, step 368/574 completed (loss: 0.002947652479633689, acc: 1.0)
[2024-11-08 04:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:39:59][root][INFO] - Training Epoch: 10/10, step 369/574 completed (loss: 0.010506905615329742, acc: 1.0)
[2024-11-08 04:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:00][root][INFO] - Training Epoch: 10/10, step 370/574 completed (loss: 0.07674898952245712, acc: 1.0)
[2024-11-08 04:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:01][root][INFO] - Training Epoch: 10/10, step 371/574 completed (loss: 0.13301250338554382, acc: 0.9473684430122375)
[2024-11-08 04:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:02][root][INFO] - Training Epoch: 10/10, step 372/574 completed (loss: 0.022688936442136765, acc: 1.0)
[2024-11-08 04:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:02][root][INFO] - Training Epoch: 10/10, step 373/574 completed (loss: 0.00336056319065392, acc: 1.0)
[2024-11-08 04:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:03][root][INFO] - Training Epoch: 10/10, step 374/574 completed (loss: 0.024954967200756073, acc: 1.0)
[2024-11-08 04:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:03][root][INFO] - Training Epoch: 10/10, step 375/574 completed (loss: 0.00091700250050053, acc: 1.0)
[2024-11-08 04:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:03][root][INFO] - Training Epoch: 10/10, step 376/574 completed (loss: 0.001103979884646833, acc: 1.0)
[2024-11-08 04:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:04][root][INFO] - Training Epoch: 10/10, step 377/574 completed (loss: 0.05312782898545265, acc: 1.0)
[2024-11-08 04:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:04][root][INFO] - Training Epoch: 10/10, step 378/574 completed (loss: 0.02534988708794117, acc: 1.0)
[2024-11-08 04:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:05][root][INFO] - Training Epoch: 10/10, step 379/574 completed (loss: 0.01609613560140133, acc: 1.0)
[2024-11-08 04:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:05][root][INFO] - Training Epoch: 10/10, step 380/574 completed (loss: 0.04022819176316261, acc: 1.0)
[2024-11-08 04:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:07][root][INFO] - Training Epoch: 10/10, step 381/574 completed (loss: 0.005362797528505325, acc: 1.0)
[2024-11-08 04:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:08][root][INFO] - Training Epoch: 10/10, step 382/574 completed (loss: 0.053768690675497055, acc: 1.0)
[2024-11-08 04:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:08][root][INFO] - Training Epoch: 10/10, step 383/574 completed (loss: 0.014169834554195404, acc: 1.0)
[2024-11-08 04:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:09][root][INFO] - Training Epoch: 10/10, step 384/574 completed (loss: 0.0007881714263930917, acc: 1.0)
[2024-11-08 04:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:09][root][INFO] - Training Epoch: 10/10, step 385/574 completed (loss: 0.0007139595109038055, acc: 1.0)
[2024-11-08 04:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:09][root][INFO] - Training Epoch: 10/10, step 386/574 completed (loss: 0.00015616203017998487, acc: 1.0)
[2024-11-08 04:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:10][root][INFO] - Training Epoch: 10/10, step 387/574 completed (loss: 0.00018489202193450183, acc: 1.0)
[2024-11-08 04:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:10][root][INFO] - Training Epoch: 10/10, step 388/574 completed (loss: 0.002745768055319786, acc: 1.0)
[2024-11-08 04:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:10][root][INFO] - Training Epoch: 10/10, step 389/574 completed (loss: 0.01526167057454586, acc: 1.0)
[2024-11-08 04:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:11][root][INFO] - Training Epoch: 10/10, step 390/574 completed (loss: 0.06851867586374283, acc: 0.9545454382896423)
[2024-11-08 04:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:11][root][INFO] - Training Epoch: 10/10, step 391/574 completed (loss: 0.0019785410258919, acc: 1.0)
[2024-11-08 04:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:12][root][INFO] - Training Epoch: 10/10, step 392/574 completed (loss: 0.028773702681064606, acc: 1.0)
[2024-11-08 04:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:12][root][INFO] - Training Epoch: 10/10, step 393/574 completed (loss: 0.07320904731750488, acc: 0.9473684430122375)
[2024-11-08 04:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:13][root][INFO] - Training Epoch: 10/10, step 394/574 completed (loss: 0.03976118937134743, acc: 1.0)
[2024-11-08 04:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:13][root][INFO] - Training Epoch: 10/10, step 395/574 completed (loss: 0.11236218363046646, acc: 0.9523809552192688)
[2024-11-08 04:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:14][root][INFO] - Training Epoch: 10/10, step 396/574 completed (loss: 0.00046985215158201754, acc: 1.0)
[2024-11-08 04:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:14][root][INFO] - Training Epoch: 10/10, step 397/574 completed (loss: 0.011947084218263626, acc: 1.0)
[2024-11-08 04:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:14][root][INFO] - Training Epoch: 10/10, step 398/574 completed (loss: 0.0007497617043554783, acc: 1.0)
[2024-11-08 04:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:15][root][INFO] - Training Epoch: 10/10, step 399/574 completed (loss: 0.005075348541140556, acc: 1.0)
[2024-11-08 04:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:15][root][INFO] - Training Epoch: 10/10, step 400/574 completed (loss: 0.009833205491304398, acc: 1.0)
[2024-11-08 04:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:16][root][INFO] - Training Epoch: 10/10, step 401/574 completed (loss: 0.03176816552877426, acc: 1.0)
[2024-11-08 04:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:16][root][INFO] - Training Epoch: 10/10, step 402/574 completed (loss: 0.005433513317257166, acc: 1.0)
[2024-11-08 04:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:17][root][INFO] - Training Epoch: 10/10, step 403/574 completed (loss: 0.027074463665485382, acc: 1.0)
[2024-11-08 04:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:17][root][INFO] - Training Epoch: 10/10, step 404/574 completed (loss: 0.14717131853103638, acc: 0.9714285731315613)
[2024-11-08 04:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:17][root][INFO] - Training Epoch: 10/10, step 405/574 completed (loss: 0.002135650720447302, acc: 1.0)
[2024-11-08 04:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:18][root][INFO] - Training Epoch: 10/10, step 406/574 completed (loss: 0.005458364728838205, acc: 1.0)
[2024-11-08 04:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:18][root][INFO] - Training Epoch: 10/10, step 407/574 completed (loss: 0.0005507020396180451, acc: 1.0)
[2024-11-08 04:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:18][root][INFO] - Training Epoch: 10/10, step 408/574 completed (loss: 0.0181138776242733, acc: 1.0)
[2024-11-08 04:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:19][root][INFO] - Training Epoch: 10/10, step 409/574 completed (loss: 0.0002389822038821876, acc: 1.0)
[2024-11-08 04:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:19][root][INFO] - Training Epoch: 10/10, step 410/574 completed (loss: 0.0010027679381892085, acc: 1.0)
[2024-11-08 04:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:01][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4149, device='cuda:0') eval_epoch_loss=tensor(0.3471, device='cuda:0') eval_epoch_acc=tensor(0.9301, device='cuda:0')
[2024-11-08 04:41:01][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:41:01][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:41:05][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_411_loss_0.3470799922943115/model.pt
[2024-11-08 04:41:05][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:05][root][INFO] - Training Epoch: 10/10, step 411/574 completed (loss: 0.10516965389251709, acc: 0.9642857313156128)
[2024-11-08 04:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:06][root][INFO] - Training Epoch: 10/10, step 412/574 completed (loss: 0.00047368594096042216, acc: 1.0)
[2024-11-08 04:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:06][root][INFO] - Training Epoch: 10/10, step 413/574 completed (loss: 0.0026443933602422476, acc: 1.0)
[2024-11-08 04:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:07][root][INFO] - Training Epoch: 10/10, step 414/574 completed (loss: 0.018122045323252678, acc: 1.0)
[2024-11-08 04:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:07][root][INFO] - Training Epoch: 10/10, step 415/574 completed (loss: 0.022962398827075958, acc: 1.0)
[2024-11-08 04:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:07][root][INFO] - Training Epoch: 10/10, step 416/574 completed (loss: 0.02375679835677147, acc: 1.0)
[2024-11-08 04:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:08][root][INFO] - Training Epoch: 10/10, step 417/574 completed (loss: 0.08821914345026016, acc: 0.9473684430122375)
[2024-11-08 04:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:08][root][INFO] - Training Epoch: 10/10, step 418/574 completed (loss: 0.026315035298466682, acc: 1.0)
[2024-11-08 04:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:08][root][INFO] - Training Epoch: 10/10, step 419/574 completed (loss: 0.023749714717268944, acc: 1.0)
[2024-11-08 04:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:09][root][INFO] - Training Epoch: 10/10, step 420/574 completed (loss: 0.010777716524899006, acc: 1.0)
[2024-11-08 04:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:09][root][INFO] - Training Epoch: 10/10, step 421/574 completed (loss: 0.025584395974874496, acc: 1.0)
[2024-11-08 04:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:09][root][INFO] - Training Epoch: 10/10, step 422/574 completed (loss: 0.001177725032903254, acc: 1.0)
[2024-11-08 04:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:10][root][INFO] - Training Epoch: 10/10, step 423/574 completed (loss: 0.00214283331297338, acc: 1.0)
[2024-11-08 04:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:10][root][INFO] - Training Epoch: 10/10, step 424/574 completed (loss: 0.10220213234424591, acc: 0.9523809552192688)
[2024-11-08 04:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:11][root][INFO] - Training Epoch: 10/10, step 425/574 completed (loss: 0.009090122766792774, acc: 1.0)
[2024-11-08 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:11][root][INFO] - Training Epoch: 10/10, step 426/574 completed (loss: 0.116183340549469, acc: 0.9473684430122375)
[2024-11-08 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:11][root][INFO] - Training Epoch: 10/10, step 427/574 completed (loss: 0.0015843495493754745, acc: 1.0)
[2024-11-08 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:12][root][INFO] - Training Epoch: 10/10, step 428/574 completed (loss: 0.0011338964104652405, acc: 1.0)
[2024-11-08 04:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:12][root][INFO] - Training Epoch: 10/10, step 429/574 completed (loss: 0.0005893801571801305, acc: 1.0)
[2024-11-08 04:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:12][root][INFO] - Training Epoch: 10/10, step 430/574 completed (loss: 0.0002627515059430152, acc: 1.0)
[2024-11-08 04:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:13][root][INFO] - Training Epoch: 10/10, step 431/574 completed (loss: 0.0002723616489674896, acc: 1.0)
[2024-11-08 04:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:13][root][INFO] - Training Epoch: 10/10, step 432/574 completed (loss: 0.00036404223646968603, acc: 1.0)
[2024-11-08 04:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:13][root][INFO] - Training Epoch: 10/10, step 433/574 completed (loss: 0.01517069898545742, acc: 1.0)
[2024-11-08 04:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:14][root][INFO] - Training Epoch: 10/10, step 434/574 completed (loss: 0.00040204307879321277, acc: 1.0)
[2024-11-08 04:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:14][root][INFO] - Training Epoch: 10/10, step 435/574 completed (loss: 0.0022337466944009066, acc: 1.0)
[2024-11-08 04:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:14][root][INFO] - Training Epoch: 10/10, step 436/574 completed (loss: 0.0006359200342558324, acc: 1.0)
[2024-11-08 04:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:15][root][INFO] - Training Epoch: 10/10, step 437/574 completed (loss: 0.0007755461847409606, acc: 1.0)
[2024-11-08 04:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:15][root][INFO] - Training Epoch: 10/10, step 438/574 completed (loss: 0.0004283897578716278, acc: 1.0)
[2024-11-08 04:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:16][root][INFO] - Training Epoch: 10/10, step 439/574 completed (loss: 0.049278486520051956, acc: 0.9642857313156128)
[2024-11-08 04:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:16][root][INFO] - Training Epoch: 10/10, step 440/574 completed (loss: 0.0020795713644474745, acc: 1.0)
[2024-11-08 04:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:17][root][INFO] - Training Epoch: 10/10, step 441/574 completed (loss: 0.06328485161066055, acc: 0.9473684430122375)
[2024-11-08 04:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:18][root][INFO] - Training Epoch: 10/10, step 442/574 completed (loss: 0.010924668982625008, acc: 1.0)
[2024-11-08 04:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:19][root][INFO] - Training Epoch: 10/10, step 443/574 completed (loss: 0.05028756707906723, acc: 0.9545454382896423)
[2024-11-08 04:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:19][root][INFO] - Training Epoch: 10/10, step 444/574 completed (loss: 0.008638998493552208, acc: 1.0)
[2024-11-08 04:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:20][root][INFO] - Training Epoch: 10/10, step 445/574 completed (loss: 0.0006008624332025647, acc: 1.0)
[2024-11-08 04:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:20][root][INFO] - Training Epoch: 10/10, step 446/574 completed (loss: 0.11215005815029144, acc: 0.9677419066429138)
[2024-11-08 04:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:20][root][INFO] - Training Epoch: 10/10, step 447/574 completed (loss: 0.2885798513889313, acc: 0.9354838728904724)
[2024-11-08 04:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:21][root][INFO] - Training Epoch: 10/10, step 448/574 completed (loss: 0.017585286870598793, acc: 1.0)
[2024-11-08 04:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:21][root][INFO] - Training Epoch: 10/10, step 449/574 completed (loss: 0.0018615685403347015, acc: 1.0)
[2024-11-08 04:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:22][root][INFO] - Training Epoch: 10/10, step 450/574 completed (loss: 0.0032114528585225344, acc: 1.0)
[2024-11-08 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:22][root][INFO] - Training Epoch: 10/10, step 451/574 completed (loss: 0.06447675824165344, acc: 0.9473684430122375)
[2024-11-08 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:22][root][INFO] - Training Epoch: 10/10, step 452/574 completed (loss: 0.0018995287828147411, acc: 1.0)
[2024-11-08 04:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:23][root][INFO] - Training Epoch: 10/10, step 453/574 completed (loss: 0.002214520238339901, acc: 1.0)
[2024-11-08 04:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:23][root][INFO] - Training Epoch: 10/10, step 454/574 completed (loss: 0.0028098500333726406, acc: 1.0)
[2024-11-08 04:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:23][root][INFO] - Training Epoch: 10/10, step 455/574 completed (loss: 0.0028066271916031837, acc: 1.0)
[2024-11-08 04:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:24][root][INFO] - Training Epoch: 10/10, step 456/574 completed (loss: 0.0005350661813281476, acc: 1.0)
[2024-11-08 04:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:24][root][INFO] - Training Epoch: 10/10, step 457/574 completed (loss: 0.00017025569104589522, acc: 1.0)
[2024-11-08 04:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:25][root][INFO] - Training Epoch: 10/10, step 458/574 completed (loss: 0.005475922022014856, acc: 1.0)
[2024-11-08 04:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:25][root][INFO] - Training Epoch: 10/10, step 459/574 completed (loss: 0.06621450930833817, acc: 0.9545454382896423)
[2024-11-08 04:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:25][root][INFO] - Training Epoch: 10/10, step 460/574 completed (loss: 0.0007934783352538943, acc: 1.0)
[2024-11-08 04:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:26][root][INFO] - Training Epoch: 10/10, step 461/574 completed (loss: 0.0015062898164615035, acc: 1.0)
[2024-11-08 04:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:26][root][INFO] - Training Epoch: 10/10, step 462/574 completed (loss: 0.027749741449952126, acc: 1.0)
[2024-11-08 04:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:27][root][INFO] - Training Epoch: 10/10, step 463/574 completed (loss: 0.031797997653484344, acc: 1.0)
[2024-11-08 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:27][root][INFO] - Training Epoch: 10/10, step 464/574 completed (loss: 0.0025305396411567926, acc: 1.0)
[2024-11-08 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:27][root][INFO] - Training Epoch: 10/10, step 465/574 completed (loss: 0.0004250328056514263, acc: 1.0)
[2024-11-08 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:28][root][INFO] - Training Epoch: 10/10, step 466/574 completed (loss: 0.002826418960466981, acc: 1.0)
[2024-11-08 04:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:28][root][INFO] - Training Epoch: 10/10, step 467/574 completed (loss: 0.004348262213170528, acc: 1.0)
[2024-11-08 04:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:28][root][INFO] - Training Epoch: 10/10, step 468/574 completed (loss: 0.004975136835128069, acc: 1.0)
[2024-11-08 04:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:29][root][INFO] - Training Epoch: 10/10, step 469/574 completed (loss: 0.0009837313555181026, acc: 1.0)
[2024-11-08 04:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:29][root][INFO] - Training Epoch: 10/10, step 470/574 completed (loss: 0.0024060045834630728, acc: 1.0)
[2024-11-08 04:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:29][root][INFO] - Training Epoch: 10/10, step 471/574 completed (loss: 0.01052431296557188, acc: 1.0)
[2024-11-08 04:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:30][root][INFO] - Training Epoch: 10/10, step 472/574 completed (loss: 0.030368678271770477, acc: 1.0)
[2024-11-08 04:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:30][root][INFO] - Training Epoch: 10/10, step 473/574 completed (loss: 0.03803955018520355, acc: 0.9523809552192688)
[2024-11-08 04:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:31][root][INFO] - Training Epoch: 10/10, step 474/574 completed (loss: 0.06281471252441406, acc: 0.9473684430122375)
[2024-11-08 04:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:31][root][INFO] - Training Epoch: 10/10, step 475/574 completed (loss: 0.032019879668951035, acc: 1.0)
[2024-11-08 04:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:32][root][INFO] - Training Epoch: 10/10, step 476/574 completed (loss: 0.0007414891733787954, acc: 1.0)
[2024-11-08 04:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:32][root][INFO] - Training Epoch: 10/10, step 477/574 completed (loss: 0.1700516939163208, acc: 0.9090909361839294)
[2024-11-08 04:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:32][root][INFO] - Training Epoch: 10/10, step 478/574 completed (loss: 0.00037604724639095366, acc: 1.0)
[2024-11-08 04:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:33][root][INFO] - Training Epoch: 10/10, step 479/574 completed (loss: 0.09503184258937836, acc: 0.9677419066429138)
[2024-11-08 04:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:33][root][INFO] - Training Epoch: 10/10, step 480/574 completed (loss: 0.01545372512191534, acc: 1.0)
[2024-11-08 04:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:34][root][INFO] - Training Epoch: 10/10, step 481/574 completed (loss: 0.006581241264939308, acc: 1.0)
[2024-11-08 04:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:34][root][INFO] - Training Epoch: 10/10, step 482/574 completed (loss: 0.147987961769104, acc: 0.8947368264198303)
[2024-11-08 04:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:34][root][INFO] - Training Epoch: 10/10, step 483/574 completed (loss: 0.224301278591156, acc: 0.9545454382896423)
[2024-11-08 04:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:35][root][INFO] - Training Epoch: 10/10, step 484/574 completed (loss: 0.02247498743236065, acc: 1.0)
[2024-11-08 04:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:35][root][INFO] - Training Epoch: 10/10, step 485/574 completed (loss: 0.001828727894462645, acc: 1.0)
[2024-11-08 04:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:35][root][INFO] - Training Epoch: 10/10, step 486/574 completed (loss: 0.0281032957136631, acc: 1.0)
[2024-11-08 04:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:36][root][INFO] - Training Epoch: 10/10, step 487/574 completed (loss: 0.04151107743382454, acc: 1.0)
[2024-11-08 04:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:36][root][INFO] - Training Epoch: 10/10, step 488/574 completed (loss: 0.14200535416603088, acc: 0.9615384340286255)
[2024-11-08 04:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:37][root][INFO] - Training Epoch: 10/10, step 489/574 completed (loss: 0.0627068504691124, acc: 0.9523809552192688)
[2024-11-08 04:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:37][root][INFO] - Training Epoch: 10/10, step 490/574 completed (loss: 0.0011600770521908998, acc: 1.0)
[2024-11-08 04:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:37][root][INFO] - Training Epoch: 10/10, step 491/574 completed (loss: 0.1013808399438858, acc: 0.9473684430122375)
[2024-11-08 04:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:38][root][INFO] - Training Epoch: 10/10, step 492/574 completed (loss: 0.0004633528005797416, acc: 1.0)
[2024-11-08 04:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:38][root][INFO] - Training Epoch: 10/10, step 493/574 completed (loss: 0.39595475792884827, acc: 0.8571428656578064)
[2024-11-08 04:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:39][root][INFO] - Training Epoch: 10/10, step 494/574 completed (loss: 0.07665447145700455, acc: 0.9583333134651184)
[2024-11-08 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:39][root][INFO] - Training Epoch: 10/10, step 495/574 completed (loss: 0.040411435067653656, acc: 0.9642857313156128)
[2024-11-08 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:39][root][INFO] - Training Epoch: 10/10, step 496/574 completed (loss: 0.010974179022014141, acc: 1.0)
[2024-11-08 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:40][root][INFO] - Training Epoch: 10/10, step 497/574 completed (loss: 0.04447295516729355, acc: 0.949999988079071)
[2024-11-08 04:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:40][root][INFO] - Training Epoch: 10/10, step 498/574 completed (loss: 0.4118535816669464, acc: 0.8947368264198303)
[2024-11-08 04:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:41][root][INFO] - Training Epoch: 10/10, step 499/574 completed (loss: 0.0027900272980332375, acc: 1.0)
[2024-11-08 04:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:41][root][INFO] - Training Epoch: 10/10, step 500/574 completed (loss: 0.02991512045264244, acc: 1.0)
[2024-11-08 04:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:41][root][INFO] - Training Epoch: 10/10, step 501/574 completed (loss: 0.002509953221306205, acc: 1.0)
[2024-11-08 04:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:42][root][INFO] - Training Epoch: 10/10, step 502/574 completed (loss: 0.0016036959132179618, acc: 1.0)
[2024-11-08 04:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:42][root][INFO] - Training Epoch: 10/10, step 503/574 completed (loss: 0.15960898995399475, acc: 0.9354838728904724)
[2024-11-08 04:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:42][root][INFO] - Training Epoch: 10/10, step 504/574 completed (loss: 0.18764552474021912, acc: 0.95652174949646)
[2024-11-08 04:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:43][root][INFO] - Training Epoch: 10/10, step 505/574 completed (loss: 0.06512745469808578, acc: 1.0)
[2024-11-08 04:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:43][root][INFO] - Training Epoch: 10/10, step 506/574 completed (loss: 0.21653157472610474, acc: 0.9473684430122375)
[2024-11-08 04:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:44][root][INFO] - Training Epoch: 10/10, step 507/574 completed (loss: 0.009082112461328506, acc: 1.0)
[2024-11-08 04:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:45][root][INFO] - Training Epoch: 10/10, step 508/574 completed (loss: 0.12222008407115936, acc: 0.949999988079071)
[2024-11-08 04:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:45][root][INFO] - Training Epoch: 10/10, step 509/574 completed (loss: 0.5069834589958191, acc: 0.9090909361839294)
[2024-11-08 04:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:45][root][INFO] - Training Epoch: 10/10, step 510/574 completed (loss: 0.0005274916184134781, acc: 1.0)
[2024-11-08 04:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:46][root][INFO] - Training Epoch: 10/10, step 511/574 completed (loss: 0.00672734621912241, acc: 1.0)
[2024-11-08 04:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:49][root][INFO] - Training Epoch: 10/10, step 512/574 completed (loss: 0.0662788599729538, acc: 0.9642857313156128)
[2024-11-08 04:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:50][root][INFO] - Training Epoch: 10/10, step 513/574 completed (loss: 0.00994912814348936, acc: 1.0)
[2024-11-08 04:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:51][root][INFO] - Training Epoch: 10/10, step 514/574 completed (loss: 0.14132535457611084, acc: 0.9473684430122375)
[2024-11-08 04:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:51][root][INFO] - Training Epoch: 10/10, step 515/574 completed (loss: 0.018622176721692085, acc: 1.0)
[2024-11-08 04:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:52][root][INFO] - Training Epoch: 10/10, step 516/574 completed (loss: 0.0036546369083225727, acc: 1.0)
[2024-11-08 04:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:52][root][INFO] - Training Epoch: 10/10, step 517/574 completed (loss: 0.002371559152379632, acc: 1.0)
[2024-11-08 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:53][root][INFO] - Training Epoch: 10/10, step 518/574 completed (loss: 0.1399318277835846, acc: 0.9642857313156128)
[2024-11-08 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:53][root][INFO] - Training Epoch: 10/10, step 519/574 completed (loss: 0.08366475999355316, acc: 0.9629629850387573)
[2024-11-08 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:54][root][INFO] - Training Epoch: 10/10, step 520/574 completed (loss: 0.009937530383467674, acc: 1.0)
[2024-11-08 04:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:55][root][INFO] - Training Epoch: 10/10, step 521/574 completed (loss: 0.03355417400598526, acc: 1.0)
[2024-11-08 04:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:55][root][INFO] - Training Epoch: 10/10, step 522/574 completed (loss: 0.027642298489809036, acc: 1.0)
[2024-11-08 04:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:56][root][INFO] - Training Epoch: 10/10, step 523/574 completed (loss: 0.30813321471214294, acc: 0.9473684430122375)
[2024-11-08 04:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:57][root][INFO] - Training Epoch: 10/10, step 524/574 completed (loss: 0.14549733698368073, acc: 0.9545454382896423)
[2024-11-08 04:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:57][root][INFO] - Training Epoch: 10/10, step 525/574 completed (loss: 0.005545256659388542, acc: 1.0)
[2024-11-08 04:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:57][root][INFO] - Training Epoch: 10/10, step 526/574 completed (loss: 0.10266690701246262, acc: 0.9545454382896423)
[2024-11-08 04:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:58][root][INFO] - Training Epoch: 10/10, step 527/574 completed (loss: 0.0368390753865242, acc: 1.0)
[2024-11-08 04:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:58][root][INFO] - Training Epoch: 10/10, step 528/574 completed (loss: 0.060027364641427994, acc: 0.9642857313156128)
[2024-11-08 04:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:58][root][INFO] - Training Epoch: 10/10, step 529/574 completed (loss: 0.009159679524600506, acc: 1.0)
[2024-11-08 04:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:59][root][INFO] - Training Epoch: 10/10, step 530/574 completed (loss: 0.38365060091018677, acc: 0.8947368264198303)
[2024-11-08 04:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:41:59][root][INFO] - Training Epoch: 10/10, step 531/574 completed (loss: 0.0478888638317585, acc: 1.0)
[2024-11-08 04:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:00][root][INFO] - Training Epoch: 10/10, step 532/574 completed (loss: 0.02447529137134552, acc: 1.0)
[2024-11-08 04:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:00][root][INFO] - Training Epoch: 10/10, step 533/574 completed (loss: 0.004149576649069786, acc: 1.0)
[2024-11-08 04:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:00][root][INFO] - Training Epoch: 10/10, step 534/574 completed (loss: 0.011287098750472069, acc: 1.0)
[2024-11-08 04:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:01][root][INFO] - Training Epoch: 10/10, step 535/574 completed (loss: 0.002288523595780134, acc: 1.0)
[2024-11-08 04:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:01][root][INFO] - Training Epoch: 10/10, step 536/574 completed (loss: 0.007565835490822792, acc: 1.0)
[2024-11-08 04:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:02][root][INFO] - Training Epoch: 10/10, step 537/574 completed (loss: 0.005604384932667017, acc: 1.0)
[2024-11-08 04:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:02][root][INFO] - Training Epoch: 10/10, step 538/574 completed (loss: 0.013171073980629444, acc: 1.0)
[2024-11-08 04:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:03][root][INFO] - Training Epoch: 10/10, step 539/574 completed (loss: 0.03773793205618858, acc: 1.0)
[2024-11-08 04:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:03][root][INFO] - Training Epoch: 10/10, step 540/574 completed (loss: 0.13664987683296204, acc: 0.9047619104385376)
[2024-11-08 04:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:03][root][INFO] - Training Epoch: 10/10, step 541/574 completed (loss: 0.02768898941576481, acc: 1.0)
[2024-11-08 04:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:04][root][INFO] - Training Epoch: 10/10, step 542/574 completed (loss: 0.0032639673445373774, acc: 1.0)
[2024-11-08 04:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:04][root][INFO] - Training Epoch: 10/10, step 543/574 completed (loss: 0.0011451973114162683, acc: 1.0)
[2024-11-08 04:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:04][root][INFO] - Training Epoch: 10/10, step 544/574 completed (loss: 0.007777630351483822, acc: 1.0)
[2024-11-08 04:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:05][root][INFO] - Training Epoch: 10/10, step 545/574 completed (loss: 0.0009142336202785373, acc: 1.0)
[2024-11-08 04:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:05][root][INFO] - Training Epoch: 10/10, step 546/574 completed (loss: 0.1965983361005783, acc: 0.949999988079071)
[2024-11-08 04:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:06][root][INFO] - Training Epoch: 10/10, step 547/574 completed (loss: 0.005264179315418005, acc: 1.0)
[2024-11-08 04:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:06][root][INFO] - Training Epoch: 10/10, step 548/574 completed (loss: 0.016673747450113297, acc: 1.0)
[2024-11-08 04:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:06][root][INFO] - Training Epoch: 10/10, step 549/574 completed (loss: 0.003986444789916277, acc: 1.0)
[2024-11-08 04:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:07][root][INFO] - Training Epoch: 10/10, step 550/574 completed (loss: 0.0013899626210331917, acc: 1.0)
[2024-11-08 04:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:07][root][INFO] - Training Epoch: 10/10, step 551/574 completed (loss: 0.0011042546248063445, acc: 1.0)
[2024-11-08 04:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:07][root][INFO] - Training Epoch: 10/10, step 552/574 completed (loss: 0.004350927192717791, acc: 1.0)
[2024-11-08 04:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:08][root][INFO] - Training Epoch: 10/10, step 553/574 completed (loss: 0.10181953758001328, acc: 0.9523809552192688)
[2024-11-08 04:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:50][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3256, device='cuda:0') eval_epoch_loss=tensor(0.2819, device='cuda:0') eval_epoch_acc=tensor(0.9330, device='cuda:0')
[2024-11-08 04:42:50][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2024-11-08 04:42:50][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2024-11-08 04:42:53][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_554_loss_0.28186818957328796/model.pt
[2024-11-08 04:42:53][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft directory
[2024-11-08 04:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:54][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.006244595628231764, acc: 1.0)
[2024-11-08 04:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:54][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.002978774718940258, acc: 1.0)
[2024-11-08 04:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:54][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.0005138671840541065, acc: 1.0)
[2024-11-08 04:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:55][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.0019247710006311536, acc: 1.0)
[2024-11-08 04:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:55][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.06392712891101837, acc: 0.9642857313156128)
[2024-11-08 04:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:55][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.20634357631206512, acc: 0.9629629850387573)
[2024-11-08 04:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:56][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.01153360866010189, acc: 1.0)
[2024-11-08 04:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:56][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.0029389408882707357, acc: 1.0)
[2024-11-08 04:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:56][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.0014282251941040158, acc: 1.0)
[2024-11-08 04:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:57][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.002570490585640073, acc: 1.0)
[2024-11-08 04:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:57][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.01318539958447218, acc: 1.0)
[2024-11-08 04:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:58][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.044509559869766235, acc: 1.0)
[2024-11-08 04:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:58][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.020878644660115242, acc: 1.0)
[2024-11-08 04:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:58][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.0018392845522612333, acc: 1.0)
[2024-11-08 04:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:59][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.09693527966737747, acc: 0.9677419066429138)
[2024-11-08 04:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:59][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.0004973424365743995, acc: 1.0)
[2024-11-08 04:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:42:59][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.015657441690564156, acc: 1.0)
[2024-11-08 04:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:43:00][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.06164196878671646, acc: 1.0)
[2024-11-08 04:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:43:00][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.08007225394248962, acc: 0.9545454382896423)
[2024-11-08 04:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:43:01][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.03117346204817295, acc: 1.0)
[2024-11-08 04:43:01][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.0571, train_epoch_loss=0.0555, epoch time 455.1604563947767s
[2024-11-08 04:43:01][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2024-11-08 04:43:01][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 14 GB
[2024-11-08 04:43:01][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2024-11-08 04:43:01][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 7
[2024-11-08 04:43:01][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2024-11-08 04:43:01][root][INFO] - Key: avg_train_prep, Value: 1.5342999696731567
[2024-11-08 04:43:01][root][INFO] - Key: avg_train_loss, Value: 0.2715899646282196
[2024-11-08 04:43:01][root][INFO] - Key: avg_train_acc, Value: 0.9280304908752441
[2024-11-08 04:43:01][root][INFO] - Key: avg_eval_prep, Value: 1.5141657590866089
[2024-11-08 04:43:01][root][INFO] - Key: avg_eval_loss, Value: 0.3413477838039398
[2024-11-08 04:43:01][root][INFO] - Key: avg_eval_acc, Value: 0.9084854125976562
[2024-11-08 04:43:01][root][INFO] - Key: avg_epoch_time, Value: 469.0941661419347
[2024-11-08 04:43:01][root][INFO] - Key: avg_checkpoint_time, Value: 3.6707855361513793
Selected lowest loss checkpoint: asr_epoch_4_step_137_loss_0.23283101618289948
Latest file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_137_loss_0.23283101618289948/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_137_loss_0.23283101618289948
[2024-11-08 04:43:48][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False}
[2024-11-08 04:43:48][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-11-08 04:43:48][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme'}
[2024-11-08 04:43:50][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-11-08 04:43:56][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 04:43:56][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-11-08 04:43:56][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-11-08 04:43:56][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-11-08 04:43:58][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-08 04:43:58][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-08 04:43:58][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2024-11-08 04:43:58][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2024-11-08 04:44:06][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 04:44:06][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2024-11-08 04:44:06][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2024-11-08 04:44:07][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2024-11-08 04:44:07][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2024-11-08 04:44:07][slam_llm.utils.train_utils][INFO] - --> Module dual
[2024-11-08 04:44:07][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2024-11-08 04:44:07][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_137_loss_0.23283101618289948/model.pt
[2024-11-08 04:44:09][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-11-08 04:44:09][slam_llm.utils.train_utils][INFO] - --> asr has 346.244736 Million params

[2024-11-08 04:44:12][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-11-08 04:44:15][root][INFO] - --> Training Set Length = 652
[2024-11-08 04:44:15][root][INFO] - =====================================
[2024-11-08 04:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2024-11-08 04:46:37][slam_llm.models.slam_model][INFO] - modality encoder
Initial Word Error Rate (WER) before filtering: w
Number of GT lines after filtering: 652
Number of original PRED lines: 652
Number of filtered repeated lines: 0 out of 652
Filtered Word Error Rate (WER) after removing repeated lines: 0.7401486988847583
